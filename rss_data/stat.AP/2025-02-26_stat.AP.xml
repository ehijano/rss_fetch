<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simulation and Harmonic Analysis of k-Space Readout (SHAKER)</title>
      <link>https://arxiv.org/abs/2502.17620</link>
      <description>arXiv:2502.17620v1 Announce Type: new 
Abstract: In the realm of neuroimaging research, the demand for efficient and accurate simulation tools for functional magnetic resonance imaging (fMRI) data is ever increasing. We present SHAKER, a comprehensive MATLAB package for simulating complex-valued fMRI time series data that will advance understanding and implementation of the MR signal equation and related physics principles to fMRI simulation. The core objective of the package is to provide researchers with a user-friendly MATLAB graphical user interface (GUI) tool capable of generating complex-valued fMRI time series data. This tool will allow researchers to input various parameters related to the MRI scan and receive simulated k-space data with ease, facilitating a deeper understanding of the intricacies of the generation and interpretation of fMRI data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17620v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John C. Bodenschatz, Daniel B. Rowe</dc:creator>
    </item>
    <item>
      <title>StatLLM: A Dataset for Evaluating the Performance of Large Language Models in Statistical Analysis</title>
      <link>https://arxiv.org/abs/2502.17657</link>
      <description>arXiv:2502.17657v1 Announce Type: new 
Abstract: The coding capabilities of large language models (LLMs) have opened up new opportunities for automatic statistical analysis in machine learning and data science. However, before their widespread adoption, it is crucial to assess the accuracy of code generated by LLMs. A major challenge in this evaluation lies in the absence of a benchmark dataset for statistical code (e.g., SAS and R). To fill in this gap, this paper introduces StatLLM, an open-source dataset for evaluating the performance of LLMs in statistical analysis. The StatLLM dataset comprises three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. The first component includes statistical analysis tasks spanning a variety of analyses and datasets, providing problem descriptions, dataset details, and human-verified SAS code. The second component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 for those tasks. The third component contains evaluation scores from human experts in assessing the correctness, effectiveness, readability, executability, and output accuracy of the LLM-generated code. We also illustrate the unique potential of the established benchmark dataset for (1) evaluating and enhancing natural language processing metrics, (2) assessing and improving LLM performance in statistical coding, and (3) developing and testing of next-generation statistical software - advancements that are crucial for data science and machine learning research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17657v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Song, Lina Lee, Kexin Xie, Xueying Liu, Xinwei Deng, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Protocol For An Observational Study On The Effects Of Combinations Of Adverse Childhood Experiences On Adult Depression</title>
      <link>https://arxiv.org/abs/2502.17679</link>
      <description>arXiv:2502.17679v1 Announce Type: new 
Abstract: Adverse childhood experiences (ACEs) have been linked to a wide range of negative health outcomes in adulthood. However, few studies have investigated what specific combinations of ACEs most substantially impact mental health. In this article, we provide the protocol for our observational study of the effects of combinations of ACEs on adult depression. We use data from the 2023 Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We will evaluate the replicability of our findings by splitting the sample into two discrete subpopulations of individuals. We employ data turnover for this analysis, enabling a single team of statisticians and domain experts to collaboratively evaluate the strength of evidence, and also integrating both qualitative and quantitative insights from exploratory data analysis. We outline our analysis plan using this method and conclude with a brief discussion of several specifics for our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17679v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruizhe Zhang, Jooyoung Kong, Dylan S. Small, William Bekerman</dc:creator>
    </item>
    <item>
      <title>A Unified Model of Text and Citations for Topic-Specific Citation Networks</title>
      <link>https://arxiv.org/abs/2502.17708</link>
      <description>arXiv:2502.17708v1 Announce Type: new 
Abstract: Social scientists analyze citation networks to study how documents influence subsequent work across various domains such as judicial politics and international relations. However, conventional approaches that summarize document attributes in citation networks often overlook the diverse semantic contexts in which citations occur. This paper develops the paragraph-citation topic model (PCTM), which analyzes citation networks and document texts jointly. The PCTM extends conventional topic models by assigning topics to paragraphs of citing documents, allowing citations to share topics with their embedding paragraphs. Our empirical analysis of U.S. Supreme Court opinions in the privacy issue domain, which includes cases on reproductive rights, demonstrates that citations within individual documents frequently span multiple substantive areas, and citations to individual documents show considerable topical diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17708v1</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>ByungKoo Kim, Saki Kuzushima, Yuki Shiraito</dc:creator>
    </item>
    <item>
      <title>A Dynamic Dirichlet Process Mixture Model for the Partisan Realignment of Civil Rights Issues in the U.S. House of Representatives</title>
      <link>https://arxiv.org/abs/2502.17733</link>
      <description>arXiv:2502.17733v1 Announce Type: new 
Abstract: Evolutionary societal changes often prompt a debate. The positions of the two major political parties in the United States on civil rights issues underwent a reversal in the 20th century. The conventional view holds that this shift was a structural break in the 1960s, driven by party elites, while recent studies argue that the change was a more gradual process that began as early as the 1930s, driven by local rank-and-file party members. Motivated by this controversy, this paper develops a nonparametric Bayesian model that incorporates a hidden Markov model into the Dirichlet process mixture model. A distinctive feature of the proposed approach is that it models a process in which multiple latent clusters emerge and diminish as a continuing process so that it uncovers any of steady, sudden, and repeated shifts in analysing longitudinal data. Our model estimates each party's positions on civil rights in each state based on the legislative activities of their Congressional members, identifying cross- and within-party coalitions over time. We find evidence of gradual racial realignment in the 20th century, with two periods of fast changes during the 1948 election and the Civil Rights Movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17733v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nuannuan Xiang, Yuki Shiraito</dc:creator>
    </item>
    <item>
      <title>Measuring Interlayer Dependence of Large Degrees in Multilayer Inhomogeneous Random Graphs</title>
      <link>https://arxiv.org/abs/2502.17934</link>
      <description>arXiv:2502.17934v1 Announce Type: new 
Abstract: Accurately capturing interlayer dependence is essential for understanding the structure of complex multilayer networks. We propose an upper tail dependence estimator specifically designed for multilayer networks, leveraging multilayer inhomogeneous random graphs and multivariate regular variation to model extremal dependence. We establish the consistency of the estimator and demonstrate its practical effectiveness through real-data analysis of Reddit. Our findings reveal how financial market dynamics influence user interactions in the BitcoinMarkets subreddit and how seasonal trends shape engagement in sports-related subreddits. This work provides a rigorous and practical tool for quantifying extremal dependence across network layers, offering valuable insights into risk propagation and interaction patterns in multilayer systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17934v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoye Han, Tiandong Wang</dc:creator>
    </item>
    <item>
      <title>Differentially private synthesis of Spatial Point Processes</title>
      <link>https://arxiv.org/abs/2502.18198</link>
      <description>arXiv:2502.18198v1 Announce Type: new 
Abstract: This paper proposes a method to generate synthetic data for spatial point patterns within the differential privacy (DP) framework. Specifically, we define a differentially private Poisson point synthesizer (PPS) and Cox point synthesizer (CPS) to generate synthetic point patterns with the concept of the $\alpha$-neighborhood that relaxes the original definition of DP. We present three example models to construct a differentially private PPS and CPS, providing sufficient conditions on their parameters to ensure the DP given a specified privacy budget. In addition, we demonstrate that the synthesizers can be applied to point patterns on the linear network. Simulation experiments demonstrate that the proposed approaches effectively maintain the privacy and utility of synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18198v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dangchan Kim, Chae Young Lim</dc:creator>
    </item>
    <item>
      <title>Permutation extropy: a time series complexity measure</title>
      <link>https://arxiv.org/abs/2502.17453</link>
      <description>arXiv:2502.17453v1 Announce Type: cross 
Abstract: On account of a greater need for understanding the complexity of time series like physiological time series, financial time series, and many more that enter into picture for their inculpation with real-world problems, several complexity parameters have already been proposed in the literature. Permutation entropy, Lyapunov exponents are such complexity parameters out of many. In this article, we introduce a new time series complexity parameter, that is, the permutation extropy. The failure of permutation entropy in correctly specifying complexity of some chaotic time series motivates us to come up with a better complexity parameter, hence we propose this permutation extropy measure. We try to combine the ideas behind the permutation entropy and extopy to construct this measure. We also validate our proposed measure using several chaotic maps like logistic map, Henon map and Burger map. We apply the proposed complexity parameter to study the complexity of financial time series of the stock market and time series constructed using WHO data, finding a better complexity specification than permutation entropy. The proposed measure is kind of robust, fast calculation and invariant with respect to monotonous nonlinear transformation like permutation entropy, but it gives us a better result in specifying complexity in some cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17453v1</guid>
      <category>nlin.CD</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritik Roshan Giri, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Rapid Parameter Inference with Uncertainty Quantification for a Radiological Plume Source Identification Problem</title>
      <link>https://arxiv.org/abs/2502.17492</link>
      <description>arXiv:2502.17492v1 Announce Type: cross 
Abstract: In the event of a nuclear accident, or the detonation of a radiological dispersal device, quickly locating the source of the accident or blast is important for emergency response and environmental decontamination. At a specified time after a simulated instantaneous release of an aerosolized radioactive contaminant, measurements are recorded downwind from an array of radiation sensors. Neural networks are employed to infer the source release parameters in an accurate and rapid manner using sensor and mean wind speed data. We consider two neural network constructions that quantify the uncertainty of the predicted values; a categorical classification neural network and a Bayesian neural network. With the categorical classification neural network, we partition the spatial domain and treat each partition as a separate class for which we estimate the probability that it contains the true source location. In a Bayesian neural network, the weights and biases have a distribution rather than a single optimal value. With each evaluation, these distributions are sampled, yielding a different prediction with each evaluation. The trained Bayesian neural network is thus evaluated to construct posterior densities for the release parameters. Results are compared to Markov chain Monte Carlo (MCMC) results found using the Delayed Rejection Adaptive Metropolis Algorithm. The Bayesian neural network approach is generally much cheaper computationally than the MCMC approach as it relies on the computational cost of the neural network evaluation to generate posterior densities as opposed to the MCMC approach which depends on the computational expense of the transport and radiation detection models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17492v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Edwards, Ralph C Smith</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Forecasting in Climate Data Using EOFs and Machine Learning Models: A Case Study in Chile</title>
      <link>https://arxiv.org/abs/2502.17495</link>
      <description>arXiv:2502.17495v1 Announce Type: cross 
Abstract: Effective resource management and environmental planning in regions with high climatic variability, such as Chile, demand advanced predictive tools. This study addresses this challenge by employing an innovative and computationally efficient hybrid methodology that integrates machine learning (ML) methods for time series forecasting with established statistical techniques. The spatiotemporal data undergo decomposition using time-dependent Empirical Orthogonal Functions (EOFs), denoted as \(\phi_{k}(t)\), and their corresponding spatial coefficients, \(\alpha_{k}(s)\), to reduce dimensionality. Wavelet analysis provides high-resolution time and frequency information from the \(\phi_{k}(t)\) functions, while neural networks forecast these functions within a medium-range horizon \(h\). By utilizing various ML models, particularly a Wavelet - ANN hybrid model, we forecast \(\phi_{k}(t+h)\) up to a time horizon \(h\), and subsequently reconstruct the spatiotemporal data using these extended EOFs. This methodology is applied to a grid of climate data covering the territory of Chile. It transitions from a high-dimensional multivariate spatiotemporal data forecasting problem to a low-dimensional univariate forecasting problem. Additionally, cluster analysis with Dynamic Time Warping for defining similarities between rainfall time series, along with spatial coherence and predictability assessments, has been instrumental in identifying geographic areas where model performance is enhanced. This approach also elucidates the reasons behind poor forecast performance in regions or clusters with low spatial coherence and predictability. By utilizing cluster medoids, the forecasting process becomes more practical and efficient. This compound approach significantly reduces computational complexity while generating forecasts of reasonable accuracy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17495v1</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Herrera, Francisca Kleisinger, Andr\'es Wils\'on</dc:creator>
    </item>
    <item>
      <title>AI-driven 3D Spatial Transcriptomics</title>
      <link>https://arxiv.org/abs/2502.17761</link>
      <description>arXiv:2502.17761v1 Announce Type: cross 
Abstract: A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sectioning, are complex, are not compatible with non-destructive 3D tissue imaging technologies, and often lack scalability. Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX), an AI framework that leverages 3D tissue morphology and minimal 2D ST to predict volumetric 3D ST. By pretraining on diverse 3D morphology-transcriptomic pairs from heterogeneous tissue samples and then fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX learns both generic tissue-related and sample-specific morphological correlates of gene expression. This approach enables dense, high-throughput, and fast 3D ST, scaling seamlessly to large tissue volumes far beyond the reach of existing 3D ST techniques. By offering a cost-effective and minimally destructive route to obtaining volumetric molecular insights, we anticipate that VORTEX will accelerate biomarker discovery and our understanding of morphomolecular associations and cell states in complex tissues. Interactive 3D ST volumes can be viewed at https://vortex-demo.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17761v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cristina Almagro-P\'erez, Andrew H. Song, Luca Weishaupt, Ahrong Kim, Guillaume Jaume, Drew F. K. Williamson, Konstantin Hemker, Ming Y. Lu, Kritika Singh, Bowen Chen, Long Phi Le, Alexander S. Baras, Sizun Jiang, Ali Bashashati, Jonathan T. C. Liu, Faisal Mahmood</dc:creator>
    </item>
    <item>
      <title>Enhancing External Validity of Experiments with Ongoing Sampling</title>
      <link>https://arxiv.org/abs/2502.18253</link>
      <description>arXiv:2502.18253v1 Announce Type: cross 
Abstract: Participants in online experiments often enroll over time, which can compromise sample representativeness due to temporal shifts in covariates. This issue is particularly critical in A/B tests, online controlled experiments extensively used to evaluate product updates, since these tests are cost-sensitive and typically short in duration. We propose a novel framework that dynamically assesses sample representativeness by dividing the ongoing sampling process into three stages. We then develop stage-specific estimators for Population Average Treatment Effects (PATE), ensuring that experimental results remain generalizable across varying experiment durations. Leveraging survival analysis, we develop a heuristic function that identifies these stages without requiring prior knowledge of population or sample characteristics, thereby keeping implementation costs low. Our approach bridges the gap between experimental findings and real-world applicability, enabling product decisions to be based on evidence that accurately represents the broader target population. We validate the effectiveness of our framework on three levels: (1) through a real-world online experiment conducted on WeChat; (2) via a synthetic experiment; and (3) by applying it to 600 A/B tests on WeChat in a platform-wide application. Additionally, we provide practical guidelines for practitioners to implement our method in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18253v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chen Wang, Shichao Han, Shan Huang</dc:creator>
    </item>
    <item>
      <title>Smart and Efficient IoT-Based Irrigation System Design: Utilizing a Hybrid Agent-Based and System Dynamics Approach</title>
      <link>https://arxiv.org/abs/2502.18298</link>
      <description>arXiv:2502.18298v1 Announce Type: cross 
Abstract: Regarding problems like reduced precipitation and an increase in population, water resource scarcity has become one of the most critical problems in modern-day societies, as a consequence, there is a shortage of available water resources for irrigation in arid and semi-arid countries. On the other hand, it is possible to utilize modern technologies to control irrigation and reduce water loss. One of these technologies is the Internet of Things (IoT). Despite the possibility of using the IoT in irrigation control systems, there are complexities in designing such systems. Considering this issue, it is possible to use agent-oriented software engineering (AOSE) methodologies to design complex cyber-physical systems such as IoT-based systems. In this research, a smart irrigation system is designed based on Prometheus AOSE methodology, to reduce water loss by maintaining soil moisture in a suitable interval. The designed system comprises sensors, a central agent, and irrigation nodes. These agents follow defined rules to maintain soil moisture at a desired level cooperatively. For system simulation, a hybrid agent-based and system dynamics model was designed. In this hybrid model, soil moisture dynamics were modeled based on the system dynamics approach. The proposed model, was implemented in AnyLogic computer simulation software. Utilizing the simulation model, irrigation rules were examined. The system's functionality in automatic irrigation mode was tested based on a 256-run, fractional factorial design, and the effects of important factors such as soil properties on total irrigated water and total operation time were analyzed. Based on the tests, the system consistently irrigated nearly optimal water amounts in all tests. Moreover, the results were also used to minimize the system's energy consumption by reducing the system's operational time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18298v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ahmadi Pargo, Mohsen Akbarpour Shirazi, Dawud Fadai</dc:creator>
    </item>
    <item>
      <title>Attractiveness and equal treatment in a group draw</title>
      <link>https://arxiv.org/abs/2502.18332</link>
      <description>arXiv:2502.18332v1 Announce Type: cross 
Abstract: National teams from different continents can play against each other only in afew sports competitions. Therefore, a reasonable aim is maximising the number of intercontinental games in world cups, as done in basketball and football, in contrast to handball and volleyball. However, this objective requires additional draw constraints that imply the violation of equal treatment. In addition, the standard draw mechanism is non-uniformly distributed on the set of valid assignments, which may lead to further distortions. Our paper analyses this novel trade-off between attractiveness and fairness through the example of the 2025 World Men's Handball Championship. We introduce a measure of inequality, which enables considering 32 sets of reasonable geographical restrictions to determine the Pareto frontier. The proposed methodology can be used by policy-makers to select the optimal set of draw constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18332v1</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Synthesizing data products, mathematical models, and observational measurements for lake temperature forecasting</title>
      <link>https://arxiv.org/abs/2407.03312</link>
      <description>arXiv:2407.03312v3 Announce Type: replace 
Abstract: We present a novel forecasting framework for lake water temperature, which is crucial for managing lake ecosystems and drinking water resources. The General Lake Model (GLM) has been previously used for this purpose, but, similar to many process-based simulation models, it: requires a large number of inputs, many of which are stochastic; presents challenges for uncertainty quantification (UQ); and can exhibit model bias. To address these issues, we propose a Gaussian process (GP) surrogate-based forecasting approach that efficiently handles large, high-dimensional data and accounts for input-dependent variability and systematic GLM bias. We validate the proposed approach and compare it with other forecasting methods, including a climatological model and raw GLM simulations. Our results demonstrate that our bias-corrected GP surrogate (GPBC) can outperform competing approaches in terms of forecast accuracy and UQ up to two weeks into the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03312v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maike F. Holthuijzen, Robert B. Gramacy, Cayelan C. Carey, Dave M. Higdon, R. Quinn Thomas</dc:creator>
    </item>
    <item>
      <title>Granger causal inference for climate change attribution</title>
      <link>https://arxiv.org/abs/2408.16004</link>
      <description>arXiv:2408.16004v2 Announce Type: replace 
Abstract: Climate change detection and attribution (D&amp;A) is concerned with determining the extent to which anthropogenic activities have influenced specific aspects of the global climate system. D&amp;A fits within the broader field of causal inference, the collection of statistical methods that identify cause and effect relationships. There are a wide variety of methods for making attribution statements, each of which require different types of input data and each of which are conditional to varying extents. Some methods are based on Pearl causality (experimental interference) while others leverage Granger (predictive) causality, and the causal framing provides important context for how the resulting attribution conclusion should be interpreted. However, while Granger-causal attribution analyses have become more common, there is no clear statement of their strengths and weaknesses and no clear consensus on where and when Granger-causal perspectives are appropriate. In this prospective paper, we provide a formal definition for Granger-based approaches to trend and event attribution and a clear comparison with more traditional methods for assessing the human influence on extreme weather and climate events. Broadly speaking, Granger-causal attribution statements can be constructed quickly from observations and do not require computationally-intesive dynamical experiments. These analyses also enable rapid attribution, which is useful in the aftermath of a severe weather event, and provide multiple lines of evidence for anthropogenic climate change when paired with Pearl-causal attribution. Confidence in attribution statements is increased when different methodologies arrive at similar conclusions. Moving forward, we encourage the D&amp;A community to embrace hybrid approaches to climate change attribution that leverage the strengths of both Granger and Pearl causality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16004v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Mohammed Ombadi, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v3 Announce Type: replace-cross 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
  </channel>
</rss>
