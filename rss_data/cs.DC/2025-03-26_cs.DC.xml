<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization</title>
      <link>https://arxiv.org/abs/2503.19050</link>
      <description>arXiv:2503.19050v1 Announce Type: new 
Abstract: Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28$\times$ (up to 1.73$\times$) and 1.27$\times$ (up to 2.04$\times$) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19050v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689031.3717461</arxiv:DOI>
      <dc:creator>Zhanda Zhu, Christina Giannoula, Muralidhar Andoorveedu, Qidong Su, Karttikeya Mangalam, Bojian Zheng, Gennady Pekhimenko</dc:creator>
    </item>
    <item>
      <title>Reliability is Blind: Collective Incentives for Decentralized Computing Marketplaces without Individual Behavior Information</title>
      <link>https://arxiv.org/abs/2503.19055</link>
      <description>arXiv:2503.19055v1 Announce Type: new 
Abstract: In decentralized cloud computing marketplaces, ensuring fair and efficient interactions among asset providers and end-users is crucial. A key concern is meeting agreed-upon service-level objectives like the service's reliability. In this decentralized context, traditional mechanisms often fail to address the complexity of task failures, due to limited available and trustworthy insights into these independent actors' individual behavior. This paper proposes a collective incentive mechanism that blindly punishes all involved parties when a task fails. Based on ruin theory, we show that Collective Incentives improve behavior in the marketplace by creating a disincentive for faults and misbehavior even when the parties at fault are unknown, in turn leading to a more robust marketplace. Simulations for small and large pools of marketplace assets show that Collective Incentives enable to meet or exceed a reliability target, i.e., the success-rate of tasks run using marketplace assets, by eventually discarding failure-prone assets while preserving reliable ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19055v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Mont, Matthieu Bettinger, Sonia Ben Mokhtar, Anthony Simonet-Boulogne</dc:creator>
    </item>
    <item>
      <title>COoL-TEE: Client-TEE Collaboration for Resilient Distributed Search</title>
      <link>https://arxiv.org/abs/2503.19063</link>
      <description>arXiv:2503.19063v1 Announce Type: new 
Abstract: Current marketplaces rely on search mechanisms with distributed systems but centralized governance, making them vulnerable to attacks, failures, censorship and biases. While search mechanisms with more decentralized governance (e.g., DeSearch) have been recently proposed, these are still exposed to information head-start attacks (IHS) despite the use of Trusted Execution Environments (TEEs). These attacks allow malicious users to gain a head-start over other users for the discovery of new assets in the market, which give them an unfair advantage in asset acquisition. We propose COoL-TEE, a TEE-based provider selection mechanism for distributed search, running in single- or multi-datacenter environments, that is resilient to information head-start attacks. COoL-TEE relies on a Client-TEE collaboration, which enables clients to distinguish between slow providers and malicious ones. Performance evaluations in single- and multi-datacenter environments show that, using COoL-TEE, malicious users respectively gain only up to 2% and 7% of assets more than without IHS, while they can claim 20% or more on top of their fair share in the same conditions with DeSearch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19063v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthieu Bettinger, Etienne Rivi\`ere, Sonia Ben Mokhtar, Anthony Simonet-Boulogne</dc:creator>
    </item>
    <item>
      <title>LOCO: Rethinking Objects for Network Memory</title>
      <link>https://arxiv.org/abs/2503.19270</link>
      <description>arXiv:2503.19270v1 Announce Type: new 
Abstract: In this work, we explore an object-based programming model for filling the space between shared memory and distributed systems programming. We argue that the natural representation for resources distributed across a memory network (e.g. RDMA or CXL) is the traditional shared memory object. This concurrent object (which we call a "channel" object) exports traditional methods, but, especially in an incoherent or uncacheable memory network, stores its state in a distributed fashion across all participating nodes. In a sense, the channel object's state is stored "across the network".
  Based on this philosophy, we introduce the Library of Channel Objects (LOCO), a library for building multi-node objects on RDMA. Channel objects are composable and designed for both the strong locality effects and the weak consistency of RDMA. Unlike prior work, channel objects do not hide memory complexity, instead relying on the programmer to use NUMA-like techniques to explicitly manage each object. As a consequence, our channel objects have performance similar to custom RDMA systems (e.g. distributed maps), but with a far simpler programming model. Our distributed map channel has better read and comparable write performance to a state-of-the-art custom RDMA solution, using well-encapsulated and reusable primitives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19270v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George Hodgkins, Mark Madler, Joseph Izraelevitz</dc:creator>
    </item>
    <item>
      <title>Empirical Evaluation and Scalability Analysis of Proof of Team Sprint (PoTS): Reward Fairness, Energy Efficiency, and System Stability</title>
      <link>https://arxiv.org/abs/2503.19289</link>
      <description>arXiv:2503.19289v1 Announce Type: new 
Abstract: This paper presents an empirical evaluation of the Proof of Team Sprint (PoTS) consensus algorithm, focusing on reward fairness, energy efficiency, system stability, and scalability. We conducted large-scale simulations comparing PoTS with conventional Proof of Work (PoW) across various team sizes and computational conditions. In PoW, the highest-performance node ranked first in all 100 trials, demonstrating extreme centralization. In contrast, PoTS reduced this dominance: the same node ranked first only 54 times, indicating fairer reward distribution. Statistical analysis showed that as team size increased, skewness and kurtosis of reward distributions decreased, confirming improved equity among participants. PoTS also demonstrated significant energy savings. The total active computation time followed a near $1/N$ scaling trend, reducing energy use by up to 64 times when team size was 64, while preserving consensus integrity. Repeated simulations showed stable reward distributions and system performance, affirming PoTS's robustness. Furthermore, the correlation between performance and reward peaked at 0.90 for team size 16, reflecting an optimal balance between fairness and meritocracy. Overall, PoTS offers a cooperative, energy-efficient alternative to PoW, mitigating centralization risks and promoting equitable participation. These findings validate PoTS as a sustainable and fair consensus mechanism suited for future blockchain systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19289v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Yonezawa</dc:creator>
    </item>
    <item>
      <title>Robustness of Proof of Team Sprint (PoTS) Against Attacks: A Simulation-Based Analysis</title>
      <link>https://arxiv.org/abs/2503.19293</link>
      <description>arXiv:2503.19293v1 Announce Type: new 
Abstract: This study evaluates the robustness of Proof of Team Sprint (PoTS) against adversarial attacks through simulations, focusing on the attacker win rate and computational efficiency under varying team sizes (\( N \)) and attacker ratios (\( \alpha \)). Our results demonstrate that PoTS effectively reduces an attacker's ability to dominate the consensus process. For instance, when \( \alpha = 0.5 \), the attacker win rate decreases from 50.7\% at \( N = 1 \) to below 0.4\% at \( N = 8 \), effectively neutralizing adversarial influence. Similarly, at \( \alpha = 0.8 \), the attacker win rate drops from 80.47\% at \( N = 1 \) to only 2.79\% at \( N = 16 \). In addition to its strong security properties, PoTS maintains high computational efficiency. We introduce the concept of Normalized Computation Efficiency (NCE) to quantify this efficiency gain, showing that PoTS significantly improves resource utilization as team size increases. The results indicate that as \( N \) grows, PoTS not only enhances security but also achieves better computational efficiency due to the averaging effects of execution time variations. These findings highlight PoTS as a promising alternative to traditional consensus mechanisms, offering both robust security and efficient resource utilization. By leveraging team-based block generation and randomized participant reassignment, PoTS provides a scalable and resilient approach to decentralized consensus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19293v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Yonezawa</dc:creator>
    </item>
    <item>
      <title>Fairness in Proof of Team Sprint (PoTS): Evaluating Reward Distribution Across Performance Levels</title>
      <link>https://arxiv.org/abs/2503.19301</link>
      <description>arXiv:2503.19301v1 Announce Type: new 
Abstract: Blockchain consensus mechanisms must balance security, decentralization, and efficiency while ensuring fair participation. Proof of Team Sprint (PoTS) is a cooperative consensus mechanism designed to address the energy inefficiencies and centralization tendencies of traditional Proof of Work (PoW). Unlike PoW, where rewards disproportionately favor high-performance nodes, PoTS encourages collaboration by forming teams and distributing rewards more equitably among participants. In this study, we evaluate the fairness properties of PoTS by analyzing reward distribution under varying computational power distributions. Through extensive simulations, we compare equal-share allocation and proportional reward allocation, highlighting their impact on decentralization and participation. Our results demonstrate that PoTS significantly reduces reward disparity between high-performance and low-performance nodes, fostering a more inclusive ecosystem. Additionally, we observe that as team sizes increase, the influence of individual computational power is mitigated, allowing lower-performance nodes to contribute meaningfully. Moreover, our findings reveal that the marginal benefit of investing in extremely high-performance hardware diminishes, which discourages centralization and aligns incentives toward sustainable participation. We also discuss the economic implications of PoTS, particularly its potential to reshape blockchain mining strategies by balancing fairness with computational efficiency. These insights contribute to the broader discussion on blockchain fairness and provide a foundation for further research into cooperative consensus mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19301v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Yonezawa</dc:creator>
    </item>
    <item>
      <title>A Tight Meta-theorem for LOCAL Certification of MSO$_2$ Properties within Bounded Treewidth Graphs</title>
      <link>https://arxiv.org/abs/2503.19671</link>
      <description>arXiv:2503.19671v1 Announce Type: new 
Abstract: Distributed networks are prone to errors so verifying their output is critical. Hence, we develop LOCAL certification protocols for graph properties in which nodes are given certificates that allow them to check whether their network as a whole satisfies some fixed property while only communicating with their local network. Most known LOCAL certification protocols are specifically tailored to the problem they work on and cannot be translated more generally. Thus we target general protocols that can certify any property expressible within a certain logical framework. We consider Monadic Second Order Logic (MSO$_2$), a powerful framework that can express properties such as non-$k$-colorability, Hamiltonicity, and $H$-minor-freeness. Unfortunately, in general, there are MSO$_2$-expressible properties that cannot be certified without huge certificates. For instance, non-3-colorability requires certificates of size $\Omega(n^2/\log n)$ on general $n$-vertex graphs (G\"o\"os, Suomela 2016). Hence, we impose additional structural restrictions on the graph.
  We provide a LOCAL certification protocol for certifying any MSO$_2$-expressible property on graphs of bounded treewidth and, consequently, a LOCAL certification protocol for certifying bounded treewidth. That is for each integer $k$ and each MSO$_2$-expressible property $\Pi$ we give a LOCAL Certification protocol to certify that a graph satisfies $\Pi$ and has treewidth at most $k$ using certificates of size $\mathcal{O}(\log n)$ (which is asymptotically optimal). Our LOCAL certification protocol requires only one round of distributed communication, hence it is also proof-labeling scheme.
  Our result improves upon work by Fraigniaud, Montealegre, Rapaport, and Todinca (Algorithmica 2024), Bousquet, Feuilloley, Pierron (PODC 2022), and the very recent work of Baterisna and Chang.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19671v1</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda Cook, Eun Jung Kim, Tom\'a\v{s} Masa\v{r}\'ik</dc:creator>
    </item>
    <item>
      <title>AIGC-assisted Federated Learning for Vehicular Edge Intelligence: Vehicle Selection, Resource Allocation and Model Augmentation</title>
      <link>https://arxiv.org/abs/2503.19676</link>
      <description>arXiv:2503.19676v1 Announce Type: new 
Abstract: To leverage the vast amounts of onboard data while ensuring privacy and security, federated learning (FL) is emerging as a promising technology for supporting a wide range of vehicular applications. Although FL has great potential to improve the architecture of intelligent vehicular networks, challenges arise due to vehicle mobility, wireless channel instability, and data heterogeneity. To mitigate the issue of heterogeneous data across vehicles, artificial intelligence-generated content (AIGC) can be employed as an innovative data synthesis technique to enhance FL model performance. In this paper, we propose AIGC-assisted Federated Learning for Vehicular Edge Intelligence (GenFV). We further propose a weighted policy using the Earth Mover's Distance (EMD) to quantify data distribution heterogeneity and introduce a convergence analysis for GenFV. Subsequently, we analyze system delay and formulate a mixed-integer nonlinear programming (MINLP) problem to minimize system delay. To solve this MINLP NP-hard problem, we propose a two-scale algorithm. At large communication scale, we implement label sharing and vehicle selection based on velocity and data heterogeneity. At the small computation scale, we optimally allocate bandwidth, transmission power and amount of generated data. Extensive experiments show that GenFV significantly improves the performance and robustness of FL in dynamic, resource-constrained environments, outperforming other schemes and confirming the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19676v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianke Qiang, Zheng Chang, Geyong Min</dc:creator>
    </item>
    <item>
      <title>Comparing the Run-time Behavior of Modern PDES Engines on Alternative Hardware Architectures</title>
      <link>https://arxiv.org/abs/2503.19857</link>
      <description>arXiv:2503.19857v1 Announce Type: new 
Abstract: The current trend of technology has brought parallel machines equipped with multiple processors and multiple memory sockets to be available off-the-shelf -- or via renting through Iaas Clouds -- at reasonable costs. This has opened the possibility of natively supporting HPC in diffused realities, like industry or academic labs. At the same time, the Parallel Discrete Event Simulation (PDES) area has given rise to attractive simulation engines, designed with orientation to high performance and scalability, also targeting differentiated exploitation of the specific support offered by the underlying hardware. In this article, we present an experimental study where we deploy two last-generation open-source PDES platforms -- one optimistic (USE) and one conservative (PARSIR) -- on top of two significantly different hardware chipsets based on either {\sf x86} CISC or {\sf powerPC} RISC technology, both offering multiple Non-Uniform-Memory-Access (NUMA) nodes and multiple tens of cores and hardware-threads (logical CPUs). Also, we consider real-world simulation models configured in a variety of different manners in order to investigate the actual execution profile of the PDES engines on the two distinct hardware platforms. Our objective is the one of providing insights on current performance trends, which can support decisions in terms of both strategies -- for software platforms to adopt -- and investments -- in terms of hardware platforms -- in the area of discrete event simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19857v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romolo Marotta, Francesco Quaglia</dc:creator>
    </item>
    <item>
      <title>RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning</title>
      <link>https://arxiv.org/abs/2503.19886</link>
      <description>arXiv:2503.19886v1 Announce Type: cross 
Abstract: We address the problem of cluster identity estimation in a personalized federated learning (PFL) setting in which users aim to learn different personal models. The backbone of effective learning in such a setting is to cluster users into groups whose objectives are similar. A typical approach in the literature is to achieve this by training users' data on different proposed personal models and assign them to groups based on which model achieves the lowest value of the users' loss functions. This process is to be done iteratively until group identities converge. A key challenge in such a setting arises when users have noisy labeled data, which may produce misleading values of their loss functions, and hence lead to ineffective clustering. To overcome this challenge, we propose a label-agnostic data similarity-based clustering algorithm, coined RCC-PFL, with three main advantages: the cluster identity estimation procedure is independent from the training labels; it is a one-shot clustering algorithm performed prior to the training; and it requires fewer communication rounds and less computation compared to iterative-based clustering methods. We validate our proposed algorithm using various models and datasets and show that it outperforms multiple baselines in terms of average accuracy and variance reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19886v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdulmoneam Ali, Ahmed Arafa</dc:creator>
    </item>
    <item>
      <title>DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs</title>
      <link>https://arxiv.org/abs/2312.05215</link>
      <description>arXiv:2312.05215v3 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05215v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaozhe Yao, Qinghao Hu, Ana Klimovic</dc:creator>
    </item>
    <item>
      <title>Towards Datacenter Environmental Sustainability Using Carbon Depreciation Models</title>
      <link>https://arxiv.org/abs/2403.04976</link>
      <description>arXiv:2403.04976v3 Announce Type: replace 
Abstract: Recently, the growing need for increasingly capable computing resources to be available on-demand has led to the prosperity of data centers. These data centers have led to several challenges and opportunities to address the environmental impacts from this computing resource. Conventional thinking has been concerned with minimizing energy usage of data centers to address sustainability. However, due to energy efficiency trends and renewable energy integration, recent evidence has demonstrated that embodied carbon is increasingly important and calls for improvements in data center provisioning strategies. In this paper we propose to adopt carbon depreciation models to better encourage the longer lifetime of hardware in the data center. Carbon depreciation models apply a higher proportion of embodied carbon to newly provisioned servers. This promotes provisioning fewer new servers to service jobs only with strict quality-of-service (QoS) constraints and extending lifetime of existing servers whose embodied carbon has already been mostly recovered. Along with carbon depreciation, we make the case that both embodied and operational carbon from server idle time must also be recovered during active jobs. This promotes provisioning strategies that maintain high rates of utilization. We show that prior carbon accounting strategies are counterproductive for sustainability with a greedy job scheduler that attempts to minimize carbon under QoS constraints as they price jobs as 25% cheaper on new versus old hardware. Our approach uses a greedy scheduler that prefers older hardware due to non-linear carbon depreciation promoting sustainable provisioning. Our approach reduces carbon by between 28--57% depending on assumptions for server lifetimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04976v3</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixin Ji, Zhuoping Yang, Alex K. Jones, Peipei Zhou</dc:creator>
    </item>
    <item>
      <title>T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge</title>
      <link>https://arxiv.org/abs/2407.00088</link>
      <description>arXiv:2407.00088v2 Announce Type: replace 
Abstract: The deployment of Large Language Models (LLMs) on edge devices is increasingly important to enhance on-device intelligence. Weight quantization is crucial for reducing the memory footprint of LLMs on devices. However, low-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low precision weights and high precision activations during inference. Existing systems, lacking native support for mpGEMM, resort to dequantize weights for high precision computation. Such an indirect way can lead to a significant inference overhead.
  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based method designed for efficient low-bit LLM (i.e., weight-quantized LLM) inference on CPUs. T-MAC directly supports mpGEMM without dequantization, while simultaneously eliminating multiplications and reducing additions required. Specifically, T-MAC transforms the traditional data-type-centric multiplication to bit-wise table lookup, and enables a unified and scalable mpGEMM solution.
  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on low-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in throughput and 70% reduction in energy consumption compared to llama.cpp. For BitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s with a single core and 71 tokens/s with eight cores on M2-Ultra, and 11 tokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds the adult average reading speed. T-MAC with LUT-based computing paradigm, paves the way for the practical deployment of low-bit LLMs on resource-constrained edge devices without compromising computational efficiency. The system is open-sourced at https://github.com/microsoft/T-MAC .</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00088v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3689031.3696099</arxiv:DOI>
      <dc:creator>Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, Mao Yang</dc:creator>
    </item>
    <item>
      <title>SeBS-Flow: Benchmarking Serverless Cloud Function Workflows</title>
      <link>https://arxiv.org/abs/2410.03480</link>
      <description>arXiv:2410.03480v3 Announce Type: replace 
Abstract: Serverless computing has emerged as a prominent paradigm, with a significant adoption rate among cloud customers. While this model offers advantages such as abstraction from the deployment and resource scheduling, it also poses limitations in handling complex use cases due to the restricted nature of individual functions. Serverless workflows address this limitation by orchestrating multiple functions into a cohesive application. However, existing serverless workflow platforms exhibit significant differences in their programming models and infrastructure, making fair and consistent performance evaluations difficult in practice. To address this gap, we propose the first serverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic workflow model that enables consistent benchmarking across various platforms. SeBS-Flow includes six real-world application benchmarks and four microbenchmarks representing different computational patterns. We conduct comprehensive evaluations on three major cloud platforms, assessing performance, cost, scalability, and runtime deviations. We make our benchmark suite open-source, enabling rigorous and comparable evaluations of serverless workflows over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03480v3</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689031.3717465</arxiv:DOI>
      <dc:creator>Larissa Schmid, Marcin Copik, Alexandru Calotoiu, Laurin Brandner, Anne Koziolek, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in LLM Serving</title>
      <link>https://arxiv.org/abs/2411.06364</link>
      <description>arXiv:2411.06364v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) continue to grow, reducing costs and alleviating GPU demands has become increasingly critical. However, existing schedulers primarily target either GPU compute or Key-Value Cache (KVC) utilization, failing to fully optimize both GPU compute and KVC usage during each iteration or guarantee timely KVC allocations when needed. To address these challenges, we conducted a trace-based experimental analysis and made insightful observations, leading to the design of a system called EconoServe. EconoServe maximizes multi-resource utilization while ensuring service-level objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch to maximize GPU utilization in each iteration, EconoServe maintains separate waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It batches GTs with the same predicted response lengths (RL) to save scheduling time and allocates KVC space for the predicted RL to avoid KVC allocation failures. It further has a novel KVC pipelining method, allowing sharing allocated but unused KVC space to enhance KVC utilization. In addition, it prioritizes queued requests that occupy more KVC to release KVC earlier and satisfy request service-level-objective (SLO). Experimental results demonstrate that EconoServe increases throughput by up to 4$\times$ with the same level of latency, generates up to 91\% lower job completion time and up to 91\% higher SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs used in DistServe by up to 78\% while maintaining the same level of goodput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06364v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haiying Shen, Tanmoy Sen</dc:creator>
    </item>
    <item>
      <title>Boosting Blockchain Throughput: Parallel EVM Execution with Asynchronous Storage for Reddio</title>
      <link>https://arxiv.org/abs/2503.04595</link>
      <description>arXiv:2503.04595v2 Announce Type: replace 
Abstract: The increasing adoption of blockchain technology has led to a growing demand for higher transaction throughput. Traditional blockchain platforms, such as Ethereum, execute transactions sequentially within each block, limiting scalability. Parallel execution has been proposed to enhance performance, but existing approaches either impose strict dependency annotations, rely on conservative static analysis, or suffer from high contention due to inefficient state management. Moreover, even when transaction execution is parallelized at the upper layer, storage operations remain a bottleneck due to sequential state access and I/O amplification. In this paper, we propose Reddio, a batch-based parallel transaction execution framework with asynchronous storage. Reddio processes transactions in parallel while addressing the storage bottleneck through three key techniques: (i) direct state reading, which enables efficient state access without traversing the Merkle Patricia Trie (MPT); (ii) asynchronous parallel node loading, which preloads trie nodes concurrently with execution to reduce I/O overhead; and (iii) pipelined workflow, which decouples execution, state reading, and storage updates into overlapping phases to maximize hardware utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04595v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaodong Qi, Xinran Chen,  Asiy, Neil Han</dc:creator>
    </item>
    <item>
      <title>Monte Cimone v2: Down the Road of RISC-V High-Performance Computers</title>
      <link>https://arxiv.org/abs/2503.18543</link>
      <description>arXiv:2503.18543v2 Announce Type: replace 
Abstract: Many RISC-V platforms and SoCs have been announced in recent years targeting the HPC sector, but only a few of them are commercially available and engineered to fit the HPC requirements. The Monte Cimone project targeted assessing their capabilities and maturity, aiming to make RISC-V a competitive choice when building a datacenter. Nowadays, RV SoCs with vector extension, form factor and memory capacity suitable for HPC applications are available in the market, but it is unclear how compilers and open-source libraries can take advantage of its performance. In this paper, we describe the performance assessment of the upgrade of the Monte Cimone (MCv2) cluster with the Sophgo SG2042 processor's HPC operations. The upgrade increases the attained node's performance by 127x on HPL DP FLOP/s and 69x on Stream Memory Bandwidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18543v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Venieri, Simone Manoni, Giacomo Madella, Federico Ficarelli, Daniele Gregori, Daniele Cesarini, Luca Benini, Andrea Bartolini</dc:creator>
    </item>
    <item>
      <title>Efficient Distributed Algorithms for Shape Reduction via Reconfigurable Circuits</title>
      <link>https://arxiv.org/abs/2503.18663</link>
      <description>arXiv:2503.18663v2 Announce Type: replace 
Abstract: In this paper, we study the problem of efficiently reducing geometric shapes into other such shapes in a distributed setting through size-changing operations. We develop distributed algorithms using the reconfigurable circuit model to enable fast node-to-node communication. Our study considers two graph update models: the connectivity model and the adjacency model. Let $n$ denote the number of nodes and $k$ the number of turning points in the initial shape. In the connectivity model, we show that the system of nodes can reduce itself from any tree to a single node using only shrinking operations in $O(k \log n)$ rounds w.h.p. and any tree to its minimal (incompressible) form in $O(\log n)$ rounds with additional knowledge or $O(k \log n)$ without, w.h.p. We also give an algorithm to transform any tree to any topologically equivalent tree in $O(k \log n+\log^2 n)$ rounds w.h.p. if both shrinking and growth operations are available to the nodes. On the negative side, we show that one cannot hope for $o(\log^2 n)$-round transformations for all shapes of $O(\log n)$ turning points: for all reasonable values of $k$, there exists a pair of geometrically equivalent paths of $k$ turning points each, such that $\Omega(k\log n)$ rounds are required to reduce one to the other. In the adjacency model, we show that the system can reduce itself from any connected shape to a single node using only shrinking in $O(\log n)$ rounds w.h.p.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18663v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nada Almalki, Siddharth Gupta, Othon Michail, Andreas Padalkin</dc:creator>
    </item>
  </channel>
</rss>
