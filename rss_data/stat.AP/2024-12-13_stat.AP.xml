<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inferring latent structure in ecological communities via barcodes</title>
      <link>https://arxiv.org/abs/2412.08793</link>
      <description>arXiv:2412.08793v1 Announce Type: new 
Abstract: Accelerating global biodiversity loss has highlighted the role of complex relationships and shared patterns among species in mediating responses to environmental changes. The structure of ecological communities signals their fragility or robustness more so than individual niches of species. We focus on obtaining community-level insights that characterize underlying patterns in abundances of bird species in Finland. We propose a novel \texttt{barcode} framework for inferring latent binary features underlying samples and species. \texttt{barcode} provides a more nuanced alternative to clustering, while improving current multivariate abundance models. \texttt{barcode} addresses key limitations of popular methods for model-based ordination and expands the class of concurrent ordinations. A key feature is our use of binary latent variables, which admit simple interpretations such as habitat and sampling factors that explain observed variation. In studying 137 bird species using this framework, we find that three of the five leading factors indicate different types of forest habitat, signaling the importance of diverse forest in this community. In contrast, a single factor simultaneously proxies both human intervention and coastal habitats. Supervised species clusters and species-specific geospatial distributions are also inferred.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08793v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Braden Scherting, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>The Causal Effect of the Two-For-One Strategy in the National Basketball Association</title>
      <link>https://arxiv.org/abs/2412.08840</link>
      <description>arXiv:2412.08840v1 Announce Type: new 
Abstract: This study evaluates the effectiveness of the two-for-one strategy in basketball by applying a causal inference framework to play-by-play data from the 2018-19 and 2021-22 National Basketball Association regular seasons. Incorporating factors such as player lineup, betting odds, and player ratings, we compute the average treatment effect and find that the two-for-one strategy has a positive impact on game outcomes, suggesting it can benefit teams when employed effectively. Additionally, we investigate potential heterogeneity in the strategy's effectiveness using the causal forest framework, with tests indicating no significant variation across different contexts. These findings offer valuable insights into the tactical advantages of the two-for-one strategy in professional basketball.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08840v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prateek Sasan, Daryl Swartzentruber</dc:creator>
    </item>
    <item>
      <title>Beyond Reweighting: On the Predictive Role of Covariate Shift in Effect Generalization</title>
      <link>https://arxiv.org/abs/2412.08869</link>
      <description>arXiv:2412.08869v1 Announce Type: new 
Abstract: Many existing approaches to generalizing statistical inference amidst distribution shift operate under the covariate shift assumption, which posits that the conditional distribution of unobserved variables given observable ones is invariant across populations. However, recent empirical investigations have demonstrated that adjusting for shift in observed variables (covariate shift) is often insufficient for generalization. In other words, covariate shift does not typically ``explain away'' the distribution shift between settings. As such, addressing the unknown yet non-negligible shift in the unobserved variables given observed ones (conditional shift) is crucial for generalizable inference.
  In this paper, we present a series of empirical evidence from two large-scale multi-site replication studies to support a new role of covariate shift in ``predicting'' the strength of the unknown conditional shift. Analyzing 680 studies across 65 sites, we find that even though the conditional shift is non-negligible, its strength can often be bounded by that of the observable covariate shift. However, this pattern only emerges when the two sources of shifts are quantified by our proposed standardized, ``pivotal'' measures. We then interpret this phenomenon by connecting it to similar patterns that can be theoretically derived from a random distribution shift model. Finally, we demonstrate that exploiting the predictive role of covariate shift leads to reliable and efficient uncertainty quantification for target estimates in generalization tasks with partially observed data. Overall, our empirical and theoretical analyses suggest a new way to approach the problem of distributional shift, generalizability, and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08869v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Naoki Egami, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand</title>
      <link>https://arxiv.org/abs/2412.08927</link>
      <description>arXiv:2412.08927v1 Announce Type: new 
Abstract: Background. The excess mortality rate in Aotearoa New Zealand during the Covid-19 pandemic is frequently estimated to be among the lowest in the world. However, to facilitate international comparisons, many of the methods that have been used to estimate excess mortality do not use age-stratified data on deaths and population size, which may compromise their accuracy.
  Methods. We used a quasi-Poisson regression model for monthly all-cause deaths among New Zealand residents, controlling for age, sex and seasonality. We fitted the model to deaths data for 2014-19. We estimated monthly excess mortality for 2020-23 as the difference between actual deaths and projected deaths according to the model. We conducted sensitivity analysis on the length of the pre-pandemic period used to fit the model. We benchmarked our results against a simple linear regression on the standardised annual mortality rate.
  Results. We estimated cumulative excess mortality in New Zealand in 2020-23 was 1040 (95% confidence interval [-1134, 2927]), equivalent to 0.7% [-0.8%, 2.0%] of expected mortality. Excess mortality was negative in 2020-21. The magnitude, timing and age-distribution of the positive excess mortality in 2022-23 were closely matched with confirmed Covid-19 deaths.
  Conclusions. Negative excess mortality in 2020-21 reflects very low levels of Covid-19 and major reductions in seasonal respiratory diseases during this period. In 2022-23, Covid-19 deaths were the main contributor to excess mortality and there was little or no net non-Covid-19 excess. Overall, New Zealand experienced one of the lowest rates of pandemic excess mortality in the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08927v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael John Plank, Pubudu Senanayake, Richard Lyon</dc:creator>
    </item>
    <item>
      <title>The Global Carbon Budget as a cointegrated system</title>
      <link>https://arxiv.org/abs/2412.09226</link>
      <description>arXiv:2412.09226v1 Announce Type: new 
Abstract: The Global Carbon Budget, maintained by the Global Carbon Project, summarizes Earth's global carbon cycle through four annual time series beginning in 1959: atmospheric CO$_2$ concentrations, anthropogenic CO$_2$ emissions, and CO$_2$ uptake by land and ocean. We analyze these four time series as a multivariate (cointegrated) system. Statistical tests show that the four time series are cointegrated with rank three and identify anthropogenic CO$_2$ emissions as the single stochastic trend driving the nonstationary dynamics of the system. The three cointegrated relations correspond to the physical relations that the sinks are linearly related to atmospheric concentrations and that the change in concentrations equals emissions minus the combined uptake by land and ocean. Furthermore, likelihood ratio tests show that a parametrically restricted error-correction model that embodies these physical relations and accounts for the El-Ni\~no/Southern Oscillation cannot be rejected on the data. Finally, projections based on this model, using Shared Socioeconomic Pathways scenarios, yield results consistent with established climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09226v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Morten {\O}rregaard Nielsen</dc:creator>
    </item>
    <item>
      <title>Probabilistic digital twins for geotechnical design and construction</title>
      <link>https://arxiv.org/abs/2412.09432</link>
      <description>arXiv:2412.09432v1 Announce Type: new 
Abstract: The digital twin approach has gained recognition as a promising solution to the challenges faced by the Architecture, Engineering, Construction, Operations, and Management (AECOM) industries. However, its broader application across AECOM sectors remains limited. One significant obstacle is that traditional digital twins rely on deterministic models, which require deterministic input parameters. This limits their accuracy, as they do not account for the substantial uncertainties inherent in AECOM projects. These uncertainties are particularly pronounced in geotechnical design and construction. To address this challenge, we propose a Probabilistic Digital Twin (PDT) framework that extends traditional digital twin methodologies by incorporating uncertainties, and is tailored to the requirements of geotechnical design and construction. The PDT framework provides a structured approach to integrating all sources of uncertainty, including aleatoric, data, model, and prediction uncertainties, and propagates them throughout the entire modeling process. To ensure that site-specific conditions are accurately reflected as additional information is obtained, the PDT leverages Bayesian methods for model updating. The effectiveness of the probabilistic digital twin framework is showcased through an application to a highway foundation construction project, demonstrating its potential to improve decision-making and project outcomes in the face of significant uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09432v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dafydd Cotoarb\u{a}, Daniel Straub, Ian FC Smith</dc:creator>
    </item>
    <item>
      <title>Assessing the Role of Volumetric Brain Information in Multiple Sclerosis Progression</title>
      <link>https://arxiv.org/abs/2412.09497</link>
      <description>arXiv:2412.09497v1 Announce Type: new 
Abstract: Multiple sclerosis is a chronic autoimmune disease that affects the central nervous system. Understanding multiple sclerosis progression and identifying the implicated brain structures is crucial for personalized treatment decisions. Deformation-based morphometry utilizes anatomical magnetic resonance imaging to quantitatively assess volumetric brain changes at the voxel level, providing insight into how each brain region contributes to clinical progression with regards to neurodegeneration. Utilizing such voxel-level data from a relapsing multiple sclerosis clinical trial, we extend a model-agnostic feature importance metric to identify a robust and predictive feature set that corresponds to clinical progression. These features correspond to brain regions that are clinically meaningful in MS disease research, demonstrating their scientific relevance. When used to predict progression using classical survival models and 3D convolutional neural networks, the identified regions led to the best-performing models, demonstrating their prognostic strength. We also find that these features generalize well to other definitions of clinical progression and can compensate for the omission of highly prognostic clinical features, underscoring the predictive power and clinical relevance of deformation-based morphometry as a regional identification tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09497v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy A. Shen, Aidan McLoughlin, Zoe Vernon, Jonathan Lin, Richard A. D. Carano, Peter J. Bickel, Zhuang Song, Haiyan Huang</dc:creator>
    </item>
    <item>
      <title>Gradient-Boosted Mixture Regression Models for Postprocessing Ensemble Weather Forecasts</title>
      <link>https://arxiv.org/abs/2412.09583</link>
      <description>arXiv:2412.09583v1 Announce Type: new 
Abstract: Nowadays, weather forecasts are commonly generated by ensemble forecasts based on multiple runs of numerical weather prediction models. However, such forecasts are usually miscalibrated and/or biased, thus require statistical postprocessing. Non-homogeneous regression models, such as the ensemble model output statistics are frequently applied to correct these forecasts. Nonetheless, these methods often rely on the assumption of an unimodal parametric distribution, leading to improved, but sometimes not fully calibrated forecasts. To address this issue, a mixture regression model is presented, where the ensemble forecasts of each exchangeable group are linked to only one mixture component and mixture weight, called mixture of model output statistics (MIXMOS). In order to remove location specific effects and to use a longer training data, the standardized anomalies of the response and the ensemble forecasts are employed for the mixture of standardized anomaly model output statistics (MIXSAMOS). As carefully selected covariates, e.g. from different weather variables, can enhance model performance, the non-cyclic gradient-boosting algorithm for mixture regression models is introduced. Furthermore, MIXSAMOS is extended by this gradient-boosting algorithm (MIXSAMOS-GB) providing an automatic variable selection. The novel mixture regression models substantially outperform state-of-the-art postprocessing models in a case study for 2m surface temperature forecasts in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09583v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Jobst</dc:creator>
    </item>
    <item>
      <title>Log-Ergodic Dynamics in Stochastic Monetary Velocity: Theoretical Insights and Economic Implications</title>
      <link>https://arxiv.org/abs/2412.08657</link>
      <description>arXiv:2412.08657v1 Announce Type: cross 
Abstract: We suggest employing log-ergodic processes to simulate the velocity of money in an ergodic manner. Our approach sheds light on economic behavior, policy implications, and financial dynamics by maintaining long-term stability. By bridging theory and practice, the partially ergodic model helps analysts and policymakers comprehend and forecast velocity of money. The empirical analysis, using historical U.S. GDP and money supply data, demonstrates the model's effectiveness in capturing the long-term stability of the velocity of money. Key findings indicate that the log-ergodic model offers superior predictive power compared to traditional models, making it a valuable tool for policymakers to control economic factors in vital situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08657v1</guid>
      <category>q-fin.GN</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiarash Firouzi, Mohammad Jelodari Mamaghani</dc:creator>
    </item>
    <item>
      <title>GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction</title>
      <link>https://arxiv.org/abs/2412.08661</link>
      <description>arXiv:2412.08661v1 Announce Type: cross 
Abstract: Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 68.33% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08661v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiayin Lou, Peng Luo, Liqiu Meng</dc:creator>
    </item>
    <item>
      <title>Modeling EEG Spectral Features through Warped Functional Mixed Membership Models</title>
      <link>https://arxiv.org/abs/2412.08762</link>
      <description>arXiv:2412.08762v1 Announce Type: cross 
Abstract: A common concern in the field of functional data analysis is the challenge of temporal misalignment, which is typically addressed using curve registration methods. Currently, most of these methods assume the data is governed by a single common shape or a finite mixture of population level shapes. We introduce more flexibility using mixed membership models. Individual observations are assumed to partially belong to different clusters, allowing variation across multiple functional features. We propose a Bayesian hierarchical model to estimate the underlying shapes, as well as the individual time-transformation functions and levels of membership. Motivating this work is data from EEG signals in children with autism spectrum disorder (ASD). Our method agrees with the neuroimaging literature, recovering the 1/f pink noise feature distinctly from the peak in the alpha band. Furthermore, the introduction of a regression component in the estimation of time-transformation functions quantifies the effect of age and clinical designation on the location of the peak alpha frequency (PAF).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08762v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Landry, Damla Senturk, Shafali Jeste, Charlotte DiStefano, Abigail Dickinson, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for Spectrophotometric Calibration</title>
      <link>https://arxiv.org/abs/2412.08809</link>
      <description>arXiv:2412.08809v1 Announce Type: cross 
Abstract: We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky faint DA white dwarf (DA WD) spectrophotometric standards ($16.5 &lt; V &lt; 19.5$) alongside the three CALSPEC standards, from 912 \r{A} to 32 $\mu$m. The framework is the first of its kind to jointly infer photometric zeropoints and WD parameters ($\log g$, $T_{\text{eff}}$, $A_V$, $R_V$) by simultaneously modelling both photometric and spectroscopic data. We model panchromatic HST/WFC3 UVIS and IR fluxes, HST/STIS UV spectroscopy and ground-based optical spectroscopy to sub-percent precision. Photometric residuals for the sample are the lowest yet yielding $&lt;0.004$ mag RMS on average from the UV to the NIR, achieved by jointly inferring time-dependent changes in system sensitivity and WFC3/IR count-rate nonlinearity. Our GPU-accelerated implementation enables efficient sampling via Hamiltonian Monte Carlo, critical for exploring the high-dimensional posterior space. The hierarchical nature of the model enables population analysis of intrinsic WD and dust parameters. Inferred SEDs from this model will be essential for calibrating the James Webb Space Telescope as well as next-generation surveys, including Vera Rubin Observatory's Legacy Survey of Space and Time, and the Nancy Grace Roman Space Telescope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08809v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin M. Boyd, Gautham Narayan, Kaisey S. Mandel, Matthew Grayling, Aidan Berres, Mai Li, Aaron Do, Abhijit Saha, Tim Axelrod, Thomas Matheson, Edward W. Olszewski, Ralph C. Bohlin, Annalisa Calamida, Jay B. Holberg, Ivan Hubeny, John W. Mackenty, Armin Rest, Elena Sabbi, Christopher W. Stubbs</dc:creator>
    </item>
    <item>
      <title>Fully Bayesian Wideband Direction-of-Arrival Estimation and Detection via RJMCMC</title>
      <link>https://arxiv.org/abs/2412.08895</link>
      <description>arXiv:2412.08895v1 Announce Type: cross 
Abstract: We propose a fully Bayesian approach to wideband, or broadband, direction-of-arrival (DoA) estimation and signal detection. Unlike previous works in wideband DoA estimation and detection, where the signals were modeled in the time-frequency domain, we directly model the time-domain representation and treat the non-causal part of the source signal as latent variables. Furthermore, our Bayesian model allows for closed-form marginalization of the latent source signals by leveraging conjugacy. To further speed up computation, we exploit the sparse ``stripe matrix structure'' of the considered system, which stems from the circulant matrix representation of linear time-invariant (LTI) systems. This drastically reduces the time complexity of computing the likelihood from $\mathcal{O}(N^3 k^3)$ to $\mathcal{O}(N k^3)$, where $N$ is the number of samples received by the array and $k$ is the number of sources. These computational improvements allow for efficient posterior inference through reversible jump Markov chain Monte Carlo (RJMCMC). We use the non-reversible extension of RJMCMC (NRJMCMC), which often achieves lower autocorrelation and faster convergence than the conventional reversible variant. Detection, estimation, and reconstruction of the latent source signals can then all be performed in a fully Bayesian manner through the samples drawn using NRJMCMC. We evaluate the detection performance of the procedure by comparing against generalized likelihood ratio testing (GLRT) and information criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08895v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Philip T. Clemson, James P. Reilly, Jason F. Ralph, Simon Maskell</dc:creator>
    </item>
    <item>
      <title>Multivariate Aspects of Phylogenetic Comparative Methods</title>
      <link>https://arxiv.org/abs/2412.09325</link>
      <description>arXiv:2412.09325v1 Announce Type: cross 
Abstract: This thesis concerns multivariate phylogenetic comparative methods. We investigate two aspects of them. The first is the bias caused by measurement error in regression studies of comparative data. We calculate the formula for the bias and show how to correct for it. We also study whether it is always advantageous to correct for the bias as correction can increase the mean square error of the estimate. We propose a criterion, which depends on the observed data, that indicates whether it is beneficial to correct or not. Accompanying the results is an R program that offers the bias correction tool.
  The second topic is a multivariate model for trait evolution which is based on an Ornstein-Uhlenbeck type stochastic process, often used for studying trait adaptation, co-evolution, allometry or trade-offs. Alongside the description of the model and presentation of its most important features we present an R program estimating the model's parameters. To the best of our knowledge our program is the first program that allows for nearly all combinations of key model parameters providing the biologist with a flexible tool for studying multiple interacting traits in the Ornstein-Uhlenbeck framework. There are numerous packages available that include the Ornstein-Uhlenbeck process but their multivariate capabilities seem limited. [COMMENT: Please note that this abstract and thesis is from 2011]</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09325v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krzysztof Bartoszek</dc:creator>
    </item>
    <item>
      <title>Stochastic models in phylogenetic comparative methods: analytical properties and parameter estimation</title>
      <link>https://arxiv.org/abs/2412.09327</link>
      <description>arXiv:2412.09327v1 Announce Type: cross 
Abstract: Phylogenetic comparative methods are well established tools for using inter-species variation to analyse phenotypic evolution and adaptation. They are generally hampered, however, by predominantly univariate approaches and failure to include uncertainty and measurement error in the phylogeny as well as the measured traits. This thesis addresses all these three issues.
  First, by investigating the effects of correlated measurement errors on a phylogenetic regression. Second, by developing a multivariate Ornstein-Uhlenbeck model combined with a maximum-likelihood estimation package in R. This model allows, uniquely, a direct way of testing adaptive coevolution.
  Third, accounting for the often substantial phylogenetic uncertainty in comparative studies requires an explicit model for the tree. Based on recently developed conditioned branching processes, with Brownian and Ornstein-Uhlenbeck evolution on top, expected species similarities are derived, together with phylogenetic confidence intervals for the optimal trait value. Finally, inspired by these developments, the phylogenetic framework is illustrated by an exploration of questions concerning "time since hybridization", the distribution of which proves to be asymptotically exponential. [COMMENT: Please note that this abstract and thesis is from 2013]</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09327v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krzysztof Bartoszek</dc:creator>
    </item>
    <item>
      <title>Auto-Regressive Moving Diffusion Models for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2412.09328</link>
      <description>arXiv:2412.09328v1 Announce Type: cross 
Abstract: Time series forecasting (TSF) is essential in various domains, and recent advancements in diffusion-based TSF models have shown considerable promise. However, these models typically adopt traditional diffusion patterns, treating TSF as a noise-based conditional generation task. This approach neglects the inherent continuous sequential nature of time series, leading to a fundamental misalignment between diffusion mechanisms and the TSF objective, thereby severely impairing performance. To bridge this misalignment, and inspired by the classic Auto-Regressive Moving Average (ARMA) theory, which views time series as continuous sequential progressions evolving from previous data points, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to first achieve the continuous sequential diffusion-based TSF. Unlike previous methods that start from white Gaussian noise, our model employs chain-based diffusion with priors, accurately modeling the evolution of time series and leveraging intermediate state information to improve forecasting accuracy and stability. Specifically, our approach reinterprets the diffusion process by considering future series as the initial state and historical series as the final state, with intermediate series generated using a sliding-based technique during the forward process. This design aligns the diffusion model's sampling procedure with the forecasting objective, resulting in an unconditional, continuous sequential diffusion TSF model. Extensive experiments conducted on seven widely used datasets demonstrate that our model achieves state-of-the-art performance, significantly outperforming existing diffusion-based TSF models. Our code is available on GitHub: https://github.com/daxin007/ARMD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09328v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Gao, Qinglong Cao, Yuntian Chen</dc:creator>
    </item>
    <item>
      <title>Predicting Coastal Water Levels in the Context of Climate Change Using Kolmogorov-Zurbenko Time Series Analysis Methods</title>
      <link>https://arxiv.org/abs/2412.09419</link>
      <description>arXiv:2412.09419v1 Announce Type: cross 
Abstract: Given recent increases in ocean water levels brought on by climate change, this investigation decomposed changes in coastal water levels into its fundamental components to predict maximum water levels for a given coastal location. The study focused on Virginia Key, Florida, in the United States, located near the coast of Miami. Hourly mean lower low water (MLLW) levels were obtained from the National Data Buoy Center from January 28, 1994, through December 31, 2023. In the temporal dimension, Kolmogorov-Zurbenko filters were used to extract long-term trends, annual and daily tides, and higher frequency harmonics, while in the spectral dimension, Kolmogorov-Zurbenko periodograms with DiRienzo-Zurbenko algorithm smoothing were used to confirm known tidal frequencies and periods. A linear model predicted that the long-term trend in water level will rise 2.02 feet from January 1994 to December 2050, while a quadratic model predicted a rise of 5.91 during the same period. In addition, the combined crests of annual tides, daily tides, and higher frequency harmonics increase water levels up to 2.16 feet, yielding a combined total of 4.18 feet as a lower bound and a combined total of 8.09 feet as an upper bound. These findings provide a foundation for more accurate prediction of coastal flooding during severe weather events and provide an impetus for policy choices with respect to residential communities, businesses, and wildlife habitats. Further, using Kolmogorov-Zurbenko analytic methods to study coastal sites throughout the world could draw a more comprehensive picture of the impact climate change is having on coastal waters globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09419v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>A Kernel Score Perspective on Forecast Disagreement and the Linear Pool</title>
      <link>https://arxiv.org/abs/2412.09430</link>
      <description>arXiv:2412.09430v1 Announce Type: cross 
Abstract: The variance of a linearly combined forecast distribution (or linear pool) consists of two components: The average variance of the component distributions (`average uncertainty'), and the average squared difference between the components' means and the pool's mean (`disagreement'). This paper shows that similar decompositions hold for a class of uncertainty measures that can be constructed as entropy functions of kernel scores. The latter are a rich family of scoring rules that covers point and distribution forecasts for univariate and multivariate, discrete and continuous settings. The results in this paper are useful for two reasons. First, they provide a generic description of the uncertainty implicit in the linear pool. Second, they suggest principled measures of forecast disagreement in a wide range of applied settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09430v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Inference for Bradley--Terry Models with Ties: An Application to Honour Based Abuse</title>
      <link>https://arxiv.org/abs/2405.13399</link>
      <description>arXiv:2405.13399v2 Announce Type: replace 
Abstract: Honour based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at local level, where participants where shown pairs of wards and asked which had a higher rate of honour based abuse. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for tied comparisons reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit a model with ties, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police and Oxford Against Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13399v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/02664763.2024.2436608</arxiv:DOI>
      <dc:creator>Rowland G Seymour, Fabian Hernandez</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of uRLLC in scalable Cell-free Radio Access Network System</title>
      <link>https://arxiv.org/abs/2411.09128</link>
      <description>arXiv:2411.09128v2 Announce Type: replace-cross 
Abstract: As a critical component of beyond fifth-generation (B5G) and sixth-generation (6G) mobile communication systems, ultra-reliable low-latency communication (uRLLC) imposes stringent requirements on latency and reliability. In recent years, with the improvement of mobile communication network, centralized and distributed processing schemes for cellfree massive multiple-input multiple-output (CF-mMIMO) have attracted significant research attention. This paper investigates the performance of a novel scalable cell-free radio access network (CF-RAN) architecture featuring multiple edge distributed units (EDUs) under the finite block length regime. Closed expressions for the upper and lower bounds of its expected spectral efficiency (SE) performance are derived, where centralized and fully distributed deployment can be treated as two special cases, respectively. Furthermore, the spatial distribution of user equipments (UEs) and remote radio units (RRUs) is examined and the analysis reveals that the interleaving RRUs deployment associated with the EDU can enhance SE performance under finite block length constraints with specific transmission error probability. The paper also compares Monte Carlo simulation results with multi-RRU clustering-based collaborative processing, validating the accuracy of the space-time exchange theory in the scalable CF-RAN scenario. By deploying scalable EDUs, a practical trade-off between latency and reliability can be achieved through spatial degree-of-freedom (DoF), offering a distributed and scalable realization of the space-time exchange theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09128v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Zhang, Dongming Wang, Yunxiang Guo, Yang Cao, Xiaohu You</dc:creator>
    </item>
  </channel>
</rss>
