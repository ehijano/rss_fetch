<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When can in-context learning generalize out of task distribution?</title>
      <link>https://arxiv.org/abs/2506.05574</link>
      <description>arXiv:2506.05574v1 Announce Type: cross 
Abstract: In-context learning (ICL) is a remarkable capability of pretrained transformers that allows models to generalize to unseen tasks after seeing only a few examples. We investigate empirically the conditions necessary on the pretraining distribution for ICL to emerge and generalize \emph{out-of-distribution}. Previous work has focused on the number of distinct tasks necessary in the pretraining dataset. Here, we use a different notion of task diversity to study the emergence of ICL in transformers trained on linear functions. We find that as task diversity increases, transformers undergo a transition from a specialized solution, which exhibits ICL only within the pretraining task distribution, to a solution which generalizes out of distribution to the entire task space. We also investigate the nature of the solutions learned by the transformer on both sides of the transition, and observe similar transitions in nonlinear regression problems. We construct a phase diagram to characterize how our concept of task diversity interacts with the number of pretraining tasks. In addition, we explore how factors such as the depth of the model and the dimensionality of the regression problem influence the transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05574v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab</dc:creator>
    </item>
    <item>
      <title>Invariant transports of stationary random measures: asymptotic variance, hyperuniformity, and examples</title>
      <link>https://arxiv.org/abs/2506.05907</link>
      <description>arXiv:2506.05907v1 Announce Type: cross 
Abstract: We consider invariant transports of stationary random measures on $\mathbb{R}^d$ and establish natural mixing criteria that guarantee persistence of asymptotic variances. To check our mixing assumptions, which are based on two-point Palm probabilities, we combine factorial moment expansion with stopping set techniques, among others. We complement our results by providing formulas for the Bartlett spectral measure of the destinations. We pay special attention to the case of a vanishing asymptotic variance, known as hyperuniformity. By constructing suitable transports from a hyperuniform source we are able to rigorously establish hyperuniformity for many point processes and random measures. On the other hand, our method can also refute hyperuniformity. For instance, we show that finitely many steps of Lloyd's algorithm or of a random organization model preserve the asymptotic variance if we start from a Poisson process or a point process with exponentially fast decaying correlation. Finally, we define a hyperuniformerer that turns any ergodic point process with finite intensity into a hyperuniform process by randomizing each point within its cell of a fair partition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05907v1</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael A. Klatt, G\"unter Last, Luca Lotz, D. Yogeshwaran</dc:creator>
    </item>
    <item>
      <title>Competing automorphisms and disordered Floquet codes</title>
      <link>https://arxiv.org/abs/2410.02398</link>
      <description>arXiv:2410.02398v2 Announce Type: replace-cross 
Abstract: Topological order is a promising basis for quantum error correction, a key milestone towards large-scale quantum computing. Floquet codes provide a dynamical scheme for this while also exhibiting Floquet-enriched topological order (FET) where anyons periodically undergo a measurement-induced automorphism that acts uniformly in space. We study disordered Floquet codes where automorphisms have a spatiotemporally heterogeneous distribution -- the automorphisms "compete". We characterize the effect of this competition, showing how key features of the purification dynamics of mixed codestates can be inferred from anyon and automorphism properties for any Abelian topological order. This perspective can explain the protection or measurement of logical information in a dynamic automorphism (DA) code when subjected to a noise model of missing measurements. We demonstrate this using a DA color code with perturbed measurement sequences. The framework of competing automorphisms captures essential features of Floquet codes and robustness to noise, and may elucidate key mechanisms involving topological order, automorphisms, and fault-tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02398v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.str-el</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.111.235112</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 111, 235112 (2025)</arxiv:journal_reference>
      <dc:creator>Cory T. Aitchison, Benjamin B\'eri</dc:creator>
    </item>
    <item>
      <title>Learning the Electronic Hamiltonian of Large Atomic Structures</title>
      <link>https://arxiv.org/abs/2501.19110</link>
      <description>arXiv:2501.19110v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have shown promise in learning the ground-state electronic properties of materials, subverting ab initio density functional theory (DFT) calculations when the underlying lattices can be represented as small and/or repeatable unit cells (i.e., molecules and periodic crystals). Realistic systems are, however, non-ideal and generally characterized by higher structural complexity. As such, they require large (10+ Angstroms) unit cells and thousands of atoms to be accurately described. At these scales, DFT becomes computationally prohibitive, making GNNs especially attractive. In this work, we present a strictly local equivariant GNN capable of learning the electronic Hamiltonian (H) of realistically extended materials. It incorporates an augmented partitioning approach that enables training on arbitrarily large structures while preserving local atomic environments beyond boundaries. We demonstrate its capabilities by predicting the electronic Hamiltonian of various systems with up to 3,000 nodes (atoms), 500,000+ edges, ~28 million orbital interactions (nonzero entries of H), and $\leq$0.53% error in the eigenvalue spectra. Our work expands the applicability of current electronic property prediction methods to some of the most challenging cases encountered in computational materials science, namely systems with disorder, interfaces, and defects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19110v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Hao Xia, Manasa Kaniselvan, Alexandros Nikolaos Ziogas, Marko Mladenovi\'c, Rayen Mahjoub, Alexander Maeder, Mathieu Luisier</dc:creator>
    </item>
    <item>
      <title>In-context denoising with one-layer transformers: connections between attention and associative memory retrieval</title>
      <link>https://arxiv.org/abs/2502.05164</link>
      <description>arXiv:2502.05164v2 Announce Type: replace-cross 
Abstract: We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05164v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Smart, Alberto Bietti, Anirvan M. Sengupta</dc:creator>
    </item>
    <item>
      <title>Monitored Fluctuating Hydrodynamics</title>
      <link>https://arxiv.org/abs/2504.02734</link>
      <description>arXiv:2504.02734v2 Announce Type: replace-cross 
Abstract: We introduce a hydrodynamic framework for describing monitored classical stochastic processes. We study the conditional ensembles for these monitored processes -- i.e., we compute spacetime correlation functions conditioned on a fixed, typical measurement record. In the presence of global symmetries we show that these conditional ensembles can undergo measurement-induced "sharpening" phase transitions as a function of the monitoring rate; moreover, even weak monitoring can give rise to novel critical phases, derived entirely from a classical perspective. We give a simple hydrodynamic derivation of the known charge-sharpening transition for diffusive many-body quantum systems. We show that although the unmonitored symmetric and asymmetric exclusion processes are in different universality classes of transport, their conditional ensembles flow to the same fixed point with emergent relativistic invariance under monitoring. On the other hand, weakly monitored systems with non-Abelian symmetries enter a novel strongly coupled fixed point with non-trivial dynamical exponent, which we characterize. Our formalism naturally accounts for monitoring general observables, such as currents or density gradients, and allows for a direct calculation of information-theoretic diagnostics of sharpening transitions, including the Shannon entropy of the measurement record.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02734v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarang Gopalakrishnan, Ewan McCulloch, Romain Vasseur</dc:creator>
    </item>
  </channel>
</rss>
