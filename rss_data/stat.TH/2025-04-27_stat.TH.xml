<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Apr 2025 04:01:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Non-identifiability distinguishes Neural Networks among Parametric Models</title>
      <link>https://arxiv.org/abs/2504.18017</link>
      <description>arXiv:2504.18017v1 Announce Type: new 
Abstract: One of the enduring problems surrounding neural networks is to identify the factors that differentiate them from traditional statistical models. We prove a pair of results which distinguish feedforward neural networks among parametric models at the population level, for regression tasks. Firstly, we prove that for any pair of random variables $(X,Y)$, neural networks always learn a nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove that for reasonable smooth parametric models, under local and global identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which the parametric model learns the constant predictor $\mathbb{E}[Y]$. Together, our results suggest that a lack of identifiability distinguishes neural networks among the class of smooth parametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18017v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Bernstein Polynomial Processes for Continuous Time Change Detection</title>
      <link>https://arxiv.org/abs/2504.17876</link>
      <description>arXiv:2504.17876v1 Announce Type: cross 
Abstract: There is a lack of methodological results for continuous time change detection due to the challenges of noninformative prior specification and efficient posterior inference in this setting. Most methodologies to date assume data are collected according to uniformly spaced time intervals. This assumption incurs bias in the continuous time setting where, a priori, two consecutive observations measured closely in time are less likely to change than two consecutive observations that are far apart in time. Models proposed in this setting have required MCMC sampling which is not ideal. To address these issues, we derive the heterogeneous continuous time Markov chain that models change point transition probabilities noninformatively. By construction, change points under this model can be inferred efficiently using the forward backward algorithm and do not require MCMC sampling. We then develop a novel loss function for the continuous time setting, derive its Bayes estimator, and demonstrate its performance on synthetic data. A case study using time series of remotely sensed observations is then carried out on three change detection applications. To reduce falsely detected changes in this setting, we develop a semiparametric mean function that captures interannual variability due to weather in addition to trend and seasonal components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17876v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Cunha, Mark Friedl, Luis Carvalho</dc:creator>
    </item>
    <item>
      <title>Maximal Inequalities for Independent Random Vectors</title>
      <link>https://arxiv.org/abs/2504.17885</link>
      <description>arXiv:2504.17885v1 Announce Type: cross 
Abstract: Maximal inequalities refer to bounds on expected values of the supremum of averages of random variables over a collection. They play a crucial role in the study of non-parametric and high-dimensional estimators, and especially in the study of empirical risk minimizers. Although the expected supremum over an infinite collection appears more often in these applications, the expected supremum over a finite collection is a basic building block. This follows from the generic chaining argument. For the case of finite maximum, most existing bounds stem from the Bonferroni inequality (or the union bound). The optimality of such bounds is not obvious, especially in the context of heavy-tailed random vectors.
  In this article, we consider the problem of finding sharp upper and lower bounds for the expected $L_{\infty}$ norm of the mean of finite-dimensional random vectors under marginal variance bounds and an integrable envelope condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17885v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Supratik Basu, Arun K Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Score-Based Deterministic Density Sampling</title>
      <link>https://arxiv.org/abs/2504.18130</link>
      <description>arXiv:2504.18130v1 Announce Type: cross 
Abstract: We propose and analyze a deterministic sampling framework using Score-Based Transport Modeling (SBTM) for sampling an unnormalized target density $\pi$. While diffusion generative modeling relies on pre-training the score function $\nabla \log f_t$ using samples from $\pi$, SBTM addresses the more general and challenging setting where only $\nabla \log\pi$ is known. SBTM approximates the Wasserstein gradient flow on KL$(f_t\|\pi)$ by learning the time-varying score $\nabla \log f_t$ on the fly using score matching. The learned score gives immediate access to relative Fisher information, providing a built-in convergence diagnostic. The deterministic trajectories are smooth, interpretable, and free of Brownian-motion noise, while having the same distribution as ULA. We prove that SBTM dissipates relative entropy at the same rate as the exact gradient flow, provided sufficient training. We further extend our framework to annealed dynamics, to handle non log-concave targets. Numerical experiments validate our theoretical findings: SBTM converges at the optimal rate, has smooth trajectories, and is easily integrated with annealed dynamics. We compare to the baselines of ULA and annealed ULA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18130v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasily Ilin, Bamdad Hosseini, Jingwei Hu</dc:creator>
    </item>
    <item>
      <title>Kalman-Langevin dynamics : exponential convergence, particle approximation and numerical approximation</title>
      <link>https://arxiv.org/abs/2504.18139</link>
      <description>arXiv:2504.18139v1 Announce Type: cross 
Abstract: Langevin dynamics has found a large number of applications in sampling, optimization and estimation. Preconditioning the gradient in the dynamics with the covariance - an idea that originated in literature related to solving estimation and inverse problems using Kalman techniques - results in a mean-field (McKean-Vlasov) SDE. We demonstrate exponential convergence of the time marginal law of the mean-field SDE to the Gibbs measure with non-Gaussian potentials. This extends previous results, obtained in the Gaussian setting, to a broader class of potential functions. We also establish uniform in time bounds on all moments and convergence in $p$-Wasserstein distance. Furthermore, we show convergence of a weak particle approximation, that avoids computing the square root of the empirical covariance matrix, to the mean-field limit. Finally, we prove that an explicit numerical scheme for approximating the particle dynamics converges, uniformly in number of particles, to its continuous-time limit, addressing non-global Lipschitzness in the measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18139v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel Ringh, Akash Sharma</dc:creator>
    </item>
    <item>
      <title>Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels</title>
      <link>https://arxiv.org/abs/2504.18184</link>
      <description>arXiv:2504.18184v1 Announce Type: cross 
Abstract: This paper investigates regularized stochastic gradient descent (SGD) algorithms for estimating nonlinear operators from a Polish space to a separable Hilbert space. We assume that the regression operator lies in a vector-valued reproducing kernel Hilbert space induced by an operator-valued kernel. Two significant settings are considered: an online setting with polynomially decaying step sizes and regularization parameters, and a finite-horizon setting with constant step sizes and regularization parameters. We introduce regularity conditions on the structure and smoothness of the target operator and the input random variables. Under these conditions, we provide a dimension-free convergence analysis for the prediction and estimation errors, deriving both expectation and high-probability error bounds. Our analysis demonstrates that these convergence rates are nearly optimal. Furthermore, we present a new technique for deriving bounds with high probability for general SGD schemes, which also ensures almost-sure convergence. Finally, we discuss potential extensions to more general operator-valued kernels and the encoder-decoder framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18184v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Regression graphs and sparsity-inducing reparametrizations</title>
      <link>https://arxiv.org/abs/2402.09112</link>
      <description>arXiv:2402.09112v4 Announce Type: replace 
Abstract: That parametrization and sparsity are inherently linked raises the possibility that relevant models, not obviously sparse in their natural formulation, exhibit a population-level sparsity after reparametrization. In covariance models, positive-definiteness enforces additional constraints on how sparsity can legitimately manifest. It is therefore natural to consider reparametrization maps in which sparsity respects positive definiteness. The main purpose of this paper is to provide insight into structures on the physically-natural scale that induce and are induced by sparsity after reparametrization. The richest of the four structures initially uncovered can be generated, under a causal ordering, by the joint-response graphs studied by Wermuth &amp; Cox (2004), while the most restrictive is that induced by sparsity on the scale of the matrix logarithm, studied by Battey (2017). The Iwasawa decomposition of the general linear group, combined with the graphical-models interpretation, points to a class of reparametrizations for the chain-graph models (Andersson et al. 2001), with undirected and directed acyclic graphs as special cases. An important insight is the interpretation of approximate zeros, explaining the modelling implications of enforcing sparsity after reparameterization: in effect, the relation between two variables would be declared null if relatively direct regression effects were negligible and others manifested through long paths. The insights have a bearing on methodology, some aspects of which are presented. A detailed simulation uses the theoretical insights to further explore regimes under which reparametrization is beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09112v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Rybak, Heather Battey, Karthik Bharath</dc:creator>
    </item>
    <item>
      <title>RandALO: Out-of-sample risk estimation in no time flat</title>
      <link>https://arxiv.org/abs/2409.09781</link>
      <description>arXiv:2409.09781v2 Announce Type: replace 
Abstract: Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09781v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parth Nobel, Daniel LeJeune, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds for Multiple Models in Matrix Completion</title>
      <link>https://arxiv.org/abs/2411.13199</link>
      <description>arXiv:2411.13199v3 Announce Type: replace 
Abstract: In this paper, we demonstrate how a class of advanced matrix concentration inequalities, introduced in \cite{brailovskaya2024universality}, can be used to eliminate the dimensional factor in the convergence rate of matrix completion. This dimensional factor represents a significant gap between the upper bound and the minimax lower bound, especially in high dimension. Through a more precise spectral norm analysis, we remove the dimensional factors for three popular matrix completion estimators, thereby establishing their minimax rate optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13199v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dali Liu, Haolei Weng</dc:creator>
    </item>
    <item>
      <title>Deep Learning for Individual Heterogeneity</title>
      <link>https://arxiv.org/abs/2010.14694</link>
      <description>arXiv:2010.14694v3 Announce Type: replace-cross 
Abstract: This paper integrates deep neural networks (DNNs) into structural economic models to increase flexibility and capture rich heterogeneity while preserving interpretability. Economic structure and machine learning are complements in empirical modeling, not substitutes: DNNs provide the capacity to learn complex, non-linear heterogeneity patterns, while the structural model ensures the estimates remain interpretable and suitable for decision making and policy analysis. We start with a standard parametric structural model and then enrich its parameters into fully flexible functions of observables, which are estimated using a particular DNN architecture whose structure reflects the economic model. We illustrate our framework by studying demand estimation in consumer choice. We show that by enriching a standard demand model we can capture rich heterogeneity, and further, exploit this heterogeneity to create a personalized pricing strategy. This type of optimization is not possible without economic structure, but cannot be heterogeneous without machine learning. Finally, we provide theoretical justification of each step in our proposed methodology. We first establish non-asymptotic bounds and convergence rates of our structural deep learning approach. Next, a novel and quite general influence function calculation allows for feasible inference via double machine learning in a wide variety of contexts. These results may be of interest in many other contexts, as they generalize prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.14694v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max H. Farrell, Tengyuan Liang, Sanjog Misra</dc:creator>
    </item>
  </channel>
</rss>
