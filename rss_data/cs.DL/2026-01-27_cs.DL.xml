<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 02:52:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Automated Classification of Research Papers Toward Sustainable Development Goals: A Boolean Query-Based Computational Framework</title>
      <link>https://arxiv.org/abs/2601.16988</link>
      <description>arXiv:2601.16988v1 Announce Type: new 
Abstract: The rapid expansion of scholarly publications across diverse disciplines has made it increasingly difficult to systematically evaluate how research contributes to the United Nations Sustainable Development Goals (SDGs). Domain classification of research articles done manually through research experts is extremely impractical because of the number of publications, expensive in time and may not be consistent when done by human beings. This paper proposes an automated and rule-based computational model of classifying research papers based on SDGs with expert curated Boolean query mappings to overcome these challenges. The proposed system has a web-based interface to input data and display results, a backend application programming interface to do high throughput processing, and a Python-based classification engine which uses structured Boolean expressions to process bibliographic metadata (titles, abstracts, and keywords). The framework can be used to support single-paper-based classification and batch-based classification as well as offer clear and understandable outputs that clearly show what query parts motivated each SDG assignment. The experimental testing on massive bibliographic data sets has shown that the system can process thousands of research records in an hour with reproducible and consistent results. The proposed approach provides a viable solution to institutions, researchers and policymakers who are interested in analysis of research alignment with the goal of sustainability in a systematic fashion that would not involve the use of machine learning models whose inputs and outputs are not easily understandable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16988v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Dewani, Kiran Sharma</dc:creator>
    </item>
    <item>
      <title>pyBiblioNet: a Python library for a comprehensive network-based bibliometric analysis</title>
      <link>https://arxiv.org/abs/2601.16990</link>
      <description>arXiv:2601.16990v1 Announce Type: new 
Abstract: Bibliometric analysis is a critical tool for understanding the structure, dynamics, and impact of scientific research. Traditional methods often fall short in capturing the intricate relationships and evolving trends within scientific literature. To address this gap, we present pyBiblioNet, a Python library designed to facilitate comprehensive network-based bibliometric analysis, providing insights into citation networks, co-authorship networks, and keyword co-occurrence networks. The library integrates with OpenAlex, a popular and open catalogue to the global research system, enabling users to easily preprocess, visualize, and analyse bibliometric data. Key features include topic selection, automatic data download via OpenAlex APIs, creation of the root and base sets of manuscripts to analyze, creation of the citation and co-authorship networks, network visualization tools, and a suite of algorithms for computing network centralities, clustering, and community detection, all of them tailored to the bibliometric domain. Additionally, it enables the analysis of key topics and concepts using NLP techniques. We showcase the main functions of the library by performing a bibliometric analysis on the multidisciplinary "15-minute city paradigm", demonstrating the utility of pyBiblioNet in uncovering hidden patterns and emerging trends in various scientific domains. pyBiblioNet can empower researchers, librarians, and policymakers with a powerful, user-friendly tool for enhancing their bibliometric analyses and making data-driven decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16990v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11192-025-05458-0</arxiv:DOI>
      <dc:creator>Mirko Lai, Salvatore Vilella, Federica Cena, Giancarlo Ruffo</dc:creator>
    </item>
    <item>
      <title>BibAgent: An Agentic Framework for Traceable Miscitation Detection in Scientific Literature</title>
      <link>https://arxiv.org/abs/2601.16993</link>
      <description>arXiv:2601.16993v1 Announce Type: new 
Abstract: Citations are the bedrock of scientific authority, yet their integrity is compromised by widespread miscitations: ranging from nuanced distortions to fabricated references. Systematic citation verification is currently unfeasible; manual review cannot scale to modern publishing volumes, while existing automated tools are restricted by abstract-only analysis or small-scale, domain-specific datasets in part due to the "paywall barrier" of full-text access. We introduce BibAgent, a scalable, end-to-end agentic framework for automated citation verification. BibAgent integrates retrieval, reasoning, and adaptive evidence aggregation, applying distinct strategies for accessible and paywalled sources. For paywalled references, it leverages a novel Evidence Committee mechanism that infers citation validity via downstream citation consensus. To support systematic evaluation, we contribute a 5-category Miscitation Taxonomy and MisciteBench, a massive cross-disciplinary benchmark comprising 6,350 miscitation samples spanning 254 fields. Our results demonstrate that BibAgent outperforms state-of-the-art Large Language Model (LLM) baselines in citation verification accuracy and interpretability, providing scalable, transparent detection of citation misalignments across the scientific literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16993v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Peiran Li, Fangzhou Lin, Shuo Xing, Xiang Zheng, Xi Hong, Jiashuo Sun, Zhengzhong Tu, Chaoqun Ni</dc:creator>
    </item>
    <item>
      <title>How Do We Engage with Other Disciplines? A Framework to Study Meaningful Interdisciplinary Discourse in Scholarly Publications</title>
      <link>https://arxiv.org/abs/2601.17020</link>
      <description>arXiv:2601.17020v1 Announce Type: new 
Abstract: With the rising popularity of interdisciplinary work and increasing institutional incentives in this direction, there is a growing need to understand how resulting publications incorporate ideas from multiple disciplines. Existing computational approaches, such as affiliation diversity, keywords, and citation patterns, do not account for how individual citations are used to advance the citing work. Although, in line with addressing this gap, prior studies have proposed taxonomies to classify citation purpose, these frameworks are not well-suited to interdisciplinary research and do not provide quantitative measures of citation engagement quality. To address these limitations, we propose a framework for the evaluation of citation engagement in interdisciplinary Natural Language Processing (NLP) publications. Our approach introduces a citation purpose taxonomy tailored to interdisciplinary work, supported by an annotation study. We demonstrate the utility of this framework through a thorough analysis of publications at the intersection of NLP and Computational Social Science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17020v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bagyasree Sudharsan, Alexandria Leto, Maria Leonor Pacheco</dc:creator>
    </item>
    <item>
      <title>Big Deal cancellations and scholarly publishing: Insights from faculty and graduate student interviews</title>
      <link>https://arxiv.org/abs/2601.17033</link>
      <description>arXiv:2601.17033v1 Announce Type: new 
Abstract: Big Deal cancellations are increasingly undertaken by academic librarians faced with rising subscription costs and shrinking collections budgets. While past research has focused on librarians' decision-making processes and communication strategies, this study aims to understand the perspectives and experiences of faculty and graduate students with Big Deal cancellations through interviews at three medium-sized Canadian institutions. It considers cancellations as a collaborative process of information exchange, rather than a top-down process. This study's findings can inform how and why cancellation projects can be undertaken with enhanced understandings of their lived realities of Big Deals and the current state of scholarly publishing. This study has been accepted by College and Research Libraries and will be published in July 2026.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17033v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madelaine Hare, Philippe Mongeon, Samuel Cassady, Catherine A. Johnson</dc:creator>
    </item>
    <item>
      <title>Deferred Acceptance Algorithm Improves Peer Review Process</title>
      <link>https://arxiv.org/abs/2601.17035</link>
      <description>arXiv:2601.17035v1 Announce Type: new 
Abstract: The peer review process is essential to the success of science, but it also delays publications and absorbs considerable effort. Journals find it increasingly difficult to recruit competent reviewers. This study presents the results of agent-based simulation that models the current peer review process. We compared it to the simulation of a new peer review process that uses the Deferred Acceptance Algorithm (DAA) to match papers to journals. The matches are just as good while dramatically reducing the required number of reviews and delays. The results show that it is possible for the scientific community to significantly optimise the peer review process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17035v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christoph Bartneck, Richard Watt, Etienne Borde, Pattara Klinpibul</dc:creator>
    </item>
    <item>
      <title>LLM-Generated or Human-Written? Comparing Review and Non-Review Papers on ArXiv</title>
      <link>https://arxiv.org/abs/2601.17036</link>
      <description>arXiv:2601.17036v1 Announce Type: new 
Abstract: ArXiv recently prohibited the upload of unpublished review papers to its servers in the Computer Science domain, citing a high prevalence of LLM-generated content in these categories. However, this decision was not accompanied by quantitative evidence. In this work, we investigate this claim by measuring the proportion of LLM-generated content in review vs. non-review research papers in recent years. Using two high-quality detection methods, we find a substantial increase in LLM-generated content across both review and non-review papers, with a higher prevalence in review papers. However, when considering the number of LLM-generated papers published in each category, the estimates of non-review LLM-generated papers are almost six times higher. Furthermore, we find that this policy will affect papers in certain domains far more than others, with the CS subdiscipline Computers &amp; Society potentially facing cuts of 50%. Our analysis provides an evidence-based framework for evaluating such policy decisions, and we release our code to facilitate future investigations at: https://github.com/yanaiela/llm-review-arxiv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17036v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanai Elazar, Maria Antoniak</dc:creator>
    </item>
    <item>
      <title>Authority Signals in AI Cited Health Sources: A Framework for Evaluating Source Credibility in ChatGPT Responses</title>
      <link>https://arxiv.org/abs/2601.17109</link>
      <description>arXiv:2601.17109v1 Announce Type: new 
Abstract: Health information seeking has fundamentally changed since the onset of Large Language Models (LLM), with nearly one third of ChatGPT's 800 million users asking health questions weekly. Understanding the sources of those AI generated responses is vital, as health organizations and providers are also investing in digital strategies to organically improve their ranking, reach and visibility in LLM systems like ChatGPT. As AI search optimization strategies are gaining maturity, this study introduces an Authority Signals Framework, organized in four domains that reflect key components to health information seeking, starting with "Who wrote it?" (Author Credentials), followed by "Who published it?" (Institutional Affiliation), "How was it vetted?" (Quality Assurance), and "How does AI find it?" (Digital Authority). This descriptive cross-sectional study randomly selected 100 questions from HealthSearchQA which contains 3,173 consumer health questions curated by Google Research from publicly available search engine suggestions. Those questions were entered into ChatGPT 5.2 Pro to record and code the cited sources through the lens of the Authority Signals Framework's four domains. Descriptive statistics were calculated for all cited sources (n=615), and cross tabulations were conducted to examine distinction among organization types. Over 75% of the sources cited in ChatGPT's health generated responses were from established institutional sources, such as Mayo Clinic, Cleveland Clinic, Wikipedia, National Health Service, PubMed with the remaining citations sourced from alternative health information sources that lacked established institutional backing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17109v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin Jacques (York College, CUNY), Erela Datuowei (Teachers College, Columbia University), Vincent Jones II (York College, CUNY), Corey Basch (William Paterson University), Celeta Vanderpool (Teachers College, Columbia University), Nkechi Udeozo (CUNY School of Public Health), Griselda Chapa (York College, CUNY)</dc:creator>
    </item>
    <item>
      <title>Using LibCal Seats to Better Serve Students</title>
      <link>https://arxiv.org/abs/2601.18230</link>
      <description>arXiv:2601.18230v1 Announce Type: new 
Abstract: This chapter examines the evolution of library services at the University of Li\`ege (ULi\`ege), with a focus on the implementation and assessment of the LibCal Seats booking module. Introduced in September 2020 in response to the COVID-19 pandemic, this system was designed to manage occupancy and maintain social distancing. While initially a temporary measure, the seat booking service remains in use during peak periods. Drawing on survey data from 2022 and 2023, the chapter analyses user perceptions of the system. Results indicate strong student appreciation, particularly regarding stress reduction and equitable access to study spaces. Despite overall satisfaction, issues such as unoccupied reserved seats and an unnecessarily complex booking process emerged, leading to targeted improvements. This chapter highlights the importance of responsive, user-centred services in academic libraries. The adoption of the booking system helped address challenges such as overcrowding and "seat hogging," ultimately contributing to a more organised and accessible environment. The case study illustrates how technology can enhance library service delivery, offering insights for institutions seeking to optimise space management. The continued evaluation of the system reflects a broader commitment to adapting services in alignment with user needs and institutional priorities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18230v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Renaville, Fabienne Prosmans</dc:creator>
    </item>
    <item>
      <title>Designing large language model prompts to extract scores from messy text: A shared dataset and challenge</title>
      <link>https://arxiv.org/abs/2601.18271</link>
      <description>arXiv:2601.18271v1 Announce Type: new 
Abstract: In some areas of computing, natural language processing and information science, progress is made by sharing datasets and challenging the community to design the best algorithm for an associated task. This article introduces a shared dataset of 1446 short texts, each of which describes a research quality score on the UK scale of 1* to 4*. This is a messy collection, with some texts not containing scores and others including invalid scores or strange formats. With this dataset there is also a description of what constitutes a valid score and a "gold standard" of the correct scores for these texts (including missing values). The challenge is to design a prompt for Large Language Models (LLMs) to extract the scores from these texts as accurately as possible. The format for the response should be a number and no other text so there are two aspects to the challenge: ensuring that the LLM returns only a number, and instructing it to deduce the correct number for the text. As part of this, the LLM prompt needs to explain when to return the missing value code, -1, instead of a number when the text does not clearly contain one. The article also provides an example of a simple prompt. The purpose of the challenge is twofold: to get an effective solution to this problem, and to increase understanding of prompt design and LLM capabilities for complex numerical tasks. The initial solution suggested has an accuracy of 72.6%, so the challenge is to beat this.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18271v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Trends in Information Management, 13(2), paper 1 (2025)</arxiv:journal_reference>
      <dc:creator>Mike Thelwall</dc:creator>
    </item>
    <item>
      <title>Issues regarding the Indexing of Publication Types and Study Designs</title>
      <link>https://arxiv.org/abs/2601.18616</link>
      <description>arXiv:2601.18616v1 Announce Type: new 
Abstract: Objectives. Major research and implementation efforts have been devoted to indexing articles according to the major topics discussed, but much less effort to indexing their publication types and study designs (collectively, PTs). In this Perspective, we discuss how indexing PTs differs from topical MeSH indexing and requires a different approach. Materials and Methods. Rather than focus on the technical aspects of machine learning-based indexing models, we emphasize the goals and purposes for which biomedical articles are indexed, and the surprisingly thorny question of how indexing systems should be evaluated. Results. Topical Medical Subject Heading (MeSH) terms are assigned to articles that cover the major topics discussed; when more than one term is applicable, only the most specific term is assigned. In contrast, PTs are assigned to articles that have a given structure or use a particular design. To meet the needs of end users, particularly groups involved in evidence syntheses, PT indexing needs to be comprehensive and employ probabilistic prediction scores. Whereas existing NLM hierarchies place publication types and study design-related terms on separate trees from each other, a unified rubric permits more appropriate retrieval via automatic expansion. Discussion. Automated PT indexing systems should allow users to input article records or full text pdfs and receive scores in real time. This will offer consistent indexing across bibliographic databases, as well as preprints and unpublished manuscripts. Conclusions. Automated PT indexing systems, properly designed and implemented, hold the promise of greatly improving the retrieval of biomedical articles, saving substantial effort when writing evidence syntheses and benefiting other users as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18616v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Neil R. Smalheiser, Joe D. Menke, Arthur W. Holt, Halil Kilicoglu, Jodi Schneider</dc:creator>
    </item>
    <item>
      <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
      <link>https://arxiv.org/abs/2601.17431</link>
      <description>arXiv:2601.17431v1 Announce Type: cross 
Abstract: The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17431v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Kemal \.Ilter</dc:creator>
    </item>
    <item>
      <title>HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences</title>
      <link>https://arxiv.org/abs/2601.18724</link>
      <description>arXiv:2601.18724v1 Announce Type: cross 
Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18724v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</dc:creator>
    </item>
    <item>
      <title>CiteGuard: Faithful Citation Attribution for LLMs via Retrieval-Augmented Validation</title>
      <link>https://arxiv.org/abs/2510.17853</link>
      <description>arXiv:2510.17853v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However, there have been concerns regarding the quality and reliability of the generated text, one of which is the citation accuracy and faithfulness. While most recent work relies on methods such as LLM-as-a-Judge, the reliability of LLM-as-a-Judge alone is also in doubt. In this work, we reframe citation evaluation as a problem of citation attribution alignment, which assesses whether LLM-generated citations match those a human author would include for the same text. We propose CiteGuard, a retrieval-aware agent framework designed to provide more faithful grounding for citation validation. CiteGuard improves the prior baseline by 17%, and achieves up to 68.1% accuracy on the CiteME benchmark, approaching human-level performance (69.7%). It also enables the identification of alternative but valid citations and demonstrates generalization ability for cross-domain citation attribution.Our code is available at https://github.com/KathCYM/CiteGuard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17853v3</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yee Man Choi, Xuehang Guo, Yi R. Fung, Qingyun Wang</dc:creator>
    </item>
  </channel>
</rss>
