<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 03:29:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Quantifying Fire Risk Index in Chemical Industry Using Statistical Modeling Procedure</title>
      <link>https://arxiv.org/abs/2509.21736</link>
      <description>arXiv:2509.21736v1 Announce Type: new 
Abstract: Fire incident reports contain detailed textual narratives that capture causal factors often overlooked in structured records, while financial damage amounts provide measurable outcomes of these events. Integrating these two sources of information is essential for uncovering interpretable links between descriptive causes and their economic consequences. To this end, we develop a data-driven framework that constructs a composite Risk Index, enabling systematic quantification of how specific keywords relate to property damage amounts. This index facilitates both the identification of high-impact terms and the aggregation of risks across semantically related clusters, thereby offering a principled measure of fire-related financial risk. Using more than a decade of Korean fire investigation reports on the chemical industry classified as Special Buildings (2013 through 2024), we employ topic modeling and network-based embedding to estimate semantic similarities from interactions among words and subsequently apply Lasso regression to quantify their associations with property damage amounts, thereby estimate fire risk index. This approach enables us to assess fire risk not only at the level of individual terms but also within their broader textual context, where highly interactive related words provide insights into collective patterns of hazard representation and their potential impact on expected losses. The analysis highlights several domains of risk, including hazardous chemical leakage, unsafe storage practices, equipment and facility malfunctions, and environmentally induced ignition. The results demonstrate that text-derived indices provide interpretable and practically relevant insights, bridging unstructured narratives with structured loss information and offering a basis for evidence-based fire risk assessment and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21736v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyewon Jung, Seungil Ahn, Seungho Choi, Yeseul Jeon</dc:creator>
    </item>
    <item>
      <title>Personalized Oncology: Feasibility of Evaluating Treatment Effects for Individual Patients</title>
      <link>https://arxiv.org/abs/2509.22089</link>
      <description>arXiv:2509.22089v1 Announce Type: new 
Abstract: The effectiveness of personalized oncology treatments ultimately depends on whether outcomes can be causally attributed to the treatment. Advances in precision oncology have improved molecular profiling of individuals, and tailored therapies have led to more effective treatments for select patient groups. However, treatment responses still vary among individuals. As cancer is a heterogeneous and dynamic disease with varying treatment outcomes across different molecular types and resistance mechanisms, it requires customized approaches to identify cause-and-effect relationships. N-of-1 trials, or single-subject clinical trials, are designed to evaluate individual treatment effects. Several works have described different causal frameworks to identify treatment effects in N-of-1 trials, yet whether these approaches can be extended to single-cancer patient settings remains unclear. To explore this possibility, a longitudinal dataset from a single metastatic cancer patient with adaptively chosen treatments was considered. The dataset consisted of a detailed treatment plan as well as biomarker and lesion measurements recorded over time. After data processing, a treatment period with sufficient data points to conduct causal inference was selected. Under this setting, a causal framework was applied to define an estimand, identify causal relationships and assumptions, and calculate an individual-specific treatment effect using a time-varying g-formula. Through this application, we illustrate explicitly when and how causal treatment effects can be estimated in single-patient oncology settings. Our findings not only demonstrate the feasibility of applying causal methods in a single-cancer patient setting but also offer a blueprint for using causal methods across a broader spectrum of cancer types in individualized settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22089v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia Jang (Charit\'e -- Universit\"atsmedizin Berlin), Stefan Konigorski (Hasso Plattner Institute)</dc:creator>
    </item>
    <item>
      <title>Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the Connection using Survey Data and Predictive Models</title>
      <link>https://arxiv.org/abs/2509.22275</link>
      <description>arXiv:2509.22275v1 Announce Type: new 
Abstract: Chronic stress was implicated in cancer occurrence, but a direct causal connection has not been consistently established. Machine learning and causal modeling offer opportunities to explore complex causal interactions between psychological chronic stress and cancer occurrences. We developed predictive models employing variables from stress indicators, cancer history, and demographic data from self-reported surveys, unveiling the direct and immune suppression mitigated connection between chronic stress and cancer occurrence. The models were corroborated by traditional statistical methods. Our findings indicated significant causal correlations between stress frequency, stress level and perceived health impact, and cancer incidence. Although stress alone showed limited predictive power, integrating socio-demographic and familial cancer history data significantly enhanced model accuracy. These results highlight the multidimensional nature of cancer risk, with stress emerging as a notable factor alongside genetic predisposition. These findings strengthen the case for addressing chronic stress as a modifiable cancer risk factor, supporting its integration into personalized prevention strategies and public health interventions to reduce cancer incidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22275v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teddy Lazebnik, Vered Aharonson</dc:creator>
    </item>
    <item>
      <title>Discovering and Analyzing Stochastic Processes to Reduce Waste in Food Retail</title>
      <link>https://arxiv.org/abs/2509.21322</link>
      <description>arXiv:2509.21322v1 Announce Type: cross 
Abstract: This paper proposes a novel method for analyzing food retail processes with a focus on reducing food waste. The approach integrates object-centric process mining (OCPM) with stochastic process discovery and analysis. First, a stochastic process in the form of a continuous-time Markov chain is discovered from grocery store sales data. This model is then extended with supply activities. Finally, a what-if analysis is conducted to evaluate how the quantity of products in the store evolves over time. This enables the identification of an optimal balance between customer purchasing behavior and supply strategies, helping to prevent both food waste due to oversupply and product shortages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21322v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna Kalenkova, Lu Xia, Dirk Neumann</dc:creator>
    </item>
    <item>
      <title>Modeling Psychological Profiles in Volleyball via Mixed-Type Bayesian Networks</title>
      <link>https://arxiv.org/abs/2509.22111</link>
      <description>arXiv:2509.22111v1 Announce Type: cross 
Abstract: Psychological attributes rarely operate in isolation: coaches reason about networks of related traits. We analyze a new dataset of 164 female volleyball players from Italy's C and D leagues that combines standardized psychological profiling with background information. To learn directed relationships among mixed-type variables (ordinal questionnaire scores, categorical demographics, continuous indicators), we introduce latent MMHC, a hybrid structure learner that couples a latent Gaussian copula and a constraint-based skeleton with a constrained score-based refinement to return a single DAG. We also study a bootstrap-aggregated variant for stability. In simulations spanning sample size, sparsity, and dimension, latent Max-Min Hill-Climbing (MMHC) attains lower structural Hamming distance and higher edge recall than recent copula-based learners while maintaining high specificity. Applied to volleyball, the learned network organizes mental skills around goal setting and self-confidence, with emotional arousal linking motivation and anxiety, and locates Big-Five traits (notably neuroticism and extraversion) upstream of skill clusters. Scenario analyses quantify how improvements in specific skills propagate through the network to shift preparation, confidence, and self-esteem. The approach provides an interpretable, data-driven framework for profiling psychological traits in sport and for decision support in athlete development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22111v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Iannario, Dae-Jin Lee, Manuele Leonelli</dc:creator>
    </item>
    <item>
      <title>COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics</title>
      <link>https://arxiv.org/abs/2509.22240</link>
      <description>arXiv:2509.22240v1 Announce Type: cross 
Abstract: In clinical applications, the utility of segmentation models is often based on the accuracy of derived downstream metrics such as organ size, rather than by the pixel-level accuracy of the segmentation masks themselves. Thus, uncertainty quantification for such metrics is crucial for decision-making. Conformal prediction (CP) is a popular framework to derive such principled uncertainty guarantees, but applying CP naively to the final scalar metric is inefficient because it treats the complex, non-linear segmentation-to-metric pipeline as a black box. We introduce COMPASS, a practical framework that generates efficient, metric-based CP intervals for image segmentation models by leveraging the inductive biases of their underlying deep neural networks. COMPASS performs calibration directly in the model's representation space by perturbing intermediate features along low-dimensional subspaces maximally sensitive to the target metric. We prove that COMPASS achieves valid marginal coverage under exchangeability and nestedness assumptions. Empirically, we demonstrate that COMPASS produces significantly tighter intervals than traditional CP baselines on four medical image segmentation tasks for area estimation of skin lesions and anatomical structures. Furthermore, we show that leveraging learned internal features to estimate importance weights allows COMPASS to also recover target coverage under covariate shifts. COMPASS paves the way for practical, metric-based uncertainty quantification for medical image segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22240v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Y. Cheung, Ashok Veeraraghavan, Guha Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Model Training, Data Assimilation, and Forecast Experiments with a Hybrid Atmospheric Model that Incorporates Machine Learning</title>
      <link>https://arxiv.org/abs/2509.22465</link>
      <description>arXiv:2509.22465v1 Announce Type: cross 
Abstract: The hybrid model combines the physics-based primitive-equations model SPEEDY with a machine learning-based (ML-based) model component, while ERA5 reanalyses provide the presumed true states of the atmosphere. Six-hourly simulated noisy observations are generated for a 30-year ML training period and a one-year testing period. These observations are assimilated with a Local Ensemble Transform Kalman Filter (LETKF), and a 10-day deterministic forecast is also started from each ensemble mean analysis of the testing period. In the first experiment, the physics-based model provides the background ensemble members and the 10-day deterministic forecasts. In the other three experiments, the hybrid model plays the same role as the physics-based model in the first experiment, but it is trained on a different data set in each experiment. These training data sets are analyses obtained by using the physics-based model (second experiment), the hybrid model of the previous experiment (third experiment), and for comparison, ERA5 reanalyses (fourth experiment). The results of the experiments show that hybridizing the model can substantially improve the accuracy of the analyses and forecasts. When the model is trained on ERA5 reanalyses, the biases of the analyses are negligible and the magnitude of the flow-dependent part of the analysis errors is greatly reduced. While the gains in analysis accuracy are distinctly more modest in the other two hybrid model experiments, the gains in forecast accuracy tend to be larger in those experiments after 1-3 forecast days. However, these extra gains of forecast accuracy are achieved, in part, by a modest gradual reduction of the spatial variability of the forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22465v1</guid>
      <category>nlin.CD</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Elliott, Troy Arcomano, Istvan Szunyogh, Brian R. Hunt</dc:creator>
    </item>
    <item>
      <title>Modelling non-stationary extremal dependence through a geometric approach</title>
      <link>https://arxiv.org/abs/2509.22501</link>
      <description>arXiv:2509.22501v1 Announce Type: cross 
Abstract: Non-stationary extremal dependence, whereby the relationship between the extremes of multiple variables evolves over time, is commonly observed in many environmental and financial data sets. However, most multivariate extreme value models are only suited to stationary data. A recent approach to multivariate extreme value modelling uses a geometric framework, whereby extremal dependence features are inferred through the limiting shapes of scaled sample clouds. This framework can capture a wide range of dependence structures, and a variety of inference procedures have been proposed in the stationary setting. In this work, we first extend the geometric framework to the non-stationary setting and outline assumptions to ensure the necessary convergence conditions hold. We then introduce a flexible, semi-parametric modelling framework for obtaining estimates of limit sets in the non-stationary setting. Through rigorous simulation studies, we demonstrate that our proposed framework can capture a wide range of dependence forms and is robust to different model formulations. We illustrate the proposed methods on financial returns data and present several practical uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. R. Murphy-Barltrop, J. L. Wadsworth, M. de Carvalho, B. D. Youngman</dc:creator>
    </item>
    <item>
      <title>Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory</title>
      <link>https://arxiv.org/abs/2509.22505</link>
      <description>arXiv:2509.22505v1 Announce Type: cross 
Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly popular, offering empathetic interactions, yet their psychosocial impacts remain unclear. We examined how engaging with AICCs shaped wellbeing and how users perceived these experiences. First, we conducted a large-scale quasi-experimental study of longitudinal Reddit data, applying stratified propensity score matching and Difference-in-Differences regression. Findings revealed mixed effects -- greater affective and grief expression, readability, and interpersonal focus, alongside increases in language about loneliness and suicidal ideation. Second, we complemented these results with 15 semi-structured interviews, which we thematically analyzed and contextualized using Knapp's relationship development model. We identified trajectories of initiation, escalation, and bonding, wherein AICCs provided emotional validation and social rehearsal but also carried risks of over-reliance and withdrawal. Triangulating across methods, we offer design implications for AI companions that scaffold healthy boundaries, support mindful engagement, support disclosure without dependency, and surface relationship stages -- maximizing psychosocial benefits while mitigating risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22505v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Yuan, Jiaxun Zhang, Talayeh Aledavood, Renwen Zhang, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Estimating average treatment effects when treatment data are absent in a target study</title>
      <link>https://arxiv.org/abs/2509.22543</link>
      <description>arXiv:2509.22543v1 Announce Type: cross 
Abstract: Researchers are frequently interested in understanding the causal effect of treatment interventions. However, in some cases, the treatment of interest--readily available in a randomized controlled trial (RCT)--is either not directly measured or entirely unavailable in observational datasets. This challenge has motivated the development of stochastic incremental propensity score interventions which operate on post-treatment exposures affected by the treatment of interest with the aim of approximating the causal effects of the treatment intervention. Yet, a key challenge lies in the fact that the precise distributional shift of these post-treatment exposures induced by the treatment is typically unknown, making it uncertain whether the approximation truly reflects the causal effect of interest. The primary objective of this paper is to explore data integration methodologies to characterize a distribution of post-treatment exposures resulting from the treatment in an external dataset, and to use this information to estimate counterfactual mean outcomes under treatment interventions, in settings where the observational data lack treatment information and the external data may not contain measurements of the outcome of interest. We will discuss the underlying assumptions required for this approach and provide methodological guidance on estimation strategies to address these challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22543v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Aaron L Sarvet</dc:creator>
    </item>
    <item>
      <title>Simple Macroeconomic Forecast Distributions for the G7 Economies</title>
      <link>https://arxiv.org/abs/2408.08304</link>
      <description>arXiv:2408.08304v4 Announce Type: replace 
Abstract: We present a simple method for predicting the distribution of output growth and inflation in the G7 economies. The method is based on point forecasts published by the International Monetary Fund (IMF), as well as robust statistics from the empirical distribution of the IMF's past forecast errors while imposing coherence of prediction intervals across horizons. We show that the technique yields calibrated prediction intervals and performs similar to, or better than, more complex time series models in terms of statistical loss functions. We provide a simple website with graphical illustrations of our forecasts, as well as time-stamped data files that document their real-time character.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08304v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Friederike Becker, Fabian Kr\"uger, Melanie Schienle</dc:creator>
    </item>
    <item>
      <title>Robust semi-parametric signal detection in particle physics with classifiers decorrelated via optimal transport</title>
      <link>https://arxiv.org/abs/2409.06399</link>
      <description>arXiv:2409.06399v3 Announce Type: replace 
Abstract: Searches for signals of new physics in particle physics are usually done by training a supervised classifier to separate a signal model from the known Standard Model physics (also called the background model). However, even when the signal model is correct, systematic errors in the background model can influence supervised classifiers and might adversely affect the signal detection procedure. To tackle this problem, one approach is to use the (possibly misspecified) classifier only to perform a preliminary signal-enrichment step and then to carry out a signal detection test on the signal-rich sample. For this procedure to work, we need a classifier constrained to be decorrelated with one or more protected variables used for the signal-detection step. We do this by considering an optimal transport map of the classifier output that makes it independent of the protected variable(s) for the background. We then fit a semiparametric mixture model to the distribution of the protected variable after making cuts on the transformed classifier to detect the presence of a signal. We compare and contrast this decorrelation method with previous approaches, show that the decorrelation procedure is robust to moderate background misspecification, and analyze the power and validity of the signal detection test as a function of the cut on the classifier both with and without decorrelation. We conclude that decorrelation and signal enrichment help produce a stable, robust, valid, and more powerful test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06399v3</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Purvasha Chakravarti, Lucas Kania, Olaf Behnke, Mikael Kuusela, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Forecasting the future development in quality and value of professional football players</title>
      <link>https://arxiv.org/abs/2502.07528</link>
      <description>arXiv:2502.07528v3 Announce Type: replace 
Abstract: Transfers in professional football (soccer) are risky investments because of the large transfer fees and high risks involved. Although data-driven models can be used to improve transfer decisions, existing models focus on describing players' historical progress, leaving their future performance unknown. Moreover, recent developments have called for the use of explainable models combined with uncertainty quantification of predictions. This paper assesses explainable machine learning models based on predictive accuracy and uncertainty quantification methods for the prediction of the future development in quality and transfer value of professional football players. The predictive accuracy is studied by training the models to predict the quality and value of players one year ahead. This is carried out by training them on two data sets containing data-driven indicators describing the player quality and player value in historical settings. In general, the random forest model is found to be the most suitable model because it provides accurate predictions as well as an uncertainty quantification method that naturally arises from the bagging procedure of the random forest model. Additionally, this research shows that the development of player performance contains nonlinear patterns and interactions between variables, and that time series information can provide useful information for the modeling of player performance metrics. The resulting models can help football clubs make more informed, data-driven transfer decisions by forecasting player quality and transfer value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07528v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/app15168916</arxiv:DOI>
      <arxiv:journal_reference>Appl.Sci. (2025), 15:8916</arxiv:journal_reference>
      <dc:creator>Koen W. van Arem, Floris Goes-Smit, Jakob S\"ohl</dc:creator>
    </item>
    <item>
      <title>Forecasting the U.S. Renewable-Energy Mix with an ALR-BDARMA Compositional Time-Series Framework</title>
      <link>https://arxiv.org/abs/2507.04087</link>
      <description>arXiv:2507.04087v3 Announce Type: replace 
Abstract: Accurate forecasts of the US renewable-generation mix are critical for planning transmission upgrades, sizing storage, and setting balancing-market rules. We present a Bayesian Dirichlet ARMA (BDARMA) model for monthly shares of hydro, geothermal, solar, wind, wood, municipal waste, and biofuels from January 2010 to January 2025. The mean vector follows a parsimonious VAR(2) in additive-log-ratio space, while the Dirichlet concentration parameter combines an intercept with ten Fourier harmonics, letting predictive dispersion expand or contract with the seasons.
  A 61-split rolling-origin study generates twelve-month density forecasts from January 2019 to January 2024. Relative to three benchmarks, a Gaussian VAR(2) in transform space, a seasonal naive copy of last year's proportions, and a drift-free additive-log-ratio random walk, BDARMA lowers the mean continuous ranked probability score by fifteen to sixty percent, achieves component-wise ninety percent interval coverage close to nominal, and matches Gaussian VAR point accuracy through eight months with a maximum loss of 0.02 Aitchison units thereafter. BDARMA therefore delivers sharp, well-calibrated probabilistic forecasts of multivariate renewable-energy shares without sacrificing point precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04087v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Thomas Maierhofer</dc:creator>
    </item>
    <item>
      <title>Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2505.10213</link>
      <description>arXiv:2505.10213v3 Announce Type: replace-cross 
Abstract: With the widespread adoption of Large Language Models (LLMs), there is a growing need to establish best practices for leveraging their capabilities beyond traditional natural language tasks. In this paper, a novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting -- a task of increasing relevance in fields such as energy systems, finance, and healthcare. The approach systematically infuses LLMs with structured temporal information to improve their forecasting accuracy. This study evaluates the proposed method on a real-world time series dataset and compares it to a naive baseline where the LLM receives no auxiliary information. Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization. These findings highlight the potential of knowledge transfer strategies to bridge the gap between LLMs and domain-specific forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10213v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohammadmahdi Ghasemloo, Alireza Moradi</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v2 Announce Type: replace-cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. The model can identify expression level as well as expressed proportion that could mediate disease-leading causal pathway. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Li Chen, Maaike van Gerwen, Zhigang Li</dc:creator>
    </item>
  </channel>
</rss>
