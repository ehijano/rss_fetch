<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Apr 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sacred or Secular? Religious Bias in AI-Generated Financial Advice</title>
      <link>https://arxiv.org/abs/2504.07118</link>
      <description>arXiv:2504.07118v1 Announce Type: new 
Abstract: This study examines religious biases in AI-generated financial advice, focusing on ChatGPT's responses to financial queries. Using a prompt-based methodology and content analysis, we find that 50% of the financial emails generated by ChatGPT exhibit religious biases, with explicit biases present in both ingroup and outgroup interactions. While ingroup biases personalize responses based on religious alignment, outgroup biases introduce religious framing that may alienate clients or create ideological friction. These findings align with broader research on AI bias and suggest that ChatGPT is not merely reflecting societal biases but actively shaping financial discourse based on perceived religious identity. Using the Critical Algorithm Studies framework, we argue that ChatGPT functions as a mediator of financial narratives, selectively reinforcing religious perspectives. This study underscores the need for greater transparency, bias mitigation strategies, and regulatory oversight to ensure neutrality in AI-driven financial services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07118v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Salar Khan, Hamza Umer</dc:creator>
    </item>
    <item>
      <title>From Public Data to Private Information: The Case of the Supermarket</title>
      <link>https://arxiv.org/abs/2504.07121</link>
      <description>arXiv:2504.07121v1 Announce Type: new 
Abstract: The background to this paper is that in our world of massively increasing personal digital data any control over the data about me seems illusionary - informational privacy seems a lost cause. On the other hand, the production of this digital data seems a necessary component of our present life in the industrialized world. A framework for a resolution of this apparent dilemma is provided if by the distinction between (meaningless) data and (meaningful) information. I argue that computational data processing is necessary for many present-day processes and not a breach of privacy, while collection and processing of private information is often not necessary and a breach of privacy. The problem and the sketch of its solution are illustrated in a case-study: supermarket customer cards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07121v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>(2009) in Maria Bottis (ed.), Proceedings of the 8th International Conference Computer Ethics: Philosophical Enquiry (Corfu: Nomiki Bibliothiki), 500-507</arxiv:journal_reference>
      <dc:creator>Vincent C. M\"uller</dc:creator>
    </item>
    <item>
      <title>Synergizing Self-Regulation and Artificial-Intelligence Literacy Towards Future Human-AI Integrative Learning</title>
      <link>https://arxiv.org/abs/2504.07125</link>
      <description>arXiv:2504.07125v1 Announce Type: new 
Abstract: Self-regulated learning (SRL) and Artificial-Intelligence (AI) literacy are becoming key competencies for successful human-AI interactive learning, vital to future education. However, despite their importance, students face imbalanced and underdeveloped SRL and AI literacy capabilities, inhibiting effective using AI for learning. This study analyzed data from 1,704 Chinese undergraduates using clustering methods to uncover four learner groups reflecting developing process(Potential, Development, Master, and AI-Inclined) characterized by varying SRL and AI literacy differentiation. Results highlight obvious disparities in SRL and AI literacy synchronization, with the Master Group achieving balanced development and critical AI-using for SRL, while AI-Inclined Group demonstrate over-reliance on AI and poor SRL application. The Potential Group showed a close mutual promotion trend between SRL and AI literacy, while the Development Group showed a discrete correlation. Resources and instructional guidance support emerged as key factors affecting these differentiations. To translate students to master SRL-AI literacy level and progress within it, the study proposes differentiated support strategies and suggestions. Synergizing SRL and AI literacy growth is the core of development, ensuring equitable and advanced human-centered interactive learning models for future human-AI integrating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07125v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Long (Jim),  Zhang (Cindy),  Shijun (Cindy),  Chen</dc:creator>
    </item>
    <item>
      <title>Glocalizing Generative AI in Education for the Global South: The Design Case of 21st Century Teacher Educator AI for Ghana</title>
      <link>https://arxiv.org/abs/2504.07149</link>
      <description>arXiv:2504.07149v1 Announce Type: new 
Abstract: This study presents the design and development of the 21st Century Teacher Educator for Ghana GPT, a customized Generative AI (GenAI) tool created using OpenAI's Retrieval-Augmented Generation (RAG) and Interactive Semi-Automated Prompting Strategy (ISA). Anchored in a Glocalized design approach, this tool supports pre-service teachers (PSTs) in Ghana by embedding localized linguistic, cultural, and curricular content within globally aligned principles of ethical and responsible AI use. The model utilizes structured, preloaded datasets-including Ghana's National Teacher Education Curriculum Framework (NTECF), UNESCO's (2023) AI guidelines, and culturally responsive pedagogies-to offer curriculum-aligned, linguistically adaptive, and pedagogically grounded learning support. The ISA enables users to input their institution, year, and semester, generating tailored academic content such as lecture notes, assessment practice, practicum resources, and action research guidance. The design incorporates the Culture and Context-Aware Framework, GenAI-CRSciA, and frameworks addressing GenAI neocolonialism to ensure equity, curriculum fidelity, and local relevance. Pilot implementation revealed notable strengths in language adaptation and localization, delivering bilingual support in English and Ghanaian languages like Twi, Dagbani, Mampruli, and Dagaare, with contextualized examples for deeper understanding. The GPT also generated practice assessments aligned with course objectives, reinforcing learner engagement. Challenges included occasional hallucinations due to limited corpora in some indigenous languages and access barriers tied to premium subscriptions. This design case contributes to discourse on Glocalized GenAI and calls for collaboration with OpenAI NextGen to expand access and empirically assess usage across diverse African educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07149v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba</dc:creator>
    </item>
    <item>
      <title>AI-based identification and support of at-risk students: A case study of the Moroccan education system</title>
      <link>https://arxiv.org/abs/2504.07160</link>
      <description>arXiv:2504.07160v1 Announce Type: new 
Abstract: Student dropout is a global issue influenced by personal, familial, and academic factors, with varying rates across countries. This paper introduces an AI-driven predictive modeling approach to identify students at risk of dropping out using advanced machine learning techniques. The goal is to enable timely interventions and improve educational outcomes. Our methodology is adaptable across different educational systems and levels. By employing a rigorous evaluation framework, we assess model performance and use Shapley Additive exPlanations (SHAP) to identify key factors influencing predictions. The approach was tested on real data provided by the Moroccan Ministry of National Education, achieving 88% accuracy, 88% recall, 86% precision, and an AUC of 87%. These results highlight the effectiveness of the AI models in identifying at-risk students. The framework is adaptable, incorporating historical data for both short and long-term detection, offering a comprehensive solution to the persistent challenge of student dropout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07160v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ismail Elbouknify, Ismail Berrada, Loubna Mekouar, Youssef Iraqi, El Houcine Bergou, Hind Belhabib, Younes Nail, Souhail Wardi</dc:creator>
    </item>
    <item>
      <title>Skill Demand Forecasting Using Temporal Knowledge Graph Embeddings</title>
      <link>https://arxiv.org/abs/2504.07233</link>
      <description>arXiv:2504.07233v1 Announce Type: new 
Abstract: Rapid technological advancements pose a significant threat to a large portion of the global workforce, potentially leaving them behind. In today's economy, there is a stark contrast between the high demand for skilled labour and the limited employment opportunities available to those who are not adequately prepared for the digital economy. To address this critical juncture and gain a deeper and more rapid understanding of labour market dynamics, in this paper, we approach the problem of skill need forecasting as a knowledge graph (KG) completion task, specifically, temporal link prediction. We introduce our novel temporal KG constructed from online job advertisements. We then train and evaluate different temporal KG embeddings for temporal link prediction. Finally, we present predictions of demand for a selection of skills practiced by workers in the information technology industry. The code and the data are available on our GitHub repository https://github.com/team611/JobEd.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07233v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yousra Fettacha, Adil Bahaj, Mounir Ghogho</dc:creator>
    </item>
    <item>
      <title>The Gendered Algorithm: Navigating Financial Inclusion &amp; Equity in AI-facilitated Access to Credit</title>
      <link>https://arxiv.org/abs/2504.07312</link>
      <description>arXiv:2504.07312v1 Announce Type: new 
Abstract: Various companies are developing apps that collect mobile phone data and use machine learning (ML) to provide credit scores - and subsequently, opportunities to access loans - to groups left out of traditional banking. This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to answer the question: In what ways do the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and how do these choices influence gender equity in access to finance? Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth. This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered. Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower loan amounts than men, despite women being better repayers. Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility. However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, which can be further linked to profit optimization objectives. This research introduces the concept of 'encoded gender norms', whereby without explicit attention to the gendered nature of data and algorithmic design, AI technologies reproduce existing inequalities. In doing so, they reinforce gender norms as self-fulfilling prophecies. The idea that AI technology is inherently objective and, when left alone, 'fair', is seductive and misleading. In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07312v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Genevieve Smith</dc:creator>
    </item>
    <item>
      <title>Enhancements for Developing a Comprehensive AI Fairness Assessment Standard</title>
      <link>https://arxiv.org/abs/2504.07516</link>
      <description>arXiv:2504.07516v1 Announce Type: new 
Abstract: As AI systems increasingly influence critical sectors like telecommunications, finance, healthcare, and public services, ensuring fairness in decision-making is essential to prevent biased or unjust outcomes that disproportionately affect vulnerable entities or result in adverse impacts. This need is particularly pressing as the industry approaches the 6G era, where AI will drive complex functions like autonomous network management and hyper-personalized services. The TEC Standard for Fairness Assessment and Rating of AI Systems provides guidelines for evaluating fairness in AI, focusing primarily on tabular data and supervised learning models. However, as AI applications diversify, this standard requires enhancement to strengthen its impact and broaden its applicability. This paper proposes an expansion of the TEC Standard to include fairness assessments for images, unstructured text, and generative AI, including large language models, ensuring a more comprehensive approach that keeps pace with evolving AI technologies. By incorporating these dimensions, the enhanced framework will promote responsible and trustworthy AI deployment across various sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07516v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/COMSNETS63942.2025.10885551</arxiv:DOI>
      <arxiv:journal_reference>2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS), Bengaluru, India, 2025, pp. 1216-1220</arxiv:journal_reference>
      <dc:creator>Avinash Agarwal, Mayashankar Kumar, Manisha J. Nene</dc:creator>
    </item>
    <item>
      <title>Clicks, comments, consequences: Are content creators' socio-structural and platform characteristics shaping the exposure to negative sentiment, offensive language, and hate speech on YouTube?</title>
      <link>https://arxiv.org/abs/2504.07676</link>
      <description>arXiv:2504.07676v1 Announce Type: new 
Abstract: Receiving negative sentiment, offensive comments, or even hate speech is a constant part of the working experience of content creators (CCs) on YouTube - a growing occupational group in the platform economy. This study investigates how socio-structural characteristics such as the age, gender, and race of CCs but also platform features including the number of subscribers, community strength, and the channel topic shape differences in the occurrence of these phenomena on that platform. Drawing on a random sample of n=3,695 YouTube channels from German-speaking countries, we conduct a comprehensive analysis combining digital trace data, enhanced with hand-coded variables to include socio-structural characteristics in social media data. Publicly visible negative sentiment, offensive language, and hate speech are detected with machine- and deep-learning methods using N=40,000,000 comments. Contrary to existing studies our findings indicate that female content creators are confronted with less negative communication. Notably, our analysis reveals that while BIPoC, who work as CCs, receive significantly more negative sentiment, they aren't exposed to more offensive comments or hate speech. Additionally, platform characteristics also play a crucial role, as channels publishing content on conspiracy theories or politics are more frequently subject to negative communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07676v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Wei{\ss}mann, Aaron Philipp, Roland Verwiebe, Chiara Osorio Krauter, Nina-Sophie Fritsch, Claudia Buder</dc:creator>
    </item>
    <item>
      <title>Data over dialogue: Why artificial intelligence is unlikely to humanise medicine</title>
      <link>https://arxiv.org/abs/2504.07763</link>
      <description>arXiv:2504.07763v1 Announce Type: new 
Abstract: Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07763v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.26180/24955371.v1</arxiv:DOI>
      <arxiv:journal_reference>Monash University, 2024</arxiv:journal_reference>
      <dc:creator>Joshua Hatherley</dc:creator>
    </item>
    <item>
      <title>The ISC Creator: Human-Centered Design of Learning Analytics Interactive Indicator Specification Cards</title>
      <link>https://arxiv.org/abs/2504.07811</link>
      <description>arXiv:2504.07811v1 Announce Type: new 
Abstract: Emerging research on human-centered learning analytics (HCLA) has demonstrated the importance of involving diverse stakeholders in co-designing learning analytics (LA) systems. However, there is still a demand for effective and efficient methods to co-design LA dashboards and indicators. Indicator Specification Cards (ISCs) have been introduced recently to facilitate the systematic co-design of indicators by different LA stakeholders. In this paper, we strive to enhance the user experience and usefulness of the ISC-based indicator design process. Towards this end, we present the systematic design, implementation, and evaluation details of the ISC Creator, an interactive LA tool that allows low-cost and flexible design of LA indicators. Our findings demonstrate the importance of carefully considered interactivity and recommendations for orienting and supporting non-expert LA stakeholders to design custom LA indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07811v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shoeb Joarder, Mohamed Amine Chatti</dc:creator>
    </item>
    <item>
      <title>Introducing Repository Stability</title>
      <link>https://arxiv.org/abs/2504.00542</link>
      <description>arXiv:2504.00542v1 Announce Type: cross 
Abstract: Drawing from engineering systems and control theory, we introduce a framework to understand repository stability, which is a repository activity capacity to return to equilibrium following disturbances - such as a sudden influx of bug reports, key contributor departures, or a spike in feature requests. The framework quantifies stability through four indicators: commit patterns, issue resolution, pull request processing, and community engagement, measuring development consistency, problem-solving efficiency, integration effectiveness, and sustainable participation, respectively. These indicators are synthesized into a Composite Stability Index (CSI) that provides a normalized measure of repository health proxied by its stability. Finally, the framework introduces several important theoretical properties that validate its usefulness as a measure of repository health and stability. At a conceptual phase and open to debate, our work establishes mathematical criteria for evaluating repository stability and proposes new ways to understand sustainable development practices. The framework bridges control theory concepts with modern collaborative software development, providing a foundation for future empirical validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00542v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Destefanis, Silvia Bartolucci, Daniel Graziotin, Rumyana Neykova, Marco Ortu</dc:creator>
    </item>
    <item>
      <title>ChatBench: From Static Benchmarks to Human-AI Evaluation</title>
      <link>https://arxiv.org/abs/2504.07114</link>
      <description>arXiv:2504.07114v1 Announce Type: cross 
Abstract: With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07114v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serina Chang, Ashton Anderson, Jake M. Hofman</dc:creator>
    </item>
    <item>
      <title>A Replica for our Democracies? On Using Digital Twins to Enhance Deliberative Democracy</title>
      <link>https://arxiv.org/abs/2504.07138</link>
      <description>arXiv:2504.07138v1 Announce Type: cross 
Abstract: Deliberative democracy depends on carefully designed institutional frameworks, such as participant selection, facilitation methods, and decision-making mechanisms, that shape how deliberation occurs. However, determining which institutional design best suits a given context often proves difficult when relying solely on real-world observations or laboratory experiments, which can be resource intensive and hard to replicate. To address these challenges, this paper explores Digital Twin (DT) technology as a regulatory sandbox for deliberative democracy. DTs enable researchers and policymakers to run "what if" scenarios on varied deliberative designs in a controlled virtual environment by creating dynamic, computer based models that mirror real or synthetic data. This makes systematic analysis of the institutional design possible without the practical constraints of real world or lab-based settings. The paper also discusses the limitations of this approach and outlines key considerations for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07138v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Claudio Novelli, Javier Argota S\'anchez-Vaquerizo, Dirk Helbing, Antonino Rotolo, Luciano Floridi</dc:creator>
    </item>
    <item>
      <title>Youth as Advisors in Participatory Design: Situating Teens' Expertise in Everyday Algorithm Auditing with Teachers and Researchers</title>
      <link>https://arxiv.org/abs/2504.07202</link>
      <description>arXiv:2504.07202v1 Announce Type: cross 
Abstract: Research on children and youth's participation in different roles in the design of technologies is one of the core contributions in child-computer interaction studies. Building on this work, we situate youth as advisors to a group of high school computer science teacher- and researcher-designers creating learning activities in the context of emerging technologies. Specifically, we explore algorithm auditing as a potential entry point for youth and adults to critically evaluate generative AI algorithmic systems, with the goal of designing classroom lessons. Through a two-hour session where three teenagers (16-18 years) served as advisors, we (1) examine the types of expertise the teens shared and (2) identify back stage design elements that fostered their agency and voice in this advisory role. Our discussion considers opportunities and challenges in situating youth as advisors, providing recommendations for actions that researchers, facilitators, and teachers can take to make this unusual arrangement feasible and productive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07202v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3713043.3728849</arxiv:DOI>
      <dc:creator>Daniel J. Noh, Deborah A. Fields, Luis Morales-Navarro, Alexis Cabrera-Sutch, Yasmin B. Kafai, Dana\'e Metaxa</dc:creator>
    </item>
    <item>
      <title>The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses</title>
      <link>https://arxiv.org/abs/2504.07422</link>
      <description>arXiv:2504.07422v1 Announce Type: cross 
Abstract: This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07422v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Zhang, Yisong Chen</dc:creator>
    </item>
    <item>
      <title>A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure</title>
      <link>https://arxiv.org/abs/2504.07531</link>
      <description>arXiv:2504.07531v1 Announce Type: cross 
Abstract: Whether related to machine learning models' epistemic opacity, algorithmic classification systems' discriminatory automation of testimonial prejudice, the distortion of human beliefs via the 'hallucinations' of generative AI, the inclusion of the global South in global AI governance, the execution of bureaucratic violence via algorithmic systems, or located in the interaction with conversational artificial agents epistemic injustice related to AI is a growing concern. Based on a proposed general taxonomy of epistemic injustice, this paper first sketches a taxonomy of the types of epistemic injustice in the context of AI, relying on the work of scholars from the fields of philosophy of technology, political philosophy and social epistemology. Secondly, an additional perspective on epistemic injustice in the context of AI: generative hermeneutical erasure. I argue that this injustice that can come about through the application of Large Language Models (LLMs) and contend that generative AI, when being deployed outside of its Western space of conception, can have effects of conceptual erasure, particularly in the epistemic domain, followed by forms of conceptual disruption caused by a mismatch between AI system and the interlocutor in terms of conceptual frameworks. AI systems' 'view from nowhere' epistemically inferiorizes non-Western epistemologies and thereby contributes to the erosion of their epistemic particulars, gradually contributing to hermeneutical erasure. This work's relevance lies in proposal of a taxonomy that allows epistemic injustices to be mapped in the AI domain and the proposal of a novel form of AI-related epistemic injustice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07531v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Warmhold Jan Thomas Mollema</dc:creator>
    </item>
    <item>
      <title>Synthesizing High-Quality Programming Tasks with LLM-based Expert and Student Agents</title>
      <link>https://arxiv.org/abs/2504.07655</link>
      <description>arXiv:2504.07655v1 Announce Type: cross 
Abstract: Generative AI is transforming computing education by enabling the automatic generation of personalized content and feedback. We investigate its capabilities in providing high-quality programming tasks to students. Despite promising advancements in task generation, a quality gap remains between AI-generated and expert-created tasks. The AI-generated tasks may not align with target programming concepts, could be incomprehensible for students to solve, or may contain critical issues such as incorrect tests. Existing works often require interventions from human teachers for validation. We address these challenges by introducing PyTaskSyn, a novel synthesis technique that first generates a programming task and then decides whether it meets certain quality criteria to be given to students. The key idea is to break this process into multiple stages performed by expert and student agents simulated using both strong and weaker generative models. Through extensive evaluation, we show that PyTaskSyn significantly improves task quality compared to baseline techniques and showcases the importance of each specialized agent type in our validation pipeline. Additionally, we conducted user studies using our publicly available web application and show that PyTaskSyn can deliver high-quality programming tasks comparable to expert-designed ones while reducing workload and costs, and being more engaging than programming tasks that are available in online resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07655v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manh Hung Nguyen, Victor-Alexandru P\u{a}durean, Alkis Gotovos, Sebastian Tschiatschek, Adish Singla</dc:creator>
    </item>
    <item>
      <title>Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security</title>
      <link>https://arxiv.org/abs/2504.07719</link>
      <description>arXiv:2504.07719v1 Announce Type: cross 
Abstract: Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being.
  This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility.
  Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules.
  With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07719v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, Suresh Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>"i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour and cognition a conceptual metaphor or conceptual engineering?</title>
      <link>https://arxiv.org/abs/2504.07756</link>
      <description>arXiv:2504.07756v1 Announce Type: cross 
Abstract: Given the massive integration of AI technologies into our daily lives, AI-related concepts are being used to metaphorically compare AI systems with human behaviour and/or cognitive abilities like language acquisition. Rightfully, the epistemic success of these metaphorical comparisons should be debated. Against the backdrop of the conflicting positions of the 'computational' and 'meat' chauvinisms, we ask: can the conceptual constellation of the computational and AI be applied to the human domain and what does it mean to do so? What is one doing when the conceptual constellations of AI in particular are used in this fashion? Rooted in a Wittgensteinian view of concepts and language-use, we consider two possible answers and pit them against each other: either these examples are conceptual metaphors, or they are attempts at conceptual engineering. We argue that they are conceptual metaphors, but that (1) this position is unaware of its own epistemological contingency, and (2) it risks committing the ''map-territory fallacy''. Down at the conceptual foundations of computation, (3) it most importantly is a misleading 'double metaphor' because of the metaphorical connection between human psychology and computation. In response to the shortcomings of this projected conceptual organisation of AI onto the human domain, we argue that there is a semantic catch. The perspective of the conceptual metaphors shows avenues for forms of conceptual engineering. If this methodology's criteria are met, the fallacies and epistemic shortcomings related to the conceptual metaphor view can be bypassed. At its best, the cross-pollution of the human and AI conceptual domains is one that prompts us to reflect anew on how the boundaries of our current concepts serve us and how they could be approved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07756v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Warmhold Jan Thomas Mollema, Thomas Wachter</dc:creator>
    </item>
    <item>
      <title>The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation</title>
      <link>https://arxiv.org/abs/2504.07911</link>
      <description>arXiv:2504.07911v1 Announce Type: cross 
Abstract: Next-venue recommender systems are increasingly embedded in location-based services, shaping individual mobility decisions in urban environments. While their predictive accuracy has been extensively studied, less attention has been paid to their systemic impact on urban dynamics. In this work, we introduce a simulation framework to model the human-AI feedback loop underpinning next-venue recommendation, capturing how algorithmic suggestions influence individual behavior, which in turn reshapes the data used to retrain the models. Our simulations, grounded in real-world mobility data, systematically explore the effects of algorithmic adoption across a range of recommendation strategies. We find that while recommender systems consistently increase individual-level diversity in visited venues, they may simultaneously amplify collective inequality by concentrating visits on a limited subset of popular places. This divergence extends to the structure of social co-location networks, revealing broader implications for urban accessibility and spatial segregation. Our framework operationalizes the feedback loop in next-venue recommendation and offers a novel lens through which to assess the societal impact of AI-assisted mobility-providing a computational tool to anticipate future risks, evaluate regulatory interventions, and inform the design of ethic algorithmic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07911v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giovanni Mauro, Marco Minici, Luca Pappalardo</dc:creator>
    </item>
    <item>
      <title>Hiden Topics in Robotic Process Automation -- an Approach based on AI</title>
      <link>https://arxiv.org/abs/2404.05836</link>
      <description>arXiv:2404.05836v2 Announce Type: replace 
Abstract: Robotic process automation (RPA) is a software technology that in recent years has gained a lot of attention and popularity. By now, research on RPA has spread into multiple research streams. This study aims to create a science map of RPA and its aspects by revealing latent topics related to RPA, their research interest, impact, and time development. We provide a systematic framework that is helpful to develop further research into this technology. By using an unsupervised machine learning method based on Latent Dirichlet Allocation, we were able to analyse over 2000 paper abstracts. Among these, we found 100 distinct study topics, 15 of which have been included in the science map we provide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05836v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Petr Prucha, Peter Madzik, Lukas Falat</dc:creator>
    </item>
    <item>
      <title>Not someone, but something: Rethinking trust in the age of medical AI</title>
      <link>https://arxiv.org/abs/2504.05331</link>
      <description>arXiv:2504.05331v2 Announce Type: replace 
Abstract: As artificial intelligence (AI) becomes embedded in healthcare, trust in medical decision-making is changing fast. This opinion paper argues that trust in AI isn't a simple transfer from humans to machines - it's a dynamic, evolving relationship that must be built and maintained. Rather than debating whether AI belongs in medicine, this paper asks: what kind of trust must AI earn, and how? Drawing from philosophy, bioethics, and system design, it explores the key differences between human trust and machine reliability - emphasizing transparency, accountability, and alignment with the values of good care. It argues that trust in AI shouldn't be built on mimicking empathy or intuition, but on thoughtful design, responsible deployment, and clear moral responsibility. The goal is a balanced view - one that avoids blind optimism and reflexive fear. Trust in AI must be treated not as a given, but as something to be earned over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05331v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Beger</dc:creator>
    </item>
    <item>
      <title>Conversational Medical AI: Ready for Practice</title>
      <link>https://arxiv.org/abs/2411.12808</link>
      <description>arXiv:2411.12808v2 Announce Type: replace-cross 
Abstract: The shortage of doctors is creating a critical squeeze in access to medical expertise. While conversational Artificial Intelligence (AI) holds promise in addressing this problem, its safe deployment in patient-facing roles remains largely unexplored in real-world medical settings. We present the first large-scale evaluation of a physician-supervised LLM-based conversational agent in a real-world medical setting.
  Our agent, Mo, was integrated into an existing medical advice chat service. Over a three-week period, we conducted a randomized controlled experiment with 926 cases to evaluate patient experience and satisfaction. Among these, Mo handled 298 complete patient interactions, for which we report physician-assessed measures of safety and medical accuracy.
  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p &lt; 0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p &lt; 0.05) with AI-assisted conversations compared to standard care, while showing equivalent levels of trust and perceived empathy. The high opt-in rate (81% among respondents) exceeded previous benchmarks for AI acceptance in healthcare. Physician oversight ensured safety, with 95% of conversations rated as "good" or "excellent" by general practitioners experienced in operating a medical advice chat service.
  Our findings demonstrate that carefully implemented AI medical assistants can enhance patient experience while maintaining safety standards through physician supervision. This work provides empirical evidence for the feasibility of AI deployment in healthcare communication and insights into the requirements for successful integration into existing healthcare services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12808v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Liz\'ee, Pierre-Auguste Beaucot\'e, James Whitbeck, Marion Doumeingts, Ana\"el Beaugnon, Isabelle Feldhaus</dc:creator>
    </item>
    <item>
      <title>Immersive Virtual Reality Assessments of Working Memory and Psychomotor Skills: A Comparison between Immersive and Non-Immersive Assessments</title>
      <link>https://arxiv.org/abs/2503.06333</link>
      <description>arXiv:2503.06333v2 Announce Type: replace-cross 
Abstract: Objective: Immersive virtual reality (VR) enhances ecologically validity and facilitates intuitive and ergonomic hand interactions for performing neuropsychological assessments. However, its comparability to traditional computerized methods remains unclear. This study investigates the convergent validity, user experience, and usability of VR-based versus PC-based assessments of short-term and working memory, and psychomotor skills, while also examining how demographic and IT-related skills influence performance in both modalities. Methods: Sixty-six participants performed the Digit Span Task (DST), Corsi Block Task (CBT), and Deary-Liewald Reaction Time Task (DLRTT) in both VR- and PC-based formats. Participants' experience in using computers and smartphones, and playing videogames, was considered. User experience and system usability of the formats were also evaluated. Results: While performance on DST was similar across modalities, PC assessments enabled better performance on CBT and faster reaction times in DLRTT. Moderate-to-strong correlations between VR and PC versions supported convergent validity. Regression analyses revealed that performance on PC versions was influenced by age, computing, and gaming experience, whereas performance on VR versions was largely independent of these factors, except for gaming experience predicting performance on CBT backward recall. Moreover, VR assessments received higher ratings for user experience and usability than PC-based assessments. Conclusion: Immersive VR assessments provide an engaging alternative to traditional computerized methods, with minimal reliance on prior IT experience and demographic factors. This resilience to individual differences suggests that VR may offer a more equitable and accessible platform for cognitive assessment. Future research should explore the long-term reliability of VR-based assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06333v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Kourtesis, Andrea Lizarraga, Sarah E. MacPherson</dc:creator>
    </item>
    <item>
      <title>Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance</title>
      <link>https://arxiv.org/abs/2504.03699</link>
      <description>arXiv:2504.03699v2 Announce Type: replace-cross 
Abstract: In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03699v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Jung Chen, Chi-Sheng Chen, Ahmad Albarqawi</dc:creator>
    </item>
  </channel>
</rss>
