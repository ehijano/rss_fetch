<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2024 03:44:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Using Geometric Symmetries to Achieve Super-Smoothness for Cubic Powell-Sabin Splines</title>
      <link>https://arxiv.org/abs/2411.05170</link>
      <description>arXiv:2411.05170v1 Announce Type: new 
Abstract: In this paper, we investigate $C^2$ super-smoothness of the full $C^1$ cubic spline space on a Powell-Sabin refined triangulation, for which a B-spline basis can be constructed. Blossoming is used to identify the $C^2$ smoothness conditions between the functionals of the dual basis. Some of these conditions can be enforced without difficulty on general triangulations. Others are more involved but greatly simplify if the triangulation and its corresponding Powell-Sabin refinement possess certain symmetries. Furthermore, it is shown how the $C^2$ smoothness constraints can be integrated into the spline representation by reducing the set of basis functions. As an application of the super-smooth basis functions, a reduced spline space is introduced that maintains the cubic precision of the full $C^1$ spline space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05170v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Gro\v{s}elj, Hendrik Speleers</dc:creator>
    </item>
    <item>
      <title>High-order structure-preserving schemes for the regularized logarithmic Schr\"{o}dinger equation</title>
      <link>https://arxiv.org/abs/2411.05308</link>
      <description>arXiv:2411.05308v1 Announce Type: new 
Abstract: In this paper, a novel high-order, mass and energy-conserving scheme is proposed for the regularized logarithmic Schr\"{o}dinger equation(RLogSE). Based on the idea of the supplementary variable method (SVM), we firstly reformulate the original system into an equivalent form by introducing two supplementary variables, and the resulting SVM reformulation is then discretized by applying a high-order prediction-correction scheme in time and a Fourier pseudo-spectral method in space, respectively. The newly developed scheme can produce numerical solutions along which the mass and original energy are precisely conserved, as is the case with the analytical solution. Additionally, it is extremely efficient in the sense that only requires solving a constant-coefficient linear systems plus two algebraic equations, which can be efficiently solved by the Newton iteration at every time step. Numerical experiments are presented to confirm the accuracy and structure-preserving properties of the new scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05308v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Yang, Zhida Zhou, Chaolong Jiang</dc:creator>
    </item>
    <item>
      <title>$\gamma$-deepDSM for interface reconstruction: operator learning and a Learning-Automated FEM package</title>
      <link>https://arxiv.org/abs/2411.05341</link>
      <description>arXiv:2411.05341v1 Announce Type: new 
Abstract: In this work, we propose an Operator Learning (OpL) method for solving boundary value inverse problems in partial differential equations (PDEs), focusing on recovering diffusion coefficients from boundary data. Inspired by the classical Direct Sampling Method (DSM), our operator learner, named $\gamma$-deepDSM, has two key components: (1) a data-feature generation process that applies a learnable fractional Laplace-Beltrami operator to the boundary data, and (2) a convolutional neural network that operates on these data features to produce reconstructions. To facilitate this workflow, leveraging FEALPy \cite{wei2024fealpy}, a cross-platform Computed-Aided-Engineering engine, our another contribution is to develop a set of finite element method (FEM) modules fully integrated with PyTorch, called Learning-Automated FEM (LA-FEM). The new LA-FEM modules in FEALPy conveniently allows efficient parallel GPU computing, batched computation of PDEs, and auto-differentiation, without the need for additional loops, data format conversions, or device-to-device transfers. With LA-FEM, the PDE solvers with learnable parameters can be directly integrated into neural network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05341v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yangyang Zheng, Huayi Wei, Shuhao Cao, Ruchi Guo</dc:creator>
    </item>
    <item>
      <title>Lift-and-Embed Learning Methods for Solving Scalar Hyperbolic Equations with Discontinuous Solutions</title>
      <link>https://arxiv.org/abs/2411.05382</link>
      <description>arXiv:2411.05382v1 Announce Type: new 
Abstract: Unlike traditional mesh-based approximations of differential operators, machine learning methods, which exploit the automatic differentiation of neural networks, have attracted increasing attention for their potential to mitigate stability issues encountered in the numerical simulation of hyperbolic conservation laws. However, solutions to hyperbolic problems are often piecewise smooth, rendering the differential form invalid along discontinuity interfaces and limiting the effectiveness of standard learning approaches. In this work, we propose lift-and-embed learning methods for solving scalar hyperbolic equations with discontinuous solutions, which consist of (i) embedding the Rankine-Hugoniot jump condition within a higher-dimensional space through the inclusion of an augmented variable in the solution ansatz; (ii) utilizing physics-informed neural networks to manage the increased dimensionality and to address both linear and quasi-linear problems within a unified learning framework; and (iii) projecting the trained network solution back onto the original lower-dimensional plane to obtain the approximate solution. Besides, the location of discontinuity can be parametrized as extra model parameters and inferred concurrently with the training of network solution. With collocation points sampled on piecewise surfaces rather than distributed over the entire lifted space, we conduct numerical experiments on various benchmark problems to demonstrate the capability of our methods in resolving discontinuous solutions without spurious numerical smearing and oscillations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05382v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenjiang Liu, Qi Sun, Xuejun Xu</dc:creator>
    </item>
    <item>
      <title>Handling geometrical variability in nonlinear reduced order modeling through Continuous Geometry-Aware DL-ROMs</title>
      <link>https://arxiv.org/abs/2411.05486</link>
      <description>arXiv:2411.05486v1 Announce Type: new 
Abstract: Deep Learning-based Reduced Order Models (DL-ROMs) provide nowadays a well-established class of accurate surrogate models for complex physical systems described by parametrized PDEs, by nonlinearly compressing the solution manifold into a handful of latent coordinates. Until now, design and application of DL-ROMs mainly focused on physically parameterized problems. Within this work, we provide a novel extension of these architectures to problems featuring geometrical variability and parametrized domains, namely, we propose Continuous Geometry-Aware DL-ROMs (CGA-DL-ROMs). In particular, the space-continuous nature of the proposed architecture matches the need to deal with multi-resolution datasets, which are quite common in the case of geometrically parametrized problems. Moreover, CGA-DL-ROMs are endowed with a strong inductive bias that makes them aware of geometrical parametrizations, thus enhancing both the compression capability and the overall performance of the architecture. Within this work, we justify our findings through a thorough theoretical analysis, and we practically validate our claims by means of a series of numerical tests encompassing physically-and-geometrically parametrized PDEs, ranging from the unsteady Navier-Stokes equations for fluid dynamics to advection-diffusion-reaction equations for mathematical biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05486v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Brivio, Stefania Fresca, Andrea Manzoni</dc:creator>
    </item>
    <item>
      <title>Asymptotic error analysis for stochastic gradient optimization schemes with first and second order modified equations</title>
      <link>https://arxiv.org/abs/2411.05538</link>
      <description>arXiv:2411.05538v1 Announce Type: new 
Abstract: We consider a class of stochastic gradient optimization schemes. Assuming that the objective function is strongly convex, we prove weak error estimates which are uniform in time for the error between the solution of the numerical scheme, and the solutions of continuous-time modified (or high-resolution) differential equations at first and second orders, with respect to the time-step size. At first order, the modified equation is deterministic, whereas at second order the modified equation is stochastic and depends on a modified objective function. We go beyond existing results where the error estimates have been considered only on finite time intervals and were not uniform in time. This allows us to then provide a rigorous complexity analysis of the method in the large time and small time step size regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05538v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles-Edouard Br\'ehier, Marc Dambrine, Nassim En-Nebbazi</dc:creator>
    </item>
    <item>
      <title>An evolving surface finite element method for the Cahn-Hilliard equation with a logarithmic potential</title>
      <link>https://arxiv.org/abs/2411.05650</link>
      <description>arXiv:2411.05650v1 Announce Type: new 
Abstract: In this paper we study semi-discrete and fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation with a logarithmic potential. Specifically we consider linear finite elements discretising space and backward Euler time discretisation. Our analysis relies on a specific geometric assumption on the evolution of the surface. Our main results are $L^2_{H^1}$ error bounds for both the semi-discrete and fully discrete schemes, and we provide some numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05650v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles M. Elliott, Thomas Sales</dc:creator>
    </item>
    <item>
      <title>The Helmholtz Dirichlet and Neumann problems on piecewise smooth open curves</title>
      <link>https://arxiv.org/abs/2411.05761</link>
      <description>arXiv:2411.05761v1 Announce Type: new 
Abstract: A numerical scheme is presented for solving the Helmholtz equation with Dirichlet or Neumann boundary conditions on piecewise smooth open curves, where the curves may have corners and multiple junctions. Existing integral equation methods for smooth open curves rely on analyzing the exact singularities of the density at endpoints for associated integral operators, explicitly extracting these singularities from the densities in the formulation, and using global quadrature to discretize the boundary integral equation. Extending these methods to handle curves with corners and multiple junctions is challenging because the singularity analysis becomes much more complex, and constructing high-order quadrature for discretizing layer potentials with singular and hypersingular kernels and singular densities is nontrivial. The proposed scheme is built upon the following two observations. First, the single-layer potential operator and the normal derivative of the double-layer potential operator serve as effective preconditioners for each other locally. Second, the recursively compressed inverse preconditioning (RCIP) method can be extended to address "implicit" second-kind integral equations. The scheme is high-order, adaptive, and capable of handling corners and multiple junctions without prior knowledge of the density singularity. It is also compatible with fast algorithms, such as the fast multipole method. The performance of the scheme is illustrated with several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05761v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Helsing, Shidong Jiang</dc:creator>
    </item>
    <item>
      <title>Enforcing asymptotic behavior with DNNs for approximation and regression in finance</title>
      <link>https://arxiv.org/abs/2411.05257</link>
      <description>arXiv:2411.05257v1 Announce Type: cross 
Abstract: We propose a simple methodology to approximate functions with given asymptotic behavior by specifically constructed terms and an unconstrained deep neural network (DNN).
  The methodology we describe extends to various asymptotic behaviors and multiple dimensions and is easy to implement. In this work we demonstrate it for linear asymptotic behavior in one-dimensional examples. We apply it to function approximation and regression problems where we measure approximation of only function values (``Vanilla Machine Learning''-VML) or also approximation of function and derivative values (``Differential Machine Learning''-DML) on several examples. We see that enforcing given asymptotic behavior leads to better approximation and faster convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05257v1</guid>
      <category>q-fin.CP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-fin.PR</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hardik Routray, Bernhard Hientzsch</dc:creator>
    </item>
    <item>
      <title>Can Efficient Fourier-Transform Techniques Favorably Impact on Broadband Computational Electromagnetism?</title>
      <link>https://arxiv.org/abs/2411.05626</link>
      <description>arXiv:2411.05626v1 Announce Type: cross 
Abstract: In view of recently demonstrated joint use of novel Fourier-transform techniques and effective high-accuracy frequency domain solvers related to the Method of Moments, it is argued that a set of transformative innovations could be developed for the effective, accurate and efficient simulation of problems of wave propagation and scattering of broadband, time-dependent wavefields. This contribution aims to convey the character of these methods and to highlight their applicability in computational modeling of electromagnetic configurations across various fields of science and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05626v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas G. Anderson, Mark Lyon, Tao Yin, Oscar P. Bruno</dc:creator>
    </item>
    <item>
      <title>The Delaunay Density Diagnostic</title>
      <link>https://arxiv.org/abs/2203.05685</link>
      <description>arXiv:2203.05685v2 Announce Type: replace 
Abstract: Accurate approximation of a real-valued function depends on two aspects of the available data: the density of inputs within the domain of interest and the variation of the outputs over that domain. There are few methods for assessing whether the density of inputs is \textit{sufficient} to identify the relevant variations in outputs -- i.e., the ``geometric scale'' of the function -- despite the fact that sampling density is closely tied to the success or failure of an approximation method. In this paper, we introduce a general purpose, computational approach to detecting the geometric scale of real-valued functions over a fixed domain using a deterministic interpolation technique from computational geometry. The algorithm is intended to work on scalar data in moderate dimensions (2-10). Our algorithm is based on the observation that a sequence of piecewise linear interpolants will converge to a continuous function at a quadratic rate (in $L^2$ norm) if and only if the data are sampled densely enough to distinguish the feature from noise (assuming sufficiently regular sampling). We present numerical experiments demonstrating how our method can identify feature scale, estimate uncertainty in feature scale, and assess the sampling density for fixed (i.e., static) datasets of input-output pairs. We include analytical results in support of our numerical findings and have released lightweight code that can be adapted for use in a variety of data science settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05685v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3700134</arxiv:DOI>
      <dc:creator>Andrew Gillette, Eugene Kur</dc:creator>
    </item>
    <item>
      <title>Error analysis for a finite element approximation of the steady $p(\cdot)$-Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2311.00534</link>
      <description>arXiv:2311.00534v2 Announce Type: replace 
Abstract: In this paper, we examine a finite element approximation of the steady $p(\cdot)$-Navier-Stokes equations ($p(\cdot)$ is variable dependent) and prove orders of convergence by assuming natural fractional regularity assumptions on the velocity vector field and the kinematic pressure. Compared to previous results, we treat the convective term and employ a more practicable discretization of the power-law index $p(\cdot)$. Numerical experiments confirm the quasi-optimality of the $\textit{a priori}$ error estimates (for the velocity) with respect to fractional regularity assumptions on the velocity vector field and the kinematic pressure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00534v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luigi C. Berselli, Alex Kaltenbach</dc:creator>
    </item>
    <item>
      <title>Anderson Accelerated Gauss-Newton-guided deep learning for nonlinear inverse problems with Application to Electrical Impedance Tomography</title>
      <link>https://arxiv.org/abs/2312.12693</link>
      <description>arXiv:2312.12693v2 Announce Type: replace 
Abstract: Physics-guided deep learning is an important prevalent research topic in scientific machine learning, which has tremendous potential in various complex applications including science and engineering. In these applications, data is expensive to acquire and high accuracy is required for making decisions. In this work, we introduce an efficient physics-guided deep learning framework for the variational modeling of nonlinear inverse problems, which is then applied to solve an electrical impedance tomography (EIT) inverse problem. The framework is achieved by unrolling the proposed Anderson accelerated Gauss-Newton (GNAA) algorithm into an end-to-end deep learning method. Firstly, we show the convergence of the GNAA algorithm in both cases: Anderson depth is equal to one and Anderson depth is greater than one. Then, we propose three types of strategies by combining the complementary strengths of GNAA and deep learning: GNAA of learned regularization (GNAA-LRNet), where the singular values of the regularization matrix are learned by a deep neural network; GNAA of learned proximity (GNAA-LPNet), where the regularization proximal operator is learned by using a deep neural network; GNAA of plug-and-play method (GNAA-PnPNet) where the regularization proximal operator is replaced by a pre-trained deep denoisers. Lastly, we present some numerical experiments to illustrate that the proposed approaches greatly improve the convergence rate and the quality of inverse solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12693v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingping Zhou, Guixian Xu, Zhexin Wen, Hongqiao Wang</dc:creator>
    </item>
    <item>
      <title>H-CMRH: a novel inner product free hybrid Krylov method for large-scale inverse problems</title>
      <link>https://arxiv.org/abs/2401.06918</link>
      <description>arXiv:2401.06918v2 Announce Type: replace 
Abstract: This study investigates the iterative regularization properties of two Krylov methods for solving large-scale ill-posed problems: the changing minimal residual Hessenberg method (CMRH) and a novel hybrid variant called the hybrid changing minimal residual Hessenberg method (H-CMRH). Both methods share the advantages of avoiding inner products, making them efficient and highly parallelizable, and particularly suited for implementations that exploit randomization and mixed precision arithmetic. Theoretical results and extensive numerical experiments suggest that H-CMRH exhibits comparable performance to the established hybrid GMRES method in terms of stabilizing semiconvergence, but H-CMRH has does not require any inner products, and requires less work and storage per iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06918v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariana N. Brown, Malena Sabat\'e Landman, James G. Nagy</dc:creator>
    </item>
    <item>
      <title>High order multiscale methods for advection-diffusion equation in highly oscillatory regimes: application to surfactant diffusion and generalization to arbitrary domains</title>
      <link>https://arxiv.org/abs/2401.12226</link>
      <description>arXiv:2401.12226v2 Announce Type: replace 
Abstract: In this paper, we propose high order numerical methods to solve a 2D advection diffusion equation, in the highly oscillatory regime. We use an integrator strategy that allows the construction of arbitrary high-order schemes {leading} to an accurate approximation of the solution without any time step-size restriction. This paper focuses on the multiscale challenges {in time} of the problem, that come from the velocity, an $\varepsilon-$periodic function, whose expression is explicitly known. $\varepsilon$-uniform third order in time numerical approximations are obtained. For the space discretization, this strategy is combined with high order finite difference schemes. Numerical experiments show that the proposed methods {achieve} the expected order of accuracy, and it is validated by several tests across diverse domains and boundary conditions. The novelty of the paper consists of introducing a numerical scheme that is high order accurate in space and time, with a particular attention to the dependency on a small parameter in the time scale. The high order in space is obtained enlarging the interpolation stencil already established in [44], and further refined in [46], with a special emphasis on the squared boundary, especially when a Dirichlet condition is assigned. In such case, we compute an \textit{ad hoc} Taylor expansion of the solution to ensure that there is no degradation of the accuracy order at the boundary. On the other hand, the high accuracy in time is obtained extending the work proposed in [19]. The combination of high-order accuracy in both space and time is particularly significant due to the presence of two small parameters-$\delta$ and $\varepsilon$-in space and time, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12226v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clarissa Astuto</dc:creator>
    </item>
    <item>
      <title>A nodal ghost method based on variational formulation and regular square grid for elliptic problems on arbitrary domains in two space dimensions</title>
      <link>https://arxiv.org/abs/2402.04048</link>
      <description>arXiv:2402.04048v2 Announce Type: replace 
Abstract: This paper focuses on the numerical solution of elliptic partial differential equations (PDEs) with Dirichlet and mixed boundary conditions, specifically addressing the challenges arising from irregular domains. Both finite element method (FEM) and finite difference method (FDM), face difficulties in dealing with arbitrary domains. The paper introduces a novel nodal symmetric ghost {method based on a variational formulation}, which combines the advantages of FEM and FDM. The method employs bilinear finite elements on a structured mesh and provides a detailed implementation description. A rigorous a priori convergence rate analysis is also presented. The convergence rates are validated with many numerical experiments, in both one and two space dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04048v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clarissa Astuto, Daniele Boffi, Giovanni Russo, Umberto Zerbinati</dc:creator>
    </item>
    <item>
      <title>Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation</title>
      <link>https://arxiv.org/abs/2403.18088</link>
      <description>arXiv:2403.18088v2 Announce Type: replace 
Abstract: We propose a new neural network based large eddy simulation framework for the incompressible Navier-Stokes equations based on the paradigm "discretize first, filter and close next". This leads to full model-data consistency and allows for employing neural closure models in the same environment as where they have been trained. Since the LES discretization error is included in the learning process, the closure models can learn to account for the discretization. Furthermore, we employ a divergence-consistent discrete filter defined through face-averaging and provide novel theoretical and numerical filter analysis. This filter preserves the discrete divergence-free constraint by construction, unlike general discrete filters such as volume-averaging filters. We show that using a divergence-consistent LES formulation coupled with a convolutional neural closure model produces stable and accurate results for both a-priori and a-posteriori training, while a general (divergence-inconsistent) LES model requires a-posteriori training or other stability-enforcing measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18088v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syver D{\o}ving Agdestein, Benjamin Sanderse</dc:creator>
    </item>
    <item>
      <title>Schr\"odingerisation based computationally stable algorithms for ill-posed problems in partial differential equations</title>
      <link>https://arxiv.org/abs/2403.19123</link>
      <description>arXiv:2403.19123v4 Announce Type: replace 
Abstract: We introduce a simple and stable computational method for ill-posed partial differential equation (PDE) problems. The method is based on Schr\"odingerization, introduced in [S. Jin, N. Liu and Y. Yu, arXiv:2212.13969][S. Jin, N. Liu and Y. Yu, Phys. Rev. A, 108 (2023), 032603], which maps all linear PDEs into Schr\"odinger-type equations in one higher dimension, for quantum simulations of these PDEs. Although the original problem is ill-posed, the Schr\"odingerized equations are Hamiltonian systems and time-reversible, allowing stable computation both forward and backward in time. The original variable can be recovered by data from suitably chosen domain in the extended dimension. We will use the backward heat equation and the linear convection equation with imaginary wave speed as examples. Error analysis of these algorithms are conducted and verified numerically. The methods are applicable to both classical and quantum computers, and we also lay out quantum algorithms for these methods. Moreover, we introduce a smooth initialization for the Schr\"odingerized equation which will lead to essentially spectral accuracy for the approximation in the extended space, if a spectral method is used. Consequently, the extra qubits needed due to the extra dimension, if a qubit based quantum algorithm is used, for both well-posed and ill-posed problems, becomes almost $\log\log {1/\varepsilon}$ where $\varepsilon$ is the desired precision. This optimizes the complexity of the Schr\"odingerization based quantum algorithms for any non-unitary dynamical system introduced in [S. Jin, N. Liu and Y. Yu, arXiv:2212.13969][S. Jin, N. Liu and Y. Yu, Phys. Rev. A, 108 (2023), 032603].</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19123v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shi Jin, Nana Liu, Chuwen Ma</dc:creator>
    </item>
    <item>
      <title>A numerical view on {\alpha}-dissipative solutions of the Hunter-Saxton equation</title>
      <link>https://arxiv.org/abs/2404.11174</link>
      <description>arXiv:2404.11174v2 Announce Type: replace 
Abstract: We propose a new numerical method for $\alpha$-dissipative solutions of the Hunter-Saxton equation, where $\alpha$ belongs to $W^{1, \infty}(\mathbb{R}, [0, 1))$. The method combines a projection operator with a generalized method of characteristics and an iteration scheme, which is based on enforcing minimal time steps whenever breaking times cluster. Numerical examples illustrate that these minimal time steps increase the efficiency of the algorithm substantially. Moreover, convergence of the wave profile is shown in $C([0, T], L^{\infty}(\mathbb{R}))$ for any finite $T \geq 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11174v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Christiansen, Katrin Grunert</dc:creator>
    </item>
    <item>
      <title>JKO for Landau: a variational particle method for homogeneous Landau equation</title>
      <link>https://arxiv.org/abs/2409.12296</link>
      <description>arXiv:2409.12296v2 Announce Type: replace 
Abstract: Inspired by the gradient flow viewpoint of the Landau equation and corresponding dynamic formulation of the Landau metric in [arXiv:2007.08591], we develop a novel implicit particle method for the Landau equation in the framework of the JKO scheme. We first reformulate the Landau metric in a computationally friendly form, and then translate it into the Lagrangian viewpoint using the flow map. A key observation is that, while the flow map evolves according to a rather complicated integral equation, the unknown component is merely a score function of the corresponding density plus an additional term in the null space of the collision kernel. This insight guides us in designing and training the neural network for the flow map. Additionally, the objective function is in a double summation form, making it highly suitable for stochastic methods. Consequently, we design a tailored version of stochastic gradient descent that maintains particle interactions and significantly reduces the computational complexity. Compared to other deterministic particle methods, the proposed method enjoys exact entropy dissipation and unconditional stability, therefore making it suitable for large-scale plasma simulations over extended time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12296v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Huang, Li Wang</dc:creator>
    </item>
    <item>
      <title>Long-time Integration of Nonlinear Wave Equations with Neural Operators</title>
      <link>https://arxiv.org/abs/2410.15617</link>
      <description>arXiv:2410.15617v2 Announce Type: replace 
Abstract: Neural operators have shown promise in solving many types of Partial Differential Equations (PDEs). They are significantly faster compared to traditional numerical solvers once they have been trained with a certain amount of observed data. However, their numerical performance in solving time-dependent PDEs, particularly in long-time prediction of dynamic systems, still needs improvement. In this paper, we focus on solving the long-time integration of nonlinear wave equations via neural operators by replacing the initial condition with the prediction in a recurrent manner. Given limited observed temporal trajectory data, we utilize some intrinsic features of these nonlinear wave equations, such as conservation laws and well-posedness, to improve the algorithm design and reduce accumulated error. Our numerical experiments examine these improvements in the Korteweg-de Vries (KdV) equation, the sine-Gordon equation, and the Klein-Gordon wave equation on the irregular domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15617v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanhang Lei, Zhen Lei, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Control of probability flow in Markov chain Monte Carlo -- Nonreversibility and lifting</title>
      <link>https://arxiv.org/abs/1207.0258</link>
      <description>arXiv:1207.0258v3 Announce Type: replace-cross 
Abstract: The Markov chain Monte Carlo (MCMC) method is widely used in various fields as a powerful numerical integration technique for systems with many degrees of freedom. In MCMC methods, probabilistic state transitions can be considered as a random walk in state space, and random walks allow for sampling from complex distributions. However, paradoxically, it is necessary to carefully suppress the randomness of the random walk to improve computational efficiency. By breaking detailed balance, we can create a probability flow in the state space and perform more efficient sampling along this flow. Motivated by this idea, practical and efficient nonreversible MCMC methods have been developed over the past ten years. In particular, the lifting technique, which introduces probability flows in an extended state space, has been applied to various systems and has proven more efficient than conventional reversible updates. We review and discuss several practical approaches to implementing nonreversible MCMC methods, including the shift method in the cumulative distribution and the directed-worm algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:1207.0258v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0233858</arxiv:DOI>
      <arxiv:journal_reference>J. Chem. Phys. 161, 174107 (2024)</arxiv:journal_reference>
      <dc:creator>Hidemaro Suwa, Synge Todo</dc:creator>
    </item>
    <item>
      <title>A deep solver for BSDEs with jumps</title>
      <link>https://arxiv.org/abs/2211.04349</link>
      <description>arXiv:2211.04349v2 Announce Type: replace-cross 
Abstract: The aim of this work is to propose an extension of the deep solver by Han, Jentzen, E (2018) to the case of forward backward stochastic differential equations (FBSDEs) with jumps. As in the aforementioned solver, starting from a discretized version of the FBSDE and parametrizing the (high dimensional) control processes by means of a family of artificial neural networks (ANNs), the FBSDE is viewed as a model-based reinforcement learning problem and the ANN parameters are fitted so as to minimize a prescribed loss function. We take into account both finite and infinite jump activity by introducing, in the latter case, an approximation with finitely many jumps of the forward process. We successfully apply our algorithm to option pricing problems in low and high dimension and discuss the applicability in the context of counterparty credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04349v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>q-fin.PR</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristoffer Andersson, Alessandro Gnoatto, Marco Patacca, Athena Picarelli</dc:creator>
    </item>
    <item>
      <title>Maximum a posteriori testing in statistical inverse problems</title>
      <link>https://arxiv.org/abs/2402.00686</link>
      <description>arXiv:2402.00686v3 Announce Type: replace-cross 
Abstract: This paper is concerned with a Bayesian approach to testing hypotheses in statistical inverse problems. Based on the posterior distribution $\Pi \left(\cdot |Y = y\right)$, we want to infer whether a feature $\langle\varphi, u^\dagger\rangle$ of the unknown quantity of interest $u^\dagger$ is positive. This can be done by the so-called maximum a posteriori test. We provide a frequentistic analysis of this test's properties such as level and power, and prove that it is a regularized test in the sense of Kretschmann et al. (2024). Furthermore we provide lower bounds for its power under classical spectral source conditions in case of Gaussian priors. Numerical simulations illustrate its superior performance both in moderately and severely ill-posed situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00686v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remo Kretschmann, Frank Werner</dc:creator>
    </item>
    <item>
      <title>Learning Partial Differential Equations with Deep Parallel Neural Operator</title>
      <link>https://arxiv.org/abs/2409.19976</link>
      <description>arXiv:2409.19976v2 Announce Type: replace-cross 
Abstract: In recent years, Solving partial differential equations has shifted the focus of traditional neural network studies from finite-dimensional Euclidean spaces to generalized functional spaces in research. A novel methodology is to learn an operator as a means of approximating the mapping between outputs. Currently, researchers have proposed a variety of operator architectures. Nevertheless, the majority of these architectures adopt an iterative update architecture, whereby a single operator is learned from the same function space. In practical physical science problems, the numerical solutions of partial differential equations are complex, and a serial single operator is unable to accurately approximate the intricate mapping between input and output. So, We propose a deep parallel operator model (DPNO) for efficiently and accurately solving partial differential equations. DPNO employs convolutional neural networks to extract local features and map data into distinct latent spaces. Designing a parallel block of double Fourier neural operators to solve the iterative error problem. DPNO approximates complex mappings between inputs and outputs by learning multiple operators in different potential spaces in parallel blocks. DPNO achieved the best performance on five of them, with an average improvement of 10.5\%, and ranked second on one dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19976v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qinglong Ma, Peizhi Zhao, Sen Wang, Tao Song</dc:creator>
    </item>
  </channel>
</rss>
