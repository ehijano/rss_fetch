<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 06:48:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>mNARX+: A surrogate model for complex dynamical systems using manifold-NARX and automatic feature selection</title>
      <link>https://arxiv.org/abs/2507.13301</link>
      <description>arXiv:2507.13301v1 Announce Type: new 
Abstract: We propose an automatic approach for manifold nonlinear autoregressive with exogenous inputs (mNARX) modeling that leverages the feature-based structure of functional-NARX (F-NARX) modeling. This novel approach, termed mNARX+, preserves the key strength of the mNARX framework, which is its expressivity allowing it to model complex dynamical systems, while simultaneously addressing a key limitation: the heavy reliance on domain expertise to identify relevant auxiliary quantities and their causal ordering. Our method employs a data-driven, recursive algorithm that automates the construction of the mNARX model sequence. It operates by sequentially selecting temporal features based on their correlation with the model prediction residuals, thereby automatically identifying the most critical auxiliary quantities and the order in which they should be modeled. This procedure significantly reduces the need for prior system knowledge. We demonstrate the effectiveness of the mNARX+ algorithm on two case studies: a Bouc-Wen oscillator with strong hysteresis and a complex aero-servo-elastic wind turbine simulator. The results show that the algorithm provides a systematic, data-driven method for creating accurate and stable surrogate models for complex dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13301v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Sch\"ar, S. Marelli, B. Sudret</dc:creator>
    </item>
    <item>
      <title>Differentially Private Conformal Prediction via Quantile Binary Search</title>
      <link>https://arxiv.org/abs/2507.12497</link>
      <description>arXiv:2507.12497v1 Announce Type: cross 
Abstract: Most Differentially Private (DP) approaches focus on limiting privacy leakage from learners based on the data that they are trained on, there are fewer approaches that consider leakage when procedures involve a calibration dataset which is common in uncertainty quantification methods such as Conformal Prediction (CP). Since there is a limited amount of approaches in this direction, in this work we deliver a general DP approach for CP that we call Private Conformity via Quantile Search (P-COQS). The proposed approach adapts an existing randomized binary search algorithm for computing DP quantiles in the calibration phase of CP thereby guaranteeing privacy of the consequent prediction sets. This however comes at a price of slightly under-covering with respect to the desired $(1 - \alpha)$-level when using finite-sample calibration sets (although broad empirical results show that the P-COQS generally targets the required level in the considered cases). Confirming properties of the adapted algorithm and quantifying the approximate coverage guarantees of the consequent CP, we conduct extensive experiments to examine the effects of privacy noise, sample size and significance level on the performance of our approach compared to existing alternatives. In addition, we empirically evaluate our approach on several benchmark datasets, including CIFAR-10, ImageNet and CoronaHack. Our results suggest that the proposed method is robust to privacy noise and performs favorably with respect to the current DP alternative in terms of empirical coverage, efficiency, and informativeness. Specifically, the results indicate that P-COQS produces smaller conformal prediction sets while simultaneously targeting the desired coverage and privacy guarantees in all these experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12497v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ogonnaya M. Romanus, Roberto Molinari</dc:creator>
    </item>
    <item>
      <title>On Accelerated Mixing of the No-U-turn Sampler</title>
      <link>https://arxiv.org/abs/2507.13259</link>
      <description>arXiv:2507.13259v1 Announce Type: cross 
Abstract: Recent progress on the theory of variational hypocoercivity established that Randomized Hamiltonian Monte Carlo -- at criticality -- can achieve pronounced acceleration in its convergence and hence sampling performance over diffusive dynamics. Manual critical tuning being unfeasible in practice has motivated automated algorithmic solutions, notably the No-U-turn Sampler. Beyond its empirical success, a rigorous study of this method's ability to achieve accelerated convergence has been missing. We initiate this investigation combining a concentration of measure approach to examine the automatic tuning mechanism with a coupling based mixing analysis for Hamiltonian Monte Carlo. In certain Gaussian target distributions, this yields a precise characterization of the sampler's behavior resulting, in particular, in rigorous mixing guarantees describing the algorithm's ability and limitations in achieving accelerated convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13259v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Oberd\"orster</dc:creator>
    </item>
    <item>
      <title>Scalable Bernoulli factories for Bayesian inference with intractable likelihoods</title>
      <link>https://arxiv.org/abs/2505.05438</link>
      <description>arXiv:2505.05438v2 Announce Type: replace 
Abstract: Bernoulli factory MCMC algorithms implement accept-reject Markov chains without explicit computation of acceptance probabilities, and are used to target posterior distributions associated with intractable likelihood models. Intractable likelihoods naturally arise in continuous-time models and mixture distributions, or from the marginalisation of a tractable augmented model. Bernoulli factory MCMC algorithms often mix better than alternatives that target a tractable augmented posterior. However, for a likelihood that factorizes over observations, we show that their computational performance typically deteriorates exponentially with data size. To address this, we propose a simple divide-and-conquer Bernoulli factory MCMC algorithm and prove that it has polynomial complexity of degree between 1 and 2, with the exact degree depending on the existence of efficient unbiased estimators of the intractable likelihood ratio. We demonstrate the effectiveness of our approach with applications to Bayesian inference in two intractable likelihood models, and observe respective polynomial cost of degree 1.2 and 1 in the data size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05438v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'ee Stumpf-F\'etizon, Fl\'avio B. Gon\c{c}alves</dc:creator>
    </item>
    <item>
      <title>Sampling-Based Estimation of Jaccard Containment and Similarity</title>
      <link>https://arxiv.org/abs/2507.10019</link>
      <description>arXiv:2507.10019v2 Announce Type: replace 
Abstract: This paper addresses the problem of estimating the containment and similarity between two sets using only random samples from each set, without relying on sketches or full data access. The study introduces a binomial model for predicting the overlap between samples, demonstrating that it is both accurate and practical when sample sizes are small compared to the original sets. The paper compares this model to previous approaches and shows that it provides better estimates under the considered conditions. It also analyzes the statistical properties of the estimator, including error bounds and sample size requirements needed to achieve a desired level of accuracy and confidence. The framework is extended to estimate set similarity, and the paper provides guidance for applying these methods in large scale data systems where only partial or sampled data is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10019v2</guid>
      <category>stat.CO</category>
      <category>cs.DB</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranav Joshi</dc:creator>
    </item>
    <item>
      <title>Branching Stein Variational Gradient Descent for sampling multimodal distributions</title>
      <link>https://arxiv.org/abs/2506.13916</link>
      <description>arXiv:2506.13916v2 Announce Type: replace-cross 
Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13916v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isa\'ias Ba\~nales, Arturo Jaramillo, Joshu\'e Hel\'i Ricalde-Guerrero</dc:creator>
    </item>
  </channel>
</rss>
