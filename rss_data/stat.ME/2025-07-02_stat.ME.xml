<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jul 2025 01:33:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hybrid methods for missing categorical covariates in Cox model</title>
      <link>https://arxiv.org/abs/2507.00151</link>
      <description>arXiv:2507.00151v1 Announce Type: new 
Abstract: Survival analysis aims to explore the relationship between covariates and the time until the occurrence of an event. The Cox proportional hazards model is commonly used for right-censored data, but it is not strictly limited to this type of data. However, the presence of missing values among the covariates, particularly categorical ones, can compromise the validity of the estimates. To address this issue, various classical methods for handling missing data have been proposed within the Cox model framework, including parametric imputation, nonparametric imputation, and semiparametric methods. It is well-documented that none of these methods is universally ideal or optimal, making the choice of the preferred method often complex and challenging. To overcome these limitations, we propose hybrid methods that combine the advantages of classical methods to enhance the robustness of the analyses. Through a simulation study, we demonstrate that these hybrid methods provide increased flexibility, simplified implementation, and improved robustness compared to classical methods. The results from the simulation study highlight that hybrid methods offer increased flexibility, simplified implementation, and greater robustness compared to classical approaches. In particular, they allow for a reduction in estimation bias; however, this improvement comes at the cost of reduced precision, due to increased variability. This observation reflects a well-known methodological trade-off between bias and variance, inherent to the combination of complementary imputation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00151v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdoulaye Dioni, Lynne Moore, Aida Eslami</dc:creator>
    </item>
    <item>
      <title>Penalized FCI for Causal Structure Learning in a Sparse DAG for Biomarker Discovery in Parkinson's Disease</title>
      <link>https://arxiv.org/abs/2507.00173</link>
      <description>arXiv:2507.00173v1 Announce Type: new 
Abstract: Parkinson's disease (PD) is a progressive neurodegenerative disorder that lacks reliable early-stage biomarkers for diagnosis, prognosis, and therapeutic monitoring. While cerebrospinal fluid (CSF) biomarkers, such as alpha-synuclein seed amplification assays (alphaSyn-SAA), offer diagnostic potential, their clinical utility is limited by invasiveness and incomplete specificity. Plasma biomarkers provide a minimally invasive alternative, but their mechanistic role in PD remains unclear. A major challenge is distinguishing whether plasma biomarkers causally reflect primary neurodegenerative processes or are downstream consequences of disease progression. To address this, we leverage the Parkinson's Progression Markers Initiative (PPMI) Project 9000, containing 2,924 plasma and CSF biomarkers, to systematically infer causal relationships with disease status. However, only a sparse subset of these biomarkers and their interconnections are actually relevant for the disease. Existing causal discovery algorithms, such as Fast Causal Inference (FCI) and its variants, struggle with the high dimensionality of biomarker datasets under sparsity, limiting their scalability. We propose Penalized Fast Causal Inference (PFCI), a novel approach that incorporates sparsity constraints to efficiently infer causal structures in large-scale biological datasets. By applying PFCI to PPMI data, we aim to identify biomarkers that are causally linked to PD pathology, enabling early diagnosis and patient stratification. Our findings will facilitate biomarker-driven clinical trials and contribute to the development of neuroprotective therapies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Dhrubajyoti Ghosh, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Optimal Targeting in Dynamic Systems</title>
      <link>https://arxiv.org/abs/2507.00312</link>
      <description>arXiv:2507.00312v1 Announce Type: new 
Abstract: Modern treatment targeting methods often rely on estimating the conditional average treatment effect (CATE) using machine learning tools. While effective in identifying who benefits from treatment on the individual level, these approaches typically overlook system-level dynamics that may arise when treatments induce strain on shared capacity. We study the problem of targeting in Markovian systems, where treatment decisions must be made one at a time as units arrive, and early decisions can impact later outcomes through delayed or limited access to resources. We show that optimal policies in such settings compare CATE-like quantities to state-specific thresholds, where each threshold reflects the expected cumulative impact on the system of treating an additional individual in the given state. We propose an algorithm that augments standard CATE estimation with off-policy evaluation techniques to estimate these thresholds from observational data. Theoretical results establish consistency and convergence guarantees, and empirical studies demonstrate that our method improves long-run outcomes considerably relative to individual-level CATE targeting rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00312v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Hu, Shuangning Li, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Clustering-based accelerometer measures to model relationships between physical activity and key outcomes</title>
      <link>https://arxiv.org/abs/2507.00484</link>
      <description>arXiv:2507.00484v1 Announce Type: new 
Abstract: Accelerometers produce enormous amounts of data. Research that incorporates such data often involves a derived summary metric to describe physical activity. Traditional metrics have often ignored the temporal nature of the data. We build on previous work that applies unsupervised machine learning techniques to describe physical activity patterns over time. Specifically, we evaluate a summary measure of accelerometer data derived from unsupervised clustering in a regression framework through comparisons with other traditional measures: duration of time spent in different activity intensity states, Time Active Mean (TAM), Time Active Variability (TAV), Activity Intensity Mean (AIM), and Activity Intensity Variability (AIV) using data from 268 children participating in the Stanford GOALS trial. The proportion of variation explained by the new measure was comparable to that of traditional measures across regressions of three pre-specified clinical outcomes (waist circumference, fasting insulin levels, and fasting triglyceride levels). For example, cluster membership explained 25%, 11%, and 6% of the variation in waist circumference, fasting insulin levels, and fasting triglyceride levels whereas TAM explained 25%, 10%, and 6% for these same outcomes. Importantly, however, there are challenges when regressing an outcome on a variable derived from unsupervised machine learning techniques, particularly regarding replicability. This includes the processing involved in deriving the variable as well as the machine learning approach itself. While these remain open topics to resolve, our findings demonstrate the promise of a new summary measure that enables addressing questions involving a temporal component that other traditional summary metrics do not reflect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00484v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyatt Moore IV, Thomas N. Robinson, Alexandria Jensen, Fatma Gunturkun, K. Farish Haydel, Kristopher I Kapphahn, Manisha Desai</dc:creator>
    </item>
    <item>
      <title>Differential Distance Correlation and Its Applications</title>
      <link>https://arxiv.org/abs/2507.00524</link>
      <description>arXiv:2507.00524v1 Announce Type: new 
Abstract: In this paper, we propose a novel coefficient, named differential distance correlation, to measure the strength of dependence between a random variable $ Y \in \mathbb {R} $ and a random vector $ X \in \mathbb {R}^{p} $. The coefficient has a concise expression and is invariant to arbitrary orthogonal transformations of the random vector. Moreover, the coefficient is a strongly consistent estimator of a simple and interpretable dependent measure, which is 0 if and only if $ X $ and $ Y $ are independent and equal to 1 if and only if $ Y $ determines $ X $ almost surely. Furthermore, the coefficient exhibits asymptotic normality with a simple variance under the independent hypothesis, facilitating fast and accurate estimation of p-value for testing independence. Two simulated experiments demonstrate that our proposed coefficient outperforms some dependence measures in identifying relationships with higher oscillatory behavior. We also apply our method to analyze a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00524v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Liu, Pengjian Shang</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of the causal reference-based model for missing data in clinical trials</title>
      <link>https://arxiv.org/abs/2507.00680</link>
      <description>arXiv:2507.00680v2 Announce Type: new 
Abstract: The statistical analysis of clinical trials is often complicated by missing data. Patients sometimes experience intercurrent events (ICEs), which usually (although not always) lead to missing subsequent outcome measurements for such individuals. The reference-based imputation methods were proposed by Carpenter et al. (2013) and have been commonly adopted for handling missing data due to ICEs when estimating treatment policy strategy estimands. Conventionally, the variance for reference-based estimators was obtained using Rubin's rules. However, Rubin's rules variance estimator is biased compared to the repeated sampling variance of the point estimator, due to uncongeniality. Repeated sampling variance estimators were proposed as an alternative to variance estimation for reference-based estimators. However, these have the property that they decrease as the proportion of ICEs increases. White et al. (2019) introduced a causal model incorporating the concept of a 'maintained treatment effect' following the occurrence of ICEs and showed that this causal model included common reference-based estimators as special cases. Building on this framework, we propose introducing a prior distribution for the maintained effect parameter to account for uncertainty in this assumption. Our approach provides inference for reference-based estimators that explicitly reflects our uncertainty about how much treatment effects are maintained after the occurrence of ICEs. In trials where no or little post-ICE data are observed, our proposed Bayesian reference-based causal model approach can be used to estimate the treatment policy treatment effect, incorporating uncertainty about the reference-based assumption. We compare the frequentist properties of this approach with existing reference-based methods through simulations and by application to an antidepressant trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00680v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendah Nansereko, Marcel Wolbers, James Carpenter, Jonathan Bartlett</dc:creator>
    </item>
    <item>
      <title>A new machine learning framework for occupational accidents forecasting with safety inspections integration</title>
      <link>https://arxiv.org/abs/2507.00089</link>
      <description>arXiv:2507.00089v1 Announce Type: cross 
Abstract: We propose a generic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments to better inform decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Unlike the other approaches, the long short-term memory (LSTM) network outperforms the other approaches and detects the upcoming high-risk periods with a balanced accuracy of 0.86, confirming the robustness of our methodology and demonstrating that a binary time series model can anticipate these critical periods based on safety inspections. The proposed methodology converts routine safety inspection data into clear weekly risk scores, detecting the periods when accidents are most likely. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00089v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aho Yapi, Pierre Latouche, Arnaud Guillin, Yan Bailly</dc:creator>
    </item>
    <item>
      <title>Disentangled Feature Importance</title>
      <link>https://arxiv.org/abs/2507.00260</link>
      <description>arXiv:2507.00260v1 Announce Type: cross 
Abstract: Feature importance quantification faces a fundamental challenge: when predictors are correlated, standard methods systematically underestimate their contributions. We prove that major existing approaches target identical population functionals under squared-error loss, revealing why they share this correlation-induced bias.
  To address this limitation, we introduce \emph{Disentangled Feature Importance (DFI)}, a nonparametric generalization of the classical $R^2$ decomposition via optimal transport. DFI transforms correlated features into independent latent variables using a transport map, eliminating correlation distortion. Importance is computed in this disentangled space and attributed back through the transport map's sensitivity. DFI provides a principled decomposition of importance scores that sum to the total predictive variability for latent additive models and to interaction-weighted functional ANOVA variances more generally, under arbitrary feature dependencies.
  We develop a comprehensive semiparametric theory for DFI. For general transport maps, we establish root-$n$ consistency and asymptotic normality of importance estimators in the latent space, which extends to the original feature space for the Bures-Wasserstein map. Notably, our estimators achieve second-order estimation error, which vanishes if both regression function and transport map estimation errors are $o_{\mathbb{P}}(n^{-1/4})$. By design, DFI avoids the computational burden of repeated submodel refitting and the challenges of conditional covariate distribution estimation, thereby achieving computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00260v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Extrapolation in Regression Discontinuity Design Using Comonotonicity</title>
      <link>https://arxiv.org/abs/2507.00289</link>
      <description>arXiv:2507.00289v1 Announce Type: cross 
Abstract: We present a novel approach for extrapolating causal effects away from the margin between treatment and non-treatment in sharp regression discontinuity designs with multiple covariates. Our methods apply both to settings in which treatment is a function of multiple observables and settings in which treatment is determined based on a single running variable. Our key identifying assumption is that conditional average treated and untreated potential outcomes are comonotonic: covariate values associated with higher average untreated potential outcomes are also associated with higher average treated potential outcomes. We provide an estimation method based on local linear regression. Our estimands are weighted average causal effects, even if comonotonicity fails. We apply our methods to evaluate counterfactual mandatory summer school policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00289v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Deaner, Soonwoo Kwon</dc:creator>
    </item>
    <item>
      <title>GRAND: Graph Release with Assured Node Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.00402</link>
      <description>arXiv:2507.00402v1 Announce Type: cross 
Abstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases entire networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00402v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqing Liu, Xuan Bi, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>A Recipe for Causal Graph Regression: Confounding Effects Revisited</title>
      <link>https://arxiv.org/abs/2507.00440</link>
      <description>arXiv:2507.00440v1 Announce Type: cross 
Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00440v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Yin, Tianyi Qu, Zihao Wang, Yifan Chen</dc:creator>
    </item>
    <item>
      <title>Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws</title>
      <link>https://arxiv.org/abs/2507.00641</link>
      <description>arXiv:2507.00641v1 Announce Type: cross 
Abstract: Traditional machine learning approaches in physics rely on global optimization, limiting interpretability and enforcing physical constraints externally. We introduce the Hebbian Physics Network (HPN), a self-organizing computational framework in which learning emerges from local Hebbian updates driven by violations of conservation laws. Grounded in non-equilibrium thermodynamics and inspired by Prigogine/'s theory of dissipative structures, HPNs eliminate the need for global loss functions by encoding physical laws directly into the system/'s local dynamics. Residuals - quantified imbalances in continuity, momentum, or energy - serve as thermodynamic signals that drive weight adaptation through generalized Hebbian plasticity. We demonstrate this approach on incompressible fluid flow and continuum diffusion, where physically consistent structures emerge from random initial conditions without supervision. HPNs reframe computation as a residual-driven thermodynamic process, offering an interpretable, scalable, and physically grounded alternative for modeling complex dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00641v1</guid>
      <category>nlin.AO</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gunjan Auti, Hirofumi Daiguji, Gouhei Tanaka</dc:creator>
    </item>
    <item>
      <title>Comparing Misspecified Models with Big Data: A Variational Bayesian Perspective</title>
      <link>https://arxiv.org/abs/2507.00763</link>
      <description>arXiv:2507.00763v1 Announce Type: cross 
Abstract: Optimal data detection in massive multiple-input multiple-output (MIMO) systems often requires prohibitively high computational complexity. A variety of detection algorithms have been proposed in the literature, offering different trade-offs between complexity and detection performance. In recent years, Variational Bayes (VB) has emerged as a widely used method for addressing statistical inference in the context of massive data. This study focuses on misspecified models and examines the risk functions associated with predictive distributions derived from variational posterior distributions. These risk functions, defined as the expectation of the Kullback-Leibler (KL) divergence between the true data-generating density and the variational predictive distributions, provide a framework for assessing predictive performance. We propose two novel information criteria for predictive model comparison based on these risk functions. Under certain regularity conditions, we demonstrate that the proposed information criteria are asymptotically unbiased estimators of their respective risk functions. Through comprehensive numerical simulations and empirical applications in economics and finance, we demonstrate the effectiveness of these information criteria in comparing misspecified models in the context of massive data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00763v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Li, Sushanta K. Mallick, Tao Zeng, Junxing Zhang</dc:creator>
    </item>
    <item>
      <title>Randomization Inference with Sample Attrition</title>
      <link>https://arxiv.org/abs/2507.00795</link>
      <description>arXiv:2507.00795v1 Announce Type: cross 
Abstract: Although appealing, randomization inference for treatment effects can suffer from severe size distortion due to sample attrition. We propose new, computationally efficient methods for randomization inference that remain valid under a range of potentially informative missingness mechanisms. We begin by constructing valid p-values for testing sharp null hypotheses, using the worst-case p-value from the Fisher randomization test over all possible imputations of missing outcomes. Leveraging distribution-free test statistics, this worst-case p-value admits a closed-form solution, connecting naturally to bounds in the partial identification literature. Our test statistics incorporate both potential outcomes and missingness indicators, allowing us to exploit structural assumptions-such as monotone missingness-for increased power. We further extend our framework to test non-sharp null hypotheses concerning quantiles of individual treatment effects. The methods are illustrated through simulations and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00795v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinran Li, Peizan Sheng, Zeyang Yu</dc:creator>
    </item>
    <item>
      <title>Stochastic highway capacity: Unsuitable Kaplan-Meier estimator, revised maximum likelihood estimator, and impact of speed harmonisation</title>
      <link>https://arxiv.org/abs/2507.00893</link>
      <description>arXiv:2507.00893v1 Announce Type: cross 
Abstract: The Kaplan-Meier estimate, also known as the product-limit method (PLM), is a widely used non-parametric maximum likelihood estimator (MLE) in survival analysis. In the context of highway engineering, it has been repeatedly applied to estimate stochastic traffic flow capacity. However, this paper demonstrates that PLM is fundamentally unsuitable for this purpose. The method implicitly assumes continuous exposure to failure risk over time - a premise invalid for traffic flow, where intensity does not increase linearly, and capacity is not even directly observable. Although parametric MLE approach offers a viable alternative, earlier derivation suffers from flawed likelihood formulation, likely due to attempt to preserve consistency with PLM. This study derives a corrected likelihood formula for stochastic capacity MLE and validates it using two empirical datasets. The proposed method is then applied in a case study examining the effect of a variable speed limit (VSL) system used for traffic flow speed harmonisation at a 2 to 1 lane drop. Results show that the VSL improved capacity by approximately 10% or reduced breakdown probability at the same flow intensity by up to 50%. The findings underscore the methodological importance of correct model formulation and highlight the practical relevance of stochastic capacity estimation for evaluating traffic control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00893v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikol\'a\v{s}ek</dc:creator>
    </item>
    <item>
      <title>SOFARI: High-Dimensional Manifold-Based Inference</title>
      <link>https://arxiv.org/abs/2309.15032</link>
      <description>arXiv:2309.15032v2 Announce Type: replace 
Abstract: Multi-task learning is a widely used technique for harnessing information from various tasks. Recently, the sparse orthogonal factor regression (SOFAR) framework, based on the sparse singular value decomposition (SVD) within the coefficient matrix, was introduced for interpretable multi-task learning, enabling the discovery of meaningful latent feature-response association networks across different layers. However, conducting precise inference on the latent factor matrices has remained challenging due to the orthogonality constraints inherited from the sparse SVD constraints. In this paper, we suggest a novel approach called the high-dimensional manifold-based SOFAR inference (SOFARI), drawing on the Neyman near-orthogonality inference while incorporating the Stiefel manifold structure imposed by the SVD constraints. By leveraging the underlying Stiefel manifold structure that is crucial to enabling inference, SOFARI provides easy-to-use bias-corrected estimators for both latent left factor vectors and singular values, for which we show to enjoy the asymptotic mean-zero normal distributions with estimable variances. We introduce two SOFARI variants to handle strongly and weakly orthogonal latent factors, where the latter covers a broader range of applications. We illustrate the effectiveness of SOFARI and justify our theoretical results through simulation examples and a real data application in economic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15032v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zemin Zheng, Xin Zhou, Yingying Fan, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>Nonparametric causal inference for optogenetics: sequential excursion effects for dynamic regimes</title>
      <link>https://arxiv.org/abs/2405.18597</link>
      <description>arXiv:2405.18597v3 Announce Type: replace 
Abstract: Optogenetics is a powerful neuroscience technique for studying how neural circuit manipulation affects behavior. Standard analysis conventions discard information and severely limit the scope of the causal questions that can be probed. To address this gap, we 1) draw connections to the causal inference literature on sequentially randomized experiments, 2) propose a non-parametric framework for analyzing "open-loop" (static regime) optogenetics behavioral experiments, 3) derive extensions of history-restricted marginal structural models for dynamic treatment regimes with positivity violations for "closed-loop" designs, and 4) propose a taxonomy of identifiable causal effects that encompass a far richer collection of scientific questions compared to standard methods. From another view, our work extends "excursion effect" methods, popularized recently in the mobile health literature, to enable estimation of causal contrasts for treatment sequences in the presence of positivity violations. We describe sufficient conditions for identifiability of the proposed causal estimands, and provide asymptotic statistical guarantees for a proposed inverse probability-weighted estimator, a multiply-robust estimator (for two intervention timepoints), a framework for hypothesis testing, and a computationally scalable implementation. Finally, we apply our framework to data from a recent neuroscience study and show how it provides insight into causal effects of optogenetics on behavior that are obscured by standard analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18597v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Loewinger, Alexander W. Levis, Francisco Pereira</dc:creator>
    </item>
    <item>
      <title>Bayesian regression discontinuity design with unknown cutoff</title>
      <link>https://arxiv.org/abs/2406.11585</link>
      <description>arXiv:2406.11585v3 Announce Type: replace 
Abstract: The regression discontinuity design (RDD) is a quasi-experimental approach used to estimate the causal effects of an intervention assigned based on a cutoff criterion. RDD exploits the idea that close to the cutoff units below and above are similar; hence, they can be meaningfully compared. Consequently, the causal effect can be estimated only locally at the cutoff point. This makes the cutoff point an essential element of RDD. However, the exact cutoff location may not always be disclosed to the researchers, and even when it is, the actual location may deviate from the official one. As we illustrate on the application of RDD to the HIV treatment eligibility data, estimating the causal effect at an incorrect cutoff point leads to meaningless results. The method we present, LoTTA (Local Trimmed Taylor Approximation), can be applied both as an estimation and validation tool in RDD. We use a Bayesian approach to incorporate prior knowledge and uncertainty about the cutoff location in the causal effect estimation. At the same time, LoTTA is fitted globally to the whole data, whereas RDD is a local, boundary point estimation problem. In this work we address a natural question that arises: how to make Bayesian inference more local to render a meaningful and powerful estimate of the treatment effect?</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11585v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Kowalska, Mark van de Wiel, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic Survival Analysis in Triple-Negative Breast Cancer</title>
      <link>https://arxiv.org/abs/2406.19213</link>
      <description>arXiv:2406.19213v2 Announce Type: replace 
Abstract: In high-dimensional survival analysis, effective variable selection is crucial for both model interpretation and predictive performance. This paper investigates Cox regression with lasso and adaptive lasso penalties in genomic datasets where covariates far outnumber observations. We propose and evaluate four weight calculation strategies for adaptive lasso specifically designed for high-dimensional settings: ridge regression, principal component analysis (PCA), univariate Cox regression, and random survival forest (RSF) based weights. To address the inherent variability in high dimensional model selection, we develop a robust procedure that evaluates performance across multiple data partitions and selects variables based on a novel importance index. Extensive simulation studies demonstrate that adaptive lasso with ridge and PCA weights significantly outperforms standard lasso in variable selection accuracy while maintaining similar or better predictive performance across various correlation structures, censoring proportions (0-80%), and dimensionality settings. These improvements are particularly pronounced in highly-censored scenarios, making our approach valuable for real-world genetic studies with limited observed events. We apply our methodology to triple-negative breast cancer data with 234 patients, over 19500 variables and 82% censoring, identifying key genetic and clinical prognostic factors. Our findings demonstrate that adaptive lasso with appropriate weight calculation provides more stable and interpretable models for high-dimensional survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19213v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pilar Gonz\'alez-Barquero (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid), Rosa E. Lillo (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Statistics, Universidad Carlos III de Madrid), \'Alvaro M\'endez-Civieta (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Biostatistics, Columbia University, New York)</dc:creator>
    </item>
    <item>
      <title>Comparing Causal Inference Methods for Point Exposures with Missing Confounders: A Simulation Study</title>
      <link>https://arxiv.org/abs/2407.06038</link>
      <description>arXiv:2407.06038v3 Announce Type: replace 
Abstract: Causal inference methods based on electronic health record (EHR) databases must simultaneously handle confounding and missing data. Vast scholarship exists aimed at addressing these two issues separately, but surprisingly few papers attempt to address them simultaneously. In practice, when faced with simultaneous missing data and confounding, analysts may proceed by first imputing missing data and subsequently using outcome regression or inverse-probability weighting (IPW) to address confounding. However, little is known about the theoretical performance of such $\textit{ad hoc}$ methods. In a recent paper Levis $\textit{et al.}$ outline a robust framework for tackling these problems together under certain identifying conditions, and introduce a pair of estimators for the average treatment effect (ATE), one of which is non-parametric efficient. In this work we present a series of simulations, motivated by a published EHR based study of the long-term effects of bariatric surgery on weight outcomes, to investigate these new estimators and compare them to existing $\textit{ad hoc}$ methods. While the latter perform well in certain scenarios, no single estimator is uniformly best. We conclude with recommendations for good practice in the face of partially missing confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06038v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Alexander Levis, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Joint Learning from Heterogeneous Rank Data</title>
      <link>https://arxiv.org/abs/2407.10846</link>
      <description>arXiv:2407.10846v3 Announce Type: replace 
Abstract: The statistical modelling of ranking data has a long history and encompasses various perspectives on how observed rankings arise. One of the most common models, the Plackett-Luce model, is frequently used to aggregate rankings from multiple rankers into a single ranking that corresponds to the underlying quality of the ranked objects. Given that rankers frequently exhibit heterogeneous preferences, mixture-type models have been developed to group rankers with more or less homogeneous preferences together to reduce bias. However, occasionally, these preference groups are known a-priori. Under these circumstances, current practice consists of fitting Plackett-Luce models separately for each group. Nevertheless, there might be some commonalities between different groups of rankers, such that separate estimation implies a loss of information. We propose an extension of the Plackett-Luce model, the Sparse Fused Plackett-Luce model, that allows for joint learning of such heterogeneous rank data, whereby information from different groups is utilised to achieve better model performance. The observed rankings can be considered a function of variables pertaining to the ranked objects. As such, we allow for these types of variables, where information on the coefficients is shared across groups. Moreover, as not all variables might be relevant for the ranking of an object, we impose sparsity on the coefficients to improve interpretability, estimation and prediction of the model. Simulations studies indicate superior performance of the proposed method compared to existing approaches. To illustrate the usage and interpretation of the method, an application on data consisting of consumer preferences regarding various sweet potato varieties is provided. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=SFPL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10846v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Root cause discovery via permutations and Cholesky decomposition</title>
      <link>https://arxiv.org/abs/2410.12151</link>
      <description>arXiv:2410.12151v5 Announce Type: replace 
Abstract: This work is motivated by the following problem: Can we identify the disease-causing gene in a patient affected by a monogenic disorder? This problem is an instance of root cause discovery. In particular, we aim to identify the intervened variable in one interventional sample using a set of observational samples as reference. We consider a linear structural equation model where the causal ordering is unknown. We begin by examining a simple method that uses squared z-scores and characterize the conditions under which this method succeeds and fails, showing that it generally cannot identify the root cause. We then prove, without additional assumptions, that the root cause is identifiable even if the causal ordering is not. Two key ingredients of this identifiability result are the use of permutations and the Cholesky decomposition, which allow us to exploit an invariant property across different permutations to discover the root cause. Furthermore, we characterize permutations that yield the correct root cause and, based on this, propose a valid method for root cause discovery. We also adapt this approach to high-dimensional settings. Finally, we evaluate the performance of our methods through simulations and apply the high-dimensional method to discover disease-causing genes in the gene expression dataset that motivates this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12151v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Benjamin B. Chu, Ines F. Scheller, Julien Gagneur, Marloes H. Maathuis</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of causal effects from observational categorical data</title>
      <link>https://arxiv.org/abs/2504.05198</link>
      <description>arXiv:2504.05198v3 Announce Type: replace 
Abstract: We present a Bayesian procedure for estimation of pairwise intervention effects in a high-dimensional system of categorical variables. We assume that we have observational data generated from an unknown causal Bayesian network for which there are no latent confounders. Most of the existing methods developed for this setting assume that the underlying model is linear Gaussian, including the Bayesian IDA (BIDA) method that we build upon in this work. By combining a Bayesian backdoor estimator with model averaging, we obtain a posterior over the intervention distributions of a cause-effect pair that can be expressed as a mixture over stochastic linear combinations of Dirichlet distributions. Although there is no closed-form expression for the posterior density, it is straightforward to produce Monte Carlo approximations of target quantities through direct sampling, and we also derive closed-form expressions for a few selected moments. To scale up the proposed procedure, we employ Markov Chain Monte Carlo (MCMC), which also enables us to use more efficient adjustment sets compared to the current exact BIDA. Finally, we use Jensen-Shannon divergence to define a novel causal effect based on a set of intervention distributions in the general categorical setting. We compare our method to the original IDA method and existing Bayesian approaches in numerical simulations and show that categorical BIDA performs favorably against the existing alternative methods in terms of producing point estimates and discovering strong effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05198v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vera Kvisgaard, Johan Pensar</dc:creator>
    </item>
    <item>
      <title>Conformal Survival Bands for Risk Screening under Right-Censoring</title>
      <link>https://arxiv.org/abs/2505.04568</link>
      <description>arXiv:2505.04568v2 Announce Type: replace 
Abstract: We propose a method to quantify uncertainty around individual survival distribution estimates using right-censored data, compatible with any survival model. Unlike classical confidence intervals, the survival bands produced by this method offer predictive rather than population-level inference, making them useful for personalized risk screening. For example, in a low-risk screening scenario, they can be applied to flag patients whose survival band at 12 months lies entirely above 50\%, while ensuring that at least half of flagged individuals will survive past that time on average. Our approach builds on recent advances in conformal inference and integrates ideas from inverse probability of censoring weighting and multiple testing with false discovery rate control. We provide asymptotic guarantees and show promising performance in finite samples with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04568v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Conditional Local Independence Testing for Dynamic Causal Discovery</title>
      <link>https://arxiv.org/abs/2506.07844</link>
      <description>arXiv:2506.07844v2 Announce Type: replace 
Abstract: Inferring causal relationships from dynamical systems is the central interest of many scientific inquiries. Conditional Local Independence (CLI), which describes whether the evolution of one process is influenced by another process given additional processes, is important for causal learning in such systems. However, existing CLI tests were limited to counting processes. In this paper, we propose a nonparametric CLT test for It\^o processes. Specifically, we first introduce a testing statistic based on the Local Covariance Measure (LCM) by constructing a martingale from the conditional expectation of the process of interest. For estimation, we propose an efficient estimator based on the optimal filtering equation, which can achieve root-N consistency. To establish the asymptotic level and power of the test, we relax the restrictive boundedness condition to a moment bound condition, which is practical for It\^o processes. We verify the proposed test in synthetic and real-world experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07844v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhou Liu, Xinwei Sun, Yizhou Wang</dc:creator>
    </item>
    <item>
      <title>Fortified Proximal Causal Inference with Many Invalid Proxies</title>
      <link>https://arxiv.org/abs/2506.13152</link>
      <description>arXiv:2506.13152v2 Announce Type: replace 
Abstract: Causal inference from observational data often relies on the assumption of no unmeasured confounding, an assumption frequently violated in practice due to unobserved or poorly measured covariates. Proximal causal inference (PCI) offers a promising framework for addressing unmeasured confounding using a pair of outcome and treatment confounding proxies. However, existing PCI methods typically assume all specified proxies are valid, which may be unrealistic and is untestable without extra assumptions. In this paper, we develop a semiparametric approach for a many-proxy PCI setting that accommodates potentially invalid treatment confounding proxies. We introduce a new class of fortified confounding bridge functions and establish nonparametric identification of the population average treatment effect (ATE) under the assumption that at least $\gamma$ out of $K$ candidate treatment confounding proxies are valid, for any $\gamma \leq K$ set by the analyst without requiring knowledge of which proxies are valid. We establish a local semiparametric efficiency bound and develop a class of multiply robust, locally efficient estimators for the ATE. These estimators are thus simultaneously robust to invalid treatment confounding proxies and model misspecification of nuisance parameters. The proposed methods are evaluated through simulation and applied to assess the effect of right heart catheterization in critically ill patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13152v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myeonghun Yu, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Over the Stability Space of a Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.22407</link>
      <description>arXiv:2506.22407v2 Announce Type: replace 
Abstract: This paper jointly addresses the challenges of non-stationarity and high dimensionality in analysing multivariate time series. Building on the classical concept of cointegration, we introduce a more flexible notion, called stability space, aimed at capturing stationary components in settings where traditional assumptions may not hold. Based on the dimensionality reduction techniques of Partial Least Squares and Principal Component Analysis, we proposed two non-parametric procedures for estimating such a space and a targeted selection of components that prioritises stationarity. We compare these alternatives with the parametric Johansen procedure, when possible. Through simulations and real-data applications, we evaluated the performance of these methodologies across various scenarios, including high-dimensional configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22407v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto V\'asquez-Mart\'inez, Graciela Gonz\'alez-Far\'ias, Jos\'e Ulises M\'arquez Urbina, Francisco Corona</dc:creator>
    </item>
    <item>
      <title>Fr\'echet Mean Set Estimation in the Hausdorff Metric, via Relaxation</title>
      <link>https://arxiv.org/abs/2212.12057</link>
      <description>arXiv:2212.12057v2 Announce Type: replace-cross 
Abstract: This work resolves the following question in non-Euclidean statistics: Is it possible to consistently estimate the Fr\'echet mean set of an unknown population distribution, with respect to the Hausdorff metric, when given access to independent identically-distributed samples? Our affirmative answer is based on a careful analysis of the "relaxed empirical Fr\'echet mean set estimators" which identify the set of near-minimizers of the empirical Fr\'echet functional and where the amount of "relaxation" vanishes as the number of data tends to infinity. On the theoretical side, our results include exact descriptions of which relaxation rates give weak consistency and which give strong consistency, as well as a description of an estimator which (assuming only the finiteness of certain moments and a mild condition on the metric entropy of the underlying metric space) adaptively finds the fastest possible relaxation rate for strongly consistent estimation. On the applied side, we consider the problem of estimating the set of Fermat-Weber points of an unknown distribution in the space of equidistant trees endowed with the tropical projective metric; in this setting, we provide an algorithm that provably implements our adaptive estimator, and we apply this method to real phylogenetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12057v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3150/24-BEJ1734</arxiv:DOI>
      <arxiv:journal_reference>Bernoulli 31(1): 432-456 (February 2025)</arxiv:journal_reference>
      <dc:creator>Moise Blanchard, Adam Quinn Jaffe</dc:creator>
    </item>
    <item>
      <title>Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees</title>
      <link>https://arxiv.org/abs/2503.02859</link>
      <description>arXiv:2503.02859v2 Announce Type: replace-cross 
Abstract: Stability for dynamic network embeddings ensures that nodes behaving the same at different times receive the same embedding, allowing comparison of nodes in the network across time. We present attributed unfolded adjacency spectral embedding (AUASE), a stable unsupervised representation learning framework for dynamic networks in which nodes are attributed with time-varying covariate information. To establish stability, we prove uniform convergence to an associated latent position model. We quantify the benefits of our dynamic embedding by comparing with state-of-the-art network representation learning methods on four real attributed networks. To the best of our knowledge, AUASE is the only attributed dynamic embedding that satisfies stability guarantees without the need for ground truth labels, which we demonstrate provides significant improvements for link prediction and node classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02859v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Ceccherini, Ian Gallagher, Andrew Jones, Daniel Lawson</dc:creator>
    </item>
  </channel>
</rss>
