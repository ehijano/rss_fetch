<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Axiomatic Approach to Comparing Sensitivity Parameters</title>
      <link>https://arxiv.org/abs/2504.21106</link>
      <description>arXiv:2504.21106v1 Announce Type: new 
Abstract: Many methods are available for assessing the importance of omitted variables. These methods typically make different, non-falsifiable assumptions. Hence the data alone cannot tell us which method is most appropriate. Since it is unreasonable to expect results to be robust against all possible robustness checks, researchers often use methods deemed "interpretable", a subjective criterion with no formal definition. In contrast, we develop the first formal, axiomatic framework for comparing and selecting among these methods. Our framework is analogous to the standard approach for comparing estimators based on their sampling distributions. We propose that sensitivity parameters be selected based on their covariate sampling distributions, a design distribution of parameter values induced by an assumption on how covariates are assigned to be observed or unobserved. Using this idea, we define a new concept of parameter consistency, and argue that a reasonable sensitivity parameter should be consistent. We prove that the literature's most popular approach is inconsistent, while several alternatives are consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21106v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Publication Design with Incentives in Mind</title>
      <link>https://arxiv.org/abs/2504.21156</link>
      <description>arXiv:2504.21156v1 Announce Type: new 
Abstract: The publication process both determines which research receives the most attention, and influences the supply of research through its impact on the researcher's private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21156v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ravi Jagadeesan, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Real-time Program Evaluation using Anytime-valid Rank Tests</title>
      <link>https://arxiv.org/abs/2504.21595</link>
      <description>arXiv:2504.21595v1 Announce Type: new 
Abstract: Counterfactual mean estimators such as difference-in-differences and synthetic control have grown into workhorse tools for program evaluation. Inference for these estimators is well-developed in settings where all post-treatment data is available at the time of analysis. However, in settings where data arrives sequentially, these tests do not permit real-time inference, as they require a pre-specified sample size T. We introduce real-time inference for program evaluation through anytime-valid rank tests. Our methodology relies on interpreting the absence of a treatment effect as exchangeability of the treatment estimates. We then convert these treatment estimates into sequential ranks, and construct optimal finite-sample valid sequential tests for exchangeability. We illustrate our methods in the context of difference-in-differences and synthetic control. In simulations, they control size even under mild exchangeability violations. While our methods suffer slight power loss at T, they allow for early rejection (before T) and preserve the ability to reject later (after T).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21595v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam van Meer, Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Mixture Models in the Presence of Hidden Markov Regimes with Covariate-Dependent Transition Probabilities</title>
      <link>https://arxiv.org/abs/2504.21669</link>
      <description>arXiv:2504.21669v1 Announce Type: new 
Abstract: This paper studies the robustness of quasi-maximum-likelihood (QML) estimation in hidden Markov models (HMMs) when the regime-switching structure is misspecified. Specifically, we examine the case where the true data-generating process features a hidden Markov regime sequence with covariate-dependent transition probabilities, but estimation proceeds under a simplified mixture model that assumes regimes are independent and identically distributed. We show that the parameters governing the conditional distribution of the observables can still be consistently estimated under this misspecification, provided certain regularity conditions hold. Our results highlight a practical benefit of using computationally simpler mixture models in settings where regime dependence is complex or difficult to model directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21669v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Demian Pouzo, Martin Sola, Zacharias Psaradakis</dc:creator>
    </item>
    <item>
      <title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
      <link>https://arxiv.org/abs/2504.21060</link>
      <description>arXiv:2504.21060v1 Announce Type: cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ``Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21060v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyuan Jiang, Yi Man</dc:creator>
    </item>
    <item>
      <title>Inference for Regression with Variables Generated by AI or Machine Learning</title>
      <link>https://arxiv.org/abs/2402.15585</link>
      <description>arXiv:2402.15585v5 Announce Type: replace 
Abstract: Researchers now routinely use AI or other machine learning methods to estimate latent variables of economic interest, then plug-in the estimates as covariates in a regression. We show both theoretically and empirically that naively treating AI/ML-generated variables as "data" leads to biased estimates and invalid inference. To restore valid inference, we propose two methods: (1) an explicit bias correction with bias-corrected confidence intervals, and (2) joint estimation of the regression parameters and latent variables. We illustrate these ideas through applications involving label imputation, dimensionality reduction, and index construction via classification and aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15585v5</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Battaglia, Timothy Christensen, Stephen Hansen, Szymon Sacher</dc:creator>
    </item>
    <item>
      <title>Estimating Wage Disparities Using Foundation Models</title>
      <link>https://arxiv.org/abs/2409.09894</link>
      <description>arXiv:2409.09894v2 Announce Type: replace-cross 
Abstract: The rise of foundation models marks a paradigm shift in machine learning: instead of training specialized models from scratch, foundation models are first trained on massive datasets before being adapted or fine-tuned to make predictions on smaller datasets. Initially developed for text, foundation models have also excelled at making predictions about social science data. However, while many estimation problems in the social sciences use prediction as an intermediate step, they ultimately require different criteria for success. In this paper, we develop methods for fine-tuning foundation models to perform these estimation problems. We first characterize an omitted variable bias that can arise when a foundation model is only fine-tuned to maximize predictive accuracy. We then provide a novel set of conditions for fine-tuning under which estimates derived from a foundation model are root-n-consistent. Based on this theory, we develop new fine-tuning algorithms that empirically mitigate this omitted variable bias. To demonstrate our ideas, we study gender wage decomposition. This is a statistical estimation problem from econometrics where the goal is to decompose the gender wage gap into components that can and cannot be explained by career histories of workers. Classical methods for decomposing the wage gap employ simple predictive models of wages which condition on coarse summaries of career history that may omit factors that are important for explaining the gap. Instead, we use a custom-built foundation model to decompose the gender wage gap, which captures a richer representation of career history. Using data from the Panel Study of Income Dynamics, we find that career history explains more of the gender wage gap than standard econometric models can measure, and we identify elements of career history that are omitted by standard models but are important for explaining the wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09894v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keyon Vafa, Susan Athey, David M. Blei</dc:creator>
    </item>
  </channel>
</rss>
