<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network</title>
      <link>https://arxiv.org/abs/2510.11866</link>
      <description>arXiv:2510.11866v1 Announce Type: new 
Abstract: Decentralized storage is one of the most natural applications built on blockchains and a central component of the Web3 ecosystem. Yet despite a decade of active development -- from IPFS and Filecoin to more recent entrants -- most of these storage protocols have received limited formal analysis of their incentive properties. Claims of incentive compatibility are sometimes made, but rarely proven. This gap matters: without well-designed incentives, a system may distribute storage but fail to truly decentralize it.
  We analyze Shelby -- a storage network protocol recently proposed by Aptos Labs and Jump Crypto -- and provide the first formal proof of its incentive properties. Our game-theoretic model shows that while off-chain audits alone collapse to universal shirking, Shelby's combination of peer audits with occasional on-chain verification yields incentive compatibility under natural parameter settings. We also examine coalition behavior and outline a simple modification that strengthens the protocol's collusion-resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11866v1</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Crystal, Guy Goren, Scott Duke Kominers</dc:creator>
    </item>
    <item>
      <title>Fair Division of Indivisible Items</title>
      <link>https://arxiv.org/abs/2510.12158</link>
      <description>arXiv:2510.12158v1 Announce Type: new 
Abstract: We study the fair division of indivisible items. In the general model, the goal is to allocate $m$ indivisible items to $n$ agents while satisfying fairness criteria such as MMS, EF1, and EFX. We also study a recently-introduced graphical model that represents the fair division problem as a multigraph, in which vertices correspond to agents and edges to items. The graphical model stipulates that an item can have non-zero marginal utility to an agent only if its corresponding edge is incident to the agent's corresponding vertex. We study orientations (allocations that allocate each edge to an endpoint) in this model, as they are particularly desirable.
  Our first contribution concerns MMS allocations of mixed manna (i.e. a mixture of goods and chores) in the general model. It is known that MMS allocations of goods exist when $m \leq n+5$. We generalize this and show that when $m \leq n+5$, MMS allocations of mixed manna exist as long as $n \leq 3$, there is an agent whose MMS threshold is non-negative, or every item is a chore. Remarkably, our result leaves only the case where every agent has a negative MMS threshold unanswered.
  Our second contribution concerns EFX orientations of multigraphs of goods. We show that deciding whether EFX orientations exist for multigraphs is NP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show symmetric bi-valued multigraphs that do not contain non-trivial odd multitrees have EFX orientations that can be found in polynomial time.
  Our third contribution concerns EF1 and EFX orientations of graphs and multigraphs of chores. We obtain polynomial-time algorithms for deciding whether such graphs have EF1 and EFX orientations, resolving a previous conjecture and showing a fundamental difference between goods and chores division. In addition, we show that the analogous problems for multigraphs are NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12158v1</guid>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Hsu</dc:creator>
    </item>
    <item>
      <title>Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes</title>
      <link>https://arxiv.org/abs/2510.12641</link>
      <description>arXiv:2510.12641v1 Announce Type: new 
Abstract: We study stability in additively separable hedonic games when coalition sizes have to respect fixed size bounds. We consider four classic notions of stability based on single-agent deviations, namely, Nash stability, individual stability, contractual Nash stability, and contractual individual stability. For each stability notion, we consider two variants: in one, the coalition left behind by a deviator must still be of a valid size, and in the other there is no such constraint. We provide a full picture of the existence of stable outcomes with respect to given size parameters. Additionally, when there are only upper bounds, we fully characterize the computational complexity of the associated existence problem. In particular, we obtain polynomial-time algorithms for contractual individual stability and contractual Nash stability, where the latter requires an upper bound of 2. We obtain further results for Nash stability and contractual individual stability, when the lower bound is at least 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12641v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bullinger, Adam Dunajski, Edith Elkind, Matan Gilboa</dc:creator>
    </item>
    <item>
      <title>Edge-weighted Online Stochastic Matching: Beating $1-\frac1e$</title>
      <link>https://arxiv.org/abs/2210.12543</link>
      <description>arXiv:2210.12543v3 Announce Type: cross 
Abstract: We study the edge-weighted online stochastic matching problem. Since Feldman, Mehta, Mirrokni, and Muthukrishnan proposed the $(1-\frac1e)$-competitive Suggested Matching algorithm, there has been no improvement for the general edge-weighted online stochastic matching problem. In this paper, we introduce the first algorithm beating the $1-\frac1e$ barrier in this setting, achieving a competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we design an algorithmic preprocessing, dividing all edges into two classes. Then based on the Suggested Matching algorithm, we adjust the matching strategy to improve the performance on one class in the early stage and on another class in the late stage, while keeping the matching events of different edges highly independent. By balancing them, we finally guarantee the matched probability of every single edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12543v3</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Yan</dc:creator>
    </item>
    <item>
      <title>AI Agents for the Dhumbal Card Game: A Comparative Study</title>
      <link>https://arxiv.org/abs/2510.11736</link>
      <description>arXiv:2510.11736v1 Announce Type: cross 
Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a culturally significant multiplayer card game with imperfect information, through a systematic comparison of rule-based, search-based, and learning-based strategies. We formalize Dhumbal's mechanics and implement diverse agents, including heuristic approaches (Aggressive, Conservative, Balanced, Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning approaches including Deep Q-Network (DQN) and Proximal Policy Optimization (PPO), and a random baseline. Evaluation involves within-category tournaments followed by a cross-category championship. Performance is measured via win rate, economic outcome, Jhyap success, cards discarded per round, risk assessment, and decision efficiency. Statistical significance is assessed using Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95% confidence intervals (CI). Across 1024 simulated rounds, the rule-based Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]), outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of Jhyap declarations. The study contributes a reproducible AI framework, insights into heuristic efficacy under partial information, and open-source code, thereby advancing AI research and supporting digital preservation of cultural games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11736v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahaj Raj Malla</dc:creator>
    </item>
    <item>
      <title>Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling</title>
      <link>https://arxiv.org/abs/2510.11877</link>
      <description>arXiv:2510.11877v1 Announce Type: cross 
Abstract: The Transformer, a highly expressive architecture for sequence modeling, has recently been adapted to solve sequential decision-making, most notably through the Decision Transformer (DT), which learns policies by conditioning on desired returns. Yet, the adversarial robustness of reinforcement learning methods based on sequence modeling remains largely unexplored. Here we introduce the Conservative Adversarially Robust Decision Transformer (CART), to our knowledge the first framework designed to enhance the robustness of DT in adversarial stochastic games. We formulate the interaction between the protagonist and the adversary at each stage as a stage game, where the payoff is defined as the expected maximum value over subsequent states, thereby explicitly incorporating stochastic state transitions. By conditioning Transformer policies on the NashQ value derived from these stage games, CART generates policy that are simultaneously less exploitable (adversarially robust) and conservative to transition uncertainty. Empirically, CART achieves more accurate minimax value estimation and consistently attains superior worst-case returns across a range of adversarial stochastic games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11877v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohang Tang, Zhuowen Cheng, Satyabrat Kumar</dc:creator>
    </item>
    <item>
      <title>Perceived Fairness in Networks</title>
      <link>https://arxiv.org/abs/2510.12028</link>
      <description>arXiv:2510.12028v1 Announce Type: cross 
Abstract: The usual definitions of algorithmic fairness focus on population-level statistics, such as demographic parity or equal opportunity. However, in many social or economic contexts, fairness is not perceived globally, but locally, through an individual's peer network and comparisons. We propose a theoretical model of perceived fairness networks, in which each individual's sense of discrimination depends on the local topology of interactions. We show that even if a decision rule satisfies standard criteria of fairness, perceived discrimination can persist or even increase in the presence of homophily or assortative mixing. We propose a formalism for the concept of fairness perception, linking network structure, local observation, and social perception. Analytical and simulation results highlight how network topology affects the divergence between objective fairness and perceived fairness, with implications for algorithmic governance and applications in finance and collaborative insurance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12028v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arthur Charpentier</dc:creator>
    </item>
    <item>
      <title>PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators</title>
      <link>https://arxiv.org/abs/2403.15524</link>
      <description>arXiv:2403.15524v3 Announce Type: replace 
Abstract: In this paper, we present the Proportional Payoff Allocation Game (PPA-Game), which characterizes situations where agents compete for divisible resources. In the PPA-game, agents select from available resources, and their payoffs are proportionately determined based on heterogeneous weights attributed to them. Such dynamics simulate content creators on online recommender systems like YouTube and TikTok, who compete for finite consumer attention, with content exposure reliant on inherent and distinct quality. We first conduct a game-theoretical analysis of the PPA-Game. While the PPA-Game does not always guarantee the existence of a pure Nash equilibrium (PNE), we identify prevalent scenarios ensuring its existence. Simulated experiments further prove that the cases where PNE does not exist rarely happen. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta &gt; 0$. Empirical results further validate the effectiveness of our online learning approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15524v3</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711896.3737085</arxiv:DOI>
      <dc:creator>Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui</dc:creator>
    </item>
    <item>
      <title>Eco-driving Incentive Mechanisms for Mitigating Emissions in Urban Transportation</title>
      <link>https://arxiv.org/abs/2410.07952</link>
      <description>arXiv:2410.07952v2 Announce Type: replace 
Abstract: This paper develops incentive mechanisms for promoting eco-driving with the overarching goal of minimizing emissions in transportation networks. The system operator provides drivers with energy-efficient driving guidance throughout their trips and measures compliance through vehicle telematics that capture how closely drivers follow this guidance. Drivers optimize their behaviors based on personal trade-offs between travel times and emissions. To design effective incentives, the operator elicits driver preferences regarding trip urgency and willingness to eco-drive, while determining optimal budget allocations and eco-driving recommendations. Two distinct settings based on driver behavior are analyzed. When drivers report their preferences truthfully, an incentive mechanism ensuring obedience (drivers find it optimal to follow recommendations) is designed by implementing eco-driving recommendations as a Nash equilibrium. When drivers may report strategically, the mechanism is extended to be both obedient and truthful (drivers find it optimal to report truthfully). Unlike existing works that focus on congestion or routing decisions in transportation networks, our framework explicitly targets emissions reduction by incentivizing drivers. The proposed mechanism addresses both strategic behavior and network effects arising from driver interactions, without requiring the operator to reveal system parameters to the drivers. Numerical simulations demonstrate the effects of budget constraints, driver types, and strategic misreporting on equilibrium outcomes and emissions reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07952v2</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Umar B. Niazi, Jung-Hoon Cho, Munther A. Dahleh, Roy Dong, Cathy Wu</dc:creator>
    </item>
    <item>
      <title>A Generalization of von Neumann's Reduction from the Assignment Problem to Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2410.10767</link>
      <description>arXiv:2410.10767v2 Announce Type: replace 
Abstract: The equivalence between von Neumann's Minimax Theorem for zero-sum games and the LP Duality Theorem connects cornerstone problems of the two fields of game theory and optimization, respectively, and has been the subject of intense scrutiny for seven decades. Yet, as observed in this paper, the proof of the difficult direction of this equivalence is unsatisfactory: It does not assign distinct roles to the two players of the game, as is natural from the definition of a zero-sum game.
  In retrospect, a partial resolution to this predicament was provided in another brilliant paper of von Neumann, which reduced the assignment problem to zero-sum games. However, the underlying LP is highly specialized; all entries of its objective function vector are strictly positive, the constraint vector is all ones, and the constraint matrix is 0/1.
  We generalize von Neumann's result along two directions, each allowing negative entries in certain parts of the LP. Our reductions make explicit the roles of the two players of the reduced game, namely their maximin strategies are to play optimal solutions to the primal and dual LPs. Furthermore, unlike previous reductions, the value of the reduced game reveals the value of the given LP. Our generalizations encompass several basic economic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10767v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilan Adler, Martin Bullinger, Vijay V. Vazirani</dc:creator>
    </item>
    <item>
      <title>EFX Orientations of Multigraphs</title>
      <link>https://arxiv.org/abs/2410.12039</link>
      <description>arXiv:2410.12039v2 Announce Type: replace 
Abstract: We study EFX orientations of multigraphs with self-loops. In this setting, vertices represent agents, edges represent goods, and a good provides positive utility to an agent only if it is incident to the agent. We focus on the bi-valued symmetric case in which each edge has equal utility to both incident agents, and edges have one of two possible utilities $\alpha &gt; \beta \geq 0$. In contrast with the case of simple graphs for which bipartiteness implies the existence of an EFX orientation, we show that deciding whether a symmetric multigraph $G$ of any multiplicity $q \geq 2$ has an EFX orientation is NP-complete even if $G$ is bipartite, $\alpha &gt; q\beta$, and $G$ contains a structure called a non-trivial odd multitree (NTOM). Moreover, we show that NTOMs are a problematic structure in the sense that even very simple NTOMs can fail to have EFX orientations, and multigraphs that do not contain NTOMs always have EFX orientations that can be found in polynomial-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12039v2</guid>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Hsu</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Algorithms for Fair Orientations of Chores</title>
      <link>https://arxiv.org/abs/2501.13481</link>
      <description>arXiv:2501.13481v2 Announce Type: replace 
Abstract: This paper addresses the problem of finding fair orientations of graphs of chores, in which each vertex corresponds to an agent, each edge corresponds to a chore, and a chore has zero marginal utility to an agent if its corresponding edge is not incident to the vertex corresponding to the agent. Recently, Zhou et al. (IJCAI, 2024) analyzed the complexity of deciding whether graphs containing a mixture of goods and chores have EFX orientations, and conjectured that deciding whether graphs containing only chores have EFX orientations is NP-complete. We resolve this conjecture by giving polynomial-time algorithms that find EF1 and EFX orientations of graphs containing only chores if they exist, even if there are self-loops. Remarkably, our result demonstrates a surprising separation between the case of goods and the case of chores, because deciding whether graphs containing only goods have EFX orientations was shown to be NP-complete by Christodoulou et al. (EC, 2023). In addition, we show the EF1 and EFX orientation problems for multigraphs to be NP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13481v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Hsu, Valerie King</dc:creator>
    </item>
    <item>
      <title>Nash Equilibria in Games with Playerwise Concave Coupling Constraints: Existence and Computation</title>
      <link>https://arxiv.org/abs/2509.14032</link>
      <description>arXiv:2509.14032v2 Announce Type: replace 
Abstract: We study the existence and computation of Nash equilibria in continuous static games where the players' admissible strategies are subject to shared coupling constraints, i.e., constraints that depend on their \emph{joint} strategies. Specifically, we focus on a class of games characterized by playerwise concave utilities and playerwise concave constraints. Prior results on the existence of Nash equilibria are not applicable to this class, as they rely on strong assumptions such as joint convexity of the feasible set. By leveraging topological fixed point theory and novel structural insights into the contractibility of feasible sets under playerwise concave constraints, we give an existence proof for Nash equilibria under weaker conditions. Having established existence, we then focus on the computation of Nash equilibria via independent gradient methods under the additional assumption that the utilities admit a potential function. To account for the possibly nonconvex feasible region, we employ a log barrier regularized gradient ascent with adaptive stepsizes. Starting from an initial feasible strategy profile and under exact gradient feedback, the proposed method converges to an $\epsilon$-approximate constrained Nash equilibrium within $\mathcal{O}(\epsilon^{-3})$ iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14032v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Jordan, Maryam Kamgarpour</dc:creator>
    </item>
    <item>
      <title>Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2510.04407</link>
      <description>arXiv:2510.04407v2 Announce Type: replace 
Abstract: A considerable chasm has been looming for decades between theory and practice in zero-sum game solving through first-order methods. Although a convergence rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox algorithm and Nesterov's excessive gap technique in the early 2000s, the most effective paradigm in practice is *counterfactual regret minimization*, which is based on *regret matching* and its modern variants. In particular, the state of the art across most benchmarks is *predictive* regret matching$^+$ (PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new scale-invariant and parameter-free variant of PRM$^+$, which we call IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$ (i.e., optimal) average-iterate convergence guarantees, while also being on par with PRM$^+$ on benchmark games. From a technical standpoint, we draw an analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive* learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the regret vector -- which can be thought of as the inverse of the learning rate -- can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the invariance that the norm of the regret vector is nondecreasing. This enables us to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive version of optimistic gradient descent that we introduce whose learning rate depends on the misprediction error, demystifying the effectiveness of the regret matching family *vis-a-vis* more standard optimization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04407v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Hu Zhang, Ioannis Anagnostides, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>Mechanism design and equilibrium analysis of smart contract mediated resource allocation</title>
      <link>https://arxiv.org/abs/2510.05504</link>
      <description>arXiv:2510.05504v2 Announce Type: replace 
Abstract: Decentralized coordination and digital contracting are becoming critical in complex industrial ecosystems, yet existing approaches often rely on ad hoc heuristics or purely technical blockchain implementations without a rigorous economic foundation. This study develops a mechanism design framework for smart contract-based resource allocation that explicitly embeds efficiency and fairness in decentralized coordination. We establish the existence and uniqueness of contract equilibria, extending classical results in mechanism design, and introduce a decentralized price adjustment algorithm with provable convergence guarantees that can be implemented in real time. To evaluate performance, we combine extensive synthetic benchmarks with a proof-of-concept real-world dataset (MovieLens). The synthetic tests probe robustness under fee volatility, participation shocks, and dynamic demand, while the MovieLens case study illustrates how the mechanism can balance efficiency and fairness in realistic allocation environments. Results demonstrate that the proposed mechanism achieves substantial improvements in both efficiency and equity while remaining resilient to abrupt perturbations, confirming its stability beyond steady state analysis. The findings highlight broad managerial and policy relevance for supply chains, logistics, energy markets, healthcare resource allocation, and public infrastructure, where transparent and auditable coordination is increasingly critical. By combining theoretical rigor with empirical validation, the study shows how digital contracts can serve not only as technical artifacts but also as institutional instruments for transparency, accountability, and resilience in high-stakes resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05504v2</guid>
      <category>cs.GT</category>
      <category>q-fin.GN</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinho Cha, Justin Yu, Eunchan Daniel Cha, Emily Yoo, Caedon Geoffrey, Hyoshin Song</dc:creator>
    </item>
    <item>
      <title>Offline Fictitious Self-Play for Competitive Games</title>
      <link>https://arxiv.org/abs/2403.00841</link>
      <description>arXiv:2403.00841v2 Announce Type: replace-cross 
Abstract: Offline Reinforcement Learning (RL) enables policy improvement from fixed datasets without online interactions, making it highly suitable for real-world applications lacking efficient simulators. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces OFF-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn the best responses to different opponents and employ the Offline Self-Play learning framework. To overcome the challenge of partial coverage, we combine the single-agent offline RL method with Fictitious Self-Play (FSP) to approximate NE by constraining the approximate best responses away from out-of-distribution actions. Experiments on matrix games, extensive-form poker, and board games demonstrate that OFF-FSP achieves significantly lower exploitability than state-of-the-art baselines. Finally, we validate OFF-FSP on a real-world human-robot competitive task, demonstrating its potential for solving complex, hard-to-simulate real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00841v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingxiao Chen, Weiji Xie, Weinan Zhang, Yong yu, Ying Wen</dc:creator>
    </item>
    <item>
      <title>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</title>
      <link>https://arxiv.org/abs/2410.23223</link>
      <description>arXiv:2410.23223v2 Announce Type: replace-cross 
Abstract: Many alignment methods, including reinforcement learning from human feedback (RLHF), rely on the Bradley-Terry reward assumption, which is not always sufficient to capture the full range and complexity of general human preferences. We explore RLHF under a general preference framework by modeling the alignment problem as a two-player zero-sum game in a game-theoretic framework, where the Nash equilibrium policy guarantees a 50% win rate against any competing policy. However, previous self-play algorithms for finding the Nash policy either diverge or only converge to a Nash policy in a modified game, even in a simple synthetic setting, thereby failing to maintain the 50% win rate guarantee against all other policies. We propose a meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), for language model alignment with general preferences, inspired by convergent algorithms in game theory. We provide theoretical analysis that our meta-algorithm converges to an exact Nash policy in the last iterate and demonstrate its effectiveness on a range of synthetic and preference optimization datasets. COMAL is simple and can be integrated with many existing methods designed for preference optimization with minimal changes, and empirically it consistently maintains above 60.2% and 56.8% win rates, when applied to Llama-3-8B-Instruct and Qwen2.5-7B, against all compared algorithms under controlled evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23223v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Liu, Argyris Oikonomou, Weiqiang Zheng, Yang Cai, Arman Cohan</dc:creator>
    </item>
    <item>
      <title>Construction of Compromise Values for Cooperative Games</title>
      <link>https://arxiv.org/abs/2503.05381</link>
      <description>arXiv:2503.05381v2 Announce Type: replace-cross 
Abstract: We explore a broad class of values for cooperative games in characteristic function form, known as \emph{compromise values\/}. These values efficiently allocate payoffs by linearly combining well-specified upper and lower bounds on payoffs. We identify subclasses of games that admit non-trivial efficient allocations within the considered bounds, which we call \emph{bound-balanced games}. Subsequently, we define the associated compromise value. We also provide an axiomatisation of this class of compromise values using variants of the minimal rights property and restricted proportionality.
  We introduce two construction methods for properly devised compromise values. Under mild conditions, one can use either a lower or an upper bound to construct a well-defined compromise value.
  We construct and axiomatise various well-known and new compromise values based on these methods, including the $\tau$-, the $\chi$-, the Gately, the CIS-, the PANSC-, the EANSC-, and the new KM-values. We conclude that this approach establishes a common foundation for a wide range of different values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05381v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert P. Gilles, Ren\'e van den Brink</dc:creator>
    </item>
    <item>
      <title>Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining</title>
      <link>https://arxiv.org/abs/2509.09071</link>
      <description>arXiv:2509.09071v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly embedded in collaborative human activities such as business negotiations and group coordination, it becomes critical to evaluate both the performance gains they can achieve and how they interact in dynamic, multi-agent environments. Unlike traditional statistical agents such as Bayesian models, which may excel under well-specified conditions, large language models (LLMs) can generalize across diverse, real-world scenarios, raising new questions about how their strategies and behaviors compare to those of humans and other agent types. In this work, we compare outcomes and behavioral dynamics across humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in a dynamic negotiation setting under identical conditions. Bayesian agents extract the highest surplus through aggressive optimization, at the cost of frequent trade rejections. Humans and LLMs achieve similar overall surplus, but through distinct behaviors: LLMs favor conservative, concessionary trades with few rejections, while humans employ more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find that performance parity -- a common benchmark in agent evaluation -- can conceal fundamental differences in process and alignment, which are critical for practical deployment in real-world coordination tasks. By establishing foundational behavioral baselines under matched conditions, this work provides a baseline for future studies in more applied, variable-rich environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09071v3</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Crystal Qian, Kehang Zhu, John Horton, Benjamin S. Manning, Vivian Tsai, James Wexler, Nithum Thain</dc:creator>
    </item>
  </channel>
</rss>
