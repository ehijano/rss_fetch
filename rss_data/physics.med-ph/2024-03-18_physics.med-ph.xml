<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Mar 2024 04:04:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Should the choice of BOIN design parameter p.tox only depend on the target DLT rate?</title>
      <link>https://arxiv.org/abs/2403.09667</link>
      <description>arXiv:2403.09667v1 Announce Type: new 
Abstract: When the early stopping parameter n.earlystop is relatively small or the cohortsize value is not optimized via simulation, it may be better to use p.tox &lt; 1.4 * target.DLT.rate, or try out different cohort sizes, or increase n.earlystop, whichever is both feasible and provides better operating characteristics. This is because if the cohortsize was not optimized via simulation, even when n.earlystop = 12, the BOIN escalation/de-escalation rules generated using p.tox = 1.4 * target.DLT.rate could be exactly the same as those calculated using p.tox &gt; 3 * target.DLT.rate, which might not be acceptable for some pediatric trials targeting 10% DLT rate. The traditional 3+3 design stops the dose finding process when 3 patients have been treated at the current dose level, 0 DLT has been observed, and the next higher dose has already been eliminated. If additional 3 patients were required to be treated at the current dose in the situation described above, the corresponding boundary table could be generated using BOIN design with target DLT rates ranging from 18% to 29%, p.saf ranging from 8% to 26%, and p.tox ranging from 39% to 99%. To generate the boundary table of this 3+3 design variant, BOIN parameters also need to satisfy a set of conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09667v1</guid>
      <category>physics.med-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Lu</dc:creator>
    </item>
    <item>
      <title>Spectral CT Two-step and One-step Material Decomposition using Diffusion Posterior Sampling</title>
      <link>https://arxiv.org/abs/2403.10183</link>
      <description>arXiv:2403.10183v1 Announce Type: new 
Abstract: This paper proposes a novel approach to spectral computed tomography (CT) material decomposition that uses the recent advances in generative diffusion models (DMs) for inverse problems. Spectral CT and more particularly photon-counting CT (PCCT) can perform transmission measurements at different energy levels which can be used for material decomposition. It is an ill-posed inverse problem and therefore requires regularization. DMs are a class of generative model that can be used to solve inverse problems via diffusion posterior sampling (DPS). In this paper we adapt DPS for material decomposition in a PCCT setting. We propose two approaches, namely Two-step Diffusion Posterior Sampling (TDPS) and One-step Diffusion Posterior Sampling (ODPS). Early results from an experiment with simulated low-dose PCCT suggest that DPSs have the potential to outperform state-of-the-art model-based iterative reconstruction (MBIR). Moreover, our results indicate that TDPS produces material images with better peak signal-to-noise ratio (PSNR) than images produced with ODPS with similar structural similarity (SSIM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10183v1</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corentin Vazia, Alexandre Bousse, Jacques Froment, B\'eatrice Vedel, Franck Vermet, Zhihan Wang, Thore Dassow, Jean-Pierre Tasu, Dimitris Visvikis</dc:creator>
    </item>
    <item>
      <title>pyCEPS: A cross-platform Electroanatomic Mapping Data to Computational Model Conversion Platform for the Calibration of Digital Twin Models of Cardiac Electrophysiology</title>
      <link>https://arxiv.org/abs/2403.10394</link>
      <description>arXiv:2403.10394v1 Announce Type: new 
Abstract: Background and Objective: Data from electro-anatomical mapping (EAM) systems are playing an increasingly important role in computational modeling studies for the patient-specific calibration of digital twin models. However, data exported from commercial EAM systems are challenging to access and parse. Converting to data formats that are easily amenable to be viewed and analyzed with commonly used cardiac simulation software tools such as openCARP remains challenging. We therefore developed an open-source platform, pyCEPS, for parsing and converting clinical EAM data conveniently to standard formats widely adopted within the cardiac modeling community. Methods and Results: pyCEPS is an open-source Python-based platform providing the following functions: (i) access and interrogate the EAM data exported from clinical mapping systems; (ii) efficient browsing of EAM data to preview mapping procedures, electrograms (EGMs), and electro-cardiograms (ECGs); (iii) conversion to modeling formats according to the openCARP standard, to be amenable to analysis with standard tools and advanced workflows as used for in silico EAM data. Documentation and training material to facilitate access to this complementary research tool for new users is provided. We describe the technological underpinnings and demonstrate the capabilities of pyCEPS first, and showcase its use in an exemplary modeling application where we use clinical imaging data to build a patient-specific anatomical model. Conclusion: With pyCEPS we offer an open-source framework for accessing EAM data, and converting these to cardiac modeling standard formats. pyCEPS provides the core functionality needed to integrate EAM data in cardiac modeling research. We detail how pyCEPS could be integrated into model calibration workflows facilitating the calibration of a computational model based on EAM data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10394v1</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Arnold, Anton J. Prassl, Aurel Neic, Franz Thaler, Christoph M. Augustin, Matthias A. F. Gsell, Karli Gillette, Martin Manninger, Daniel Scherr, Gernot Plank</dc:creator>
    </item>
    <item>
      <title>End-to-end Adaptive Dynamic Subsampling and Reconstruction for Cardiac MRI</title>
      <link>https://arxiv.org/abs/2403.10346</link>
      <description>arXiv:2403.10346v1 Announce Type: cross 
Abstract: Accelerating dynamic MRI is essential for enhancing clinical applications, such as adaptive radiotherapy, and improving patient comfort. Traditional deep learning (DL) approaches for accelerated dynamic MRI reconstruction typically rely on predefined or random subsampling patterns, applied uniformly across all temporal phases. This standard practice overlooks the potential benefits of leveraging temporal correlations and lacks the adaptability required for case-specific subsampling optimization, which holds the potential for maximizing reconstruction quality. Addressing this gap, we present a novel end-to-end framework for adaptive dynamic MRI subsampling and reconstruction. Our pipeline integrates a DL-based adaptive sampler, generating case-specific dynamic subsampling patterns, trained end-to-end with a state-of-the-art 2D dynamic reconstruction network, namely vSHARP, which effectively reconstructs the adaptive dynamic subsampled data into a moving image. Our method is assessed using dynamic cine cardiac MRI data, comparing its performance against vSHARP models that employ common subsampling trajectories, and pipelines trained to optimize dataset-specific sampling schemes alongside vSHARP reconstruction. Our results indicate superior reconstruction quality, particularly at high accelerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10346v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Yiasemis, Jan-Jakob Sonke, Jonas Teuwen</dc:creator>
    </item>
    <item>
      <title>Optimization and Validation of the DESIGNER dMRI preprocessing pipeline in white matter aging</title>
      <link>https://arxiv.org/abs/2305.14445</link>
      <description>arXiv:2305.14445v2 Announce Type: replace 
Abstract: Various diffusion MRI (dMRI) preprocessing pipelines are currently available to yield more accurate diffusion parameters. Here, we evaluated accuracy and robustness of the optimized Diffusion parameter EStImation with Gibbs and NoisE Removal (DESIGNER) pipeline in a large clinical dMRI dataset and using ground truth phantoms. DESIGNER has been modified to improve denoising and target Gibbs ringing for partial Fourier acquisitions. We compared the revisited DESIGNER (Dv2) (including denoising, Gibbs removal, correction for motion, EPI distortion, and eddy currents) against the original DESIGNER (Dv1) pipeline, minimal preprocessing (including correction for motion, EPI distortion, and eddy currents only), and no preprocessing on a large clinical dMRI dataset of 524 control subjects with ages between 25 and 75 years old. We evaluated the effect of specific processing steps on age correlations in white matter with DTI and DKI metrics. We also evaluated the added effect of minimal Gaussian smoothing to deal with noise and to reduce outliers in parameter maps compared to DESIGNER (Dv2)'s noise removal method. Moreover, DESIGNER (Dv2)'s updated noise and Gibbs removal methods were assessed using ground truth dMRI phantom to evaluate accuracy. Results show age correlation in white matter with DTI and DKI metrics were affected by the preprocessing pipeline, causing systematic differences in absolute parameter values and loss or gain of statistical significance. Both in clinical dMRI and ground truth phantoms, DESIGNER (Dv2) pipeline resulted in the smallest number of outlier voxels and improved accuracy in DTI and DKI metrics as noise was reduced and Gibbs removal was improved. Thus, DESIGNER (Dv2) provides more accurate and robust DTI and DKI parameter maps as compared to no preprocessing or minimal preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14445v2</guid>
      <category>physics.med-ph</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Chen, Benjamin Ades-Aron, Hong-Hsi Lee, Subah Mehrin, Michelle Pang, Dmitry S. Novikov, Jelle Veraart, Els Fieremans</dc:creator>
    </item>
    <item>
      <title>Diffusion Posterior Sampling for Synergistic Reconstruction in Spectral Computed Tomography</title>
      <link>https://arxiv.org/abs/2403.06308</link>
      <description>arXiv:2403.06308v2 Announce Type: replace 
Abstract: Using recent advances in generative artificial intelligence (AI) brought by diffusion models, this paper introduces a new synergistic method for spectral computed tomography (CT) reconstruction. Diffusion models define a neural network to approximate the gradient of the log-density of the training data, which is then used to generate new images similar to the training ones. Following the inverse problem paradigm, we propose to adapt this generative process to synergistically reconstruct multiple images at different energy bins from multiple measurements. The experiments suggest that using multiple energy bins simultaneously improves the reconstruction by inverse diffusion and outperforms state-of-the-art synergistic reconstruction techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06308v2</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corentin Vazia, Alexandre Bousse, B\'eatrice Vedel, Franck Vermet, Zhihan Wang, Thore Dassow, Jean-Pierre Tasu, Dimitris Visvikis, Jacques Froment</dc:creator>
    </item>
  </channel>
</rss>
