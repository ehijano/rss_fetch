<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 02:56:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Interval-Valued Fuzzy Fault Tree Analysis through Qualitative Data Processing and its Applications in Marine Operations</title>
      <link>https://arxiv.org/abs/2411.15249</link>
      <description>arXiv:2411.15249v1 Announce Type: new 
Abstract: Marine accidents highlight the crucial need for human safety. They result in loss of life, environmental harm, and significant economic costs, emphasizing the importance of being proactive and taking precautionary steps. This study aims to identify the root causes of accidents, to develop effective strategies for preventing them. Due to the lack of accurate quantitative data or reliable probability information, we employ qualitative approaches to assess the reliability of complex systems. We collect expert judgments regarding the failure likelihood of each basic event and aggregate those opinions using the Similarity-based Aggregation Method (SAM) to form a collective assessment. In SAM, we convert expert opinions into failure probability using interval-valued triangular fuzzy numbers. Since each expert possesses different knowledge and various levels of experience, we need to assign weights to their opinions to reflect their relative expertise. We employ the Best-Worst Method (BWM) to calculate the weights of each criterion, and then use the weighting scores to determine the weights of each expert. Ranking of basic events according to their criticality is a crucial step, and in this study, we use the FVI measure to prioritize and rank these events according to their criticality level. To demonstrate the effectiveness and validity of our proposed methodology, we apply our method to two case studies: (1) chemical cargo contamination, and (2) the loss of ship steering ability. These case studies serve as examples to illustrate the practicality and utility of our approach in evaluating criticality and assessing risk in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15249v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hitesh Khungla, Kulbir Singh, Mohit Kumar</dc:creator>
    </item>
    <item>
      <title>Regional consistency evaluation and sample size calculation under two MRCTs</title>
      <link>https://arxiv.org/abs/2411.15567</link>
      <description>arXiv:2411.15567v1 Announce Type: new 
Abstract: Multi-regional clinical trial (MRCT) has been common practice for drug development and global registration. The FDA guidance "Demonstrating Substantial Evidence of Effectiveness for Human Drug and Biological Products Guidance for Industry" (FDA, 2019) requires that substantial evidence of effectiveness of a drug/biologic product to be demonstrated for market approval. In the situations where two pivotal MRCTs are needed to establish effectiveness of a specific indication for a drug or biological product, a systematic approach of consistency evaluation for regional effect is crucial. In this paper, we first present some existing regional consistency evaluations in a unified way that facilitates regional sample size calculation under the simple fixed effect model. Second, we extend the two commonly used consistency assessment criteria of MHLW (2007) in the context of two MRCTs and provide their evaluation and regional sample size calculation. Numerical studies demonstrate the proposed regional sample size attains the desired probability of showing regional consistency. A hypothetical example is provided for illustration of application. We provide an R package for implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15567v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunhai Qing, Xinru Ren, Jin Xu</dc:creator>
    </item>
    <item>
      <title>Corrected Support Vector Regression for intraday point forecasting of prices in the continuous power market</title>
      <link>https://arxiv.org/abs/2411.16237</link>
      <description>arXiv:2411.16237v1 Announce Type: new 
Abstract: In this paper, we develop a new approach to the very short-term point forecasting of electricity prices in the continuous market. It is based on the Support Vector Regression with a kernel correction built on additional forecast of dependent variable. We test the proposed approach on a dataset from the German intraday continuous market and compare its forecast accuracy with several benchmarks: classic SVR, the LASSO model, Random Forest and the na\"{i}ve forecast. The analysis is performed for different forecasting horizons, deliveries, and lead times. We train the models on three expert sets of explanatory variables and apply the forecast averaging schemes. Overall, the proposed cSVR approach with the averaging scheme yields the highest forecast accuracy, being at the same time the fastest from the considered benchmarks. The highest improvement in forecast accuracy is obtained for deliveries in the morning and evening peaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16237v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrzej Pu\'c, Joanna Janczura</dc:creator>
    </item>
    <item>
      <title>When Is Heterogeneity Actionable for Personalization?</title>
      <link>https://arxiv.org/abs/2411.16552</link>
      <description>arXiv:2411.16552v1 Announce Type: new 
Abstract: Targeting and personalization policies can be used to improve outcomes beyond the uniform policy that assigns the best performing treatment in an A/B test to everyone. Personalization relies on the presence of heterogeneity of treatment effects, yet, as we show in this paper, heterogeneity alone is not sufficient for personalization to be successful. We develop a statistical model to quantify "actionable heterogeneity," or the conditions when personalization is likely to outperform the best uniform policy. We show that actionable heterogeneity can be visualized as crossover interactions in outcomes across treatments and depends on three population-level parameters: within-treatment heterogeneity, cross-treatment correlation, and the variation in average responses. Our model can be used to predict the expected gain from personalization prior to running an experiment and also allows for sensitivity analysis, providing guidance on how changing treatments can affect the personalization gain. To validate our model, we apply five common personalization approaches to two large-scale field experiments with many interventions that encouraged flu vaccination. We find an 18% gain from personalization in one and a more modest 4% gain in the other, which is consistent with our model. Counterfactual analysis shows that this difference in the gains from personalization is driven by a drastic difference in within-treatment heterogeneity. However, reducing cross-treatment correlation holds a larger potential to further increase personalization gains. Our findings provide a framework for assessing the potential from personalization and offer practical recommendations for improving gains from targeting in multi-intervention settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16552v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anya Shchetkina, Ron Berman</dc:creator>
    </item>
    <item>
      <title>The ultimate issue error: mistaking parameters for hypotheses</title>
      <link>https://arxiv.org/abs/2411.15398</link>
      <description>arXiv:2411.15398v1 Announce Type: cross 
Abstract: In a criminal investigation, an inferential error occurs when the probability that a suspect is the source of some evidence -- such as a fingerprint -- is taken as the probability of guilt. This is known as the ultimate issue error, and the same error occurs in statistical inference when the probability that a parameter equals some value is incorrectly taken to be the probability of a hypothesis. Almost all statistical inference in the social and biological sciences is subject to this error, and replacing every instance of "hypothesis testing" with "parameter testing" in these fields would more accurately describe the target of inference. The relationship between parameter values and quantities derived from them, such as p-values or Bayes factors, have no direct quantitative relationship with scientific hypotheses. Here, we describe the problem, its consequences, and suggest options for improving scientific inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15398v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
    <item>
      <title>Forecasting with Markovian max-stable fields in space and time: An application to wind gust speeds</title>
      <link>https://arxiv.org/abs/2411.15511</link>
      <description>arXiv:2411.15511v1 Announce Type: cross 
Abstract: Hourly maxima of 3-second wind gust speeds are prominent indicators of the severity of wind storms, and accurately forecasting them is thus essential for populations, civil authorities and insurance companies. Space-time max-stable models appear as natural candidates for this, but those explored so far are not suited for forecasting and, more generally, the forecasting literature for max-stable fields is limited. To fill this gap, we consider a specific space-time max-stable model, more precisely a max-autoregressive model with advection, that is well-adapted to model and forecast atmospheric variables. We apply it, as well as our related forecasting strategy, to reanalysis 3-second wind gust data for France in 1999, and show good performance compared to a competitor model. On top of demonstrating the practical relevance of our model, we meticulously study its theoretical properties and show the consistency and asymptotic normality of the space-time pairwise likelihood estimator which is used to calibrate the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15511v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cotsakis, Erwan Koch, Christian-Yann Robert</dc:creator>
    </item>
    <item>
      <title>"All that Glitters": Approaches to Evaluations with Unreliable Model and Human Annotations</title>
      <link>https://arxiv.org/abs/2411.15634</link>
      <description>arXiv:2411.15634v1 Announce Type: cross 
Abstract: "Gold" and "ground truth" human-mediated labels have error. The effects of this error can escape commonly reported metrics of label quality or obscure questions of accuracy, bias, fairness, and usefulness during model evaluation. This study demonstrates methods for answering such questions even in the context of very low reliabilities from expert humans. We analyze human labels, GPT model ratings, and transformer encoder model annotations describing the quality of classroom teaching, an important, expensive, and currently only human task. We answer the question of whether such a task can be automated using two Large Language Model (LLM) architecture families--encoders and GPT decoders, using novel approaches to evaluating label quality across six dimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness. First, we demonstrate that using standard metrics in the presence of poor labels can mask both label and model quality: the encoder family of models achieve state-of-the-art, even "super-human", results across all classroom annotation tasks. But not all these positive results remain after using more rigorous evaluation measures which reveal spurious correlations and nonrandom racial biases across models and humans. This study then expands these methods to estimate how model use would change to human label quality if models were used in a human-in-the-loop context, finding that the variance captured in GPT model labels would worsen reliabilities for humans influenced by these models. We identify areas where some LLMs, within the generalizability of the current data, could improve the quality of expensive human ratings of classroom instruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15634v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Hardy</dc:creator>
    </item>
    <item>
      <title>Improving Pre-Trained Self-Supervised Embeddings Through Effective Entropy Maximization</title>
      <link>https://arxiv.org/abs/2411.15931</link>
      <description>arXiv:2411.15931v1 Announce Type: cross 
Abstract: A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends--whether explicitly or implicitly--upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15931v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deep Chakraborty, Yann LeCun, Tim G. J. Rudner, Erik Learned-Miller</dc:creator>
    </item>
    <item>
      <title>Shortest Path Lengths in Poisson Line Cox Processes: Approximations and Applications</title>
      <link>https://arxiv.org/abs/2411.16441</link>
      <description>arXiv:2411.16441v1 Announce Type: cross 
Abstract: We derive exact expressions for the shortest path length to a point of a Poisson line Cox process (PLCP) from the typical point of the PLCP and from the typical intersection of the underlying Poisson line process (PLP), restricted to a single turn. For the two turns case, we derive a bound on the shortest path length from the typical point and demonstrate conditions under which the bound is tight. We also highlight the line process and point process densities for which the shortest path from the typical intersection under the one turn restriction may be shorter than the shortest path from the typical point under the two turns restriction. Finally, we discuss two applications where our results can be employed for a statistical characterization of system performance: in a re-configurable intelligent surface (RIS) enabled vehicle-to-vehicle (V2V) communication system and in electric vehicle charging point deployment planning in urban streets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16441v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gourab Ghatak, Sanjoy Kumar Jhawar, Martin Haenggi</dc:creator>
    </item>
    <item>
      <title>Dynamic Causal Models of Time-Varying Connectivity</title>
      <link>https://arxiv.org/abs/2411.16582</link>
      <description>arXiv:2411.16582v1 Announce Type: cross 
Abstract: This paper introduces a novel approach for modelling time-varying connectivity in neuroimaging data, focusing on the slow fluctuations in synaptic efficacy that mediate neuronal dynamics. Building on the framework of Dynamic Causal Modelling (DCM), we propose a method that incorporates temporal basis functions into neural models, allowing for the explicit representation of slow parameter changes. This approach balances expressivity and computational efficiency by modelling these fluctuations as a Gaussian process, offering a middle ground between existing methods that either strongly constrain or excessively relax parameter fluctuations. We validate the ensuing model through simulations and real data from an auditory roving oddball paradigm, demonstrating its potential to explain key aspects of brain dynamics. This work aims to equip researchers with a robust tool for investigating time-varying connectivity, particularly in the context of synaptic modulation and its role in both healthy and pathological brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16582v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Karl J. Friston, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics</title>
      <link>https://arxiv.org/abs/2405.09989</link>
      <description>arXiv:2405.09989v2 Announce Type: replace 
Abstract: With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily. However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative. In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds. We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space. A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space. Using molecular fingerprints, a numerical representation of a compound's location within the chemical space, we find that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent. Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound's efficacy, based on two criteria derived from the proposed model. Simulation studies are conducted to demonstrate the suitability of the proposed methods. Our model is demonstrated on a hazard classification problem of organic solvents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09989v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arron Gosnell, Evangelos Evangelou</dc:creator>
    </item>
    <item>
      <title>Integrating Dynamic Correlation Shifts and Weighted Benchmarking in Extreme Value Analysis</title>
      <link>https://arxiv.org/abs/2411.13608</link>
      <description>arXiv:2411.13608v2 Announce Type: replace 
Abstract: This paper presents an innovative approach to Extreme Value Analysis (EVA) by introducing the Extreme Value Dynamic Benchmarking Method (EVDBM). EVDBM integrates extreme value theory to detect extreme events and is coupled with the novel Dynamic Identification of Significant Correlation (DISC)-Thresholding algorithm, which enhances the analysis of key variables under extreme conditions. By integrating return values predicted through EVA into the benchmarking scores, we are able to transform these scores to reflect anticipated conditions more accurately. This provides a more precise picture of how each case is projected to unfold under extreme conditions. As a result, the adjusted scores offer a forward-looking perspective, highlighting potential vulnerabilities and resilience factors for each case in a way that static historical data alone cannot capture. By incorporating both historical and probabilistic elements, the EVDBM algorithm provides a comprehensive benchmarking framework that is adaptable to a range of scenarios and contexts. The methodology is applied to real PV data, revealing critical low - production scenarios and significant correlations between variables, which aid in risk management, infrastructure design, and long-term planning, while also allowing for the comparison of different production plants. The flexibility of EVDBM suggests its potential for broader applications in other sectors where decision-making sensitivity is crucial, offering valuable insights to improve outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13608v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios P. Panagoulias, Elissaios Sarmas, Vangelis Marinakis, Maria Virvou, George A. Tsihrintzis</dc:creator>
    </item>
    <item>
      <title>Generalized bootstrap in the Bures-Wasserstein space</title>
      <link>https://arxiv.org/abs/2111.12612</link>
      <description>arXiv:2111.12612v2 Announce Type: replace-cross 
Abstract: This study focuses on finite-sample inference on the non-linear Bures-Wasserstein manifold and introduces a generalized bootstrap procedure for estimating Bures-Wasserstein barycenters. We provide non-asymptotic statistical guarantees for the resulting bootstrap confidence sets. The proposed approach incorporates classical resampling methods, including the multiplier bootstrap highlighted as a specific example. Additionally, the paper compares bootstrap-based confidence sets with asymptotic sets obtained in the work arXiv:1901.00226v2, evaluating their statistical performance and computational complexities. The methodology is validated through experiments on synthetic datasets and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12612v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey Kroshnin, Vladimir Spokoiny, Alexandra Suvorikova</dc:creator>
    </item>
    <item>
      <title>Regression-based Physics Informed Neural Networks (Reg-PINNs) for Magnetopause Tracking</title>
      <link>https://arxiv.org/abs/2306.09621</link>
      <description>arXiv:2306.09621v5 Announce Type: replace-cross 
Abstract: Previous research in the scientific field has utilized statistical empirical models and machine learning to address fitting challenges. While empirical models have the advantage of numerical generalization, they often sacrifice accuracy. However, conventional machine learning methods can achieve high precision but may lack the desired generalization. The article introduces a Regression-based Physics-Informed Neural Networks (Reg-PINNs), which embeds physics-inspired empirical models into the neural network's loss function, thereby combining the benefits of generalization and high accuracy. The study validates the proposed method using the magnetopause boundary location as the target and explores the feasibility of methods including Shue et al. [1998], a data overfitting model, a fully-connected networks, Reg-PINNs with Shue's model, and Reg-PINNs with the overfitting model. Compared to Shue's model, this technique achieves approximately a 30% reduction in RMSE, presenting a proof-of-concept improved solution for the scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09621v5</guid>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Po-Han Hou, Sung-Chi Hsieh</dc:creator>
    </item>
    <item>
      <title>Causally Sound Priors for Binary Experiments</title>
      <link>https://arxiv.org/abs/2308.13713</link>
      <description>arXiv:2308.13713v3 Announce Type: replace-cross 
Abstract: We introduce the BREASE framework for the Bayesian analysis of randomized controlled trials with a binary treatment and a binary outcome. Approaching the problem from a causal inference perspective, we propose parameterizing the likelihood in terms of the baselinerisk, efficacy, and adverse side effects of the treatment, along with a flexible, yet intuitive and tractable jointly independent beta prior distribution on these parameters, which we show to be a generalization of the Dirichlet prior for the joint distribution of potential outcomes. Our approach has a number of desirable characteristics when compared to current mainstream alternatives: (i) it naturally induces prior dependence between expected outcomes in the treatment and control groups; (ii) as the baseline risk, efficacy and risk of adverse side effects are quantities commonly present in the clinicians' vocabulary, the hyperparameters of the prior are directly interpretable, thus facilitating the elicitation of prior knowledge and sensitivity analysis; and (iii) we provide analytical formulae for the marginal likelihood, Bayes factor, and other posterior quantities, as well as an exact posterior sampling algorithm and an accurate and fast data-augmented Gibbs sampler in cases where traditional MCMC fails. Empirical examples demonstrate the utility of our methods for estimation, hypothesis testing, and sensitivity analysis of treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13713v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. Irons, Carlos Cinelli</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Entity Representation for Medicinal Synergy Prediction</title>
      <link>https://arxiv.org/abs/2406.10778</link>
      <description>arXiv:2406.10778v2 Announce Type: replace-cross 
Abstract: Medicinal synergy prediction is a powerful tool in drug discovery and development that harnesses the principles of combination therapy to enhance therapeutic outcomes by improving efficacy, reducing toxicity, and preventing drug resistance. While a myriad of computational methods has emerged for predicting synergistic drug combinations, a large portion of them may overlook the intricate, yet critical relationships between various entities in drug interaction networks, such as drugs, cell lines, and diseases. These relationships are complex and multidimensional, requiring sophisticated modeling to capture nuanced interplay that can significantly influence therapeutic efficacy. We introduce a salient deep hypergraph learning method, namely, Heterogeneous Entity Representation for MEdicinal Synergy prediction (HERMES), to predict anti-cancer drug synergy. HERMES integrates heterogeneous data sources, encompassing drug, cell line, and disease information, to provide a comprehensive understanding of the interactions involved. By leveraging advanced hypergraph neural networks with gated residual mechanisms, HERMES can effectively learn complex relationships/interactions within the data. Our results show HERMES demonstrates state-of-the-art performance, particularly in forecasting new drug combinations, significantly surpassing previous methods. This advancement underscores the potential of HERMES to facilitate more effective and precise drug combination predictions, thereby enhancing the development of novel therapeutic strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10778v2</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Wu, Jun Wen, Mingyuan Yan, Anqi Dong, Shuai Gao, Ren Wang, Can Chen</dc:creator>
    </item>
    <item>
      <title>Ridge, lasso, and elastic-net estimations of the modified Poisson and least-squares regressions for binary outcome data</title>
      <link>https://arxiv.org/abs/2408.13474</link>
      <description>arXiv:2408.13474v2 Announce Type: replace-cross 
Abstract: Logistic regression is a standard method in multivariate analysis for binary outcome data in epidemiological and clinical studies; however, the resultant odds-ratio estimates fail to provide directly interpretable effect measures. The modified Poisson and least-squares regressions are alternative standard methods that can provide risk-ratio and risk difference estimates without computational problems. However, the bias and invalid inference problems of these regression analyses under small or sparse data conditions (i.e.,the "separation" problem) have been insufficiently investigated. We show that the separation problem can adversely affect the inferences of the modified Poisson and least squares regressions, and to address these issues, we apply the ridge, lasso, and elastic-net estimating approaches to the two regression methods. As the methods are not founded on the maximum likelihood principle, we propose regularized quasi-likelihood approaches based on the estimating equations for these generalized linear models. The methods provide stable shrinkage estimates of risk ratios and risk differences even under separation conditions, and the lasso and elastic-net approaches enable simultaneous variable selection. We provide a bootstrap method to calculate the confidence intervals on the basis of the regularized quasi-likelihood estimation. The proposed methods are applied to a hematopoietic stem cell transplantation cohort study and the National Child Development Survey. We also provide an R package, regconfint, to implement these methods with simple commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13474v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Kitano, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Negative Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v2 Announce Type: replace-cross 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using negative control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects, which motivates our semiparametric inference method. Our method extends to projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v4 Announce Type: replace-cross 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v2 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
