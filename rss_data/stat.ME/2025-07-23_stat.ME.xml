<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian implementation of Targeted Maximum Likelihood Estimation for uncertainty quantification in causal effect estimation</title>
      <link>https://arxiv.org/abs/2507.15909</link>
      <description>arXiv:2507.15909v1 Announce Type: new 
Abstract: Robust decision making involves making decisions in the presence of uncertainty and is often used in critical domains such as healthcare, supply chains, and finance. Causality plays a crucial role in decision-making as it predicts the change in an outcome (usually a key performance indicator) due to a treatment (also called an intervention). To facilitate robust decision making using causality, this paper proposes three Bayesian approaches of the popular Targeted Maximum Likelihood Estimation (TMLE) algorithm, a flexible semi-parametric double robust estimator, for a probabilistic quantification of uncertainty in causal effects with binary treatment, and binary and continuous outcomes. In the first two approaches, the three TMLE models (outcome, treatment, and fluctuation) are trained sequentially. Since Bayesian implementation of treatment and outcome yields probabilistic predictions, the first approach uses mean predictions, while the second approach uses both the mean and standard deviation of predictions for training the fluctuation model (targeting step). The third approach trains all three models simultaneously through a Bayesian network (called BN-TMLE in this paper). The proposed approaches were demonstrated for two examples with binary and continuous outcomes and validated against classical implementations. This paper also investigated the effect of data sizes and model misspecifications on causal effect estimation using the BN-TMLE approach. Results showed that the proposed BN-TMLE outperformed classical implementations in small data regimes and performed similarly in large data regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15909v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saideep Nannapaneni, Joseph Sakaya, Kyle Caron, Pedro HM Albuquerque, Zaid Tashman</dc:creator>
    </item>
    <item>
      <title>Comment on "Average Hazard as Harmonic Mean" by Chiba</title>
      <link>https://arxiv.org/abs/2507.15985</link>
      <description>arXiv:2507.15985v1 Announce Type: new 
Abstract: In a recent article published in Pharmaceutical Statistics, Chiba proposed a reinterpretation of the average hazard as a harmonic mean of the hazard function and questioned the validity of the Kaplan-Meier plug-in estimator when the truncation time does not coincide with an observed event time. In this commentary, we examine the arguments presented and highlight several points that warrant clarification. Through simulation studies, we further show that the plug-in estimator provides reliable estimates across a range of truncation times, even in small samples. These support the continued utilization of the Kaplan-Meier plug-in estimator for the average hazard and help clarify its proper interpretation and implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15985v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajime Uno, Lu Tian, Miki Horiguchi, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Predictive inference for discrete-valued time series</title>
      <link>https://arxiv.org/abs/2507.16035</link>
      <description>arXiv:2507.16035v1 Announce Type: new 
Abstract: For discrete-valued time series, predictive inference cannot be implemented through the construction of prediction intervals to some predetermined coverage level, as this is the case for real-valued time series. To address this problem, we propose to reverse the construction principle by considering preselected sets of interest and estimating the probability that a future observation of the process falls into these sets. The accuracy of the prediction is then evaluated by quantifying the uncertainty associated with estimation of these predictive probabilities. We consider parametric and non-parametric approaches and derive asymptotic theory for the estimators involved. Suitable bootstrap approaches to evaluate the distribution of the estimators considered also are introduced. They have the advantage to imitate the distributions of interest under different possible settings, including the practical important case where uncertainty holds true about the correctness of a parametric model used for prediction. Theoretical justification of the bootstrap is given, which also requires investigation of asymptotic properties of parameter estimators under model misspecification. We elaborate on bootstrap implementations under different scenarios and focus on parametric prediction using INAR and INARCH models and (conditional) maximum likelihood estimators. Simulations investigate the finite sample performance of the predictive method developed and applications to real life data sets are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16035v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxime Faymonville, Carsten Jentsch, Efstathios Paparoditis</dc:creator>
    </item>
    <item>
      <title>Bayesian unanchored additive models for component network meta-analysis</title>
      <link>https://arxiv.org/abs/2507.16047</link>
      <description>arXiv:2507.16047v1 Announce Type: new 
Abstract: Component network meta-analysis (CNMA) models are an extension of standard network meta-analysis (NMA) models which account for the use of multicomponent treatments in the network. This article contributes innovatively to several statistical aspects of CNMA. First, by introducing a unified notation, we establish that currently available methods differ in the way they assume additivity, an important distinction that has been overlooked so far in the literature. In particular, one model uses a more restrictive form of additivity than the other which we term an anchored and unanchored model, respectively. We show that an anchored model can provide a poor fit to the data if it is misspecified. Second, given that Bayesian models are often preferred by practitioners, we develop two novel unanchored Bayesian CNMA models presented under the unified notation. An extensive simulation study examining bias, coverage probabilities, and treatment rankings confirms the favorable performance of the novel models. This is the first simulation study to compare the statistical properties of CNMA models in the literature. Finally, the use of our novel models is demonstrated on a real dataset, and the results of CNMA models on the dataset are compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16047v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.9520</arxiv:DOI>
      <arxiv:journal_reference>Stat Med 41(22): 4444-4466 (2022)</arxiv:journal_reference>
      <dc:creator>Augustine Wigle, Audrey B\'eliveau</dc:creator>
    </item>
    <item>
      <title>Evaluating virtual-control-augmented trials for reproducing treatment effect from original RCTs</title>
      <link>https://arxiv.org/abs/2507.16048</link>
      <description>arXiv:2507.16048v1 Announce Type: new 
Abstract: This study investigates the use of virtual patient data to augment control arms in randomised controlled trials (RCTs). Using data from the IST and IST3 trials, we simulated RCTs in which the recruitment in the control arms would stop after a fraction of the initially planned sample size, and would be completed by virtual patients generated by CTGAN and TVAE, two AI algorithms trained on the recruited control patients. In IST, the absolute risk difference(ARD) on death or dependency at 14 days was -0.012 (SE 0.014). Completing the control arm by CTGAN-generated virtual patients after the recruitment of 10% and 50% of participants, yielded an ARD of 0.004 (SE 0.014) (relative difference 133%) and -0.021 (SE 0.014) (relative difference 76%), respectively. Results were comparable with IST3 or TVAE. This is the first empirical demonstration of the risk of errors and misleading conclusions associated with generating virtual controls solely from trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16048v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Fernandes, Rapha\"el Porcher, Viet-Thi Tran, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Recursive Equations For Imputation Of Missing Not At Random Data With Sparse Pattern Support</title>
      <link>https://arxiv.org/abs/2507.16107</link>
      <description>arXiv:2507.16107v1 Announce Type: new 
Abstract: A common approach for handling missing values in data analysis pipelines is multiple imputation via software packages such as MICE (Van Buuren and Groothuis-Oudshoorn, 2011) and Amelia (Honaker et al., 2011). These packages typically assume the data are missing at random (MAR), and impose parametric or smoothing assumptions upon the imputing distributions in a way that allows imputation to proceed even if not all missingness patterns have support in the data. Such assumptions are unrealistic in practice, and induce model misspecification bias on any analysis performed after such imputation.
  In this paper, we provide a principled alternative. Specifically, we develop a new characterization for the full data law in graphical models of missing data. This characterization is constructive, is easily adapted for the calculation of imputation distributions for both MAR and MNAR (missing not at random) mechanisms, and is able to handle lack of support for certain patterns of missingness. We use this characterization to develop a new imputation algorithm -- Multivariate Imputation via Supported Pattern Recursion (MISPR) -- which uses Gibbs sampling, by analogy with the Multivariate Imputation with Chained Equations (MICE) algorithm, but which is consistent under both MAR and MNAR settings, and is able to handle missing data patterns with no support without imposing additional assumptions beyond those already imposed by the missing data model itself.
  In simulations, we show MISPR obtains comparable results to MICE when data are MAR, and superior, less biased results when data are MNAR. Our characterization and imputation algorithm based on it are a step towards making principled missing data methods more practical in applied settings, where the data are likely both MNAR and sufficiently high dimensional to yield missing data patterns with no support at available sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16107v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trung Phung, Kyle Reese, Ilya Shpitser, Rohit Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Density Prediction of Income Distribution Based on Mixed Frequency Data</title>
      <link>https://arxiv.org/abs/2507.16150</link>
      <description>arXiv:2507.16150v1 Announce Type: new 
Abstract: Modeling large dependent datasets in modern time series analysis is a crucial research area. One effective approach to handle such datasets is to transform the observations into density functions and apply statistical methods for further analysis. Income distribution forecasting, a common application scenario, benefits from predicting density functions as it accounts for uncertainty around point estimates, leading to more informed policy formulation. However, predictive modeling becomes challenging when dealing with mixed-frequency data. To address this challenge, this paper introduces a mixed data sampling regression model for probability density functions (PDF-MIDAS). To mitigate variance inflation caused by high-frequency prediction variables, we utilize exponential Almon polynomials with fewer parameters to regularize the coefficient structure. Additionally, we propose an iterative estimation method based on quadratic programming and the BFGS algorithm. Simulation analyses demonstrate that as the sample size for estimating density functions and observation length increase, the estimator approaches the true value. Real data analysis reveals that compared to single-sequence prediction models, PDF-MIDAS incorporating high-frequency exogenous variables offers a wider range of application scenarios with superior fitting and prediction performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yinzhi Wang, Yingqiu Zhu, Ben-Chang Shia, Lei Qin</dc:creator>
    </item>
    <item>
      <title>Estimating the variance-covariance matrix of two-step estimates of latent variable models: A general simulation-based approach</title>
      <link>https://arxiv.org/abs/2507.16324</link>
      <description>arXiv:2507.16324v1 Announce Type: new 
Abstract: We propose a general procedure for estimating the variance-covariance matrix of two-step estimates of structural parameters in latent variable models. The method is partially simulation-based, in that it includes drawing simulated values of the measurement parameters of the model from their sampling distribution obtained from the first step of two-step estimation, and using them to quantify part of the variability in the parameter estimates from the second step. This is asymptotically equal with the standard closed-form estimate of the variance-covariance matrix, but it avoids the need to evaluate a cross-derivative matrix which is the most inconvenient element of the standard estimate. The method can be applied to any types of latent variable models. We present it in more detail in the context of two common models where the measurement items are categorical: latent class models with categorical latent variables and latent trait models with continuous latent variables. The good performance of the proposed procedure is demonstrated with simulation studies and illustrated with two applied examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16324v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Di Mari, Jouni Kuha</dc:creator>
    </item>
    <item>
      <title>A Bayesian Geoadditive Model for Spatial Disaggregation</title>
      <link>https://arxiv.org/abs/2507.16376</link>
      <description>arXiv:2507.16376v1 Announce Type: new 
Abstract: We present a novel Bayesian spatial disaggregation model for count data, providing fast and flexible inference at high resolution. First, it incorporates non-linear covariate effects using penalized splines, a flexible approach that is not typically included in existing spatial disaggregation methods. Additionally, it employs a spline-based low-rank kriging approximation for modeling spatial dependencies. The use of Laplace approximation provides computational advantages over traditional Markov Chain Monte Carlo (MCMC) approaches, facilitating scalability to large datasets. We explore two estimation strategies: one using the exact likelihood and another leveraging a spatially discrete approximation for enhanced computational efficiency. Simulation studies demonstrate that both methods perform well, with the approximate method offering significant computational gains. We illustrate the applicability of our model by disaggregating disease rates in the United Kingdom and Belgium, showcasing its potential for generating high-resolution risk maps. By combining flexibility in covariate modeling, computational efficiency and ease of implementation, our approach offers a practical and effective framework for spatial disaggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16376v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sara Rutten, Thomas Neyens, Elisa Duarte, Christel Faes</dc:creator>
    </item>
    <item>
      <title>A Bayesian block maxima over threshold approach applied to corrosion assessment in heat exchanger tubes</title>
      <link>https://arxiv.org/abs/2507.16416</link>
      <description>arXiv:2507.16416v1 Announce Type: new 
Abstract: Corrosion poses a hurdle for numerous industrial processes, and though corrosion can be measured directly, statistical approaches are often required to either correct for measurement error or extrapolate estimates of corrosion severity where measurements are unavailable. This article considers corrosion in heat exchangers tubes, where corrosion is typically reported in terms of maximum pit depth per inspected tube, and only a small proportion of tubes are inspected, suggesting extreme value theory (EVT) as suitable methodology. However, in data analysis of heat exchanger data, shallow tube-maxima pits often cannot be considered as extreme; although previous EVT approaches assume all the data are extreme. We overcome this by introducing a threshold - suggesting a block maxima over threshold approach, which leads to more robust inference around model parameters and predicted maximum pit depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16416v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jess Spearing, Jarno Hartog</dc:creator>
    </item>
    <item>
      <title>Effective sample size estimation based on concordance between p-value and posterior probability of the null hypothesis</title>
      <link>https://arxiv.org/abs/2507.16422</link>
      <description>arXiv:2507.16422v1 Announce Type: new 
Abstract: Estimating the effective sample size (ESS) of a prior distribution is an age-old yet pivotal challenge, with great implications for clinical trials and various biomedical applications. Although numerous endeavors have been dedicated to this pursuit, most of them neglect the likelihood context in which the prior is embedded, thereby considering all priors as "beneficial". In the limited studies of addressing harmful priors, specifying a baseline prior remains an indispensable step. In this paper, by means of the elegant bridge between the p-value and the posterior probability of the null hypothesis, we propose a new ESS estimation method based on p-value in the framework of hypothesis testing, expanding the scope of existing ESS estimation methods in three key aspects:
  (i) We address the specific likelihood context of the prior, enabling the possibility of negative ESS values in case of prior-likelihood disconcordance;
  (ii) By leveraging the well-established bridge between the frequentist and Bayesian configurations under noninformative priors, there is no need to specify a baseline prior which incurs another criticism of subjectivity;
  (iii) By incorporating ESS into the hypothesis testing framework, our $p$-value ESS estimation method transcends the conventional one-ESS-one-prior paradigm and accommodates one-ESS-multiple-priors paradigm, where the sole ESS may reflect the collaborative impact of multiple priors in diverse contexts.
  Through comprehensive simulation analyses, we demonstrate the superior performance of the p-value ESS estimation method in comparison with existing approaches. Furthermore, by applying this approach to an expression quantitative trait loci (eQTL) data analysis, we show the effectiveness of informative priors in uncovering gene eQTL loci.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16422v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Wang, Yan Dora Zhang, Guosheng Yin</dc:creator>
    </item>
    <item>
      <title>Adaptive Multi-task Learning for Multi-sector Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2507.16433</link>
      <description>arXiv:2507.16433v1 Announce Type: new 
Abstract: Accurate transfer of information across multiple sectors to enhance model estimation is both significant and challenging in multi-sector portfolio optimization involving a large number of assets in different classes. Within the framework of factor modeling, we propose a novel data-adaptive multi-task learning methodology that quantifies and learns the relatedness among the principal temporal subspaces (spanned by factors) across multiple sectors under study. This approach not only improves the simultaneous estimation of multiple factor models but also enhances multi-sector portfolio optimization, which heavily depends on the accurate recovery of these factor models. Additionally, a novel and easy-to-implement algorithm, termed projection-penalized principal component analysis, is developed to accomplish the multi-task learning procedure. Diverse simulation designs and practical application on daily return data from Russell 3000 index demonstrate the advantages of multi-task learning methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16433v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingliang Fan, Ruike Wu, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>Bayesian Variational Inference for Mixed Data Mixture Models</title>
      <link>https://arxiv.org/abs/2507.16545</link>
      <description>arXiv:2507.16545v1 Announce Type: new 
Abstract: Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. In addition, we show that the posterior means from CAVI for this model converge to the true parameter value as the sample size n tends to infinity, providing theoretical justification for our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16545v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junyang Wang, James Bennett, Victor Lhoste, Sarah Filippi</dc:creator>
    </item>
    <item>
      <title>Estimating Transition Rates in Two-State Non-Homogeneous Markov Jump Processes with Intermittent Observations: A Pseudo-Marginal McMC Approach via Honest Times</title>
      <link>https://arxiv.org/abs/2507.16603</link>
      <description>arXiv:2507.16603v1 Announce Type: new 
Abstract: A possibly time-dependent transition intensity matrix or generator $(Q(t))$ characterizes the law of a Markov jump process (MP). For a time homogeneous MP, the transition probability matrix (TPM) can be expressed as a matrix exponential of $Q$. However, when dealing with a time non-homogeneous MP, there is often no simple analytical form of the TPM in terms of $Q(t)$, unless they all commute. This poses a challenge because when a continuous MP is observed intermittently, a TPM is required to build a likelihood. In this paper, we show that the estimation of the transition intensities of a two-state nonhomogeneous Markov model can be carried out by augmenting the intermittent observations with honest random times associated with two independent driving Poisson point processes, and that sampling the full path is not required. We propose a pseudo-marginal McMC algorithm to estimate the transition rates using the augmented data. Finally, we illustrate our approach by simulating a continuous MP and by using observed (intermittent) time grids extracted from real clinical visits data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16603v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dario Gasbarra, Sangita Kulathinal, Etienne Sebag</dc:creator>
    </item>
    <item>
      <title>Power Studies For Two-sample Methods For Multivariate Data</title>
      <link>https://arxiv.org/abs/2507.16630</link>
      <description>arXiv:2507.16630v1 Announce Type: new 
Abstract: We present the results of a large number of simulation studies regarding the power of various non-parametric two-sample tests for multivariate data. This includes both continuous and discrete data. In general no single method can be relied upon to provide good power, any one method may be quite good for some combination of null hypothesis and alternative and may fail badly for another. Based on the results of these studies we propose a fairly small number of methods chosen such that for any of the case studies included here at least one of the methods has good power. The studies were carried out using the R package MD2sample, available from CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16630v1</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Rolke</dc:creator>
    </item>
    <item>
      <title>On Causal Inference for the Survivor Function</title>
      <link>https://arxiv.org/abs/2507.16691</link>
      <description>arXiv:2507.16691v1 Announce Type: new 
Abstract: In this expository paper, we consider the problem of causal inference and efficient estimation for the counterfactual survivor function. This problem has previously been considered in the literature in several papers, each relying on the imposition of conditions meant to identify the desired estimand from the observed data. These conditions, generally referred to as either implying or satisfying coarsening at random, are inconsistently imposed across this literature and, in all cases, fail to imply coarsening at random. We establish the first general characterization of coarsening at random, and also sequential coarsening at random, for this estimation problem. Other contributions include the first general characterization of the set of all influence functions for the counterfactual survival probability under sequential coarsening at random, and the corresponding nonparametric efficient influence function. These characterizations are general in that neither impose continuity assumptions on either the underlying failure or censoring time distributions. We further show how the latter compares to alternative forms recently derived in the literature, including establishing the pointwise equivalence of the influence functions for our nonparametric efficient estimator and that recently given in Westling et al (2024, Journal of the American Statistical Association).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16691v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Ashkan Ertefaie, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>Bootstrapped Control Limits for Score-Based Concept Drift Control Charts</title>
      <link>https://arxiv.org/abs/2507.16749</link>
      <description>arXiv:2507.16749v1 Announce Type: new 
Abstract: Monitoring for changes in a predictive relationship represented by a fitted supervised learning model (aka concept drift detection) is a widespread problem, e.g., for retrospective analysis to determine whether the predictive relationship was stable over the training data, for prospective analysis to determine when it is time to update the predictive model, for quality control of processes whose behavior can be characterized by a predictive relationship, etc. A general and powerful Fisher score-based concept drift approach has recently been proposed, in which concept drift detection reduces to detecting changes in the mean of the model's score vector using a multivariate exponentially weighted moving average (MEWMA). To implement the approach, the initial data must be split into two subsets. The first subset serves as the training sample to which the model is fit, and the second subset serves as an out-of-sample test set from which the MEWMA control limit (CL) is determined. In this paper, we develop a novel bootstrap procedure for computing the CL. Our bootstrap CL provides much more accurate control of false-alarm rate, especially when the sample size and/or false-alarm rate is small. It also allows the entire initial sample to be used for training, resulting in a more accurate fitted supervised learning model. We show that a standard nested bootstrap (inner loop accounting for future data variability and outer loop accounting for training sample variability) substantially underestimates variability and develop a 632-like correction that appropriately accounts for this. We demonstrate the advantages with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16749v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiezhong Wu, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Inference for Discretely Observed Continuous Time Markov Chains</title>
      <link>https://arxiv.org/abs/2507.16756</link>
      <description>arXiv:2507.16756v1 Announce Type: new 
Abstract: Inference for continuous-time Markov chains (CTMCs) becomes challenging when the process is only observed at discrete time points. The exact likelihood is intractable, and existing methods often struggle even in medium-dimensional state-spaces. We propose a scalable Bayesian framework for CTMC inference based on a pseudo-likelihood that bypasses the need for the full intractable likelihood. Our approach jointly estimates the probability transition matrix and a biorthogonal spectral decomposition of the generator, enabling an efficient Gibbs sampling procedure that obeys embeddability. Existing methods typically integrate out the unobserved transitions, which becomes computationally burdensome as the number of data or dimensions increase. The computational cost of our method is near-invariant in the number of data and scales well to medium-high dimensions. We justify our pseudo-likelihood approach by establishing theoretical guarantees, including a Bernstein-von Mises theorem for the probability transition matrix and posterior consistency for the spectral parameters of the generator. Through simulation and applications, we showcase the flexibility and robustness of our approach, offering a tractable and scalable approach to Bayesian inference for CTMCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16756v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Tang, Lachlan Astfalck, David Dunson</dc:creator>
    </item>
    <item>
      <title>Structured linear factor models for tail dependence</title>
      <link>https://arxiv.org/abs/2507.16340</link>
      <description>arXiv:2507.16340v1 Announce Type: cross 
Abstract: A common object to describe the extremal dependence of a $d$-variate random vector $X$ is the stable tail dependence function $L$. Various parametric models have emerged, with a popular subclass consisting of those stable tail dependence functions that arise for linear and max-linear factor models with heavy tailed factors. The stable tail dependence function is then parameterized by a $d \times K$ matrix $A$, where $K$ is the number of factors and where $A$ can be interpreted as a factor loading matrix. We study estimation of $L$ under an additional assumption on $A$ called the `pure variable assumption'. Both $K \in \{1, \dots, d\}$ and $A \in [0, \infty)^{d \times K}$ are treated as unknown, which constitutes an unconventional parameter space that does not fit into common estimation frameworks. We suggest two algorithms that allow to estimate $K$ and $A$, and provide finite sample guarantees for both algorithms. Remarkably, the guarantees allow for the case where the dimension $d$ is larger than the sample size $n$. The results are illustrated with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16340v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Boulin, Axel B\"ucher</dc:creator>
    </item>
    <item>
      <title>Structural Effect and Spectral Enhancement of High-Dimensional Regularized Linear Discriminant Analysis</title>
      <link>https://arxiv.org/abs/2507.16682</link>
      <description>arXiv:2507.16682v1 Announce Type: cross 
Abstract: Regularized linear discriminant analysis (RLDA) is a widely used tool for classification and dimensionality reduction, but its performance in high-dimensional scenarios is inconsistent. Existing theoretical analyses of RLDA often lack clear insight into how data structure affects classification performance. To address this issue, we derive a non-asymptotic approximation of the misclassification rate and thus analyze the structural effect and structural adjustment strategies of RLDA. Based on this, we propose the Spectral Enhanced Discriminant Analysis (SEDA) algorithm, which optimizes the data structure by adjusting the spiked eigenvalues of the population covariance matrix. By developing a new theoretical result on eigenvectors in random matrix theory, we derive an asymptotic approximation on the misclassification rate of SEDA. The bias correction algorithm and parameter selection strategy are then obtained. Experiments on synthetic and real datasets show that SEDA achieves higher classification accuracy and dimensionality reduction compared to existing LDA methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16682v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghan Zhang, Zhangni Pu, Lu Yan, Jiang Hu</dc:creator>
    </item>
    <item>
      <title>Bridged Posterior: Optimization, Profile Likelihood and a New Approach to Generalized Bayes</title>
      <link>https://arxiv.org/abs/2403.00968</link>
      <description>arXiv:2403.00968v2 Announce Type: replace 
Abstract: Optimization is widely used in statistics, and often efficiently delivers point estimates on useful spaces involving structural constraints or combinatorial structure. To quantify uncertainty, Gibbs posterior exponentiates the negative loss function to form a posterior density. Nevertheless, Gibbs posteriors are supported in high-dimensional spaces, and do not inherit the computational efficiency or constraint formulations from optimization. In this article, we explore a new generalized Bayes approach, viewing the likelihood as a function of data, parameters, and latent variables conditionally determined by an optimization sub-problem. Marginally, the latent variable given the data remains stochastic, and is characterized by its posterior distribution. This framework, coined bridged posterior, conforms to the Bayesian paradigm. Besides providing a novel generative model, we obtain a positively surprising theoretical finding that under mild conditions, the $\sqrt{n}$-adjusted posterior distribution of the parameters under our model converges to the same normal distribution as that of the canonical integrated posterior. Therefore, our result formally dispels a long-held belief that partial optimization of latent variables may lead to underestimation of parameter uncertainty. We demonstrate the practical advantages of our approach under several settings, including maximum-margin classification, latent normal models, and harmonization of multiple networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00968v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Zeng, Eleni Dilma, Jason Xu, Leo L Duan</dc:creator>
    </item>
    <item>
      <title>Nonparametric FBST for Validating Linear Models</title>
      <link>https://arxiv.org/abs/2406.15608</link>
      <description>arXiv:2406.15608v2 Announce Type: replace 
Abstract: The Full Bayesian Significance Test (FBST) possesses many desirable aspects, such as dismissing the need for hypotheses to have positive prior probability and providing a measure of evidence against $H_0$. Still, few attempts have been made to bring the FBST to nonparametric settings, with the main drawback being the need to obtain the highest posterior density (HPD) in a function space. In this work, we use a Gaussian processes prior to derive the FBST for hypotheses of the type $$ H_0: g(\boldsymbol{x}) = \boldsymbol{b}(\boldsymbol{x})\boldsymbol{\beta}, \quad \forall \boldsymbol{x} \in \mathcal{X}, \quad \boldsymbol{\beta} \in \mathbb{R}^k, $$ where $g(\cdot)$ is the regression function, $\boldsymbol{b}(\cdot)$ is a vector of linearly independent linear functions -- such as $\boldsymbol{b}(\boldsymbol{x}) = \boldsymbol{x}'$ -- and $\mathcal{X}$ is the covariates' domain. We also make use of pragmatic hypotheses to verify if the data might be compatible with a linear model when factors such as measurement errors or utility judgments are accounted for. This contribution extends the theory of the FBST, allowing its application in nonparametric settings and providing a procedure that easily tests if linear models are adequate for the data and that can automatically perform variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15608v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo F. L. Lassance, Julio M. Stern, Rafael B. Stern</dc:creator>
    </item>
    <item>
      <title>Piecewise-linear modeling of multivariate geometric extremes</title>
      <link>https://arxiv.org/abs/2412.05195</link>
      <description>arXiv:2412.05195v3 Announce Type: replace 
Abstract: A recent development in extreme value modeling uses the geometry of the dataset to perform inference on the multivariate tail. A key quantity in this inference is the gauge function, whose values define this geometry. Methodology proposed to date for capturing the gauge function either lacks flexibility due to parametric specifications, or relies on complex neural network specifications in dimensions greater than three. We propose a semiparametric gauge function that is piecewise-linear, making it simple to interpret and provides a good approximation for the true underlying gauge function. This linearity also makes optimization tasks computationally inexpensive. The piecewise-linear gauge function can be used to define both a radial and an angular model, allowing for the joint fitting of extremal pseudo-polar coordinates, a key aspect of this geometric framework. We further expand the toolkit for geometric extremal modeling through the estimation of high radial quantiles at given angular values via kernel density estimation. We apply the new methodology to air pollution data, which exhibits a complex extremal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05195v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ryan Campbell, Jennifer Wadsworth</dc:creator>
    </item>
    <item>
      <title>Explainable Linear and Generalized Linear Models by the Predictions Plot</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v3 Announce Type: replace 
Abstract: Multiple linear regression is a basic statistical tool, yielding a prediction formula with the input variables, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called predictions plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the prediction formula as it is, without depending on how the formula was derived. The input variables can be numerical or categorical. Interaction terms are also handled, and the model can be linear or generalized linear. Another display is proposed to visualize correlations and covariances between prediction terms, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>A Goodness-of-Fit Test for Sparse Networks</title>
      <link>https://arxiv.org/abs/2503.11990</link>
      <description>arXiv:2503.11990v2 Announce Type: replace 
Abstract: The stochastic block model (SBM) has been widely used to analyze network data. Various goodness-of-fit tests have been proposed to assess the adequacy of model structures. To the best of our knowledge, however, none of the existing approaches are applicable for sparse networks in which the connection probability of any two communities is of order log(n)/n, and the number of communities is divergent. To fill this gap, we propose a novel goodness-of-fit test for the stochastic block model. The key idea is to construct statistics by sampling the maximum entry-deviations of the adjacency matrix that the negative impacts of network sparsity are alleviated by the sampling process. We demonstrate theoretically that the proposed test statistic converges to the Type-I extreme value distribution under the null hypothesis regardless of the network structure. Accordingly, it can be applied to both dense and sparse networks. In addition, we obtain the asymptotic power against alternatives. Moreover, we introduce a bootstrap-corrected test statistic to improve the finite sample performance, recommend an augmented test statistic to increase the power, and extend the proposed test to the degree-corrected SBM. Simulation studies and two empirical examples with both dense and sparse networks indicate that the proposed method performs well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11990v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Wu, Wei Lan, Long Feng, Chih-Ling Tsai</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Measuring the Accuracy of Nonnegative Cross-Sectional Predictions</title>
      <link>https://arxiv.org/abs/2505.18130</link>
      <description>arXiv:2505.18130v2 Announce Type: replace 
Abstract: Measuring the accuracy of cross-sectional predictions is a subjective problem. Generally, this problem is avoided. In contrast, this paper confronts subjectivity up front by eliciting an impartial decision-maker's preferences. These preferences are embedded into an axiomatically-derived loss function, the simplest version of which is described. The parameters of the loss function can be estimated by linear regression. Specification tests for this function are described. This framework is extended to weighted averages of estimates to find the optimal weightings. Rescalings to account for changes in control data or base year data are considered. A special case occurs when the predictions represent resource allocations: the apportionment literature is used to construct the Webster-Saint Lague Rule, a particular parametrization of the loss function. These loss functions are compared to those existing in the literature. Finally, a bias measure is created that uses signed versions of these loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18130v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
    <item>
      <title>Bayesian Bootstrap based Gaussian Copula Model for Mixed Data with High Missing Rates</title>
      <link>https://arxiv.org/abs/2507.06785</link>
      <description>arXiv:2507.06785v2 Announce Type: replace 
Abstract: Missing data is a common issue in various fields such as medicine, social sciences, and natural sciences, and it poses significant challenges for accurate statistical analysis. Although numerous imputation methods have been proposed to address this issue, many of them fail to adequately capture the complex dependency structure among variables. To overcome this limitation, models based on the Gaussian copula framework have been introduced. However, most existing copula-based approaches do not account for the uncertainty in the marginal distributions, which can lead to biased marginal estimates and degraded performance, especially under high missingness rates.
  In this study, we propose a Bayesian bootstrap-based Gaussian Copula model (BBGC) that explicitly incorporates uncertainty in the marginal distributions of each variable. The proposed BBGC combines the flexible dependency modeling capability of the Gaussian copula with the Bayesian uncertainty quantification of marginal cumulative distribution functions (CDFs) via the Bayesian bootstrap. Furthermore, it is extended to handle mixed data types by incorporating methods for ordinal variable modeling.
  Through simulation studies and experiments on real-world datasets from the UCI repository, we demonstrate that the proposed BBGC outperforms existing imputation methods across various missing rates and mechanisms (MCAR, MAR). Additionally, the proposed model shows superior performance on real semiconductor manufacturing process data compared to conventional imputation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06785v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Jeunghun Oh, Hungkuk Ko, Jeongmin Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Optimal tests of the composite null hypothesis arising in mediation analysis</title>
      <link>https://arxiv.org/abs/2107.07575</link>
      <description>arXiv:2107.07575v3 Announce Type: replace-cross 
Abstract: The indirect effect of an exposure on an outcome through an intermediate variable can be identified by a product of two regression coefficients under certain causal and regression modeling assumptions. In this context, the null hypothesis of no indirect effect is a composite null hypothesis, as the null holds if either regression coefficient is zero. A consequence is that traditional hypothesis tests are severely underpowered near the origin (i.e., when both coefficients are small with respect to standard errors). We propose hypothesis tests that (i) preserve level alpha type~1 error, (ii) meaningfully improve power when both true underlying effects are small relative to sample size, and (iii) preserve power when at least one is not. One approach gives a closed-form test that is minimax optimal with respect to local power over the alternative parameter space. Another uses sparse linear programming to produce an approximately optimal test for a Bayes risk criterion. We discuss adaptations for performing large-scale hypothesis testing as well as modifications that yield improved interpretability. We provide an R package that implements our proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07575v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb H. Miles, Antoine Chambaz</dc:creator>
    </item>
    <item>
      <title>MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment</title>
      <link>https://arxiv.org/abs/2502.18699</link>
      <description>arXiv:2502.18699v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18699v3</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang</dc:creator>
    </item>
  </channel>
</rss>
