<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Computational Complexity of Variational Inequalities and Applications in Game Theory</title>
      <link>https://arxiv.org/abs/2411.04392</link>
      <description>arXiv:2411.04392v1 Announce Type: new 
Abstract: We present a computational formulation for the approximate version of several variational inequality problems, investigating their computational complexity and establishing PPAD-completeness. Examining applications in computational game theory, we specifically focus on two key concepts: resilient Nash equilibrium, and multi-leader-follower games -- domains traditionally known for the absence of general solutions. In the presence of standard assumptions and relaxation techniques, we formulate problem versions for such games that are expressible in terms of variational inequalities, ultimately leading to proofs of PPAD-completeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04392v1</guid>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruce M. Kapron, Koosha Samieefar</dc:creator>
    </item>
    <item>
      <title>Complexity theory of orbit closure intersection for tensors: reductions, completeness, and graph isomorphism hardness</title>
      <link>https://arxiv.org/abs/2411.04639</link>
      <description>arXiv:2411.04639v1 Announce Type: new 
Abstract: Many natural computational problems in computer science, mathematics, physics, and other sciences amount to deciding if two objects are equivalent. Often this equivalence is defined in terms of group actions. A natural question is to ask when two objects can be distinguished by polynomial functions that are invariant under the group action. For finite groups, this is the usual notion of equivalence, but for continuous groups like the general linear groups it gives rise to a new notion, called orbit closure intersection. It captures, among others, the graph isomorphism problem, noncommutative PIT, null cone problems in invariant theory, equivalence problems for tensor networks, and the classification of multiparty quantum states. Despite recent algorithmic progress in celebrated special cases, the computational complexity of general orbit closure intersection problems is currently quite unclear. In particular, tensors seem to give rise to the most difficult problems.
  In this work we start a systematic study of orbit closure intersection from the complexity-theoretic viewpoint. To this end, we define a complexity class TOCI that captures the power of orbit closure intersection problems for general tensor actions, give an appropriate notion of algebraic reductions that imply polynomial-time reductions in the usual sense, but are amenable to invariant-theoretic techniques, identify natural tensor problems that are complete for TOCI, including the equivalence of 2D tensor networks with constant physical dimension, and show that the graph isomorphism problem can be reduced to these complete problems, hence GI$\subseteq$TOCI. As such, our work establishes the first lower bound on the computational complexity of orbit closure intersection problems, and it explains the difficulty of finding unconditional polynomial-time algorithms beyond special cases, as has been observed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04639v1</guid>
      <category>cs.CC</category>
      <category>math.AG</category>
      <category>math.RT</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Lysikov, Michael Walter</dc:creator>
    </item>
    <item>
      <title>On the hardness of learning ground state entanglement of geometrically local Hamiltonians</title>
      <link>https://arxiv.org/abs/2411.04353</link>
      <description>arXiv:2411.04353v1 Announce Type: cross 
Abstract: Characterizing the entanglement structure of ground states of local Hamiltonians is a fundamental problem in quantum information. In this work we study the computational complexity of this problem, given the Hamiltonian as input. Our main result is that to show it is cryptographically hard to determine if the ground state of a geometrically local, polynomially gapped Hamiltonian on qudits ($d=O(1)$) has near-area law vs near-volume law entanglement. This improves prior work of Bouland et al. (arXiv:2311.12017) showing this for non-geometrically local Hamiltonians. In particular we show this problem is roughly factoring-hard in 1D, and LWE-hard in 2D. Our proof works by constructing a novel form of public-key pseudo-entanglement which is highly space-efficient, and combining this with a modification of Gottesman and Irani's quantum Turing machine to Hamiltonian construction. Our work suggests that the problem of learning so-called "gapless" quantum phases of matter might be intractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04353v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.other</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bouland, Chenyi Zhang, Zixin Zhou</dc:creator>
    </item>
    <item>
      <title>On the average-case hardness of BosonSampling</title>
      <link>https://arxiv.org/abs/2411.04566</link>
      <description>arXiv:2411.04566v1 Announce Type: cross 
Abstract: BosonSampling is a popular candidate for near-term quantum advantage, which has now been experimentally implemented several times. The original proposal of Aaronson and Arkhipov from 2011 showed that classical hardness of BosonSampling is implied by a proof of the "Gaussian Permanent Estimation" conjecture. This conjecture states that $e^{-n\log{n}-n-O(\log n)}$ additive error estimates to the output probability of most random BosonSampling experiments are $\#P$-hard. Proving this conjecture has since become the central question in the theory of quantum advantage.
  In this work we make progress by proving that $e^{-n\log n -n - O(n^\delta)}$ additive error estimates to output probabilities of most random BosonSampling experiments are $\#P$-hard, for any $\delta&gt;0$. In the process, we circumvent all known barrier results for proving the hardness of BosonSampling experiments. This is nearly the robustness needed to prove hardness of BosonSampling -- the remaining hurdle is now "merely" to show that the $n^\delta$ in the exponent can be improved to $O(\log n).$ We also obtain an analogous result for Random Circuit Sampling.
  Our result allows us to show, for the first time, a hardness of classical sampling result for random BosonSampling experiments, under an anticoncentration conjecture. Specifically, we prove the impossibility of multiplicative-error sampling from random BosonSampling experiments with probability $1-e^{-O(n)}$, unless the Polynomial Hierarchy collapses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04566v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bouland, Ishaun Datta, Bill Fefferman, Felipe Hernandez</dc:creator>
    </item>
    <item>
      <title>On the Complexity of 2-club Cluster Editing with Vertex Splitting</title>
      <link>https://arxiv.org/abs/2411.04846</link>
      <description>arXiv:2411.04846v1 Announce Type: cross 
Abstract: Editing a graph to obtain a disjoint union of s-clubs is one of the models for correlation clustering, which seeks a partition of the vertex set of a graph so that elements of each resulting set are close enough according to some given criterion. For example, in the case of editing into s-clubs, the criterion is proximity since any pair of vertices (in an s-club) are within a distance of s from each other. In this work we consider the vertex splitting operation, which allows a vertex to belong to more than one cluster. This operation was studied as one of the parameters associated with the Cluster Editing problem. We study the complexity and parameterized complexity of the s-Club Cluster Edge Deletion with Vertex Splitting and s-Club Cluster Vertex Splitting problems. Both problems are shown to be NP-Complete and APX-hard. On the positive side, we show that both problems are Fixed-Parameter Tractable with respect to the number of allowed editing operations and that s-Club Cluster Vertex Splitting is solvable in polynomial-time on the class of forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04846v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal N. Abu-Khzam, Tom Davot, Lucas Isenmann, Sergio Thoumi</dc:creator>
    </item>
    <item>
      <title>Hardness of approximation for ground state problems</title>
      <link>https://arxiv.org/abs/2411.04874</link>
      <description>arXiv:2411.04874v1 Announce Type: cross 
Abstract: After nearly two decades of research, the question of a quantum PCP theorem for quantum Constraint Satisfaction Problems (CSPs) remains wide open. As a result, proving QMA-hardness of approximation for ground state energy estimation has remained elusive. Recently, it was shown [Bittel, Gharibian, Kliesch, CCC 2023] that a natural problem involving variational quantum circuits is QCMA-hard to approximate within ratio N^(1-eps) for any eps &gt; 0 and N the input size. Unfortunately, this problem was not related to quantum CSPs, leaving the question of hardness of approximation for quantum CSPs open. In this work, we show that if instead of focusing on ground state energies, one considers computing properties of the ground space, QCMA-hardness of computing ground space properties can be shown. In particular, we show that it is (1) QCMA-complete within ratio N^(1-eps) to approximate the Ground State Connectivity problem (GSCON), and (2) QCMA-hard within the same ratio to estimate the amount of entanglement of a local Hamiltonian's ground state, denoted Ground State Entanglement (GSE). As a bonus, a simplification of our construction yields NP-completeness of approximation for a natural k-SAT reconfiguration problem, to be contrasted with the recent PCP-based PSPACE hardness of approximation results for a different definition of k-SAT reconfiguration [Karthik C.S. and Manurangsi, 2023, and Hirahara, Ohsaka, STOC 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04874v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sevag Gharibian, Carsten Hecht</dc:creator>
    </item>
    <item>
      <title>Convergence efficiency of quantum gates and circuits</title>
      <link>https://arxiv.org/abs/2411.04898</link>
      <description>arXiv:2411.04898v1 Announce Type: cross 
Abstract: We consider quantum circuit models where the gates are drawn from arbitrary gate ensembles given by probabilistic distributions over certain gate sets and circuit architectures, which we call stochastic quantum circuits. Of main interest in this work is the speed of convergence of stochastic circuits with different gate ensembles and circuit architectures to unitary t-designs. A key motivation for this theory is the varying preference for different gates and circuit architectures in different practical scenarios. In particular, it provides a versatile framework for devising efficient circuits for implementing $t$-designs and relevant applications including random circuit and scrambling experiments, as well as benchmarking the performance of gates and circuit architectures. We examine various important settings in depth. A key aspect of our study is an "ironed gadget" model, which allows us to systematically evaluate and compare the convergence efficiency of entangling gates and circuit architectures. Particularly notable results include i) gadgets of two-qubit gates with KAK coefficients $\left(\frac{\pi}{4}-\frac{1}{8}\arccos(\frac{1}{5}),\frac{\pi}{8},\frac{1}{8}\arccos(\frac{1}{5})\right)$ (which we call $\chi$ gates) directly form exact 2- and 3-designs; ii) the iSWAP gate family achieves the best efficiency for convergence to 2-designs under mild conjectures with numerical evidence, even outperforming the Haar-random gate, for generic many-body circuits; iii) iSWAP + complete graph achieve the best efficiency for convergence to 2-designs among all graph circuits. A variety of numerical results are provided to complement our analysis. We also derive robustness guarantees for our analysis against gate perturbations. Additionally, we provide cursory analysis on gates with higher locality and found that the Margolus gate outperforms various other well-known gates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04898v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.str-el</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linghang Kong, Zimu Li, Zi-Wen Liu</dc:creator>
    </item>
    <item>
      <title>Quantum Threshold is Powerful</title>
      <link>https://arxiv.org/abs/2411.04953</link>
      <description>arXiv:2411.04953v1 Announce Type: cross 
Abstract: In 2005, H{\o}yer and \v{S}palek showed that constant-depth quantum circuits augmented with multi-qubit Fanout gates are quite powerful, able to compute a wide variety of Boolean functions as well as the quantum Fourier transform. They also asked what other multi-qubit gates could rival Fanout in terms of computational power, and suggested that the quantum Threshold gate might be one such candidate. Threshold is the gate that indicates if the Hamming weight of a classical basis state input is greater than some target value.
  We prove that Threshold is indeed powerful--there are polynomial-size constant-depth quantum circuits with Threshold gates that compute Fanout to high fidelity. Our proof is a generalization of a proof by Rosenthal that exponential-size constant-depth circuits with generalized Toffoli gates can compute Fanout. Our construction reveals that other quantum gates able to "weakly approximate" Parity can also be used as substitutes for Fanout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04953v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Grier, Jackson Morris</dc:creator>
    </item>
    <item>
      <title>Uniformity testing when you have the source code</title>
      <link>https://arxiv.org/abs/2411.04972</link>
      <description>arXiv:2411.04972v1 Announce Type: cross 
Abstract: We study quantum algorithms for verifying properties of the output probability distribution of a classical or quantum circuit, given access to the source code that generates the distribution. We consider the basic task of uniformity testing, which is to decide if the output distribution is uniform on $[d]$ or $\epsilon$-far from uniform in total variation distance. More generally, we consider identity testing, which is the task of deciding if the output distribution equals a known hypothesis distribution, or is $\epsilon$-far from it. For both problems, the previous best known upper bound was $O(\min\{d^{1/3}/\epsilon^{2},d^{1/2}/\epsilon\})$. Here we improve the upper bound to $O(\min\{d^{1/3}/\epsilon^{4/3}, d^{1/2}/\epsilon\})$, which we conjecture is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04972v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement L. Canonne, Robin Kothari, Ryan O'Donnell</dc:creator>
    </item>
    <item>
      <title>There is a Hyper-Greedoid lurking behind every Graphical Accessible Computational Search Problem solvable in Polynomial Time: $P \not= NP$</title>
      <link>https://arxiv.org/abs/1802.03028</link>
      <description>arXiv:1802.03028v4 Announce Type: replace 
Abstract: Consider $G[X]$, where $G$ is a connected, isthmus-less and labelled graph, and $X$ is the edge-set or the vertex-set of the graph $G$. A Graphical Search Problem (GSP), denoted $\Pi(G[X],\gamma)$, consists of finding $Y$, where $Y \subseteq X$ and $Y$ satisfies the predicate $\gamma$ in $G$. The subset $Y$ is a solution of the problem $\Pi(G[X],\gamma)$. A sub-solution of $\Pi(G[X],\gamma)$ is a subset $Y'$ such that $Y'$ is not a solution of $\Pi(G[X],\gamma)$, but $Y'$ is a solution of the problem $\Pi(H[X'],\gamma)$, where $X' \subset X$ and $H[X']$ is a contraction-minor of $G[X]$. Solutions and sub-solutions are the feasible sets of $\Pi(G[X],\gamma)$.
  Let $\mathcal{I}$ be the family of all the feasible sets of $\Pi(G[X],\gamma)$. A Hyper-greedoid is a set system $(X, \mathcal{I})$ satisfying the following axioms.
  A1: Accessibility: if $I \in \mathcal{I}$, there is an element $x \in I$ such that $I-x \in \mathcal{I}$
  A2: Augmentability: If $I$ is a sub-solution, there is a polynomial time function $\kappa: \mathcal{I} \rightarrow \mathcal{I}$ and there is a element $x \in X-\kappa(I)$ such that $\kappa(I) \cup x \in \mathcal{I}$. That is, every sub-solution can be augmented using a polynomial time algorithm akin to Edmond Augmenting Path Algorithm.
  Given a graph $G$, the GSP MIS consists of finding an independent set of vertices of $G$. MIS satisfies axioms A1 and A2. Using the P-completeness of the Decision Problem associated to MIS, we prove that every GSP that satisfies A1 is solvable in Polynomial Time if and only if it satisfies A2. On the other hand, let HCP be the GSP that consists of finding a Hamiltonian cycle of the graph $G$. HCP satisfies A1, but does not satisfies A2. Since the Decision Problem associated with HCP is NP-Complete, we get $P \not = NP$.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.03028v4</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koko-Kalambay Kalafan Kayibi</dc:creator>
    </item>
    <item>
      <title>Quasi-Linear Size PCPs with Small Soundness from HDX</title>
      <link>https://arxiv.org/abs/2407.12762</link>
      <description>arXiv:2407.12762v2 Announce Type: replace 
Abstract: We construct 2-query, quasi-linear size probabilistically checkable proofs (PCPs) with arbitrarily small constant soundness, improving upon Dinur's 2-query quasi-linear size PCPs with soundness $1-\Omega(1)$. As an immediate corollary, we get that under the exponential time hypothesis, for all $\epsilon&gt;0$ no approximation algorithm for $3$-SAT can obtain an approximation ratio of $7/8+\epsilon$ in time $2^{n/\log^C n}$, where $C$ is a constant depending on $\epsilon$. Our result builds on a recent line of independent works by Bafna, Lifshitz and Minzer, and Dikstein, Dinur and Lubotzky, that showed the existence of linear size direct product testers with small soundness.
  The main new ingredient in our proof is a technique that embeds a given 2-CSP into a 2-CSP on a prescribed graph, provided that the latter is a graph underlying a sufficiently good high-dimensional expander (HDX). We achieve this by establishing a novel connection between PCPs and fault-tolerant distributed computing, more precisely, to the almost-everywhere reliable transmission problem introduced by Dwork, Peleg, Pippenger and Upfal (1986). We instantiate this connection by showing that graphs underlying HDXs admit routing protocols that are tolerant to adversarial edge corruptions, also improving upon the state of the art constructions of sparse edge-fault-tolerant networks in the process.
  Our PCP construction requires variants of the aforementioned direct product testers with poly-logarithmic degree. The existence and constructability of these variants is shown in an appendix by Zhiwei Yun.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12762v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitali Bafna, Dor Minzer, Nikhil Vyas</dc:creator>
    </item>
    <item>
      <title>Reasonable Space for the $\lambda$-Calculus, Logarithmically</title>
      <link>https://arxiv.org/abs/2203.00362</link>
      <description>arXiv:2203.00362v5 Announce Type: replace-cross 
Abstract: Can the $\lambda$-calculus be considered a reasonable computational model? Can we use it for measuring the time $\textit{and}$ space consumption of algorithms? While the literature contains positive answers about time, much less is known about space. This paper presents a new reasonable space cost model for the $\lambda$-calculus, based on a variant over the Krivine abstract machine. For the first time, this cost model is able to accommodate logarithmic space. Moreover, we study the time behavior of our machine and show how to transport our results to the call-by-value $\lambda$-calculus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.00362v5</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>cs.PL</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Beniamino Accattoli, Ugo Dal Lago, Gabriele Vanoni</dc:creator>
    </item>
    <item>
      <title>Finding dense sub-lattices as low-energy states of a Hamiltonian</title>
      <link>https://arxiv.org/abs/2309.16256</link>
      <description>arXiv:2309.16256v2 Announce Type: replace-cross 
Abstract: Lattice-based cryptography has emerged as one of the most prominent candidates for post-quantum cryptography, projected to be secure against the imminent threat of large-scale fault-tolerant quantum computers. The Shortest Vector Problem (SVP) is to find the shortest non-zero vector in a given lattice. It is fundamental to lattice-based cryptography and believed to be hard even for quantum computers. We study a natural generalization of the SVP known as the $K$-Densest Sub-lattice Problem ($K$-DSP): to find the densest $K$-dimensional sub-lattice of a given lattice. We formulate $K$-DSP as finding the first excited state of a Z-basis Hamiltonian, making $K$-DSP amenable to investigation via an array of quantum algorithms, including Grover search, quantum Gibbs sampling, adiabatic, and Variational Quantum Algorithms. The complexity of the algorithms depends on the basis through which the input lattice is presented. We present a classical polynomial-time algorithm that takes an arbitrary input basis and preprocesses it into inputs suited to quantum algorithms. With preprocessing, we prove that $O(KN^2)$ qubits suffice for solving $K$-DSP for $N$ dimensional input lattices. We empirically demonstrate the performance of a Quantum Approximate Optimization Algorithm $K$-DSP solver for low dimensions, highlighting the influence of a good preprocessed input basis. We then discuss the hardness of $K$-DSP in relation to the SVP, to see if there is reason to build post-quantum cryptography on $K$-DSP. We devise a quantum algorithm that solves $K$-DSP with run-time exponent $(5KN\log{N})/2$. Therefore, for fixed $K$, $K$-DSP is no more than polynomially harder than the SVP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16256v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'ulia Barber\`a-Rodr\'iguez, Nicolas Gama, Anand Kumar Narayanan, David Joseph</dc:creator>
    </item>
    <item>
      <title>On the Role of Constraints in the Complexity of Min-Max Optimization</title>
      <link>https://arxiv.org/abs/2411.03248</link>
      <description>arXiv:2411.03248v2 Announce Type: replace-cross 
Abstract: We investigate the role of constraints in the computational complexity of min-max optimization. The work of Daskalakis, Skoulakis, and Zampetakis [2021] was the first to study min-max optimization through the lens of computational complexity, showing that min-max problems with nonconvex-nonconcave objectives are PPAD-hard. However, their proof hinges on the presence of joint constraints between the maximizing and minimizing players. The main goal of this paper is to understand the role of these constraints in min-max optimization. The first contribution of this paper is a fundamentally new proof of their main result, which improves it in multiple directions: it holds for degree 2 polynomials, it is essentially tight in the parameters, and it is much simpler than previous approaches, clearly highlighting the role of constraints in the hardness of the problem. Second, we show that with general constraints (i.e., the min player and max player have different constraints), even convex-concave min-max optimization becomes PPAD-hard. Along the way, we also provide PPAD-membership of a general problem related to quasi-variational inequalities, which has applications beyond our problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03248v2</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Gabriele Farina</dc:creator>
    </item>
  </channel>
</rss>
