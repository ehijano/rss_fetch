<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Structured Analytic Mappings for Point Set Registration</title>
      <link>https://arxiv.org/abs/2602.16753</link>
      <description>arXiv:2602.16753v1 Announce Type: new 
Abstract: We present an analytic approximation model for non-rigid point set registration, grounded in the multivariate Taylor expansion of vector-valued functions. By exploiting the algebraic structure of Taylor expansions, we construct a structured function space spanned by truncated basis terms, allowing smooth deformations to be represented with low complexity and explicit form. To estimate mappings within this space, we develop a quasi-Newton optimization algorithm that progressively lifts the identity map into higher-order analytic forms. This structured framework unifies rigid, affine, and nonlinear deformations under a single closed-form formulation, without relying on kernel functions or high-dimensional parameterizations. The proposed model is embedded into a standard ICP loop -- using (by default) nearest-neighbor correspondences -- resulting in Analytic-ICP, an efficient registration algorithm with quasi-linear time complexity. Experiments on 2D and 3D datasets demonstrate that Analytic-ICP achieves higher accuracy and faster convergence than classical methods such as CPD and TPS-RPM, particularly for small and smooth deformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16753v1</guid>
      <category>eess.IV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.RA</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Feng, Tengda Wei, Haiyong Zheng</dc:creator>
    </item>
    <item>
      <title>Is there a relationship between Mean Opinion Score (MOS) and Just Noticeable Difference (JND)?</title>
      <link>https://arxiv.org/abs/2602.17010</link>
      <description>arXiv:2602.17010v1 Announce Type: new 
Abstract: Evaluating perceived video quality is essential for ensuring high Quality of Experience (QoE) in modern streaming applications. While existing subjective datasets and Video Quality Metrics (VQMs) cover a broad quality range, many practical use cases especially for premium users focus on high quality scenarios requiring finer granularity. Just Noticeable Difference (JND) has emerged as a key concept for modeling perceptual thresholds in these high end regions and plays an important role in perceptual bitrate ladder construction. However, the relationship between JND and the more widely used Mean Opinion Score (MOS) remains unclear. In this paper, we conduct a Degradation Category Rating (DCR) subjective study based on an existing JND dataset to examine how MOS corresponds to the 75% Satisfied User Ratio (SUR) points of the 1st and 2nd JNDs. We find that while MOS values at JND points generally align with theoretical expectations (e.g., 4.75 for the 75% SUR of the 1st JND), the reverse mapping from MOS to JND is ambiguous due to overlapping confidence intervals across PVS indices. Statistical significance analysis further shows that DCR studies with limited participants may not detect meaningful differences between reference and JND videos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17010v1</guid>
      <category>eess.IV</category>
      <category>cs.MM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwen Zhu, Hadi Amirpour, Wei Zhou, Patrick Le Callet</dc:creator>
    </item>
    <item>
      <title>HybridPrompt: Bridging Generative Priors and Traditional Codecs for Mobile Streaming</title>
      <link>https://arxiv.org/abs/2602.17120</link>
      <description>arXiv:2602.17120v1 Announce Type: new 
Abstract: In Video on Demand (VoD) scenarios, traditional codecs are the industry standard due to their high decoding efficiency. However, they suffer from severe quality degradation under low bandwidth conditions. While emerging generative neural codecs offer significantly higher perceptual quality, their reliance on heavy frame-by-frame generation makes real-time playback on mobile devices impractical. We ask: is it possible to combine the blazing-fast speed of traditional standards with the superior visual fidelity of neural approaches? We present HybridPrompt, the first generative-based video system capable of achieving real-time 1080p decoding at over 150 FPS on a commercial smartphone. Specifically, we employ a hybrid architecture that encodes Keyframes using a generative model while relying on traditional codecs for the remaining frames. A major challenge is that the two paradigms have conflicting objectives: the "hallucinated" details from generative models often misalign with the rigid prediction mechanisms of traditional codecs, causing bitrate inefficiency. To address this, we demonstrate that the traditional decoding process is differentiable, enabling an end-to-end optimization loop. This allows us to use subsequent frames as additional supervision, forcing the generative model to synthesize keyframes that are not only perceptually high-fidelity but also mathematically optimal references for the traditional codec. By integrating a two-stage generation strategy, our system outperforms pure neural baselines by orders of magnitude in speed while achieving an average LPIPS gain of 8% over traditional codecs at 200kbps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17120v1</guid>
      <category>eess.IV</category>
      <category>cs.MM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3798065.3798083</arxiv:DOI>
      <dc:creator>Liming Liu, Jiangkai Wu, Haoyang Wang, Peiheng Wang, Zongming Guo, Xinggong Zhang</dc:creator>
    </item>
    <item>
      <title>Gaussian surrogates do well on Poisson inverse problems</title>
      <link>https://arxiv.org/abs/2602.17274</link>
      <description>arXiv:2602.17274v1 Announce Type: new 
Abstract: In imaging inverse problems with Poisson-distributed measurements, it is common to use objectives derived from the Poisson likelihood. But performance is often evaluated by mean squared error (MSE), which raises a practical question: how much does a Poisson objective matter for MSE, even at low dose? We analyze the MSE of Poisson and Gaussian surrogate reconstruction objectives under Poisson noise. In a stylized diagonal model, we show that the unregularized Poisson maximum-likelihood estimator can incur large MSE at low dose, while Poisson MAP mitigates this instability through regularization. We then study two Gaussian surrogate objectives: a heteroscedastic quadratic objective motivated by the normal approximation of Poisson data, and a homoscedastic quadratic objective that yields a simple linear estimator. We show that both surrogates can achieve MSE comparable to Poisson MAP in the low-dose regime, despite departing from the Poisson likelihood. Numerical computed tomography experiments indicate that these conclusions extend beyond the stylized setting of our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17274v1</guid>
      <category>eess.IV</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandra Spitzer, Lorenzo Baldassari, Valentin Derbanot, Ivan Dokmani\'c</dc:creator>
    </item>
    <item>
      <title>A New Perspective on Scale: A Novel Transform for NMR Envelope Extraction</title>
      <link>https://arxiv.org/abs/2602.16841</link>
      <description>arXiv:2602.16841v1 Announce Type: cross 
Abstract: Envelope extraction in nuclear magnetic resonance (NMR) is a fundamental step for processing the data space generated by this technique. Envelope detection accuracy improves with increasing the number of sampling points; however, we propose a novel transform that enables acceptable envelope extraction with significantly fewer sampling points, even without meeting the Nyquist rate. In this paper, we challenge the traditional scale definition and demonstrate that classic scaling lacks a physical referent in all situations. To achieve this aim, we introduce a scale based on the variations of space-invariant states, rather than the observable characteristics of matter and energy. According to this definition of the scale, we distinguished two kinds of observers: scale-variant and scale-invariant. We demonstrated that converting a scale-variant observer to a scale-invariant observer is equivalent to envelop extraction. To analyse and study the theories presented in the paper, we have designed and implemented an Earth-field NMR setup and used real data generated by it to evaluate the performance of the proposed envelope-detection transform. We compared the output of the proposed transform with that of classic and state-of-the-art methods for parameter recovery of NMR signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16841v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ehsun Assadi</dc:creator>
    </item>
    <item>
      <title>A Multi-modal Detection System for Infrastructure-based Freight Signal Priority</title>
      <link>https://arxiv.org/abs/2602.17252</link>
      <description>arXiv:2602.17252v1 Announce Type: cross 
Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17252v1</guid>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <category>eess.SY</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyan Zhang, Chuheng Wei, Xuanpeng Zhao, Siyan Li, Will Snyder, Mike Stas, Peng Hao, Kanok Boriboonsomsin, Guoyuan Wu</dc:creator>
    </item>
    <item>
      <title>Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans</title>
      <link>https://arxiv.org/abs/2505.12298</link>
      <description>arXiv:2505.12298v2 Announce Type: replace 
Abstract: In this study, we propose a robust methodology for automatic segmentation of infected lung regions in COVID-19 CT scans using convolutional neural networks. The approach is based on a modified U-Net architecture enhanced with attention mechanisms, data augmentation, and postprocessing techniques. It achieved a Dice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods. The dataset was sourced from public repositories and augmented for diversity. Results demonstrate superior segmentation performance. Future work includes expanding the dataset, exploring 3D segmentation, and preparing the model for clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12298v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amal Lahchim (University of Kragujevac), Lazar Davic (University of Kragujevac)</dc:creator>
    </item>
    <item>
      <title>Adversarial Deep Learning for Simultaneous Segmentation of Ventricular and White Matter Hyperintensities in Clinical MRI</title>
      <link>https://arxiv.org/abs/2506.07123</link>
      <description>arXiv:2506.07123v4 Announce Type: replace 
Abstract: Purpose: Multiple sclerosis (MS) diagnosis requires accurate assessment of white matter hyperintensities (WMH) and ventricular changes on brain MRI. Current methods treat these structures independently, struggle to differentiate normal from pathological hyperintensities, and perform poorly on anisotropic clinical data. We present a deep learning framework that simultaneously segments ventricles and WMH while distinguishing normal periventricular hyperintensities from pathological MS lesions. Methods: We developed a 2D pix2pix architecture trained on FLAIR scans from 300 MS patients combined with the MSSEG2016 benchmark (15 patients). Five architectural variants were compared through systematic ablation using 5-fold cross-validation with patient-level stratification, progressively integrating adversarial training, attention-weighted discrimination, and adaptive hybrid loss. Performance was assessed against six established methods using Dice coefficient, Hausdorff distance, precision, and recall. Results: The final architecture (V5) achieved mean Dice 0.852+/-0.004 and HD95 4.87+/-0.13mm across all classes. Per-class performance: ventricles (Dice 0.907+/-0.002, HD95 3.00+/-0.51mm), abnormal WMH (Dice 0.825+/-0.009, HD95 4.51+/-0.32mm), normal WMH (Dice 0.677+/-0.007). V5 outperformed all baselines on local data for both ventricle and WMH segmentation. Ablation analysis confirmed adversarial training provided the largest single gain (+0.109 Dice). End-to-end processing required ~4 seconds per case-up to 36x faster than baseline methods. Conclusions: This systematically validated framework combines adversarial training, attention-weighted discrimination, and adaptive loss scheduling to achieve improved accuracy, clinically relevant lesion differentiation, and computational efficiency suitable for routine clinical workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07123v4</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mahdi Bashiri Bawil, Mousa Shamsi, Abolhassan Shakeri Bavil</dc:creator>
    </item>
    <item>
      <title>AtlasPatch: Efficient Tissue Detection and High-throughput Patch Extraction for Computational Pathology at Scale</title>
      <link>https://arxiv.org/abs/2602.03998</link>
      <description>arXiv:2602.03998v2 Announce Type: replace 
Abstract: Whole-slide image (WSI) preprocessing, comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology but remains a major bottleneck for scaling to large and heterogeneous cohorts. We present AtlasPatch, a scalable framework that couples foundation-model tissue detection with high-throughput patch extraction at minimal computational overhead. Our tissue detector achieves high precision (0.986) and remains robust across varying tissue conditions (e.g., brightness, fragmentation, boundary definition, tissue heterogeneity) and common artifacts (e.g., pen/ink markings, scanner streaks). This robustness is enabled by our annotated, heterogeneous multi-cohort training set of ~30,000 WSI thumbnails combined with efficient adaptation of the Segment-Anything (SAM) model. AtlasPatch also reduces end-to-end WSI preprocessing time by up to 16$\times$ versus widely used deep-learning pipelines, without degrading downstream task performance. The AtlasPatch tool is open-source, efficiently parallelized for practical deployment, and supports options to save extracted patches or stream them into common feature-extraction models for on-the-fly embedding, making it adaptable to both pathology departments (tissue detection and quality control) and AI researchers (dataset creation and model training). AtlasPatch software package is available at https://github.com/AtlasAnalyticsLab/AtlasPatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03998v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ahmed Alagha, Christopher Leclerc, Yousef Kotp, Omar Metwally, Calvin Moras, Peter Rentopoulos, Ghodsiyeh Rostami, Bich Ngoc Nguyen, Jumanah Baig, Abdelhakim Khellaf, Vincent Quoc-Huy Trinh, Rabeb Mizouni, Hadi Otrok, Jamal Bentahar, Mahdi S. Hosseini</dc:creator>
    </item>
    <item>
      <title>Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals</title>
      <link>https://arxiv.org/abs/2602.11903</link>
      <description>arXiv:2602.11903v2 Announce Type: replace 
Abstract: No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining. By jointly optimizing multiple full-reference (FR) objectives with adaptive task weighting, our approach learns shared representations that transfer effectively to NR-VQA. Experiments on gaming video datasets show MTL-VQA achieves performance competitive with state-of-the-art NR-VQA methods across both MOS-supervised and label-efficient/self-supervised settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11903v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Chih Chen, Michael Wang, Chieh-Dun Wen, Kai-Siang Ma, Avinab Saha, Li-Heng Chen, Alan Bovik</dc:creator>
    </item>
    <item>
      <title>SceneVGGT: VGGT-based online 3D semantic SLAM for indoor scene understanding and navigation</title>
      <link>https://arxiv.org/abs/2602.15899</link>
      <description>arXiv:2602.15899v2 Announce Type: replace-cross 
Abstract: We present SceneVGGT, a spatio-temporal 3D scene understanding framework that combines SLAM with semantic mapping for autonomous and assistive navigation. Built on VGGT, our method scales to long video streams via a sliding-window pipeline. We align local submaps using camera-pose transformations, enabling memory- and speed-efficient mapping while preserving geometric consistency. Semantics are lifted from 2D instance masks to 3D objects using the VGGT tracking head, maintaining temporally coherent identities for change detection. As a proof of concept, object locations are projected onto an estimated floor plane for assistive navigation. The pipeline's GPU memory usage remains under 17 GB, irrespectively of the length of the input sequence and achieves competitive point-cloud performance on the ScanNet++ benchmark. Overall, SceneVGGT ensures robust semantic identification and is fast enough to support interactive assistive navigation with audio feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15899v2</guid>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Gelencs\'er-Horv\'ath, Gergely Dinya, Dorka Bogl\'arka Er\H{o}s, P\'eter Hal\'asz, Islam Muhammad Muqsit, Krist\'of Karacs</dc:creator>
    </item>
  </channel>
</rss>
