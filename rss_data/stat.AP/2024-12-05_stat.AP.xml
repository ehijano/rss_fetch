<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 05:02:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Straightforward Phase I Dose-Finding Design for Healthy Volunteers Accounting for Surrogate Activity Biomarkers</title>
      <link>https://arxiv.org/abs/2412.03298</link>
      <description>arXiv:2412.03298v1 Announce Type: new 
Abstract: Conventionally, a first-in-human phase I trial in healthy volunteers aims to confirm the safety of a drug in humans. In such situations, volunteers should not suffer from any safety issues and simple algorithm-based dose-escalation schemes are often used. However, to avoid too many clinical trials in the future, it might be appealing to design these trials to accumulate information on the link between dose and efficacy/activity under strict safety constraints. Furthermore, an increasing number of molecules for which the increasing dose-activity curve reaches a plateau are emerging.In a phase I dose-finding trial context, our objective is to determine, under safety constraints, among a set of doses, the lowest dose whose probability of activity is closest to a given target. For this purpose, we propose a two-stage dose-finding design. The first stage is a typical algorithm dose escalation phase that can both check the safety of the doses and accumulate activity information. The second stage is a model-based dose-finding phase that involves selecting the best dose-activity model according to the plateau location.Our simulation study shows that our proposed method performs better than the common Bayesian logistic regression model in selecting the optimal dose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03298v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/19466315.2024.2416410</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Biopharmaceutical Research, 2024, pp.1-13</arxiv:journal_reference>
      <dc:creator>Sandrine Boulet (HeKA, CRC), Emmanuelle Comets (Irset, IAME), Antoine Guillon (CEPR, CHRU Tours), Linda B. S. Aulin (FU), Robin Michelet (FU), Charlotte Kloft (FU), Sarah Zohar (CRC, HeKA), Moreno Ursino (CRC, HeKA)</dc:creator>
    </item>
    <item>
      <title>The R.O.A.D. to clinical trial emulation</title>
      <link>https://arxiv.org/abs/2412.03528</link>
      <description>arXiv:2412.03528v1 Announce Type: new 
Abstract: Observational studies provide the only evidence on the effectiveness of interventions when randomized controlled trials (RCTs) are impractical due to cost, ethical concerns, or time constraints. While many methodologies aim to draw causal inferences from observational data, there is a growing trend to model observational study designs after RCTs, a strategy known as "target trial emulation." Despite its potential, causal inference through target trial emulation cannot fully address the confounding bias in real-world data due to the lack of randomization. In this work, we present a novel framework for target trial emulation that aims to overcome several key limitations, including confounding bias. The framework proceeds as follows: First, we apply the eligibility criteria of a specific trial to an observational cohort. We then "correct" this cohort by extracting a subset that matches both the distribution of covariates and the baseline prognosis of the control group in the target RCT. Next, we address unmeasured confounding by adjusting the prognosis estimates of the treated group to align with those observed in the trial. Following trial emulation, we go a step further by leveraging the emulated cohort to train optimal decision trees, to identify subgroups of patients with heterogeneity in treatment effects (HTE). The absence of confounding is verified using two external models, and the validity of the treatment recommendations is independently confirmed by the team responsible for the original trial we emulate. To our knowledge, this is the first framework to successfully address both observed and unobserved confounding, a challenge that has historically limited the use of randomized trial emulation and causal inference. Additionally, our framework holds promise in advancing precision medicine by identifying patient subgroups that benefit most from specific treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03528v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Bertsimas, Angelos G. Koulouras, Hiroshi Nagata, Carol Gao, Junki Mizusawa, Yukihide Kanemitsu, Georgios Antonios Margonis</dc:creator>
    </item>
    <item>
      <title>Removing Spurious Correlation from Neural Network Interpretations</title>
      <link>https://arxiv.org/abs/2412.02893</link>
      <description>arXiv:2412.02893v1 Announce Type: cross 
Abstract: The existing algorithms for identification of neurons responsible for undesired and harmful behaviors do not consider the effects of confounders such as topic of the conversation. In this work, we show that confounders can create spurious correlations and propose a new causal mediation approach that controls the impact of the topic. In experiments with two large language models, we study the localization hypothesis and show that adjusting for the effect of conversation topic, toxicity becomes less localized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02893v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milad Fotouhi, Mohammad Taha Bahadori, Oluwaseyi Feyisetan, Payman Arabshahi, David Heckerman</dc:creator>
    </item>
    <item>
      <title>Uncovering dynamics between SARS-CoV-2 wastewater concentrations and community infections via Bayesian spatial functional concurrent regression</title>
      <link>https://arxiv.org/abs/2412.02970</link>
      <description>arXiv:2412.02970v1 Announce Type: cross 
Abstract: Monitoring wastewater concentrations of SARS-CoV-2 yields a low-cost, noninvasive method for tracking disease prevalence and provides early warning signs of upcoming outbreaks in the serviced communities. There is tremendous clinical and public health interest in understanding the exact dynamics between wastewater viral loads and infection rates in the population. As both data sources may contain substantial noise and missingness, in addition to spatial and temporal dependencies, properly modeling this relationship must address these numerous complexities simultaneously while providing interpretable and clear insights. We propose a novel Bayesian functional concurrent regression model that accounts for both spatial and temporal correlations while estimating the dynamic effects between wastewater concentrations and positivity rates over time. We explicitly model the time lag between the two series and provide full posterior inference on the possible delay between spikes in wastewater concentrations and subsequent outbreaks. We estimate a time lag likely between 5 to 11 days between spikes in wastewater levels and reported clinical positivity rates. Additionally, we find a dynamic relationship between wastewater concentration levels and the strength of its association with positivity rates that fluctuates between outbreaks and non-outbreaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas Y. Sun, Julia C. Schedler, Daniel R. Kowal, Rebecca Schneider, Lauren B. Stadler, Loren Hopkins, Katherine B. Ensor</dc:creator>
    </item>
    <item>
      <title>Solving Self-calibration of ALMA Data with an Optimization Method</title>
      <link>https://arxiv.org/abs/2412.03183</link>
      <description>arXiv:2412.03183v1 Announce Type: cross 
Abstract: We reformulate the gain correction problem of the radio interferometry as an optimization problem with regularization, which is solved efficiently with an iterative algorithm. Combining this new method with our previously proposed imaging method, PRIISM, the whole process of the self-calibration of radio interferometry is redefined as a single optimization problem with regularization. As a result, the gains are corrected, and an image is estimated. We tested the new approach with ALMA observation data and found it provides promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03183v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiro Ikeda, Takeshi Nakazato, Takashi Tsukagoshi, Tsutomu T. Takeuchi, Masayuki Yamaguchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the Patient Weighted While-Alive Estimand</title>
      <link>https://arxiv.org/abs/2412.03246</link>
      <description>arXiv:2412.03246v1 Announce Type: cross 
Abstract: In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). We focus in this paper on the patient weighted while-alive estimand represented as the expected number of events divided by the time alive within a target window and develop efficient estimation for this estimand. We derive its efficient influence function and develop a one-step estimator, initially applied to the irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, the one-step estimator is practically intractable. We therefore suggest an alternative estimator that is also expected to have high efficiency focusing on the randomized treatment setting. We compare the efficiency of these two estimators in the illness-death setting. Additionally, we apply our proposed estimator to a real-world case study involving metastatic colorectal cancer patients, demonstrating the practical applicability and benefits of the while-alive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03246v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Torben Martinussen, Thomas Scheike</dc:creator>
    </item>
    <item>
      <title>Gaussian Processes for Probabilistic Estimates of Earthquake Ground Shaking: A 1-D Proof-of-Concept</title>
      <link>https://arxiv.org/abs/2412.03299</link>
      <description>arXiv:2412.03299v1 Announce Type: cross 
Abstract: Estimates of seismic wave speeds in the Earth (seismic velocity models) are key input parameters to earthquake simulations for ground motion prediction. Owing to the non-uniqueness of the seismic inverse problem, typically many velocity models exist for any given region. The arbitrary choice of which velocity model to use in earthquake simulations impacts ground motion predictions. However, current hazard analysis methods do not account for this source of uncertainty. We present a proof-of-concept ground motion prediction workflow for incorporating uncertainties arising from inconsistencies between existing seismic velocity models. Our analysis is based on the probabilistic fusion of overlapping seismic velocity models using scalable Gaussian process (GP) regression. Specifically, we fit a GP to two synthetic 1-D velocity profiles simultaneously, and show that the predictive uncertainty accounts for the differences between the models. We subsequently draw velocity model samples from the predictive distribution and estimate peak ground displacement using acoustic wave propagation through the velocity models. The resulting distribution of possible ground motion amplitudes is much wider than would be predicted by simulating shaking using only the two input velocity models. This proof-of-concept illustrates the importance of probabilistic methods for physics-based seismic hazard analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03299v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam A. Scivier, Tarje Nissen-Meyer, Paula Koelemeijer, At{\i}l{\i}m G\"une\c{s} Baydin</dc:creator>
    </item>
    <item>
      <title>Coherent forecast combination for linearly constrained multiple time series</title>
      <link>https://arxiv.org/abs/2412.03429</link>
      <description>arXiv:2412.03429v1 Announce Type: cross 
Abstract: Linearly constrained multiple time series may be encountered in many practical contexts, such as the National Accounts (e.g., GDP disaggregated by Income, Expenditure and Output), and multilevel frameworks where the variables are organized according to hierarchies or groupings, like the total energy consumption of a country disaggregated by region and energy sources. In these cases, when multiple incoherent base forecasts for each individual variable are available, a forecast combination-and-reconciliation approach, that we call coherent forecast combination, may be used to improve the accuracy of the base forecasts and achieve coherence in the final result. In this paper, we develop an optimization-based technique that combines multiple unbiased base forecasts while assuring the constraints valid for the series. We present closed form expressions for the coherent combined forecast vector and its error covariance matrix in the general case where a different number of forecasts is available for each variable. We also discuss practical issues related to the covariance matrix that is part of the optimal solution. Through simulations and a forecasting experiment on the daily Australian electricity generation hierarchical time series, we show that the proposed methodology, in addition to adhering to sound statistical principles, may yield in significant improvement on base forecasts, single-task combination and single-expert reconciliation approaches as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03429v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Visualisation for Exploratory Modelling Analysis of Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2412.03484</link>
      <description>arXiv:2412.03484v1 Announce Type: cross 
Abstract: When developing Bayesian hierarchical models, selecting the most appropriate hierarchical structure can be a challenging task, and visualisation remains an underutilised tool in this context. In this paper, we consider visualisations for the display of hierarchical models in data space and compare a collection of multiple models via their parameters and hyper-parameter estimates. Specifically, with the aim of aiding model choice, we propose new visualisations to explore how the choice of Bayesian hierarchical modelling structure impacts parameter distributions. The visualisations are designed using a robust set of principles to provide richer comparisons that extend beyond the conventional plots and numerical summaries typically used. As a case study, we investigate five Bayesian hierarchical models fit using the brms R package, a high-level interface to Stan for Bayesian modelling, to model country mathematics trends from the PISA (Programme for International Student Assessment) database. Our case study demonstrates that by adhering to these principles, researchers can create visualisations that not only help them make more informed choices between Bayesian hierarchical model structures but also enable them to effectively communicate the rationale for those choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03484v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oluwayomi Akinfenwa, Niamh Cahill, Catherine Hurley</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Statistical Models Based on Counting Processes</title>
      <link>https://arxiv.org/abs/2210.07114</link>
      <description>arXiv:2210.07114v4 Announce Type: replace 
Abstract: Since the famous paper written by Kaplan and Meier in 1958, survival analysis has become one of the most important fields in statistics. Nowadays it is one of the most important statistical tools in analyzing epidemiological and clinical data including COVID-19 pandemic. This article reviews some of the most celebrated and important results and methods, including consistency, asymptotic normality, bias and variance estimation, in survival analysis and the treatment is parallel to the monograph Statistical Models Based on Counting Processes. Other models and results such as semi-Markov models and the Turnbull's estimator that jump out of the classical counting process martingale framework are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07114v4</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui</dc:creator>
    </item>
    <item>
      <title>Dynamic Causal Models of Time-Varying Connectivity</title>
      <link>https://arxiv.org/abs/2411.16582</link>
      <description>arXiv:2411.16582v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel approach for modelling time-varying connectivity in neuroimaging data, focusing on the slow fluctuations in synaptic efficacy that mediate neuronal dynamics. Building on the framework of Dynamic Causal Modelling (DCM), we propose a method that incorporates temporal basis functions into neural models, allowing for the explicit representation of slow parameter changes. This approach balances expressivity and computational efficiency by modelling these fluctuations as a Gaussian process, offering a middle ground between existing methods that either strongly constrain or excessively relax parameter fluctuations. We validate the ensuing model through simulations and real data from an auditory roving oddball paradigm, demonstrating its potential to explain key aspects of brain dynamics. This work aims to equip researchers with a robust tool for investigating time-varying connectivity, particularly in the context of synaptic modulation and its role in both healthy and pathological brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16582v2</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Karl J. Friston, Peter Zeidman</dc:creator>
    </item>
  </channel>
</rss>
