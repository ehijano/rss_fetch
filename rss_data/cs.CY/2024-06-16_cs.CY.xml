<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Jun 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>You are what you eat? Feeding foundation models a regionally diverse food dataset of World Wide Dishes</title>
      <link>https://arxiv.org/abs/2406.09496</link>
      <description>arXiv:2406.09496v1 Announce Type: new 
Abstract: Foundation models are increasingly ubiquitous in our daily lives, used in everyday tasks such as text-image searches, interactions with chatbots, and content generation. As use increases, so does concern over the disparities in performance and fairness of these models for different people in different parts of the world. To assess these growing regional disparities, we present World Wide Dishes, a mixed text and image dataset consisting of 765 dishes, with dish names collected in 131 local languages. World Wide Dishes has been collected purely through human contribution and decentralised means, by creating a website widely distributed through social networks. Using the dataset, we demonstrate a novel means of operationalising capability and representational biases in foundation models such as language models and text-to-image generative models. We enrich these studies with a pilot community review to understand, from a first-person perspective, how these models generate images for people in five African countries and the United States.
  We find that these models generally do not produce quality text and image outputs of dishes specific to different regions. This is true even for the US, which is typically considered to be more well-resourced in training data - though the generation of US dishes does outperform that of the investigated African countries. The models demonstrate a propensity to produce outputs that are inaccurate as well as culturally misrepresentative, flattening, and insensitive. These failures in capability and representational bias have the potential to further reinforce stereotypes and disproportionately contribute to erasure based on region. The dataset and code are available at https://github.com/oxai/world-wide-dishes/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09496v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jabez Magomere, Shu Ishida, Tejumade Afonja, Aya Salama, Daniel Kochin, Foutse Yuehgoh, Imane Hamzaoui, Raesetje Sefala, Aisha Alaagib, Elizaveta Semenova, Lauren Crais, Siobhan Mackenzie Hall</dc:creator>
    </item>
    <item>
      <title>GeoSEE: Regional Socio-Economic Estimation With a Large Language Model</title>
      <link>https://arxiv.org/abs/2406.09799</link>
      <description>arXiv:2406.09799v1 Announce Type: new 
Abstract: Moving beyond traditional surveys, combining heterogeneous data sources with AI-driven inference models brings new opportunities to measure socio-economic conditions, such as poverty and population, over expansive geographic areas. The current research presents GeoSEE, a method that can estimate various socio-economic indicators using a unified pipeline powered by a large language model (LLM). Presented with a diverse set of information modules, including those pre-constructed from satellite imagery, GeoSEE selects which modules to use in estimation, for each indicator and country. This selection is guided by the LLM's prior socio-geographic knowledge, which functions similarly to the insights of a domain expert. The system then computes target indicators via in-context learning after aggregating results from selected modules in the format of natural language-based texts. Comprehensive evaluation across countries at various stages of development reveals that our method outperforms other predictive models in both unsupervised and low-shot contexts. This reliable performance under data-scarce setting in under-developed or developing countries, combined with its cost-effectiveness, underscores its potential to continuously support and monitor the progress of Sustainable Development Goals, such as poverty alleviation and equitable growth, on a global scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09799v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sungwon Han, Donghyun Ahn, Seungeon Lee, Minhyuk Song, Sungwon Park, Sangyoon Park, Jihee Kim, Meeyoung Cha</dc:creator>
    </item>
    <item>
      <title>Data Ethics in the Era of Healthcare Artificial Intelligence in Africa: An Ubuntu Philosophy Perspective</title>
      <link>https://arxiv.org/abs/2406.10121</link>
      <description>arXiv:2406.10121v1 Announce Type: new 
Abstract: Data are essential in developing healthcare artificial intelligence (AI) systems. However, patient data collection, access, and use raise ethical concerns, including informed consent, data bias, data protection and privacy, data ownership, and benefit sharing. Various ethical frameworks have been proposed to ensure the ethical use of healthcare data and AI, however, these frameworks often align with Western cultural values, social norms, and institutional contexts emphasizing individual autonomy and well-being. Ethical guidelines must reflect political and cultural settings to account for cultural diversity, inclusivity, and historical factors such as colonialism. Thus, this paper discusses healthcare data ethics in the AI era in Africa from the Ubuntu philosophy perspective. It focuses on the contrast between individualistic and communitarian approaches to data ethics. The proposed framework could inform stakeholders, including AI developers, healthcare providers, the public, and policy-makers about healthcare data ethical usage in AI in Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10121v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abdoul Jalil Djiberou Mahamadou, Aloysius Ochasi, Russ B. Altman</dc:creator>
    </item>
    <item>
      <title>Sentient House: Designing for Discourse</title>
      <link>https://arxiv.org/abs/2406.09419</link>
      <description>arXiv:2406.09419v1 Announce Type: cross 
Abstract: The Sentient House project is an investigation into approaches that the artistdesigner can take to better involve the public in developing a critical perspective on pervasive technology in the home and the surrounding environment. Using Interaction Design approaches including workshops, surveys, rapidprototyping and critical thinking, this thesis suggests a framework for developing a more participatory atmosphere for Critical Design. As the world becomes more connected, and smarter, citizens concerns are being sidelined in favour of rapid progress and solutionism. Many of these initiatives are backed by government and commercial concerns who may not have the publics best interest at heart. The designs and approaches generated from this public participation seek to provide an outlet for a more agonistic debate and to develop tools and approaches to engage the public in questioning and addressing how technology affects them in the future. The outcomes of this research suggest that the public is receptive to a more active involvement in designing their digital future, and that the designer can be a critical component in revealing hidden consequences and alternative pathways for a more transparent and desirable future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09419v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Collins</dc:creator>
    </item>
    <item>
      <title>Advancing Roadway Sign Detection with YOLO Models and Transfer Learning</title>
      <link>https://arxiv.org/abs/2406.09437</link>
      <description>arXiv:2406.09437v1 Announce Type: cross 
Abstract: Roadway signs detection and recognition is an essential element in the Advanced Driving Assistant Systems (ADAS). Several artificial intelligence methods have been used widely among of them YOLOv5 and YOLOv8. In this paper, we used a modified YOLOv5 and YOLOv8 to detect and classify different roadway signs under different illumination conditions. Experimental results indicated that for the YOLOv8 model, varying the number of epochs and batch size yields consistent MAP50 scores, ranging from 94.6% to 97.1% on the testing set. The YOLOv5 model demonstrates competitive performance, with MAP50 scores ranging from 92.4% to 96.9%. These results suggest that both models perform well across different training setups, with YOLOv8 generally achieving slightly higher MAP50 scores. These findings suggest that both models can perform well under different training setups, offering valuable insights for practitioners seeking reliable and adaptable solutions in object detection applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09437v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Selvia Nafaa, Hafsa Essam, Karim Ashour, Doaa Emad, Rana Mohamed, Mohammed Elhenawy, Huthaifa I. Ashqar, Abdallah A. Hassan, Taqwa I. Alhadidi</dc:creator>
    </item>
    <item>
      <title>Between Randomness and Arbitrariness: Some Lessons for Reliable Machine Learning at Scale</title>
      <link>https://arxiv.org/abs/2406.09548</link>
      <description>arXiv:2406.09548v1 Announce Type: cross 
Abstract: To develop rigorous knowledge about ML models -- and the systems in which they are embedded -- we need reliable measurements. But reliable measurement is fundamentally challenging, and touches on issues of reproducibility, scalability, uncertainty quantification, epistemology, and more. This dissertation addresses criteria needed to take reliability seriously: both criteria for designing meaningful metrics, and for methodologies that ensure that we can dependably and efficiently measure these metrics at scale and in practice. In doing so, this dissertation articulates a research vision for a new field of scholarship at the intersection of machine learning, law, and policy. Within this frame, we cover topics that fit under three different themes: (1) quantifying and mitigating sources of arbitrariness in ML, (2) taming randomness in uncertainty estimation and optimization algorithms, in order to achieve scalability without sacrificing reliability, and (3) providing methods for evaluating generative-AI systems, with specific focuses on quantifying memorization in language models and training latent diffusion models on open-licensed data. By making contributions in these three themes, this dissertation serves as an empirical proof by example that research on reliable measurement for machine learning is intimately and inescapably bound up with research in law and policy. These different disciplines pose similar research questions about reliable measurement in machine learning. They are, in fact, two complementary sides of the same research vision, which, broadly construed, aims to construct machine-learning systems that cohere with broader societal values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09548v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Feder Cooper</dc:creator>
    </item>
    <item>
      <title>Harm Mitigation in Recommender Systems under User Preference Dynamics</title>
      <link>https://arxiv.org/abs/2406.09882</link>
      <description>arXiv:2406.09882v1 Announce Type: cross 
Abstract: We consider a recommender system that takes into account the interplay between recommendations, the evolution of user interests, and harmful content. We model the impact of recommendations on user behavior, particularly the tendency to consume harmful content. We seek recommendation policies that establish a tradeoff between maximizing click-through rate (CTR) and mitigating harm. We establish conditions under which the user profile dynamics have a stationary point, and propose algorithms for finding an optimal recommendation policy at stationarity. We experiment on a semi-synthetic movie recommendation setting initialized with real data and observe that our policies outperform baselines at simultaneously maximizing CTR and mitigating harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09882v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jerry Chee, Shankar Kalyanaraman, Sindhu Kiranmai Ernala, Udi Weinsberg, Sarah Dean, Stratis Ioannidis</dc:creator>
    </item>
    <item>
      <title>Quantifying the Vulnerabilities of the Online Public Square to Adversarial Manipulation Tactics</title>
      <link>https://arxiv.org/abs/1907.06130</link>
      <description>arXiv:1907.06130v5 Announce Type: replace 
Abstract: Social media, seen by some as the modern public square, is vulnerable to manipulation. By controlling inauthentic accounts impersonating humans, malicious actors can amplify disinformation within target communities. The consequences of such operations are difficult to evaluate due to the challenges posed by collecting data and carrying out ethical experiments that would influence online communities. Here we use a social media model that simulates information diffusion in an empirical network to quantify the impacts of several adversarial manipulation tactics on the quality of content. We find that the presence of influential accounts, a hallmark of social media, exacerbates the vulnerabilities of online communities to manipulation. Among the explored tactics that bad actors can employ, infiltrating a community is the most likely to make low-quality content go viral. Such harm can be further compounded by inauthentic agents flooding the network with low-quality, yet appealing content, but is mitigated when bad actors focus on specific targets, such as influential or vulnerable individuals. These insights suggest countermeasures that platforms could employ to increase the resilience of social media users to manipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.06130v5</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Tran Truong, Xiaodan Lou, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Investigating writing style as a contributor to gender gaps in science and technology</title>
      <link>https://arxiv.org/abs/2204.13805</link>
      <description>arXiv:2204.13805v2 Announce Type: replace 
Abstract: A growing stream of research finds that scientific contributions are evaluated differently depending on the gender of the author. In this article, we consider whether gender differences in writing styles - how men and women communicate their work - may contribute to these observed gender gaps. We ground our investigation in a framework for characterizing the linguistic style of written text, with two sets of features - informational (i.e., features that emphasize facts) and involved (i.e., features that emphasize relationships). Using a large sample of academic papers and patents, we find significant differences in writing style by gender, with women using more involved features in their writing. Papers and patents with more involved features also tend to be cited more by women. Our findings suggest that scientific text is not devoid of personal character, which could contribute to bias in evaluation, thereby compromising the norm of universalism as a foundational principle of science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.13805v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara Kedrick, Ekaterina Levitskaya, Russell J. Funk</dc:creator>
    </item>
    <item>
      <title>Open Problems in DAOs</title>
      <link>https://arxiv.org/abs/2310.19201</link>
      <description>arXiv:2310.19201v2 Announce Type: replace 
Abstract: Decentralized autonomous organizations (DAOs) are a new, rapidly-growing class of organizations governed by smart contracts. Here we describe how researchers can contribute to the emerging science of DAOs and other digitally-constituted organizations. From granular privacy primitives to mechanism designs to model laws, we identify high-impact problems in the DAO ecosystem where existing gaps might be tackled through a new data set or by applying tools and ideas from existing research fields such as political science, computer science, economics, law, and organizational science. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the wider research community to join the global effort to invent the next generation of organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19201v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Tan, Tara Merk, Sarah Hubbard, Eliza R. Oak, Helena Rong, Joni Pirovich, Ellie Rennie, Rolf Hoefer, Michael Zargham, Jason Potts, Chris Berg, Reuben Youngblom, Primavera De Filippi, Seth Frey, Jeff Strnad, Morshed Mannan, Kelsie Nabben, Silke Noa Elrifai, Jake Hartnell, Benjamin Mako Hill, Tobin South, Ryan L. Thomas, Jonathan Dotan, Ariana Spring, Alexia Maddox, Woojin Lim, Kevin Owocki, Ari Juels, Dan Boneh</dc:creator>
    </item>
    <item>
      <title>Browsing behavior exposes identities on the Web</title>
      <link>https://arxiv.org/abs/2312.15489</link>
      <description>arXiv:2312.15489v2 Announce Type: replace 
Abstract: How easy is it to uniquely identify a person based solely on their web browsing behavior? Here we show that when people navigate the Web, their online traces produce fingerprints that identify them. Merely the four most visited web domains are enough to identify 95% of the individuals. These digital fingerprints are stable and render high re-identifiability. We demonstrate that we can re-identify 80% of the individuals in separate time slices of data. Such a privacy threat persists even with limited information about individuals' browsing behavior, reinforcing existing concerns around online privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15489v2</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcos Oliveira, Junran Yang, Daniel Griffiths, Denis Bonnay, Juhi Kulshrestha</dc:creator>
    </item>
    <item>
      <title>The Interplay of Learning, Analytics, and Artificial Intelligence in Education</title>
      <link>https://arxiv.org/abs/2403.16081</link>
      <description>arXiv:2403.16081v3 Announce Type: replace 
Abstract: This paper presents a multi-dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualizations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human mental models, and the extension of human cognition via tightly integrated human-AI systems. Examples from current research and practice are examined as instances of the three conceptualizations, highlighting the potential value and limitations of each conceptualization for education, as well as the perils of overemphasis on externalizing human cognition. It is argued that AI models can be useful as objects to think about learning, even though some aspects of learning might just come through the slow experience of living those learning moments and cannot be fully explained with AI models to be hacked with predictions. The paper concludes with advocacy for a broader approach to AI in Education that goes beyond considerations on the design and development of AI solutions in education, but also includes educating people about AI and innovating educational systems to remain relevant in an AI-ubiquitous world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16081v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mutlu Cukurova</dc:creator>
    </item>
    <item>
      <title>Mysterious and Manipulative Black Boxes: A Qualitative Analysis of Perceptions on Recommender Systems</title>
      <link>https://arxiv.org/abs/2302.09933</link>
      <description>arXiv:2302.09933v5 Announce Type: replace-cross 
Abstract: Recommender systems are used to provide relevant suggestions on various matters. Although these systems are a classical research topic, knowledge is still limited regarding the public opinion about these systems. Public opinion is also important because the systems are known to cause various problems. To this end, this paper presents a qualitative analysis of the perceptions of ordinary citizens, civil society groups, businesses, and others on recommender systems in Europe. The dataset examined is based on the answers submitted to a consultation about the Digital Services Act (DSA) recently enacted in the European Union (EU). Therefore, not only does the paper contribute to the pressing question about regulating new technologies and online platforms, but it also reveals insights about the policy-making of the DSA. According to the qualitative results, Europeans have generally negative opinions about recommender systems and the quality of their recommendations. The systems are widely seen to violate privacy and other fundamental rights. According to many Europeans, these also cause various societal problems, including even threats to democracy. Furthermore, existing regulations in the EU are commonly seen to have failed due to a lack of proper enforcement. Numerous suggestions were made by the respondents to the consultation for improving the situation, but only a few of these ended up to the DSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09933v5</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5210/fm.v29i6.13357</arxiv:DOI>
      <dc:creator>Jukka Ruohonen</dc:creator>
    </item>
    <item>
      <title>Disparate Impact on Group Accuracy of Linearization for Private Inference</title>
      <link>https://arxiv.org/abs/2402.03629</link>
      <description>arXiv:2402.03629v2 Announce Type: replace-cross 
Abstract: Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03629v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saswat Das, Marco Romanelli, Ferdinando Fioretto</dc:creator>
    </item>
    <item>
      <title>Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain</title>
      <link>https://arxiv.org/abs/2402.19226</link>
      <description>arXiv:2402.19226v3 Announce Type: replace-cross 
Abstract: Chronic pain significantly diminishes the quality of life for millions worldwide. While psychoeducation and therapy can improve pain outcomes, many individuals experiencing pain lack access to evidence-based treatments or fail to complete the necessary number of sessions to achieve benefit. Reinforcement learning (RL) shows potential in tailoring personalized pain management interventions according to patients' individual needs while ensuring the efficient use of scarce clinical resources. However, clinicians, patients, and healthcare decision-makers are concerned that RL solutions could exacerbate disparities associated with patient characteristics like race or gender. In this article, we study gender fairness in personalized pain care recommendations using a real-world application of reinforcement learning (Piette et al., 2022a). Here, adhering to gender fairness translates to minimal or no disparity in the utility received by subpopulations as defined by gender. We investigate whether the selection of relevant patient information (referred to as features) used to assist decision-making affects gender fairness. Our experiments, conducted using real-world data Piette, 2022), indicate that included features can impact gender fairness. Moreover, we propose an RL solution, NestedRecommendation, that demonstrates the ability: i) to adaptively learn to select the features that optimize for utility and fairness, and ii) to accelerate feature selection and in turn, improve pain care recommendations from early on, by leveraging clinicians' domain expertise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19226v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratik Gajane, Sean Newman, Mykola Pechenizkiy, John D. Piette</dc:creator>
    </item>
    <item>
      <title>Network-Based Transfer Learning Helps Improve Short-Term Crime Prediction Accuracy</title>
      <link>https://arxiv.org/abs/2406.06645</link>
      <description>arXiv:2406.06645v2 Announce Type: replace-cross 
Abstract: Deep learning architectures enhanced with human mobility data have been shown to improve the accuracy of short-term crime prediction models trained with historical crime data. However, human mobility data may be scarce in some regions, negatively impacting the correct training of these models. To address this issue, we propose a novel transfer learning framework for short-term crime prediction models, whereby weights from the deep learning crime prediction models trained in source regions with plenty of mobility data are transferred to target regions to fine-tune their local crime prediction models and improve crime prediction accuracy. Our results show that the proposed transfer learning framework improves the F1 scores for target cities with mobility data scarcity, especially when the number of months of available mobility data is small. We also show that the F1 score improvements are pervasive across different types of crimes and diverse cities in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06645v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahui Wu, Vanessa Frias-Martinez</dc:creator>
    </item>
    <item>
      <title>LGB: Language Model and Graph Neural Network-Driven Social Bot Detection</title>
      <link>https://arxiv.org/abs/2406.08762</link>
      <description>arXiv:2406.08762v2 Announce Type: replace-cross 
Abstract: Malicious social bots achieve their malicious purposes by spreading misinformation and inciting social public opinion, seriously endangering social security, making their detection a critical concern. Recently, graph-based bot detection methods have achieved state-of-the-art (SOTA) performance. However, our research finds many isolated and poorly linked nodes in social networks, as shown in Fig.1, which graph-based methods cannot effectively detect. To address this problem, our research focuses on effectively utilizing node semantics and network structure to jointly detect sparsely linked nodes. Given the excellent performance of language models (LMs) in natural language understanding (NLU), we propose a novel social bot detection framework LGB, which consists of two main components: language model (LM) and graph neural network (GNN). Specifically, the social account information is first extracted into unified user textual sequences, which is then used to perform supervised fine-tuning (SFT) of the language model to improve its ability to understand social account semantics. Next, the semantically enriched node representation is fed into the pre-trained GNN to further enhance the node representation by aggregating information from neighbors. Finally, LGB fuses the information from both modalities to improve the detection performance of sparsely linked nodes. Extensive experiments on two real-world datasets demonstrate that LGB consistently outperforms state-of-the-art baseline models by up to 10.95%. LGB is already online: https://botdetection.aminer.cn/robotmain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08762v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Zhou, Dan Zhang, Yuandong Wang, Yangli-ao Geng, Yuxiao Dong, Jie Tang</dc:creator>
    </item>
  </channel>
</rss>
