<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AI Load Dynamics--A Power Electronics Perspective</title>
      <link>https://arxiv.org/abs/2502.01647</link>
      <description>arXiv:2502.01647v1 Announce Type: new 
Abstract: As AI-driven computing infrastructures rapidly scale, discussions around data center design often emphasize energy consumption, water and electricity usage, workload scheduling, and thermal management. However, these perspectives often overlook the critical interplay between AI-specific load transients and power electronics. This paper addresses that gap by examining how large-scale AI workloads impose unique demands on power conversion chains and, in turn, how the power electronics themselves shape the dynamic behavior of AI-based infrastructure. We illustrate the fundamental constraints imposed by multi-stage power conversion architectures and highlight the key role of final-stage modules in defining realistic power slew rates for GPU clusters. Our analysis shows that traditional designs, optimized for slower-varying or CPU-centric workloads, may not adequately accommodate the rapid load ramps and drops characteristic of AI accelerators. To bridge this gap, we present insights into advanced converter topologies, hierarchical control methods, and energy buffering techniques that collectively enable robust and efficient power delivery. By emphasizing the bidirectional influence between AI workloads and power electronics, we hope this work can set a good starting point and offer practical design considerations to ensure future exascale-capable data centers can meet the stringent performance, reliability, and scalability requirements of next-generation AI deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01647v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhuo Li, Yunwei Li</dc:creator>
    </item>
    <item>
      <title>A Hardware-Efficient Photonic Tensor Core: Accelerating Deep Neural Networks with Structured Compression</title>
      <link>https://arxiv.org/abs/2502.01670</link>
      <description>arXiv:2502.01670v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence (AI) and deep neural networks (DNNs) have revolutionized numerous fields, enabling complex tasks by extracting intricate features from large datasets. However, the exponential growth in computational demands has outstripped the capabilities of traditional electrical hardware accelerators. Optical computing offers a promising alternative due to its inherent advantages of parallelism, high computational speed, and low power consumption. Yet, current photonic integrated circuits (PICs) designed for general matrix multiplication (GEMM) are constrained by large footprints, high costs of electro-optical (E-O) interfaces, and high control complexity, limiting their scalability. To overcome these challenges, we introduce a block-circulant photonic tensor core (CirPTC) for a structure-compressed optical neural network (StrC-ONN) architecture. By applying a structured compression strategy to weight matrices, StrC-ONN significantly reduces model parameters and hardware requirements while preserving the universal representability of networks and maintaining comparable expressivity. Additionally, we propose a hardware-aware training framework to compensate for on-chip nonidealities to improve model robustness and accuracy. We experimentally demonstrate image processing and classification tasks, achieving up to a 74.91% reduction in trainable parameters while maintaining competitive accuracies. Performance analysis expects a computational density of 5.84 tera operations per second (TOPS) per mm^2 and a power efficiency of 47.94 TOPS/W, marking a 6.87-times improvement achieved through the hardware-software co-design approach. By reducing both hardware requirements and control complexity across multiple dimensions, this work explores a new pathway to push the limits of optical computing in the pursuit of high efficiency and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01670v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shupeng Ning, Hanqing Zhu, Chenghao Feng, Jiaqi Gu, David Z. Pan, Ray T. Chen</dc:creator>
    </item>
    <item>
      <title>Life-Cycle Emissions of AI Hardware: A Cradle-To-Grave Approach and Generational Trends</title>
      <link>https://arxiv.org/abs/2502.01671</link>
      <description>arXiv:2502.01671v1 Announce Type: new 
Abstract: Specialized hardware accelerators aid the rapid advancement of artificial intelligence (AI), and their efficiency impacts AI's environmental sustainability. This study presents the first publication of a comprehensive AI accelerator life-cycle assessment (LCA) of greenhouse gas emissions, including the first publication of manufacturing emissions of an AI accelerator.
  Our analysis of five Tensor Processing Units (TPUs) encompasses all stages of the hardware lifespan - from raw material extraction, manufacturing, and disposal, to energy consumption during development, deployment, and serving of AI models. Using first-party data, it offers the most comprehensive evaluation to date of AI hardware's environmental impact. We include detailed descriptions of our LCA to act as a tutorial, road map, and inspiration for other computer engineers to perform similar LCAs to help us all understand the environmental impacts of our chips and of AI.
  A byproduct of this study is the new metric compute carbon intensity (CCI) that is helpful in evaluating AI hardware sustainability and in estimating the carbon footprint of training and inference. This study shows that CCI improves 3x from TPU v4i to TPU v6e.
  Moreover, while this paper's focus is on hardware, software advancements leverage and amplify these gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01671v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Schneider, Hui Xu, Stephan Benecke, David Patterson, Keguo Huang, Parthasarathy Ranganathan, Cooper Elsworth</dc:creator>
    </item>
    <item>
      <title>Hardware and software build flow with SoCMake</title>
      <link>https://arxiv.org/abs/2502.02065</link>
      <description>arXiv:2502.02065v1 Announce Type: new 
Abstract: The increasing demand for electronics is driving shorter development cycles for application-specific integrated circuits (ASICs). To meet these constraints, hardware designers emphasize reusability and modularity of IP blocks, leveraging standard system-on-chip (SoC) architectures with integrated processors and common interconnects. While these architectures reduce design and verification efforts, they also introduce complexity, as verification must encompass both hardware and software execution.
  To enhance reusability, hardware IP blocks are often described in higher-abstraction-level languages such as Chisel and SystemRDL, relying on compilers to generate Verilog for RTL simulation and implementation. At the system level, SoC modeling and verification leverage C++ and SystemC, underscoring the need for software compilation. Consequently, an effective build system must support both hardware design flows and software compilation, including cross-compilation for C++, C, and assembly.
  Existing hardware build systems lack sufficient support for software compilation, necessitating the development of a new solution. In response, the Microelectronics section of CERN initiated SoCMake, initially as part of the System-on-Chip Radiation Tolerant Ecosystem (SOCRATES). Designed to automate the generation of fault-tolerant RISC-V SoCs for high-energy physics environments, SoCMake has since evolved into a generic open-source build tool for SoC generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02065v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Risto Peja\v{s}inovi\'c, Alessandro Caratelli, Anvesh Nookala, Beno\^it Walter Denkinger, Marco Andorno</dc:creator>
    </item>
    <item>
      <title>Towards Efficient LUT-based PIM: A Scalable and Low-Power Approach for Modern Workloads</title>
      <link>https://arxiv.org/abs/2502.02142</link>
      <description>arXiv:2502.02142v1 Announce Type: new 
Abstract: Data movement in memory-intensive workloads, such as deep learning, incurs energy costs that are over three orders of magnitude higher than the cost of computation. Since these workloads involve frequent data transfers between memory and processing units, addressing data movement overheads is crucial for improving performance. Processing-using-memory (PuM) offers an effective solution by enabling in-memory computation, thereby minimizing data transfers. In this paper we propose Lama, a LUT-based PuM architecture designed to efficiently execute SIMD operations by supporting independent column accesses within each mat of a DRAM subarray. Lama exploits DRAM's mat-level parallelism and open-page policy to significantly reduce the number of energy-intensive memory activation (ACT) commands, which are the primary source of overhead in most PuM architectures. Unlike prior PuM solutions, Lama supports up to 8-bit operand precision without decomposing computations, while incurring only a 2.47% area overhead. Our evaluation shows Lama achieves an average performance improvement of 8.5x over state-of-the-art PuM architectures and a 3.8x improvement over CPU, along with energy efficiency gains of 6.9x/8x, respectively, for bulk 8-bit multiplication.
  We also introduce LamaAccel, an HBM-based PuM accelerator that utilizes Lama to accelerate the inference of attention-based models. LamaAccel employs exponential quantization to optimize product/accumulation in dot-product operations, transforming them into simpler tasks like addition and counting. LamaAccel delivers up to 9.3x/19.2x reduction in energy and 4.8x/9.8x speedup over TPU/GPU, along with up to 5.8x energy reduction and 2.1x speedup over a state-of-the-art PuM baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02142v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bahareh Khabbazan, Marc Riera, Antonio Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Random Adaptive Cache Placement Policy</title>
      <link>https://arxiv.org/abs/2502.02349</link>
      <description>arXiv:2502.02349v1 Announce Type: new 
Abstract: This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation. Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies. The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management. RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.
  We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate. Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02349v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vrushank Ahire, Pranav Menon, Aniruddh Muley, Abhinandan S. Prasad</dc:creator>
    </item>
    <item>
      <title>FPGA Innovation Research in the Netherlands: Present Landscape and Future Outlook</title>
      <link>https://arxiv.org/abs/2502.02404</link>
      <description>arXiv:2502.02404v1 Announce Type: new 
Abstract: FPGAs have transformed digital design by enabling versatile and customizable solutions that balance performance and power efficiency, yielding them essential for today's diverse computing challenges. Research in the Netherlands, both in academia and industry, plays a major role in developing new innovative FPGA solutions. This survey presents the current landscape of FPGA innovation research in the Netherlands by delving into ongoing projects, advancements, and breakthroughs in the field. Focusing on recent research outcome (within the past 5 years), we have identified five key research areas: a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and high-performance computing, d) programming models and tools, and e) applications. This survey provides in-depth insights beyond a mere snapshot of the current innovation research landscape by highlighting future research directions within each key area; these insights can serve as a foundational resource to inform potential national-level investments in FPGA technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02404v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Alachiotis, Sjoerd van den Belt, Steven van der Vlugt, Reinier van der Walle, Mohsen Safari, Bruno Endres Forlin, Tiziano De Matteis, Zaid Al-Ars, Roel Jordans, Ant\'onio J. Sousa de Almeida, Federico Corradi, Christiaan Baaij, Ana-Lucia Varbanescu</dc:creator>
    </item>
    <item>
      <title>DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale</title>
      <link>https://arxiv.org/abs/2502.01681</link>
      <description>arXiv:2502.01681v1 Announce Type: cross 
Abstract: Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce DeepGate4, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) an update strategy tailored for circuit graphs, which reduce memory complexity to sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse transformer with global and local structural encodings for AIGs; and (3) an inference acceleration CUDA kernel that fully exploit the unique sparsity patterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5% and 31.1% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory usage by 46.8%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01681v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Zheng, Shan Huang, Jianyuan Zhong, Zhengyuan Shi, Guohao Dai, Ningyi Xu, Qiang Xu</dc:creator>
    </item>
    <item>
      <title>Evaluating the effects of reducing voltage margins for energy-efficient operation of MPSoCs</title>
      <link>https://arxiv.org/abs/2209.12134</link>
      <description>arXiv:2209.12134v2 Announce Type: replace 
Abstract: Voltage margins, or guardbands, are imposed on DVFS systems to account for process, voltage, and temperature variability effects. While necessary to assure correctness, guardbands reduce energy efficiency, a crucial requirement for embedded systems. The literature shows that error detection techniques can be used to maintain the system's reliability while reducing or eliminating the guardbands. This letter assesses the practically available margins of a commercial RISC-V MPSoC while violating its guardband limits. The primary motivation of this work is to support the development of an efficient system leveraging the redundancy of multicore architectures for an error detection and correction scheme capable of mitigating the errors caused by aggressive voltage margin reduction. For an equivalent performance, we achieved up to 27% energy reduction while violating the manufacturer's defined guardband, leaving reasonable energy margins for further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.12134v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LES.2023.3240625</arxiv:DOI>
      <arxiv:journal_reference>IEEE Embedded Systems Letters (Volume: 16, Issue: 1, March 2024) pp.25-28</arxiv:journal_reference>
      <dc:creator>Diego V. Cirilo do Nascimento, Kyriakos Georgiou, Kerstin I. Eder, Samuel Xavier-de-Souza</dc:creator>
    </item>
    <item>
      <title>Revisiting VerilogEval: A Year of Improvements in Large-Language Models for Hardware Code Generation</title>
      <link>https://arxiv.org/abs/2408.11053</link>
      <description>arXiv:2408.11053v2 Announce Type: replace 
Abstract: The application of large-language models (LLMs) to digital hardware code generation is an emerging field, with most LLMs primarily trained on natural language and software code. Hardware code like Verilog constitutes a small portion of training data, and few hardware benchmarks exist. The open-source VerilogEval benchmark, released in November 2023, provided a consistent evaluation framework for LLMs on code completion tasks. Since then, both commercial and open models have seen significant development.
  In this work, we evaluate new commercial and open models since VerilogEval's original release-including GPT-4o, GPT-4 Turbo, Llama3.1 (8B/70B/405B), Llama3 70B, Mistral Large, DeepSeek Coder (33B and 6.7B), CodeGemma 7B, and RTL-Coder-against an improved VerilogEval benchmark suite. We find measurable improvements in state-of-the-art models: GPT-4o achieves a 63% pass rate on specification-to-RTL tasks. The recently released and open Llama3.1 405B achieves a 58% pass rate, almost matching GPT-4o, while the smaller domain-specific RTL-Coder 6.7B models achieve an impressive 34% pass rate.
  Additionally, we enhance VerilogEval's infrastructure by automatically classifying failures, introducing in-context learning support, and extending the tasks to specification-to-RTL translation. We find that prompt engineering remains crucial for achieving good pass rates and varies widely with model and task. A benchmark infrastructure that allows for prompt engineering and failure analysis is essential for continued model development and deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11053v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Pinckney, Christopher Batten, Mingjie Liu, Haoxing Ren, Brucek Khailany</dc:creator>
    </item>
    <item>
      <title>Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI</title>
      <link>https://arxiv.org/abs/2411.14299</link>
      <description>arXiv:2411.14299v3 Announce Type: replace 
Abstract: Masala-CHAI is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation. Automating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification. We identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14299v3</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jitendra Bhandari, Vineet Bhat, Yuheng He, Siddharth Garg, Hamed Rahmani, Ramesh Karri</dc:creator>
    </item>
    <item>
      <title>Hamun: An Approximate Computation Method to Prolong the Lifespan of ReRAM-Based Accelerators</title>
      <link>https://arxiv.org/abs/2502.01502</link>
      <description>arXiv:2502.01502v2 Announce Type: replace 
Abstract: ReRAM-based accelerators exhibit enormous potential to increase computational efficiency for DNN inference tasks, delivering significant performance and energy savings over traditional platforms. By incorporating adaptive scheduling, these accelerators dynamically adjust to DNN requirements, optimizing allocation of constrained hardware resources. However, ReRAM cells have limited endurance cycles due to wear-out from multiple updates for each inference execution, which shortens the lifespan of ReRAM-based accelerators and presents a practical challenge in positioning them as alternatives to conventional platforms like TPUs. Addressing these endurance limitations is essential for making ReRAM-based solutions viable for long-term, high-performance DNN inference. To address the lifespan limitations of ReRAM-based accelerators, we introduce Hamun, an approximate computing method designed to extend the lifespan of ReRAM-based accelerators through a range of optimizations. Hamun incorporates a novel mechanism that detects faulty cell due to wear-out and retires them, avoiding in this way their otherwise adverse impact on DNN accuracy. Moreover, Hamun extends the lifespan of ReRAM-based accelerators by adapting wear-leveling techniques across various abstraction levels of the accelerator and implementing a batch execution scheme to maximize ReRAM cell usage for multiple inferences. On average, evaluated on a set of popular DNNs, Hamun demonstrates an improvement in lifespan of 13.2x over a state-of-the-art baseline. The main contributors to this improvement are the fault handling and batch execution schemes, which provide 4.6x and 2.6x lifespan improvements respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01502v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sabri, Marc Riera, Antonio Gonzalez</dc:creator>
    </item>
  </channel>
</rss>
