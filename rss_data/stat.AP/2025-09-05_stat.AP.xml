<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:01:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Small Dataset May Go a Long Way: Process Duration Prediction in Clinical Settings</title>
      <link>https://arxiv.org/abs/2509.03522</link>
      <description>arXiv:2509.03522v1 Announce Type: new 
Abstract: Context: Utilization of operating theaters is a major cost driver in hospitals. Optimizing this variable through optimized surgery schedules may significantly lower cost and simultaneously improve medical outcomes. Previous studies proposed various complex models to predict the duration of procedures, the key ingredient to optimal schedules. They did so perusing large amounts of data.
  Goals: We aspire to create an effective and efficient model to predict operation durations based on only a small amount of data. Ideally, our model is also simpler in structure, and thus easier to use.
  Methods: We immerse ourselves in the application domain to leverage practitioners expertise. This way, we make the best use of our limited supply of clinical data, and may conduct our data analysis in a theory-guided way. We do a combined factor analysis and develop regression models to predict the duration of the perioperative process.
  Findings: We found simple methods of central tendency to perform on a par with much more complex methods proposed in the literature. In fact, they sometimes outperform them. We conclude that combining expert knowledge with data analysis may improve both data quality and model performance, allowing for more accurate forecasts.
  Conclusion: We yield better results than previous researchers by integrating conventional data science methods with qualitative studies of clinical settings and process structure. Thus, we are able to leverage even small datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03522v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harald St\"orrle, Anastasia Hort</dc:creator>
    </item>
    <item>
      <title>Latent space projections and atlases: A cautionary tale in deep neuroimaging using autoencoders</title>
      <link>https://arxiv.org/abs/2509.03675</link>
      <description>arXiv:2509.03675v1 Announce Type: new 
Abstract: This study introduces a deep learning pipeline for the unsupervised analysis of 3D brain MRI using a simple convolutional autoencoder architecture. Trained on segmented gray matter images from the ADNI dataset, the model learns compact latent representations that preserve neuroanatomical structure and reflect clinical variability across cognitive states. We apply dimensionality reduction techniques (PCA, tSNE, PLS, UMAP) to visualize and interpret the latent space, correlating it with anatomical regions defined by the AAL atlas. As a novel contribution, we propose the Latent Regional Correlation Profiling (LRCP) framework, which combines statistical association and supervised discriminability to identify brain regions that encode clinically relevant latent features. Our results show that even minimal architectures can capture meaningful patterns associated with progression to Alzheimer Disease. Furthermore, we validate the interpretability of latent features using SHAP-based regression and statistical agnostic methods, highlighting the importance of rigorous evaluation in neuroimaging. This work demonstrates the potential of autoencoders as exploratory tools for biomarker discovery and hypothesis generation in clinical neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03675v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. M. Gorriz, F. Segovia, C. Jimenez, J. E. Arco, F. J. Martinez, J Ramirez, S. Abulikemu, J. Suckling</dc:creator>
    </item>
    <item>
      <title>Seasonal and periodic patterns of ischemic heart disease in New York using the Variable Multiple Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2509.03710</link>
      <description>arXiv:2509.03710v1 Announce Type: new 
Abstract: Seasonal patterns of the incidence, hospital visits, and mortality of ischemic heart disease (IHD) have been widely reported. This study aims to investigate seasonal and periodic patterns of IHD hospitalizations in New York using a novel bootstrap approach, the Variable Bandpass Periodic Block Bootstrap (VBPBB) method. Using a bandpass filter, VBPBB isolates the periodically correlated (PC) component of interest from other PC components and noise before bootstrapping, preserving correlation structures and yielding more precise 95\% confidence intervals than existing periodic bootstrapping methods. We examine weekly, monthly, and annual patterns, along with their harmonic frequencies, in the IHD hospitalization. In addition to the pre-defined frequencies, we also examine the frequencies with the highest amplitudes in the periodogram. By aggregating bootstrap results from statistically significant PC components, a 95\% CI band that preserves multiple periodic correlation structures was obtained. Statistically significant variation was observed for the weekly, annual component, and its 2nd, 3rd, 5th, and 6th harmonics. CI bands obtained from VBPBB were much narrower than those from existing periodic bootstrapping methods. VBPBB substantially improves the precision of periodic mean estimates while preserving periodic correlation structures, making it suitable for time series with multiple periodic patterns and high noise, such as in environmental or healthcare data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03710v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yineng Chen, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>History matching for functional data and its application to tsunami warnings in the Indian Ocean</title>
      <link>https://arxiv.org/abs/2509.04342</link>
      <description>arXiv:2509.04342v1 Announce Type: new 
Abstract: Traditional History Matching (HM) identifies implausible regions of the input parameter space by comparing scalar outputs of a computer model to observations. It offers higher computational efficiency than Bayesian calibration, making it suitable for high-dimensional problems. However, in real physical systems, outputs are often functional, such as time series or spatial fields, and conventional HM cannot fully exploit such information. We propose a novel method, Functional History Matching (FHM), which extends HM to handle functional data. FHM incorporates the Outer Product Emulator, an extension of the Gaussian Process emulator designed for time series, to enhance computational efficiency. FHM also leverages Random Projection to extract dynamic features from infinite-dimensional data, including derivatives. FHM supports uncertainty quantification essential for decision-making and naturally accommodates model discrepancies. To demonstrate its practical effectiveness, we apply FHM to a synthetic tsunami forecasting scenario in the Indian Ocean, assuming a realistic event in the Makran subduction zone. Wave elevation time series from offshore buoy data are used to predict wave elevations over the Indian coastline. Our results show that FHM significantly outperforms scalar-based HM in accuracy. FHM enables reliable forecasting from functional data within feasible computational constraints, offering a robust framework for early warning systems and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04342v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryuichi Kanai, Nicol\'as Hern\'andez, Devaraj Gopinathan, Serge Guillas</dc:creator>
    </item>
    <item>
      <title>Aligning load flexibility with emissions reduction: empirical insights from a multi-site study of cryptocurrency data centers</title>
      <link>https://arxiv.org/abs/2509.04380</link>
      <description>arXiv:2509.04380v1 Announce Type: new 
Abstract: The power sector is responsible for 32 percent of global greenhouse gas emissions. Data centers and cryptocurrencies use significant amounts of electricity and contribute to these emissions. Demand-side flexibility of data centers is one possible approach for reducing greenhouse gas emissions from these industries. To explore this, we use novel data collected from the Bitcoin mining industry to investigate the impact of load flexibility on power system decarbonization. Employing engineered metrics to explore curtailment dynamics and emissions alignment, we provide the first empirical analysis of cryptocurrency data centers' capability for reducing greenhouse gas emissions in response to real-time grid signals. Our results highlight the importance of strategically aligning operational behaviors with emissions signals to maximize avoided emissions. These findings offer insights for policymakers and industry stakeholders to enhance load flexibility and meet climate goals in these otherwise energy intensive data centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04380v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Veronica M. Paez, Neda Mohammadi, John E. Taylor</dc:creator>
    </item>
    <item>
      <title>Multilayer networks characterize human-mobility patterns by industry sector for the 2021 Texas winter storm</title>
      <link>https://arxiv.org/abs/2509.03642</link>
      <description>arXiv:2509.03642v1 Announce Type: cross 
Abstract: Understanding human mobility during disastrous events is crucial for emergency planning and disaster management. Here, we develop a methodology involving the construction of time-varying, multilayer networks in which edges encode observed movements between spatial regions (census tracts) and network layers encode different movement categories according to industry sectors (e.g., visitations to schools, hospitals, and grocery stores). This approach provides a rich characterization of human mobility, thereby complementing studies examining the risk-aversion activities of evacuation and sheltering in place. Focusing on the 2021 Texas winter storm as a case study which led to many casualties, we find that people largely reduced their movements to ambulatory healthcare services, restaurants, and schools, but prioritized movements to grocery stores and gas stations. Additionally, we study the predictability of nodes' in- and out-degrees in the multilayer networks, which encode movements into and out of census tracts. We find that inward movements are harder to predict than outward movements, and even more so during this winter storm. Our findings about the reduction, prioritization, and predictability of sector-specific human movements could inform mobility-related decisions arising from future extreme weather events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03642v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Butler, Alisha Khan, Francis Afrifa, Yingjie Hu, Dane Taylor</dc:creator>
    </item>
    <item>
      <title>How many patients could we save with LLM priors?</title>
      <link>https://arxiv.org/abs/2509.04250</link>
      <description>arXiv:2509.04250v1 Announce Type: cross 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04250v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer</dc:creator>
    </item>
    <item>
      <title>A Framework for Supervised and Unsupervised Segmentation and Classification of Materials Microstructure Images</title>
      <link>https://arxiv.org/abs/2502.07107</link>
      <description>arXiv:2502.07107v2 Announce Type: replace 
Abstract: Microstructure of materials is often characterized through image analysis to understand processing-structure-properties linkages. We propose a largely automated framework that integrates unsupervised and supervised learning methods to classify micrographs according to microstructure phase/class and, for multiphase microstructures, segments them into different homogeneous regions. With the advance of manufacturing and imaging techniques, the ultra-high resolution of imaging that reveals the complexity of microstructures and the rapidly increasing quantity of images (i.e., micrographs) enables and necessitates a more powerful and automated framework to extract materials characteristics and knowledge. The framework we propose can be used to gradually build a database of microstructure classes relevant to a particular process or group of materials, which can help in analyzing and discovering/identifying new materials. The framework has three steps: (1) segmentation of multiphase micrographs through a recently developed score-based method so that different microstructure homogeneous regions can be identified in an unsupervised manner; (2) {identification and classification of} homogeneous regions of micrographs through an uncertainty-aware supervised classification network trained using the segmented micrographs from Step $1$ with their identified labels verified via the built-in uncertainty quantification and minimal human inspection; (3) supervised segmentation (more powerful than the segmentation in Step $1$) of multiphase microstructures through a segmentation network trained with micrographs and the results from Steps $1$-$2$ using a form of data augmentation. This framework can iteratively characterize/segment new homogeneous or multiphase materials while expanding the database to enhance performance. The framework is demonstrated on various sets of materials and texture images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07107v2</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kungang Zhang, Wei Chen, Wing K. Liu, L. Catherine Brinson, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>dsld: A Socially Relevant Tool for Teaching Statistics</title>
      <link>https://arxiv.org/abs/2411.04228</link>
      <description>arXiv:2411.04228v3 Announce Type: replace-cross 
Abstract: The growing influence of data science in statistics education requires tools that make key concepts accessible through real-world applications. We introduce "Data Science Looks At Discrimination" (dsld), an R package that provides a comprehensive set of analytical and graphical methods for examining issues of discrimination involving attributes such as race, gender, and age. By positioning fairness analysis as a teaching tool, the package enables instructors to demonstrate confounder effects, model bias, and related topics through applied examples. An accompanying 80-page Quarto book guides students and legal professionals in understanding these principles and applying them to real data. We describe the implementation of the package functions and illustrate their use with examples. Python interfaces are also available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04228v3</guid>
      <category>stat.ME</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Mittal, Taha Abdullah, Arjun Ashok, Brandon Zarate Estrada, Shubhada Martha, Billy Ouattara, Jonathan Tran, Norman Matloff</dc:creator>
    </item>
    <item>
      <title>Increasing competitiveness by imbalanced groups: The example of the 48-team FIFA World Cup</title>
      <link>https://arxiv.org/abs/2502.08565</link>
      <description>arXiv:2502.08565v2 Announce Type: replace-cross 
Abstract: A match played in a sports tournament can be called stakeless if at least one team is indifferent to its outcome because it already has qualified or has been eliminated. Such a game threatens fairness since teams may not exert full effort without incentives. This paper suggests a novel classification for stakeless matches based on their expected outcome: they are more costly if the indifferent team is more likely to win by playing honestly. Our approach is illustrated with the 2026 FIFA World Cup, the first edition of the competition with 48 teams. We propose a novel format based on imbalanced groups, which substantially reduces the probability of stakeless matches played by the strongest teams according to Monte Carlo simulations. The new design also increases the uncertainty of match outcomes and requires fewer matches. Governing bodies in sports are encouraged to consider our innovative idea in order to enhance the competitiveness of their tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08565v2</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi</dc:creator>
    </item>
    <item>
      <title>Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms</title>
      <link>https://arxiv.org/abs/2509.00167</link>
      <description>arXiv:2509.00167v2 Announce Type: replace-cross 
Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00167v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. F. Lamberti, S. R. Lawrence, D. White, S. Kim, S. Abdullah</dc:creator>
    </item>
  </channel>
</rss>
