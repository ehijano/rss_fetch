<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When an Approximate Model Suffices for Optimal Control</title>
      <link>https://arxiv.org/abs/2601.09826</link>
      <description>arXiv:2601.09826v1 Announce Type: new 
Abstract: In this paper, we develop an optimal control framework for dynamical systems when only an approximate model of the underlying plant is available. We consider a setting in which the control strategy is synthesized using a model-based optimal control problem that includes a penalty term capturing deviation from the plant trajectory, while the same control input is applied to both the model and the actual system. For a general class of optimal control problems, we establish conditions under which the control minimizing the model-based Hamiltonian coincides with the plant-optimal control, despite mismatch between the model and the true dynamics. We further specialize these results to problems with quadratic control effort, where explicit and easily verifiable sufficient conditions guarantee equivalence and uniqueness of the resulting optimal control. These results show that accurate control synthesis does not require an exact model of the underlying system, but rather alignment of the optimality conditions that govern control selection. From a learning perspective, this suggests that data-driven efforts can focus on identifying regimes in which model-based and plant-based Hamiltonian minimizers coincide, thereby providing a theoretical basis for robust model-based decision making and the effective use of digital twins under modeling error. We provide examples to illustrate the theoretical findings and demonstrate equivalence of the resulting control trajectories even in the presence of significant model mismatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09826v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas A. Malikopoulos</dc:creator>
    </item>
    <item>
      <title>Global convergence of the subgradient method for robust signal recovery</title>
      <link>https://arxiv.org/abs/2601.10062</link>
      <description>arXiv:2601.10062v1 Announce Type: new 
Abstract: We study the subgradient method for factorized robust signal recovery problems, including robust PCA, robust phase retrieval, and robust matrix sensing. These objectives are nonsmooth and nonconvex, and may have unbounded sublevel sets, so standard arguments for analyzing first-order optimization algorithms based on descent and coercivity do not apply. For locally Lipschitz semialgebraic objectives, we develop a convergence framework under the assumption that continuous-time subgradient trajectories are bounded: for sufficiently small step sizes of order \(1/k\), any subgradient sequence remains bounded and converges to a critical point. We verify this trajectory boundedness assumption for the robust objectives by adapting and extending existing trajectory analyses, requiring only a mild nondegeneracy condition in the matrix sensing case. Finally, for rank-one symmetric robust PCA, we show that the subgradient method avoids spurious critical points for almost every initialization, and therefore converges to a global minimum under the same step-size regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10062v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zesheng Cai, Lexiao Lai, Tiansheng Li</dc:creator>
    </item>
    <item>
      <title>Line-search and Adaptive Step Sizes for Nonconvex-strongly-concave Minimax Optimization</title>
      <link>https://arxiv.org/abs/2601.10086</link>
      <description>arXiv:2601.10086v1 Announce Type: new 
Abstract: In this paper, we propose a novel reformulation of the smooth nonconvex-strongly-concave (NC-SC) minimax problems that casts the problem as a joint minimization. We show that our reformulation preserves not only first-order stationarity, but also global and local optimality, second-order stationarity, and the Kurdyka-{\L}ojasiewicz (KL) property, of the original NC-SC problem, which is substantially stronger than its nonsmooth counterpart in the literature. With these enhanced structures, we design a versatile parameter-free and nonmonotone line-search framework that does not require evaluating the inner maximization. Under mild conditions, global convergence rates can be obtained, and, with KL property, full sequence convergence with asymptotic rates is also established. In particular, we show our framework is compatible with the gradient descent-ascent (GDA) algorithm. By equipping GDA with Barzilai-Borwein (BB) step sizes and nonmonotone line-search, our method exhibits superior numerical performance against the compared benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10086v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohao Ma, Nachuan Xiao, Junyu Zhang</dc:creator>
    </item>
    <item>
      <title>Controllability score for linear time-invariant systems on an infinite time horizon</title>
      <link>https://arxiv.org/abs/2601.10260</link>
      <description>arXiv:2601.10260v1 Announce Type: new 
Abstract: We introduce a scaled controllability Gramian that can be computed reliably even for unstable systems. Using this scaled Gramian, we reformulate the controllability scoring problems into equivalent but numerically stable optimization problems. Their optimal solutions define dynamics-aware network centrality measures, referred to as the volumetric controllability score (VCS) and the average energy controllability score (AECS). We then formulate controllability scoring problems on an infinite time horizon. Under suitable assumptions, we prove that the resulting VCS and AECS are unique and that the finite-horizon scores converge to them. We further show that VCS and AECS can differ markedly in this limit, because VCS enforces controllability of the full system, whereas AECS accounts only for the stable modes. Finally, using Laplacian dynamics as a representative example, we present numerical experiments that illustrate this convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10260v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kota Umezu, Kazuhiro Sato</dc:creator>
    </item>
    <item>
      <title>A two-step inertial method with a new step-size rule for variational inequalities in hilbert spaces</title>
      <link>https://arxiv.org/abs/2601.10370</link>
      <description>arXiv:2601.10370v1 Announce Type: new 
Abstract: In this paper, a two-step inertial Tseng extragradient method involving self-adaptive and Armijo-like step sizes is introduced for solving variational inequalities with a quasimonotone cost function in the setting of a real Hilbert space. Weak convergence of the sequence generated by the proposed algorithm is proved without assuming the Lipschitz condition. An interesting feature of the proposed algorithm is its ability to select the better step size between the self-adaptive and Armijo-like options at each iteration step. Moreover, removing the requirement for the Lipschitz condition on the cost function broadens the applicability of the proposed method. Finally, the algorithm accelerates and complements several existing iterative algorithms for solving variational inequalities in Hilbert spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10370v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian-Wen Peng, Jun-Jie Luo, Abubakar Adamu</dc:creator>
    </item>
    <item>
      <title>Algebraic Farkas Lemma and Strong Duality for Perturbed Conic Linear Programming</title>
      <link>https://arxiv.org/abs/2601.10390</link>
      <description>arXiv:2601.10390v1 Announce Type: new 
Abstract: This paper addresses the study of algebraic versions of Farkas lemma and strong duality results in the very broad setting of infinite-dimensional conic linear programming in dual pairs of vector spaces. To this end, purely algebraic properties of perturbed optimal value functions of both primal and dual problems and their corresponding hypergraph/epigraph are investigated. The newly developed hypergraphical/epigraphical sets, inspired by Kretschmer's closedness conditions \cite{Kretschmer61}, together with their novel convex separation-type characterizations, give rise to various perturbed Farkas-type lemmas which allow us to derive complete characterizations of ``zero duality gap''. Principally, when certain structures of algebraic or topological duals are imposed, illuminating implications of the developed condition are also explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10390v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>P. D. Khanh, V. V. H. Khoa, T. H. Mo</dc:creator>
    </item>
    <item>
      <title>Positive Damping Region: A Graphic Tool for Passivization Analysis with Passivity Index</title>
      <link>https://arxiv.org/abs/2601.10475</link>
      <description>arXiv:2601.10475v1 Announce Type: new 
Abstract: This paper presents a geometric framework for analyzing output-feedback and input-feedforward passivization of linear time-invariant systems. We reveal that a system is passivizable with a given passivity index when the Nyquist plot for SISO systems or the Rayleigh quotient of the transfer function for MIMO systems lies within a specific, index-dependent region in the complex plane, termed the positive damping region. The criteria enable a convenient graphic tool for analyzing the passivization, the associated frequency bands, the maximum achievable passivity index, and the waterbed effect between them. Additionally, the tool can be encoded into classical tools such as the Nyquist plot, the Nichols plot, and the generalized KYP lemma to aid control design. Finally, we demonstrate its application in passivity-based power system stability analysis and discuss its implications for electrical engineers regarding device controller design trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10475v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Peng, Xi Ru, Zhongze Li, Jianxin Zhang, Xinghua Chen, Feng Liu</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Analysis of Gradient Flow for Extensive-Width Quadratic Neural Networks</title>
      <link>https://arxiv.org/abs/2601.10483</link>
      <description>arXiv:2601.10483v1 Announce Type: new 
Abstract: We study the high-dimensional training dynamics of a shallow neural network with quadratic activation in a teacher-student setup. We focus on the extensive-width regime, where the teacher and student network widths scale proportionally with the input dimension, and the sample size grows quadratically. This scaling aims to describe overparameterized neural networks in which feature learning still plays a central role. In the high-dimensional limit, we derive a dynamical characterization of the gradient flow, in the spirit of dynamical mean-field theory (DMFT). Under l2-regularization, we analyze these equations at long times and characterize the performance and spectral properties of the resulting estimator. This result provides a quantitative understanding of the effect of overparameterization on learning and generalization, and reveals a double descent phenomenon in the presence of label noise, where generalization improves beyond interpolation. In the small regularization limit, we obtain an exact expression for the perfect recovery threshold as a function of the network widths, providing a precise characterization of how overparameterization influences recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10483v1</guid>
      <category>math.OC</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Martin (DI-ENS, LPENS, SIERRA), Giulio Biroli (LPENS), Francis Bach (DI-ENS, SIERRA)</dc:creator>
    </item>
    <item>
      <title>Self-Organizing Dual-Buffer Adaptive Clustering Experience Replay (SODASER) for Safe Reinforcement Learning in Optimal Control</title>
      <link>https://arxiv.org/abs/2601.06540</link>
      <description>arXiv:2601.06540v1 Announce Type: cross 
Abstract: This paper proposes a novel reinforcement learning framework, named Self-Organizing Dual-buffer Adaptive Clustering Experience Replay (SODACER), designed to achieve safe and scalable optimal control of nonlinear systems. The proposed SODACER mechanism consisting of a Fast-Buffer for rapid adaptation to recent experiences and a Slow-Buffer equipped with a self-organizing adaptive clustering mechanism to maintain diverse and non-redundant historical experiences. The adaptive clustering mechanism dynamically prunes redundant samples, optimizing memory efficiency while retaining critical environmental patterns. The approach integrates SODASER with Control Barrier Functions (CBFs) to guarantee safety by enforcing state and input constraints throughout the learning process. To enhance convergence and stability, the framework is combined with the Sophia optimizer, enabling adaptive second-order gradient updates. The proposed SODACER-Sophia's architecture ensures reliable, effective, and robust learning in dynamic, safety-critical environments, offering a generalizable solution for applications in robotics, healthcare, and large-scale system optimization. The proposed approach is validated on a nonlinear Human Papillomavirus (HPV) transmission model with multiple control inputs and safety constraints. Comparative evaluations against random and clustering-based experience replay methods demonstrate that SODACER achieves faster convergence, improved sample efficiency, and a superior bias-variance trade-off, while maintaining safe system trajectories, validated via the Friedman test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06540v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roya Khalili Amirabadi, Mohsen Jalaeian Farimani, Omid Solaymani Fard</dc:creator>
    </item>
    <item>
      <title>Forecasting Seasonal Peaks of Pediatric Respiratory Infections Using an Alert-Based Model Combining SIR Dynamics and Historical Trends in Santiago, Chile</title>
      <link>https://arxiv.org/abs/2601.09821</link>
      <description>arXiv:2601.09821v1 Announce Type: cross 
Abstract: Acute respiratory infections (ARI) are a major cause of pediatric hospitalization in Chile, producing marked winter increases in demand that challenge hospital planning. This study presents an alert-based forecasting model to predict the timing and magnitude of ARI hospitalization peaks in Santiago. The approach integrates a seasonal SIR model with a historical mobile predictor, activated by a derivative-based alert system that detects early epidemic growth. Daily hospitalization data from DEIS were smoothed using a 15-day moving average and Savitzky-Golay filtering, and parameters were estimated using a penalized loss function to reduce sensitivity to noise. Retrospective evaluation and real-world implementation in major Santiago pediatric hospitals during 2023 and 2024 show that peak date can be anticipated about one month before the event and predicted with high accuracy two weeks in advance. Peak magnitude becomes informative roughly ten days before the peak and stabilizes one week prior. The model provides a practical and interpretable tool for hospital preparedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09821v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gloria Henr\'iquez, Jhoan B\'aez, V\'ictor Riquelme, Pedro Gajardo, Michel Royer, H\'ector Ram\'irez</dc:creator>
    </item>
    <item>
      <title>A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch</title>
      <link>https://arxiv.org/abs/2601.09831</link>
      <description>arXiv:2601.09831v1 Announce Type: cross 
Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09831v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guixian Xu, Jinglai Li, Junqi Tang</dc:creator>
    </item>
    <item>
      <title>Accelerated Regularized Wasserstein Proximal Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2601.09848</link>
      <description>arXiv:2601.09848v1 Announce Type: cross 
Abstract: We consider sampling from a Gibbs distribution by evolving a finite number of particles using a particular score estimator rather than Brownian motion. To accelerate the particles, we consider a second-order score-based ODE, similar to Nesterov acceleration. In contrast to traditional kernel density score estimation, we use the recently proposed regularized Wasserstein proximal method, yielding the Accelerated Regularized Wasserstein Proximal method (ARWP). We provide a detailed analysis of continuous- and discrete-time non-asymptotic and asymptotic mixing rates for Gaussian initial and target distributions, using techniques from Euclidean acceleration and accelerated information gradients. Compared with the kinetic Langevin sampling algorithm, the proposed algorithm exhibits a higher contraction rate in the asymptotic time regime. Numerical experiments are conducted across various low-dimensional experiments, including multi-modal Gaussian mixtures and ill-conditioned Rosenbrock distributions. ARWP exhibits structured and convergent particles, accelerated discrete-time mixing, and faster tail exploration than the non-accelerated regularized Wasserstein proximal method and kinetic Langevin methods. Additionally, ARWP particles exhibit better generalization properties for some non-log-concave Bayesian neural network tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09848v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Stanley Osher, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series</title>
      <link>https://arxiv.org/abs/2601.09949</link>
      <description>arXiv:2601.09949v1 Announce Type: cross 
Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09949v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Griffin Kearney</dc:creator>
    </item>
    <item>
      <title>On directional second-order tangent sets of analytic sets and applications in optimization</title>
      <link>https://arxiv.org/abs/2601.09991</link>
      <description>arXiv:2601.09991v1 Announce Type: cross 
Abstract: In this paper we study directional second-order tangent sets of real and complex analytic sets. For an analytic set $X\subseteq\mathbb{K}^n$ and a nonzero tangent direction $u\in T_0X$, we compare the geometric second-order tangent set $T^2_{0,u}X$, defined via second-order expansions of analytic arcs, with the algebraic second-order tangent set $T^{2,a}_{0,u}X$, defined by initial forms of the defining equations. We prove the general inclusion $T^2_{0,u}X\subseteq T^{2,a}_{0,u}X$ and construct explicit real and complex analytic examples showing that the inclusion is strict.
  We introduce a second-jet formulation along fixed tangent directions and show that $T^2_{0,u}X=T^{2,a}_{0,u}X$ if and only if the natural second-jet map from analytic arcs in $X$ to jets on the tangent cone $C_0X$ is surjective. This surjectivity is established for smooth analytic germs, homogeneous analytic cones, hypersurfaces with nondegenerate tangent directions, and nondegenerate analytic complete intersections. As an application, we derive second-order necessary and sufficient optimality conditions for $C^2$ optimization problems on analytic sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09991v1</guid>
      <category>math.AG</category>
      <category>math.CV</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Le Cong Trinh</dc:creator>
    </item>
    <item>
      <title>Introduction to optimization methods for training SciML models</title>
      <link>https://arxiv.org/abs/2601.10222</link>
      <description>arXiv:2601.10222v1 Announce Type: cross 
Abstract: Optimization is central to both modern machine learning (ML) and scientific machine learning (SciML), yet the structure of the underlying optimization problems differs substantially across these domains. Classical ML typically relies on stochastic, sample-separable objectives that favor first-order and adaptive gradient methods. In contrast, SciML often involves physics-informed or operator-constrained formulations in which differential operators induce global coupling, stiffness, and strong anisotropy in the loss landscape. As a result, optimization behavior in SciML is governed by the spectral properties of the underlying physical models rather than by data statistics, frequently limiting the effectiveness of standard stochastic methods and motivating deterministic or curvature-aware approaches. This document provides a unified introduction to optimization methods in ML and SciML, emphasizing how problem structure shapes algorithmic choices. We review first- and second-order optimization techniques in both deterministic and stochastic settings, discuss their adaptation to physics-constrained and data-driven SciML models, and illustrate practical strategies through tutorial examples, while highlighting open research directions at the interface of scientific computing and scientific machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10222v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alena Kopani\v{c}\'akov\'a, Elisa Riccietti</dc:creator>
    </item>
    <item>
      <title>Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers</title>
      <link>https://arxiv.org/abs/2601.10274</link>
      <description>arXiv:2601.10274v1 Announce Type: cross 
Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10274v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emre Ozbas, Melih Bastopcu</dc:creator>
    </item>
    <item>
      <title>Optimisation of the lowest Robin eigenvalue in exterior domains of the hyperbolic plane</title>
      <link>https://arxiv.org/abs/2601.10280</link>
      <description>arXiv:2601.10280v1 Announce Type: cross 
Abstract: We consider the Robin Laplacian in the exterior of a bounded simply-connected Lipschitz domain in the hyperbolic plane. We show that the essential spectrum of this operator is $[\frac14,\infty)$ and that, under convexity assumption on the domain, there exist discrete eigenvalues below $\frac14$ if, and only if, the Robin parameter is below a non-positive critical constant, which depends on the shape of the domain. As the main result, we prove that the lowest Robin eigenvalue for the exterior of a bounded geodesically convex domain $\Omega$ in the hyperbolic plane does not exceed such an eigenvalue for the exterior of the geodesic disk, whose geodesic curvature of the boundary is not smaller than the averaged geodesic curvature of the boundary of $\Omega$. This result implies as a consequence that under fixed area or fixed perimeter constraints the exterior of the geodesic disk maximises the lowest Robin eigenvalue among exteriors of bounded geodesically convex domains. Moreover, we obtain under the same geometric constraints a reverse inequality between the critical constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10280v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Celentano, David Krejcirik, Vladimir Lotoreichik</dc:creator>
    </item>
    <item>
      <title>Dynamic reinsurance via martingale transport</title>
      <link>https://arxiv.org/abs/2601.10375</link>
      <description>arXiv:2601.10375v1 Announce Type: cross 
Abstract: We formulate a dynamic reinsurance problem in which the insurer seeks to control the terminal distribution of its surplus while minimizing the L2-norm of the ceded risk. Using techniques from martingale optimal transport, we show that, under suitable assumptions, the problem admits a tractable solution analogous to the Bass martingale. We first consider the case where the insurer wants to match a given terminal distribution of the surplus process, and then relax this condition by only requiring certain moment or risk-based constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10375v1</guid>
      <category>q-fin.RM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Beatrice Acciaio, Brandon Garcia Flores, Antonio Marini, Gudmund Pammer</dc:creator>
    </item>
    <item>
      <title>A Mirror-Descent Algorithm for Computing the Petz-R\'enyi Capacity of Classical-Quantum Channels</title>
      <link>https://arxiv.org/abs/2601.10558</link>
      <description>arXiv:2601.10558v1 Announce Type: cross 
Abstract: We study the computation of the $\alpha$-R\'enyi capacity of a classical-quantum (c-q) channel for $\alpha\in(0,1)$. We propose an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Our analysis establishes relative smoothness with respect to the entropy geometry, guaranteeing a global sublinear convergence of the objective values. Furthermore, under a natural tangent-space nondegeneracy condition (and a mild spectral lower bound in one regime), we prove local linear (geometric) convergence in Kullback-Leibler divergence on a truncated probability simplex, with an explicit contraction factor once the local curvature constants are bounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10558v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Hong Lai, Hao-Chung Cheng</dc:creator>
    </item>
    <item>
      <title>Combinatorial Optimization Augmented Machine Learning</title>
      <link>https://arxiv.org/abs/2601.10583</link>
      <description>arXiv:2601.10583v1 Announce Type: cross 
Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10583v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Schiffer, Heiko Hoppe, Yue Su, Louis Bouvier, Axel Parmentier</dc:creator>
    </item>
    <item>
      <title>Mixed-Integer Linear Optimization for Semi-Supervised Optimal Classification Trees</title>
      <link>https://arxiv.org/abs/2401.09848</link>
      <description>arXiv:2401.09848v2 Announce Type: replace 
Abstract: Decision trees are one of the most popular methods for solving classification problems, mainly because of their good interpretability properties. Moreover, due to advances in recent years in mixed-integer optimization, several models have been proposed to formulate the problem of computing optimal classification trees. The goal is, given a set of labeled points, to split the feature spacewith hyperplanes and assign a class to each part of the resulting partition. In certain scenarios, however, labels are only available for a subset of the given points. Additionally, this subset may be non-representative, such as in the case of self-selection in a survey. Semi-supervised decision trees tackle the setting of labeled and unlabeled data and often contribute to enhancing the reliability of the results. Furthermore, undisclosed sources may provide extra information about the size of the classes. We propose a mixed-integer linear optimization model for computing semi-supervised optimal classification trees that cover the setting of labeled and unlabeled data points as well as the overall number of points in each class for a binary classification. Our numerical results show that our approach leads to a better accuracy and a better Matthews correlation coefficient for biased samples compared to other optimal classification trees, even if onlyfew labeled points are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09848v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Pablo Burgard, Maria Eduarda Pinheiro, Martin Schmidt</dc:creator>
    </item>
    <item>
      <title>Convex optimization with $p$-norm oracles</title>
      <link>https://arxiv.org/abs/2410.24158</link>
      <description>arXiv:2410.24158v2 Announce Type: replace 
Abstract: In recent years, there have been significant advances in efficiently solving $\ell_s$-regression using linear system solvers and $\ell_2$-regression [Adil-Kyng-Peng-Sachdeva, J. ACM'24]. Would efficient smoothed $\ell_p$-norm solvers lead to even faster rates for solving $\ell_s$-regression when $2 \leq p &lt; s$? In this paper, we give an affirmative answer to this question and show how to solve $\ell_s$-regression using $\tilde{O}(n^{\frac{\nu}{1+\nu}})$ iterations of solving smoothed $\ell_p$ regression problems, where $\nu := \frac{1}{p} - \frac{1}{s}$. To obtain this result, we provide improved accelerated rates for convex optimization problems when given access to an $\ell_p^s(\lambda)$-proximal oracle, which, for a point $c$, returns the solution of the regularized problem $\min_{x} f(x) + \lambda ||x-c||_p^s$. Additionally, we show that these rates for the $\ell_p^s(\lambda)$-proximal oracle are optimal for algorithms that query in the span of the outputs of the oracle, and we further apply our techniques to settings of high-order and quasi-self-concordant optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24158v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deeksha Adil, Brian Bullins, Arun Jambulapati, Aaron Sidford</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Fault Detection Trade-off Design with Prior Fault Information</title>
      <link>https://arxiv.org/abs/2412.20237</link>
      <description>arXiv:2412.20237v5 Announce Type: replace 
Abstract: The robustness of fault detection algorithms against uncertainty is crucial in the real-world industrial environment. Recently, a new probabilistic design scheme called distributionally robust fault detection (DRFD) has emerged and received immense interest. Despite its robustness against unknown distributions in practice, current DRFD focuses on the overall detectability of all possible faults rather than the detectability of critical faults that are a priori known. Henceforth, a new DRFD trade-off design scheme is put forward in this work by utilizing prior fault information. The key contribution includes a novel distributional robustness metric of detecting a known fault and a new relaxed distributionally robust chance constraint that ensures robust detectability. Then, a new DRFD design problem of fault detection under unknown probability distributions is proposed, and this offers a flexible balance between the robustness of detecting known critical faults and the overall detectability against all possible faults. To address the resulting semi-infinite chance-constrained problem, we first reformulate it to a finite-dimensional problem characterized by bilinear matrix inequalities. Subsequently, a tailored heuristic solution algorithm is developed, which includes a sequential minimization procedure and an initialization strategy. Finally, case studies on a simulated three-tank system and a real-world battery cell are carried out to showcase the effectiveness of the proposed heuristic algorithm and the advantages of our DRFD method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20237v5</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin Feng, Hailang Jin, Steven X. Ding, Hao Ye, Chao Shang</dc:creator>
    </item>
    <item>
      <title>Non-Expansive Mappings in Two-Time-Scale Stochastic Approximation: Finite-Time Analysis</title>
      <link>https://arxiv.org/abs/2501.10806</link>
      <description>arXiv:2501.10806v3 Announce Type: replace 
Abstract: Two-time-scale stochastic approximation algorithms are iterative methods used in applications such as optimization, reinforcement learning, and control. Finite-time analysis of these algorithms has primarily focused on fixed point iterations where both time-scales have contractive mappings. In this work, we broaden the scope of such analyses by considering settings where the slower time-scale has a non-expansive mapping. For such algorithms, the slower time-scale can be viewed as a stochastic inexact Krasnoselskii-Mann iteration. We also study a variant where the faster time-scale has a projection step which leads to non-expansiveness in the slower time-scale. We show that the last-iterate mean square residual error for such algorithms decays at a rate $O(1/k^{1/4-\epsilon})$, where $\epsilon&gt;0$ is arbitrarily small. We further establish almost sure convergence of iterates to the set of fixed points. We demonstrate the applicability of our framework by applying our results to minimax optimization, linear stochastic approximation, and Lagrangian optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10806v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Chandak</dc:creator>
    </item>
    <item>
      <title>INTHOP: A Second-Order Globally Convergent Method for Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2510.22342</link>
      <description>arXiv:2510.22342v3 Announce Type: replace 
Abstract: Second-order Newton-type algorithms that leverage the exact Hessian or its approximation are central to solve nonlinear optimization problems. However, their applications in solving large-scale nonconvex problems are hindered by three primary challenges: (1) the high computational cost associated with Hessian evaluations, (2) its inversion, and (3) ensuring descent direction at points where the Hessian becomes indefinite. We propose INTHOP, an interval Hessian-based optimization algorithm for nonconvex problems to deal with these primary challenges. The proposed search direction is based on approximating the original Hessian matrix by a positive definite matrix. The novelty of the proposed method is that the proposed search direction is guaranteed to be descent and requires approximation of Hessian and its inversion only at specific iterations. We prove that the difference between the calculated approximate and exact Hessian is bounded within an interval. Accordingly, the approximate Hessian matrix is reused if the iterates are in that chosen interval while computing the gradients at each iteration. We develop various algorithm variants based on the interval size updating methods and minimum eigenvalue computation methods. We also prove the global convergence of the proposed algorithm. Further, we apply the algorithm to an extensive set of test problems and compare its performance with the existing methods such as steepest descent, quasi-Newton, and Newton method. We show empirically that the proposed method solves more problems in fewer function and gradient evaluations than steepest descent and the quasi-Newton method. While in the comparison to the Newton method, we illustrate that for nonconvex optimization problems, we require substantially less $O(n^3)$ operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22342v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishan Kumar, Ashutosh Sharma, Gauransh Dingwani, Nikhil Gupta, Vaishnavi Gupta, Ishan Bajaj</dc:creator>
    </item>
    <item>
      <title>A Proximal-Gradient Method for Solving Regularized Optimization Problems with General Constraints</title>
      <link>https://arxiv.org/abs/2512.23166</link>
      <description>arXiv:2512.23166v2 Announce Type: replace 
Abstract: We propose, analyze, and test a proximal-gradient method for solving regularized optimization problems with general constraints. The method employs a decomposition strategy to compute trial steps and uses a merit function to determine step acceptance or rejection. Under various assumptions, we establish a worst-case iteration complexity result, prove that limit points are first-order KKT points, and show that manifold identification and active-set identification properties hold. Preliminary numerical experiments on a subset of the CUTEst test problems and sparse canonical correlation analysis problems demonstrate the promising performance of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23166v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank E. Curtis, Xiaoyi Qu, Daniel P. Robinson</dc:creator>
    </item>
    <item>
      <title>Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs</title>
      <link>https://arxiv.org/abs/2601.05943</link>
      <description>arXiv:2601.05943v2 Announce Type: replace 
Abstract: Recent progress in LLM-driven algorithm discovery, exemplified by DeepMind's AlphaEvolve, has produced new best-known solutions for a range of hard geometric and combinatorial problems. This raises a natural question: to what extent can modern off-the-shelf global optimization solvers match such results when the problems are formulated directly as nonlinear optimization problems (NLPs)?
  We revisit a subset of problems from the AlphaEvolve benchmark suite and evaluate straightforward NLP formulations with two state-of-the-art solvers, the commercial FICO Xpress and the open-source SCIP. Without any solver modifications, both solvers reproduce, and in several cases improve upon, the best solutions previously reported in the literature, including the recent LLM-driven discoveries. Our results not only highlight the maturity of generic NLP technology and its ability to tackle nonlinear mathematical problems that were out of reach for general-purpose solvers only a decade ago, but also position global NLP solvers as powerful tools that may be exploited within LLM-driven algorithm discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05943v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Timo Berthold, Dominik Kamp, Gioni Mexi, Sebastian Pokutta, Imre P\'olik</dc:creator>
    </item>
    <item>
      <title>Genetic Algorithm Based Combinatorial Optimization for the Optimal Design of Water Distribution Network of Gurudeniya Service Zone, Sri Lanka</title>
      <link>https://arxiv.org/abs/2304.09720</link>
      <description>arXiv:2304.09720v4 Announce Type: replace-cross 
Abstract: This paper brings an in detail Genetic Algorithm (GA) based combinatorial optimization method used for the optimal design of the water distribution network (WDN) of Gurudeniya Service Zone, Sri Lanka. Genetic Algorithm (GA) mimics the survival of the fittest principle of nature to develop a search process. Methodology employs fuzzy combinations of pipe diameters to check their suitability to be considered as the cost effective optimal design solutions. Furthermore, the hydraulic constraints were implicitly evaluated within the GA itself in its aim to reaching the global optimum solution. Upon analysis, the results of this approach delivered agreeable design outputs. In addition, the comparison made between the results obtained by a previous study inspired by the Honey Bee Mating Optimization (HBMO) Algorithm and results obtained by the GA based approach, proves competency of GA for the optimal design of water distribution network in Gurudeniya Service Zone, Sri Lanka.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09720v4</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. H. M. R. N. Senavirathna, C. K. Walgampaya</dc:creator>
    </item>
    <item>
      <title>Cross-Dimensional Mathematics: A Foundation For STP/STA</title>
      <link>https://arxiv.org/abs/2406.12920</link>
      <description>arXiv:2406.12920v5 Announce Type: replace-cross 
Abstract: A new mathematical structure, called the cross-dimensional mathematics (CDM), is proposed. The CDM considered in this paper consists of three parts: hyper algebra, hyper geometry, and hyper Lie group/Lie algebra. Hyper algebra proposes some new algebraic structures such as hyper group, hyper ring, and hyper module over matrices and vectors with mixed dimensions (MVMDs). They have sets of classical groups, rings, and modules as their components and cross-dimensional connections among their components. Their basic properties are investigated. Hyper geometry starts from mixed dimensional Euclidian space, and hyper vector space. Then the hyper topological vector space, hyper inner product space, and hyper manifold are constructed. They have a joined cross-dimensional geometric structure. Finally, hyper metric space, topological hyper group and hyper Lie algebra are built gradually, and finally, the corresponding hyper Lie group is introduced. All these concepts are built over MVMDs, and to reach our purpose in addition to existing semi-tensor products (STPs) and semi-tensor additions (STAs), a couple of most general STP and STA are introduced. Some existing structures/results about STPs/STAs have also been resumed and integrated into this CDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12920v5</guid>
      <category>math.RA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11425-024-2528-4</arxiv:DOI>
      <dc:creator>Daizhan Cheng</dc:creator>
    </item>
    <item>
      <title>Adaptive Economic Model Predictive Control: Performance Guarantees for Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2412.13046</link>
      <description>arXiv:2412.13046v3 Announce Type: replace-cross 
Abstract: We consider the problem of optimizing the economic performance of nonlinear constrained systems subject to uncertain time-varying parameters and bounded disturbances. In particular, we propose an adaptive economic model predictive control (MPC) framework that: (i) directly minimizes transient economic costs, (ii) addresses parametric uncertainty through online model adaptation, (iii) determines optimal setpoints online, and (iv) ensures robustness by using a tube-based approach. The proposed design ensures recursive feasibility, robust constraint satisfaction, and a transient performance bound. In case the disturbances have a finite energy and the parameter variations have a finite path length, the asymptotic average performance is (approximately) not worse than the performance obtained when operating at the best reachable steady-state. We highlight performance benefits in a numerical example involving a chemical reactor with unknown time-invariant and time-varying parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13046v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Degner, Raffaele Soloperto, Melanie N. Zeilinger, John Lygeros, Johannes K\"ohler</dc:creator>
    </item>
    <item>
      <title>Online Scheduling for LLM Inference with KV Cache Constraints</title>
      <link>https://arxiv.org/abs/2502.07115</link>
      <description>arXiv:2502.07115v5 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose a novel batching and scheduling algorithm that minimizes inference latency while effectively managing the KV cache's memory.
  More specifically, we make the following contributions. First, to evaluate the performance of online algorithms for scheduling in LLM inference, we introduce a hindsight optimal benchmark, formulated as an integer program that computes the minimum total inference latency under full future information. Second, we prove that no deterministic online algorithm can achieve a constant competitive ratio when the arrival process is arbitrary. Third, motivated by the computational intractability of solving the integer program at scale, we propose a polynomial-time online scheduling algorithm and show that under certain conditions it can achieve a constant competitive ratio. We also demonstrate our algorithm's strong empirical performance by comparing it to the hindsight optimal in a synthetic dataset. Finally, we conduct empirical evaluations on a real-world public LLM inference dataset, simulating the Llama2-70B model on A100 GPUs, and show that our algorithm significantly outperforms the benchmark algorithms. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07115v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Jaillet, Jiashuo Jiang, Konstantina Mellou, Marco Molinaro, Chara Podimata, Zijie Zhou</dc:creator>
    </item>
    <item>
      <title>Deep Learning for Continuous-Time Stochastic Control with Jumps</title>
      <link>https://arxiv.org/abs/2505.15602</link>
      <description>arXiv:2505.15602v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce a model-based deep-learning approach to solve finite-horizon continuous-time stochastic control problems with jumps. We iteratively train two neural networks: one to represent the optimal policy and the other to approximate the value function. Leveraging a continuous-time version of the dynamic programming principle, we derive two different training objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the networks capture the underlying stochastic dynamics. Empirical evaluations on different problems illustrate the accuracy and scalability of our approach, demonstrating its effectiveness in solving complex high-dimensional stochastic control tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15602v3</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Cheridito, Jean-Loup Dupret, Donatien Hainaut</dc:creator>
    </item>
    <item>
      <title>Canonical Frames for Bracket Generating Rank 2 Distributions which are not Goursat</title>
      <link>https://arxiv.org/abs/2508.09307</link>
      <description>arXiv:2508.09307v5 Announce Type: replace-cross 
Abstract: We complete a uniform construction of canonical absolute parallelism for bracket generating rank $2$ distributions with $5$-dimensional cube on $n$-dimensional manifold with $n\geq 5$ by showing that the condition of maximality of class that was assumed previously by Doubrov-Zelenko for such a construction holds automatically at generic points. This also gives analogous constructions in the case when the cube is not $5$-dimensional but the distribution is not Goursat through the procedure of iterative Cartan deprolongation. This together with the classical theory of Goursat distributions covers in principle the local geometry of all bracket generating rank 2 distributions in a neighborhood of generic points. As a byproduct, for any $n\geq 5$ we describe the maximally symmetric germs among bracket generating rank $2$ distributions with $5$-dimensional cube, as well as among those which reduce to such a distribution under a fixed number of Cartan deprolongations. Another consequence of our results on maximality of class is for optimal control problems with constraint given by a rank $2$ distribution with $5$-dimensional cube: it implies that for a generic point $q_0$ of $M$, there are plenty abnormal extremal trajectories of corank $1$ (which is the minimal possible corank) starting at $q_0$. The set of such points contains all points where the distribution is equiregular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09307v5</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicklas Day, Igor Zelenko</dc:creator>
    </item>
    <item>
      <title>Polynomial Stability of Non-Linearly Damped Contraction Semigroups</title>
      <link>https://arxiv.org/abs/2509.04275</link>
      <description>arXiv:2509.04275v2 Announce Type: replace-cross 
Abstract: We investigate the stability properties of an abstract class of semi-linear systems. Our main result establishes rational rates of decay for classical solutions assuming a certain non-uniform observability estimate for the linear part and suitable conditions on the non-linearity. We illustrate the strength of our abstract results by applying them to a one-dimensional wave equation with weak non-linear damping and to an Euler-Bernoulli beam with a tip mass subject to non-linear damping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04275v2</guid>
      <category>math.FA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lassi Paunonen, David Seifert</dc:creator>
    </item>
    <item>
      <title>Data selection: at the interface of PDE-based inverse problem and randomized linear algebra</title>
      <link>https://arxiv.org/abs/2510.01567</link>
      <description>arXiv:2510.01567v2 Announce Type: replace-cross 
Abstract: All inverse problems rely on data to recover unknown parameters, yet not all data are equally informative. This raises the central question of data selection. A distinctive challenge in PDE-based inverse problems is their inherently infinite-dimensional nature: both the parameter space and the design space are infinite, which greatly complicates the selection process. Somewhat unexpectedly, randomized numerical linear algebra (RNLA), originally developed in very different contexts, has provided powerful tools for addressing this challenge. These methods are inherently probabilistic, with guarantees typically stating that information is preserved with probability at least 1-p when using N randomly selected, weighted samples. Here, the notion of "information" can take different mathematical forms depending on the setting. In this review, we survey the problem of data selection in PDE-based inverse problems, emphasize its unique infinite-dimensional aspects, and highlight how RNLA strategies have been adapted and applied in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01567v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kathrin Hellmuth, Ruhui Jin, Qin Li, Stephen J. Wright</dc:creator>
    </item>
    <item>
      <title>Learning Regularization Functionals for Inverse Problems: A Comparative Study</title>
      <link>https://arxiv.org/abs/2510.01755</link>
      <description>arXiv:2510.01755v2 Announce Type: replace-cross 
Abstract: In recent years, a variety of learned regularization frameworks for solving inverse problems in imaging have emerged. These offer flexible modeling together with mathematical insights. The proposed methods differ in their architectural design and training strategies, making direct comparison challenging due to non-modular implementations. We address this gap by collecting and unifying the available code into a common framework. This unified view allows us to systematically compare the approaches and highlight their strengths and limitations, providing valuable insights into their future potential. We also provide concise descriptions of each method, complemented by practical guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01755v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Hertrich, Hok Shing Wong, Alexander Denker, Stanislas Ducotterd, Zhenghan Fang, Markus Haltmeier, \v{Z}eljko Kereta, Erich Kobler, Oscar Leong, Mohammad Sadegh Salehi, Carola-Bibiane Sch\"onlieb, Johannes Schwab, Zakhar Shumaylov, Jeremias Sulam, German Sh\^ama Wache, Martin Zach, Yasi Zhang, Matthias J. Ehrhardt, Sebastian Neumayer</dc:creator>
    </item>
    <item>
      <title>Constrained dynamics for searching saddle points on general Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2601.03931</link>
      <description>arXiv:2601.03931v2 Announce Type: replace-cross 
Abstract: Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03931v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukuan Hu, Laura Grazioli</dc:creator>
    </item>
  </channel>
</rss>
