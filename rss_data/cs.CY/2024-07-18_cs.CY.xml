<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 01:57:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exploratory Data Analysis for Banking and Finance: Unveiling Insights and Patterns</title>
      <link>https://arxiv.org/abs/2407.11976</link>
      <description>arXiv:2407.11976v1 Announce Type: new 
Abstract: This paper explores the application of Exploratory Data Analytics (EDA) in the banking and finance domain, focusing on credit card usage and customer churning. It presents a step-by-step analysis using EDA techniques such as descriptive statistics, data visualization, and correlation analysis. The study examines transaction patterns, credit limits, and usage across merchant categories, providing insights into consumer behavior. It also considers demographic factors like age, gender, and income on usage patterns. Additionally, the report addresses customer churning, analyzing churn rates and factors such as demographics, transaction history, and satisfaction levels. These insights help banking professionals make data-driven decisions, improve marketing strategies, and enhance customer retention, ultimately contributing to profitability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11976v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankur Agarwal, Shashi Prabha, Raghav Yadav</dc:creator>
    </item>
    <item>
      <title>An investigation into the scientific landscape of the conversational and generative artificial intelligence, and human-chatbot interaction in education and research</title>
      <link>https://arxiv.org/abs/2407.12004</link>
      <description>arXiv:2407.12004v1 Announce Type: new 
Abstract: Artificial intelligence (AI) as a disruptive technology is not new. However, its recent evolution, engineered by technological transformation, big data analytics, and quantum computing, produces conversational and generative AI (CGAI/GenAI) and human-like chatbots that disrupt conventional operations and methods in different fields. This study investigates the scientific landscape of CGAI and human-chatbot interaction/collaboration and evaluates use cases, benefits, challenges, and policy implications for multidisciplinary education and allied industry operations. The publications trend showed that just 4% (n=75) occurred during 2006-2018, while 2019-2023 experienced astronomical growth (n=1763 or 96%). The prominent use cases of CGAI (e.g., ChatGPT) for teaching, learning, and research activities occurred in computer science [multidisciplinary and AI] (32%), medical/healthcare (17%), engineering (7%), and business fields (6%). The intellectual structure shows strong collaboration among eminent multidisciplinary sources in business, Information Systems, and other areas. The thematic structure of SLP highlights prominent CGAI use cases, including improved user experience in human-computer interaction, computer programs/code generation, and systems creation. Widespread CGAI usefulness for teachers, researchers, and learners includes syllabi/course content generation, testing aids, and academic writing. The concerns about abuse and misuse (plagiarism, academic integrity, privacy violations) and issues about misinformation, danger of self-diagnoses, and patient privacy in medical/healthcare applications are prominent. Formulating strategies and policies to address potential CGAI challenges in teaching/learning and practice are priorities. Developing discipline-based automatic detection of GenAI contents to check abuse is proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12004v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ikpe Justice Akpan, Yawo M. Kobara, Josiah Owolabi, Asuama Akpam, Onyebuchi Felix Offodile</dc:creator>
    </item>
    <item>
      <title>Towards a Harms Taxonomy of AI Likeness Generation</title>
      <link>https://arxiv.org/abs/2407.12030</link>
      <description>arXiv:2407.12030v1 Announce Type: new 
Abstract: Generative artificial intelligence models, when trained on a sufficient number of a person's images, can replicate their identifying features in a photorealistic manner. We refer to this process as 'likeness generation'. Likeness-featuring synthetic outputs often present a person's likeness without their control or consent, and may lead to harmful consequences. This paper explores philosophical and policy issues surrounding generated likeness. It begins by offering a conceptual framework for understanding likeness generation by examining the novel capabilities introduced by generative systems. The paper then establishes a definition of likeness by tracing its historical development in legal literature. Building on this foundation, we present a taxonomy of harms associated with generated likeness, derived from a comprehensive meta-analysis of relevant literature. This taxonomy categorises harms into seven distinct groups, unified by shared characteristics. Utilising this taxonomy, we raise various considerations that need to be addressed for the deployment of appropriate mitigations. Given the multitude of stakeholders involved in both the creation and distribution of likeness, we introduce concepts such as indexical sufficiency, a distinction between generation and distribution, and harms as having a context-specific nature. This work aims to serve industry, policymakers, and future academic researchers in their efforts to address the societal challenges posed by likeness generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12030v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Bariach, Bernie Hogan, Keegan McBride</dc:creator>
    </item>
    <item>
      <title>Evaluation of Bias Towards Medical Professionals in Large Language Models</title>
      <link>https://arxiv.org/abs/2407.12031</link>
      <description>arXiv:2407.12031v1 Announce Type: new 
Abstract: This study evaluates whether large language models (LLMs) exhibit biases towards medical professionals. Fictitious candidate resumes were created to control for identity factors while maintaining consistent qualifications. Three LLMs (GPT-4, Claude-3-haiku, and Mistral-Large) were tested using a standardized prompt to evaluate resumes for specific residency programs. Explicit bias was tested by changing gender and race information, while implicit bias was tested by changing names while hiding race and gender. Physician data from the Association of American Medical Colleges was used to compare with real-world demographics. 900,000 resumes were evaluated. All LLMs exhibited significant gender and racial biases across medical specialties. Gender preferences varied, favoring male candidates in surgery and orthopedics, while preferring females in dermatology, family medicine, obstetrics and gynecology, pediatrics, and psychiatry. Claude-3 and Mistral-Large generally favored Asian candidates, while GPT-4 preferred Black and Hispanic candidates in several specialties. Tests revealed strong preferences towards Hispanic females and Asian males in various specialties. Compared to real-world data, LLMs consistently chose higher proportions of female and underrepresented racial candidates than their actual representation in the medical workforce. GPT-4, Claude-3, and Mistral-Large showed significant gender and racial biases when evaluating medical professionals for residency selection. These findings highlight the potential for LLMs to perpetuate biases and compromise healthcare workforce diversity if used without proper bias mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12031v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Chen, Yang Xu, MingKe You, Li Wang, WeiZhi Liu, Jian Li</dc:creator>
    </item>
    <item>
      <title>Improving Health Information Access in the World's Largest Maternal Mobile Health Program via Bandit Algorithms</title>
      <link>https://arxiv.org/abs/2407.12131</link>
      <description>arXiv:2407.12131v1 Announce Type: new 
Abstract: Harnessing the wide-spread availability of cell phones, many nonprofits have launched mobile health (mHealth) programs to deliver information via voice or text to beneficiaries in underserved communities, with maternal and infant health being a key area of such mHealth programs. Unfortunately, dwindling listenership is a major challenge, requiring targeted interventions using limited resources. This paper focuses on Kilkari, the world's largest mHealth program for maternal and child care - with over 3 million active subscribers at a time - launched by India's Ministry of Health and Family Welfare (MoHFW) and run by the non-profit ARRMAN. We present a system called CHAHAK that aims to reduce automated dropouts as well as boost engagement with the program through the strategic allocation of interventions to beneficiaries. Past work in a similar domain has focused on a much smaller scale mHealth program and used markovian restless multiarmed bandits to optimize a single limited intervention resource. However this paper demonstrates the challenges in adopting a markovian approach in Kilkari; therefore CHAHAK instead relies on non-markovian time-series restless bandits, and optimizes multiple interventions to improve listenership. We use real Kilkari data from the Odisha state in India to show CHAHAK's effectiveness in harnessing multiple interventions to boost listenership, benefiting marginalized communities. When deployed CHAHAK will assist the largest maternal mHealth program to date.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12131v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arshika Lalan, Shresth Verma, Paula Rodriguez Diaz, Panayiotis Danassis, Amrita Mahale, Kumar Madhu Sudan, Aparna Hegde, Milind Tambe, Aparna Taneja</dc:creator>
    </item>
    <item>
      <title>Gaming and Blockchain: Hype and Reality</title>
      <link>https://arxiv.org/abs/2407.12134</link>
      <description>arXiv:2407.12134v1 Announce Type: new 
Abstract: This paper explores the adoption of blockchain technology in the gaming industry. While supporters affirm that distributed ledger technology has potential to revolutionize gaming economies and provide players with control over their virtual assets, there are practical challenges such as energy consumption and user adoption to be addressed, and detractors question whether blockchain integration is even necessary. This report characterises popular blockchain-based gaming projects like Enjin and Axie Infinity, then compares metrics such as transaction cost and player feedback to evaluate the longevity of blockchain-integrated gaming as a whole.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12134v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max McGuinness</dc:creator>
    </item>
    <item>
      <title>Trustworthy AI in practice: an analysis of practitioners' needs and challenges</title>
      <link>https://arxiv.org/abs/2407.12135</link>
      <description>arXiv:2407.12135v1 Announce Type: new 
Abstract: Recently, there has been growing attention on behalf of both academic and practice communities towards the ability of Artificial Intelligence (AI) systems to operate responsibly and ethically. As a result, a plethora of frameworks and guidelines have appeared to support practitioners in implementing Trustworthy AI applications (TAI). However, little research has been done to investigate whether such frameworks are being used and how. In this work, we study the vision AI practitioners have on TAI principles, how they address them, and what they would like to have - in terms of tools, knowledge, or guidelines - when they attempt to incorporate such principles into the systems they develop. Through a survey and semi-structured interviews, we systematically investigated practitioners' challenges and needs in developing TAI systems. Based on these practical findings, we highlight recommendations to help AI practitioners develop Trustworthy AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12135v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3661167.3661214</arxiv:DOI>
      <arxiv:journal_reference>28th International Conference on Evaluation and Assessment in Software Engineering (EASE 2024), June 18-21, 2024, Salerno, Italy</arxiv:journal_reference>
      <dc:creator>Maria Teresa Baldassarre, Domenico Gigante, Marcos Kalinowski, Azzurra Ragone, Sara Tibid\`o</dc:creator>
    </item>
    <item>
      <title>Combining data from multiple sources for urban travel mode choice modelling</title>
      <link>https://arxiv.org/abs/2407.12137</link>
      <description>arXiv:2407.12137v1 Announce Type: new 
Abstract: Demand for sustainable mobility is particularly high in urban areas. Hence, there is a growing need to predict when people will decide to use different travel modes with an emphasis on environmentally friendly travel modes. As travel mode choice (TMC) is influenced by multiple factors, in a growing number of cases machine learning methods are used to predict travel mode choices given respondent and journey features. Typically, travel diaries are used to provide core relevant data. However, other features such as attributes of mode alternatives including, but not limited to travel times, and, in the case of public transport (PT), also walking distances have a major impact on whether a person decides to use a travel mode of interest.
  Hence, in this work, we propose an architecture of a software platform performing the data fusion combining data documenting journeys with the features calculated to summarise transport options available for these journeys, built environment and environmental factors such as weather conditions possibly influencing travel mode decisions. Furthermore, we propose various novel features, many of which we show to be among the most important for TMC prediction. We propose how stream processing engines and other Big Data systems can be used for their calculation.
  The data processed by the platform is used to develop machine learning models predicting travel mode choices. To validate the platform, we propose ablation studies investigating the importance of individual feature subsets calculated by it and their impact on the TMC models built with them. In our experiments, we combine survey data, GPS traces, weather and pollution time series, transport model data, and spatial data of the built environment. The growth in the accuracy of TMC models built with the additional features is up to 18.2% compared to the use of core survey data only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12137v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maciej Grzenda, Marcin Luckner, Jakub Zawieska, Przemys{\l}aw Wrona</dc:creator>
    </item>
    <item>
      <title>Propensity towards Ownership and Use of Automated Vehicles: Who Are the Adopters? Who Are the Non-adopters? Who Is Hesitant?</title>
      <link>https://arxiv.org/abs/2407.12139</link>
      <description>arXiv:2407.12139v1 Announce Type: new 
Abstract: The objective of this study is to investigate automated vehicle (AV) adoption perceptions, including ownership intentions and the willingness to use self-driving mobility services. In this paper, we use data from the 2018 California Transportation Survey, and use K-means, a clustering technique in data mining, to reveal patterns of potential AV owners (and non-owners) as well as AV users (and non-users) of self-driving services. The results reveal seven clusters, namely Multitaskers/ environmentalists/ impaired drivers, Tech mavens/ travelers, Life in transition, Captive car-users, Public/ active transport users, Sub-urban Dwellers, and Car enthusiasts. The first two clusters include adopters who are largely familiar with AVs, are tech savvy, and who make good use of time during their commute. The last cluster comprise of non-adopters who are car enthusiasts. On the other hand, people who are Life in transition, Captive car-users, Public/ active transport users, and Sub-urban dwellers show uncertain perceptions towards being AV adopters. They are either pursuing higher education, having a busy schedule, supporting for sustainable society via government policies, or have a stable life, respectively. Insights from this study help practitioners to build business models and strategic planning, addressing potential market segments of individuals that are willing to own an AV vs. those that are more inclined to use self-driving mobility services. The "gray" segments identify a latent untapped demand and a potential target for marketing, campaigns, and sales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12139v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tho Le, Giovanni Circella</dc:creator>
    </item>
    <item>
      <title>False consensus biases AI against vulnerable stakeholders</title>
      <link>https://arxiv.org/abs/2407.12143</link>
      <description>arXiv:2407.12143v1 Announce Type: new 
Abstract: The deployment of AI systems for welfare benefit allocation allows for accelerated decision-making and faster provision of critical help, but has already led to an increase in unfair benefit denials and false fraud accusations. Collecting data in the US and the UK (N = 2449), we explore the public acceptability of such speed-accuracy trade-offs in populations of claimants and non-claimants. We observe a general willingness to trade off speed gains for modest accuracy losses, but this aggregate view masks notable divergences between claimants and non-claimants. Although welfare claimants comprise a relatively small proportion of the general population (e.g., 20% in the US representative sample), this vulnerable group is much less willing to accept AI deployed in welfare systems, raising concerns that solely using aggregate data for calibration could lead to policies misaligned with stakeholder preferences. Our study further uncovers asymmetric insights between claimants and non-claimants. The latter consistently overestimate claimant willingness to accept speed-accuracy trade-offs, even when financially incentivized for accurate perspective-taking. This suggests that policy decisions influenced by the dominant voice of non-claimants, however well-intentioned, may neglect the actual preferences of those directly affected by welfare AI systems. Our findings underline the need for stakeholder engagement and transparent communication in the design and deployment of these systems, particularly in contexts marked by power imbalances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12143v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchen Dong, Jean-Fran\c{c}ois Bonnefon, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Adoption and Impact of ChatGPT in Computer Science Education: A Case Study on a Database Administration Course</title>
      <link>https://arxiv.org/abs/2407.12145</link>
      <description>arXiv:2407.12145v1 Announce Type: new 
Abstract: Contribution: The combination of ChatGPT with traditional learning resources is very effective in computer science education. High-performing students are the ones who are using ChatGPT the most. So, a new digital trench could be rising between these students and those with lower degree of fundamentals and worse prompting skills, who may not take advantage of all the ChatGPT possibilities. Background: The irruption of GenAI such as ChatGPT has changed the educational landscape. Therefore, methodological guidelines and more empirical experiences in computer science education are needed to better understand these tools and know how to use them to their fullest potential. Research Questions: This article addresses three questions. The first two explore the degree of use and perceived usefulness of ChatGPT among computer science students to learn database administration, where as the third one explore how the utilization of ChatGPT can impact academic performance. Methodology: This contribution presents an exploratory and correlational study conducted with 37 students who used ChatGPT as a support tool to learn database administration. The student grades and a comprehensive questionnaire were employed as research instruments. Findings: The obtained results indicate that traditional learning resources, such as teacher explanations and student reports, were widely used and correlated positively with student grade. The usage and perceived utility of ChatGPT were moderate, but positive correlations between student grade and ChatGPT usage were found. Indeed, a significantly higher use of this tool was identified among the group of outstanding students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12145v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel L\'opez-Fern\'andez, Ricardo Vergaz</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Student Performance Predictions in Online Courses using Heterogeneous Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2407.12153</link>
      <description>arXiv:2407.12153v1 Announce Type: new 
Abstract: As online courses become the norm in the higher-education landscape, investigations into student performance between students who take online vs on-campus versions of classes become necessary. While attention has been given to looking at differences in learning outcomes through comparisons of students' end performance, less attention has been given in comparing students' engagement patterns between different modalities. In this study, we analyze a heterogeneous knowledge graph consisting of students, course videos, formative assessments and their interactions to predict student performance via a Graph Convolutional Network (GCN). Using students' performance on the assessments, we attempt to determine a useful model for identifying at-risk students. We then compare the models generated between 5 on-campus and 2 fully-online MOOC-style instances of the same course. The model developed achieved a 70-90\% accuracy of predicting whether a student would pass a particular problem set based on content consumed, course instance, and modality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12153v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Trask, Dr. Nicholas Lytle, Michael Boyle, Dr. David Joyner, Dr. Ahmed Mubarak</dc:creator>
    </item>
    <item>
      <title>Cyberbullying Detection: Exploring Datasets, Technologies, and Approaches on Social Media Platforms</title>
      <link>https://arxiv.org/abs/2407.12154</link>
      <description>arXiv:2407.12154v1 Announce Type: new 
Abstract: Cyberbullying has been a significant challenge in the digital era world, given the huge number of people, especially adolescents, who use social media platforms to communicate and share information. Some individuals exploit these platforms to embarrass others through direct messages, electronic mail, speech, and public posts. This behavior has direct psychological and physical impacts on victims of bullying. While several studies have been conducted in this field and various solutions proposed to detect, prevent, and monitor cyberbullying instances on social media platforms, the problem continues. Therefore, it is necessary to conduct intensive studies and provide effective solutions to address the situation. These solutions should be based on detection, prevention, and prediction criteria methods. This paper presents a comprehensive systematic review of studies conducted on cyberbullying detection. It explores existing studies, proposed solutions, identified gaps, datasets, technologies, approaches, challenges, and recommendations, and then proposes effective solutions to address research gaps in future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12154v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adamu Gaston Philipo, Doreen Sebastian Sarwatt, Jianguo Ding, Mahmoud Daneshmand, Huansheng Ning</dc:creator>
    </item>
    <item>
      <title>GPT-4V Cannot Generate Radiology Reports Yet</title>
      <link>https://arxiv.org/abs/2407.12176</link>
      <description>arXiv:2407.12176v1 Announce Type: new 
Abstract: GPT-4V's purported strong multimodal abilities raise interests in using it to automate radiology report writing, but there lacks thorough evaluations. In this work, we perform a systematic evaluation of GPT-4V in generating radiology reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt to directly generate reports using GPT-4V through different prompting strategies and find that it fails terribly in both lexical metrics and clinical efficacy metrics. To understand the low performance, we decompose the task into two steps: 1) the medical image reasoning step of predicting medical condition labels from images; and 2) the report synthesis step of generating reports from (groundtruth) conditions. We show that GPT-4V's performance in image reasoning is consistently low across different prompts. In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully. Even when given groundtruth conditions in report synthesis, its generated reports are less correct and less natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt on the viability of using GPT-4V in a radiology workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12176v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>Are Educational Escape Rooms More Effective Than Traditional Lectures for Teaching Software Engineering? A Randomized Controlled Trial</title>
      <link>https://arxiv.org/abs/2407.12355</link>
      <description>arXiv:2407.12355v1 Announce Type: new 
Abstract: Contribution: This article analyzes the learning effectiveness of a virtual educational escape room for teaching software engineering and compares this activity with traditional teaching through a randomized controlled trial. Background: Educational escape rooms have been used across a wide variety of disciplines at all levels of education and they are becoming increasingly popular among teachers. Nevertheless, there is a clear general need for more robust empirical evidence on the learning effectiveness of these novel activities and, particularly, on their application in software engineering education. Research Questions: Is game-based learning using educational escape rooms more effective than traditional lectures for teaching software engineering? What are the perceptions of software engineering students toward game-based learning using educational escape rooms? Methodology: The study presented in this article is a randomized controlled trial with a pre-and post-test design that was completed by a total of 326 software engineering students. The 164 students belonging to the experimental group learned software modeling by playing an educational escape room whereas the 162 students belonging to the control group learned the same subject matter through a traditional lecture. Findings: The results of the randomized controlled trial show that the students who learned software modeling through the educational escape room had very positive perceptions toward this activity, significantly increased their knowledge, and outperformed those students who learned through a traditional lecture in terms of knowledge acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12355v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TE.2024.3403913</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Education, 2024</arxiv:journal_reference>
      <dc:creator>Aldo Gordillo, Daniel L\'opez-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Across Platforms and Languages: Dutch Influencers and Legal Disclosures on Instagram, YouTube and TikTok</title>
      <link>https://arxiv.org/abs/2407.12451</link>
      <description>arXiv:2407.12451v1 Announce Type: new 
Abstract: Content monetization on social media fuels a growing influencer economy. Influencer marketing remains largely undisclosed or inappropriately disclosed on social media. Non-disclosure issues have become a priority for national and supranational authorities worldwide, who are starting to impose increasingly harsher sanctions on them. This paper proposes a transparent methodology for measuring whether and how influencers comply with disclosures based on legal standards. We introduce a novel distinction between disclosures that are legally sufficient (green) and legally insufficient (yellow). We apply this methodology to an original dataset reflecting the content of 150 Dutch influencers publicly registered with the Dutch Media Authority based on recently introduced registration obligations. The dataset consists of 292,315 posts and is multi-language (English and Dutch) and cross-platform (Instagram, YouTube and TikTok). We find that influencer marketing remains generally underdisclosed on social media, and that bigger influencers are not necessarily more compliant with disclosure standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12451v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Gui, Thales Bertaglia, Catalina Goanta, Sybe de Vries, Gerasimos Spanakis</dc:creator>
    </item>
    <item>
      <title>Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter</title>
      <link>https://arxiv.org/abs/2407.12471</link>
      <description>arXiv:2407.12471v1 Announce Type: new 
Abstract: Understanding the dynamics of language toxicity on social media is important for us to investigate the propagation of misinformation and the development of echo chambers for political scenarios such as U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics across social media platforms. However, research on the toxicity dynamics is not enough. This study aims to provide a first exploration of the potential language toxicity flow among Left, Right and Center users. Specifically, we aim to examine whether Left users were easier to be attacked by language toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that Left users received much more toxic replies than Right and Center users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12471v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wentao Xu</dc:creator>
    </item>
    <item>
      <title>What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from the Perspective of Approximate Justice</title>
      <link>https://arxiv.org/abs/2407.12488</link>
      <description>arXiv:2407.12488v1 Announce Type: new 
Abstract: In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12488v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Hertweck, Christoph Heitz, Michele Loi</dc:creator>
    </item>
    <item>
      <title>Conspiracy theories and where to find them on TikTok</title>
      <link>https://arxiv.org/abs/2407.12545</link>
      <description>arXiv:2407.12545v1 Announce Type: new 
Abstract: TikTok has skyrocketed in popularity over recent years, especially among younger audiences, thanks to its viral trends and social challenges. However, concerns have been raised about the potential of this platform to promote and amplify online harmful and dangerous content. Leveraging the official TikTok Research API and collecting a longitudinal dataset of 1.5M videos shared in the US over a period of 3 years, our study analyzes the presence of videos promoting conspiracy theories, providing a lower-bound estimate of their prevalence (approximately 0.1% of all videos) and assessing the effects of the new Creator Program, which provides new ways for creators to monetize, on the supply of conspiratorial content. We evaluate the capabilities of state-of-the-art open Large Language Models to identify conspiracy theories after extracting audio transcriptions of videos, finding that they can detect harmful content with high precision but with overall performance comparable to fine-tuned traditional language models such as RoBERTa. Our findings are instrumental for content moderation strategies that aim to understand and mitigate the spread of harmful content on rapidly evolving social media platforms like TikTok.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12545v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Corso, Francesco Pierri, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes</title>
      <link>https://arxiv.org/abs/2407.12680</link>
      <description>arXiv:2407.12680v1 Announce Type: new 
Abstract: Biased information (recently termed bisinformation) continues to be taught in medical curricula, often long after having been debunked. In this paper, we introduce BRICC, a firstin-class initiative that seeks to mitigate medical bisinformation using machine learning to systematically identify and flag text with potential biases, for subsequent review in an expert-in-the-loop fashion, thus greatly accelerating an otherwise labor-intensive process. A gold-standard BRICC dataset was developed throughout several years, and contains over 12K pages of instructional materials. Medical experts meticulously annotated these documents for bias according to comprehensive coding guidelines, emphasizing gender, sex, age, geography, ethnicity, and race. Using this labeled dataset, we trained, validated, and tested medical bias classifiers. We test three classifier approaches: a binary type-specific classifier, a general bias classifier; an ensemble combining bias type-specific classifiers independently-trained; and a multitask learning (MTL) model tasked with predicting both general and type-specific biases. While MTL led to some improvement on race bias detection in terms of F1-score, it did not outperform binary classifiers trained specifically on each task. On general bias detection, the binary classifier achieves up to 0.923 of AUC, a 27.8% improvement over the baseline. This work lays the foundations for debiasing medical curricula by exploring a novel dataset and evaluating different training model strategies. Hence, it offers new pathways for more nuanced and effective mitigation of bisinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12680v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiman Salavati, Shannon Song, Willmar Sosa Diaz, Scott A. Hale, Roberto E. Montenegro, Fabricio Murai, Shiri Dori-Hacohen</dc:creator>
    </item>
    <item>
      <title>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach</title>
      <link>https://arxiv.org/abs/2407.12687</link>
      <description>arXiv:2407.12687v1 Announce Type: new 
Abstract: A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12687v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irina Jurenka, Markus Kunesch, Kevin R. McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger, Shubham Milind Phal, Katherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, Ankit Anand, Miruna P\^islar, Stephanie Chan, Lisa Wang, Jennifer She, Parsa Mahmoudieh, Aliya Rysbek, Wei-Jen Ko, Andrea Huber, Brett Wiltshire, Gal Elidan, Roni Rabin, Jasmin Rubinovitz, Amit Pitaru, Mac McAllister, Julia Wilkowski, David Choi, Roee Engelberg, Lidan Hackmon, Adva Levin, Rachel Griffin, Michael Sears, Filip Bar, Mia Mesar, Mana Jabbour, Arslan Chaudhry, James Cohan, Sridhar Thiagarajan, Nir Levine, Ben Brown, Dilan Gorur, Svetlana Grant, Rachel Hashimoshoni, Laura Weidinger, Jieru Hu, Dawn Chen, Kuba Dolecki, Canfer Akbulut, Maxwell Bileschi, Laura Culp, Wen-Xin Dong, Nahema Marchal, Kelsie Van Deman, Hema Bajaj Misra, Michael Duah, Moran Ambar, Avi Caciularu, Sandra Lefdal, Chris Summerfield, James An, Pierre-Alexandre Kamienny, Abhinit Mohdi, Theofilos Strinopoulous, Annie Hale, Wayne Anderson, Luis C. Cobo, Niv Efron, Muktha Ananda, Shakir Mohamed, Maureen Heymans, Zoubin Ghahramani, Yossi Matias, Ben Gomes, Lila Ibrahim</dc:creator>
    </item>
    <item>
      <title>The Dual Imperative: Innovation and Regulation in the AI Era</title>
      <link>https://arxiv.org/abs/2407.12690</link>
      <description>arXiv:2407.12690v1 Announce Type: new 
Abstract: This article addresses the societal costs associated with the lack of regulation in Artificial Intelligence and proposes a framework combining innovation and regulation. Over fifty years of AI research, catalyzed by declining computing costs and the proliferation of data, have propelled AI into the mainstream, promising significant economic benefits. Yet, this rapid adoption underscores risks, from bias amplification and labor disruptions to existential threats posed by autonomous systems. The discourse is polarized between accelerationists, advocating for unfettered technological advancement, and doomers, calling for a slowdown to prevent dystopian outcomes. This piece advocates for a middle path that leverages technical innovation and smart regulation to maximize the benefits of AI while minimizing its risks, offering a pragmatic approach to the responsible progress of AI technology. Technical invention beyond the most capable foundation models is needed to contain catastrophic risks. Regulation is required to create incentives for this research while addressing current issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12690v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulo Carv\~ao</dc:creator>
    </item>
    <item>
      <title>How to Mitigate the Dependencies of ChatGPT-4o in Engineering Education</title>
      <link>https://arxiv.org/abs/2407.12693</link>
      <description>arXiv:2407.12693v1 Announce Type: new 
Abstract: The rapid evolution of large multimodal models (LMMs) has significantly impacted modern teaching and learning, especially in computer engineering. While LMMs offer extensive opportunities for enhancing learning, they also risk undermining traditional teaching methods and fostering excessive reliance on automated solutions. To counter this, we have developed strategies within curriculum to reduce the dependencies on LMMs that represented by ChatGPT-4o. These include designing course topics that encourage hands-on problem-solving. The proposed strategies were demonstrated through an actual course implementation. Preliminary results show that the methods effectively enhance student engagement and understanding, balancing the benefits of technology with the preservation of traditional learning principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12693v1</guid>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maoyang Xiang, T. Hui Teo</dc:creator>
    </item>
    <item>
      <title>An efficient machine learning approach for extracting eSports players distinguishing features and classifying their skill levels using symbolic transfer entropy and consensus nested cross validation</title>
      <link>https://arxiv.org/abs/2407.11972</link>
      <description>arXiv:2407.11972v1 Announce Type: cross 
Abstract: Discovering features that set elite players apart is of great significance for eSports coaches as it enables them to arrange a more effective training program focused on improving those features. Moreover, finding such features results in a better evaluation of eSports players skills, which, besides coaches, is of interest for game developers to design games automatically adaptable to the players expertise. Sensor data combined with machine learning have already proved effective in classifying eSports players. However, the existing methods do not provide sufficient information about features that distinguish high-skilled players. In this paper, we propose an efficient method to find these features and then use them to classify players' skill levels. We first apply a time window to extract the players' sensor data, including heart rate, hand activities, etc., before and after game events in the League of Legends game. We use the extracted segments and symbolic transfer entropy to calculate connectivity features between sensors. The most relevant features are then selected using the newly developed consensus nested cross validation method. These features, representing the harmony between body parts, are finally used to find the optimum window size and classify players' skills. The classification results demonstrate a significant improvement by achieving 90.1% accuracy. Also, connectivity features between players gaze positions and keyboard, mouse, and hand activities were the most distinguishing features in classifying players' skills. The proposed method in this paper can be similarly applied to sportspeople data and potentially revolutionize the training programs in both eSports and sports industries</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11972v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s41060-024-00529-6</arxiv:DOI>
      <dc:creator>Amin Noroozi, Mohammad S. Hasan, Maryam Ravan, Elham Norouzi, Ying-Ying Law</dc:creator>
    </item>
    <item>
      <title>Preliminary Study of the Impact of AI-Based Interventions on Health and Behavioral Outcomes in Maternal Health Programs</title>
      <link>https://arxiv.org/abs/2407.11973</link>
      <description>arXiv:2407.11973v1 Announce Type: cross 
Abstract: Automated voice calls are an effective method of delivering maternal and child health information to mothers in underserved communities. One method to fight dwindling listenership is through an intervention in which health workers make live service calls. Previous work has shown that we can use AI to identify beneficiaries whose listenership gets the greatest boost from an intervention. It has also been demonstrated that listening to the automated voice calls consistently leads to improved health outcomes for the beneficiaries of the program. These two observations combined suggest the positive effect of AI-based intervention scheduling on behavioral and health outcomes. This study analyzes the relationship between the two. Specifically, we are interested in mothers' health knowledge in the post-natal period, measured through survey questions. We present evidence that improved listenership through AI-scheduled interventions leads to a better understanding of key health issues during pregnancy and infancy. This improved understanding has the potential to benefit the health outcomes of mothers and their babies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11973v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpan Dasgupta, Niclas Boehmer, Neha Madhiwalla, Aparna Hedge, Bryan Wilder, Milind Tambe, Aparna Taneja</dc:creator>
    </item>
    <item>
      <title>Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents</title>
      <link>https://arxiv.org/abs/2407.11977</link>
      <description>arXiv:2407.11977v1 Announce Type: cross 
Abstract: The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11977v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangzhi Sun, Xiao Zhan, Jose Such</dc:creator>
    </item>
    <item>
      <title>Interpret3C: Interpretable Student Clustering Through Individualized Feature Selection</title>
      <link>https://arxiv.org/abs/2407.11979</link>
      <description>arXiv:2407.11979v1 Announce Type: cross 
Abstract: Clustering in education, particularly in large-scale online environments like MOOCs, is essential for understanding and adapting to diverse student needs. However, the effectiveness of clustering depends on its interpretability, which becomes challenging with high-dimensional data. Existing clustering approaches often neglect individual differences in feature importance and rely on a homogenized feature set. Addressing this gap, we introduce Interpret3C (Interpretable Conditional Computation Clustering), a novel clustering pipeline that incorporates interpretable neural networks (NNs) in an unsupervised learning context. This method leverages adaptive gating in NNs to select features for each student. Then, clustering is performed using the most relevant features per student, enhancing clusters' relevance and interpretability. We use Interpret3C to analyze the behavioral clusters considering individual feature importances in a MOOC with over 5,000 students. This research contributes to the field by offering a scalable, robust clustering methodology and an educational case study that respects individual student differences and improves interpretability for high-dimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11979v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isadora Salles, Paola Mejia-Domenzain, Vinitra Swamy, Julian Blackwell, Tanja K\"aser</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Learning Buddy and Teaching Assistant: Pre-service Teachers' Uses and Attitudes</title>
      <link>https://arxiv.org/abs/2407.11983</link>
      <description>arXiv:2407.11983v1 Announce Type: cross 
Abstract: To uncover pre-service teachers' (PSTs') user experience and perceptions of generative artificial intelligence (GenAI) applications, we surveyed 167 Ghana PSTs' specific uses of GenAI as a learning buddy and teaching assistant, and their attitudes towards these applications. Employing exploratory factor analysis (EFA), we identified three key factors shaping PSTs' attitudes towards GenAI: teaching, learning, and ethical and advocacy factors. The mean scores of these factors revealed a generally positive attitude towards GenAI, indicating high levels of agreement on its potential to enhance PSTs' content knowledge and access to learning and teaching resources, thereby reducing their need for assistance from colleagues. Specifically, PSTs use GenAI as a learning buddy to access reading materials, in-depth content explanations, and practical examples, and as a teaching assistant to enhance teaching resources, develop assessment strategies, and plan lessons. A regression analysis showed that background factors such as age, gender, and year of study do not predict PSTs' attitudes towards GenAI, but age and year of study significantly predict the frequency of their use of GenAI, while gender does not. These findings suggest that older PSTs and those further along in their teacher education programs may use GenAI more frequently, but their perceptions of the application remain unchanged. However, PSTs expressed concerns about the accuracy and trustworthiness of the information provided by GenAI applications. We, therefore, suggest addressing these concerns to ensure PSTs can confidently rely on these applications in their teacher preparation programs. Additionally, we recommend targeted strategies to integrate GenAI more effectively into both learning and teaching processes for PSTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11983v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Lehong Shi, Macharious Nabang, Xiaoming Zhai, Patrick Kyeremeh, Samuel Arthur Ayoberd, Bismark Nyaaba Akanzire</dc:creator>
    </item>
    <item>
      <title>Evaluating Contextually Personalized Programming Exercises Created with Generative AI</title>
      <link>https://arxiv.org/abs/2407.11994</link>
      <description>arXiv:2407.11994v1 Announce Type: cross 
Abstract: Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students' interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners' situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students' interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11994v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evanfiya Logacheva, Arto Hellas, James Prather, Sami Sarsa, Juho Leinonen</dc:creator>
    </item>
    <item>
      <title>Surprising Performances of Students with Autism in Classroom with NAO Robot</title>
      <link>https://arxiv.org/abs/2407.12014</link>
      <description>arXiv:2407.12014v1 Announce Type: cross 
Abstract: Autism is a developmental disorder that manifests in early childhood and persists throughout life, profoundly affecting social behavior and hindering the acquisition of learning and social skills in those diagnosed. As technological advancements progress, an increasing array of technologies is being utilized to support the education of students with Autism Spectrum Disorder (ASD), aiming to improve their educational outcomes and social capabilities. Numerous studies on autism intervention have highlighted the effectiveness of social robots in behavioral treatments. However, research on the integration of social robots into classroom settings for children with autism remains sparse. This paper describes the design and implementation of a group experiment in a collective classroom setting mediated by the NAO robot. The experiment involved special education teachers and the NAO robot collaboratively conducting classroom activities, aiming to foster a dynamic learning environment through interactions among teachers, the robot, and students. Conducted in a special education school, this experiment served as a foundational study in anticipation of extended robot-assisted classroom sessions. Data from the experiment suggest that ASD students in classrooms equipped with the NAO robot exhibited notably better performance compared to those in regular classrooms. The humanoid features and body language of the NAO robot captivated the students' attention, particularly during talent shows and command tasks, where students demonstrated heightened engagement and a decrease in stereotypical repetitive behaviors and irrelevant minor movements commonly observed in regular settings. Our preliminary findings indicate that the NAO robot significantly enhances focus and classroom engagement among students with ASD, potentially improving educational performance and fostering better social behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12014v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qin Yang, Huan Lu, Dandan Liang, Shengrong Gong, Huanghao Feng</dc:creator>
    </item>
    <item>
      <title>A Benchmark for Fairness-Aware Graph Learning</title>
      <link>https://arxiv.org/abs/2407.12112</link>
      <description>arXiv:2407.12112v1 Announce Type: cross 
Abstract: Fairness-aware graph learning has gained increasing attention in recent years. Nevertheless, there lacks a comprehensive benchmark to evaluate and compare different fairness-aware graph learning methods, which blocks practitioners from choosing appropriate ones for broader real-world applications. In this paper, we present an extensive benchmark on ten representative fairness-aware graph learning methods. Specifically, we design a systematic evaluation protocol and conduct experiments on seven real-world datasets to evaluate these methods from multiple perspectives, including group fairness, individual fairness, the balance between different fairness criteria, and computational efficiency. Our in-depth analysis reveals key insights into the strengths and limitations of existing methods. Additionally, we provide practical guidance for applying fairness-aware graph learning methods in applications. To the best of our knowledge, this work serves as an initial step towards comprehensively understanding representative fairness-aware graph learning methods to facilitate future advancements in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12112v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yushun Dong, Song Wang, Zhenyu Lei, Zaiyi Zheng, Jing Ma, Chen Chen, Jundong Li</dc:creator>
    </item>
    <item>
      <title>Physical partisan proximity outweighs online ties in predicting US voting outcomes</title>
      <link>https://arxiv.org/abs/2407.12146</link>
      <description>arXiv:2407.12146v1 Announce Type: cross 
Abstract: Affective polarization and increasing social divisions affect social mixing and the spread of information across online and physical spaces, reinforcing social and electoral cleavages and influencing political outcomes. Here, using aggregated and de-identified co-location and online network data, we investigate the relationship between partisan exposure and voting patterns in the USA by comparing three dimensions of partisan exposure: physical proximity and exposure to the same social contexts, online social ties, and residential sorting. By leveraging various statistical modeling approaches, we consistently find that partisan exposure in the physical space, as captured by co-location patterns, more accurately predicts electoral outcomes in US counties, outperforming online and residential exposures across metropolitan and non-metro areas. Moreover, our results show that physical partisan proximity is the best predictor of voting patterns in swing counties, where the election results are most uncertain. We also estimate county-level experienced partisan segregation and examine its relationship with individuals' demographic and socioeconomic characteristics. Focusing on metropolitan areas, our results confirm the presence of extensive partisan segregation in the US and show that offline partisan isolation, both considering physical encounters or residential sorting, is higher than online segregation and is primarily associated with educational attainment. Our findings emphasize the importance of physical space in understanding the relationship between social networks and political behavior, in contrast to the intense scrutiny focused on online social networks and elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12146v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Tonin, Bruno Lepri, Michele Tizzoni</dc:creator>
    </item>
    <item>
      <title>Questionable practices in machine learning</title>
      <link>https://arxiv.org/abs/2407.12220</link>
      <description>arXiv:2407.12220v1 Announce Type: cross 
Abstract: Evaluating modern ML models is hard. The strong incentive for researchers and companies to report a state-of-the-art result on some metric often leads to questionable research practices (QRPs): bad practices which fall short of outright research fraud. We describe 43 such practices which can undermine reported results, giving examples where possible. Our list emphasises the evaluation of large language models (LLMs) on public benchmarks. We also discuss "irreproducible research practices", i.e. decisions that make it difficult or impossible for other researchers to reproduce, build on or audit previous research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12220v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin Leech, Juan J. Vazquez, Misha Yagudin, Niclas Kupper, Laurence Aitchison</dc:creator>
    </item>
    <item>
      <title>The Better Angels of Machine Personality: How Personality Relates to LLM Safety</title>
      <link>https://arxiv.org/abs/2407.12344</link>
      <description>arXiv:2407.12344v1 Announce Type: cross 
Abstract: Personality psychologists have analyzed the relationship between personality and safety behaviors in human society. Although Large Language Models (LLMs) demonstrate personality traits, the relationship between personality traits and safety abilities in LLMs still remains a mystery. In this paper, we discover that LLMs' personality traits are closely related to their safety abilities, i.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale. Meanwhile, the safety alignment generally increases various LLMs' Extraversion, Sensing, and Judging traits. According to such findings, we can edit LLMs' personality traits and improve their safety performance, e.g., inducing personality from ISTJ to ISTP resulted in a relative improvement of approximately 43% and 10% in privacy and fairness performance, respectively. Additionally, we find that LLMs with different personality traits are differentially susceptible to jailbreak. This study pioneers the investigation of LLM safety from a personality perspective, providing new insights into LLM safety enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12344v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Zhang, Dongrui Liu, Chen Qian, Ziyue Gan, Yong Liu, Yu Qiao, Jing Shao</dc:creator>
    </item>
    <item>
      <title>PersLLM: A Personified Training Approach for Large Language Models</title>
      <link>https://arxiv.org/abs/2407.12393</link>
      <description>arXiv:2407.12393v2 Announce Type: cross 
Abstract: Large language models exhibit aspects of human-level intelligence that catalyze their application as human-like agents in domains such as social simulations, human-machine interactions, and collaborative multi-agent systems. However, the absence of distinct personalities, such as displaying ingratiating behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs utility in practical applications. Addressing this, the development of personality traits in LLMs emerges as a crucial area of research to unlock their latent potential. Existing methods to personify LLMs generally involve strategies like employing stylized training data for instruction tuning or using prompt engineering to simulate different personalities. These methods only capture superficial linguistic styles instead of the core of personalities and are therefore not stable. In this study, we propose PersLLM, integrating psychology-grounded principles of personality: social practice, consistency, and dynamic development, into a comprehensive training methodology. We incorporate personality traits directly into the model parameters, enhancing the model's resistance to induction, promoting consistency, and supporting the dynamic evolution of personality. Single-agent evaluation validates our method's superiority, as it produces responses more aligned with reference personalities compared to other approaches. Case studies for multi-agent communication highlight its benefits in enhancing opinion consistency within individual agents and fostering collaborative creativity among multiple agents in dialogue contexts, potentially benefiting human simulation and multi-agent cooperation. Additionally, human-agent interaction evaluations indicate that our personified models significantly enhance interactive experiences, underscoring the practical implications of our research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12393v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhiyuan Liu, Maosong Sun</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Sustainable Framework for Machine Learning and Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2407.12445</link>
      <description>arXiv:2407.12445v1 Announce Type: cross 
Abstract: In financial applications, regulations or best practices often lead to specific requirements in machine learning relating to four key pillars: fairness, privacy, interpretability and greenhouse gas emissions. These all sit in the broader context of sustainability in AI, an emerging practical AI topic. However, although these pillars have been individually addressed by past literature, none of these works have considered all the pillars. There are inherent trade-offs between each of the pillars (for example, accuracy vs fairness or accuracy vs privacy), making it even more important to consider them together. This paper outlines a new framework for Sustainable Machine Learning and proposes FPIG, a general AI pipeline that allows for these critical topics to be considered simultaneously to learn the trade-offs between the pillars better. Based on the FPIG framework, we propose a meta-learning algorithm to estimate the four key pillars given a dataset summary, model architecture, and hyperparameters before model training. This algorithm allows users to select the optimal model architecture for a given dataset and a given set of user requirements on the pillars. We illustrate the trade-offs under the FPIG model on three classical datasets and demonstrate the meta-learning approach with an example of real-world datasets and models with different interpretability, showcasing how it can aid model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12445v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Pagliari, Peter Hill, Po-Yu Chen, Maciej Dabrowny, Tingsheng Tan, Francois Buet-Golfouse</dc:creator>
    </item>
    <item>
      <title>Towards Understanding Unsafe Video Generation</title>
      <link>https://arxiv.org/abs/2407.12581</link>
      <description>arXiv:2407.12581v1 Announce Type: cross 
Abstract: Video generation models (VGMs) have demonstrated the capability to synthesize high-quality output. It is important to understand their potential to produce unsafe content, such as violent or terrifying videos. In this work, we provide a comprehensive understanding of unsafe video generation.
  First, to confirm the possibility that these models could indeed generate unsafe videos, we choose unsafe content generation prompts collected from 4chan and Lexica, and three open-source SOTA VGMs to generate unsafe videos. After filtering out duplicates and poorly generated content, we created an initial set of 2112 unsafe videos from an original pool of 5607 videos. Through clustering and thematic coding analysis of these generated videos, we identify 5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic, Violent/Bloody, and Political. With IRB approval, we then recruit online participants to help label the generated videos. Based on the annotations submitted by 403 participants, we identified 937 unsafe videos from the initial video set. With the labeled information and the corresponding prompts, we created the first dataset of unsafe videos generated by VGMs.
  We then study possible defense mechanisms to prevent the generation of unsafe videos. Existing defense methods in image generation focus on filtering either input prompt or output results. We propose a new approach called Latent Variable Defense (LVD), which works within the model's internal sampling process. LVD can achieve 0.90 defense accuracy while reducing time and computing resources by 10x when sampling a large number of unsafe prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12581v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Pang, Aiping Xiong, Yang Zhang, Tianhao Wang</dc:creator>
    </item>
    <item>
      <title>The Future of Learning: Large Language Models through the Lens of Students</title>
      <link>https://arxiv.org/abs/2407.12723</link>
      <description>arXiv:2407.12723v1 Announce Type: cross 
Abstract: As Large-Scale Language Models (LLMs) continue to evolve, they demonstrate significant enhancements in performance and an expansion of functionalities, impacting various domains, including education. In this study, we conducted interviews with 14 students to explore their everyday interactions with ChatGPT. Our preliminary findings reveal that students grapple with the dilemma of utilizing ChatGPT's efficiency for learning and information seeking, while simultaneously experiencing a crisis of trust and ethical concerns regarding the outcomes and broader impacts of ChatGPT. The students perceive ChatGPT as being more "human-like" compared to traditional AI. This dilemma, characterized by mixed emotions, inconsistent behaviors, and an overall positive attitude towards ChatGPT, underscores its potential for beneficial applications in education and learning. However, we argue that despite its human-like qualities, the advanced capabilities of such intelligence might lead to adverse consequences. Therefore, it's imperative to approach its application cautiously and strive to mitigate potential harms in future developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12723v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Zhang, Jingyi Xie, Chuhao Wu, Jie Cai, ChanMin Kim, John M. Carroll</dc:creator>
    </item>
    <item>
      <title>The Role of Network and Identity in the Diffusion of Hashtags</title>
      <link>https://arxiv.org/abs/2407.12771</link>
      <description>arXiv:2407.12771v1 Announce Type: cross 
Abstract: Although the spread of behaviors is influenced by many social factors, existing literature tends to study the effects of single factors -- most often, properties of the social network -- on the final cascade. In order to move towards a more integrated view of cascades, this paper offers the first comprehensive investigation into the role of two social factors in the diffusion of 1,337 popular hashtags representing the production of novel culture on Twitter: 1) the topology of the Twitter social network and 2) performance of each user's probable demographic identity. Here, we show that cascades are best modeled using a combination of network and identity, rather than either factor alone. This combined model best reproduces a composite index of ten cascade properties across all 1,337 hashtags. However, there is important heterogeneity in what social factors are required to reproduce different properties of hashtag cascades. For instance, while a combined network+identity model best predicts the popularity of cascades, a network-only model has better performance in predicting cascade growth and an identity-only model in adopter composition. We are able to predict what type of hashtag is best modeled by each combination of features and use this to further improve performance. Additionally, consistent with prior literature on the combined network+identity model most outperforms the single-factor counterfactuals among hashtags used for expressing racial or regional identity, stance-taking, talking about sports, or variants of existing cultural trends with very slow- or fast-growing communicative need. In sum, our results imply the utility of multi-factor models in predicting cascades, in order to account for the varied ways in which network, identity, and other social factors play a role in the diffusion of hashtags on Twitter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12771v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aparna Ananthasubramaniam, Yufei Zhu, David Jurgens, Daniel Romero</dc:creator>
    </item>
    <item>
      <title>Beyond Incompatibility: Trade-offs between Mutually Exclusive Fairness Criteria in Machine Learning and Law</title>
      <link>https://arxiv.org/abs/2212.00469</link>
      <description>arXiv:2212.00469v5 Announce Type: replace 
Abstract: Fair and trustworthy AI is becoming ever more important in both machine learning and legal domains. One important consequence is that decision makers must seek to guarantee a 'fair', i.e., non-discriminatory, algorithmic decision procedure. However, there are several competing notions of algorithmic fairness that have been shown to be mutually incompatible under realistic factual assumptions. This concerns, for example, the widely used fairness measures of 'calibration within groups' and 'balance for the positive/negative class'. In this paper, we present a novel algorithm (FAir Interpolation Method: FAIM) for continuously interpolating between these three fairness criteria. Thus, an initially unfair prediction can be remedied to, at least partially, meet a desired, weighted combination of the respective fairness conditions. We demonstrate the effectiveness of our algorithm when applied to synthetic data, the COMPAS data set, and a new, real-world data set from the e-commerce sector. Finally, we discuss to what extent FAIM can be harnessed to comply with conflicting legal obligations. The analysis suggests that it may operationalize duties in traditional legal fields, such as credit scoring and criminal justice proceedings, but also for the latest AI regulations put forth in the EU, like the Digital Markets Act and the recently enacted AI Act.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.00469v5</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Meike Zehlike, Alex Loosley, H{\aa}kan Jonsson, Emil Wiedemann, Philipp Hacker</dc:creator>
    </item>
    <item>
      <title>Bike Frames: Understanding the Implicit Portrayal of Cyclists in the News</title>
      <link>https://arxiv.org/abs/2301.06178</link>
      <description>arXiv:2301.06178v2 Announce Type: replace 
Abstract: Increasing cycling for transportation or recreation can boost health and reduce the environmental impacts of vehicles. However, news agencies' ideologies and reporting styles often influence public perception of cycling. For example, if news agencies overly report cycling accidents, it may make people perceive cyclists as "dangerous," reducing the number of cyclists who opt to cycle. Additionally, a decline in cycling can result in less government funding for safe infrastructure. In this paper, we develop a method for detecting the perceived perception of cyclists within news headlines. We introduce a new dataset called ``Bike Frames'' to accomplish this. The dataset consists of 31,480 news headlines and 1,500 annotations. Our focus is on analyzing 11,385 headlines from the United States. We also introduce the BikeFrame Chain-of-Code framework to predict cyclist perception, identify accident-related headlines, and determine fault. This framework uses pseudocode for precise logic and integrates news agency bias analysis for improved predictions over traditional chain-of-thought reasoning in large language models. Our method substantially outperforms other methods, and most importantly, we find that incorporating news bias information substantially impacts performance, improving the average F1 from .739 to .815. Finally, we perform a comprehensive case study on US-based news headlines, finding reporting differences between news agencies and cycling-specific websites as well as differences in reporting depending on the gender of cyclists. WARNING: This paper contains descriptions of accidents and death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06178v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingmeng Zhao, Dan Schumacher, Sashank Nalluri, Xavier Walton, Suhana Shrestha, Anthony Rios</dc:creator>
    </item>
    <item>
      <title>When Should Algorithms Resign? A Proposal for AI Governance</title>
      <link>https://arxiv.org/abs/2402.18326</link>
      <description>arXiv:2402.18326v2 Announce Type: replace 
Abstract: Algorithmic resignation is a strategic approach for managing the use of artificial intelligence (AI) by embedding governance directly into AI systems. It involves deliberate and informed disengagement from AI, such as restricting access AI outputs or displaying performance disclaimers, in specific scenarios to aid the appropriate and effective use of AI. By integrating algorithmic resignation as a governance mechanism, organizations can better control when and how AI is used, balancing the benefits of automation with the need for human oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18326v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Bhatt, Holli Sargeant</dc:creator>
    </item>
    <item>
      <title>TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models</title>
      <link>https://arxiv.org/abs/2312.01261</link>
      <description>arXiv:2312.01261v2 Announce Type: replace-cross 
Abstract: Text-to-Image (TTI) generative models have shown great progress in the past few years in terms of their ability to generate complex and high-quality imagery. At the same time, these models have been shown to suffer from harmful biases, including exaggerated societal biases (e.g., gender, ethnicity), as well as incidental correlations that limit such a model's ability to generate more diverse imagery. In this paper, we propose a general approach to study and quantify a broad spectrum of biases, for any TTI model and for any prompt, using counterfactual reasoning. Unlike other works that evaluate generated images on a predefined set of bias axes, our approach automatically identifies potential biases that might be relevant to the given prompt, and measures those biases. In addition, we complement quantitative scores with post-hoc explanations in terms of semantic concepts in the images generated. We show that our method is uniquely capable of explaining complex multi-dimensional biases through semantic concepts, as well as the intersectionality between different biases for any given prompt. We perform extensive user studies to illustrate that the results of our method and analysis are consistent with human judgements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01261v2</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Chinchure, Pushkar Shukla, Gaurav Bhatt, Kiri Salij, Kartik Hosanagar, Leonid Sigal, Matthew Turk</dc:creator>
    </item>
  </channel>
</rss>
