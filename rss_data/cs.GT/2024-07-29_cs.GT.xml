<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Approval-Based Committee Voting under Uncertainty</title>
      <link>https://arxiv.org/abs/2407.19391</link>
      <description>arXiv:2407.19391v1 Announce Type: new 
Abstract: We study approval-based committee voting in which a target number of candidates are selected based on voters' approval preferences over candidates. In contrast to most of the work, we consider the setting where voters express uncertain approval preferences and explore four different types of uncertain approval preference models. For each model, we study the problems such as computing a committee with the highest probability of satisfying axioms such as justified representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19391v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hariz Aziz, Venkateswara Rao Kagita, Baharak Rastegari, Mashbat Suzuki</dc:creator>
    </item>
    <item>
      <title>A Stackelberg Game Model of Flocking</title>
      <link>https://arxiv.org/abs/2407.19678</link>
      <description>arXiv:2407.19678v1 Announce Type: new 
Abstract: We study a Stackelberg game to examine how two agents determine to cooperate while competing with each other. Each selects an arrival time to a destination, the earlier one fetching a higher reward. There is, however, an inherent penalty in arriving too early as well as a risk in traveling alone. This gives rise to the possibility of the agents cooperating by traveling together while competing for the reward. In our prior work [1] we studied this problem as a sequential game among a set of $N$ competing agents in continuous time, and defined the formation of a group traveling together as arriving at exactly the same time. In the present study, we relax this definition to allow arrival times within a small window, and study a 2-agent game in both continuous and discrete time, referred to as the flock formation game. We derive and examine the properties of the subgame perfect equilibrium (SPE) of this game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19678v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenlan Wang, Mehrdad Moharrami, Mingyan Liu</dc:creator>
    </item>
    <item>
      <title>Online Test Synthesis From Requirements: Enhancing Reinforcement Learning with Game Theory</title>
      <link>https://arxiv.org/abs/2407.18994</link>
      <description>arXiv:2407.18994v1 Announce Type: cross 
Abstract: We consider the automatic online synthesis of black-box  test cases from functional requirements specified as automata for reactive implementations. The goal of the tester is to reach some given state, so as to satisfy a coverage criterion, while monitoring the violation of the requirements. We develop an approach based on Monte Carlo Tree Search, which is a classical technique in reinforcement learning for efficiently selecting promising inputs. Seeing the automata requirements as a game between the implementation and the tester, we develop a heuristic by biasing the search towards inputs that are promising in this game. We experimentally show that our heuristic accelerates the convergence of the Monte Carlo Tree Search algorithm, thus improving the performance of testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18994v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ocan Sankur (DEVINE, UR), Thierry J\'eron (DEVINE, UR), Nicolas Markey (DEVINE, UR), David Mentr\'e (MERCE-France), Reiya Noguchi</dc:creator>
    </item>
    <item>
      <title>A Family of Switching Pursuit Strategies for a Multi-Pursuer Single-Evader Game</title>
      <link>https://arxiv.org/abs/2407.19954</link>
      <description>arXiv:2407.19954v1 Announce Type: cross 
Abstract: A new family of pursuit strategies is introduced for a multi-pursuer single-evader game. By exploiting the optimal solution of the game involving two pursuers, conditions are derived under which the multi-pursuer game becomes equivalent to the two-pursuer one. This opens the possibility of designing a number of pursuit strategies in which the pursuers first try to enforce the satisfaction of the aforementioned condition and then switch to a two-pursuer game as soon as it is verified. The contribution is useful in two ways. First, new winning pursuit strategies can be devised starting from simple plans, such as pure pursuit. Moreover, the performance of existing pursuit strategies, like those based on Voronoi partitions, can be significantly improved by resorting to the corresponding switching version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19954v1</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Casini, Andrea Garulli</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Guarantees for Best-Response Learning Dynamics in Zero-Sum Matrix Games</title>
      <link>https://arxiv.org/abs/2407.20128</link>
      <description>arXiv:2407.20128v1 Announce Type: cross 
Abstract: We study best-response type learning dynamics for two player zero-sum matrix games. We consider two settings that are distinguished by the type of information that each player has about the game and their opponent's strategy. The first setting is the full information case, in which each player knows their own and the opponent's payoff matrices and observes the opponent's mixed strategy. The second setting is the minimal information case, where players do not observe the opponent's strategy and are not aware of either of the payoff matrices (instead they only observe their realized payoffs). For this setting, also known as the radically uncoupled case in the learning in games literature, we study a two-timescale learning dynamics that combine smoothed best-response type updates for strategy estimates with a TD-learning update to estimate a local payoff function. For these dynamics, without additional exploration, we provide polynomial-time finite-sample guarantees for convergence to an $\epsilon$-Nash equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20128v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fathima Zarin Faizal, Asuman Ozdaglar, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Opponent Modeling in Multiplayer Imperfect-Information Games</title>
      <link>https://arxiv.org/abs/2212.06027</link>
      <description>arXiv:2212.06027v4 Announce Type: replace 
Abstract: In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06027v4</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Ganzfried, Kevin A. Wang, Max Chiswick</dc:creator>
    </item>
    <item>
      <title>Contract Design for Pandora's Box</title>
      <link>https://arxiv.org/abs/2403.02317</link>
      <description>arXiv:2403.02317v2 Announce Type: replace 
Abstract: We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.
  In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios.
  A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02317v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Hoefer, Conrad Schecker, Kevin Schewior</dc:creator>
    </item>
    <item>
      <title>Regular Games with Imperfect Information Are Not That Regular</title>
      <link>https://arxiv.org/abs/2403.20133</link>
      <description>arXiv:2403.20133v2 Announce Type: replace 
Abstract: We consider two-player games with imperfect information and the synthesis of a randomized strategy for one player that ensures the objective is satisfied almost-surely (i.e., with probability 1), regardless of the strategy of the other player. Imperfect information is modeled by an indistinguishability relation describing the pairs of histories that the first player cannot distinguish, a generalization of the traditional model with partial observations. The game is regular if it admits a regular function whose kernel commutes with the indistinguishability relation.
  The synthesis of pure strategies that ensure all possible outcomes satisfy the objective is possible in regular games, by a generic reduction that holds for all objectives. While the solution for pure strategies extends to randomized strategies in the traditional model with partial observations (which is always regular), we show that a similar reduction does not exist in the more general model. Despite that, we show that in regular games with Buechi objectives the synthesis problem is decidable for randomized strategies that ensure the outcome satisfies the objective almost-surely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20133v2</guid>
      <category>cs.GT</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurent Doyen, Thomas Soullard</dc:creator>
    </item>
    <item>
      <title>Strategic Cost Selection in Participatory Budgeting</title>
      <link>https://arxiv.org/abs/2407.18092</link>
      <description>arXiv:2407.18092v2 Announce Type: replace 
Abstract: We study strategic behavior of project proposers in the context of approval-based participatory budgeting (PB). In our model we assume that the votes are fixed and known and the proposers want to set as high project prices as possible, provided that their projects get selected and the prices are not below the minimum costs of their delivery. We study the existence of pure Nash equilibria (NE) in such games, focusing on the AV/Cost, Phragm\'en, and Method of Equal Shares rules. Furthermore, we report an experimental study of strategic cost selection on real-life PB election data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18092v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Faliszewski, {\L}ukasz Janeczko, Andrzej Kaczmarczyk, Grzegorz Lisowski, Piotr Skowron, Stanis{\l}aw Szufa</dc:creator>
    </item>
    <item>
      <title>Learning in Mean Field Games: A Survey</title>
      <link>https://arxiv.org/abs/2205.12944</link>
      <description>arXiv:2205.12944v4 Announce Type: replace-cross 
Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of MFGs. We then present a general framework for classical iterative methods (based on best-response computation or policy evaluation) to solve MFGs in an exact way. Building on these algorithms and the connection with Markov Decision Processes, we explain how RL can be used to learn MFG solutions in a model-free way. Last, we present numerical illustrations on a benchmark problem, and conclude with some perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.12944v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Lauri\`ere, Sarah Perrin, Julien P\'erolat, Sertan Girgin, Paul Muller, Romuald \'Elie, Matthieu Geist, Olivier Pietquin</dc:creator>
    </item>
    <item>
      <title>Evolving Diverse Red-team Language Models in Multi-round Multi-agent Games</title>
      <link>https://arxiv.org/abs/2310.00322</link>
      <description>arXiv:2310.00322v5 Announce Type: replace-cross 
Abstract: The primary challenge in deploying Large Language Model (LLM) is ensuring its harmlessness. Red team can identify vulnerabilities by attacking LLM to attain safety. However, current efforts heavily rely on single-round prompt designs and unilateral red team optimizations against fixed blue teams. These static approaches lead to significant reductions in generation diversity, known as the mode collapse, which makes it difficult to discover the potential risks in the increasingly complex human-LLM interactions. Here we introduce dynamic Red Team Game (RTG) to comprehensively analyze the multi-round offensive and defensive interactions between red team and blue team. Furthermore, we develop a Gamified Red Team Solver (GRTS) with diversity measures to mitigate mode collapse and theoretically guarantee the convergence of approximate Nash equilibrium which results in better strategies for both teams. Empirical results demonstrate that GRTS explore diverse and implicit attacks to adaptively exploit various LLMs, surpassing the constraints of specific modes. Insightfully, the geometrical structure we unveil of the red team task aligns with the spinning top hypothesis, confirming the necessity of constructing a diverse LLM population as a promising proxy for heterogeneous human expert red-teamers. This paves the way for scalable toxicity detection and safe alignment for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00322v5</guid>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengdong Ma, Ziran Yang, Hai Ci, Jun Gao, Minquan Gao, Xuehai Pan, Yaodong Yang</dc:creator>
    </item>
    <item>
      <title>Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs</title>
      <link>https://arxiv.org/abs/2405.04941</link>
      <description>arXiv:2405.04941v2 Announce Type: replace-cross 
Abstract: Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known. Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets. While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values. These novel semantics for RPOMDPs give access to results for POSGs, studied in game theory; concretely, we show the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04941v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eline M. Bovy, Marnix Suilen, Sebastian Junges, Nils Jansen</dc:creator>
    </item>
    <item>
      <title>Strategy-proof Selling: a Geometric Approach</title>
      <link>https://arxiv.org/abs/2406.12279</link>
      <description>arXiv:2406.12279v2 Announce Type: replace-cross 
Abstract: We consider one buyer and one seller. For a bundle $(t,q)\in [0,\infty[\times [0,1]=\mathbb{Z}$, $q$ either refers to the wining probability of an object or a share of a good, and $t$ denotes the payment that the buyer makes. We define classical and restricted classical preferences of the buyer on $\mathbb{Z}$; they incorporate quasilinear, non-quasilinear, risk averse preferences with multidimensional pay-off relevant parameters. We define rich single-crossing subsets of the two classes, and characterize strategy-proof mechanisms by using monotonicity of the mechanisms and continuity of the indirect preference correspondences. We also provide a computationally tractable optimization program to compute the optimal mechanism. We do not use revenue equivalence and virtual valuations as tools in our proofs. Our proof techniques bring out the geometric interaction between the single-crossing property and the positions of bundles $(t,q)$s. Our proofs are simple and provide computationally tractable optimization program to compute the optimal mechanism. The extension of the optimization program to the $n-$ buyer environment is immediate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12279v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mridu Prabal Goswami</dc:creator>
    </item>
  </channel>
</rss>
