<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Aug 2025 04:03:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Smooth Populations of Parameters with Trial Heterogeneity</title>
      <link>https://arxiv.org/abs/2507.23140</link>
      <description>arXiv:2507.23140v1 Announce Type: new 
Abstract: We consider the classical problem of estimating the mixing distribution of binomial mixtures, but under trial heterogeneity and smoothness. This problem has been studied extensively when the trial parameter is homogeneous, but not under the more general scenario of heterogeneous trials, and only within a low smoothness regime, where the resulting rates are slow. Under the assumption that the density is s-smooth, we derive fast error rates for the kernel density estimator under trial heterogeneity that depend on the harmonic mean of the trials. Importantly, even when reduced to the homogeneous case, our result improves on the state-of-the-art rate of Ye and Bickel (2021). We also study nonparametric estimation of the difference between two densities, which can be smoother than the individual densities, in both i.i.d. and binomial-mixture settings. Our work is motivated by an application in criminal justice: comparing conviction rates of indigent representation in Pennsylvania. We find that the estimated conviction rates for appointed counsel (court-appointed private attorneys) are generally higher than those for public defenders, potentially due to a confounding factor: appointed counsel are more likely to take on severe cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23140v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JungHo Lee, Valerio Ba\'cak, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>CLT in high-dimensional Bayesian linear regression with low SNR</title>
      <link>https://arxiv.org/abs/2507.23285</link>
      <description>arXiv:2507.23285v1 Announce Type: new 
Abstract: We study central limit theorems for linear statistics in high-dimensional Bayesian linear regression with product priors. Unlike the existing literature where the focus is on posterior contraction, we work under a non-contracting regime where neither the likelihood nor the prior dominates the other. This is motivated by modern high-dimensional datasets characterized by a bounded signal-to-noise ratio. This work takes a first step towards understanding limit distributions for one-dimensional projections of the posterior, as well as the posterior mean, in such regimes. Analogous to contractive settings, the resulting limiting distributions are Gaussian, but they heavily depend on the chosen prior and center around the Mean-Field approximation of the posterior. We study two concrete models of interest to illustrate this phenomenon -- the white noise design, and the (misspecified) Bayesian model. As an application, we construct credible intervals and compute their coverage probability under any misspecified prior. Our proofs rely on a combination of recent developments in Berry-Esseen type bounds for Random Field Ising models and both first and second order Poincar\'{e} inequalities. Notably, our results do not require any sparsity assumptions on the prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23285v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Nabarun Deb, Sumit Mukherjee</dc:creator>
    </item>
    <item>
      <title>Optimal-Transport Based Multivariate Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2507.23490</link>
      <description>arXiv:2507.23490v1 Announce Type: new 
Abstract: Characteristic-function based goodness-of-fit tests are suggested for multivariate observations. The test statistics, which are straightforward to compute, are defined as two-sample criteria measuring discrepancy between multivariate ranks of the original observations and the corresponding ranks obtained from an artificial sample generated from the reference distribution under test. Multivariate ranks are constructed using the theory of the optimal measure transport, thus rendering the tests of a simple null hypothesis distribution-free, while bootstrap approximations are still necessary for testing composite null hypotheses. Asymptotic theory is developed and a simulation study, concentrating on comparisons with previously proposed tests of multivariate normality, demonstrates that the method performs well in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23490v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zden\v{e}k Hl\'avka, \v{S}\'arka Hudecov\'a, Simos G. Meintanis</dc:creator>
    </item>
    <item>
      <title>Information geometry of L\'evy processes and financial models</title>
      <link>https://arxiv.org/abs/2507.23646</link>
      <description>arXiv:2507.23646v1 Announce Type: new 
Abstract: We explore the information geometry of L\'evy processes. As a starting point, we derive the $\alpha$-divergence between two L\'evy processes. Subsequently, the Fisher information matrix and the $\alpha$-connection associated with the geometry of L\'evy processes are computed from the $\alpha$-divergence. In addition, we discuss statistical applications of this information geometry. As illustrative examples, we investigate the differential-geometric structures of various L\'evy processes relevant to financial modeling, including tempered stable processes, the CGMY model, and variance gamma processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23646v1</guid>
      <category>stat.TH</category>
      <category>cs.IT</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyung Choi</dc:creator>
    </item>
    <item>
      <title>Phylogenetic network models as graphical models</title>
      <link>https://arxiv.org/abs/2507.23056</link>
      <description>arXiv:2507.23056v1 Announce Type: cross 
Abstract: The displayed tree phylogenetic network model is shown to sit as a natural submodel of the graphical model associated to a directed acyclic graph (DAG). This representation allows to derive a number of results about the displayed tree model. In particular, the concept of a local modification to a DAG model is developed and applied to the displayed tree model. As an application, some nonidentifiability issues related to the displayed tree models are highlighted as they relate to reticulation edges and stacked reticulations in the networks. We also derive rank conditions on flattenings of probability tensors for the displayed tree model, generalizing classic results for phylogenetic tree models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23056v1</guid>
      <category>q-bio.PE</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seth Sullivant</dc:creator>
    </item>
    <item>
      <title>Incorporating structural uncertainty in causal decision making</title>
      <link>https://arxiv.org/abs/2507.23495</link>
      <description>arXiv:2507.23495v1 Announce Type: cross 
Abstract: Practitioners making decisions based on causal effects typically ignore structural uncertainty. We analyze when this uncertainty is consequential enough to warrant methodological solutions (Bayesian model averaging over competing causal structures). Focusing on bivariate relationships ($X \rightarrow Y$ vs. $X \leftarrow Y$), we establish that model averaging is beneficial when: (1) structural uncertainty is moderate to high, (2) causal effects differ substantially between structures, and (3) loss functions are sufficiently sensitive to the size of the causal effect. We prove optimality results of our suggested methodological solution under regularity conditions and demonstrate through simulations that modern causal discovery methods can provide, within limits, the necessary quantification. Our framework complements existing robust causal inference approaches by addressing a distinct source of uncertainty typically overlooked in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23495v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maurits Kaptein</dc:creator>
    </item>
    <item>
      <title>Rank Distributions for Independent Normals with a Single Outlier</title>
      <link>https://arxiv.org/abs/2401.00952</link>
      <description>arXiv:2401.00952v3 Announce Type: replace 
Abstract: Thurstone's latent-normal model, introduced a century ago to describe human preferences in psychometrics (1927), remains a cornerstone for modeling random rankings. Yet when the underlying normals differ in distribution, the joint law of ranks $R_{i}:=\sum_{j=1}^{n}\mathbf{1}_{X_{j}\leq X_{i}}$ is virtually unexplored. We study the simplest non-identically-distributed case: $n+1$ independent normals with $X_{0}\sim\mathcal{N}\left(\mu_{0},\,\sigma_{0}^{2}\right)$ and $X_{i}\sim\mathcal{N}\left(\mu,\,\sigma^{2}\right)$ for $1\leq i\leq n$. Here, $R_0 \mid X_0 \;\sim\; 1 + \mathrm{Binomial}\bigl(n,\;\Phi\bigl(\bigl(X_0 - \mu\bigr)\big/\sigma\bigr)\bigr)$, and the success probability $\Phi\bigl(\bigl(X_0 - \mu\bigr)\big/\sigma\bigr)$ is accurately modeled by a beta distribution. Exploiting beta-binomial conjugacy, we observe that $R_{0}-1$ follows a beta-binomial law, which then yields a precise approximation for the joint distribution of $\left(R_{0},R_{i_{1}},\ldots,R_{i_{m}}\right)$. We derive closed-form expressions for $\mathbb{E}R_{i}$, $\mathrm{Cov}\left(R_{i},R_{j}\right)$, and the limiting distributions of $\left(R_{0},R_{i_{1}},\ldots,R_{i_{m}}\right)$ as key parameters grow large or small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00952v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip T. Labo</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v5 Announce Type: replace 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. It is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes four chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains seven chapters of advanced topics. The book collates important results from a variety of modern papers on e-values and related concepts, and also contains many results not published elsewhere. It offers a coherent and comprehensive picture on a fast-growing research area, and is ready to use as the basis of a graduate course in statistics and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Proper scoring rules for estimation and forecast evaluation</title>
      <link>https://arxiv.org/abs/2504.01781</link>
      <description>arXiv:2504.01781v3 Announce Type: replace 
Abstract: Proper scoring rules have been a subject of growing interest in recent years, not only as tools for evaluation of probabilistic forecasts but also as methods for estimating probability distributions. In this article, we review the mathematical foundations of proper scoring rules including general characterization results and important families of scoring rules. We discuss their role in statistics and machine learning for estimation and forecast evaluation. Furthermore, we comment on interesting developments of their usage in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01781v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kartik Waghmare, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Covariance scanning for adaptively optimal change point detection in high-dimensional linear models</title>
      <link>https://arxiv.org/abs/2507.02552</link>
      <description>arXiv:2507.02552v2 Announce Type: replace 
Abstract: This paper investigates the detection and estimation of a single change in high-dimensional linear models. We derive minimax lower bounds for the detection boundary and the estimation rate, which uncover a phase transition governed the sparsity of the covariance-weighted differential parameter. This form of "inherent sparsity" captures a delicate interplay between the covariance structure of the regressors and the change in regression coefficients on the detectability of a change point. Complementing the lower bounds, we introduce two covariance scanning-based methods, McScan and QcSan, which achieve minimax optimal performance (up to possible logarithmic factors) in the sparse and the dense regimes, respectively. In particular, QcScan is the first method shown to achieve consistency in the dense regime and further, we devise a combined procedure which is adaptively minimax optimal across sparse and dense regimes without the knowledge of the sparsity. Computationally, covariance scanning-based methods avoid costly computation of Lasso-type estimators and attain worst-case computation complexity that is linear in the dimension and sample size. Additionally, we consider the post-detection estimation of the differential parameter and the refinement of the change point estimator. Simulation studies support the theoretical findings and demonstrate the computational and statistical efficiency of the proposed covariance scanning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02552v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haeran Cho, Housen Li</dc:creator>
    </item>
    <item>
      <title>Weighted least-squares approximation with determinantal point processes and generalized volume sampling</title>
      <link>https://arxiv.org/abs/2312.14057</link>
      <description>arXiv:2312.14057v4 Announce Type: replace-cross 
Abstract: We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\boldsymbol{\varphi}$, using evaluations of the function at random points $x_1, \dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\boldsymbol{\varphi}(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation error in $L^2$ is almost surely bounded by the best approximation error measured in the $H$-norm. This includes the cases of functions from $L^\infty$ or reproducing kernel Hilbert spaces. Finally, we present an alternative strategy consisting in using independent repetitions of projection DPP (or volume sampling), yielding similar error bounds as with i.i.d. or volume sampling, but in practice with a much lower number of samples. Numerical experiments illustrate the performance of the different strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14057v4</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Nouy, Bertrand Michel</dc:creator>
    </item>
    <item>
      <title>Tensor Product Neural Networks for Functional ANOVA Model</title>
      <link>https://arxiv.org/abs/2502.15215</link>
      <description>arXiv:2502.15215v5 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as components), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel neural network which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. We call our proposed neural network ANOVA Tensor Product Neural Network (ANOVA-TPNN) since it is motivated by the tensor product basis expansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well. Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15215v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seokhun Park, Insung Kong, Yongchan Choi, Chanmoo Park, Yongdai Kim</dc:creator>
    </item>
  </channel>
</rss>
