<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jan 2025 02:33:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Borrowing Information from an Unidentifiable Model: Guaranteed Efficiency Gain with a Dichotomized Outcome in the External Data</title>
      <link>https://arxiv.org/abs/2501.06360</link>
      <description>arXiv:2501.06360v1 Announce Type: new 
Abstract: In the era of big data, the increasing availability of diverse data sources has driven interest in analytical approaches that integrate information across sources to enhance statistical accuracy, efficiency, and scientific insights. Many existing methods assume exchangeability among data sources and often implicitly require that sources measure identical covariates or outcomes, or that the error distribution is correctly specified-assumptions that may not hold in complex real-world scenarios. This paper explores the integration of data from sources with distinct outcome scales, focusing on leveraging external data to improve statistical efficiency. Specifically, we consider a scenario where the primary dataset includes a continuous outcome, and external data provides a dichotomized version of the same outcome. We propose two novel estimators: the first estimator remains asymptotically consistent even when the error distribution is potentially misspecified, while the second estimator guarantees an efficiency gain over weighted least squares estimation that uses the primary study data alone. Theoretical properties of these estimators are rigorously derived, and extensive simulation studies are conducted to highlight their robustness and efficiency gains across various scenarios. Finally, a real-world application using the NHANES dataset demonstrates the practical utility of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06360v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Wang, Yanyuan Ma, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Technical Note: Targeted Maximum Likelihood Estimator for an ATE Standardized for New Target Population</title>
      <link>https://arxiv.org/abs/2501.06381</link>
      <description>arXiv:2501.06381v1 Announce Type: new 
Abstract: In this technical note we present a targeted maximum likelihood estimator (TMLE) for a previously studied target parameter that aims to transport an average treatment effect (ATE) on a clinical outcome in a source population to what the ATE would have been in another target population. It is assumed that one only observes baseline covariates in the target population, while we assume that one can learn the conditional treatment effect on the outcome of interest in the source population. We also allow that one might observe only a subset of the covariates in the target population while all covariates are measured in the source population. We consider the case that the outcome is a clinical outcome at some future time point that is subject to missingness, or that our outcome of interest is a time to event that is subject to right-censoring. We derive the canonical gradients and present the corresponding TMLEs for these two cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06381v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark van der Laan, Susan Gruber</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Database</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v1 Announce Type: new 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU , available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example with a simulated real-world dataset is presented to explore three critical dimensions of MHSU detection based on the DDDM. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
    <item>
      <title>Data collaboration for causal inference from limited medical testing and medication data</title>
      <link>https://arxiv.org/abs/2501.06511</link>
      <description>arXiv:2501.06511v1 Announce Type: new 
Abstract: Observational studies enable causal inferences when randomized controlled trials (RCTs) are not feasible. However, integrating sensitive medical data across multiple institutions introduces significant privacy challenges. The data collaboration quasi-experiment (DC-QE) framework addresses these concerns by sharing "intermediate representations" -- dimensionality-reduced data derived from raw data -- instead of the raw data. While the DC-QE can estimate treatment effects, its application to medical data remains unexplored. This study applied the DC-QE framework to medical data from a single institution to simulate distributed data environments under independent and identically distributed (IID) and non-IID conditions. We propose a novel method for generating intermediate representations within the DC-QE framework. Experimental results demonstrated that DC-QE consistently outperformed individual analyses across various accuracy metrics, closely approximating the performance of centralized analysis. The proposed method further improved performance, particularly under non-IID conditions. These outcomes highlight the potential of the DC-QE framework as a robust approach for privacy-preserving causal inferences in healthcare. Broader adoption of this framework and increased use of intermediate representations could grant researchers access to larger, more diverse datasets while safeguarding patient confidentiality. This approach may ultimately aid in identifying previously unrecognized causal relationships, support drug repurposing efforts, and enhance therapeutic interventions for rare diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06511v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoru Nakayama, Yuji Kawamata, Akihiro Toyoda, Akira Imakura, Rina Kagawa, Masaru Sanuki, Ryoya Tsunoda, Kunihiro Yamagata, Tetsuya Sakurai, Yukihiko Okada</dc:creator>
    </item>
    <item>
      <title>Estimation and inference of high-dimensional partially linear regression models with latent factors</title>
      <link>https://arxiv.org/abs/2501.06529</link>
      <description>arXiv:2501.06529v1 Announce Type: new 
Abstract: In this paper, we introduce a novel high-dimensional Factor-Adjusted sparse Partially Linear regression Model (FAPLM), to integrate the linear effects of high-dimensional latent factors with the nonparametric effects of low-dimensional covariates. The proposed FAPLM combines the interpretability of linear models, the flexibility of nonparametric models, with the ability to effectively capture the dependencies among highdimensional covariates. We develop a penalized estimation approach for the model by leveraging B-spline approximations and factor analysis techniques. Theoretical results establish error bounds for the estimators, aligning with the minimax rates of standard Lasso problems. To assess the significance of the linear component, we introduce a factor-adjusted projection debiased procedure and employ the Gaussian multiplier bootstrap method to derive critical values. Theoretical guarantees are provided under regularity conditions. Comprehensive numerical experiments validate the finite-sample performance of the proposed method. Its successful application to a birth weight dataset, the motivating example for this study, highlights both its effectiveness and practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06529v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Shi, Meiling Hao, Yanlin Tang, Xu Guo</dc:creator>
    </item>
    <item>
      <title>Design and analysis for constrained order-of-addition experiments</title>
      <link>https://arxiv.org/abs/2501.06559</link>
      <description>arXiv:2501.06559v1 Announce Type: new 
Abstract: In an order-of-addition (OofA) experiment, the sequence of m different components can significantly impact the experiment's response. In many OofA experiments, the components are subject to constraints, where certain orders are impossible. For example, in survey design and job scheduling, the components are often arranged into groups, and these groups of components must be placed in a fixed order. If two components are in different groups, their pairwise order is determined by the fixed order of their groups. Design and analysis are needed for these pairwise-group constrained OofA experiments. A new model is proposed to accommodate pairwise-group constraints. This paper also introduces a model for mixed-pairwise constrained OofA experiments, which allows one pair of components within each group to have a pre-determined pairwise order. It is proven that the full design, which uses all feasible orders exactly once, is D- and G-optimal under the proposed models. Systematic construction methods are used to find optimal fractional designs for pairwise-group and mixed-pairwise constrained OofA experiments. The proposed methods efficiently assess the impact of question order in a survey dataset, where participants answered generalized intelligence questions in a randomly assigned order under mixed-pairwise constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06559v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianbin Chen, Dennis K. J. Lin, Nicholas Rios, Xueru Zhang</dc:creator>
    </item>
    <item>
      <title>Correcting Annotator Bias in Training Data: Population-Aligned Instance Replication (PAIR)</title>
      <link>https://arxiv.org/abs/2501.06826</link>
      <description>arXiv:2501.06826v1 Announce Type: new 
Abstract: Models trained on crowdsourced labels may not reflect broader population views when annotator pools are not representative. Since collecting representative labels is challenging, we propose Population-Aligned Instance Replication (PAIR), a method to address this bias through statistical adjustment. Using a simulation study of hate speech and offensive language detection, we create two types of annotators with different labeling tendencies and generate datasets with varying proportions of the types. Models trained on unbalanced annotator pools show poor calibration compared to those trained on representative data. However, PAIR, which duplicates labels from underrepresented annotator groups to match population proportions, significantly reduces bias without requiring new data collection. These results suggest statistical techniques from survey research can help align model training with target populations even when representative annotator pools are unavailable. We conclude with three practical recommendations for improving training data quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06826v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephanie Eckman, Bolei Ma, Christoph Kern, Rob Chew, Barbara Plank, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>REML Implementations of Kernel-based Multi-environment Genomic Prediction Models</title>
      <link>https://arxiv.org/abs/2501.06844</link>
      <description>arXiv:2501.06844v1 Announce Type: new 
Abstract: High-throughput pheno-, geno-, and envirotyping allows routine characterization of plant varieties and the trials they are evaluated in. These datasets can be integrated into statistical models for genomic prediction in several ways. One approach is to create linear or non-linear kernels which are subsequently used in reproducing kernel hilbert spaces (RKHS) regression. Software packages implementing a Bayesian approach are typically used for these RKHS models. However, they often lack some of the flexibility offered by dedicated linear mixed model software such as ASReml-R. Furthermore, a Bayesian approach is often computationally more demanding than a frequentist model. Here we show how frequentist RKHS models can be implemented in ASReml-R and extend these models to allow for heterogeneous (i.e., trial-specific) genetic variances. We also show how an alternative to the typically Bayesian kernel averaging approach can be implemented by treating the bandwidth associated with the non-linear kernel as a parameter to be estimated using restricted maximum likelihood. We show that these REML implementations with homo- or heterogeneous variances perform similarly or better than the Bayesian models. We also show that the REML implementation comes with a significant increase in computational efficiency, being up to 12 times faster than the Bayesian models while using less memory. Finally, we discuss the significant flexibility provided by this approach and the options regarding further customization of variance models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06844v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Killian A. C. Melsen, Salvador Gezan, Fred van Eeuwijk, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Fisher's Randomization Test for Causality with General Types of Treatments</title>
      <link>https://arxiv.org/abs/2501.06864</link>
      <description>arXiv:2501.06864v1 Announce Type: new 
Abstract: Researchers has long been focusing on causal inference with binary or categorical treatments, where causal estimands are well understood and inference tools are rich. However, causal problems involving continuous treatments are common in practice, yet a formal framework is scarce in the literature. We extend classic Fisher's randomization test to address an initial question: does a treatment have effect on the outcome of interest conditional on a set of covariates. Our theory starts from randomized experiments and generalizes to observational studies. Inference tools are developed to establish causal relationships and to verify underlying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06864v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Zhong, Shan Huang, Donald B. Rubin</dc:creator>
    </item>
    <item>
      <title>Driver Age and Its Effect on Key Driving Metrics: Insights from Dynamic Vehicle Data</title>
      <link>https://arxiv.org/abs/2501.06918</link>
      <description>arXiv:2501.06918v1 Announce Type: new 
Abstract: By 2030, the senior population aged 65 and older is expected to increase by over 50%, significantly raising the number of older drivers on the road. Drivers over 70 face higher crash death rates compared to those in their forties and fifties, underscoring the importance of developing more effective safety interventions for this demographic. Although the impact of aging on driving behavior has been studied, there is limited research on how these behaviors translate into real-world driving scenarios. This study addresses this need by leveraging Naturalistic Driving Data (NDD) to analyze driving performance measures - specifically, speed limit adherence on interstates and deceleration at stop intersections, both of which may be influenced by age-related declines. Using NDD, we developed Cumulative Distribution Functions (CDFs) to establish benchmarks for key driving behaviors among senior and young drivers. Our analysis, which included anomaly detection, benchmark comparisons, and accuracy evaluations, revealed significant differences in driving patterns primarily related to speed limit adherence at 75mph. While our approach shows promising potential for enhancing Advanced Driver Assistance Systems (ADAS) by providing tailored interventions based on age-specific adherence to speed limit driving patterns, we recognize the need for additional data to refine and validate metrics for other driving behaviors. By establishing precise benchmarks for various driving performance metrics, ADAS can effectively identify anomalies, such as abrupt deceleration, which may indicate impaired driving or other safety concerns. This study lays a strong foundation for future research aimed at improving safety interventions through detailed driving behavior analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06918v1</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aparna Joshi, Kojo Adugyamfi, Jennifer Merickel, Pujitha Gunaratne, Anuj Sharma</dc:creator>
    </item>
    <item>
      <title>The ladder of abstraction in statistical graphics</title>
      <link>https://arxiv.org/abs/2501.06920</link>
      <description>arXiv:2501.06920v2 Announce Type: new 
Abstract: Graphical forms such as scatterplots, line plots, and histograms are so familiar that it can be easy to forget how abstract they are. As a result, we often produce graphs that are difficult to follow. We propose a strategy for graphical communication by climbing a ladder of abstraction (a term from linguistics that we borrow from Hayakawa, 1939), starting with simple plots of special cases and then at each step embedding a graph into a more general framework. We demonstrate with two examples, first graphing a set of equations related to a modeled trajectory and then graphing data from an analysis of income and voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06920v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Moment-assisted subsampling method for Cox proportional hazards model with large-scale data</title>
      <link>https://arxiv.org/abs/2501.06924</link>
      <description>arXiv:2501.06924v1 Announce Type: new 
Abstract: The Cox proportional hazards model is widely used in survival analysis to model time-to-event data. However, it faces significant computational challenges in the era of large-scale data, particularly when dealing with time-dependent covariates. This paper proposes a moment-assisted subsampling method that is both statistically and computationally efficient for inference under the Cox model. This efficiency is achieved by integrating the computationally efficient uniform subsampling estimator and whole data sample moments that are easy to compute even for large datasets. The resulting estimator is asymptotically normal with a smaller variance than the uniform subsampling estimator. Additionally, we derive the optimal sample moment for the Cox model that minimizes the asymptotic variance in Loewner order. With the optimal moment, the proposed estimator can achieve the same estimation efficiency as the whole data-based partial likelihood estimator while maintaining the computational advantages of subsampling. Simulation studies and real data analyses demonstrate the promising finite sample performance of the proposed estimator in terms of both estimation and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06924v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaomiao Su, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference on Causal Derivative Effects for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2501.06969</link>
      <description>arXiv:2501.06969v1 Announce Type: new 
Abstract: Statistical methods for causal inference with continuous treatments mainly focus on estimating the mean potential outcome function, commonly known as the dose-response curve. However, it is often not the dose-response curve but its derivative function that signals the treatment effect. In this paper, we investigate nonparametric inference on the derivative of the dose-response curve with and without the positivity condition. Under the positivity and other regularity conditions, we propose a doubly robust (DR) inference method for estimating the derivative of the dose-response curve using kernel smoothing. When the positivity condition is violated, we demonstrate the inconsistency of conventional inverse probability weighting (IPW) and DR estimators, and introduce novel bias-corrected IPW and DR estimators. In all settings, our DR estimator achieves asymptotic normality at the standard nonparametric rate of convergence. Additionally, our approach reveals an interesting connection to nonparametric support and level set estimation problems. Finally, we demonstrate the applicability of our proposed estimators through simulations and a case study of evaluating a job training program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06969v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Monotone Curve Estimation via Convex Duality</title>
      <link>https://arxiv.org/abs/2501.06975</link>
      <description>arXiv:2501.06975v2 Announce Type: new 
Abstract: A principal curve serves as a powerful tool for uncovering underlying structures of data through 1-dimensional smooth and continuous representations. On the basis of optimal transport theories, this paper introduces a novel principal curve framework constrained by monotonicity with rigorous theoretical justifications. We establish statistical guarantees for our monotone curve estimate, including expected empirical and generalized mean squared errors, while proving the existence of such estimates. These statistical foundations justify adopting the popular early stopping procedure in machine learning to implement our numeric algorithm with neural networks. Comprehensive simulation studies reveal that the proposed monotone curve estimate outperforms competing methods in terms of accuracy when the data exhibits a monotonic structure. Moreover, through two real-world applications on future prices of copper, gold, and silver, and avocado prices and sales volume, we underline the robustness of our curve estimate against variable transformation, further confirming its effective applicability for noisy and complex data sets. We believe that this monotone curve-fitting framework offers significant potential for numerous applications where monotonic relationships are intrinsic or need to be imposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06975v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongseok Lim, Kyeongsik Nam, Jinwon Sohn</dc:creator>
    </item>
    <item>
      <title>A Weighted Similarity Metric for Community Detection in Sparse Data</title>
      <link>https://arxiv.org/abs/2501.07025</link>
      <description>arXiv:2501.07025v1 Announce Type: new 
Abstract: Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07025v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yong Zhang, Eric Herrison Gyamfi</dc:creator>
    </item>
    <item>
      <title>A Beta Cauchy-Cauchy (BECCA) shrinkage prior for Bayesian variable selection</title>
      <link>https://arxiv.org/abs/2501.07061</link>
      <description>arXiv:2501.07061v1 Announce Type: new 
Abstract: This paper introduces a novel Bayesian approach for variable selection in high-dimensional and potentially sparse regression settings. Our method replaces the indicator variables in the traditional spike and slab prior with continuous, Beta-distributed random variables and places half Cauchy priors over the parameters of the Beta distribution, which significantly improves the predictive and inferential performance of the technique. Similar to shrinkage methods, our continuous parameterization of the spike and slab prior enables us explore the posterior distributions of interest using fast gradient-based methods, such as Hamiltonian Monte Carlo (HMC), while at the same time explicitly allowing for variable selection in a principled framework. We study the frequentist properties of our model via simulation and show that our technique outperforms the latest Bayesian variable selection methods in both linear and logistic regression. The efficacy, applicability and performance of our approach, are further underscored through its implementation on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07061v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linduni M. Rodrigo, Robert Kohn, Hadi M. Afshar, Sally Cripps</dc:creator>
    </item>
    <item>
      <title>Functional Linear Cox Regression Model with Frailty</title>
      <link>https://arxiv.org/abs/2501.07450</link>
      <description>arXiv:2501.07450v1 Announce Type: new 
Abstract: This paper presents a functional linear Cox regression model with frailty to tackle unobserved heterogeneity in survival data with functional covariates. While traditional Cox models are common, they struggle to incorporate frailty effects that represent individual differences not captured by observed covariates. Our model combines scalar and functional covariates with a frailty term to address these unmeasured influences, creating a robust framework for high-dimensional survival analysis. We estimate parameters using functional principal component analysis and apply penalized partial likelihood for the frailty structure. A simulation study shows that our model outperforms traditional approaches in estimation accuracy and predictive capacity, especially with high frailty. We also analyze data from the National Health and Nutrition Examination Survey, highlighting significant links between physical activity and mortality in frail subpopulations. Our findings demonstrate the model's effectiveness in managing complex survival data, with potential applications in biomedical research related to unobserved heterogeneity. The method is available as an R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07450v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deniz Inan, Ufuk Beyaztas, Carmen D. Tekwe, Xiwei Chen, Roger S. Zoh</dc:creator>
    </item>
    <item>
      <title>Cluster Catch Digraphs with the Nearest Neighbor Distance</title>
      <link>https://arxiv.org/abs/2501.06268</link>
      <description>arXiv:2501.06268v1 Announce Type: cross 
Abstract: We introduce a new method for clustering based on Cluster Catch Digraphs (CCDs). The new method addresses the limitations of RK-CCDs by employing a new variant of spatial randomness test that employs the nearest neighbor distance (NND) instead of the Ripley's K function used by RK-CCDs. We conduct a comprehensive Monte Carlo analysis to assess the performance of our method, considering factors such as dimensionality, data set size, number of clusters, cluster volumes, and inter-cluster distance. Our method is particularly effective for high-dimensional data sets, comparable to or outperforming KS-CCDs and RK-CCDs that rely on a KS-type statistic or the Ripley's K function. We also evaluate our methods using real and complex data sets, comparing them to well-known clustering methods. Again, our methods exhibit competitive performance, producing high-quality clusters with desirable properties.
  Keywords: Graph-based clustering, Cluster catch digraphs, High-dimensional data, The nearest neighbor distance, Spatial randomness test</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06268v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Shi, Nedret Billor, Elvan Ceyhan</dc:creator>
    </item>
    <item>
      <title>Trends in urban flows: A transfer entropy approach</title>
      <link>https://arxiv.org/abs/2501.06316</link>
      <description>arXiv:2501.06316v1 Announce Type: cross 
Abstract: The accurate estimation of human activity in cities is one of the first steps towards understanding the structure of the urban environment. Human activities are highly granular and dynamic in spatial and temporal dimensions. Estimating confidence is crucial for decision-making in numerous applications such as urban management, retail, transport planning and emergency management. Detecting general trends in the flow of people between spatial locations is neither obvious nor easy due to the high cost of capturing these movements without compromising the privacy of those involved. This research intends to address this problem by examining the movement of people in a SmartStreetSensors network at a fine spatial and temporal resolution using a Transfer Entropy approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06316v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Murcio, Balamurugan Soundararaj</dc:creator>
    </item>
    <item>
      <title>Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing</title>
      <link>https://arxiv.org/abs/2501.06366</link>
      <description>arXiv:2501.06366v2 Announce Type: cross 
Abstract: When applied in healthcare, reinforcement learning (RL) seeks to dynamically match the right interventions to subjects to maximize population benefit. However, the learned policy may disproportionately allocate efficacious actions to one subpopulation, creating or exacerbating disparities in other socioeconomically-disadvantaged subgroups. These biases tend to occur in multi-stage decision making and can be self-perpetuating, which if unaccounted for could cause serious unintended consequences that limit access to care or treatment benefit. Counterfactual fairness (CF) offers a promising statistical tool grounded in causal inference to formulate and study fairness. In this paper, we propose a general framework for fair sequential decision making. We theoretically characterize the optimal CF policy and prove its stationarity, which greatly simplifies the search for optimal CF policies by leveraging existing RL algorithms. The theory also motivates a sequential data preprocessing algorithm to achieve CF decision making under an additive noise assumption. We prove and then validate our policy learning approach in controlling unfairness and attaining optimal value through simulations. Analysis of a digital health dataset designed to reduce opioid misuse shows that our proposal greatly enhances fair access to counseling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06366v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jitao Wang, Chengchun Shi, John D. Piette, Joshua R. Loftus, Donglin Zeng, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>CeViT: Copula-Enhanced Vision Transformer in multi-task learning and bi-group image covariates with an application to myopia screening</title>
      <link>https://arxiv.org/abs/2501.06540</link>
      <description>arXiv:2501.06540v1 Announce Type: cross 
Abstract: We aim to assist image-based myopia screening by resolving two longstanding problems, "how to integrate the information of ocular images of a pair of eyes" and "how to incorporate the inherent dependence among high-myopia status and axial length for both eyes." The classification-regression task is modeled as a novel 4-dimensional muti-response regression, where discrete responses are allowed, that relates to two dependent 3rd-order tensors (3D ultrawide-field fundus images). We present a Vision Transformer-based bi-channel architecture, named CeViT, where the common features of a pair of eyes are extracted via a shared Transformer encoder, and the interocular asymmetries are modeled through separated multilayer perceptron heads. Statistically, we model the conditional dependence among mixture of discrete-continuous responses given the image covariates by a so-called copula loss. We establish a new theoretical framework regarding fine-tuning on CeViT based on latent representations, allowing the black-box fine-tuning procedure interpretable and guaranteeing higher relative efficiency of fine-tuning weight estimation in the asymptotic setting. We apply CeViT to an annotated ultrawide-field fundus image dataset collected by Shanghai Eye \&amp; ENT Hospital, demonstrating that CeViT enhances the baseline model in both accuracy of classifying high-myopia and prediction of AL on both eyes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06540v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Yang Li, Jinfeng Xu, Xiang Fu, Yunhao Liu, Qiuyi Huang, Danjuan Yang, Meiyan Li, Aiyi Liu, Alan H. Welsh, Xingtao Zhou, Bo Fu, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>Singularities in Bayesian Inference: Crucial or Overstated?</title>
      <link>https://arxiv.org/abs/2501.06618</link>
      <description>arXiv:2501.06618v1 Announce Type: cross 
Abstract: Over the past two decades, shrinkage priors have become increasingly popular, and many proposals can be found in the literature. These priors aim to shrink small effects to zero while maintaining true large effects. Horseshoe-type priors have been particularly successful in various applications, mainly due to their computational advantages. However, there is no clear guidance on choosing the most appropriate prior for a specific setting. In this work, we propose a framework that encompasses a large class of shrinkage distributions, including priors with and without a singularity at zero. By reframing such priors in the context of reliability theory and wealth distributions, we provide insights into the prior parameters and shrinkage properties. The paper's key contributions are based on studying the folded version of such distributions, which we refer to as the Gambel distribution. The Gambel can be rewritten as the ratio between a Generalised Gamma and a Generalised Beta of the second kind. This representation allows us to gain insights into the behaviours near the origin and along the tails, compute measures to compare their distributional properties, derive consistency results, devise MCMC schemes for posterior inference and ultimately provide guidance on the choice of the hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06618v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria De Iorio, Andreas Heinecke, Beatrice Franzolini, Rafael Cabral</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v1 Announce Type: cross 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under a fixed normal chart centered at the true parameter, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings-including spheres, the Stiefel manifold, fixed-rank matrices manifolds, and rank-one tensor manifolds-and, for Euclidean submanifolds, introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Learning dynamical systems with hit-and-run random feature maps</title>
      <link>https://arxiv.org/abs/2501.06661</link>
      <description>arXiv:2501.06661v1 Announce Type: cross 
Abstract: We show how random feature maps can be used to forecast dynamical systems with excellent forecasting skill. We consider the tanh activation function and judiciously choose the internal weights in a data-driven manner such that the resulting features explore the nonlinear, non-saturated regions of the activation function. We introduce skip connections and construct a deep variant of random feature maps by combining several units. To mitigate the curse of dimensionality, we introduce localization where we learn local maps, employing conditional independence. Our modified random feature maps provide excellent forecasting skill for both single trajectory forecasts as well as long-time estimates of statistical properties, for a range of chaotic dynamical systems with dimensions up to 512. In contrast to other methods such as reservoir computers which require extensive hyperparameter tuning, we effectively need to tune only a single hyperparameter, and are able to achieve state-of-the-art forecast skill with much smaller networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06661v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinak Mandal, Georg A. Gottwald</dc:creator>
    </item>
    <item>
      <title>Variable Selection Methods for Multivariate, Functional, and Complex Biomedical Data in the AI Age</title>
      <link>https://arxiv.org/abs/2501.06868</link>
      <description>arXiv:2501.06868v1 Announce Type: cross 
Abstract: Many problems within personalized medicine and digital health rely on the analysis of continuous-time functional biomarkers and other complex data structures emerging from high-resolution patient monitoring. In this context, this work proposes new optimization-based variable selection methods for multivariate, functional, and even more general outcomes in metrics spaces based on best-subset selection. Our framework applies to several types of regression models, including linear, quantile, or non parametric additive models, and to a broad range of random responses, such as univariate, multivariate Euclidean data, functional, and even random graphs. Our analysis demonstrates that our proposed methodology outperforms state-of-the-art methods in accuracy and, especially, in speed-achieving several orders of magnitude improvement over competitors across various type of statistical responses as the case of mathematical functions. While our framework is general and is not designed for a specific regression and scientific problem, the article is self-contained and focuses on biomedical applications. In the clinical areas, serves as a valuable resource for professionals in biostatistics, statistics, and artificial intelligence interested in variable selection problem in this new technological AI-era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Causal Claims in Economics</title>
      <link>https://arxiv.org/abs/2501.06873</link>
      <description>arXiv:2501.06873v1 Announce Type: cross 
Abstract: We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a custom language model to construct knowledge graphs that map economic concepts and their relationships. We distinguish between general claims and those documented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document a substantial rise in the share of causal claims-from roughly 4% in 1990 to nearly 28% in 2020-reflecting the growing influence of the "credibility revolution." We find that causal narrative complexity (e.g., the depth of causal chains) strongly predicts both publication in top-5 journals and higher citation counts, whereas non-causal complexity tends to be uncorrelated or negatively associated with these outcomes. Novelty is also pivotal for top-5 publication, but only when grounded in credible causal methods: introducing genuinely new causal edges or paths markedly increases both the likelihood of acceptance at leading outlets and long-run citations, while non-causal novelty exhibits weak or even negative effects. Papers engaging with central, widely recognized concepts tend to attract more citations, highlighting a divergence between factors driving publication success and long-term academic impact. Finally, bridging underexplored concept pairs is rewarded primarily when grounded in causal methods, yet such gap filling exhibits no consistent link with future citations. Overall, our findings suggest that methodological rigor and causal innovation are key drivers of academic recognition, but sustained impact may require balancing novel contributions with conceptual integration into established economic discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06873v1</guid>
      <category>econ.GN</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashant Garg, Thiemo Fetzer</dc:creator>
    </item>
    <item>
      <title>Automatic Double Reinforcement Learning in Semiparametric Markov Decision Processes with Applications to Long-Term Causal Inference</title>
      <link>https://arxiv.org/abs/2501.06926</link>
      <description>arXiv:2501.06926v1 Announce Type: cross 
Abstract: Double reinforcement learning (DRL) enables statistically efficient inference on the value of a policy in a nonparametric Markov Decision Process (MDP) given trajectories generated by another policy. However, this approach necessarily requires stringent overlap between the state distributions, which is often violated in practice. To relax this requirement and extend DRL, we study efficient inference on linear functionals of the $Q$-function (of which policy value is a special case) in infinite-horizon, time-invariant MDPs under semiparametric restrictions on the $Q$-function. These restrictions can reduce the overlap requirement and lower the efficiency bound, yielding more precise estimates. As an important example, we study the evaluation of long-term value under domain adaptation, given a few short trajectories from the new domain and restrictions on the difference between the domains. This can be used for long-term causal inference. Our method combines flexible estimates of the $Q$-function and the Riesz representer of the functional of interest (e.g., the stationary state density ratio for policy value) and is automatic in that we do not need to know the form of the latter - only the functional we care about. To address potential model misspecification bias, we extend the adaptive debiased machine learning (ADML) framework of \citet{van2023adaptive} to construct nonparametrically valid and superefficient estimators that adapt to the functional form of the $Q$-function. As a special case, we propose a novel adaptive debiased plug-in estimator that uses isotonic-calibrated fitted $Q$-iteration - a new calibration algorithm for MDPs - to circumvent the computational challenges of estimating debiasing nuisances from min-max objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06926v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, David Hubbard, Allen Tran, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Quality Control of Lifetime Drift in Discrete Electrical Parameters in Semiconductor Devices via Transition Modeling</title>
      <link>https://arxiv.org/abs/2501.07115</link>
      <description>arXiv:2501.07115v1 Announce Type: cross 
Abstract: Semiconductors are widely used in various applications and critical infrastructures. These devices have specified lifetimes and quality targets that manufacturers must achieve. Lifetime estimation is conducted through accelerated stress tests. Electrical parameters are measured at multiple times during a stress test procedure. The change in these Electrical parameters is called lifetime drift. Data from these tests can be used to develop a statistical model predicting the lifetime behavior of the electrical parameters in real devices. These models can provide early warnings in production processes, identify critical parameter drift, and detect outliers. While models for continuous electrical parameters exists, there may be bias when estimating the lifetime of discrete parameters. To address this, we propose a semi-parametric model for degradation trajectories based on longitudinal stress test data. This model optimizes guard bands, or quality guaranteeing tighter limits, for discrete electrical parameters at production testing. It is scalable, data-driven, and explainable, offering improvements over existing methods for continuous underlying data, such as faster calculations, arbitrary non-parametric conditional distribution modeling, and a natural extension of optimization algorithms to the discrete case using Markov transition matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07115v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.microrel.2024.115555.</arxiv:DOI>
      <arxiv:journal_reference>Microelectronics Reliability, Volume 164, 2025, 115555, ISSN 0026-2714</arxiv:journal_reference>
      <dc:creator>Lukas Sommeregger, J\"urgen Pilz</dc:creator>
    </item>
    <item>
      <title>Subtype-Aware Registration of Longitudinal Electronic Health Records</title>
      <link>https://arxiv.org/abs/2501.07336</link>
      <description>arXiv:2501.07336v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) contain extensive patient information that can inform downstream clinical decisions, such as mortality prediction, disease phenotyping, and disease onset prediction. A key challenge in EHR data analysis is the temporal gap between when a condition is first recorded and its actual onset time. Such timeline misalignment can lead to artificially distinct biomarker trends among patients with similar disease progression, undermining the reliability of downstream analysis and complicating tasks like disease subtyping. To address this challenge, we provide a subtype-aware timeline registration method that leverages data projection and discrete optimization to simultaneously correct timeline misalignment and improve disease subtyping. Through simulation and real-world data analyses, we demonstrate that the proposed method effectively aligns distorted observed records with the true disease progression patterns, enhancing subtyping clarity and improving performance in downstream clinical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07336v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xin Gai, Shiyi Jiang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>disco: Distributional Synthetic Controls</title>
      <link>https://arxiv.org/abs/2501.07550</link>
      <description>arXiv:2501.07550v1 Announce Type: cross 
Abstract: The method of synthetic controls is widely used for evaluating causal effects of policy changes in settings with observational data. Often, researchers aim to estimate the causal impact of policy interventions on a treated unit at an aggregate level while also possessing data at a finer granularity. In this article, we introduce the new disco command, which implements the Distributional Synthetic Controls method introduced in Gunsilius (2023). This command allows researchers to construct entire synthetic distributions for the treated unit based on an optimally weighted average of the distributions of the control units. Several aggregation schemes are provided to facilitate clear reporting of the distributional effects of the treatment. The package offers both quantile-based and CDF-based approaches, comprehensive inference procedures via bootstrap and permutation methods, and visualization capabilities. We empirically illustrate the use of the package by replicating the results in Van Dijcke et al. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07550v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gunsilius, David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees</title>
      <link>https://arxiv.org/abs/2203.04341</link>
      <description>arXiv:2203.04341v3 Announce Type: replace 
Abstract: A new Bayesian modelling framework is introduced for piece-wise homogeneous variable-memory Markov chains, along with a collection of effective algorithmic tools for change-point detection and segmentation of discrete time series. Building on the recently introduced Bayesian Context Trees (BCT) framework, the distributions of different segments in a discrete time series are described as variable-memory Markov chains. Inference for the presence and location of change-points is then performed via Markov chain Monte Carlo sampling. The key observation that facilitates effective sampling is that, using one of the BCT algorithms, the prior predictive likelihood of the data can be computed exactly, integrating out all the models and parameters in each segment. This makes it possible to sample directly from the posterior distribution of the number and location of the change-points, leading to accurate estimates and providing a natural quantitative measure of uncertainty in the results. Estimates of the actual model in each segment can also be obtained, at essentially no additional computational cost. Results on both simulated and real-world data indicate that the proposed methodology performs better than or as well as state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.04341v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentinian Lungu, Ioannis Papageorgiou, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>Posterior covariance information criterion for general loss functions</title>
      <link>https://arxiv.org/abs/2206.05887</link>
      <description>arXiv:2206.05887v2 Announce Type: replace 
Abstract: We propose a novel computationally low-cost method for estimating a general predictive measure of generalised Bayesian inference. The proposed method utilises posterior covariance and provides estimators of the Gibbs and the plugin generalisation errors. We present theoretical guarantees of the proposed method, clarifying the connection to the Bayesian sensitivity analysis and the infinitesimal jackknife approximation of Bayesian leave-one-out cross validation. We illustrate several applications of our methods, including applications to differential privacy-preserving learning, the Bayesian hierarchical modeling, the Bayesian regression in the presence of influential observations, and the bias reduction of the widely-applicable information criterion. The applicability in high dimensions is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05887v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukito Iba, Keisuke Yano</dc:creator>
    </item>
    <item>
      <title>The Target Study: A Conceptual Model and Framework for Measuring Disparity</title>
      <link>https://arxiv.org/abs/2207.00530</link>
      <description>arXiv:2207.00530v4 Announce Type: replace 
Abstract: We present a conceptual model to measure disparity--the target study--where social groups may be similarly situated (i.e., balanced) on allowable covariates. Our model, based on a sampling design, does not intervene to assign social group membership or alter allowable covariates. To address non-random sample selection, we extend our model to generalize or transport disparity or to assess disparity after an intervention on eligibility-related variables that eliminates forms of collider-stratification. To avoid bias from differential timing of enrollment, we aggregate time-specific study results by balancing calendar time of enrollment across social groups. To provide a framework for emulating our model, we discuss study designs, data structures, and G-computation and weighting estimators. We compare our sampling-based model to prominent decomposition-based models used in healthcare and algorithmic fairness. We provide R code for all estimators and apply our methods to measure health system disparities in hypertension control using electronic medical records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00530v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John W. Jackson, Yea-Jen Hsu, Raquel C. Greer, Romsai T. Boonyasai, Chanelle J. Howe</dc:creator>
    </item>
    <item>
      <title>Neural Bayes Estimators for Irregular Spatial Data using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2310.02600</link>
      <description>arXiv:2310.02600v3 Announce Type: replace 
Abstract: Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. Although they are appealing to use with spatial models, where estimation is often a computational bottleneck, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be re-trained for new data sets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks to tackle the important problem of parameter point estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, our architecture leads to substantial computational benefits, since the estimator can be used with any configuration or number of locations and independent replicates, thus amortising the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology on a data set of global sea-surface temperature, where we estimate the parameters of a Gaussian process model in 2161 spatial regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02600v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Jordan Richards, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Two-phase rejective sampling and its asymptotic properties</title>
      <link>https://arxiv.org/abs/2403.01477</link>
      <description>arXiv:2403.01477v3 Announce Type: replace 
Abstract: Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling. We start with the theory for the population mean and then extend the theory to parameters defined by general estimating equations. Our asymptotic results for TPRS immediately cover the existing single-phase rejective sampling, under which the asymptotic theory has not been fully established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01477v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of the Royal Statistical Society: Series B, 2025</arxiv:journal_reference>
      <dc:creator>Shu Yang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>A comparative study of augmented inverse propensity weighted estimators using outcome-oriented covariate selection via penalization with outcome-adaptive lasso</title>
      <link>https://arxiv.org/abs/2405.11522</link>
      <description>arXiv:2405.11522v2 Announce Type: replace 
Abstract: When estimating causal effects from observational data with numerous covariates, employing penalized covariate selection can improve the estimation efficiency. Outcome-oriented covariate selection, which involves selecting covariates related to the outcome, can enhance efficiency, even for propensity score (PS) methods. For outcome-oriented covariate selection in PS models, outcome-adaptive lasso (OAL) can be used for penalization with the oracle property. The performance of inverse propensity weighted (IPW) estimators using the OAL was shown to be superior to that of the IPW estimators using other covariate selection methods for parametric models. However, the augmented IPW (AIPW) estimator is typically employed as a doubly robust estimator for the average treatment effect, which requires both PS and outcome models. Despite this, which covariate selection method for outcome models should be combined with the OAL to form the AIPW estimator remains unclear. We evaluated the performance of the AIPW estimators using the OAL for PS models and various outcome-oriented covariate selection via penalization for outcome models. We conducted numerical experiments to evaluate the performance of AIPW estimators using various covariate selection via penalization. The performance of the AIPW estimators using outcome-oriented covariate selection via penalization with the oracle property for both PS and outcome models was superior to that of the other estimators and similar to that of the AIPW estimator, which relies on true confounders and outcome predictors. In contrast, the bias of the AIPW estimators not relying on the oracle property was high. In a clinical trial dataset analysis, the AIPW estimators using outcome-oriented covariate selection via penalization with and without the oracle property showed similar estimates and standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11522v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Hongo, Shuji Ando, Jun Tsuchida, Takashi Sozu</dc:creator>
    </item>
    <item>
      <title>Approximations to worst-case data dropping: unmasking failure modes</title>
      <link>https://arxiv.org/abs/2408.09008</link>
      <description>arXiv:2408.09008v4 Announce Type: replace 
Abstract: A data analyst might worry about generalization if dropping a very small fraction of data points from a study could change its substantive conclusions. Finding the worst-case data subset to drop poses a combinatorial optimization problem. To overcome this intractability, recent works propose using additive approximations, which treat the contribution of a collection of data points as the sum of their individual contributions, and greedy approximations, which iteratively select the point with the highest impact to drop and re-run the data analysis without that point [Broderick et al., 2020, Kuschnig et al., 2021]. We identify that, even in a setting as simple as OLS linear regression, many of these approximations can break down in realistic data arrangements. Several of our examples reflect masking, where one outlier may hide or conceal the effect of another outlier. Based on the failures we identify, we provide recommendations for users and suggest directions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09008v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Y. Huang, David R. Burt, Tin D. Nguyen, Yunyi Shen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Batch Predictive Inference</title>
      <link>https://arxiv.org/abs/2409.13990</link>
      <description>arXiv:2409.13990v3 Announce Type: replace 
Abstract: Constructing prediction sets with coverage guarantees for unobserved outcomes is a core problem in modern statistics. Methods for predictive inference have been developed for a wide range of settings, but usually only consider test data points one at a time. Here we study the problem of distribution-free predictive inference for a batch of multiple test points, aiming to construct prediction sets for functions -- such as the mean or median -- of any number of unobserved test datapoints. This setting includes constructing simultaneous prediction sets with a high probability of coverage, and selecting datapoints satisfying a specified condition while controlling the number of false claims.
  For the general task of predictive inference on a function of a batch of test points, we introduce a methodology called batch predictive inference (batch PI), and provide a distribution-free coverage guarantee under exchangeability of the calibration and test data. Batch PI requires the quantiles of a rank ordering function defined on certain subsets of ranks. While computing these quantiles is NP-hard in general, we show that it can be done efficiently in many cases of interest, most notably for batch score functions with a compositional structure -- which includes examples of interest such as the mean -- via a dynamic programming algorithm that we develop. Batch PI has advantages over naive approaches (such as partitioning the calibration data or directly extending conformal prediction) in many settings, as it can deliver informative prediction sets even using small calibration sample sizes. We illustrate that our procedures provide informative inference across the use cases mentioned above, through experiments on both simulated data and a drug-target interaction dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13990v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Double-Estimation-Friendly Inference for High-Dimensional Measurement Error Models with Non-Sparse Adaptability</title>
      <link>https://arxiv.org/abs/2409.16463</link>
      <description>arXiv:2409.16463v3 Announce Type: replace 
Abstract: In this paper, we introduce an innovative testing procedure for assessing individual hypotheses in high-dimensional linear regression models with measurement errors. This method remains robust even when either the X-model or Y-model is misspecified. We develop a double robust score function that maintains a zero expectation if one of the models is incorrect, and we construct a corresponding score test. We first show the asymptotic normality of our approach in a low-dimensional setting, and then extend it to the high-dimensional models. Our analysis of high-dimensional settings explores scenarios both with and without the sparsity condition, establishing asymptotic normality and non-trivial power performance under local alternatives. Simulation studies and real data analysis demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16463v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Xu Guo, Songshan Yang, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>Causal inference targeting a concentration index for studies of health inequalities</title>
      <link>https://arxiv.org/abs/2410.08849</link>
      <description>arXiv:2410.08849v2 Announce Type: replace 
Abstract: A concentration index, a standardized covariance between a health variable and relative income ranks, is often used to quantify income-related health inequalities. There is a lack of formal approach to study the effect of an exposure, e.g., education, on such measures of inequality. In this paper we contribute by filling this gap and developing the necessary theory and method. Thus, we define a counterfactual concentration index for different levels of an exposure. We give conditions for their identification, and then deduce their efficient influence function. This allows us to propose estimators, which are regular asymptotic linear under certain conditions. In particular, these estimators are $\sqrt n$-consistent and asymptotically normal, as well as locally efficient. The implementation of the estimators is based on the fit of several nuisance functions. The estimators proposed have rate robustness properties allowing for convergence rates slower than $\sqrt{n}$-rate for some of the nuisance function fits. The relevance of the asymptotic results for finite samples is studied with simulation experiments. We also present a case study of the effect of education on income-related health inequalities for a Swedish cohort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08849v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ghasempour, Xavier de Luna, Per E. Gustafsson</dc:creator>
    </item>
    <item>
      <title>A Bivariate Transformation Model for Time-to-Event Data Affected by Unobserved Confounding: Revisiting the Illinois Reemployment Bonus Experiment</title>
      <link>https://arxiv.org/abs/2410.15968</link>
      <description>arXiv:2410.15968v5 Announce Type: replace 
Abstract: Motivated by empirical studies investigating treatment effects in survival analysis, we propose a bivariate transformation model to quantify the impact of a binary treatment on a time-to-event outcome. The model equations are connected through a bivariate Gaussian distribution, with the dependence parameter capturing unobserved confounding, and are specified as functions of additive predictors to flexibly account for the impacts of observed confounders. Moreover, the baseline survival function is estimated using monotonic P-splines, the effects of binary or factor instruments can be regularized through a ridge penalty approach, and interactions between treatment and observed confounders can be incorporated to accommodate potential variations in treatment effects across subgroups. The proposal naturally provides the survival average treatment effect. Parameter estimation is achieved via an efficient and stable penalized maximum likelihood estimation approach, and intervals constructed using related inferential results. We revisit a dataset from the Illinois Reemployment Bonus Experiment to estimate the effect of a cash bonus on the probability of remaining unemployed at several time points, unveiling interesting insights. The modeling framework is incorporated into the R package GJRM, enabling researchers and practitioners to employ the proposed model and ensuring the reproducibility of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15968v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation for Nonresponse in Complex Surveys Using Design Weights and Auxiliary Margins</title>
      <link>https://arxiv.org/abs/2412.10988</link>
      <description>arXiv:2412.10988v3 Announce Type: replace 
Abstract: Survey data typically have missing values due to unit and item nonresponse. Sometimes, survey organizations know the marginal distributions of certain categorical variables in the survey. As shown in previous work, survey organizations can leverage these distributions in multiple imputation for nonignorable unit nonresponse, generating imputations that result in plausible completed-data estimates for the variables with known margins. However, this prior work does not use the design weights for unit nonrespondents; rather, it relies on a set of fabricated weights for these units. We extend this previous work to utilize the design weights for all sampled units. We illustrate the approach using simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10988v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewei Xu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Statistical and Computational Efficiency for Smooth Tensor Estimation with Unknown Permutations</title>
      <link>https://arxiv.org/abs/2111.04681</link>
      <description>arXiv:2111.04681v2 Announce Type: replace-cross 
Abstract: We consider the problem of structured tensor denoising in the presence of unknown permutations. Such data problems arise commonly in recommendation system, neuroimaging, community detection, and multiway comparison applications. Here, we develop a general family of smooth tensor models up to arbitrary index permutations; the model incorporates the popular tensor block models and Lipschitz hypergraphon models as special cases. We show that a constrained least-squares estimator in the block-wise polynomial family achieves the minimax error bound. A phase transition phenomenon is revealed with respect to the smoothness threshold needed for optimal recovery. In particular, we find that a polynomial of degree up to $(m-2)(m+1)/2$ is sufficient for accurate recovery of order-$m$ tensors, whereas higher degree exhibits no further benefits. This phenomenon reveals the intrinsic distinction for smooth tensor estimation problems with and without unknown permutations. Furthermore, we provide an efficient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. The efficacy of our procedure is demonstrated through both simulations and Chicago crime data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04681v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2419114</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association (2024): 1-14</arxiv:journal_reference>
      <dc:creator>Chanwoo Lee, Miaoyan Wang</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for cellular automata</title>
      <link>https://arxiv.org/abs/2301.13320</link>
      <description>arXiv:2301.13320v2 Announce Type: replace-cross 
Abstract: Self-organizing complex systems can be modeled using cellular automaton models. However, the parametrization of these models is crucial and significantly determines the resulting structural pattern. In this research, we introduce and successfully apply a sound statistical method to estimate these parameters. The decisive difference to earlier applications of such approaches is that, in our case, both the CA rules and the resulting patterns are discrete. The method is based on constructing Gaussian likelihoods using characteristics of the structures, such as the mean particle size. We show that our approach is robust for the method parameters, domain size of patterns, or CA iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13320v2</guid>
      <category>nlin.CG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey Kazarnikov, Nadja Ray, Heikki Haario, Joona Lappalainen, Andreas Rupp</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection using an informed reversible jump in imaging genetics: an application to schizophrenia</title>
      <link>https://arxiv.org/abs/2307.01134</link>
      <description>arXiv:2307.01134v2 Announce Type: replace-cross 
Abstract: From a practical perspective, proposals are one of the main bottleneck for any Markov Chain Monte Carlo (MCMC) algorithm. This paper suggests a novel data driven or informed proposal for reversible jump MCMC for Bayesian variable selection in the context of predictive risk assessment for schizophrenia based on imaging genetic data. Given functional Magnetic Resonance Image and Single Nucleotide Polymorphisms information of healthy and people diagnosed with schizophrenia, we use a Bayesian probit model to select discriminating variables for inferential purposes, while to estimate the predictive risk, the most promising models are combined using a Bayesian model averaging scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01134v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Djidenou Montcho, Daiane Zuanetti, Thierry Chekouo, Luis Milan</dc:creator>
    </item>
    <item>
      <title>Integral Probability Metrics Meet Neural Networks: The Radon-Kolmogorov-Smirnov Test</title>
      <link>https://arxiv.org/abs/2309.02422</link>
      <description>arXiv:2309.02422v4 Announce Type: replace-cross 
Abstract: Integral probability metrics (IPMs) constitute a general class of nonparametric two-sample tests that are based on maximizing the mean difference between samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the IPM defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness degree $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. We can thus leverage the power of modern neural network optimization toolkits to (approximately) maximize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02422v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>A novel characterization of structures in smooth regression curves: from a viewpoint of persistent homology</title>
      <link>https://arxiv.org/abs/2310.19435</link>
      <description>arXiv:2310.19435v3 Announce Type: replace-cross 
Abstract: We characterize structures such as monotonicity, convexity, and modality in smooth regression curves using persistent homology. Persistent homology is a key tool in topological data analysis that detects higher dimensional topological features such as connected components and holes (cycles or loops) in the data. In other words, persistent homology is a multiscale version of homology that characterizes sets based on the connected components and holes. We use super-level sets of functions to extract geometric features via persistent homology. In particular, we explore structures in regression curves via the persistent homology of super-level sets of a function, where the function of interest is - the first derivative of the regression function.
  In the course of this study, we extend an existing procedure of estimating the persistent homology for the first derivative of a regression function and establish its consistency. Moreover, as an application of the proposed methodology, we demonstrate that the persistent homology of the derivative of a function can reveal hidden structures in the function that are not visible from the persistent homology of the function itself. In particular, we characterize structures such as monotonicity, convexity, and modality, and propose a measure of statistical significance to infer these structures in practice. Finally, we conduct an empirical study to implement the proposed methodology on simulated and real data sets and compare the derived results with an existing methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19435v3</guid>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satish Kumar, Subhra Sankar Dhar</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayes in Latent Variable Models</title>
      <link>https://arxiv.org/abs/2311.06831</link>
      <description>arXiv:2311.06831v2 Announce Type: replace-cross 
Abstract: Latent variable models are widely used to account for unobserved determinants of economic behavior. This paper introduces a quasi-Bayes approach to nonparametrically estimate a large class of latent variable models. As an application, we model U.S. individual log earnings from the Panel Study of Income Dynamics (PSID) as the sum of latent permanent and transitory components. Simulations illustrate the favorable performance of quasi-Bayes estimators relative to common alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06831v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v2 Announce Type: replace-cross 
Abstract: When we interpret linear regression estimates as causal effects justified by quasi-experiments, what do we mean? This paper characterizes the necessary implications when researchers ascribe a design-based interpretation to a given regression. To do so, we define a notion of potential weights, which encode counterfactual decisions a given regression makes to unobserved potential outcomes. A plausible design-based interpretation for a regression estimand implies linear restrictions on the true distribution of treatment; the coefficients in these linear equations are exactly potential weights. Solving these linear restrictions leads to a set of implicit designs that necessarily include the true design if the regression were to admit a causal interpretation. These necessary implications lead to practical diagnostics that add transparency and robustness when design-based interpretation is invoked for a regression. They also lead to new theoretical insights: They serve as a framework that unifies and extends existing results, and they lead to new results for widely used but less understood specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
  </channel>
</rss>
