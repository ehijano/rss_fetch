<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 03:44:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Integrative characterization of the topography of V4 neural codes using deep learning approaches</title>
      <link>https://arxiv.org/abs/2511.22050</link>
      <description>arXiv:2511.22050v1 Announce Type: new 
Abstract: Area V4 is a mid-level stage of the macaque ventral visual stream, known to encode intermediate visual features such as color, curvature, corners, texture, three-dimensional (3D) solids, and local form. Classical neurophysiological studies have typically examined these dimensions in isolation, contrasting V4 selectivity for shape versus texture, 3D solid surfaces versus two-dimensional (2D) flat patterns, or object form versus texture. Yet how these tunings relate to one another within individual neurons, and how they are jointly organized across the cortical surface, remain unknown. For instance, does a neuron selective for 2D contour-defined shape prefer 3D solid surfaces or 2D flat surfaces? How are preferences for such heterogeneous attributes arranged in a common topographic map? To address these questions, we leverage V4 "digital twins" -- deep neural network models fitted to large-scale, wide-field calcium imaging data comprising tens of thousands of natural images. These digital twins allow us to systematically probe not only the stimulus dimensions explored in earlier studies, but also new, multidimensional stimulus sets that reveal additional aspects of the V4 code. In this study, we find that neural pixels preferring 2D contour-defined shapes also tend to prefer 3D surface shape defined by shading or texture gradients and by object form. In contrast, pixels preferring 2D texture tend to prefer flat surfaces defined by uniform texture or reflectance. We propose that this division of labor suggests that V4 may decompose the encoding of geometrical shape and surface appearance of visual stimuli into distinct populations of neurons, organized as interleaved clusters in the V4 topographic map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22050v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingjue Bian, Tianye Wang, Shiming Tang, Tai Sing Lee</dc:creator>
    </item>
    <item>
      <title>Short-term plasticity recalls forgotten memories through a trampoline mechanism</title>
      <link>https://arxiv.org/abs/2511.22848</link>
      <description>arXiv:2511.22848v1 Announce Type: new 
Abstract: We analyze continuous Hopfield associative memories augmented by additional, rapid short-term associative synaptic plasticity. Through the cavity method, we determine the boundary between the retrieval and forgetting, or spin-glass phase, of the network as a function of the fraction of stored memories and the neuronal gain. We find that short-term synaptic plasticity yields marginal improvements in critical memory capacity. However, through dynamical mean field theory, backed by extensive numerical simulations, we find that short-term synaptic plasticity has a dramatic impact on memory retrieval above the critical capacity. When short-term synaptic plasticity is turned on, the combined neuronal and synaptic dynamics descends a high-dimensional energy landscape over both neurons and synapses. The energy landscape over neurons alone is thus dynamic, and is lowered in the vicinity of recent neuronal patterns visited by the network, just like the surface of a trampoline is lowered in the vicinity of regions recently visited by a heavy ball. This trampoline-like reactivity of the neuronal energy landscape to short-term plasticity in synapses can lead to the recall of stored memories that would otherwise have been forgotten. This occurs because the dynamics without short-term plasticity transiently moves towards a stored memory before departing away from it. Thus short-term plasticity, operating during the transient, lowers the energy in the vicinity of the stored memory, eventually trapping the combined neuronal and synaptic dynamics at a fixed point close to the stored memory. In this manner, short-term plasticity enables the recall of memories that would otherwise be forgotten, by trapping transients that would otherwise escape. We furthermore find an optimal time constant for short-term synaptic plasticity, matched to the transient dynamics, to empower recall of forgotten memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22848v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Del Gaudio, Federico Ghimenti, Surya Ganguli</dc:creator>
    </item>
    <item>
      <title>Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics</title>
      <link>https://arxiv.org/abs/2511.21848</link>
      <description>arXiv:2511.21848v1 Announce Type: cross 
Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21848v1</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Leonardis, Akira Nagamori, Ayesha Thanawalla, Yuanjia Yang, Joshua Park, Hutton Saunders, Eiman Azim, Talmo Pereira</dc:creator>
    </item>
    <item>
      <title>Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection</title>
      <link>https://arxiv.org/abs/2511.21940</link>
      <description>arXiv:2511.21940v1 Announce Type: cross 
Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21940v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiran Nair, Hubert Cecotti</dc:creator>
    </item>
    <item>
      <title>Fast dynamical similarity analysis</title>
      <link>https://arxiv.org/abs/2511.22828</link>
      <description>arXiv:2511.22828v1 Announce Type: cross 
Abstract: To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22828v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Behrad, Mitchell Ostrow, Mohammad Taha Fakharian, Ila Fiete, Christian Beste, Shervin Safavi</dc:creator>
    </item>
    <item>
      <title>Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis</title>
      <link>https://arxiv.org/abs/2511.22870</link>
      <description>arXiv:2511.22870v1 Announce Type: cross 
Abstract: Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22870v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jungwoo Seo, David Keetae Park, Shinjae Yoo, Jiook Cha</dc:creator>
    </item>
    <item>
      <title>Observing hidden neuronal states in experiments</title>
      <link>https://arxiv.org/abs/2308.15477</link>
      <description>arXiv:2308.15477v3 Announce Type: replace 
Abstract: In this article we demonstrate a general protocol for constructing systematically experimental steady-state bifurcation diagrams for electrophysiologically active cells. We perform our experiments on entorhinal cortex neurons, both excitatory (pyramidal neurons) and inhibitiory (interneurons). A slowly ramped voltage-clamp electrophysiology protocol serves as closed-loop feedback controlled experiment for the subsequent current-clamp open-loop protocol on the same cell. In this way, the voltage-clamped experiment determines dynamically stable and unstable (hidden) steady states of the current-clamp experiment. The transitions between observable steady states and observable spiking states in the current-clamp experiment provide partial evidence for stability and bifurcations of the steady states. This technique for completing steady-state bifurcation diagrams in a model-independent way expands support for model validation to otherwise inaccessible regions of the phase space. Overlaying the voltage-clamp and current-clamp protocols leads to an experimental validation of the classical slow-fast dissection method introduced by J. Rinzel in the 1980s and routinely applied ever since in order to analyse slow-fast neuronal models. Our approach opens doors to observing further complex hidden states with more advanced control strategies, allowing to control real cells beyond pharmacological manipulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15477v3</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Amakhin, Anton Chizhov, Guillaume Girier, Mathieu Desroches, Jan Sieber, Serafim Rodrigues</dc:creator>
    </item>
    <item>
      <title>YARE-GAN: Yet Another Resting State EEG-GAN</title>
      <link>https://arxiv.org/abs/2503.02636</link>
      <description>arXiv:2503.02636v4 Announce Type: replace 
Abstract: Resting-state EEG offers a non-invasive view of spontaneous brain activity, yet the extraction of meaningful patterns is often constrained by limited availability of high-quality data, and heavy reliance on manually engineered EEG features. Generative Adversarial Networks (GANs) offer not only a means to synthesize and augment neural signals, but also a promising way for learning meaningful representations directly from raw data, a dual capability that remains largely unexplored in EEG research. In this study, we introduce a scalable GAN-based framework for resting-state EEG that serves this dual role: 1) synthesis and 2) unsupervised feature extraction. The generated time series closely replicate key statistical and spectral properties of real EEG, as validated through both visual and quantitative evaluations. Importantly, we demonstrate that the model's learned representations can be repurposed for a downstream gender classification task, achieving higher out-of-sample accuracy than models trained directly on EEG signals and performing comparably to recent EEG foundation models, while using significantly less data and computational resources. These findings highlight the potential of generative models to serve as both neural signal generators and unsupervised feature extractors, paving the way for more data-efficient, architecture-driven approaches to EEG analysis with reduced reliance on manual feature engineering. The implementation code for this study is available at: https://github.com/Yeganehfrh/YARE-GAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02636v4</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yeganeh Farahzadi, Morteza Ansarinia, Zoltan Kekecs</dc:creator>
    </item>
    <item>
      <title>On a Geometry of Interbrain Networks</title>
      <link>https://arxiv.org/abs/2509.10650</link>
      <description>arXiv:2509.10650v3 Announce Type: replace 
Abstract: Effective analysis in neuroscience benefits significantly from robust conceptual frameworks. Traditional metrics of interbrain synchrony in social neuroscience typically depend on fixed, correlation-based approaches, restricting their explanatory capacity to descriptive observations. Inspired by the successful integration of geometric insights in network science, we propose leveraging discrete geometry to examine the dynamic reconfigurations in neural interactions during social exchanges. Unlike conventional synchrony approaches, our method interprets inter-brain connectivity changes through the evolving geometric structures of neural networks. This geometric framework is realized through a pipeline that identifies critical transitions in network connectivity using entropy metrics derived from curvature distributions. By doing so, we significantly enhance the capacity of hyperscanning methodologies to uncover underlying neural mechanisms in interactive social behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10650v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'as Hinrichs, Noah Guzm\'an, Melanie Weber</dc:creator>
    </item>
    <item>
      <title>A thermoinformational formulation for the description of neuropsychological systems</title>
      <link>https://arxiv.org/abs/2511.09506</link>
      <description>arXiv:2511.09506v2 Announce Type: replace 
Abstract: Complex systems produce high-dimensional signals that lack macroscopic variables analogous to entropy, temperature, or free energy. This work introduces a thermoinformational formulation that derives entropy, internal energy, temperature, and Helmholtz free energy directly from empirical microstate distributions of arbitrary datasets. The approach provides a data-driven description of how a system reorganizes, exchanges information, and moves between stable and unstable states. Applied to dual-EEG recordings from mother-infant dyads performing the A-not-B task, the formulation captures increases in informational heat during switches and errors, and reveals that correct choices arise from more stable, low-temperature states. In an independent optogenetic dam-pup experiment, the same variables separate stimulation conditions and trace coherent trajectories in thermodynamic state space. Across both human and rodent systems, this thermoinformational formulation yields compact and physically interpretable macroscopic variables that generalize across species, modalities, and experimental paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09506v2</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George-Rafael Domenikos, Victoria Leong</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings</title>
      <link>https://arxiv.org/abs/2505.22563</link>
      <description>arXiv:2505.22563v2 Announce Type: replace-cross 
Abstract: Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22563v2</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Lei, Xingyang Ge, Yi Zhang, Yiming Yang, Bolei Ma</dc:creator>
    </item>
  </channel>
</rss>
