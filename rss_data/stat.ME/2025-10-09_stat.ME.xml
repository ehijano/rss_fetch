<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 02:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Maximum softly penalised likelihood in factor analysis</title>
      <link>https://arxiv.org/abs/2510.06465</link>
      <description>arXiv:2510.06465v1 Announce Type: new 
Abstract: Estimation in exploratory factor analysis often yields estimates on the boundary of the parameter space. Such occurrences, known as Heywood cases, are characterised by non-positive variance estimates and can cause issues in numerical optimisation procedures or convergence failures, which, in turn, can lead to misleading inferences, particularly regarding factor scores and model selection. We derive sufficient conditions on the model and a penalty to the log-likelihood function that i) guarantee the existence of maximum penalised likelihood estimates in the interior of the parameter space, and ii) ensure that the corresponding estimators possess the desirable asymptotic properties expected by the maximum likelihood estimator, namely consistency and asymptotic normality. Consistency and asymptotic normality are achieved when the penalisation is soft enough, in a way that adapts to the information accumulation about the model parameters. We formally show, for the first time, that the penalties of Akaike (1987) and Hirose et al. (2011) to the log-likelihood of the normal linear factor model satisfy the conditions for existence, and, hence, deal with Heywood cases. Their vanilla versions, though, can result in questionable finite-sample properties in estimation, inference, and model selection. The maximum softly-penalised likelihood framework we introduce enables the careful scaling of those penalties to ensure that the resulting estimation and inference procedures are asymptotically optimal. Through comprehensive simulation studies and the analysis of real data sets, we illustrate the desirable finite-sample properties of the maximum softly penalised likelihood estimators and associated procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06465v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Sterzinger, Ioannis Kosmids, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>Likelihood-based inference for the Gompertz model with Poisson errors</title>
      <link>https://arxiv.org/abs/2510.06787</link>
      <description>arXiv:2510.06787v1 Announce Type: new 
Abstract: Population dynamics models play an important role in a number of fields, such as actuarial science, demography, and ecology. Statistical inference for these models can be difficult when, in addition to the process' inherent stochasticity, one also needs to account for sampling error. Ignoring the latter can lead to biases in the estimation, which in turn can produce erroneous conclusions about the system's behavior. The Gompertz model is widely used to infer population size dynamics, but a full likelihood approach can be computationally prohibitive when sampling error is accounted for. We close this gap by developing efficient computational tools for statistical inference in the Gompertz model based on the full likelihood. The approach is illustrated in both the Bayesian and frequentist paradigms. Performance is illustrated with simulations and data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06787v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Onorati, Sofia Ruiz-Suarez, Radu Craiu</dc:creator>
    </item>
    <item>
      <title>Rank Aggregation under Weak Stochastic Transitivity via a Maximum Score Estimator</title>
      <link>https://arxiv.org/abs/2510.06789</link>
      <description>arXiv:2510.06789v1 Announce Type: new 
Abstract: Stochastic transitivity is central for rank aggregation based on pairwise comparison data. The existing models, including the Thurstone, Bradley-Terry (BT), and nonparametric BT models, adopt a strong notion of stochastic transitivity, known as strong stochastic transitivity (SST). This assumption imposes restrictive monotonicity constraints on the pairwise comparison probabilities, which is often unrealistic for real-world applications. This paper introduces a maximum score estimator for aggregating ranks, which only requires the assumption of weak stochastic transitivity (WST), the weakest assumption needed for the existence of a global ranking. The proposed estimator allows for sparse settings where the comparisons between many pairs are missing with possibly nonuniform missingness probabilities. We show that the proposed estimator is consistent, in the sense that the proportion of discordant pairs converges to zero in probability as the number of players diverges. We also establish that the proposed estimator is nearly minimax optimal for the convergence of a loss function based on Kendall's tau distance. The power of the proposed method is shown via a simulation study and an application to rank professional tennis players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06789v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Zhang, Yunxiao Chen</dc:creator>
    </item>
    <item>
      <title>Inference in pseudo-observation-based regression using (biased) covariance estimation and naive bootstrapping</title>
      <link>https://arxiv.org/abs/2510.06815</link>
      <description>arXiv:2510.06815v1 Announce Type: new 
Abstract: We demonstrate that the usual Huber-White estimator is not consistent for the limiting covariance of parameter estimates in pseudo-observation regression approaches. By confirming that a plug-in estimator can be used instead, we obtain asymptotically exact and consistent tests for general linear hypotheses in the parameters of the model. Additionally, we confirm that naive bootstrapping can not be used for covariance estimation in the pseudo-observation model either. However, it can be used for hypothesis testing by applying a suitable studentization. Simulations illustrate the good performance of our proposed methods in many scenarios. Finally, we obtain a general uniform law of large numbers for U- and V-statistics, as such statistics are central in the mathematical analysis of the inference procedures developed in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06815v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simon Mack, Morten Overgaard, Dennis Dobler</dc:creator>
    </item>
    <item>
      <title>Confidence Regions for Multiple Outcomes, Effect Modifiers, and Other Multiple Comparisons</title>
      <link>https://arxiv.org/abs/2510.07076</link>
      <description>arXiv:2510.07076v1 Announce Type: new 
Abstract: In epidemiology, some have argued that multiple comparison corrections are not necessary as there is rarely interest in the universal null hypothesis. From a parameter estimation perspective, epidemiologists may still be interested in multiple parameters. In this context, standard confidence intervals are not guaranteed to provide simultaneous coverage of more than one parameter. In other words, use of confidence intervals in these cases will understate the uncertainty due to random error. To address this challenge, one can use confidence bands, an extension of confidence intervals to parameter vectors. We illustrate the use of confidence bands in three case studies: estimation of multiple causal effects, effect measure modification by a binary variable, and effect measure modification by a continuous variable. Each example uses publicly available data is accompanied by SAS, R, and Python code. The type of confidence region reported by epidemiologists should depend on whether scientific interest is in a single parameter or a set of parameters. For sets of parameters, like in cases where multiple actions or outcomes, effect measure modification, dose-response, or other functions are of interest, sup-t confidence bands are preferred due to their statistical properties, computational simplicity, and ease of presentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07076v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Stephen R Cole, Noah Greifer, Lina M Montoya, Michael R Kosorok, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>On Assessing Overall Survival (OS) in Oncology Studies</title>
      <link>https://arxiv.org/abs/2510.07122</link>
      <description>arXiv:2510.07122v1 Announce Type: new 
Abstract: In assessing Overall Survival (OS) in oncology studies, it is essential for the efficacy measure to be Logic-respecting, for otherwise patients may be incorrectly targeted. This paper explains, while Time Ratio (TR) is Logic-respecting, Hazard Ratio (HR) is not Logic-respecting. With Time Ratio (TR) being recommended, a smooth transitioning strategy is suggested. The conclusion states: Logicality requires, and Subgroup Mixable Estimation (SME) delivers, an efficacy assessment for the overall population within the range of minimum and maximum efficacy in the subgroups, no matter how outcome is measured, whichever logic-respecting efficacy measure is chosen, the same efficacy assessment regardless of how subgroups are stratified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07122v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>jmstate, a Flexible Python Package for Multi-State Joint Modeling</title>
      <link>https://arxiv.org/abs/2510.07128</link>
      <description>arXiv:2510.07128v1 Announce Type: new 
Abstract: Classical joint modeling approaches often rely on competing risks or recurrent event formulations to account for complex real-world processes involving evolving longitudinal markers and discrete event occurrences. However, these frameworks typically capture only limited aspects of the underlying event dynamics.
  Multi-state joint models offer a more flexible alternative by representing full event histories through a network of possible transitions, including recurrent cycles and terminal absorptions, all potentially influenced by longitudinal covariates.
  In this paper, we propose a general framework that unifies longitudinal biomarker modeling with multi-state event processes defined on arbitrary directed graphs. Our approach accommodates both Markovian and semi-Markovian transition structures, and extends classical joint models by coupling nonlinear mixed-effects longitudinal submodels with multi-state survival processes via shared latent structures.
  We derive the full likelihood and develop scalable inference procedures based on stochastic gradient descent. Furthermore, we introduce a dynamic prediction framework, enabling individualized risk assessments along complex state-transition trajectories.
  To facilitate reproducibility and dissemination, we provide an open-source Python library \texttt{jmstate} implementing the proposed methodology, available on \href{https://pypi.org/project/jmstate/}{PyPI}. Simulation experiments and a biomedical case study demonstrate the flexibility and performance of the framework in representing complex longitudinal and multi-state event dynamics. The full Python notebooks used to reproduce the experiments as well as the source code of this paper are available on \href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07128v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Laplante, Christophe Ambroise, Estelle Kuhn, Sarah Lemler</dc:creator>
    </item>
    <item>
      <title>Randomization Restrictions: Their Impact on Type I Error When Experimenting with Finite Populations</title>
      <link>https://arxiv.org/abs/2510.07153</link>
      <description>arXiv:2510.07153v1 Announce Type: new 
Abstract: Participants in clinical trials are often viewed as a unique, finite population. Yet, statistical analyses often assume that participants were randomly sampled from a larger population. Under Complete Randomization, Randomization-Based Inference (RBI; a finite population inference) and Analysis of Variance (ANOVA; a random sampling inference) provide asymptotically equivalent difference-in-means tests. However, sequentially-enrolling trials typically employ restricted randomization schemes, such as block or Maximum Tolerable Imbalance (MTI) designs, to reduce the chance of chronological treatment imbalances. The impact of these restrictions on RBI and ANOVA concordance is not well understood. With real-world frames of reference, such as rare and ultra-rare diseases, we review full versus random sampling of finite populations and empirically evaluate finite population Type I error when using ANOVA following randomization restrictions. Randomization restrictions strongly impacted ANOVA Type I error, even for trials with 1,000 participants. Properly adjusting for restrictions corrected Type I error. We corrected for block randomization, yet leave open how to correct for MTI designs. More directly, RBI accounts for randomization restrictions while ensuring correct finite population Type I error. Novel contributions are: 1) deepening the understanding and correction of RBI and ANOVA concordance under block and MTI restrictions and 2) using finite populations to estimate the convergence of Type I error to a nominal rate. We discuss the challenge of specifying an estimand's population and reconciling with sampled trial participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07153v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan J. Chipman, Oleksandr Sverdlov, Diane Uschner</dc:creator>
    </item>
    <item>
      <title>A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling</title>
      <link>https://arxiv.org/abs/2510.06264</link>
      <description>arXiv:2510.06264v1 Announce Type: cross 
Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the study of civil resistance. This study investigates the central paradox of the success of this student-led civilian uprising: how state violence, intended to quell dissent, ultimately fueled the movement's victory. We employ a mixed-methods approach. First, we develop a qualitative narrative of the conflict's timeline to generate specific, testable hypotheses. Then, using a disaggregated, event-level dataset, we employ a multi-method quantitative analysis to dissect the complex relationship between repression and mobilisation. We provide a framework to analyse explosive modern uprisings like the July Revolution. Initial pooled regression models highlight the crucial role of protest momentum in sustaining the movement. To isolate causal effects, we specify a Two-Way Fixed Effects panel model, which provides robust evidence for a direct and statistically significant local suppression backfire effect. Our Vector Autoregression (VAR) analysis provides clear visual evidence of an immediate, nationwide mobilisation in response to increased lethal violence. We further demonstrate that this effect was non-linear. A structural break analysis reveals that the backfire dynamic was statistically insignificant in the conflict's early phase but was triggered by the catalytic moral shock of the first wave of lethal violence, and its visuals circulated around July 16th. A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65) corroborates this from a predictive standpoint, identifying "excessive force against protesters" as the single most dominant predictor of nationwide escalation. We conclude that the July Revolution was driven by a contingent, non-linear backfire, triggered by specific catalytic moral shocks and accelerated by the viral reaction to the visual spectacle of state brutality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06264v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Saiful Bari Siddiqui, Anupam Debashis Roy</dc:creator>
    </item>
    <item>
      <title>Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2510.06735</link>
      <description>arXiv:2510.06735v1 Announce Type: cross 
Abstract: Bayesian causal discovery benefits from prior information elicited from domain experts, and in heterogeneous domains any prior knowledge would be badly needed. However, so far prior elicitation approaches have assumed a single causal graph and hence are not suited to heterogeneous domains. We propose a causal elicitation strategy for heterogeneous settings, based on Bayesian experimental design (BED) principles, and a variational mixture structure learning (VaMSL) method -- extending the earlier differentiable Bayesian structure learning (DiBS) method -- to iteratively infer mixtures of causal Bayesian networks (CBNs). We construct an informative graph prior incorporating elicited expert feedback in the inference of mixtures of CBNs. Our proposed method successfully produces a set of alternative causal models (mixture components or clusters), and achieves an improved structure learning performance on heterogeneous synthetic data when informed by a simulated expert. Finally, we demonstrate that our approach is capable of capturing complex distributions in a breast cancer database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06735v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachris Bj\"orkman, Jorge Lor\'ia, Sophie Wharrie, Samuel Kaski</dc:creator>
    </item>
    <item>
      <title>Estimating temporary emigration from capture-recapture data in the presence of latent identification</title>
      <link>https://arxiv.org/abs/2510.06755</link>
      <description>arXiv:2510.06755v1 Announce Type: cross 
Abstract: Most capture-recapture models assume that individuals either do not emigrate or emigrate permanently from the sampling area during the sampling period. This assumption is violated when individuals temporarily leave the sampling area and return during later capture occasions, which can result in biased or less precise inferences under normal capture-recapture models. Existing temporary emigration models require that individuals are uniquely and correctly identified. To our knowledge, no studies to date have addressed temporary emigration in the presence of latent individual identification, which can arise in many scenarios such as misidentification, data integration, and batch marking. In this paper, we propose a new latent multinomial temporary emigration modelling framework for analysing capture-recapture data with latent identification. The framework is applicable to both closed- and open-population problems, accommodates data with or without individual identification, and flexibly incorporates different emigration processes, including the completely random and Markovian emigration. Through simulations, we demonstrate that model parameters can be reliably estimated in various emigration scenarios. We apply the proposed framework to a real dataset on golden mantella collected using batch marks under Pollock's robust design. The results show that accounting for temporary emigration provides a better fit to the data compared to the previous model without temporary emigration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06755v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarina Skopalova, Jafet Osuna, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Testing the equality of estimable parameters across many populations</title>
      <link>https://arxiv.org/abs/2510.06763</link>
      <description>arXiv:2510.06763v1 Announce Type: cross 
Abstract: The comparison of a parameter in $k$ populations is a classical problem in statistics. Testing for the equality of means or variances are typical examples. Most procedures designed to deal with this problem assume that $k$ is fixed and that samples with increasing sample sizes are available from each population. This paper introduces and studies a test for the comparison of an estimable parameter across $k$ populations, when $k$ is large and the sample sizes from each population are small when compared with $k$. The proposed test statistic is asymptotically distribution-free under the null hypothesis of parameter homogeneity, enabling asymptotically exact inference without parametric assumptions. Additionally, the behaviour of the proposal is studied under alternatives. Simulations are conducted to evaluate its finite-sample performance, and a linear bootstrap method is implemented to improve its behaviour for small $k$. Finally, an application to a real dataset is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06763v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcos Romero-Madro\~nal, Mar\'ia de los Remedios Sillero-Denamiel, Mar\'ia Dolores Jim\'enez-Gamero</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of Self- and Cross-Impact</title>
      <link>https://arxiv.org/abs/2510.06879</link>
      <description>arXiv:2510.06879v1 Announce Type: cross 
Abstract: We introduce an offline nonparametric estimator for concave multi-asset propagator models based on a dataset of correlated price trajectories and metaorders. Compared to parametric models, our framework avoids parameter explosion in the multi-asset case and yields confidence bounds for the estimator. We implement the estimator using both proprietary metaorder data from Capital Fund Management (CFM) and publicly available S&amp;P order flow data, where we augment the former dataset using a metaorder proxy. In particular, we provide unbiased evidence that self-impact is concave and exhibits a shifted power-law decay, and show that the metaorder proxy stabilizes the calibration. Moreover, we find that introducing cross-impact provides a significant gain in explanatory power, with concave specifications outperforming linear ones, suggesting that the square-root law extends to cross-impact. We also measure asymmetric cross-impact between assets driven by relative liquidity differences. Finally, we demonstrate that a shape-constrained projection of the nonparametric kernel not only ensures interpretability but also slightly outperforms established parametric models in terms of predictive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06879v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.MF</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natascha Hey, Eyal Neuman, Sturmius Tuschmann</dc:creator>
    </item>
    <item>
      <title>Root Cause Analysis of Outliers in Unknown Cyclic Graphs</title>
      <link>https://arxiv.org/abs/2510.06995</link>
      <description>arXiv:2510.06995v1 Announce Type: cross 
Abstract: We study the propagation of outliers in cyclic causal graphs with linear structural equations, tracing them back to one or several "root cause" nodes. We show that it is possible to identify a short list of potential root causes provided that the perturbation is sufficiently strong and propagates according to the same structural equations as in the normal mode. This shortlist consists of the true root causes together with those of its parents lying on a cycle with the root cause. Notably, our method does not require prior knowledge of the causal graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06995v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniela Schkoda, Dominik Janzing</dc:creator>
    </item>
    <item>
      <title>Beyond the Oracle Property: Adaptive LASSO in Cointegrating Regressions</title>
      <link>https://arxiv.org/abs/2510.07204</link>
      <description>arXiv:2510.07204v1 Announce Type: cross 
Abstract: This paper establishes new asymptotic results for the adaptive LASSO estimator in cointegrating regression models. We study model selection probabilities, estimator consistency, and limiting distributions under both standard and moving-parameter asymptotics. We also derive uniform convergence rates and the fastest local-to-zero rates that can still be detected by the estimator, complementing and extending the results of Lee, Shi, and Gao (2022, Journal of Econometrics, 229, 322--349). Our main findings include that under conservative tuning, the adaptive LASSO estimator is uniformly $T$-consistent and the cut-off rate for local-to-zero coefficients that can be detected by the procedure is $1/T$. Under consistent tuning, however, both rates are slower and depend on the tuning parameter. The theoretical results are complemented by a detailed simulation study showing that the finite-sample distribution of the adaptive LASSO estimator deviates substantially from what is suggested by the oracle property, whereas the limiting distributions derived under moving-parameter asymptotics provide much more accurate approximations. Finally, we show that our results also extend to models with local-to-unit-root regressors and to predictive regressions with unit-root predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07204v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karsten Reichold, Ulrike Schneider</dc:creator>
    </item>
    <item>
      <title>A Bernstein polynomial approach for the estimation of cumulative distribution functions in the presence of missing data</title>
      <link>https://arxiv.org/abs/2510.07235</link>
      <description>arXiv:2510.07235v1 Announce Type: cross 
Abstract: We study nonparametric estimation of univariate cumulative distribution functions (CDFs) pertaining to data missing at random. The proposed estimators smooth the inverse probability weighted (IPW) empirical CDF with the Bernstein operator, yielding monotone, $[0,1]$-valued curves that automatically adapt to bounded supports. We analyze two versions: a pseudo estimator that uses known propensities and a feasible estimator that uses propensities estimated nonparametrically from discrete auxiliary variables, the latter scenario being much more common in practice. For both, we derive pointwise bias and variance expansions, establish the optimal polynomial degree $m$ with respect to the mean integrated squared error, and prove the asymptotic normality. A key finding is that the feasible estimator has a smaller variance than the pseudo estimator by an explicit nonnegative correction term. We also develop an efficient degree selection procedure via least-squares cross-validation. Monte Carlo experiments demonstrate that, for moderate to large sample sizes, the Bernstein-smoothed feasible estimator outperforms both its unsmoothed counterpart and an integrated version of the IPW kernel density estimator proposed by Dubnicka (2009) in the same context. A real-data application to fasting plasma glucose from the 2017-2018 NHANES survey illustrates the method in a practical setting. All code needed to reproduce our analyses is readily accessible on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07235v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rihab Gharbi, Wissem Jedidi, Salah Khardani, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Multiscale Quantile Regression with Local Error Control</title>
      <link>https://arxiv.org/abs/2403.11356</link>
      <description>arXiv:2403.11356v3 Announce Type: replace 
Abstract: For robust and efficient detection of change points, we introduce a novel methodology MUSCLE (Multiscale qUantile Segmentation Controlling Local Error) that partitions serial data into multiple segments, each sharing a common quantile. It leverages multiple tests for quantile changes over different scales and locations, and variational estimation. Unlike the often adopted global error control, MUSCLE focuses on local errors defined on individual segments, significantly improving detection power in finding change points. Meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. Further, we obtain the consistency and the localization error rates in estimating change points, under mild signal-to-noise-ratio conditions. Both match (up to log factors) the minimax optimality results in the Gaussian setup. All theories hold under the only distributional assumption of serial independence. Incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing MUSCLE. Extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. An implementation via R package muscle is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11356v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Liu, Housen Li</dc:creator>
    </item>
    <item>
      <title>The BPgWSP test: a Bayesian Weibull Shape Parameter signal detection test for adverse drug reactions</title>
      <link>https://arxiv.org/abs/2412.05463</link>
      <description>arXiv:2412.05463v3 Announce Type: replace 
Abstract: We develop the Bayesian Power generalized Weibull shape parameter (BPgWSP) test as statistical method for signal detection of possible drug-adverse event associations using electronic health records for pharmacovigilance. The Bayesian approach allows the incorporation of prior knowledge about the likely time of occurrence along time-to-event data. The test is based on the shape parameters of the Power generalized Weibull (PgW) distribution. When both shape parameters are equal to one, the PgW distribution reduces to an exponential distribution, yielding a constant hazard function. This is interpreted as no temporal association between drug and adverse event (AE). The BPgWSP test involves comparing a region of practical equivalence (ROPE) around one reflecting the null hypothesis with estimated credibility intervals (CI) reflecting the posterior means of the shape parameters. The decision to raise a signal is based on the CI+ROPE tests and the selected combination rule for these outcomes. The test development requires a simulation study for tuning of the ROPE and CIs to optimize specificity and sensitivity of the test. Samples are generated under various conditions, including differences in sample size, prevalence of adverse drug reactions (ADRs), and the proportion of AEs. We explore prior assumptions reflecting the belief in the presence or absence of ADRs at different points in the observation period. Various types of ROPE, CIs, and combination rules are assessed, and optimal tuning parameters are identified based on the area under the curve. The tuned BPgWSP test is illustrated in a case study in which the time-dependent correlation between the intake of bisphosphonates and four AEs is investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05463v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Dyck, Odile Sauzet</dc:creator>
    </item>
    <item>
      <title>Order Determination for Functional Data</title>
      <link>https://arxiv.org/abs/2503.03000</link>
      <description>arXiv:2503.03000v2 Announce Type: replace 
Abstract: Dimension reduction is often necessary in functional data analysis, with functional principal component analysis being one of the most widely used techniques. A key challenge in applying these methods is determining the number of eigen-pairs to retain, a problem known as order determination. When a covariance function admits a finite representation, the challenge becomes estimating the rank of the associated covariance operator. While this problem is straightforward when the full trajectories of functional data are available, in practice, functional data are typically collected discretely and are subject to measurement error contamination. This contamination introduces a ridge to the empirical covariance function, which obscures the true rank of the covariance operator. We propose a novel procedure to identify the true rank of the covariance operator by leveraging the information of eigenvalues and eigenfunctions. By incorporating the nonparametric nature of functional data through smoothing techniques, the method is applicable to functional data collected at random, subject-specific points. Extensive simulation studies demonstrate the excellent performance of our approach across a wide range of settings, outperforming commonly used information-criterion-based methods and maintaining effectiveness even in high-noise scenarios. We further illustrate our method with two real-world data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03000v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
    <item>
      <title>Conditional Local Independence Testing for It\^o processes with Applications to Dynamic Causal Discovery</title>
      <link>https://arxiv.org/abs/2506.07844</link>
      <description>arXiv:2506.07844v4 Announce Type: replace 
Abstract: Inferring causal relationships from dynamical systems is the central interest of many scientific inquiries. Conditional local independence, which describes whether the evolution of one process is influenced by another process given additional processes, is important for causal learning in such systems. In this paper, we propose a hypothesis test for conditional local independence in It\^o processes. Our test is grounded in the semimartingale decomposition of the It\^o process, with which we introduce a stochastic integral process that is a martingale under the null hypothesis. We then apply a test for the martingale property, quantifying potential deviation from local independence. The test statistics is estimated using the optimal filtering equation. We show the consistency of the estimation, thereby establishing the level and power of our test. Numerical verification and a real-world application to causal discovery in brain resting-state fMRIs are conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07844v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhou Liu, Xinwei Sun, Yizhou Wang</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Joint Potential Outcome Distributions from a Single Study</title>
      <link>https://arxiv.org/abs/2509.20506</link>
      <description>arXiv:2509.20506v2 Announce Type: replace 
Abstract: Most causal inference methods focus on estimating marginal average treatment effects, but many important causal estimands depend on the joint distribution of potential outcomes, including the probability of causation and proportions benefiting from or harmed by treatment. Wu et al (2025) recently established nonparametric identification of this joint distribution for categorical outcomes under binary treatment by leveraging variation across multiple studies. We demonstrate that their multi-study framework can be implemented within a single study by using a baseline covariate that is associated with untreated potential outcomes but does not modify treatment effects conditional on those outcomes. This reframing substantially broadens the practical applicability of their results, as it eliminates the need for multiple independent datasets and gives analysts control over covariate selection to satisfy key identifying assumptions. We provide complete identification and estimation theory for the single-study setting, including a Neyman-orthogonal estimator for cases where the conditional independence assumption only holds after adjusting for covariates. We validate the estimator in a simulation and apply it to data from a large field experiment assessing the effect of mailings on voter turnout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20506v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, David Madigan</dc:creator>
    </item>
    <item>
      <title>A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data</title>
      <link>https://arxiv.org/abs/2510.05353</link>
      <description>arXiv:2510.05353v2 Announce Type: replace 
Abstract: A fundamental challenge in comparing two survival distributions with right censored data is the selection of an appropriate nonparametric test, as the power of standard tests like the Log rank and Wilcoxon is highly dependent on the often unknown nature of the alternative hypothesis. This paper introduces a new, distribution free two sample test designed to overcome this limitation. The proposed method is based on a strategic decomposition of the data into uncensored and censored subsets, from which a composite test statistic is constructed as the sum of two independent Mann Whitney statistics. This design allows the test to automatically and inherently adapt to various patterns of difference including early, late, and crossing hazards without requiring pre specified parameters, pre testing, or complex weighting schemes. An extensive Monte Carlo simulation study demonstrates that the proposed test robustly maintains the nominal Type I error rate. Crucially, its power is highly competitive with the optimal traditional tests in standard scenarios and superior in complex settings with crossing survival curves, while also exhibiting remarkable robustness to high levels of censoring. The test power effectively approximates the maximum power achievable by either the Log rank or Wilcoxon tests across a wide range of alternatives, offering a powerful, versatile, and computationally simple tool for survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05353v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abid Hussain, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>Automatic Order, Bandwidth Selection and Flaws of Eigen Adjustment in HAC Estimation</title>
      <link>https://arxiv.org/abs/2509.23256</link>
      <description>arXiv:2509.23256v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a new heteroskedasticity and autocorrelation consistent covariance matrix estimator based on the prewhitened kernel estimator and a localized leave-one-out frequency domain cross-validation (FDCV). We adapt the cross-validated log likelihood (CVLL) function to simultaneously select the order of the prewhitening vector autoregression (VAR) and the bandwidth. The prewhitening VAR is estimated by the Burg method without eigen adjustment as we find the eigen adjustment rule of Andrews and Monahan (1992) can be triggered unnecessarily and harmfully when regressors have nonzero mean. Through Monte Carlo simulations and three empirical examples, we illustrate the flaws of eigen adjustment and the reliability of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23256v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoxun Li, Clifford M. Hurvich</dc:creator>
    </item>
    <item>
      <title>Simulation-based inference via telescoping ratio estimation for trawl processes</title>
      <link>https://arxiv.org/abs/2510.04042</link>
      <description>arXiv:2510.04042v2 Announce Type: replace-cross 
Abstract: The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method's effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04042v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Leonte, Rapha\"el Huser, Almut E. D. Veraart</dc:creator>
    </item>
  </channel>
</rss>
