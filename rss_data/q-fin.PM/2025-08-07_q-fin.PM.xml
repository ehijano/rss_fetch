<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Aug 2025 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Novel Risk Measures for Portfolio Optimization Using Equal-Correlation Portfolio Strategy</title>
      <link>https://arxiv.org/abs/2508.03704</link>
      <description>arXiv:2508.03704v1 Announce Type: new 
Abstract: Portfolio optimization has long been dominated by covariance-based strategies, such as the Markowitz Mean-Variance framework. However, these approaches often fail to ensure a balanced risk structure across assets, leading to concentration in a few securities. In this paper, we introduce novel risk measures grounded in the equal-correlation portfolio strategy, aiming to construct portfolios where each asset maintains an equal correlation with the overall portfolio return. We formulate a mathematical optimization framework that explicitly controls portfolio-wide correlation while preserving desirable risk-return trade-offs. The proposed models are empirically validated using historical stock market data. Our findings show that portfolios constructed via this approach demonstrate superior risk diversification and more stable returns under diverse market conditions. This methodology offers a compelling alternative to conventional diversification techniques and holds practical relevance for institutional investors, asset managers, and quantitative trading strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03704v1</guid>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biswarup Chakraborty</dc:creator>
    </item>
    <item>
      <title>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</title>
      <link>https://arxiv.org/abs/2307.07694</link>
      <description>arXiv:2307.07694v3 Announce Type: replace-cross 
Abstract: We evaluate benchmark deep reinforcement learning algorithms on the task of portfolio optimisation using simulated data. The simulator to generate the data is based on correlated geometric Brownian motion with the Bertsimas-Lo market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact as an upper bound to measure performance when including market impact. We find that the off-policy algorithms DDPG, TD3 and SAC are unable to learn the right $Q$-function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation, are able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we find that PPO, combined with a hidden Markov model to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high for applications using real data, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07694v3</guid>
      <category>cs.CE</category>
      <category>q-fin.PM</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chung I Lu</dc:creator>
    </item>
  </channel>
</rss>
