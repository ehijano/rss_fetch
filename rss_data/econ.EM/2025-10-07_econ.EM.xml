<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 01:46:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Identification in Auctions with Truncated Transaction Prices</title>
      <link>https://arxiv.org/abs/2510.04464</link>
      <description>arXiv:2510.04464v1 Announce Type: new 
Abstract: Many auction datasets with reserve prices do not include bids that fall below the reserve. This paper establishes nonparametric identification results in first- and second-price auctions when transaction prices are truncated by a binding reserve price under a range of information structures. In the simplest case-where the number of potential bidders is fixed and known across all auctions-if only the transaction price is observed, the bidders' private-value distribution is identified in second-price auctions but not in first-price auctions. Identification in first-price auctions can be achieved if either the number of active bidders (those whose bids exceed the reserve) or the number of auctions with no sales (all bids below the reserve) is observed. When the number of potential bidders varies across auctions and is unknown, the bidders' private-value distribution is identified in first-price auctions but not in second-price auctions, provided that both the transaction price and the number of active bidders are observed. Finally, I extend these results to auctions with entry costs, which face a similar truncation issue when data on potential bidders who do not enter are missing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04464v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tonghui Qi</dc:creator>
    </item>
    <item>
      <title>Risk-Adjusted Policy Learning and the Social Cost of Uncertainty: Theory and Evidence from CAP evaluation</title>
      <link>https://arxiv.org/abs/2510.05007</link>
      <description>arXiv:2510.05007v1 Announce Type: new 
Abstract: This paper develops a risk-adjusted alternative to standard optimal policy learning (OPL) for observational data by importing Roy's (1952) safety-first principle into the treatment assignment problem. We formalize a welfare functional that maximizes the probability that outcomes exceed a socially required threshold and show that the associated pointwise optimal rule ranks treatments by the ratio of conditional means to conditional standard deviations. We implement the framework using microdata from the Italian Farm Accountancy Data Network to evaluate the allocation of subsidies under the EU Common Agricultural Policy. Empirically, risk-adjusted optimal policies systematically dominate the realized allocation across specifications, while risk aversion lowers overall welfare relative to the risk-neutral benchmark, making transparent the social cost of insurance against uncertainty. The results illustrate how safety-first OPL provides an implementable, interpretable tool for risk-sensitive policy design, quantifying the efficiency-insurance trade-off that policymakers face when outcomes are volatile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05007v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Cerulli, Francesco Caracciolo</dc:creator>
    </item>
    <item>
      <title>Improving S&amp;P 500 Volatility Forecasting through Regime-Switching Methods</title>
      <link>https://arxiv.org/abs/2510.03236</link>
      <description>arXiv:2510.03236v1 Announce Type: cross 
Abstract: Accurate prediction of financial market volatility is critical for risk management, derivatives pricing, and investment strategy. In this study, we propose a multitude of regime-switching methods to improve the prediction of S&amp;P 500 volatility by capturing structural changes in the market across time. We use eleven years of SPX data, from May 1st, 2014 to May 27th, 2025, to compute daily realized volatility (RV) from 5-minute intraday log returns, adjusted for irregular trading days. To enhance forecast accuracy, we engineered features to capture both historical dynamics and forward-looking market sentiment across regimes. The regime-switching methods include a soft Markov switching algorithm to estimate soft-regime probabilities, a distributional spectral clustering method that uses XGBoost to assign clusters at prediction time, and a coefficient-based soft regime algorithm that extracts HAR coefficients from time segments segmented through the Mood test and clusters through Bayesian GMM for soft regime weights, using XGBoost to predict regime probabilities. Models were evaluated across three time periods--before, during, and after the COVID-19 pandemic. The coefficient-based clustering algorithm outperformed all other models, including the baseline autoregressive model, during all time periods. Additionally, each model was evaluated on its recursive forecasting performance for 5- and 10-day horizons during each time period. The findings of this study demonstrate the value of regime-aware modeling frameworks and soft clustering approaches in improving volatility forecasting, especially during periods of heightened uncertainty and structural change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03236v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ava C. Blake, Nivika A. Gandhi, Anurag R. Jakkula</dc:creator>
    </item>
    <item>
      <title>Change-Point Testing for Risk Measures in Time Series</title>
      <link>https://arxiv.org/abs/1809.02303</link>
      <description>arXiv:1809.02303v3 Announce Type: replace 
Abstract: We propose novel methods for change-point testing for nonparametric estimators of expected shortfall and related risk measures in weakly dependent time series. We can detect general multiple structural changes in the tails of marginal distributions of time series under general assumptions. Self-normalization allows us to avoid the issues of standard error estimation. The theoretical foundations for our methods are functional central limit theorems, which we develop under weak assumptions. An empirical study of S&amp;P 500 and US Treasury bond returns illustrates the practical use of our methods in detecting and quantifying instability in the tails of financial time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.02303v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Fan, Junting Duan, Peter W. Glynn, Markus Pelger</dc:creator>
    </item>
    <item>
      <title>Finite Sample Inference in Incomplete Models</title>
      <link>https://arxiv.org/abs/2204.00473</link>
      <description>arXiv:2204.00473v3 Announce Type: replace 
Abstract: We propose confidence regions for the parameters of incomplete models with exact coverage of the true parameter in finite samples. Our confidence region inverts a test, which generalizes Monte Carlo tests to incomplete models. The test statistic is a discrete analogue of a new optimal transport characterization of the sharp identified region. Both test statistic and critical values rely on simulation drawn from the distribution of latent variables and are computed using solutions to discrete optimal transport, hence linear programming problems. We also propose a fast preliminary search in the parameter space with an alternative, more conservative yet consistent test, based on a parameter free critical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.00473v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lixiong Li, Marc Henry</dc:creator>
    </item>
    <item>
      <title>Identification- and many moment-robust inference via invariant moment conditions</title>
      <link>https://arxiv.org/abs/2303.07822</link>
      <description>arXiv:2303.07822v4 Announce Type: replace 
Abstract: Identification-robust hypothesis tests are commonly based on the continuous updating GMM objective function. When the number of moment conditions grows proportionally with the sample size, the large-dimensional weighting matrix prohibits the use of conventional asymptotic approximations and the behavior of these tests remains unknown. We show that the structure of the weighting matrix opens up an alternative route to asymptotic results when, under the null hypothesis, the distribution of the moment conditions satisfies a symmetry condition known as reflection invariance. We provide several examples in which the invariance follows from standard assumptions. Our results show that existing tests will be asymptotically conservative, and we propose an adjustment to attain nominal size in large samples. We illustrate our findings through simulations for various linear and nonlinear models, and an empirical application on the effect of the concentration of financial activities in banks on systemic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07822v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Boot, Johannes W. Ligtenberg</dc:creator>
    </item>
    <item>
      <title>Leveraging Uniformization and Sparsity for Computation and Estimation of Continuous Time Dynamic Discrete Choice Games</title>
      <link>https://arxiv.org/abs/2407.14914</link>
      <description>arXiv:2407.14914v2 Announce Type: replace 
Abstract: Continuous-time empirical dynamic discrete choice games offer notable computational advantages over discrete-time models. This paper addresses remaining computational challenges to further improve both model solution and maximum likelihood estimation. We establish convergence rates for value iteration and policy evaluation with fixed beliefs, and develop Newton-Kantorovich methods that exploit analytical Jacobians and sparse matrix structure. We apply uniformization both to derive a new representation of the value function that draws direct analogies to discrete-time models and to enable stable computation of the matrix exponential and its parameter derivatives for likelihood-based estimation with snapshot data. Critically, these methods provide a complete chain of analytical derivatives from the equilibrium value function through the log likelihood function, eliminating numerical approximations in both model solution and estimation and improving finite-sample statistical properties. Monte Carlo experiments demonstrate substantial gains in computational time and estimator accuracy, enabling estimation of richer models of strategic interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14914v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason R. Blevins</dc:creator>
    </item>
    <item>
      <title>Inference for the Marginal Value of Public Funds</title>
      <link>https://arxiv.org/abs/2410.00217</link>
      <description>arXiv:2410.00217v3 Announce Type: replace 
Abstract: Economists often estimate causal effects of policies on multiple outcomes and summarize them into scalar measures of cost-effectiveness or welfare, such as the Marginal Value of Public Funds (MVPF). In many settings, microdata underlying these estimates are unavailable, leaving researchers with only published estimates and their standard errors. We develop tools for valid inference on functions of causal effects, such as the MVPF, when the correlation structure is unknown. Our approach is to construct worst-case confidence intervals, leveraging experimental designs to tighten them, and to assess robustness using breakdown analyses. We illustrate our method with MVPFs for eight policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00217v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vedant Vohra</dc:creator>
    </item>
    <item>
      <title>On Selection of Cross-Section Averages in Non-stationary Environments</title>
      <link>https://arxiv.org/abs/2505.08615</link>
      <description>arXiv:2505.08615v4 Announce Type: replace 
Abstract: Information criteria (IC) have been widely used in factor models to estimate an unknown number of latent factors. It has recently been shown that IC perform well in Common Correlated Effects (CCE) and related setups in selecting a set of cross-section averages (CAs) sufficient for the factor space under stationary factors. As CAs can proxy non-stationary factors, it is tempting to claim such generality of IC, too. We show formally and in simulations that IC have a severe underselection issue even under very mild forms of factor non-stationarity, which goes against the sentiment in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08615v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Ditzen, Ovidijus Stauskas</dc:creator>
    </item>
    <item>
      <title>Causal Mediation in Natural Experiments</title>
      <link>https://arxiv.org/abs/2508.05449</link>
      <description>arXiv:2508.05449v2 Announce Type: replace 
Abstract: Natural experiments are a cornerstone of applied economics, providing settings for estimating causal effects with a compelling argument for treatment randomisation, but give little indication of the mechanisms behind causal effects. Causal Mediation (CM) is a framework for sufficiently identifying a mechanism behind the treatment effect, decomposing it into an indirect effect channel through a mediator mechanism and a remaining direct effect. By contrast, a suggestive analysis of mechanisms gives necessary but not sufficient evidence. Conventional CM methods require that the relevant mediator mechanism is as-good-as-randomly assigned; when people choose the mediator based on costs and benefits (whether to visit a doctor, to attend university, etc.), this assumption fails and conventional CM analyses are at risk of bias. I propose an alternative strategy that delivers unbiased estimates of CM effects despite unobserved selection, using instrumental variation in mediator take-up costs. The method identifies CM effects via the marginal effect of the mediator, with parametric or semi-parametric estimation that is simple to implement in two stages. Applying these methods to the Oregon Health Insurance Experiment reveals a substantial portion of the Medicaid lottery's effect on subjective health and well-being flows through increased healthcare usage -- an effect that a conventional CM analysis would mistake. This approach gives applied researchers an alternative method to estimate CM effects when an initial treatment is quasi-randomly assigned, but a mediator mechanism is not, as is common in natural experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05449v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Senan Hogan-Hennessy</dc:creator>
    </item>
    <item>
      <title>Optimal testing in a class of nonregular models</title>
      <link>https://arxiv.org/abs/2403.16413</link>
      <description>arXiv:2403.16413v2 Announce Type: replace-cross 
Abstract: This paper studies optimal hypothesis testing for nonregular econometric models with parameter-dependent support. We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on a limit experiment. Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness, and can be inverted to construct a confidence set for the nonregular parameter. Simulation results illustrate desirable finite sample properties of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16413v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Vector Copula Variational Inference and Dependent Block Posterior Approximations</title>
      <link>https://arxiv.org/abs/2503.01072</link>
      <description>arXiv:2503.01072v2 Announce Type: replace-cross 
Abstract: The key to VI is the selection of a tractable density to approximate the Bayesian posterior. For large and complex models a common choice is to assume independence between multivariate blocks in a partition of the parameter space. While this simplifies the problem it can reduce accuracy. This paper proposes using vector copulas to capture dependence between the blocks parsimoniously. Tailored multivariate marginals are constructed using learnable transport maps. We call the resulting joint distribution a ``dependent block posterior'' approximation. Vector copula models are suggested that make tractable and flexible variational approximations. They allow for differing marginals, numbers of blocks, block sizes and forms of between block dependence. They also allow for solution of the variational optimization using efficient stochastic gradient methods. The approach is demonstrated using four different statistical models and 16 datasets which have posteriors that are challenging to approximate. This includes models that use global-local shrinkage priors for regularization, and hierarchical models for smoothing and heteroscedastic time series. In all cases, our method produces more accurate posterior approximations than benchmark VI methods that either assume block independence or factor-based dependence, at limited additional computational cost. A python package implementing the method is available on GitHub at https://github.com/YuFuOliver/VCVI_Rep_PyPackage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01072v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Fu, Michael Stanley Smith, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v3 Announce Type: replace-cross 
Abstract: In the context of macroeconomic/financial time series, the FARS package provides a comprehensive framework in R for the construction of conditional densities of the variable of interest based on the factor-augmented quantile regressions (FA-QRs) methodology, with the factors extracted from multi-level dynamic factor models (ML-DFMs) with potential overlapping group-specific factors. Furthermore, the package also allows the construction of measures of risk as well as modeling and designing economic scenarios based on the conditional densities. In particular, the package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the FA-QRs together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; and (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v3</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
  </channel>
</rss>
