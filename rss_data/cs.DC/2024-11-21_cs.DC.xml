<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>TrustMesh: A Blockchain-Enabled Trusted Distributed Computing Framework for Open Heterogeneous IoT Environments</title>
      <link>https://arxiv.org/abs/2411.13039</link>
      <description>arXiv:2411.13039v1 Announce Type: new 
Abstract: The rapid evolution of Internet of Things (IoT) environments has created an urgent need for secure and trustworthy distributed computing systems, particularly when dealing with heterogeneous devices and applications where centralized trust cannot be assumed. This paper proposes TrustMesh, a novel blockchain-enabled framework that addresses these challenges through a unique three-layer architecture combining permissioned blockchain technology with a novel multi-phase Practical Byzantine Fault Tolerance (PBFT) consensus protocol. The key innovation lies in TrustMesh's ability to support non-deterministic scheduling algorithms while maintaining Byzantine fault tolerance - features traditionally considered mutually exclusive in blockchain systems. The framework supports a sophisticated resource management approach that enables flexible scheduling decisions while preserving the security guarantees of blockchain-based verification. Our experimental evaluation using a real-world cold chain monitoring scenario demonstrates that TrustMesh successfully maintains Byzantine fault tolerance with fault detection latencies under 150 milliseconds, while maintaining consistent framework overhead across varying computational workloads even with network scaling. These results establish TrustMesh's effectiveness in balancing security, performance, and flexibility requirements in trustless IoT environments, advancing the state-of-the-art in secure distributed computing frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13039v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murtaza Rangwala, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>ReinFog: A DRL Empowered Framework for Resource Management in Edge and Cloud Computing Environments</title>
      <link>https://arxiv.org/abs/2411.13121</link>
      <description>arXiv:2411.13121v1 Announce Type: new 
Abstract: The growing IoT landscape requires effective server deployment strategies to meet demands including real-time processing and energy efficiency. This is complicated by heterogeneous, dynamic applications and servers. To address these challenges, we propose ReinFog, a modular distributed software empowered with Deep Reinforcement Learning (DRL) for adaptive resource management across edge/fog and cloud environments. ReinFog enables the practical development/deployment of various centralized and distributed DRL techniques for resource management in edge/fog and cloud computing environments. It also supports integrating native and library-based DRL techniques for diverse IoT application scheduling objectives. Additionally, ReinFog allows for customizing deployment configurations for different DRL techniques, including the number and placement of DRL Learners and DRL Workers in large-scale distributed systems. Besides, we propose a novel Memetic Algorithm for DRL Component (e.g., DRL Learners and DRL Workers) Placement in ReinFog named MADCP, which combines the strengths of Genetic Algorithm, Firefly Algorithm, and Particle Swarm Optimization. Experiments reveal that the DRL mechanisms developed within ReinFog have significantly enhanced both centralized and distributed DRL techniques implementation. These advancements have resulted in notable improvements in IoT application performance, reducing response time by 45%, energy consumption by 39%, and weighted cost by 37%, while maintaining minimal scheduling overhead. Additionally, ReinFog exhibits remarkable scalability, with a rise in DRL Workers from 1 to 30 causing only a 0.3-second increase in startup time and around 2 MB more RAM per Worker. The proposed MADCP for DRL component placement further accelerates the convergence rate of DRL techniques by up to 38%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13121v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyu Wang, Mohammad Goudarzi, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Transforming the Hybrid Cloud for Emerging AI Workloads</title>
      <link>https://arxiv.org/abs/2411.13239</link>
      <description>arXiv:2411.13239v1 Announce Type: new 
Abstract: This white paper, developed through close collaboration between IBM Research and UIUC researchers within the IIDAI Institute, envisions transforming hybrid cloud systems to meet the growing complexity of AI workloads through innovative, full-stack co-design approaches, emphasizing usability, manageability, affordability, adaptability, efficiency, and scalability. By integrating cutting-edge technologies such as generative and agentic AI, cross-layer automation and optimization, unified control plane, and composable and adaptive system architecture, the proposed framework addresses critical challenges in energy efficiency, performance, and cost-effectiveness. Incorporating quantum computing as it matures will enable quantum-accelerated simulations for materials science, climate modeling, and other high-impact domains. Collaborative efforts between academia and industry are central to this vision, driving advancements in foundation models for material design and climate solutions, scalable multimodal data processing, and enhanced physics-based AI emulators for applications like weather forecasting and carbon sequestration. Research priorities include advancing AI agentic systems, LLM as an Abstraction (LLMaaA), AI model optimization and unified abstractions across heterogeneous infrastructure, end-to-end edge-cloud transformation, efficient programming model, middleware and platform, secure infrastructure, application-adaptive cloud systems, and new quantum-classical collaborative workflows. These ideas and solutions encompass both theoretical and practical research questions, requiring coordinated input and support from the research community. This joint initiative aims to establish hybrid clouds as secure, efficient, and sustainable platforms, fostering breakthroughs in AI-driven applications and scientific discovery across academia, industry, and society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13239v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deming Chen, Alaa Youssef, Ruchi Pendse, Andr\'e Schleife, Bryan K. Clark, Hendrik Hamann, Jingrui He, Teodoro Laino, Lav Varshney, Yuxiong Wang, Avirup Sil, Reyhaneh Jabbarvand, Tianyin Xu, Volodymyr Kindratenko, Carlos Costa, Sarita Adve, Charith Mendis, Minjia Zhang, Santiago N\'u\~nez-Corrales, Raghu Ganti, Mudhakar Srivatsa, Nam Sung Kim, Josep Torrellas, Jian Huang, Seetharami Seelam, Klara Nahrstedt, Tarek Abdelzaher, Tamar Eilam, Huimin Zhao, Matteo Manica, Ravishankar Iyer, Martin Hirzel, Vikram Adve, Darko Marinov, Hubertus Franke, Hanghang Tong, Elizabeth Ainsworth, Han Zhao, Deepak Vasisht, Minh Do, Fabio Oliveira, Giovanni Pacifici, Ruchir Puri, Priya Nagpurkar</dc:creator>
    </item>
    <item>
      <title>Sublinear-time Sampling of Spanning Trees in the Congested Clique</title>
      <link>https://arxiv.org/abs/2411.13334</link>
      <description>arXiv:2411.13334v1 Announce Type: new 
Abstract: We present the first sublinear round algorithm for approximately sampling uniform spanning trees in the CongestedClique model of distributed computing. In particular, our algorithm requires $\~O(n^{0.658})$ rounds for sampling a spanning tree from a distribution within total variation distance $1/n^c$, for arbitrary constant $c &gt; 0$, from the uniform distribution. More precisely, our algorithm requires $\~O(n^{1/2 + \alpha})$ rounds, where $O(n^\alpha)$ is the running time of matrix multiplication in the CongestedClique model, currently at $\alpha = 1 - 2/\omega = 0.158$, where $\omega$ is the sequential matrix multiplication time exponent.
  In addition, we show how to take somewhat shorter random walks even more efficiently in the CongestedClique model. Specifically, we show how to construct length-$\tau$ walks, for $\tau = \Omega(n/\log n)$, in $O\left(\frac{\tau}{n} \log \tau \log n\right)$ rounds and for $\tau = O(n/\log n)$ in $O(\log \tau)$ rounds. This implies an $O(\log^3 n)$-round algorithm in the CongestedClique model for sampling spanning trees for Erd\H{o}s-R\'enyi graphs and regular expander graphs due to the $O(n \log n)$ bound on their cover time. This also implies that polylogarithmic-length walks, which are useful for page rank estimation, can be constructed in $O(\log \log n)$ rounds in the CongestedClique model. These results are obtained by adding a load balancing component to the random walk algorithm of Bahmani, Chakrabarti and Xin (SIGMOD 2011) that uses the ``doubling'' technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13334v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sriram V. Pemmaraju, Sourya Roy, Joshua Z. Sobel</dc:creator>
    </item>
    <item>
      <title>Distributed weak independent sets in hypergraphs: Upper and lower bounds</title>
      <link>https://arxiv.org/abs/2411.13377</link>
      <description>arXiv:2411.13377v1 Announce Type: new 
Abstract: In this paper, we consider the problem of finding weak independent sets in a distributed network represented by a hypergraph. In this setting, each edge contains a set of r vertices rather than simply a pair, as in a standard graph. A k-weak independent set in a hypergraph is a set where no edge contains more than k vertices in the independent set. We focus two variations of this problem. First, we study the problem of finding k-weak maximal independent sets, k-weak independent sets where each vertex belongs to at least one edge with k vertices in the independent set. Second we introduce a weaker variant that we call (\alpha, \beta)-independent sets where the independent set is \beta-weak, and each vertex belongs to at least one edge with at least \alpha vertices in the independent set. Finally, we consider the problem of finding a (2, k)-ruling set on hypergraphs, i.e. independent sets where no vertex is a distance of more than k from the nearest member of the set.
  Given a hypergraph H of rank r and maximum degree \Delta, we provide a LLL formulation for finding an (\alpha, \beta)-independent set when (\beta - \alpha)^2 / (\beta + \alpha) \geq 6 \log(16 r \Delta), an O(\Delta r / (\beta - \alpha + 1) + \log^* n) round deterministic algorithm finding an (\alpha, \beta)-independent set, and a O(\Delta^2(r - k) \log r + \Delta \log r \log^* r + \log^* n) round algorithm for finding a k-weak maximal independent set. Additionally, we provide zero round randomized algorithms for finding (\alpha, \beta) independent sets, when (\beta - \alpha)^2 / (\beta + \alpha) \geq 6 c \log n + 6 for some constant c, and finding an m-weak independent set for some m \geq r / 2k where k is a given parameter. Finally, we provide lower bounds of \Omega(\Delta + \log^* n) and \Omega(r + \log^* n) on the problems of finding a k-weak maximal independent sets for some values of k.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13377v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duncan Adamson, Will Rosenbaum, Paul G. Spirakis</dc:creator>
    </item>
    <item>
      <title>A Case Study of API Design for Interoperability and Security of the Internet of Things</title>
      <link>https://arxiv.org/abs/2411.13441</link>
      <description>arXiv:2411.13441v1 Announce Type: new 
Abstract: Heterogeneous distributed systems, including the Internet of Things (IoT) or distributed cyber-physical systems (CPS), often suffer a lack of interoperability and security, which hinders the wider deployment of such systems. Specifically, the different levels of security requirements and the heterogeneity in terms of communication models, for instance, point-to-point vs. publish-subscribe, are the example challenges of IoT and distributed CPS consisting of heterogeneous devices and applications. In this paper, we propose a working application programming interface (API) and runtime to enhance interoperability and security while addressing the challenges that stem from the heterogeneity in the IoT and distributed CPS. In our case study, we design and implement our application programming interface (API) design approach using open-source software, and with our working implementation, we evaluate the effectiveness of our proposed approach. Our experimental results suggest that our approach can achieve both interoperability and security in the IoT and distributed CPS with a reasonably small overhead and better-managed software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13441v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dongha Kim, Chanhee Lee, Hokeun Kim</dc:creator>
    </item>
    <item>
      <title>A Distributed-memory Tridiagonal Solver Based on a Specialised Data Structure Optimised for CPU and GPU Architectures</title>
      <link>https://arxiv.org/abs/2411.13532</link>
      <description>arXiv:2411.13532v1 Announce Type: new 
Abstract: Various numerical methods used for solving partial differential equations (PDE) result in tridiagonal systems. Solving tridiagonal systems on distributed-memory environments is not straightforward, and often requires significant amount of communication. In this article, we present a novel distributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a specialised data structure. DistD2-TDS algorithm takes advantage of the diagonal dominance in tridiagonal systems to reduce the communications in distributed-memory environments. The underlying data structure plays a crucial role for the performance of the algorithm. First, the data structure improves data localities and makes it possible to minimise data movements via cache blocking and kernel fusion strategies. Second, data continuity enables a contiguous data access pattern and results in efficient utilisation of the available memory bandwidth. Finally, the data layout supports vectorisation on CPUs and thread level parallelisation on GPUs for improved performance. In order to demonstrate the robustness of the algorithm, we implemented and benchmarked the algorithm on CPUs and GPUs. We investigated the single rank performance and compared against existing algorithms. Furthermore, we analysed the strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to 8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the algorithm by using compact finite difference schemes to solve a 3D non-linear PDE. The results demonstrate that DistD2 algorithm can sustain around 66% of the theoretical peak bandwidth at scale on CPU and GPU based supercomputers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13532v1</guid>
      <category>cs.DC</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Semih Akkurt, S\'ebastien Lemaire, Paul Bartholomew, Sylvain Laizet</dc:creator>
    </item>
    <item>
      <title>Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training</title>
      <link>https://arxiv.org/abs/2411.13055</link>
      <description>arXiv:2411.13055v1 Announce Type: cross 
Abstract: Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from certain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for large model training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13055v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared Fernandez, Luca Wehrstedt, Leonid Shamis, Mostafa Elhoushi, Kalyan Saladi, Yonatan Bisk, Emma Strubell, Jacob Kahn</dc:creator>
    </item>
    <item>
      <title>AMECOS: A Modular Event-based Framework for Concurrent Object Specification</title>
      <link>https://arxiv.org/abs/2405.10057</link>
      <description>arXiv:2405.10057v2 Announce Type: replace 
Abstract: In this work, we introduce a modular framework for specifying distributed systems that we call AMECOS. Specifically, our framework departs from the traditional use of sequential specification, which presents limitations both on the specification expressiveness and implementation efficiency of inherently concurrent objects, as documented by Casta\~neda, Rajsbaum and Raynal in CACM 2023. Our framework focuses on the interactions between the various system components, specified as concurrent objects. Interactions are described with sequences of object events. This provides a modular way of specifying distributed systems and separates legality (object semantics) from other issues, such as consistency. We demonstrate the usability of our framework by (i) specifying various well-known concurrent objects, such as registers, shared memory, message-passing, reliable broadcast, and consensus, (ii) providing hierarchies of ordering semantics (namely, consistency hierarchy, memory hierarchy, and reliable broadcast hierarchy), and (iii) presenting a novel axiomatic proof of the impossibility of the well-known Consensus problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10057v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'e Albouy (IRISA), Antonio Fern\'andez Anta (UCY), Chryssis Georgiou (UCY), Mathieu Gestin, Nicolas Nicolaou, Junlang Wang</dc:creator>
    </item>
    <item>
      <title>Auxiliary MCMC and particle Gibbs samplers for parallelisable inference in latent dynamical systems</title>
      <link>https://arxiv.org/abs/2303.00301</link>
      <description>arXiv:2303.00301v2 Announce Type: replace-cross 
Abstract: We study the problem of designing efficient exact MCMC algorithms for sampling from the full posterior distribution of high-dimensional (in the number of time steps and the dimension of the latent space) non-linear non-Gaussian latent dynamical models. Particle Gibbs, also known as conditional sequential Monte Carlo (SMC), constitutes the de facto golden standard to do so, but suffers from degeneracy problems when the dimension of the latent space increases. On the other hand, the routinely employed globally Gaussian-approximated (e.g., extended Kalman filtering) biased solutions are seldom used for this same purpose even though they are more robust than their SMC counterparts. In this article, we show how, by introducing auxiliary observation variables in the model, we can both implement efficient exact Kalman-based samplers for large state-space models, as well as dramatically improve the mixing speed of particle Gibbs algorithms when the dimension of the latent space increases. We demonstrate when and how we can parallelise these auxiliary samplers along the time dimension, resulting in algorithms that scale logarithmically with the number of time steps when implemented on graphics processing units (GPUs). Both algorithms are easily tuned and can be extended to accommodate sophisticated approximation techniques. We demonstrate the improved statistical and computational performance of our auxiliary samplers compared to state-of-the-art alternatives for high-dimensional (in both time and state space) latent dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00301v2</guid>
      <category>stat.CO</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Simo S\"arkk\"a</dc:creator>
    </item>
  </channel>
</rss>
