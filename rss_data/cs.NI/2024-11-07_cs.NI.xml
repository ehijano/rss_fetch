<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Nov 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>xApp-Level Conflict Mitigation in O-RAN, a Mobility Driven Energy Saving Case</title>
      <link>https://arxiv.org/abs/2411.03326</link>
      <description>arXiv:2411.03326v1 Announce Type: new 
Abstract: This paper investigates the emerging challenges of conflict detection and mitigation in Open Radio Access Network (O-RAN). Conflicts between xApps can arise that affect network performance and stability due to the disaggregated nature of O-RAN. This work provides a detailed theoretical framework of Extended Application (xApp)-level conflicts, i.e., direct, indirect, and implicit conflicts. Leveraging conflict graphs, we further highlight how conflicts impact Key Performance Indicators (KPIs) and explore strategies for conflict detection using Service Level Agreements (SLAs) and Quality of Service (QoS) thresholds. We evaluate the effectiveness of several mitigation strategies in a simulated environment with Mobility Robustness Optimization (MRO) and Energy Saving (ES) xApps and present experimental results showing comparisons among these strategies. The findings of this research provide significant insights for enhancing O-RAN deployments with flexible and efficient conflict management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03326v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Wadud, Fatemeh Golpayegani, Nima Afraz</dc:creator>
    </item>
    <item>
      <title>TwiNet: Connecting Real World Networks to their Digital Twins Through a Live Bidirectional Link</title>
      <link>https://arxiv.org/abs/2411.03503</link>
      <description>arXiv:2411.03503v1 Announce Type: new 
Abstract: Only the chairs can edit The wireless spectrum's increasing complexity poses challenges and opportunities, highlighting the necessity for real-time solutions and robust data processing capabilities. Digital Twin (DT), virtual replicas of physical systems, integrate real-time data to mirror their real-world counterparts, enabling precise monitoring and optimization. Incorporating DTs into wireless communication enhances predictive maintenance, resource allocation, and troubleshooting, thus bolstering network reliability. Our paper introduces TwiNet, enabling bidirectional, near-realtime links between real-world wireless spectrum scenarios and DT replicas. Utilizing the protocol, MQTT, we can achieve data transfer times with an average latency of 14 ms, suitable for real-time communication. This is confirmed by monitoring real-world traffic and mirroring it in real-time within the DT's wireless environment. We evaluate TwiNet's performance in two use cases: (i) assessing risky traffic configurations of UEs in a Safe Adaptive Data Rate (SADR) system, improving network performance by approximately 15% compared to original network selections; and (ii) deploying new CNNs in response to jammed pilots, achieving up to 97% accuracy training on artificial data and deploying a new model in as low as 2 minutes to counter persistent adversaries. TwiNet enables swift deployment and adaptation of DTs, addressing crucial challenges in modern wireless communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03503v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clifton Paul Robinson, Andrea Lacava, Pedram Johari, Francesca Cuomo, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Model-based Deep Learning for QoS-Aware Rate-Splitting Multiple Access Wireless Systems</title>
      <link>https://arxiv.org/abs/2411.03507</link>
      <description>arXiv:2411.03507v1 Announce Type: new 
Abstract: Next generation communications demand for better spectrum management, lower latency, and guaranteed quality-of-service (QoS). Recently, Artificial intelligence (AI) has been widely introduced to advance these aspects in next generation wireless systems. However, such AI applications suffer from limited training data, low robustness, and poor generalization capabilities. To address these issues, a model-driven deep unfolding (DU) algorithm is introduced in this paper to bridge the gap between traditional model-driven communication algorithms and data-driven deep learning. Focusing on the QoS-aware rate-splitting multiple access (RSMA) resource allocation problem in multi-user communications, a conventional fractional programming (FP) algorithm is first applied as a benchmark. The solution is then refined by the application of projection gradient descent (PGD). DU is employed to further speed up convergence procedure, hence improving the efficiency of PGD. Moreover, the feasibility of results is guaranteed by designing a low-complexity projection based on scale factors, plus adding violation control mechanisms into the loss function that minimizes error rates. Finally, we provide a detailed analysis of the computational complexity and analysis design of the proposed DU algorithm. Extensive simulations are conducted and the results demonstrate that the proposed DU algorithm can reach the optimal communication efficiency with a mere $0.024\%$ violation rate for 4 layers DU. The DU algorithm also exhibits robustness in out-of-distribution tests and can be effectively trained with as few as 50 samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03507v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanwen Zhang, Mingzhe Chen, Alireza Vahid, Feng Ye, Haijian Sun</dc:creator>
    </item>
    <item>
      <title>Potential Use of IoT Distance Measurement Tool in Boule Sports</title>
      <link>https://arxiv.org/abs/2411.03585</link>
      <description>arXiv:2411.03585v1 Announce Type: new 
Abstract: In Petanque, each player aims to throw the boule closer to the jack. The closest boule to the jack among players will score the point. Currently, the distance of the boule to the jack is still measured using manual measurement tools such as measuring tape, string, and calipers. The manual measurement method is considered time-consuming and prone to inconsistent reading, which the ordinary referees and players conduct. A steady hand is required to hold the tape at two ends while squatting or kneeling. The technique of reading the measurement is also important to determine the accuracy of the length. This project aims to design and develop a prototype device that can measure the distance between jack and boule using a microcontroller and ultrasonic sensor technology. The device is expected to provide an instant measurement of the distance between the jack and the boule. The measurement data can be displayed on the mobile device to ease the user to view the result. This prototype device also counts the score points and determines the winner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03585v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijwmn.2024.16501</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Wireless &amp; Mobile Networks (IJWMN), Vol.16, No.4/5. Oct. 2024</arxiv:journal_reference>
      <dc:creator>Wahidah Md Shah, M Azim. Adnan, Aslinda Hassan, Norharyati Harum, Isredza Rahmi A. Hamid</dc:creator>
    </item>
    <item>
      <title>Digital Twin-Assisted Robust and Adaptive Resource Slicing in LEO Satellite Networks</title>
      <link>https://arxiv.org/abs/2411.03635</link>
      <description>arXiv:2411.03635v1 Announce Type: new 
Abstract: Resource slicing in low Earth orbit satellite networks (LSN) is essential to support diversified services. In this paper, we investigate a resource slicing problem in LSN to reserve resources in satellites to achieve efficient resource provisioning. To address the challenges of non-stationary service demands, inaccurate prediction, and satellite mobility, we propose an adaptive digital twin (DT)-assisted resource slicing scheme for robust and adaptive resource management in LSN. Specifically, a slice DT, being able to capture the service demand prediction uncertainty through collected service demand data, is constructed to enhance the robustness of resource slicing decisions for dynamic service demands. In addition, the constructed DT can emulate resource slicing decisions for evaluating their performance, enabling adaptive slicing decision updates to efficiently reserve resources in LSN. Simulation results demonstrate that the proposed scheme outperforms benchmark methods, achieving low service demand violations with efficient resource consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03635v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingcheng He, Huaqing Wu, Conghao Zhou, Shisheng Hu, Zhixuan Tang, Weihua Zhuang</dc:creator>
    </item>
    <item>
      <title>PyroGuardian: An IoT-Enabled System for Health and Location Monitoring in High-Risk Firefighting Environments</title>
      <link>https://arxiv.org/abs/2411.03654</link>
      <description>arXiv:2411.03654v1 Announce Type: new 
Abstract: First responders risk their lives to reduce property damage and prevent injuries during disasters. Among first responders, firefighters work with fires in residential properties, forests, or other locations where fire occurs. We built the PyroGuardian system that uses wearable modules to transmit unit information over Long Range (LoRa) to an Android tablet. The tablet runs our application, PyroPortal, to assign each firefighter's stats, such as body temperature, heart rate, and GPS location. PyroPortal displays this information on unit dashboards, and markers on Google Maps represent the firefighter's location and the direction they are facing. These dashboards can help the incident commander (IC) make more informed decisions on mission control operations and remove specific units whose health stats, such as oximeter and pulse, passed certain thresholds. PyroGuardian completes all these tasks at an affordable cost and in an impressive maximum range between the units and IC. In addition, PyroGuardian has various application scenarios, such as law enforcement and military operations, besides firefighting. We also conducted a sample mission inside a burning building while real firefighters watched. After the demonstration, they completed a survey on system usability and PyroGuardian's potential to meet their requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03654v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berkay Kaplan, Buhe Li</dc:creator>
    </item>
    <item>
      <title>Learn to Slice, Slice to Learn: Unveiling Online Optimization and Reinforcement Learning for Slicing AI Services</title>
      <link>https://arxiv.org/abs/2411.03686</link>
      <description>arXiv:2411.03686v1 Announce Type: new 
Abstract: In the face of increasing demand for zero-touch networks to automate network management and operations, two pivotal concepts have emerged: "Learn to Slice" (L2S) and "Slice to Learn" (S2L). L2S involves leveraging Artificial intelligence (AI) techniques to optimize network slicing for general services, while S2L centers on tailoring network slices to meet the specific needs of various AI services. The complexity of optimizing and automating S2L surpasses that of L2S due to intricate AI services' requirements, such as handling uncontrollable parameters, learning in adversarial conditions, and achieving long-term performance goals. This paper aims to automate and optimize S2L by integrating the two concepts of L2S and S2L by using an intelligent slicing agent to solve S2L. Indeed, we choose two candidate slicing agents, namely the Exploration and Exploitation (EXP3) and Deep Q-Network (DQN) from the Online Convex Optimization (OCO) and Deep Reinforcement Learning (DRL) frameworks, and compare them. Our evaluation involves a series of carefully designed experiments that offer valuable insights into the strengths and limitations of EXP3 and DQN in slicing for AI services, thereby contributing to the advancement of zero-touch network capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03686v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amr Abo-eleneen, Menna Helmy, Alaa Awad Abdellatif, Aiman Erbad, Amr Mohamed, Mohamed Abdallah</dc:creator>
    </item>
    <item>
      <title>Fundamental Limits of Routing Attack on Network Overload</title>
      <link>https://arxiv.org/abs/2411.03749</link>
      <description>arXiv:2411.03749v1 Announce Type: new 
Abstract: We quantify the threat of network adversaries to inducing \emph{network overload} through \emph{routing attacks}, where a subset of network nodes are hijacked by an adversary. We develop routing attacks on the hijacked nodes for two objectives related to overload: \emph{no-loss throughput minimization} and \emph{loss maximization}. The first objective attempts to identify a routing attack that minimizes the network's throughput that is guaranteed to survive. We develop a polynomial-time algorithm that can output the optimal routing attack in multi-hop networks with global information on the network's topology, and an algorithm with an approximation ratio of $2$ under partial information. The second objective attempts to maximize the throughput loss. We demonstrate that this problem is NP-hard, and develop two approximation algorithms with multiplicative and additive guarantees respectively in single-hop networks. We further investigate the adversary's optimal selection of nodes to hijack that can maximize network overload. We propose a heuristic polynomial-time algorithm to solve this NP-hard problem, and prove its optimality in special cases. We validate the near-optimal performance of the proposed algorithms over a wide range of network settings. Our results demonstrate that the proposed algorithms can accurately quantify the risk of overload given an arbitrary set of hijacked nodes and identify the critical nodes that should be protected against routing attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03749v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Wu, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>An Ordinary Differential Equation Framework for Stability Analysis of Networks with Finite Buffers</title>
      <link>https://arxiv.org/abs/2411.03780</link>
      <description>arXiv:2411.03780v1 Announce Type: new 
Abstract: We consider the problem of network stability in finite-buffer systems. We observe that finite buffer may affect stability even in simplest network structure, and we propose an ordinary differential equation (ODE) model to capture the queuing dynamics and analyze the stability in buffered communication networks with general topology. For single-commodity systems, we propose a sufficient condition, which follows the fundamental idea of backpressure, for local transmission policies to stabilize the networks based on ODE stability theory. We further extend the condition to multi-commodity systems, with an additional restriction on the coupling level between different commodities, which can model networks with per-commodity buffers and shared buffers. The framework characterizes a set of policies that can stabilize buffered networks, and is useful for analyzing the effect of finite buffers on network stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03780v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Wu, Dan Wu, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Towards Achieving Energy Efficiency and Service Availability in O-RAN via Formal Verification</title>
      <link>https://arxiv.org/abs/2411.03943</link>
      <description>arXiv:2411.03943v1 Announce Type: new 
Abstract: As Open Radio Access Networks (O-RAN) continue to expand, AI-driven applications (xApps) are increasingly being deployed enhance network management. However, developing xApps without formal verification risks introducing logical inconsistencies, particularly in balancing energy efficiency and service availability. In this paper, we argue that prior to their development, the formal analysis of xApp models should be a critical early step in the O-RAN design process. Using the PRISM model checker, we demonstrate how our results provide realistic insights into the thresholds between energy efficiency and service availability. While our models are simplified, the findings highlight how AI-informed decisions can enable more effective cell-switching policies. We position formal verification as an essential practice for future xApp development, avoiding fallacies in real-world applications and ensuring networks operate efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03943v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Metere, Kangfeng Ye, Yue Gu, Zhi Zhang, Dalal Alrajeh, Michele Sevegnani, Poonam Yadav</dc:creator>
    </item>
    <item>
      <title>Prototyping O-RAN Enabled UAV Experimentation for the AERPAW Testbed</title>
      <link>https://arxiv.org/abs/2411.04027</link>
      <description>arXiv:2411.04027v1 Announce Type: new 
Abstract: The Open Radio Access Network (O-RAN) architecture is reshaping the telecommunications landscape by enhancing network flexibility, openness, and intelligence. This paper establishes the requirements, evaluates the design tradeoffs, and introduces a scalable architecture and prototype of an open-source O-RAN experimentation platform within the Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW), an at scale testbed that integrates unmanned aerial vehicles (UAVs) with advanced wireless network technologies, offering experimentation in both outdoor testbed and emulation via a custom digital twin (DT). Through a series of aerial experiments, we evaluate FlexRIC, an open-source RAN Intelligent Controller, within the AERPAW hardware-software platform for network data monitoring, providing valuable insights into the proposed integration and revealing opportunities for leveraging O-RAN to create custom service based optimizations for cellular connected UAVs. We discuss the challenges and potential use cases of this integration and demonstrate the use of a generative artificial intelligence model for generating realistic data based on collected real-world data to support AERPAW's DT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04027v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Moore, Aly Sabri Abdalla, Charles Ueltschey, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks</title>
      <link>https://arxiv.org/abs/2411.03354</link>
      <description>arXiv:2411.03354v1 Announce Type: cross 
Abstract: In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100\% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6\% in both classification accuracy and recall.The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03354v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederic Adjewa, Moez Esseghir, Leila Merghem-Boulahia</dc:creator>
    </item>
    <item>
      <title>Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis</title>
      <link>https://arxiv.org/abs/2411.03365</link>
      <description>arXiv:2411.03365v1 Announce Type: cross 
Abstract: In the rapidly evolving landscape of 5G technology, safeguarding Radio Frequency (RF) environments against sophisticated intrusions is paramount, especially in dynamic spectrum access and management. This paper presents an enhanced experimental model that integrates a self-attention mechanism with a Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous spectral activities in 5G networks at the waveform level. Our approach, grounded in time-series analysis, processes in-phase and quadrature (I/Q) samples to identify irregularities that could indicate potential jamming attacks. The model's architecture, augmented with a self-attention layer, extends the capabilities of RNN autoencoders, enabling a more nuanced understanding of temporal dependencies and contextual relationships within the RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a comprehensive stream of data that reflects real-world RF spectrum conditions and attack scenarios. The model is trained to reconstruct standard signal behavior, establishing a normative baseline against which deviations, indicative of security threats, are identified. The proposed architecture is designed to balance between detection precision and computational efficiency, so the LSTM network, enriched with self-attention, continues to optimize for minimal execution latency and power consumption. Conducted on a real-world SDR-based testbed, our results demonstrate the model's improved performance and accuracy in threat detection.
  Keywords: self-attention, real-time intrusion detection, RNN autoencoder, Transformer architecture, LSTM, time series anomaly detection, 5G Security, spectrum access security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03365v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammadreza Kouchaki, Minglong Zhang, Aly S. Abdalla, Guangchen Lan, Christopher G. Brinton, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>Personal Data Protection in AI-Native 6G Systems</title>
      <link>https://arxiv.org/abs/2411.03368</link>
      <description>arXiv:2411.03368v1 Announce Type: cross 
Abstract: As 6G evolves into an AI-native technology, the integration of artificial intelligence (AI) and Generative AI into cellular communication systems presents unparalleled opportunities for enhancing connectivity, network optimization, and personalized services. However, these advancements also introduce significant data protection challenges, as AI models increasingly depend on vast amounts of personal data for training and decision-making. In this context, ensuring compliance with stringent data protection regulations, such as the General Data Protection Regulation (GDPR), becomes critical for the design and operational integrity of 6G networks. These regulations shape key system architecture aspects, including transparency, accountability, fairness, bias mitigation, and data security.
  This paper identifies and examines the primary data protection risks associated with AI-driven 6G networks, focusing on the complex data flows and processing activities throughout the 6G lifecycle. By exploring these risks, we provide a comprehensive analysis of the potential privacy implications and propose effective mitigation strategies. Our findings stress the necessity of embedding privacy-by-design and privacy-by-default principles in the development of 6G standards to ensure both regulatory compliance and the protection of individual rights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03368v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keivan Navaie</dc:creator>
    </item>
    <item>
      <title>Blockchain-Based Multi-Path Mobile Access Point Selection for Secure 5G VANETs</title>
      <link>https://arxiv.org/abs/2411.03371</link>
      <description>arXiv:2411.03371v1 Announce Type: cross 
Abstract: This letter presents a blockchain-based multi-path mobile access point (MAP) selection strategy for secure 5G vehicular ad-hoc networks (VANETs). The proposed method leverages blockchain technology for decentralized, transparent, and secure MAP selection, while the multi-path transmission strategy enhances network reliability and reduces communication delays. A trust-based attack detection mechanism is integrated to ensure network security. Simulation results demonstrate that the proposed algorithm reduces both handover frequency and average communication delay by over 80%, and successfully identifies and excludes more than 95% of Sybil nodes, ensuring reliable and secure communication in highly dynamic vehicular environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03371v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiou Zhang, Weian Guo, Li Li, Dongyang Li</dc:creator>
    </item>
    <item>
      <title>An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services</title>
      <link>https://arxiv.org/abs/2411.03376</link>
      <description>arXiv:2411.03376v1 Announce Type: cross 
Abstract: This article presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03376v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCC.2024.3398609</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Cloud Computing ( Volume: 12, Issue: 2, April-June 2024)</arxiv:journal_reference>
      <dc:creator>Zerui Wang, Yan Liu, Jun Huang</dc:creator>
    </item>
    <item>
      <title>Age of Gossip With Time-Varying Topologies</title>
      <link>https://arxiv.org/abs/2411.04114</link>
      <description>arXiv:2411.04114v1 Announce Type: cross 
Abstract: We consider a gossiping network, where a source node sends updates to a network of $n$ gossiping nodes. Meanwhile, the connectivity topology of the gossiping network changes over time, among a finite number of connectivity ''states,'' such as the fully connected graph, the ring graph, the grid graph, etc. The transition of the connectivity graph among the possible options is governed by a finite state continuous time Markov chain (CTMC). When the CTMC is in a particular state, the associated graph topology of the gossiping network is in the way indicated by that state. We evaluate the impact of time-varying graph topologies on the freshness of information for nodes in the network. We use the version age of information metric to quantify the freshness of information at the nodes. Using a method similar to the first passage percolation method, we show that, if one of the states of the CTMC is the fully connected graph and the transition rates of the CTMC are constant, then the version age of a typical node in the network scales logarithmically with the number of nodes, as in the case if the network was always fully connected. That is, there is no loss in the age scaling, even if the network topology deviates from full connectivity, in this setting. We perform numerical simulations and analyze more generally how having different topologies and different CTMC rates (that might depend on the number of nodes) affect the average version age scaling of a node in the gossiping network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04114v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arunabh Srivastava, Thomas Jacob Maranzatto, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Queueing Delay Minimization in Overloaded Networks</title>
      <link>https://arxiv.org/abs/2312.04054</link>
      <description>arXiv:2312.04054v2 Announce Type: replace 
Abstract: We develop link rate control policies to minimize the queueing delay of packets in overloaded networks. We show that increasing link rates does not guarantee delay reduction during overload. We consider a fluid queueing model that facilitates explicit characterization of the queueing delay of packets, and establish explicit conditions on link rates that can minimize the average and maximum queueing delay in both single-hop and multi-stage (switching) networks. These min-delay conditions require maintaining an identical ratio between the ingress and egress rates of different nodes at the same layer of the network. We term the policies that follow these conditions rate-proportional policies. We further generalize the rate-proportional policies to queue-proportional policies, which minimize the queueing delay asymptotically based on the time-varying queue length while remaining agnostic of packet arrival rates. We validate that the proposed policies lead to minimum queueing delay under various network topologies and settings, compared with benchmarks including the backpressure policy that maximizes network throughput and the max-link-rate policy that fully utilizes bandwidth. We further remark that the explicit min-delay policy design in multi-stage networks facilitates co-optimization with other metrics, such as minimizing total bandwidth, balancing link utilization and node buffer usage. This demonstrates the wider utility of our main results in data center network optimization in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04054v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Wu, Dan Wu, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Optimizing AoI at Query in Multiuser Wireless Uplink Networks: A Whittle Index Approach</title>
      <link>https://arxiv.org/abs/2411.02108</link>
      <description>arXiv:2411.02108v3 Announce Type: replace-cross 
Abstract: In this paper, we explore how to schedule multiple users to optimize information freshness in a pull-based wireless network, where the status updates from users are requested by randomly arriving queries at the destination. We use the age of information at query (QAoI) to characterize the performance of information freshness. Such a decision-making problem is naturally modeled as a Markov decision process (MDP), which, however, is prohibitively high to be solved optimally by the standard method due to the curse of dimensionality. To address this issue, we employ Whittle index approach, which allows us to decouple the original MDP into multiple sub-MDPs by relaxing the scheduling constraints. However, the binary Markovian query arrival process results in a bi-dimensional state and complex state transitions within each sub-MDP, making it challenging to verify Whittle indexability using conventional methods. After a thorough analysis of the sub-MDP's structure, we show that it is unichain and its optimal policy follows a threshold-type structure. This facilitates the verification of Whittle indexability of the sub-MDP by employing an easy-to-verify condition. Subsequently, the steady-state probability distributions of the sub-MDP under different threshold-type policies are analyzed, constituting the analytical expressions of different Whittle indices in terms of the expected average QAoI and scheduling time of the sub-MDP. Building on these, we devise an efficient algorithm to calculate Whittle indices for the formulated sub-MDPs. The simulation results validate our analyses and show the proposed Whittle index policy outperforms baseline policies and achieves near-optimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02108v3</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Liu, He Chen</dc:creator>
    </item>
  </channel>
</rss>
