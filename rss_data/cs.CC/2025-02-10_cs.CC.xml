<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 04:04:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Completeness Theorems for k-SUM and Geometric Friends: Deciding Fragments of Integer Linear Arithmetic</title>
      <link>https://arxiv.org/abs/2502.04581</link>
      <description>arXiv:2502.04581v1 Announce Type: new 
Abstract: In the last three decades, the $k$-SUM hypothesis has emerged as a satisfying explanation of long-standing time barriers for a variety of algorithmic problems. Yet to this day, the literature knows of only few proven consequences of a refutation of this hypothesis. Taking a descriptive complexity viewpoint, we ask: What is the largest logically defined class of problems \emph{captured} by the $k$-SUM problem?
  To this end, we introduce a class $\mathsf{FOP}_{\mathbb{Z}}$ of problems corresponding to deciding sentences in Presburger arithmetic/linear integer arithmetic over finite subsets of integers.
  We establish two large fragments for which the $k$-SUM problem is complete under fine-grained reductions:
  1. The $k$-SUM problem is complete for deciding the sentences with $k$ existential quantifiers.
  2. The $3$-SUM problem is complete for all $3$-quantifier sentences of $\mathsf{FOP}_{\mathbb{Z}}$ expressible using at most $3$ linear inequalities.
  Specifically, a faster-than-$n^{\lceil k/2 \rceil \pm o(1)}$ algorithm for $k$-SUM (or faster-than-$n^{2 \pm o(1)}$ algorithm for $3$-SUM, respectively) directly translate to polynomial speedups of a general algorithm for \emph{all} sentences in the respective fragment.
  Observing a barrier for proving completeness of $3$-SUM for the entire class $\mathsf{FOP}_{\mathbb{Z}}$, we turn to the question which other -- seemingly more general -- problems are complete for $\mathsf{FOP}_{\mathbb{Z}}$. In this direction, we establish $\mathsf{FOP}_{\mathbb{Z}}$-completeness of the \emph{problem pair} of Pareto Sum Verification and Hausdorff Distance under $n$ Translations under the $L_\infty$/$L_1$ norm in $\mathbb{Z}^d$. In particular, our results invite to investigate Pareto Sum Verification as a high-dimensional generalization of 3-SUM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04581v1</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.LO</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geri Gokaj, Marvin K\"unnemann</dc:creator>
    </item>
    <item>
      <title>Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives</title>
      <link>https://arxiv.org/abs/2502.04358</link>
      <description>arXiv:2502.04358v1 Announce Type: cross 
Abstract: Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04358v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot Meyerson, Xin Qiu</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Noisy Computation of High-Influence Functions, Connectivity, and Threshold</title>
      <link>https://arxiv.org/abs/2502.04632</link>
      <description>arXiv:2502.04632v1 Announce Type: cross 
Abstract: In the noisy query model, the (binary) return value of every query (possibly repeated) is independently flipped with some fixed probability $p \in (0, 1/2)$. In this paper, we obtain tight bounds on the noisy query complexity of several fundamental problems.
  Our first contribution is to show that any Boolean function with total influence $\Omega(n)$ has noisy query complexity $\Theta(n\log n)$. Previous works often focus on specific problems, and it is of great interest to have a characterization of noisy query complexity for general functions. Our result is the first noisy query complexity lower bound of this generality, beyond what was known for random Boolean functions [Reischuk and Schmeltz, FOCS 1991].
  Our second contribution is to prove that Graph Connectivity has noisy query complexity $\Theta(n^2 \log n)$. In this problem, the goal is to determine whether an undirected graph is connected using noisy edge queries. While the upper bound can be achieved by a simple algorithm, no non-trivial lower bounds were known prior to this work.
  Last but not least, we determine the exact number of noisy queries (up to lower order terms) needed to solve the $k$-Threshold problem and the Counting problem. The $k$-Threshold problem asks to decide whether there are at least $k$ ones among $n$ bits, given noisy query access to the bits. We prove that $(1\pm o(1)) \frac{n\log (\min\{k,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ queries are both sufficient and necessary to achieve error probability $\delta = o(1)$. Previously, such a result was only known when $\min\{k,n-k+1\}=o(n)$ [Wang, Ghaddar, Zhu and Wang, arXiv 2024]. We also show a similar $(1\pm o(1)) \frac{n\log (\min\{k+1,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ bound for the Counting problem, where one needs to count the number of ones among $n$ bits given noisy query access and $k$ denotes the answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04632v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhou Gu, Xin Li, Yinzhan Xu</dc:creator>
    </item>
    <item>
      <title>Exact Algorithms for Distance to Unique Vertex Cover</title>
      <link>https://arxiv.org/abs/2502.05059</link>
      <description>arXiv:2502.05059v1 Announce Type: cross 
Abstract: Horiyama et al. (AAAI 2024) studied the problem of generating graph instances that possess a unique minimum vertex cover under specific conditions. Their approach involved pre-assigning certain vertices to be part of the solution or excluding them from it. Notably, for the \textsc{Vertex Cover} problem, pre-assigning a vertex is equivalent to removing it from the graph. Horiyama et al.~focused on maintaining the size of the minimum vertex cover after these modifications. In this work, we extend their study by relaxing this constraint: our goal is to ensure a unique minimum vertex cover, even if the removal of a vertex may not incur a decrease on the size of said cover.
  Surprisingly, our relaxation introduces significant theoretical challenges. We observe that the problem is $\Sigma^2_P$-complete, and remains so even for planar graphs of maximum degree 5. Nevertheless, we provide a linear time algorithm for trees, which is then further leveraged to show that MU-VC is in \textsf{FPT} when parameterized by the combination of treewidth and maximum degree. Finally, we show that MU-VC is in \textsf{XP} when parameterized by clique-width while it is fixed-parameter tractable (FPT) if we add the size of the solution as part of the parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05059v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Fioravantes, Du\v{s}an Knop, Nikolaos Melissinos, Michal Opler, Manolis Vasilakis</dc:creator>
    </item>
    <item>
      <title>Noise Sensitivity of Hierarchical Functions and Deep Learning Lower Bounds in General Product Measures</title>
      <link>https://arxiv.org/abs/2502.05073</link>
      <description>arXiv:2502.05073v1 Announce Type: cross 
Abstract: Recent works explore deep learning's success by examining functions or data with hierarchical structure. Complementarily, research on gradient descent performance for deep nets has shown that noise sensitivity of functions under independent and identically distributed (i.i.d.) Bernoulli inputs establishes learning complexity bounds. This paper aims to bridge these research streams by demonstrating that functions constructed through repeated composition of non-linear functions are noise sensitive under general product measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05073v1</guid>
      <category>math.PR</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rupert Li, Elchanan Mossel</dc:creator>
    </item>
    <item>
      <title>Algebraic metacomplexity and representation theory</title>
      <link>https://arxiv.org/abs/2411.03444</link>
      <description>arXiv:2411.03444v2 Announce Type: replace 
Abstract: In the algebraic metacomplexity framework we prove that the decomposition of metapolynomials into their isotypic components can be implemented efficiently, namely with only a quasipolynomial blowup in the circuit size. We use this to resolve an open question posed by Grochow, Kumar, Saks &amp; Saraf (2017). Our result means that many existing algebraic complexity lower bound proofs can be efficiently converted into isotypic lower bound proofs via highest weight metapolynomials, a notion studied in geometric complexity theory. In the context of algebraic natural proofs, it means that without loss of generality algebraic natural proofs can be assumed to be isotypic. Our proof is built on the Poincar\'e-Birkhoff-Witt theorem for Lie algebras and on Gelfand-Tsetlin theory, for which we give the necessary comprehensive background.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03444v2</guid>
      <category>cs.CC</category>
      <category>math.AG</category>
      <category>math.RT</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxim van den Berg, Pranjal Dutta, Fulvio Gesmundo, Christian Ikenmeyer, Vladimir Lysikov</dc:creator>
    </item>
    <item>
      <title>Tight Bounds on the Spooky Pebble Game: Recycling Qubits with Measurements</title>
      <link>https://arxiv.org/abs/2110.08973</link>
      <description>arXiv:2110.08973v3 Announce Type: replace-cross 
Abstract: Pebble games are popular models for analyzing time-space trade-offs. In particular, the reversible pebble game is often applied in quantum algorithms like Grover's search to efficiently simulate classical computation on inputs in superposition. However, the reversible pebble game cannot harness the additional computational power granted by irreversible intermediate measurements. The spooky pebble game, which models interleaved measurements and adaptive phase corrections, reduces the number of qubits beyond what reversible approaches can achieve. While the spooky pebble game does not reduce the total space (bits plus qubits) complexity of the simulation, it reduces the amount of space that must be stored in qubits. We prove asymptotically tight trade-offs for the spooky pebble game on a line with any pebble bound, giving a tight time-qubit tradeoff for simulating arbitrary classical sequential computation with the spooky pebble game. For example, for all $\epsilon \in (0,1]$, any classical computation requiring time $T$ and space $S$ can be implemented on a quantum computer using only $O(T/ \epsilon)$ gates and $O(T^{\epsilon}S^{1-\epsilon})$ qubits. This improves on the best known bound for the reversible pebble game with that number of qubits, which uses $O(2^{1/\epsilon} T)$ gates.
  We also consider the spooky pebble game on more general directed acyclic graphs (DAGs), capturing fine-grained data dependency in computation. We show that for an arbitrary DAG even approximating the number of required pebbles in the spooky pebble game is PSPACE-hard. Despite this, we are able to construct a time-efficient strategy for pebbling binary trees that uses the minimum number of pebbles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.08973v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Kornerup, Jonathan Sadun, David Soloveichik</dc:creator>
    </item>
  </channel>
</rss>
