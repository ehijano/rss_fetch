<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Effects of clustering heterogeneity on the spectral density of sparse networks</title>
      <link>https://arxiv.org/abs/2404.08152</link>
      <description>arXiv:2404.08152v1 Announce Type: new 
Abstract: We derive exact equations for the spectral density of sparse networks with an arbitrary distribution of the number of single edges and triangles per node. These equations enable a systematic investigation of the effect of clustering on the spectral properties of the network adjacency matrix. In the case of heterogeneous networks, we demonstrate that the spectral density becomes more symmetric as the fluctuations in the triangle-degree sequence increase. This phenomenon is explained by the small clustering coefficient of networks with a large variance of the triangle-degree distribution. In the homogeneous case of regular clustered networks, we find that both perturbative and non-perturbative approximations fail to predict the spectral density in the high-connectivity limit. This suggests that traditional large-degree approximations may be ineffective in studying the spectral properties of networks with more complex motifs. Our theoretical results are fully confirmed by numerical diagonalizations of finite adjacency matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08152v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuan Minh Pham, Thomas Peron, Fernando L. Metz</dc:creator>
    </item>
    <item>
      <title>Quantum entropy couples matter with geometry</title>
      <link>https://arxiv.org/abs/2404.08556</link>
      <description>arXiv:2404.08556v1 Announce Type: new 
Abstract: We propose a theory for coupling matter fields with discrete geometry on higher-order networks, i.e. cell complexes. The key idea of the approach is to associate to a higher-order network the quantum entropy of its metric. Specifically we propose an action given by the quantum relative entropy between the metric of the higher-order network and the metric induced by the matter and gauge fields. The induced metric is defined in terms of the topological spinors and the discrete Dirac operators. The topological spinors, defined on nodes, edges and higher-dimensional cells, encode for the matter fields. The discrete Dirac operators act on topological spinors, and depend on the metric of the higher-order network as well as on the gauge fields via a discrete version of the minimal substitution. We derive the coupled dynamical equations for the metric, the matter and the gauge fields, providing an information theory principle to obtain the field theory equations in discrete curved space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08556v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>gr-qc</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ginestra Bianconi</dc:creator>
    </item>
    <item>
      <title>Interaction networks in persistent Lotka-Volterra communities</title>
      <link>https://arxiv.org/abs/2404.08600</link>
      <description>arXiv:2404.08600v1 Announce Type: cross 
Abstract: A central concern of community ecology is the interdependence between interaction strengths and the underlying structure of the network upon which species interact. In this work we present a solvable example of such a feedback mechanism in a generalised Lotka-Volterra dynamical system. Beginning with a community of species interacting on a network with arbitrary degree distribution, we provide an analytical framework from which properties of the eventual `surviving community' can be derived. We find that highly-connected species are less likely to survive than their poorly connected counterparts, which skews the eventual degree distribution towards a preponderance of species with low degree, a pattern commonly observed in real ecosystems. Further, the average abundance of the neighbours of a species in the surviving community is lower than the community average (reminiscent of the famed friendship paradox). Finally, we show that correlations emerge between the connectivity of a species and its interactions with its neighbours. More precisely, we find that highly-connected species tend to benefit from their neighbours more than their neighbours benefit from them. These correlations are not present in the initial pool of species and are a result of the dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08600v1</guid>
      <category>q-bio.PE</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyle Poley, Tobias Galla, Joseph W. Baron</dc:creator>
    </item>
    <item>
      <title>Transport properties in non-Fermi liquid phases of nodal-point semimetals</title>
      <link>https://arxiv.org/abs/2404.08635</link>
      <description>arXiv:2404.08635v1 Announce Type: cross 
Abstract: In this review, we survey the current progress in computing transport properties in semimetals which harbour non-Fermi liquid phases. We first discuss the widely used Kubo formalism, which can be applied to the effective theory describing the stable non-Fermi liquid pase obtained via a renormalization group procedure and, hence, is applicable for temperatures close to zero (e.g., optical conductivity). For finite temperature regimes, which apply to the computations of the generalized dc conductivity tensors, we elucidate the memory matrix approach. This approach is based on an effective hydrodynamic description of the system, and is especially suited for tackling transport calculations in strongly-interacting quantum field theories, because it does not rely on the existence of long-lived quasiparticles. As a concrete example, we apply these two approaches to find the response of the so-called Luttinger-Abrikosov-Benelavskii phase of isotropic three-dimensional Luttinger semimetals, which arise under the effects of long-ranged (unscreened) Coulomb interactions, with the chemical potential fine-tuned to cut exactly the nodal point. In particular, we focus on the electric conductivity tensors, thermal and thermoelectric response, Raman response, free energy, entropy density, and shear viscosity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08635v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <category>hep-th</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ipsita Mandal, Hermann Freire</dc:creator>
    </item>
    <item>
      <title>Explaining the Machine Learning Solution of the Ising Model</title>
      <link>https://arxiv.org/abs/2402.11701</link>
      <description>arXiv:2402.11701v2 Announce Type: replace 
Abstract: As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications. This work shows how this can be accomplished for the ferromagnetic Ising model, the main target of several ML studies in statistical physics. Here it is demonstrated that the successful unsupervised identification of the phases and order parameter by principal component analysis, a common method in those studies, detects that the magnetization per spin has its greatest variation with the temperature, the actual control parameter of the phase transition. Then, by using a neural network (NN) without hidden layers (the simplest possible) and informed by the symmetry of the Hamiltonian, an explanation is provided for the strategy used in finding the supervised learning solution for the critical temperature of the model's continuous phase transition. This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which becomes also explainable. These results pave the way to a physics-informed explainable generalized framework, enabling the extraction of physical laws and principles from the parameters of the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11701v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto C. Alamino</dc:creator>
    </item>
    <item>
      <title>Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations</title>
      <link>https://arxiv.org/abs/2211.03226</link>
      <description>arXiv:2211.03226v3 Announce Type: replace-cross 
Abstract: The difficult problem of relating the static structure of glassy liquids and their dynamics is a good target for Machine Learning, an approach which excels at finding complex patterns hidden in data. Indeed, this approach is currently a hot topic in the glassy liquids community, where the state of the art consists in Graph Neural Networks (GNNs), which have great expressive power but are heavy models and lack interpretability. Inspired by recent advances in the field of Machine Learning group-equivariant representations, we build a GNN that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance. We show that this constraint significantly improves the predictive power at comparable or reduced number of parameters but most importantly, improves the ability to generalize to unseen temperatures. While remaining a Deep network, our model has improved interpretability compared to other GNNs, as the action of our basic convolution layer relates directly to well-known rotation-invariant expert features. Through transfer-learning experiments displaying unprecedented performance, we demonstrate that our network learns a robust representation, which allows us to push forward the idea of a learned structural order parameter for glasses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.03226v3</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Saverio Pezzicoli, Guillaume Charpiat, Fran\c{c}ois P. Landes</dc:creator>
    </item>
    <item>
      <title>Solitons in binary compounds with stacked two-dimensional honeycomb lattices</title>
      <link>https://arxiv.org/abs/2312.16949</link>
      <description>arXiv:2312.16949v2 Announce Type: replace-cross 
Abstract: We model the electronic properties of thin films of binary compounds with stacked layers where each layer is a two-dimensional honeycomb lattice with two atoms per unit cell. The two atoms per cell are assigned different onsite energies in order to consider six different stacking orders: ABC, ABA, AA, ABC$^{\prime}$, ABA$^{\prime}$, and AA$^{\prime}$. Using a minimal tight-binding model with nearest-neighbor hopping, we consider whether a fault in the texture of onsite energies in the vertical, stacking direction supports localized states, and we find localized states within the bulk band gap for ABC, ABA, and AA$^{\prime}$ stacking. Depending on the stacking type, parameter values, and whether the soliton is atomically sharp or a smooth texture, there are a range of different band structures including soliton bands that are either isolated or that hybridize with other states, such as surface states, and soliton bands that are either dispersive or flat, the latter yielding narrow features in the density of states. We discuss the relevance of our results to specific materials including graphene, hexagonal boron nitride and other binary compounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16949v2</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.109.165416</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 109, 165416 (2024)</arxiv:journal_reference>
      <dc:creator>James H. Muten, Louise H. Frankland, Edward McCann</dc:creator>
    </item>
    <item>
      <title>A Dynamical Model of Neural Scaling Laws</title>
      <link>https://arxiv.org/abs/2402.01092</link>
      <description>arXiv:2402.01092v2 Announce Type: replace-cross 
Abstract: On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate $\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01092v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan</dc:creator>
    </item>
  </channel>
</rss>
