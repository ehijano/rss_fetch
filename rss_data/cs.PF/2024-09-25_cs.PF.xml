<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 01:48:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FRSZ2 for In-Register Block Compression Inside GMRES on GPUs</title>
      <link>https://arxiv.org/abs/2409.15468</link>
      <description>arXiv:2409.15468v1 Announce Type: new 
Abstract: The performance of the GMRES iterative solver on GPUs is limited by the GPU main memory bandwidth. Compressed Basis GMRES outperforms GMRES by storing the Krylov basis in low precision, thereby reducing the memory access. An open question is whether compression techniques that are more sophisticated than casting to low precision can enable large runtime savings while preserving the accuracy of the final results. This paper presents the lightweight in-register compressor FRSZ2 that can decompress at the bandwidth speed of a modern NVIDIA H100 GPU. In an experimental evaluation, we demonstrate using FRSZ2 instead of low precision for compression of the Krylov basis can bring larger runtime benefits without impacting final accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15468v1</guid>
      <category>cs.PF</category>
      <category>cs.DS</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Gr\"utzmacher, Robert Underwood, Sheng Di, Franck Cappello, Hartwig Anzt</dc:creator>
    </item>
    <item>
      <title>Performance and scaling of the LFRic weather and climate model on different generations of HPE Cray EX supercomputers</title>
      <link>https://arxiv.org/abs/2409.15859</link>
      <description>arXiv:2409.15859v1 Announce Type: cross 
Abstract: This study presents scaling results and a performance analysis across different supercomputers and compilers for the Met Office weather and climate model, LFRic. The model is shown to scale to large numbers of nodes which meets the design criteria, that of exploitation of parallelism to achieve good scaling. The model is written in a Domain-Specific Language, embedded in modern Fortran and uses a Domain-Specific Compiler, PSyclone, to generate the parallel code. The performance analysis shows the effect of choice of algorithm, such as redundant computation and scaling with OpenMP threads. The analysis can be used to motivate a discussion of future work to improve the OpenMP performance of other parts of the code. Finally, an analysis of the performance tuning of the I/O server, XIOS is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15859v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Mark Bull (EPCC University of Edinburgh), Andrew Coughtrie (Met Office), Deva Deeptimahanti (Pawsey Supercomputing Research Centre), Mark Hedley (Met Office), Caoimh\'in Laoide-Kemp (EPCC University of Edinburgh), Christopher Maynard (Met Office), Harry Shepherd (Met Office), Sebastiaan van de Bund (EPCC University of Edinburgh), Mich\`ele Weiland (Met Office), Benjamin Went (Met Office)</dc:creator>
    </item>
    <item>
      <title>Inspection of I/O Operations from System Call Traces using Directly-Follows-Graph</title>
      <link>https://arxiv.org/abs/2408.07378</link>
      <description>arXiv:2408.07378v2 Announce Type: replace 
Abstract: We aim to identify the differences in Input/Output(I/O) behavior between multiple user programs through the inspection of system calls (i.e., requests made to the operating system). A typical program issues a large number of I/O requests to the operating system, thereby making the process of inspection challenging. In this paper, we address this challenge by presenting a methodology to synthesize I/O system call traces into a specific type of directed graph, known as the Directly-Follows-Graph (DFG). Based on the DFG, we present a technique to compare the traces from multiple programs or different configurations of the same program, such that it is possible to identify the differences in the I/O behavior. We apply our methodology to the IOR benchmark, and compare the contentions for file accesses when the benchmark is run with different options for file output and software interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07378v2</guid>
      <category>cs.PF</category>
      <category>cs.OS</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aravind Sankaran, Ilya Zhukov, Wolfgang Frings, Paolo Bientinesi</dc:creator>
    </item>
    <item>
      <title>The Bicameral Cache: a split cache for vector architectures</title>
      <link>https://arxiv.org/abs/2407.15440</link>
      <description>arXiv:2407.15440v3 Announce Type: replace-cross 
Abstract: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15440v3</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu</dc:creator>
    </item>
  </channel>
</rss>
