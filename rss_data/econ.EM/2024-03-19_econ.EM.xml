<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:11:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Macroeconomic Spillovers of Weather Shocks across U.S. States</title>
      <link>https://arxiv.org/abs/2403.10907</link>
      <description>arXiv:2403.10907v1 Announce Type: new 
Abstract: We estimate the short-run effects of severe weather shocks on local economic activity and assess cross-border spillovers operating through economic linkages between U.S. states. We measure weather shocks using a detailed county-level database on emergency declarations triggered by natural disasters and estimate their impacts with a monthly Global Vector Autoregressive (GVAR) model for the U.S. states. Impulse responses highlight significant country-wide macroeconomic effects of weather shocks hitting individual regions. We also show that (i) taking into account economic interconnections between states allows capturing much stronger spillover effects than those associated with mere spatial adjacency, (ii) geographical heterogeneity is critical for assessing country-wide effects of weather shocks, and (iii) network effects amplify the local impacts of these shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10907v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Bacchiocchi, Andrea Bastianin, Graziano Moramarco</dc:creator>
    </item>
    <item>
      <title>Comprehensive OOS Evaluation of Predictive Algorithms with Statistical Decision Theory</title>
      <link>https://arxiv.org/abs/2403.11016</link>
      <description>arXiv:2403.11016v1 Announce Type: new 
Abstract: We argue that comprehensive out-of-sample (OOS) evaluation using statistical decision theory (SDT) should replace the current practice of K-fold and Common Task Framework validation in machine learning (ML) research. SDT provides a formal framework for performing comprehensive OOS evaluation across all possible (1) training samples, (2) populations that may generate training data, and (3) populations of prediction interest. Regarding feature (3), we emphasize that SDT requires the practitioner to directly confront the possibility that the future may not look like the past and to account for a possible need to extrapolate from one population to another when building a predictive algorithm. SDT is simple in abstraction, but it is often computationally demanding to implement. We discuss progress in tractable implementation of SDT when prediction accuracy is measured by mean square error or by misclassification rate. We summarize research studying settings in which the training data will be generated from a subpopulation of the population of prediction interest. We also consider conditional prediction with alternative restrictions on the state space of possible populations that may generate training data. We conclude by calling on ML researchers to join with econometricians and statisticians in expanding the domain within which implementation of SDT is tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11016v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Dominitz, Charles F. Manski</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification and Estimation with Non-Classical Errors-in-Variables</title>
      <link>https://arxiv.org/abs/2403.11309</link>
      <description>arXiv:2403.11309v1 Announce Type: new 
Abstract: This paper considers nonparametric identification and estimation of the regression function when a covariate is mismeasured. The measurement error need not be classical. Employing the small measurement error approximation, we establish nonparametric identification under weak and easy-to-interpret conditions on the instrumental variable. The paper also provides nonparametric estimators of the regression function and derives their rates of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11309v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Limits of Approximating the Median Treatment Effect</title>
      <link>https://arxiv.org/abs/2403.10618</link>
      <description>arXiv:2403.10618v1 Announce Type: cross 
Abstract: Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where median($\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the value $a_i$ or $b_i$, but not both, makes estimating MTE particularly challenging. In this work, we argue that MTE is not estimable and detail a novel notion of approximation that relies on the sorted order of the values in $\mathbf{a-b}$. Next, we identify a quantity called variability that exactly captures the complexity of MTE estimation. By drawing connections to instance-optimality studied in theoretical computer science, we show that every algorithm for estimating the MTE obtains an approximation error that is no better than the error of an algorithm that computes variability. Finally, we provide a simple linear time algorithm for computing the variability exactly. Unlike much prior work, a particular highlight of our work is that we make no assumptions about how the potential outcome vectors are generated or how they are correlated, except that the potential outcome values are $k$-ary, i.e., take one of $k$ discrete values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10618v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raghavendra Addanki, Siddharth Bhandari</dc:creator>
    </item>
    <item>
      <title>Identification of Information Structures in Bayesian Games</title>
      <link>https://arxiv.org/abs/2403.11333</link>
      <description>arXiv:2403.11333v1 Announce Type: cross 
Abstract: To what extent can an external observer observing an equilibrium action distribution in an incomplete information game infer the underlying information structure? We investigate this issue in a general linear-quadratic-Gaussian framework. A simple class of canonical information structures is offered and proves rich enough to rationalize any possible equilibrium action distribution that can arise under an arbitrary information structure. We show that the class is parsimonious in the sense that the relevant parameters can be uniquely pinned down by an observed equilibrium outcome, up to some qualifications. Our result implies, for example, that the accuracy of each agent's signal about the state is identified, as measured by how much observing the signal reduces the state variance. Moreover, we show that a canonical information structure characterizes the lower bound on the amount by which each agent's signal can reduce the state variance, across all observationally equivalent information structures. The lower bound is tight, for example, when the actual information structure is uni-dimensional, or when there are no strategic interactions among agents, but in general, there is a gap since agents' strategic motives confound their private information about fundamental and strategic uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11333v1</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Masaki Miyashita</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Inference in Categorical Data</title>
      <link>https://arxiv.org/abs/2403.11954</link>
      <description>arXiv:2403.11954v1 Announce Type: cross 
Abstract: In empirical science, many variables of interest are categorical. Like any model, models for categorical responses can be misspecified, leading to possibly large biases in estimation. One particularly troublesome source of misspecification is inattentive responding in questionnaires, which is well-known to jeopardize the validity of structural equation models (SEMs) and other survey-based analyses. I propose a general estimator that is designed to be robust to misspecification of models for categorical responses. Unlike hitherto approaches, the estimator makes no assumption whatsoever on the degree, magnitude, or type of misspecification. The proposed estimator generalizes maximum likelihood estimation, is strongly consistent, asymptotically Gaussian, has the same time complexity as maximum likelihood, and can be applied to any model for categorical responses. In addition, I develop a novel test that tests whether a given response can be fitted well by the assumed model, which allows one to trace back possible sources of misspecification. I verify the attractive theoretical properties of the proposed methodology in Monte Carlo experiments, and demonstrate its practical usefulness in an empirical application on a SEM of personality traits, where I find compelling evidence for the presence of inattentive responding whose adverse effects the proposed estimator can withstand, unlike maximum likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11954v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Welz</dc:creator>
    </item>
    <item>
      <title>Simple Estimation of Semiparametric Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2306.14311</link>
      <description>arXiv:2306.14311v2 Announce Type: replace 
Abstract: We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a "corrected" set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14311v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Marginal Effects for Probit and Tobit with Endogeneity</title>
      <link>https://arxiv.org/abs/2306.14862</link>
      <description>arXiv:2306.14862v2 Announce Type: replace 
Abstract: When evaluating partial effects, it is important to distinguish between structural endogeneity and measurement errors. In contrast to linear models, these two sources of endogeneity affect partial effects differently in nonlinear models. We study this issue focusing on the Instrumental Variable (IV) Probit and Tobit models. We show that even when a valid IV is available, failing to differentiate between the two types of endogeneity can lead to either under- or over-estimation of the partial effects. We develop simple estimators of the bounds on the partial effects and provide easy to implement confidence intervals that correctly account for both types of endogeneity. We illustrate the methods in a Monte Carlo simulation and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14862v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Ilze Kalnina, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Real-time Prediction of the Great Recession and the Covid-19 Recession</title>
      <link>https://arxiv.org/abs/2310.08536</link>
      <description>arXiv:2310.08536v4 Announce Type: replace 
Abstract: A series of standard and penalized logistic regression models is employed to model and forecast the Great Recession and the Covid-19 recession in the US. The empirical analysis explores the predictive content of numerous macroeconomic and financial indicators with respect to NBER recession indicator. The predictive ability of the underlying models is evaluated using a set of statistical evaluation metrics. The recessions are scrutinized by closely examining the movement of five most influential predictors that are chosen through automatic variable selections of the Lasso regression, along with the regression coefficients and the predicted recession probabilities. The results strongly support the application of penalized logistic regression models in the area of recession forecasting. Specifically, the analysis indicates that the mixed usage of different penalized logistic regression models over different forecast horizons largely outperform standard logistic regression models in the prediction of Great recession in the US, as they achieve higher predictive accuracy across 5 different forecast horizons. The Great Recession is largely predictable, whereas the Covid-19 recession remains unpredictable, given that the Covid-19 pandemic is a real exogenous event. The empirical study reaffirms the traditional role of the term spread as one of the most important recession indicators. The results are validated by constructing via PCA on a set of selected variables a recession indicator that suffers less from publication lags and exhibits a very high association with the NBER recession indicator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08536v4</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seulki Chung</dc:creator>
    </item>
    <item>
      <title>Inside the black box: Neural network-based real-time prediction of US recessions</title>
      <link>https://arxiv.org/abs/2310.17571</link>
      <description>arXiv:2310.17571v2 Announce Type: replace 
Abstract: A standard feedforward neural network (FFN) and two specific types of recurrent neural networks, long short-term memory (LSTM) and gated recurrent unit (GRU), are used for modeling US recessions in the period from 1967 to 2021. The estimated models are then employed to conduct real-time predictions of the Great Recession and the Covid-19 recession in the US. Their predictive performances are compared to those of the traditional linear models, the standard logit model and the ridge logit model. The out-of-sample performance suggests the application of LSTM and GRU in the area of recession forecasting, especially for the long-term forecasting tasks. They outperform other types of models across five different forecast horizons with respect to a selected set of statistical metrics. Shapley additive explanations (SHAP) method is applied to GRU and the ridge logit model as the best performer in the neural network and linear model group, respectively, to gain insight into the variable importance. The evaluation of variable importance differs between GRU and the ridge logit model, as reflected in their unequal variable orders determined by the SHAP values. These different weight assignments can be attributed to GRUs flexibility and capability to capture the business cycle asymmetries and nonlinearities. The SHAP method delivers some key recession indicators. For forecasting up to 3 months, the stock price index, real GDP, and private residential fixed investment show great short-term predictability, while for longer-term forecasting up to 12 months, the term spread and the producer price index have strong explanatory power for recessions. These findings are robust against other interpretation methods such as the local interpretable model-agnostic explanations (LIME) for GRU and the marginal effects for the ridge logit model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17571v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seulki Chung</dc:creator>
    </item>
    <item>
      <title>Set-Valued Control Functions</title>
      <link>https://arxiv.org/abs/2403.00347</link>
      <description>arXiv:2403.00347v2 Announce Type: replace 
Abstract: The control function approach allows the researcher to identify various causal effects of interest. While powerful, it requires a strong invertibility assumption, which limits its applicability. This paper expands the scope of the nonparametric control function approach by allowing the control function to be set-valued and derive sharp bounds on structural parameters. The proposed generalization accommodates a wide range of selection processes involving discrete endogenous variables, random coefficients, treatment selections with interference, and dynamic treatment selections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00347v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han, Hiroaki Kaido</dc:creator>
    </item>
    <item>
      <title>Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances</title>
      <link>https://arxiv.org/abs/2312.12741</link>
      <description>arXiv:2312.12741v2 Announce Type: replace-cross 
Abstract: We address the problem of best arm identification (BAI) with a fixed budget for two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the best arm, an arm with the highest expected reward, through an adaptive experiment. Kaufmann et al. (2016) develops a lower bound for the probability of misidentifying the best arm. They also propose a strategy, assuming that the variances of rewards are known, and show that it is asymptotically optimal in the sense that its probability of misidentification matches the lower bound as the budget approaches infinity. However, an asymptotically optimal strategy is unknown when the variances are unknown. For this open issue, we propose a strategy that estimates variances during an adaptive experiment and draws arms with a ratio of the estimated standard deviations. We refer to this strategy as the Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW) strategy. We then demonstrate that this strategy is asymptotically optimal by showing that its probability of misidentification matches the lower bound when the budget approaches infinity, and the gap between the expected rewards of two arms approaches zero (small-gap regime). Our results suggest that under the worst-case scenario characterized by the small-gap regime, our strategy, which employs estimated variance, is asymptotically optimal even when the variances are unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12741v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
