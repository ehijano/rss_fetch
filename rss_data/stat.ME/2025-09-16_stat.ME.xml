<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>New generalized unit distributions based on order statistics</title>
      <link>https://arxiv.org/abs/2509.12276</link>
      <description>arXiv:2509.12276v1 Announce Type: new 
Abstract: In the present paper, the author discusses the derivation of unit distributions and the derivation of the generalized form using the order statistics. The author discusses the Kumaraswamy as the smallest order statistic of the unit power distribution derived from the inverse Weibull distribution. The author discusses the unit Rayleigh distribution and how it can be generalized using the smallest, largest, and kth order statistics. Using the order statistics to generalize a distribution differs from other techniques like the power transformation and T-X family (transformed-transformer) method. For the discussed distribution, the author demonstrates the basic functions and properties with real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12276v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Significant inference and confidence sets for graphical models</title>
      <link>https://arxiv.org/abs/2509.12292</link>
      <description>arXiv:2509.12292v1 Announce Type: new 
Abstract: The problem of identifying statistically significant inferences about the structure of the graphical model is considered, along with the related task of constructing a confidence set for a graphical model. It has been proven that the procedure for constructing such set is equivalent to the procedure for simultaneous testing of hypotheses and alternatives regarding the composition of the graphical model. Some variants of the simultaneous testing of hypotheses and alternatives are discussed. It is shown that under the condition of free combination of hypotheses and alternatives, a simple generalization of the closure method leads to singlestep procedures for simultaneous testing of hypotheses and alternatives. The structure of the confidence set for the graphical model is analyzed, demonstrating how the confidence set leads to a separation of inferences about the graphical model into statistically significant and insignificant categories, or into an area of uncertainty. General results are detailed by analyzing confidence sets for undirected Gaussian graphical model selection. Examples are provided that illustrate the separation of inferences about the composition of undirected Gaussian graphical models into significant results and areas of uncertainty, and a comparison is made with known results obtained using the SINful approach to undirected Gaussian graphical model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12292v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. A. Koldanov, A. P. Koldanov</dc:creator>
    </item>
    <item>
      <title>System Reliability Estimation via Shrinkage</title>
      <link>https://arxiv.org/abs/2509.12420</link>
      <description>arXiv:2509.12420v1 Announce Type: new 
Abstract: In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12420v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beidi Qiang, Edsel Pena</dc:creator>
    </item>
    <item>
      <title>A computational method for type I error rate control in power-maximizing response-adaptive randomization</title>
      <link>https://arxiv.org/abs/2509.12448</link>
      <description>arXiv:2509.12448v1 Announce Type: new 
Abstract: Maximizing statistical power in experimental design often involves imbalanced treatment allocation, but several challenges hinder its practical adoption: (1) the misconception that equal allocation always maximizes power, (2) when only targeting maximum power, more than half the participants may be expected to obtain inferior treatment, and (3) response-adaptive randomization (RAR) targeting maximum statistical power may inflate type I error rates substantially. Recent work identified issue (3) and proposed a novel allocation procedure combined with the asymptotic score test. Instead, the current research focuses on finite-sample guarantees. First, we analyze the power for traditional power-maximizing RAR procedures under exact tests, including a novel generalization of Boschloo's test. Second, we evaluate constrained Markov decision process (CMDP) RAR procedures under exact tests. These procedures target maximum average power under constraints on pointwise and average type I error rates, with averages taken across the parametric space. A combination of the unconditional exact test and the CMDP procedure protecting allocations to the superior arm gives the best performance, providing substantial power gains over equal allocation while allocating more participants in expectation to the superior treatment. Future research could focus on the randomization test, in which CMDP procedures exhibited lower power compared to other examined RAR procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12448v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stef Baas, Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Cox Regression on the Plane</title>
      <link>https://arxiv.org/abs/2509.12473</link>
      <description>arXiv:2509.12473v1 Announce Type: new 
Abstract: The Cox proportional hazards model is the most widely used regression model in univariate survival analysis. Extensions of the Cox model to bivariate survival data, however, remain scarce. We propose two novel extensions based on a Lehmann-type representation of the survival function. The first, the simple Lehmann model, is a direct extension that retains a straightforward structure. The second, the generalized Lehmann model, allows greater flexibility by incorporating three distinct regression parameters and includes the simple Lehmann model as a special case. For both models, we derive the corresponding regression formulations for the three bivariate hazard functions and discuss their interpretation and model validity. To estimate the regression parameters, we adopt a bivariate pseudo-observations approach. For the generalized Lehmann model, we extend this approach to accommodate a trivariate structure: trivariate pseudo-observations and a trivariate link function. We then propose a two-step estimation procedure, where the marginal regression parameters are estimated in the first step, and the remaining parameters are estimated in the second step. Finally, we establish the consistency and asymptotic normality of the resulting estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12473v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yael Travis-Lumer, Micha Mandel, Rebecca A. Betensky, Malka Gorfine</dc:creator>
    </item>
    <item>
      <title>Instrument, Variable and Model Selection with Nonignorable Nonresponse</title>
      <link>https://arxiv.org/abs/2509.12557</link>
      <description>arXiv:2509.12557v1 Announce Type: new 
Abstract: With nonignorable nonresponse, an effective method to construct valid estimators of population parameters is to use a covariate vector called instrument that can be excluded from the nonresponse propensity but are still useful covariate even when other covariates are conditioned. The existing work in this approach assumes such an instrument is given, which is frequently not the case in applications. In this paper we investigate how to search for an instrument from a given set of covariates. The method for estimation we apply is the pseudo likelihood proposed by Tang et al. (2003) and Zhao and Shao (2015), which assumed that an instrument is given and the distribution of response given covariates is parametric and the propensity is nonparametric. Thus, in addition to the challenge of searching an instrument, we also need to do variable and model selection simultaneously. We propose a method for instrument, variable, and model selection and show that our method produces consistent instrument and model selection as the sample size tends to infinity, under some regularity conditions. Empirical results including two simulation studies and two real examples are present to show that the proposed method works well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12557v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ji Chen, Jun Shao</dc:creator>
    </item>
    <item>
      <title>Inverse regression for causal inference with multiple outcomes</title>
      <link>https://arxiv.org/abs/2509.12587</link>
      <description>arXiv:2509.12587v1 Announce Type: new 
Abstract: With multiple outcomes in empirical research, a common strategy is to define a composite outcome as a weighted average of the original outcomes. However, the choices of weights are often subjective and can be controversial. We propose an inverse regression strategy for causal inference with multiple outcomes. The key idea is to regress the treatment on the outcomes, which is the inverse of the standard regression of the outcomes on the treatment. Although this strategy is simple and even counterintuitive, it has several advantages. First, testing for zero coefficients of the outcomes is equivalent to testing for the null hypothesis of zero effects, even though the inverse regression is deemed misspecified. Second, the coefficients of the outcomes provide a data-driven choice of the weights for defining a composite outcome. We also discuss the associated inference issues. Third, this strategy is applicable to general study designs. We illustrate the theory in both randomized experiments and observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12587v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhang, Qizhai Li, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Power-Dominance in Estimation Theory: A Third Pathological Axis</title>
      <link>https://arxiv.org/abs/2509.12691</link>
      <description>arXiv:2509.12691v1 Announce Type: new 
Abstract: This paper introduces a novel framework for estimation theory by introducing a second-order diagnostic for estimator design. While classical analysis focuses on the bias-variance trade-off, we present a more foundational constraint. This result is model-agnostic, domain-agnostic, and is valid for both parametric and non-parametric problems, Bayesian and frequentist frameworks. We propose to classify the estimators into three primary power regimes. We theoretically establish that any estimator operating in the `power-dominant regime' incurs an unavoidable mean-squared error penalty, making it structurally prone to sub-optimal performance. We propose a `safe-zone law' and make this diagnostic intuitive through two safe-zone maps. One map is a geometric visualization analogous to a receiver operating characteristic curve for estimators, and the other map shows that the safe-zone corresponds to a bounded optimization problem, while the forbidden `power-dominant zone' represents an unbounded optimization landscape. This framework reframes estimator design as a path optimization problem, providing new theoretical underpinnings for regularization and inspiring novel design philosophies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12691v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
    <item>
      <title>Multivariate Low-Rank State-Space Model with SPDE Approach for High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2509.12825</link>
      <description>arXiv:2509.12825v1 Announce Type: new 
Abstract: This paper proposes a novel low-rank approximation to the multivariate State-Space Model. The Stochastic Partial Differential Equation (SPDE) approach is applied component-wise to the independent-in-time Mat\'ern Gaussian innovation term in the latent equation, assuming component independence. This results in a sparse representation of the latent process on a finite element mesh, allowing for scalable inference through sparse matrix operations. Dependencies among observed components are introduced through a matrix of weights applied to the latent process. Model parameters are estimated using the Expectation-Maximisation algorithm, which features closed-form updates for most parameters and efficient numerical routines for the remaining parameters. We prove theoretical results regarding the accuracy and convergence of the SPDE-based approximation under fixed-domain asymptotics. Simulation studies show our theoretical results. We include an empirical application on air quality to demonstrate the practical usefulness of the proposed model, which maintains computational efficiency in high-dimensional settings. In this application, we reduce computation time by about 93%, with only a 15% increase in the validation error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12825v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacopo Rodeschini, Lorenzo Tedesco, Francesco Finazzi, Philipp Otto, Alessandro Fass\`o</dc:creator>
    </item>
    <item>
      <title>Modeling nonstationary spatial processes with normalizing flows</title>
      <link>https://arxiv.org/abs/2509.12884</link>
      <description>arXiv:2509.12884v1 Announce Type: new 
Abstract: Nonstationary spatial processes can often be represented as stationary processes on a warped spatial domain. Selecting an appropriate spatial warping function for a given application is often difficult and, as a result of this, warping methods have largely been limited to two-dimensional spatial domains. In this paper, we introduce a novel approach to modeling nonstationary, anisotropic spatial processes using neural autoregressive flows (NAFs), a class of invertible mappings capable of generating complex, high-dimensional warpings. Through simulation studies we demonstrate that a NAF-based model has greater representational capacity than other commonly used spatial process models. We apply our proposed modeling framework to a subset of the 3D Argo Floats dataset, highlighting the utility of our framework in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12884v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratik Nag, Andrew Zammit-Mangion, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Least squares estimation of the transition density in bifurcating Markov models</title>
      <link>https://arxiv.org/abs/2509.12906</link>
      <description>arXiv:2509.12906v1 Announce Type: new 
Abstract: In this article, we propose a least squares method for the estimation of the transition density in bifurcating Markov models. Unlike the kernel estimation, this method do not use the quotient which can be a source of errors. In order to study the rate of convergence for least squares estimators, we develop exponential inequalities for empirical process of bifurcating Markov chain under bracketing assumption. Unlike the classical processes, we observe that for bifurcating Markov chains, the complexity parameter depends on the ergodicity rate and as consequence, we have that the convergence rate of our estimator is a function of the ergodicity rate. We conclude with a numerical study to validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12906v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>S. Val\`ere Bitseki Penda</dc:creator>
    </item>
    <item>
      <title>Efficient estimation for flexible spatial zero-inflated models with environmental applications</title>
      <link>https://arxiv.org/abs/2509.13054</link>
      <description>arXiv:2509.13054v1 Announce Type: new 
Abstract: Spatial two-component mixture models offer a robust framework for analyzing spatially correlated data with zero inflation. To circumvent potential biases introduced by assuming a specific distribution for the response variables, we employ a flexible spatial zero-inflated model. Despite its flexibility, this model poses significant computational challenges, particularly with large datasets, due to the high dimensionality of spatially dependent latent variables, the complexity of matrix operations, and the slow convergence of estimation procedures. To overcome these challenges, we propose a projection-based approach that reduces the dimensionality of the problem by projecting spatially dependent latent variables onto a lower-dimensional space defined by a selected set of basis functions. We further develop an efficient iterative algorithm for parameter estimation, incorporating a generalized estimating equation (GEE) framework. The optimal number of basis functions is determined using Akaike's information criterion (AIC), and the stability of the parameter estimates is assessed using the block jackknife method. The proposed method is validated through a comprehensive simulation study and applied to the analysis of Taiwan's daily rainfall data for 2016, demonstrating its practical utility and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13054v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chung-Wei Shen (Department of Mathematics, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C), Bu-Ren Hsu (Graduate Institute of Statistics, National Central University, Taoyuan, Taiwan, R.O.C), Chia-Ming Hsu (Graduate Institute of Statistics, National Central University, Taoyuan, Taiwan, R.O.C), Chun-Shu Chen (Graduate Institute of Statistics, National Central University, Taoyuan, Taiwan, R.O.C)</dc:creator>
    </item>
    <item>
      <title>Scale-Location-Truncated Beta Regression: Expanding Beta Regression to Accommodate 0 and 1</title>
      <link>https://arxiv.org/abs/2509.13167</link>
      <description>arXiv:2509.13167v1 Announce Type: new 
Abstract: Beta regression is frequently used when the outcome variable y is bounded within a specific interval, transformed to the (0, 1) domain if necessary. However, standard beta regression cannot handle data observed at the boundary values of 0 or 1, as the likelihood function takes on values of either 0 or infinity. To address this issue, we propose the Scale-Location-Truncated beta (SLTB) regression model, which extends the beta distribution's domain to the [0, 1] interval. By using scale-location transformation and truncation, SLTB distribution allows positive finite mass to the boundary values, offering a flexible approach for handling values at 0 and 1. In this paper, we demonstrate the effectiveness of the SLTB regression model in comparison to standard beta regression models and other approaches like the Zero-One Inflated Beta (ZOIB) mixture model and XBX regression. Using empirical and simulated data, we compare the performance including predictive accuracy of the SLTB regression model with other methods, particularly in cases with observed boundary data values for y. The SLTB model is shown to offer great flexibility, supporting both linear and nonlinear relationships. Additionally, we implement the SLTB model within maximum likelihood and Bayesian frameworks, employing both hierarchical and non-hierarchical models. These comprehensive implementations demonstrate the broad applicability of SLTB model for modeling data with bounded values in a variety of contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13167v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingang Kim, Brent A. Kaplan, Mikhail N. Koffarnus, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Robust Sensitivity Analysis via Augmented Percentile Bootstrap under Simultaneous Violations of Unconfoundedness and Overlap</title>
      <link>https://arxiv.org/abs/2509.13169</link>
      <description>arXiv:2509.13169v1 Announce Type: new 
Abstract: The identification of causal effects in observational studies typically relies on two standard assumptions: unconfoundedness and overlap. However, both assumptions are often questionable in practice: unconfoundedness is inherently untestable, and overlap may fail in the presence of extreme unmeasured confounding. While various approaches have been developed to address unmeasured confounding and extreme propensity scores separately, few methods accommodate simultaneous violations of both assumptions. In this paper, we propose a sensitivity analysis framework that relaxes both unconfoundedness and overlap, building upon the marginal sensitivity model. Specifically, we allow the bound on unmeasured confounding to hold for only a subset of the population, thereby accommodating heterogeneity in confounding and allowing treatment probabilities to be zero or one. Moreover, unlike prior work, our approach does not require bounded outcomes and focuses on overlap-weighted average treatment effects, which are both practically meaningful and robust to non-overlap. We develop computationally efficient methods to obtain worst-case bounds via linear programming, and introduce a novel augmented percentile bootstrap procedure for statistical inference. This bootstrap method handles parameters defined through over-identified estimating equations involving unobserved variables and may be of independent interest. Our work provides a unified and flexible framework for sensitivity analysis under violations of both unconfoundedness and overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13169v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Cui, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments</title>
      <link>https://arxiv.org/abs/2509.13176</link>
      <description>arXiv:2509.13176v1 Announce Type: new 
Abstract: We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13176v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushi Bu, Wen Su, Xingqiu Zhao, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Learning Discrete Bayesian Networks with Hierarchical Dirichlet Shrinkage</title>
      <link>https://arxiv.org/abs/2509.13267</link>
      <description>arXiv:2509.13267v1 Announce Type: new 
Abstract: Discrete Bayesian networks (DBNs) provide a broadly useful framework for modeling dependence structures in multivariate categorical data. There is a vast literature on methods for inferring conditional probabilities and graphical structure in DBNs, but data sparsity and parametric assumptions are major practical issues. In this article, we detail a comprehensive Bayesian framework for learning DBNs. First, we propose a hierarchical prior for the conditional probabilities that enables complicated interactions between parent variables and stability in sparse regimes. We give a novel Markov chain Monte Carlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact posterior samples, avoiding the pitfalls of variational approximations. Moreover, we verify that the full conditional distribution of the concentration parameters is log-concave under mild conditions, facilitating efficient sampling. We then propose two methods for learning network structures, including parent sets, Markov blankets, and DAGs, from categorical data. The first cycles through individual edges each MCMC iteration, whereas the second updates the entire structure as a single step. We evaluate the accuracy, power, and MCMC performance of our methods on several simulation studies. Finally, we apply our methodology to uncover prognostic network structure from primary breast cancer samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13267v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Dombowsky, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Haussdorff consistency of MLE in folded normal and Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2509.12206</link>
      <description>arXiv:2509.12206v1 Announce Type: cross 
Abstract: We develop a constant-tracking likelihood theory for two nonregular models: the folded normal and finite Gaussian mixtures. For the folded normal, we prove boundary coercivity for the profiled likelihood, show that the profile path of the location parameter exists and is strictly decreasing by an implicit-function argument, and establish a unique profile maximizer in the scale parameter. Deterministic envelopes for the log-likelihood, the score, and the Hessian yield elementary uniform laws of large numbers with finite-sample bounds, avoiding covering numbers. Identification and Kullback-Leibler separation deliver consistency. A sixth-order expansion of the log hyperbolic cosine creates a quadratic-minus-quartic contrast around zero, leading to a nonstandard one-fourth-power rate for the location estimator at the kink and a standard square-root rate for the scale estimator, with a uniform remainder bound. For finite Gaussian mixtures with distinct components and positive weights, we give a short identifiability proof up to label permutations via Fourier and Vandermonde ideas, derive two-sided Gaussian envelopes and responsibility-based gradient bounds on compact sieves, and obtain almost-sure and high-probability uniform laws with explicit constants. Using a minimum-matching distance on permutation orbits, we prove Hausdorff consistency on fixed and growing sieves. We quantify variance-collapse spikes via an explicit spike-bonus bound and show that a quadratic penalty in location and log-scale dominates this bonus, making penalized likelihood coercive; when penalties shrink but sample size times penalty diverges, penalized estimators remain consistent. All proofs are constructive, track constants, verify measurability of maximizers, and provide practical guidance for tuning sieves, penalties, and EM-style optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12206v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Koustav Mallik</dc:creator>
    </item>
    <item>
      <title>Transporting Predictions via Double Machine Learning: Predicting Partially Unobserved Students' Outcomes</title>
      <link>https://arxiv.org/abs/2509.12533</link>
      <description>arXiv:2509.12533v1 Announce Type: cross 
Abstract: Educational policymakers often lack data on student outcomes in regions where standardized tests were not administered. Machine learning techniques can be used to predict unobserved outcomes in target populations by training models on data from a source population. However, differences between the source and target populations, particularly in covariate distributions, can reduce the transportability of these models, potentially reducing predictive accuracy and introducing bias. We propose using double machine learning for a covariate-shift weighted model. First, we estimate the overlap score-namely, the probability that an observation belongs to the source dataset given its covariates. Second, balancing weights, defined as the density ratio of target-to-source membership probabilities, are used to reweight the individual observations' contribution to the loss or likelihood function in the target outcome prediction model. This approach downweights source observations that are less similar to the target population, allowing predictions to rely more heavily on observations with greater overlap. As a result, predictions become more generalizable under covariate shift. We illustrate this framework in the context of uncertain data on students' standardized financial literacy scores (FLS). Using Bayesian Additive Regression Trees (BART), we predict missing FLS. We find minimal differences in predictive performance between the weighted and unweighted models, suggesting limited covariate shift in our empirical setting. Nonetheless, the proposed approach provides a principled framework for addressing covariate shift and is broadly applicable to predictive modeling in the social and health sciences, where differences between source and target populations are common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12533v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Falco J. Bargagli-Stoffi, Emma Landry, Kevin P. Josey, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte</dc:creator>
    </item>
    <item>
      <title>A Doubly-Flexible Model Based on Generalized Gamma Frailty for Two-component Load-sharing Systems</title>
      <link>https://arxiv.org/abs/2509.12686</link>
      <description>arXiv:2509.12686v1 Announce Type: cross 
Abstract: For two-component load-sharing systems, a doubly-flexible model is developed where the generalized Fruend bivariate (GFB) distribution is used for the baseline of the component lifetimes, and the generalized gamma (GG) family of distributions is used to incorporate a shared frailty that captures dependence between the component lifetimes. The proposed model structure results in a very general two-way class of models that enables a researcher to choose an appropriate model for a given two-component load-sharing data within the respective families of distributions. The GFB-GG model structure provides better fit to two-component load-sharing systems compared to existing models. Fitting methods for the proposed model, based on direct optimization and an expectation maximization (EM) type algorithm, are discussed. Through simulations, effectiveness of the fitting methods is demonstrated. Also, through simulations, it is shown that the proposed model serves the intended purpose of model choice for a given two-component load-sharing data. A simulation case, and analysis of a real dataset are presented to illustrate the strength of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12686v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilpi Biswas, Ayon Ganguly, Debanjan Mitra</dc:creator>
    </item>
    <item>
      <title>Optimal Conformal Prediction, E-values, Fuzzy Prediction Sets and Subsequent Decisions</title>
      <link>https://arxiv.org/abs/2509.13130</link>
      <description>arXiv:2509.13130v1 Announce Type: cross 
Abstract: We make three contributions to conformal prediction. First, we propose fuzzy conformal confidence sets that offer a degree of exclusion, generalizing beyond the binary inclusion/exclusion offered by classical confidence sets. We connect fuzzy confidence sets to e-values to show this degree of exclusion is equivalent to an exclusion at different confidence levels, capturing precisely what e-values bring to conformal prediction. We show that a fuzzy confidence set is a predictive distribution with a more appropriate error guarantee. Second, we derive optimal conformal confidence sets by interpreting the minimization of the expected measure of the confidence set as an optimal testing problem against a particular alternative. We use this to characterize exactly in what sense traditional conformal prediction is optimal. Third, we generalize the inheritance of guarantees by subsequent minimax decisions from confidence sets to fuzzy confidence sets. All our results generalize beyond the exchangeable conformal setting to prediction sets for arbitrary models. In particular, we find that any valid test (e-value) for a hypothesis automatically defines a (fuzzy) prediction confidence set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13130v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>De Finetti + Sanov = Bayes</title>
      <link>https://arxiv.org/abs/2509.13283</link>
      <description>arXiv:2509.13283v1 Announce Type: cross 
Abstract: We develop a framework for the operationalization of models and parameters by combining de Finetti's representation theorem with a conditional form of Sanov's theorem. This synthesis, the tilted de Finetti theorem, shows that conditioning exchangeable sequences on empirical moment constraints yields predictive laws in exponential families via the I-projection of a baseline measure. Parameters emerge as limits of empirical functionals, providing a probabilistic foundation for maximum entropy (MaxEnt) principles. This explains why exponential tilting governs likelihood methods and Bayesian updating, connecting naturally to finite-sample concentration rates that anticipate PAC-Bayes bounds. Examples include Gaussian scale mixtures, where symmetry uniquely selects location-scale families, and Jaynes' Brandeis dice problem, where partial information tilts the uniform law. Broadly, the theorem unifies exchangeability, large deviations, and entropy concentration, clarifying the ubiquity of exponential families and MaxEnt's role as the inevitable predictive limit under partial information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13283v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Polson, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>Identification of Partial-Differential-Equations-Based Models from Noisy Data via Splines</title>
      <link>https://arxiv.org/abs/2103.10231</link>
      <description>arXiv:2103.10231v4 Announce Type: replace 
Abstract: We propose a two-stage method called \textit{Spline Assisted Partial Differential Equation based Model Identification (SAPDEMI)} to identify partial differential equation (PDE)-based models from noisy data. In the first stage, we employ the cubic splines to estimate unobservable derivatives. The underlying PDE is based on a subset of these derivatives. This stage is computationally efficient: its computational complexity is a product of a constant with the sample size; this is the lowest possible order of computational complexity. In the second stage, we apply the Least Absolute Shrinkage and Selection Operator (Lasso) to identify the underlying PDE-based model. Statistical properties are developed, including the model identification accuracy. We validate our theory through various numerical examples and a real data case study. The case study is based on a National Aeronautics and Space Administration (NASA) data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.10231v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujie Zhao, Xiaoming Huo, Yajun Mei</dc:creator>
    </item>
    <item>
      <title>Learning from a Biased Sample</title>
      <link>https://arxiv.org/abs/2209.01754</link>
      <description>arXiv:2209.01754v4 Announce Type: replace 
Abstract: The empirical risk minimization approach to data-driven decision making requires access to training data drawn under the same conditions as those that will be faced when the decision rule is deployed. However, in a number of settings, we may be concerned that our training sample is biased in the sense that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. We propose a model of sampling bias called conditional $\Gamma$-biased sampling, where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor. Applying the distributionally robust optimization framework, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions that can generate the training distribution under $\Gamma$-biased sampling. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves, and propose a deep learning algorithm whose loss function captures our robust learning target. We empirically validate our proposed method in a case study on prediction of mental health scores from health survey data and a case study on ICU length of stay prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01754v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roshni Sahoo, Lihua Lei, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Adaptive Neyman Allocation</title>
      <link>https://arxiv.org/abs/2309.08808</link>
      <description>arXiv:2309.08808v3 Announce Type: replace 
Abstract: In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. Using online A/B testing data from a social media site, we demonstrate the effectiveness of our adaptive Neyman allocation algorithm, highlighting its practicality especially when applied with only a limited number of stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08808v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Correlated Bayesian Additive Regression Trees with Gaussian Process for Regression Analysis of Dependent Data</title>
      <link>https://arxiv.org/abs/2311.18699</link>
      <description>arXiv:2311.18699v3 Announce Type: replace 
Abstract: Bayesian Additive Regression Trees (BART) has gained widespread popularity, inspiring numerous extensions across diverse applications. However, relatively little attention has been given to modeling dependent data. To fill this gap, we introduce Correlated BART (CBART), which extends BART to account for correlated errors. With a dummy representation, efficient matrix computation was developed for the estimation of CBART. Building on CBART, we propose CBART$\unicode{0x2010}$GP, a nonparametric regression model that integrates CBART with a Gaussian process (GP) in an additive framework. In CBART$\unicode{0x2010}$GP, CBART retrieves the true signal of covariates$\unicode{0x2010}$response relationship, while the GP extracts the dependency structure of residuals. To enable scalable inference of CBART$\unicode{0x2010}$GP, we develop a two$\unicode{0x2010}$stage analysis of variance with weighted residuals approach to substantially reduce the computational complexity. Simulation studies demonstrate that CBART-GP not only accurately recovers the true covariate$\unicode{0x2010}$response relationship but also achieves strong predictive performance. A real world application further illustrates its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18699v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuetao Lu a, Robert E. McCulloch</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory of the Best-Choice Rerandomization using the Mahalanobis Distance</title>
      <link>https://arxiv.org/abs/2312.02513</link>
      <description>arXiv:2312.02513v2 Announce Type: replace 
Abstract: Rerandomization, a design that utilizes pretreatment covariates and improves their balance between different treatment groups, has received attention recently in both theory and practice. From a survey by Bruhn and McKenzie (2009), there are at least two types of rerandomization that are used in practice: the first rerandomizes the treatment assignment until covariate imbalance is below a prespecified threshold; the second randomizes the treatment assignment multiple times and chooses the one with the best covariate balance. In this paper we will consider the second type of rerandomization, namely the best-choice rerandomization, whose theory and inference are still lacking in the literature. In particular, we will focus on the best-choice rerandomization that uses the Mahalanobis distance to measure covariate imbalance, which is one of the most commonly used imbalance measure for multivariate covariates and is invariant to affine transformations of covariates. We will study the large-sample repeatedly sampling properties of the best-choice rerandomization, allowing both the number of covariates and the number of tried complete randomizations to increase with the sample size. We show that the asymptotic distribution of the difference-in-means estimator is more concentrated around the true average treatment effect under rerandomization than under the complete randomization, and propose large-sample accurate confidence intervals for rerandomization that are shorter than that for the completely randomized experiment. We further demonstrate that, with moderate number of covariates and with the number of tried randomizations increasing polynomially with the sample size, the best-choice rerandomization can achieve the ideally optimal precision that one can expect even with perfectly balanced covariates. The developed theory and methods are also illustrated using real field experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02513v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Xinran Li</dc:creator>
    </item>
    <item>
      <title>fdrSAFE: Selective Aggregation for Local False Discovery Rate Estimation</title>
      <link>https://arxiv.org/abs/2401.12865</link>
      <description>arXiv:2401.12865v2 Announce Type: replace 
Abstract: Estimating local false discovery rates (fdr) is central to large-scale multiple hypothesis testing, yet different methods often produce divergent results, and there is little guidance for selecting among them. Because ground truth hypothesis labels are unobservable, standard model selection cannot be used. We present fdrSAFE (selective aggregation for fdr estimation), a data-driven selective ensembling approach that estimates model performances on synthetic datasets designed to resemble the observed data but with known ground truth. With simulation studies and an experimental spike-in transcriptomic dataset, we show that fdrSAFE achieves robust near-optimality, performing well across diverse settings where baseline model performances vary. Along with improved fdr estimates, this framework enhances replicability by replacing arbitrary model choice with a principled, data-adaptive procedure. An open-source R software package is available on GitHub at jennalandy/fdrSAFE</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12865v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenna M. Landy, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Direction Preferring Confidence Intervals</title>
      <link>https://arxiv.org/abs/2404.00319</link>
      <description>arXiv:2404.00319v2 Announce Type: replace 
Abstract: Confidence intervals (CIs) are instrumental in statistical analysis, providing a range estimate of the parameters. In modern statistics, selective inference is common, where only certain parameters are highlighted. However, this selective approach can bias the inference, leading some to advocate for the use of CIs over p-values. To increase the flexibility of confidence intervals, we introduce direction-preferring CIs, enabling analysts to focus on parameters trending in a particular direction. We present these types of CIs in two settings: First, when there is no selection of parameters; and second, for situations involving parameter selection, where we offer a conditional version of the direction-preferring CIs. Both of these methods build upon the foundations of Modified Pratt CIs, which rely on non-equivariant acceptance regions to achieve longer intervals in exchange for improved sign exclusions. We show that for selected parameters out of m &gt; 1 initial parameters of interest, CIs aimed at controlling the false coverage rate, have higher power to determine the sign compared to conditional CIs. We also show that conditional confidence intervals control the marginal false coverage rate (mFCR) under any dependency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00319v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tzviel Frostig, Yoav Benjamini, Ruth Heller</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Observational Studies with Flexible Matched Designs</title>
      <link>https://arxiv.org/abs/2411.10623</link>
      <description>arXiv:2411.10623v2 Announce Type: replace 
Abstract: Observational studies provide invaluable opportunities to draw causal inference, but they may suffer from biases due to pretreatment difference between treated and control units. Matching is a popular approach to reduce observed covariate imbalance. To tackle unmeasured confounding, a sensitivity analysis is often conducted to investigate how robust a causal conclusion is to the strength of unmeasured confounding. For matched observational studies, Rosenbaum proposed a sensitivity analysis framework that uses the randomization of treatment assignments as the ``reasoned basis'' and imposes no model assumptions on the potential outcomes as well as their dependence on the observed and unobserved confounding factors. However, this otherwise appealing framework requires exact matching to guarantee its validity, which is hard to achieve in practice. In this paper we provide an alternative inferential framework that shares the same procedure as Rosenbaum's approach but relies on a different justification. Our framework allows flexible matching algorithms and utilizes alternative source of randomness, in particular random permutations of potential outcomes instead of treatment assignments, to guarantee statistical validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10623v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinran Li</dc:creator>
    </item>
    <item>
      <title>Clustering methods for Categorical Time Series and Sequences : A scoping review</title>
      <link>https://arxiv.org/abs/2509.07885</link>
      <description>arXiv:2509.07885v2 Announce Type: replace 
Abstract: Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
  Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
  Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/ )
  Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
  Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07885v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Semiparametric Triple Difference Estimators</title>
      <link>https://arxiv.org/abs/2502.19788</link>
      <description>arXiv:2502.19788v3 Announce Type: replace-cross 
Abstract: The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant composition of units. From the estimation perspective, we develop semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. These estimators are based on the cross-fitting technique, and flexible machine learning tools can be used to estimate the nuisance components. We characterize conditions under which our proposed estimators are efficient, doubly robust, root-n consistent and asymptotically normal. As an application of our proposed methodology, we examined the effect of mandated maternity benefits on the hourly wages of women of childbearing age and found that these mandates result in a 2.6% drop in hourly wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19788v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Akbari, Negar Kiyavash, AmirEmad Ghassami</dc:creator>
    </item>
  </channel>
</rss>
