<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.PE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.PE</link>
    <description>q-bio.PE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.PE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The rules of multiplayer cooperation in networks of communities</title>
      <link>https://arxiv.org/abs/2404.03718</link>
      <description>arXiv:2404.03718v1 Announce Type: new 
Abstract: Community organization permeates both social and biological complex systems. To study its interplay with behavior emergence, we model mobile structured populations with multiplayer interactions. We derive general analytical methods for evolutionary dynamics under high home fidelity when populations self-organize into networks of asymptotically isolated communities. In this limit, community organization dominates over the network structure and emerging behavior is independent of network topology. We obtain the rules of multiplayer cooperation in networks of communities for different types of social dilemmas. The success of cooperation is a result of the benefits shared amongst communal cooperators outperforming the benefits reaped by defectors in mixed communities. Under weak selection, cooperation can evolve and be stable for any size (Q) and number (M) of communities if the reward-to-cost ratio (V/K) of public goods is higher than a critical value. Community organization is a solid mechanism for sustaining the evolution of cooperation under public goods dilemmas, particularly when populations are organized into a higher number of smaller communities. Contrary to public goods dilemmas relating to production, the multiplayer Hawk-Dove (HD) dilemma is a commons dilemma focusing on the fair consumption of preexisting resources. This game holds mixed results but tends to favour cooperation under larger communities, highlighting that the two types of social dilemmas might lead to solid differences in the behaviour adopted under community structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03718v1</guid>
      <category>q-bio.PE</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogo L. Pires, Mark Broom</dc:creator>
    </item>
    <item>
      <title>The Origin of Mutational Epistasis</title>
      <link>https://arxiv.org/abs/2404.04041</link>
      <description>arXiv:2404.04041v1 Announce Type: new 
Abstract: The interconnected processes of protein folding, mutations, epistasis, and evolution have all been the subject of extensive analysis throughout the years due to their significance for structural and evolutionary biology. The origin (molecular basis) of epistasis (the non-additive interactions between mutations) is still, nonetheless, unknown. The existence of a new perspective on protein folding (a problem that needs to be conceived as an analytic whole) will enable us to shed light on the origin of mutational epistasis at the simplest level (within proteins) while also uncovering the reasons on why the genetic background in which they occur (a key component of molecular evolution) could foster changes in epistasis effects. Additionally, because mutations are the source of epistasis, more research is needed to determine the impact of posttranslational modifications (which have the potential to increase the diversity of the proteome by several orders of magnitude) on both mutational epistasis and protein evolvability. Finally, a protein evolution thermodynamic-based analysis that does not consider specific mutational steps or epistasis effects will be discussed. Our study explores the complex processes behind the evolution of proteins upon mutations, clearing up some previously unresolved issues and providing direction for further research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04041v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.BM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge A. Vila</dc:creator>
    </item>
    <item>
      <title>Ambiguity in the use of SIR models to fit epidemic incidence data</title>
      <link>https://arxiv.org/abs/2404.04181</link>
      <description>arXiv:2404.04181v1 Announce Type: new 
Abstract: When fitting a multi-parameter model to a data set, computer algorithms may suggest that a range of parameters provide equally reasonable fits, making the parameter estimation difficult. Here, we prove this fact for an SIR model. We say a set of parameter values is a good fit to outbreak data if the solution has the data's three most significant characteristics: the standard deviation, the mean time, and the total number of cases. In our model, in addition to the "basic reproduction number" $R_0$, three other parameters need to be estimated to fit a solution to outbreak data. We will show that those parameters can be chosen so that each gives a linear transformation of a solution's incidence data. As a result, we show that for every choice of $R_0&gt;1$, there is a good fit for each outbreak. We also illustrate our results by providing the least square best fits of the New York City and London data sets of the Omicron variant of COVID-19. Furthermore, we show how versions of the SIR model with $N$ compartments have far more good fits- - indeed a high dimensional set of good fits -- for each target -- showing that more complicated models may have an even greater problem in overparametrizing outbreak characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04181v1</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>B Shayak, Sana Jahedi, James A Yorke</dc:creator>
    </item>
    <item>
      <title>The effects of HIV self-testing on HIV incidence and awareness of status among men who have sex with men in the United States: Insights from a novel compartmental model</title>
      <link>https://arxiv.org/abs/2404.04222</link>
      <description>arXiv:2404.04222v1 Announce Type: new 
Abstract: The OraQuick In-Home HIV self-test represents a fast, inexpensive, and convenient method for users to assess their HIV status. If integrated thoughtfully into existing testing practices, accompanied by efficient pathways to formal diagnosis, self-testing could both enhance HIV awareness and reduce HIV incidence. However, currently available self-tests are less sensitive, particularly for recent infection, than gold-standard laboratory tests. It is important to understand the impact if some portion of standard testing is replaced by self-tests. We introduced a novel compartmental model to evaluate the effects of self-testing among gay, bisexual and other men who have sex with men (MSM) in the United States for the period 2020 to 2030. We varied the model for different screening rates, self-test proportions, and delays to diagnosis for those identified through self-tests to determine the potential impact on HIV incidence and awareness of status. When HIV self-tests are strictly supplemental, self-testing can decrease HIV incidence among MSM in the US by up to 10% and increase awareness of status among MSM from 85% to 91% over a 10-year period, provided linkage to care and formal diagnosis occur promptly following a positive self-test (90 days or less). As self-tests replace a higher percentage laboratory-based testing algorithms, increases in overall testing rates were necessary to ensure reductions in HIV incidence. However, such increases were small (under 10% for prompt engagement in care and moderate levels of replacement). Improvements in self-test sensitivity and/or decreases in the detection period may further reduce any necessary increases in overall testing. Our study suggests that, if properly utilized, self-testing can provide significant long-term reductions to HIV incidence and improve awareness of HIV status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04222v1</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Viguerie, Chaitra Gopalappa, Cynthia M. Lyles, Paul G. Farnham</dc:creator>
    </item>
    <item>
      <title>Computing macroscopic reaction rates in reaction-diffusion systems using Monte Carlo simulations</title>
      <link>https://arxiv.org/abs/2404.03089</link>
      <description>arXiv:2404.03089v1 Announce Type: cross 
Abstract: Stochastic reaction-diffusion models are employed to represent many complex physical, biological, societal, and ecological systems. The macroscopic reaction rates describing the large-scale kinetics in such systems are effective, scale-dependent parameters that need to be either measured experimentally or computed using a microscopic model. In a Monte Carlo simulation of stochastic reaction-diffusion systems, microscopic probabilities for specific events to happen serve as the input control parameters. Finding the functional dependence of emergent macroscopic rates on the microscopic probabilities is a difficult problem, and there is no systematic analytical way to achieve this goal. Therefore, we introduce a straightforward numerical method of using Monte Carlo simulations to evaluate the macroscopic reaction rates by directly obtaining the count statistics of how many events occur per simulation time step. Our technique is first tested on well-understood fundamental examples, namely restricted birth processes, diffusion-limited two-particle coagulation, and two-species pair annihilation kinetics. Next we utilize the thus gained experience to investigate how the microscopic algorithmic probabilities become coarse-grained into effective macroscopic rates in more complex model systems such as the Lotka--Volterra model for predator-prey competition and coexistence, as well as the rock-paper-scissors or cyclic Lotka--Volterra model as well as its May--Leonard variant that capture population dynamics with cyclic dominance motifs. Thereby we achieve a deeper understanding of coarse-graining in spatially extended stochastic reaction systems and the nontrivial relationships between the associated microscopic and macroscopic model parameters. The proposed technique should generally provide a useful means to better fit Monte Carlo simulation results to experimental or observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03089v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Swailem, Uwe C. T\"auber</dc:creator>
    </item>
    <item>
      <title>Machine learning augmented diagnostic testing to identify sources of variability in test performance</title>
      <link>https://arxiv.org/abs/2404.03678</link>
      <description>arXiv:2404.03678v1 Announce Type: cross 
Abstract: Diagnostic tests which can detect pre-clinical or sub-clinical infection, are one of the most powerful tools in our armoury of weapons to control infectious diseases. Considerable effort has been therefore paid to improving diagnostic testing for human, plant and animal diseases, including strategies for targeting the use of diagnostic tests towards individuals who are more likely to be infected. Here, we follow other recent proposals to further refine this concept, by using machine learning to assess the situational risk under which a diagnostic test is applied to augment its interpretation . We develop this to predict the occurrence of breakdowns of cattle herds due to bovine tuberculosis, exploiting the availability of exceptionally detailed testing records. We show that, without compromising test specificity, test sensitivity can be improved so that the proportion of infected herds detected by the skin test, improves by over 16 percentage points. While many risk factors are associated with increased risk of becoming infected, of note are several factors which suggest that, in some herds there is a higher risk of infection going undetected, including effects that are correlated to the veterinary practice conducting the test, and number of livestock moved off the herd.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03678v1</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Banks, Aeron Sanchez, Vicki Stewart, Kate Bowen, Graham Smith, Rowland R. Kao</dc:creator>
    </item>
    <item>
      <title>Optimal Vaccination Policy to Prevent Endemicity: A Stochastic Model</title>
      <link>https://arxiv.org/abs/2306.13633</link>
      <description>arXiv:2306.13633v2 Announce Type: replace 
Abstract: We examine here the effects of recurrent vaccination and waning immunity on the establishment of an endemic equilibrium in a population. An individual-based model that incorporates memory effects for transmission rate during infection and subsequent immunity is introduced, considering stochasticity at the individual level. By letting the population size going to infinity, we derive a set of equations describing the large scale behavior of the epidemic. The analysis of the model's equilibria reveals a criterion for the existence of an endemic equilibrium, which depends on the rate of immunity loss and the distribution of time between booster doses. The outcome of a vaccination policy in this context is influenced by the efficiency of the vaccine in blocking transmissions and the distribution pattern of booster doses within the population. Strategies with evenly spaced booster shots at the individual level prove to be more effective in preventing disease spread compared to irregularly spaced boosters, as longer intervals without vaccination increase susceptibility and facilitate more efficient disease transmission. We provide an expression for the critical fraction of the population required to adhere to the vaccination policy in order to eradicate the disease, that resembles a well-known threshold for preventing an outbreak with an imperfect vaccine. We also investigate the consequences of unequal vaccine access in a population and prove that, under reasonable assumptions, fair vaccine allocation is the optimal strategy to prevent endemicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13633v2</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F\'elix Foutel-Rodier, Arthur Charpentier, H\'el\`ene Gu\'erin</dc:creator>
    </item>
    <item>
      <title>A general multi-scale description of metastable adaptive motion across fitness valleys</title>
      <link>https://arxiv.org/abs/2112.12675</link>
      <description>arXiv:2112.12675v2 Announce Type: replace-cross 
Abstract: We consider a stochastic individual-based model of adaptive dynamics on a finite trait graph $G=(V,E)$. The evolution is driven by a linear birth rate, a density dependent logistic death rate an the possibility of mutations along the (possibly directed) edges in $E$. We study the limit of small mutation rates for a simultaneously diverging population size. Closing the gap between the works of Bovier, Coquille and Smadi (2019) and Coquille, Kraut and Smadi (2021), we give a precise description of transitions between evolutionary stable conditions (ESC), where multiple mutations are needed to cross a valley in the fitness landscape. The system shows a metastable behaviour on several divergent time scales associated to a degree of stability. We develop the framework of a meta graph that is constituted of ESCs and possible metastable transitions between those. This allows for a concise description of the multi-scale jump chain arising from concatenating several jumps. Finally, for each of the various time scale, we prove the convergence of the population process to a Markov jump process visiting only ESCs of sufficiently high stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12675v2</guid>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Esser, Anna Kraut</dc:creator>
    </item>
    <item>
      <title>Taxonomic classification with maximal exact matches in KATKA kernels and minimizer digests</title>
      <link>https://arxiv.org/abs/2402.06935</link>
      <description>arXiv:2402.06935v2 Announce Type: replace-cross 
Abstract: For taxonomic classification, we are asked to index the genomes in a phylogenetic tree such that later, given a DNA read, we can quickly choose a small subtree likely to contain the genome from which that read was drawn. Although popular classifiers such as Kraken use $k$-mers, recent research indicates that using maximal exact matches (MEMs) can lead to better classifications. For example, we can build an augmented FM-index over the the genomes in the tree concatenated in left-to-right order; for each MEM in a read, find the interval in the suffix array containing the starting positions of that MEM's occurrences in those genomes; find the minimum and maximum values stored in that interval; take the lowest common ancestor (LCA) of the genomes containing the characters at those positions. This solution is practical, however, only when the total size of the genomes in the tree is fairly small. In this paper we consider applying the same solution to three lossily compressed representations of the genomes' concatenation: a KATKA kernel, which discards characters that are not in the first or last occurrence of any $k_{\max}$-tuple, for a parameter $k_{\max}$; a minimizer digest; a KATKA kernel of a minimizer digest. With a test dataset and these three representations of it, simulated reads and various parameter settings, we checked how many reads' longest MEMs occurred only in the sequences from which those reads were generated ("true positive" reads). For some parameter settings we achieved significant compression while only slightly decreasing the true-positive rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06935v2</guid>
      <category>cs.DS</category>
      <category>q-bio.GN</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominika Draesslerov\'a, Omar Ahmed, Travis Gagie, Jan Holub, Ben Langmead, Giovanni Manzini, Gonzalo Navarro</dc:creator>
    </item>
  </channel>
</rss>
