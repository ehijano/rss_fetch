<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ballistic Convergence in Hit-and-Run Monte Carlo and a Coordinate-free Randomized Kaczmarz Algorithm</title>
      <link>https://arxiv.org/abs/2412.07643</link>
      <description>arXiv:2412.07643v1 Announce Type: new 
Abstract: Hit-and-Run is a coordinate-free Gibbs sampler, yet the quantitative advantages of its coordinate-free property remain largely unexplored beyond empirical studies. In this paper, we prove sharp estimates for the Wasserstein contraction of Hit-and-Run in Gaussian target measures via coupling methods and conclude mixing time bounds. Our results uncover ballistic and superdiffusive convergence rates in certain settings. Furthermore, we extend these insights to a coordinate-free variant of the randomized Kaczmarz algorithm, an iterative method for linear systems, and demonstrate analogous convergence rates. These findings offer new insights into the advantages and limitations of coordinate-free methods for both sampling and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07643v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Andreas Eberle, Stefan Oberd\"orster</dc:creator>
    </item>
    <item>
      <title>Statistical-Computational Trade-offs for Recursive Adaptive Partitioning Estimators</title>
      <link>https://arxiv.org/abs/2411.04394</link>
      <description>arXiv:2411.04394v2 Announce Type: cross 
Abstract: Models based on recursive adaptive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over $d$ binary features, showing that when the true regression function $f^*$ does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedy training requires $\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does satisfy MSP, greedy training can attain small estimation error with only $O(\log d)$ samples. This dichotomy mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with $O(\log d)$ samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04394v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Autonomy in the Real-World: Autonomous Trajectory Planning for Asteroid Reconnaissance via Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2412.06816</link>
      <description>arXiv:2412.06816v1 Announce Type: cross 
Abstract: This paper presents the development and evaluation of an optimization-based autonomous trajectory planning algorithm for the asteroid reconnaissance phase of a deep-space exploration mission. The reconnaissance phase is a low-altitude flyby to collect detailed information around a potential landing site. Although such autonomous deep-space exploration missions have garnered considerable interest recently, state-of-the-practice in trajectory design involves a time-intensive ground-based open-loop process that forward propagates multiple trajectories with a range of initial conditions and parameters to account for uncertainties in spacecraft knowledge and actuation. In this work, we introduce a stochastic trajectory optimization-based approach to generate trajectories that satisfy both the mission and spacecraft safety constraints during the reconnaissance phase of the Deep-space Autonomous Robotic Explorer (DARE) mission concept, which seeks to travel to and explore a near-Earth object autonomously, with minimal ground intervention. We first use the Multi-Spacecraft Concept and Autonomy Tool (MuSCAT) simulation framework to rigorously validate the underlying modeling assumptions for our trajectory planner and then propose a method to transform this stochastic optimal control problem into a deterministic one tailored for use with an off-the-shelf nonlinear solver. Finally, we demonstrate the efficacy of our proposed algorithmic approach through extensive numerical experiments and show that it outperforms the state-of-the-practice benchmark used for representative missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06816v1</guid>
      <category>physics.space-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kazuya Echigo, Abhishek Cauligi, Saptarshi Bandyopadhyay, Dan Scharf, Gregory Lantoine, Beh\c{c}et A\c{c}{\i}kme\c{s}e, Issa Nesnas</dc:creator>
    </item>
    <item>
      <title>Application of Random Matrix Theory in High-Dimensional Statistics</title>
      <link>https://arxiv.org/abs/2412.06848</link>
      <description>arXiv:2412.06848v1 Announce Type: cross 
Abstract: This review article provides an overview of random matrix theory (RMT) with a focus on its growing impact on the formulation and inference of statistical models and methodologies. Emphasizing applications within high-dimensional statistics, we explore key theoretical results from RMT and their role in addressing challenges associated with high-dimensional data. The discussion highlights how advances in RMT have significantly influenced the development of statistical methods, particularly in areas such as covariance matrix inference, principal component analysis (PCA), signal processing, and changepoint detection, demonstrating the close interplay between theory and practice in modern high-dimensional statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06848v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnaneel Bhattacharyya, Srijan Chattopadhyay, Sevantee Basu</dc:creator>
    </item>
    <item>
      <title>Automatic Doubly Robust Forests</title>
      <link>https://arxiv.org/abs/2412.07184</link>
      <description>arXiv:2412.07184v1 Announce Type: cross 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF combines the automatic debiasing framework using the Riesz representer (Chernozhukov et al., 2022) with non-parametric, forest-based estimation methods for the conditional moment (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term nor impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient for making predictions at multiple query points and significantly reduces runtime compared to methods such as Orthogonal Random Forest (Oprescu et al., 2019). We establish the consistency and asymptotic normality results of DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07184v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaomeng Chen, Junting Duan, Victor Chernozhukov, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Optimal heteroskedasticity testing in nonparametric regression</title>
      <link>https://arxiv.org/abs/2310.12424</link>
      <description>arXiv:2310.12424v3 Announce Type: replace 
Abstract: Heteroskedasticity testing in nonparametric regression is a classic statistical problem with important practical applications, yet fundamental limits are unknown. Adopting a minimax perspective, this article considers the testing problem in the context of an $\alpha$-H\"{o}lder mean and a $\beta$-H\"{o}lder variance function. For $\alpha &gt; 0$ and $\beta \in (0, 1/2)$, the sharp minimax separation rate $n^{-4\alpha} + n^{-4\beta/(4\beta+1)} + n^{-2\beta}$ is established. To achieve the minimax separation rate, a kernel-based statistic using first-order squared differences is developed. Notably, the statistic estimates a proxy rather than a natural quadratic functional (the squared distance between the variance function and its best $L^2$ approximation by a constant) suggested in previous work.
  The setting where no smoothness is assumed on the variance function is also studied; the variance profile across the design points can be arbitrary. Despite the lack of structure, consistent testing turns out to still be possible by using the Gaussian character of the noise, and the minimax rate is shown to be $n^{-4\alpha} + n^{-1/2}$. Exploiting noise information happens to be a fundamental necessity as consistent testing is impossible if nothing more than zero mean and unit variance is known about the noise distribution. Furthermore, in the setting where the variance function is $\beta$-H\"{o}lder but heteroskedasticity is measured only with respect to the design points, the minimax separation rate is shown to be $n^{-4\alpha} + n^{-\left((1/2) \vee (4\beta/(4\beta+1))\right)}$ when the noise is Gaussian and $n^{-4\alpha} + n^{-4\beta/(4\beta+1)} + n^{-2\beta}$ when the noise distribution is unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12424v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodh Kotekal, Soumyabrata Kundu</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v2 Announce Type: replace 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
    <item>
      <title>Efficient Inference on High-Dimensional Linear Models with Missing Outcomes</title>
      <link>https://arxiv.org/abs/2309.06429</link>
      <description>arXiv:2309.06429v3 Announce Type: replace-cross 
Abstract: This paper is concerned with inference on the regression function of a high-dimensional linear model when outcomes are missing at random. We propose an estimator which combines a Lasso pilot estimate of the regression function with a bias correction term based on the weighted residuals of the Lasso regression. The weights depend on estimates of the missingness probabilities (propensity scores) and solve a convex optimization program that trades off bias and variance optimally. Provided that the propensity scores can be pointwise consistently estimated at in-sample data points, our proposed estimator for the regression function is asymptotically normal and semi-parametrically efficient among all asymptotically linear estimators. Furthermore, the proposed estimator keeps its asymptotic properties even if the propensity scores are estimated by modern machine learning techniques. We validate the finite-sample performance of the proposed estimator through comparative simulation studies and the real-world problem of inferring the stellar masses of galaxies in the Sloan Digital Sky Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06429v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Alexander Giessing, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures</title>
      <link>https://arxiv.org/abs/2311.03242</link>
      <description>arXiv:2311.03242v3 Announce Type: replace-cross 
Abstract: We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03242v3</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Miranda, Janina Sch\"utte, David Sommer, Martin Eigel</dc:creator>
    </item>
  </channel>
</rss>
