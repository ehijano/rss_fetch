<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scaling and tuning to criticality in resting-state human magnetoencephalography</title>
      <link>https://arxiv.org/abs/2602.17820</link>
      <description>arXiv:2602.17820v1 Announce Type: new 
Abstract: Scaling laws in biological neural networks have long been investigated. From 1/f noise to neuronal avalanches, evidence of scaling in brain activity has been increasingly linked to tuning to or near criticality. The concept of scaling is intimately related to the renormalization group (RG), in essence providing coarse-grained, simplified descriptions that generalize to classes of diverse physical systems. Following the RG idea, a coarse-graining scheme has recently been proposed for populations of real neurons, and scaling behaviors in collective quantities have been reported in the hippocampus and in different areas of the rat cortex. To bridge the gap between neuronal population scales and species, here we consider large-scale, electrophysiological recordings of human brain activity in the awake resting-state. We demonstrate robust scaling behaviors of collective dynamics across coarse-graining scales, with exponents close to those measured in populations of spiking neurons. Further, we show that dynamics of neuronal avalanches, scale-free cascades of neural activity, are invariant under the proposed coarse-graining approach. Simulations of a non-equilibrium adaptive Ising model inferred from data and apt to reproduce a large repertoire of resting-state brain dynamics indicate that the scaling behaviors of the resting human brain activity emerge close to criticality and depend on the excitation/inhibition (E/I) balance of the network. While extending the range of validity of previous observations at small spatial scales and pointing to common scaling laws in mammals, the results open the way to a robust (currently missing) non-invasive approach to estimate the E/I balance, a key quantity in neuroscience research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17820v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Irem Topal, Anna Poggialini, Marco Dal Maschio, Daniele De Martino, Oren Shriki, Fabrizio Lombardi</dc:creator>
    </item>
    <item>
      <title>Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay</title>
      <link>https://arxiv.org/abs/2602.18401</link>
      <description>arXiv:2602.18401v1 Announce Type: cross 
Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18401v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josue Casco-Rodriguez, Nanda H. Krishna, Richard G. Baraniuk</dc:creator>
    </item>
    <item>
      <title>Critical dynamics governs deep learning</title>
      <link>https://arxiv.org/abs/2507.08527</link>
      <description>arXiv:2507.08527v2 Announce Type: replace 
Abstract: The rapid advances in artificial intelligence (AI) have largely been driven by scaling deep neural networks (DNNs) - increasing model size, data, and computational resources. Yet performance is ultimately governed by network dynamics. The lack of a principled understanding of DNN dynamics beyond heuristic design has contributed to challenges in robustness, suboptimal performance, high energy consumption, and pathologies in continual and AI-generated content learning. In contrast, the human brain appears largely resilient to these problems, and converging evidence suggests this advantage arises from dynamics poised at a critical phase transition. Inspired by this principle, we propose that criticality provides a unifying framework linking structure, dynamics, and function in DNNs. First, analyzing more than 80 state-of-the-art models, we show that a decade of AI progress has implicitly driven successful networks toward criticality - explaining why some architectures succeeded while others failed. Second, we demonstrate that explicitly incorporating criticality into training improves robustness and accuracy while mitigating key limitations of current models. Third, we show that major AI pathologies - including performance degradation in continual learning and model collapse during training on AI-generated data - reflect a loss of critical dynamics. By maintaining networks near criticality, we provide a principled solution to these failures, demonstrating that criticality-based optimization prevents degradation and collapse. Our results establish criticality as a substrate-independent principle of intelligence, connecting AI progress with fundamental principles of brain function, and offering both theoretical insight and practical strategies to ensure long-term DNN performance and resilience as models scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08527v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Vock, Christian Meisel</dc:creator>
    </item>
    <item>
      <title>Ensemble-based graph representation of fMRI data for cognitive brain state classification</title>
      <link>https://arxiv.org/abs/2508.06118</link>
      <description>arXiv:2508.06118v2 Announce Type: replace 
Abstract: fMRI is a non-invasive technique for investigating brain activity, offering high-resolution insights into neural processes. Understanding and decoding cognitive brain states from fMRI depends on how functional interactions are represented. We propose an ensemble-based graph representation in which each edge weight encodes state evidence as the difference between posterior probabilities of two states, estimated by an ensemble of edge-wise probabilistic classifiers from simple pairwise time-series features. We evaluate the method on seven task-fMRI paradigms from the Human Connectome Project, performing binary classification within each paradigm. Using compact node summaries (mean incident edge weights) and logistic regression, we obtain average accuracies of 97.07-99.74 %. We further compare ensemble graphs with conventional correlation graphs using the same graph neural network classifier; ensemble graphs consistently yield higher accuracy (88.00-99.42 % vs 61.86-97.94 % across tasks). Because edge weights have a probabilistic, state-oriented interpretation, the representation supports connection- and region-level interpretability and can be extended to multiclass decoding, regression, other neuroimaging modalities, and clinical classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06118v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniil Vlasenko, Vadim Ushakov, Alexey Zaikin, Denis Zakharov</dc:creator>
    </item>
    <item>
      <title>Simple 3D Pose Features Support Human and Machine Social Scene Understanding</title>
      <link>https://arxiv.org/abs/2511.03988</link>
      <description>arXiv:2511.03988v2 Announce Type: replace-cross 
Abstract: Humans effortlessly recognize social interactions from visual input, yet the underlying computations remain unknown, and social interaction recognition challenges even the most advanced deep neural networks (DNNs). Here, we hypothesized that humans rely on 3D visuospatial pose information to make social judgments, and that this information is largely absent from most vision DNNs. To test these hypotheses, we used a novel pose and depth estimation pipeline to automatically extract 3D body joint positions from short video clips. We compared the ability of these body joints to predict human social judgments in the videos with embeddings from over 350 vision DNNs. We found that body joints predicted social judgments better than most DNNs. We then reduced the 3D body joints to an even more compact feature set describing only the 3D position and direction of people in the videos. We found that this minimal 3D feature set, but not its 2D counterpart, was necessary and sufficient to explain the prediction performance of the full set of body joints. These minimal 3D features also predicted the extent to which DNNs aligned with human social judgments and significantly improved their performance on these tasks. Together, these findings demonstrate that human social perception depends on simple, explicit 3D pose information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03988v2</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenshuo Qin, Leyla Isik</dc:creator>
    </item>
  </channel>
</rss>
