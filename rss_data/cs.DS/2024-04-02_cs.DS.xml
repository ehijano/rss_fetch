<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Apr 2024 19:06:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On contention resolution for the hypergraph matching, knapsack, and $k$-column sparse packing problems</title>
      <link>https://arxiv.org/abs/2404.00041</link>
      <description>arXiv:2404.00041v1 Announce Type: new 
Abstract: The contention resolution framework is a versatile rounding technique used as a part of the relaxation and rounding approach for solving constrained submodular function maximization problems. We apply this framework to the hypergraph matching, knapsack, and $k$-column sparse packing problems. In the hypergraph matching setting, we adapt the technique of Guruganesh, Lee (2018) to non-constructively prove that the correlation gap is at least $\frac{1-e^{-k}}{k}$ and provide a monotone $\left(b,\frac{1-e^{-bk}}{bk}\right)$-balanced contention resolution scheme, generalizing the results of Bruggmann, Zenklusen (2019). For the knapsack problem, we prove that the correlation gap of instances where exactly $k$ copies of each item fit into the knapsack is at least $\frac{1-e^{-2}}{2}$ and provide several monotone contention resolution schemes: a $\frac{1-e^{-2}}{2}$-balanced scheme for instances where all item sizes are strictly bigger than $\frac{1}{2}$, a $\frac{4}{9}$-balanced scheme for instances where all item sizes are at most $\frac{1}{2}$, and a $0.279$-balanced scheme for instances with arbitrary item sizes. For $k$-column sparse packing integer programs, we slightly modify the $\left(2k+o\left(k\right)\right)$-approximation algorithm for $k$-CS-PIP based on the strengthened LP relaxation presented in Brubach et al. (2019) to obtain a $\frac{1}{4k+o\left(k\right)}$-balanced contention resolution scheme and hence a $\left(4k+o\left(k\right)\right)$-approximation algorithm for $k$-CS-PIP based on the natural LP relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00041v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ivan Sergeev</dc:creator>
    </item>
    <item>
      <title>Circular-arc graphs and the Helly property</title>
      <link>https://arxiv.org/abs/2404.00416</link>
      <description>arXiv:2404.00416v1 Announce Type: new 
Abstract: In this paper we investigate some problems related to the Helly properties of circular-arc graphs, which are defined as intersection graphs of arcs of a fixed circle. As such, circular-arc graphs are among the simplest classes of intersection graphs whose models might not satisfy the Helly property. In particular, some cliques of a circular-arc graph might be Helly in some but not all arc intersection models of the graph.
  Our first result is an alternative proof of a theorem by Lin and Szwarcfiter which asserts that for every circular-arc graph $G$ either every normalized model of $G$ satisfies the Helly property or no normalized model of $G$ satisfies this property.
  Further, we study the Helly properties of a single clique of a circular-arc graph $G$. We divide the cliques of $G$ into three types: a clique $C$ of $G$ is always-Helly/always-non-Helly/ambiguous if $C$ is Helly in every/no/(some but not all) normalized model of $G$. We provide a combinatorial description for the cliques of each type, and based on it, we devise a polynomial time algorithm which determines the type of a given clique.
  Finally, we study the Helly Cliques problem, in which we are given an $n$-vertex circular-arc graph $G$ and some of its cliques $C_1, \ldots, C_k$ and we ask if there is an arc intersection model of $G$ in which all the cliques $C_1, \ldots, C_k$ satisfy the Helly property. We show that:
  (1) the Helly Cliques problem admits a $2^{O(k\log{k})}n^{O(1)}$-time algorithm (that is, it is FPT when parametrized by the number of cliques given in the input),
  (2) assuming Exponential Time Hypothesis (ETH), the Helly Cliques problem cannot be solved in time $2^{o(k)}n^{O(1)}$,
  (3) the Helly Cliques problem admits a polynomial kernel of size $O(k^6)$.
  All our results use a data structure, called a PQM-tree, which maintains all normalized models of a circular-arc graph $G$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00416v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Derbisz, Tomasz Krawczyk</dc:creator>
    </item>
    <item>
      <title>Prophet Inequalities with Cancellation Costs</title>
      <link>https://arxiv.org/abs/2404.00527</link>
      <description>arXiv:2404.00527v1 Announce Type: new 
Abstract: Most of the literature on online algorithms and sequential decision-making focuses on settings with "irrevocable decisions" where the algorithm's decision upon arrival of the new input is set in stone and can never change in the future. One canonical example is the classic prophet inequality problem, where realizations of a sequence of independent random variables $X_1, X_2,\ldots$ with known distributions are drawn one by one and a decision maker decides when to stop and accept the arriving random variable, with the goal of maximizing the expected value of their pick. We consider "prophet inequalities with recourse" in the linear buyback cost setting, where after accepting a variable $X_i$, we can still discard $X_i$ later and accept another variable $X_j$, at a \textit{buyback cost} of $f \times X_i$. The goal is to maximize the expected net reward, which is the value of the final accepted variable minus the total buyback cost. Our first main result is an optimal prophet inequality in the regime of $f \geq 1$, where we prove that we can achieve an expected reward $\frac{1+f}{1+2f}$ times the expected offline optimum. The problem is still open for $0&lt;f&lt;1$ and we give some partial results in this regime. In particular, as our second main result, we characterize the asymptotic behavior of the competitive ratio for small $f$ and provide almost matching upper and lower bounds that show a factor of $1-\Theta\left(f\log(\frac{1}{f})\right)$. Our results are obtained by two fundamentally different approaches: One is inspired by various proofs of the classical prophet inequality, while the second is based on combinatorial optimization techniques involving LP duality, flows, and cuts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00527v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farbod Ekbatani, Rad Niazadeh, Pranav Nuti, Jan Vondrak</dc:creator>
    </item>
    <item>
      <title>Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs</title>
      <link>https://arxiv.org/abs/2404.00529</link>
      <description>arXiv:2404.00529v1 Announce Type: new 
Abstract: We study the efficient learnability of low-degree polynomial threshold functions (PTFs) in the presence of a constant fraction of adversarial corruptions. Our main algorithmic result is a polynomial-time PAC learning algorithm for this concept class in the strong contamination model under the Gaussian distribution with error guarantee $O_{d, c}(\text{opt}^{1-c})$, for any desired constant $c&gt;0$, where $\text{opt}$ is the fraction of corruptions. In the strong contamination model, an omniscient adversary can arbitrarily corrupt an $\text{opt}$-fraction of the data points and their labels. This model generalizes the malicious noise model and the adversarial label noise model. Prior to our work, known polynomial-time algorithms in this corruption model (or even in the weaker adversarial label noise model) achieved error $\tilde{O}_d(\text{opt}^{1/(d+1)})$, which deteriorates significantly as a function of the degree $d$.
  Our algorithm employs an iterative approach inspired by localization techniques previously used in the context of learning linear threshold functions. Specifically, we use a robust perceptron algorithm to compute a good partial classifier and then iterate on the unclassified points. In order to achieve this, we need to take a set defined by a number of polynomial inequalities and partition it into several well-behaved subsets. To this end, we develop new polynomial decomposition techniques that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00529v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Sihan Liu, Nikos Zarifis</dc:creator>
    </item>
    <item>
      <title>Improved approximation ratio for covering pliable set families</title>
      <link>https://arxiv.org/abs/2404.00683</link>
      <description>arXiv:2404.00683v1 Announce Type: new 
Abstract: A classic result of Williamson, Goemans, Mihail, and Vazirani [STOC 1993: 708-717] states that the problem of covering an uncrossable set family by a min-cost edge set admits approximation ratio $2$, by a primal-dual algorithm with a reverse delete phase. Recently, Bansal, Cheriyan, Grout, and Ibrahimpur [ICALP 2023: 15:1-15:19] showed that this algorithm achieves approximation ratio $16$ for a larger class of set families, that have much weaker uncrossing properties. In this paper we will refine their analysis and show an approximation ratio of $10$. This also improves approximation ratios for several variants of the Capacitated $k$-Edge Connected Spanning Subgraph problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00683v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeev Nutov</dc:creator>
    </item>
    <item>
      <title>Adversarially-Robust Inference on Trees via Belief Propagation</title>
      <link>https://arxiv.org/abs/2404.00768</link>
      <description>arXiv:2404.00768v1 Announce Type: new 
Abstract: We introduce and study the problem of posterior inference on tree-structured graphical models in the presence of a malicious adversary who can corrupt some observed nodes. In the well-studied broadcasting on trees model, corresponding to the ferromagnetic Ising model on a $d$-regular tree with zero external field, when a natural signal-to-noise ratio exceeds one (the celebrated Kesten-Stigum threshold), the posterior distribution of the root given the leaves is bounded away from $\mathrm{Ber}(1/2)$, and carries nontrivial information about the sign of the root. This posterior distribution can be computed exactly via dynamic programming, also known as belief propagation.
  We first confirm a folklore belief that a malicious adversary who can corrupt an inverse-polynomial fraction of the leaves of their choosing makes this inference impossible. Our main result is that accurate posterior inference about the root vertex given the leaves is possible when the adversary is constrained to make corruptions at a $\rho$-fraction of randomly-chosen leaf vertices, so long as the signal-to-noise ratio exceeds $O(\log d)$ and $\rho \leq c \varepsilon$ for some universal $c &gt; 0$. Since inference becomes information-theoretically impossible when $\rho \gg \varepsilon$, this amounts to an information-theoretically optimal fraction of corruptions, up to a constant multiplicative factor. Furthermore, we show that the canonical belief propagation algorithm performs this inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00768v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel B. Hopkins, Anqi Li</dc:creator>
    </item>
    <item>
      <title>Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs</title>
      <link>https://arxiv.org/abs/2404.00270</link>
      <description>arXiv:2404.00270v1 Announce Type: cross 
Abstract: The push-relabel algorithm is an efficient algorithm that solves the maximum flow/ minimum cut problems of its affinity to parallelization. As the size of graphs grows exponentially, researchers have used Graphics Processing Units (GPUs) to accelerate the computation of the push-relabel algorithm further. However, prior works need to handle the significant memory consumption to represent a massive residual graph. In addition, the nature of their algorithms has inherently imbalanced workload distribution on GPUs. This paper first identifies the two challenges with the memory and computational models. Based on the analysis of these models, we propose a workload-balanced push-relabel algorithm (WBPR) with two enhanced compressed sparse representations (CSR) and a vertex-centric approach. The enhanced CSR significantly reduces memory consumption, while the vertex-centric approach alleviates the workload imbalance and improves the utilization of the GPU. In the experiment, our approach reduces the memory consumption from O(V^2) to O(V + E). Moreover, we can achieve up to 7.31x and 2.29x runtime speedup compared to the state-of-the-art on real-world graphs in maximum flow and bipartite matching tasks, respectively. Our code will be open-sourced for further research on accelerating the push-relabel algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00270v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chou-Ying Hsieh, Po-Chieh Lin, Sy-Yen Kuo</dc:creator>
    </item>
    <item>
      <title>Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem</title>
      <link>https://arxiv.org/abs/2404.01198</link>
      <description>arXiv:2404.01198v1 Announce Type: cross 
Abstract: We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01198v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avrim Blum, Kavya Ravichandran</dc:creator>
    </item>
    <item>
      <title>Assigning Agents to Increase Network-Based Neighborhood Diversity</title>
      <link>https://arxiv.org/abs/2301.02876</link>
      <description>arXiv:2301.02876v5 Announce Type: replace 
Abstract: Motivated by real-world applications such as the allocation of public housing, we examine the problem of assigning a group of agents to vertices (e.g., spatial locations) of a network so that the diversity level is maximized. Specifically, agents are of two types (characterized by features), and we measure diversity by the number of agents who have at least one neighbor of a different type. This problem is known to be NP-hard, and we focus on developing approximation algorithms with provable performance guarantees. We first present a local-improvement algorithm for general graphs that provides an approximation factor of 1/2. For the special case where the sizes of agent subgroups are similar, we present a randomized approach based on semidefinite programming that yields an approximation factor better than 1/2. Further, we show that the problem can be solved efficiently when the underlying graph is treewidth-bounded and obtain a polynomial time approximation scheme (PTAS) for the problem on planar graphs. Lastly, we conduct experiments to evaluate the per-performance of the proposed algorithms on synthetic and real-world networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02876v5</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zirou Qiu, Andrew Yuan, Chen Chen, Madhav V. Marathe, S. S. Ravi, Daniel J. Rosenkrantz, Richard E. Stearns, Anil Vullikanti</dc:creator>
    </item>
    <item>
      <title>Realizing temporal graphs from fastest travel times</title>
      <link>https://arxiv.org/abs/2302.08860</link>
      <description>arXiv:2302.08860v3 Announce Type: replace 
Abstract: In this paper we initiate the study of the temporal graph realization problem with respect to the fastest path durations among its vertices, while we focus on periodic temporal graphs. Given an $n \times n$ matrix $D$ and a $\Delta \in \mathbb{N}$, the goal is to construct a $\Delta$-periodic temporal graph with $n$ vertices such that the duration of a fastest path from $v_i$ to $v_j$ is equal to $D_{i,j}$, or to decide that such a temporal graph does not exist. The variations of the problem on static graphs have been well studied and understood since the 1960's (e.g. [Erd\H{o}s and Gallai, 1960], [Hakimi and Yau, 1965]).
  As it turns out, the periodic temporal graph realization problem has a very different computational complexity behavior than its static (i.e. non-temporal) counterpart. First, we show that the problem is NP-hard in general, but polynomial-time solvable if the so-called underlying graph is a tree. Building upon those results, we investigate its parameterized computational complexity with respect to structural parameters of the underlying static graph which measure the ``tree-likeness''. We prove a tight classification between such parameters that allow fixed-parameter tractability (FPT) and those which imply W[1]-hardness. We show that our problem is W[1]-hard when parameterized by the feedback vertex number (and therefore also any smaller parameter such as treewidth, degeneracy, and cliquewidth) of the underlying graph, while we show that it is in FPT when parameterized by the feedback edge number (and therefore also any larger parameter such as maximum leaf number) of the underlying graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08860v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina Klobas, George B. Mertzios, Hendrik Molter, Paul G. Spirakis</dc:creator>
    </item>
    <item>
      <title>A Simple 2-Approximation for Maximum-Leaf Spanning Tree</title>
      <link>https://arxiv.org/abs/2303.03125</link>
      <description>arXiv:2303.03125v2 Announce Type: replace 
Abstract: For an $m$-edge connected simple graph $G$, finding a spanning tree of $G$ with the maximum number of leaves is MAXSNP-complete. The problem remains NP-complete even if $G$ is planar and the maximal degree of $G$ is at most four. Lu and Ravi gave the first known polynomial-time approximation algorithms with approximation factors $5$ and $3$. Later, they obtained a $3$-approximation algorithm that runs in near-linear time. The best known result is Solis-Oba, Bonsma, and Lowski's $O(m)$-time $2$-approximation algorithm. We show an alternative simple $O(m)$-time $2$-approximation algorithm whose analysis is simpler. This paper is dedicated to the cherished memory of our dear friend, Professor Takao Nishizeki.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03125v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1142/S0129054123420029</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Foundations of Computer Science, 2023</arxiv:journal_reference>
      <dc:creator>I-Cheng Liao, Hsueh-I Lu</dc:creator>
    </item>
    <item>
      <title>0-1 Knapsack in Nearly Quadratic Time</title>
      <link>https://arxiv.org/abs/2308.04093</link>
      <description>arXiv:2308.04093v2 Announce Type: replace 
Abstract: We study pseudo-polynomial time algorithms for the fundamental \emph{0-1 Knapsack} problem. Recent research interest has focused on its fine-grained complexity with respect to the number of items $n$ and the \emph{maximum item weight} $w_{\max}$. Under $(\min,+)$-convolution hypothesis, 0-1 Knapsack does not have $O((n+w_{\max})^{2-\delta})$ time algorithms (Cygan-Mucha-W\k{e}grzycki-W\l{}odarczyk 2017 and K\"{u}nnemann-Paturi-Schneider 2017). On the upper bound side, currently the fastest algorithm runs in $\tilde O(n + w_{\max}^{12/5})$ time (Chen, Lian, Mao, and Zhang 2023), improving the earlier $O(n + w_{\max}^3)$-time algorithm by Polak, Rohwedder, and W\k{e}grzycki (2021).
  In this paper, we close this gap between the upper bound and the conditional lower bound (up to subpolynomial factors):
  - The 0-1 Knapsack problem has a deterministic algorithm in $O(n + w_{\max}^{2}\log^4w_{\max})$ time.
  Our algorithm combines and extends several recent structural results and algorithmic techniques from the literature on knapsack-type problems:
  - We generalize the "fine-grained proximity" technique of Chen, Lian, Mao, and Zhang (2023) derived from the additive-combinatorial results of Bringmann and Wellnitz (2021) on dense subset sums. This allows us to bound the support size of the useful partial solutions in the dynamic program.
  - To exploit the small support size, our main technical component is a vast extension of the "witness propagation" method, originally designed by Deng, Mao, and Zhong (2023) for speeding up dynamic programming in the easier unbounded knapsack settings. To extend this approach to our 0-1 setting, we use a novel pruning method, as well as the two-level color-coding of Bringmann (2017) and the SMAWK algorithm on tall matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04093v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ce Jin</dc:creator>
    </item>
    <item>
      <title>An Algebraic Approach to the Longest Path Problem</title>
      <link>https://arxiv.org/abs/2312.11469</link>
      <description>arXiv:2312.11469v2 Announce Type: replace 
Abstract: The Longest Path Problem is a question of finding the maximum length between pairs of vertices of a graph. In the general case, the problem is NP-hard. However, there is a small collection of graph classes for which there exists an efficient solution. Current approaches involve either approximation or computational enumeration. For Tree-like classes of graphs, there are approximation and enumeration algorithms which solves the problem efficiently. We propose a new method of approaching the longest path problem with algebraic operations and conditions that exactly identify and or approximate the solution in polynomial time. We next introduce a 'booleanize' mapping on the adjacency matrix of a graph which we prove identifies the solution for trees, uniform block graphs, block graphs, and directed acyclic graphs, with approached conditions. Finally, we display the algorithms to find the solution, in addition to algorithms that generate all the longest paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11469v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al - Khazali</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization</title>
      <link>https://arxiv.org/abs/2403.15623</link>
      <description>arXiv:2403.15623v2 Announce Type: replace 
Abstract: We consider the problem of assigning students to schools, when students have different utilities for schools and schools have capacity. There are additional group fairness considerations over students that can be captured either by concave objectives, or additional constraints on the groups. We present approximation algorithms for this problem via convex program rounding that achieve various trade-offs between utility violation, capacity violation, and running time. We also show that our techniques easily extend to the setting where there are arbitrary covering constraints on the feasible assignment, capturing multi-criteria and ranking optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15623v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santhini K. A., Kamesh Munagala, Meghana Nasre, Govind S. Sankar</dc:creator>
    </item>
    <item>
      <title>Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability</title>
      <link>https://arxiv.org/abs/2403.18059</link>
      <description>arXiv:2403.18059v2 Announce Type: replace 
Abstract: We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years. For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline benchmark in the adversarial arrival model and in the unknown IID stochastic arrival model. In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18059v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajan Udwani</dc:creator>
    </item>
    <item>
      <title>Placing Green Bridges Optimally, with a Multivariate Analysis</title>
      <link>https://arxiv.org/abs/2102.04539</link>
      <description>arXiv:2102.04539v3 Announce Type: replace-cross 
Abstract: We study the problem of placing wildlife crossings, such as green bridges, over human-made obstacles to challenge habitat fragmentation. The main task herein is, given a graph describing habitats or routes of wildlife animals and possibilities of building green bridges, to find a low-cost placement of green bridges that connects the habitats. We develop different problem models for this task and study them from a computational complexity and parameterized algorithmics perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.04539v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00224-023-10157-5</arxiv:DOI>
      <arxiv:journal_reference>Theory Comput. Syst. (2024)</arxiv:journal_reference>
      <dc:creator>Till Fluschnik, Leon Kellerhals</dc:creator>
    </item>
    <item>
      <title>ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE</title>
      <link>https://arxiv.org/abs/2205.11720</link>
      <description>arXiv:2205.11720v3 Announce Type: replace-cross 
Abstract: When visualizing a high-dimensional dataset, dimension reduction techniques are commonly employed which provide a single 2-dimensional view of the data. We describe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously that generalizes the t-Stochastic Neighborhood Embedding approach. By using different viewpoints in ENS-t-SNE's 3D embedding, one can visualize different types of clusters within the same high-dimensional dataset. This enables the viewer to see and keep track of the different types of clusters, which is harder to do when providing multiple 2D embeddings, where corresponding points cannot be easily identified. We illustrate the utility of ENS-t-SNE with real-world applications and provide an extensive quantitative evaluation with datasets of different types and sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.11720v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Miller, Vahan Huroyan, Raymundo Navarrete, Md Iqbal Hossain, Stephen Kobourov</dc:creator>
    </item>
    <item>
      <title>Local dominance unveils clusters in networks</title>
      <link>https://arxiv.org/abs/2209.15497</link>
      <description>arXiv:2209.15497v2 Announce Type: replace-cross 
Abstract: Clusters or communities can provide a coarse-grained description of complex systems at multiple scales, but their detection remains challenging in practice. Community detection methods often define communities as dense subgraphs, or subgraphs with few connections in-between, via concepts such as the cut, conductance, or modularity. Here we consider another perspective built on the notion of local dominance, where low-degree nodes are assigned to the basin of influence of high-degree nodes, and design an efficient algorithm based on local information. Local dominance gives rises to community centers, and uncovers local hierarchies in the network. Community centers have a larger degree than their neighbors and are sufficiently distant from other centers. The strength of our framework is demonstrated on synthesized and empirical networks with ground-truth community labels. The notion of local dominance and the associated asymmetric relations between nodes are not restricted to community detection, and can be utilised in clustering problems, as we illustrate on networks derived from vector data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15497v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dingyi Shi, Fan Shang, Bingsheng Chen, Paul Expert, Linyuan L\"u, H. Eugene Stanley, Renaud Lambiotte, Tim S. Evans, Ruiqi Li</dc:creator>
    </item>
    <item>
      <title>Schr\"odinger as a Quantum Programmer: Estimating Entanglement via Steering</title>
      <link>https://arxiv.org/abs/2303.07911</link>
      <description>arXiv:2303.07911v3 Announce Type: replace-cross 
Abstract: Quantifying entanglement is an important task by which the resourcefulness of a quantum state can be measured. Here we develop a quantum algorithm that tests for and quantifies the separability of a general bipartite state, by making use of the quantum steering effect, the latter originally discovered by Schr\"odinger. Our separability test consists of a distributed quantum computation involving two parties: a computationally limited client, who prepares a purification of the state of interest, and a computationally unbounded server, who tries to steer the reduced systems to a probabilistic ensemble of pure product states. To design a practical algorithm, we replace the role of the server by a combination of parameterized unitary circuits and classical optimization techniques to perform the necessary computation. The result is a variational quantum steering algorithm (VQSA), which is a modified separability test that is better suited for the capabilities of quantum computers available today. We then simulate our VQSA on noisy quantum simulators and find favorable convergence properties on the examples tested. We also develop semidefinite programs, executable on classical computers, that benchmark the results obtained from our VQSA. Our findings here thus provide a meaningful connection between steering, entanglement, quantum algorithms, and quantum computational complexity theory. They also demonstrate the value of a parameterized mid-circuit measurement in a VQSA and represent a first-of-its-kind application for a distributed VQA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07911v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>hep-th</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aby Philip, Soorya Rethinasamy, Vincent Russo, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Improved Stabilizer Estimation via Bell Difference Sampling</title>
      <link>https://arxiv.org/abs/2304.13915</link>
      <description>arXiv:2304.13915v3 Announce Type: replace-cross 
Abstract: We study the complexity of learning quantum states in various models with respect to the stabilizer formalism and obtain the following results:
  - We prove that $\Omega(n)$ $T$-gates are necessary for any Clifford+$T$ circuit to prepare computationally pseudorandom quantum states, an exponential improvement over the previously known bound. This bound is asymptotically tight if linear-time quantum-secure pseudorandom functions exist.
  - Given an $n$-qubit pure quantum state $|\psi\rangle$ that has fidelity at least $\tau$ with some stabilizer state, we give an algorithm that outputs a succinct description of a stabilizer state that witnesses fidelity at least $\tau - \varepsilon$. The algorithm uses $O(n/(\varepsilon^2\tau^4))$ samples and $\exp\left(O(n/\tau^4)\right) / \varepsilon^2$ time. In the regime of $\tau$ constant, this algorithm estimates stabilizer fidelity substantially faster than the na\"ive $\exp(O(n^2))$-time brute-force algorithm over all stabilizer states.
  - In the special case of $\tau &gt; \cos^2(\pi/8)$, we show that a modification of the above algorithm runs in polynomial time.
  - We exhibit a tolerant property testing algorithm for stabilizer states.
  The underlying algorithmic primitive in all of our results is Bell difference sampling. To prove our results, we establish and/or strengthen connections between Bell difference sampling, symplectic Fourier analysis, and graph theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13915v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</dc:creator>
    </item>
    <item>
      <title>Metalearning with Very Few Samples Per Task</title>
      <link>https://arxiv.org/abs/2312.13978</link>
      <description>arXiv:2312.13978v2 Announce Type: replace-cross 
Abstract: Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new tasks from the metadistribution.
  We consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ can be solved by a classifier of the form $f_{P} \circ h$ where $h \in H$ is a map from features to a representation space that is shared across tasks, and $f_{P} \in F$ is a task-specific classifier from the representation space to labels. The main question we ask is how much data do we need to metalearn a good representation? Here, the amount of data is measured in terms of the number of tasks $t$ that we need to see and the number of samples $n$ per task. We focus on the regime where $n$ is extremely small. Our main result shows that, in a distribution-free setting where the feature vectors are in $\mathbb{R}^d$, the representation is a linear map from $\mathbb{R}^d \to \mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\mathbb{R}^k$, we can metalearn a representation with error $\varepsilon$ using $n = k+2$ samples per task, and $d \cdot (1/\varepsilon)^{O(k)}$ tasks. Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with $k+2$ samples per task. Our work also yields a characterization of distribution-free multitask learning and reductions between meta and multitask learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13978v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Aliakbarpour, Konstantina Bairaktari, Gavin Brown, Adam Smith, Nathan Srebro, Jonathan Ullman</dc:creator>
    </item>
    <item>
      <title>Improved Space Bounds for Subset Sum</title>
      <link>https://arxiv.org/abs/2402.13170</link>
      <description>arXiv:2402.13170v2 Announce Type: replace-cross 
Abstract: More than 40 years ago, Schroeppel and Shamir presented an algorithm that solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and space $O^*(2^{0.25n})$. The time upper bound remains unbeaten, but the space upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough paper by Nederlof and W\k{e}grzycki (STOC 2021). Their algorithm is a clever combination of a number of previously known techniques with a new reduction and a new algorithm for the Orthogonal Vectors problem.
  In this paper, we improve the space bound by Nederlof and W\k{e}grzycki to $O^*(2^{0.246n})$ and also simplify their algorithm and its analysis. We achieve this by using an idea, due to Howgrave-Graham and Joux, of using a random prime number to filter the family of subsets. We incorporate it into the algorithm by Schroeppel and Shamir and then use this amalgam inside the representation technique. This allows us to reduce an instance of Subset Sum to a larger number of instances of weighted orthogonal vector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13170v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatiana Belova, Nikolai Chukhin, Alexander S. Kulikov, Ivan Mihajlin</dc:creator>
    </item>
    <item>
      <title>A Provably Accurate Randomized Sampling Algorithm for Logistic Regression</title>
      <link>https://arxiv.org/abs/2402.16326</link>
      <description>arXiv:2402.16326v3 Announce Type: replace-cross 
Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findings, we conduct comprehensive empirical evaluations. Overall, our work sheds light on the potential of using randomized sampling approaches to efficiently approximate the estimated probabilities in logistic regression, offering a practical and computationally efficient solution for large-scale datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16326v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v38i10.29042</arxiv:DOI>
      <dc:creator>Agniva Chowdhury, Pradeep Ramuhalli</dc:creator>
    </item>
  </channel>
</rss>
