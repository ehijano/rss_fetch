<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Survival Data Simulation With the R Package rsurv</title>
      <link>https://arxiv.org/abs/2406.01750</link>
      <description>arXiv:2406.01750v1 Announce Type: new 
Abstract: In this paper we propose a novel R package, called rsurv, developed for general survival data simulation purposes. The package is built under a new approach to simulate survival data that depends heavily on the use of dplyr verbs. The proposed package allows simulations of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package rsurv also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package rsurv lies in the fact that linear predictors are specified using R formulas, facilitating the inclusion of categorical variables, interaction terms and offset variables. The functions implemented in the package rsurv can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivarite survival data, and competing risks survival data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01750v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F\'abio N. Demarqui</dc:creator>
    </item>
    <item>
      <title>Variance-reduced sampling importance resampling</title>
      <link>https://arxiv.org/abs/2406.01864</link>
      <description>arXiv:2406.01864v1 Announce Type: new 
Abstract: The sampling importance resampling method is widely utilized in various fields, such as numerical integration and statistical simulation. In this paper, two modified methods are presented by incorporating two variance reduction techniques commonly used in Monte Carlo simulation, namely antithetic sampling and Latin hypercube sampling, into the process of sampling importance resampling method respectively. Theoretical evidence is provided to demonstrate that the proposed methods significantly reduce estimation errors compared to the original approach. Furthermore, the effectiveness and advantages of the proposed methods are validated through both numerical studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01864v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Xiao, Kang Fu, Kun Li</dc:creator>
    </item>
    <item>
      <title>A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v1 Announce Type: cross 
Abstract: This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Efficient Fourier representations of families of Gaussian processes</title>
      <link>https://arxiv.org/abs/2109.14081</link>
      <description>arXiv:2109.14081v3 Announce Type: replace 
Abstract: We introduce a class of algorithms for constructing Fourier representations of Gaussian processes in $1$ dimension that are valid over ranges of hyperparameter values. The scaling and frequencies of the Fourier basis functions are evaluated numerically via generalized quadratures. The representations introduced allow for $O(m^3)$ inference, independent of $N$, for all hyperparameters in the user-specified range after $O(N + m^2\log{m})$ precomputation where $N$, the number of data points, is usually significantly larger than $m$, the number of basis functions. Inference independent of $N$ for various hyperparameters is facilitated by generalized quadratures, and the $O(N + m^2\log{m})$ precomputation is achieved with the non-uniform FFT. Numerical results are provided for Mat\'ern kernels with $\nu \in [3/2, 7/2]$ and lengthscale $\rho \in [0.1, 0.5]$ and squared-exponential kernels with lengthscale $\rho \in [0.1, 0.5]$. The algorithms of this paper generalize mathematically to higher dimensions, though they suffer from the standard curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.14081v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard</dc:creator>
    </item>
    <item>
      <title>PASOA- PArticle baSed Bayesian Optimal Adaptive design</title>
      <link>https://arxiv.org/abs/2402.07160</link>
      <description>arXiv:2402.07160v2 Announce Type: replace-cross 
Abstract: We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07160v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Iollo, Christophe Heinkel\'e, Pierre Alliez, Florence Forbes</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient MCMC for Massive Geostatistical Data</title>
      <link>https://arxiv.org/abs/2405.04531</link>
      <description>arXiv:2405.04531v2 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04531v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed A. Abba, Brian J. Reich, Reetam Majumder, Brandon Feng</dc:creator>
    </item>
  </channel>
</rss>
