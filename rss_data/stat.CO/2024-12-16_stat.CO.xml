<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Dec 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stochastic Variational Inference for Structured Additive Distributional Regression</title>
      <link>https://arxiv.org/abs/2412.10038</link>
      <description>arXiv:2412.10038v1 Announce Type: new 
Abstract: In structured additive distributional regression, the conditional distribution of the response variables given the covariate information and the vector of model parameters is modelled using a P-parametric probability density function where each parameter is modelled through a linear predictor and a bijective response function that maps the domain of the predictor into the domain of the parameter. We present a method to perform inference in structured additive distributional regression using stochastic variational inference. We propose two strategies for constructing a multivariate Gaussian variational distribution to estimate the posterior distribution of the regression coefficients. The first strategy leverages covariate information and hyperparameters to learn both the location vector and the precision matrix. The second strategy tackles the complexity challenges of the first by initially assuming independence among all smooth terms and then introducing correlations through an additional set of variational parameters. Furthermore, we present two approaches for estimating the smoothing parameters. The first treats them as free parameters and provides point estimates, while the second accounts for uncertainty by applying a variational approximation to the posterior distribution. Our model was benchmarked against state-of-the-art competitors in logistic and gamma regression simulation studies. Finally, we validated our approach by comparing its posterior estimates to those obtained using Markov Chain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics and semiconductor/computer sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10038v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianmarco Callegher, Thomas Kneib, Johannes S\"oding, Paul Wiemann</dc:creator>
    </item>
    <item>
      <title>Temporal Causal Discovery in Dynamic Bayesian Networks Using Federated Learning</title>
      <link>https://arxiv.org/abs/2412.09814</link>
      <description>arXiv:2412.09814v1 Announce Type: cross 
Abstract: Traditionally, learning the structure of a Dynamic Bayesian Network has been centralized, with all data pooled in one location. However, in real-world scenarios, data are often dispersed among multiple parties (e.g., companies, devices) that aim to collaboratively learn a Dynamic Bayesian Network while preserving their data privacy and security. In this study, we introduce a federated learning approach for estimating the structure of a Dynamic Bayesian Network from data distributed horizontally across different parties. We propose a distributed structure learning method that leverages continuous optimization so that only model parameters are exchanged during optimization. Experimental results on synthetic and real datasets reveal that our method outperforms other state-of-the-art techniques, particularly when there are many clients with limited individual sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09814v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhong Chen, Ying Ma, Xubo Yue</dc:creator>
    </item>
    <item>
      <title>$L$-estimation of Claim Severity Models Weighted by Kumaraswamy Density</title>
      <link>https://arxiv.org/abs/2412.09830</link>
      <description>arXiv:2412.09830v1 Announce Type: cross 
Abstract: Statistical modeling of claim severity distributions is essential in insurance and risk management, where achieving a balance between robustness and efficiency in parameter estimation is critical against model contaminations. Two \( L \)-estimators, the method of trimmed moments (MTM) and the method of winsorized moments (MWM), are commonly used in the literature, but they are constrained by rigid weighting schemes that either discard or uniformly down-weight extreme observations, limiting their customized adaptability. This paper proposes a flexible robust \( L \)-estimation framework weighted by Kumaraswamy densities, offering smoothly varying observation-specific weights that preserve valuable information while improving robustness and efficiency. The framework is developed for parametric claim severity models, including Pareto, lognormal, and Fr{\'e}chet distributions, with theoretical justifications on asymptotic normality and variance-covariance structures. Through simulations and application to a U.S. indemnity loss dataset, the proposed method demonstrates superior performance over MTM, MWM, and MLE approaches, particularly in handling outliers and heavy-tailed distributions, making it a flexible and reliable alternative for loss severity modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09830v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Gokarna R. Aryal, Keshav Pokhrel</dc:creator>
    </item>
    <item>
      <title>Deep Gaussian Process Priors for Bayesian Image Reconstruction</title>
      <link>https://arxiv.org/abs/2412.10248</link>
      <description>arXiv:2412.10248v1 Announce Type: cross 
Abstract: In image reconstruction, an accurate quantification of uncertainty is of great importance for informed decision making. Here, the Bayesian approach to inverse problems can be used: the image is represented through a random function that incorporates prior information which is then updated through Bayes' formula. However, finding a prior is difficult, as images often exhibit non-stationary effects and multi-scale behaviour. Thus, usual Gaussian process priors are not suitable. Deep Gaussian processes, on the other hand, encode non-stationary behaviour in a natural way through their hierarchical structure. To apply Bayes' formula, one commonly employs a Markov chain Monte Carlo (MCMC) method. In the case of deep Gaussian processes, sampling is especially challenging in high dimensions: the associated covariance matrices are large, dense, and changing from sample to sample. A popular strategy towards decreasing computational complexity is to view Gaussian processes as the solutions to a fractional stochastic partial differential equation (SPDE). In this work, we investigate efficient computational strategies to solve the fractional SPDEs occurring in deep Gaussian process sampling, as well as MCMC algorithms to sample from the posterior. Namely, we combine rational approximation and a determinant-free sampling approach to achieve sampling via the fractional SPDE. We test our techniques in standard Bayesian image reconstruction problems: upsampling, edge detection, and computed tomography. In these examples, we show that choosing a non-stationary prior such as the deep GP over a stationary GP can improve the reconstruction. Moreover, our approach enables us to compare results for a range of fractional and non-fractional regularity parameter values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10248v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Latz, Aretha L. Teckentrup, Simon Urbainczyk</dc:creator>
    </item>
  </channel>
</rss>
