<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Trek Rule for the Lyapunov Equation</title>
      <link>https://arxiv.org/abs/2407.21223</link>
      <description>arXiv:2407.21223v1 Announce Type: new 
Abstract: In this note we show a new version of the trek rule for the continuous Lyapunov equation. This linear matrix equation characterizes the cross-sectional steady-state covariance matrix of a Gaussian Markov process, and the trek rule links the graphical structure of the drift of the process to the entries of this covariance matrix. In general, the trek rule is a power series expansion of the covariance matrix, while for the special case where the drift is acyclic, it simplifies to a polynomial in the off-diagonal entries of the drift matrix. Using the trek rule we can give relatively explicit formulas for the entries of the covariance matrix for some special cases of the drift matrix. Furthermore, we use the trek rule to derive a new lower bound for the variances in the acyclic case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21223v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Richard Hansen</dc:creator>
    </item>
    <item>
      <title>Quantile processes and their applications in finite populations</title>
      <link>https://arxiv.org/abs/2407.21238</link>
      <description>arXiv:2407.21238v1 Announce Type: new 
Abstract: The weak convergence of the quantile processes, which are constructed based on different estimators of the finite population quantiles, is shown under various well-known sampling designs based on a superpopulation model. The results related to the weak convergence of these quantile processes are applied to find asymptotic distributions of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles. Based on these asymptotic distributions, confidence intervals are constructed for several finite population parameters like the median, the $\alpha$-trimmed means, the interquartile range and the quantile based measure of skewness. Comparisons of various estimators are carried out based on their asymptotic distributions. We show that the use of the auxiliary information in the construction of the estimators sometimes has an adverse effect on the performances of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles under several sampling designs. Further, the performance of each of the above-mentioned estimators sometimes becomes worse under sampling designs, which use the auxiliary information, than their performances under simple random sampling without replacement (SRSWOR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21238v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anurag Dey, Probal Chaudhuri</dc:creator>
    </item>
    <item>
      <title>Volatility and jump activity estimation in a stable Cox-Ingersoll-Ross model</title>
      <link>https://arxiv.org/abs/2407.21411</link>
      <description>arXiv:2407.21411v1 Announce Type: new 
Abstract: We consider the parametric estimation of the volatility and jump activity in a stable Cox-Ingersoll-Ross ($\alpha$-stable CIR) model driven by a standard Brownian Motion and a non-symmetric stable L\'evy process with jump activity $\alpha \in (1,2)$. The main difficulties to obtain rate efficiency in estimating these quantities arise from the superposition of the diffusion component with jumps of infinite variation. Extending the approach proposed in Mies (2020), we address the joint estimation of the volatility, scaling and jump activity parameters from high-frequency observations of the process and prove that the proposed estimators are rate optimal up to a logarithmic factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21411v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elise Bayraktar, Emmanuelle Cl\'ement</dc:creator>
    </item>
    <item>
      <title>Varextropy of doubly truncated random variable</title>
      <link>https://arxiv.org/abs/2407.21423</link>
      <description>arXiv:2407.21423v1 Announce Type: new 
Abstract: Recently, there has been growing attention to study uncertainty measures for doubly truncated random variables. In this paper, the concept of varextropy for doubly truncated random variables is introduced. The changes of this measure under linear transforms are investigated. Also, lower and upper bounds for the interval varextropy under some conditions are obtained and some other properties for this measure are proved. In the following, three estimators for the interval varextropy are proposed. A simulation study has been carried out to investigate the behavior of estimators. At the end, an application for these estimators is proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21423v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raheleh Zamini, Somayeh Ghafouri, Faranak Goodarzi</dc:creator>
    </item>
    <item>
      <title>A Ball Divergence Based Measure For Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2407.21456</link>
      <description>arXiv:2407.21456v1 Announce Type: new 
Abstract: In this paper we introduce a new measure of conditional dependence between two random vectors ${\boldsymbol X}$ and ${\boldsymbol Y}$ given another random vector $\boldsymbol Z$ using the ball divergence. Our measure characterizes conditional independence and does not require any moment assumptions. We propose a consistent estimator of the measure using a kernel averaging technique and derive its asymptotic distribution. Using this statistic we construct two tests for conditional independence, one in the model-${\boldsymbol X}$ framework and the other based on a novel local wild bootstrap algorithm. In the model-${\boldsymbol X}$ framework, which assumes the knowledge of the distribution of ${\boldsymbol X}|{\boldsymbol Z}$, applying the conditional randomization test we obtain a method that controls Type I error in finite samples and is asymptotically consistent, even if the distribution of ${\boldsymbol X}|{\boldsymbol Z}$ is incorrectly specified up to distance preserving transformations. More generally, in situations where ${\boldsymbol X}|{\boldsymbol Z}$ is unknown or hard to estimate, we design a double-bandwidth based local wild bootstrap algorithm that asymptotically controls both Type I error and power. We illustrate the advantage of our method, both in terms of Type I error and power, in a range of simulation settings and also in a real data example. A consequence of our theoretical results is a general framework for studying the asymptotic properties of a 2-sample conditional $V$-statistic, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21456v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Bhaswar B. Bhattacharya, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Bounding adapted Wasserstein metrics</title>
      <link>https://arxiv.org/abs/2407.21492</link>
      <description>arXiv:2407.21492v1 Announce Type: cross 
Abstract: The Wasserstein distance $\mathcal{W}_p$ is an important instance of an optimal transport cost. Its numerous mathematical properties as well as applications to various fields such as mathematical finance and statistics have been well studied in recent years. The adapted Wasserstein distance $\mathcal{A}\mathcal{W}_p$ extends this theory to laws of discrete time stochastic processes in their natural filtrations, making it particularly well suited for analyzing time-dependent stochastic optimization problems.
  While the topological differences between $\mathcal{A}\mathcal{W}_p$ and $\mathcal{W}_p$ are well understood, their differences as metrics remain largely unexplored beyond the trivial bound $\mathcal{W}_p\lesssim \mathcal{A}\mathcal{W}_p$. This paper closes this gap by providing upper bounds of $\mathcal{A}\mathcal{W}_p$ in terms of $\mathcal{W}_p$ through investigation of the smooth adapted Wasserstein distance. Our upper bounds are explicit and are given by a sum of $\mathcal{W}_p$, Eder's modulus of continuity and a term characterizing the tail behavior of measures. As a consequence, upper bounds on $\mathcal{W}_p$ automatically hold for $\mathcal{AW}_p$ under mild regularity assumptions on the measures considered. A particular instance of our findings is the inequality $\mathcal{A}\mathcal{W}_1\le C\sqrt{\mathcal{W}_1}$ on the set of measures that have Lipschitz kernels.
  Our work also reveals how smoothing of measures affects the adapted weak topology. In fact, we find that the topology induced by the smooth adapted Wasserstein distance exhibits a non-trivial interpolation property, which we characterize explicitly: it lies in between the adapted weak topology and the weak topology, and the inclusion is governed by the decay of the smoothing parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21492v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Blanchet, Martin Larsson, Jonghwa Park, Johannes Wiesel</dc:creator>
    </item>
    <item>
      <title>Extended Fiducial Inference: Toward an Automated Process of Statistical Inference</title>
      <link>https://arxiv.org/abs/2407.21622</link>
      <description>arXiv:2407.21622v1 Announce Type: cross 
Abstract: While fiducial inference was widely considered a big blunder by R.A. Fisher, the goal he initially set --`inferring the uncertainty of model parameters on the basis of observations' -- has been continually pursued by many statisticians. To this end, we develop a new statistical inference method called extended Fiducial inference (EFI). The new method achieves the goal of fiducial inference by leveraging advanced statistical computing techniques while remaining scalable for big data. EFI involves jointly imputing random errors realized in observations using stochastic gradient Markov chain Monte Carlo and estimating the inverse function using a sparse deep neural network (DNN). The consistency of the sparse DNN estimator ensures that the uncertainty embedded in observations is properly propagated to model parameters through the estimated inverse function, thereby validating downstream statistical inference. Compared to frequentist and Bayesian methods, EFI offers significant advantages in parameter estimation and hypothesis testing. Specifically, EFI provides higher fidelity in parameter estimation, especially when outliers are present in the observations; and eliminates the need for theoretical reference distributions in hypothesis testing, thereby automating the statistical inference process. EFI also provides an innovative framework for semi-supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21622v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faming Liang, Sehwan Kim, Yan Sun</dc:creator>
    </item>
    <item>
      <title>Shape-restricted transfer learning analysis for generalized linear regression model</title>
      <link>https://arxiv.org/abs/2407.21682</link>
      <description>arXiv:2407.21682v1 Announce Type: cross 
Abstract: Transfer learning has emerged as a highly sought-after and actively pursued research area within the statistical community. The core concept of transfer learning involves leveraging insights and information from auxiliary datasets to enhance the analysis of the primary dataset of interest. In this paper, our focus is on datasets originating from distinct yet interconnected distributions. We assume that the training data conforms to a standard generalized linear model, while the testing data exhibit a connection to the training data based on a prior probability shift assumption. Ultimately, we discover that the two-sample conditional means are interrelated through an unknown, nondecreasing function. We integrate the power of generalized estimating equations with the shape-restricted score function, creating a robust framework for improved inference regarding the underlying parameters. We theoretically establish the asymptotic properties of our estimator and demonstrate, through simulation studies, that our method yields more accurate parameter estimates compared to those based solely on the testing or training data. Finally, we apply our method to a real-world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21682v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Li, Tao Yu, Chixiang Chen, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Learning extremal graphical structures in high dimensions</title>
      <link>https://arxiv.org/abs/2111.00840</link>
      <description>arXiv:2111.00840v4 Announce Type: replace 
Abstract: Extremal graphical models encode the conditional independence structure of multivariate extremes. Key statistics for learning extremal graphical structures are empirical extremal variograms, for which we prove non-asymptotic concentration bounds that hold under general domain of attraction conditions. For the popular class of H\"usler--Reiss models, we propose a majority voting algorithm for learning the underlying graph from data through $L^1$ regularized optimization. Our concentration bounds are used to derive explicit conditions that ensure the consistent recovery of any connected graph. The methodology is illustrated through a simulation study as well as the analysis of river discharge and currency exchange data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.00840v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Micha\"el Lalancette, Stanislav Volgushev</dc:creator>
    </item>
    <item>
      <title>Likelihood ratio tests under model misspecification in high dimensions</title>
      <link>https://arxiv.org/abs/2203.05423</link>
      <description>arXiv:2203.05423v3 Announce Type: replace 
Abstract: We investigate the likelihood ratio test for a large block-diagonal covariance matrix with an increasing number of blocks under the null hypothesis. While so far the likelihood ratio statistic has only been studied for normal populations, we establish that its asymptotic behavior is invariant under a much larger class of distributions. This implies robustness against model misspecification, which is common in high-dimensional regimes. Demonstrating the flexibility of our approach, we additionally establish asymptotic normality of the log-likelihood ratio test statistic for the equality of many large sample covariance matrices under model uncertainty. For this statistic, a subtle adjustment to the centering term is needed compared to normal case. A simulation study and an analysis of a data set from psychology emphasize the usefulness of our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05423v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Multivariate Analysis 193 (2023): 105122</arxiv:journal_reference>
      <dc:creator>Nina D\"ornemann</dc:creator>
    </item>
    <item>
      <title>The Dyson Equalizer: Adaptive Noise Stabilization for Low-Rank Signal Detection and Recovery</title>
      <link>https://arxiv.org/abs/2306.11263</link>
      <description>arXiv:2306.11263v2 Announce Type: replace 
Abstract: Detecting and recovering a low-rank signal in a noisy data matrix is a fundamental task in data analysis. Typically, this task is addressed by inspecting and manipulating the spectrum of the observed data, e.g., thresholding the singular values of the data matrix at a certain critical level. This approach is well-established in the case of homoskedastic noise, where the noise variance is identical across the entries. However, in numerous applications, the noise can be heteroskedastic, where the noise characteristics may vary considerably across the rows and columns of the data. In this scenario, the spectral behavior of the noise can differ significantly from the homoskedastic case, posing various challenges for signal detection and recovery. To address these challenges, we develop an adaptive normalization procedure that equalizes the average noise variance across the rows and columns of a given data matrix. Our proposed procedure is data-driven and fully automatic, supporting a broad range of noise distributions, variance patterns, and signal structures. Our approach relies on recent results in random matrix theory, which describe the resolvent of the noise via the so-called Dyson equation. By leveraging this relation, we can accurately infer the noise level in each row and each column directly from the resolvent of the data. We establish that in many cases, our normalization enforces the standard spectral behavior of homoskedastic noise -- the Marchenko-Pastur (MP) law, allowing for simple and reliable detection of signal components. Furthermore, we demonstrate that our approach can substantially improve signal recovery in heteroskedastic settings by manipulating the spectrum after normalization. Lastly, we apply our method to single-cell RNA sequencing and spatial transcriptomics data, showcasing accurate fits to the MP law after normalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11263v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boris Landa, Yuval Kluger</dc:creator>
    </item>
    <item>
      <title>The saddlepoint approximation for averages of conditionally independent random variables</title>
      <link>https://arxiv.org/abs/2407.08915</link>
      <description>arXiv:2407.08915v2 Announce Type: replace 
Abstract: Motivated by the application of saddlepoint approximations to resampling-based statistical tests, we prove that the Lugannani-Rice formula has vanishing relative error when applied to approximate conditional tail probabilities of averages of conditionally independent random variables. In a departure from existing work, this result is valid under only sub-exponential assumptions on the summands, and does not require any assumptions on their smoothness or lattice structure. The derived saddlepoint approximation result can be directly applied to resampling-based hypothesis tests, including bootstrap, sign-flipping and conditional randomization tests. We exemplify this by providing the first rigorous justification of a saddlepoint approximation for the sign-flipping test of symmetry about the origin, initially proposed in 1955. On the way to our main result, we establish a conditional Berry-Esseen inequality for sums of conditionally independent random variables, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08915v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Interaction Screening and Pseudolikelihood Approaches for Tensor Learning in Ising Models</title>
      <link>https://arxiv.org/abs/2310.13232</link>
      <description>arXiv:2310.13232v2 Announce Type: replace-cross 
Abstract: In this paper, we study two well known methods of Ising structure learning, namely the pseudolikelihood approach and the interaction screening approach, in the context of tensor recovery in $k$-spin Ising models. We show that both these approaches, with proper regularization, retrieve the underlying hypernetwork structure using a sample size logarithmic in the number of network nodes, and exponential in the maximum interaction strength and maximum node-degree. We also track down the exact dependence of the rate of tensor recovery on the interaction order $k$, that is allowed to grow with the number of samples and nodes, for both the approaches. We then provide a comparative discussion of the performance of the two approaches based on simulation studies, which also demonstrates the exponential dependence of the tensor recovery rate on the maximum coupling strength. Our tensor recovery methods are then applied on gene data taken from the Curated Microarray Database (CuMiDa), where we focus on understanding the important genes related to hepatocellular carcinoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13232v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Liu, Somabha Mukherjee</dc:creator>
    </item>
    <item>
      <title>Statistical divergences in high-dimensional hypothesis testing and a modern technique for estimating them</title>
      <link>https://arxiv.org/abs/2405.06397</link>
      <description>arXiv:2405.06397v2 Announce Type: replace-cross 
Abstract: Hypothesis testing in high dimensional data is a notoriously difficult problem without direct access to competing models' likelihood functions. This paper argues that statistical divergences can be used to quantify the difference between the population distributions of observed data and competing models, justifying their use as the basis of a hypothesis test. We go on to point out how modern techniques for functional optimization let us estimate many divergences, without the need for population likelihood functions, using samples from two distributions alone. We use a physics-based example to show how the proposed two-sample test can be implemented in practice, and discuss the necessary steps required to mature the ideas presented into an experimental framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06397v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy J. H. Wilkinson, Christopher G. Lester</dc:creator>
    </item>
  </channel>
</rss>
