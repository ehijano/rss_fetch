<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Posterior asymptotics of high-dimensional spiked covariance model with inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2412.10753</link>
      <description>arXiv:2412.10753v1 Announce Type: new 
Abstract: We consider Bayesian inference on the spiked eigenstructures of high-dimensional covariance matrices; specifically, we focus on estimating the eigenvalues and corresponding eigenvectors of high-dimensional covariance matrices in which a few eigenvalues are significantly larger than the rest. We impose an inverse-Wishart prior distribution on the unknown covariance matrix and derive the posterior distributions of the eigenvalues and eigenvectors by transforming the posterior distribution of the covariance matrix. We prove that the posterior distribution of the spiked eigenvalues and corresponding eigenvectors converges to the true parameters under the spiked high-dimensional covariance assumption, and also that the posterior distribution of the spiked eigenvector attains the minimax optimality under the single spiked covariance model. Simulation studies and real data analysis demonstrate that our proposed method outperforms all existing methods in quantifying uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10753v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangmin Lee, Sewon Park, Seongmin Kim, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>The entropic optimal (self-)transport problem: Limit distributions for decreasing regularization with application to score function estimation</title>
      <link>https://arxiv.org/abs/2412.12007</link>
      <description>arXiv:2412.12007v1 Announce Type: new 
Abstract: Westudythestatisticalpropertiesoftheentropicoptimal(self) transport problem for smooth probability measures. We provide an accurate description of the limit distribution for entropic (self-)potentials and plans for shrinking regularization parameter, which strongly contrasts prior work where the regularization parameter is held fix. Additionally, we show that a rescaling of the barycentric projection of the empirical entropic optimal self-transport plans converges to the score function, a central object for diffusion models, and characterize the asymptotic fluctuations both pointwise and in L2. Finally, we describe under what conditions the methods used enable to derive (pointwise) limiting distribution results for the empirical entropic optimal transport potentials in the case of two different measures and appropriately chosen shrinking regularization parameter. This endeavour requires better understanding the composition of Sinkhorn operators, a result of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12007v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilles Mordant</dc:creator>
    </item>
    <item>
      <title>Optimality of the Right-Invariant Prior</title>
      <link>https://arxiv.org/abs/2412.12054</link>
      <description>arXiv:2412.12054v1 Announce Type: new 
Abstract: In this paper, we discuss optimal next-sample prediction for families of probability distributions with a locally compact topological group structure. The right-invariant prior was previously shown to yield a posterior predictive distribution minimizing the worst-case Kullback-Leibler risk among all predictive procedures. However, the assumptions for the proof are so strong that they rarely hold in practice and it is unclear when the density functions used in the proof exist. Therefore, we provide a measure-theoretic proof using a more appropriate set of assumptions. As an application, we show a strong optimality result for next-sample prediction for multivariate normal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12054v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannis Bolik, Thomas Hofmann</dc:creator>
    </item>
    <item>
      <title>Model checking for high dimensional generalized linear models based on random projections</title>
      <link>https://arxiv.org/abs/2412.10721</link>
      <description>arXiv:2412.10721v1 Announce Type: cross 
Abstract: Most existing tests in the literature for model checking do not work in high dimension settings due to challenges arising from the "curse of dimensionality", or dependencies on the normality of parameter estimators. To address these challenges, we proposed a new goodness of fit test based on random projections for generalized linear models, when the dimension of covariates may substantially exceed the sample size. The tests only require the convergence rate of parameter estimators to derive the limiting distribution. The growing rate of the dimension is allowed to be of exponential order in relation to the sample size. As random projection converts covariates to one-dimensional space, our tests can detect the local alternative departing from the null at the rate of $n^{-1/2}h^{-1/4}$ where $h$ is the bandwidth, and $n$ is the sample size. This sensitive rate is not related to the dimension of covariates, and thus the "curse of dimensionality" for our tests would be largely alleviated. An interesting and unexpected result is that for randomly chosen projections, the resulting test statistics can be asymptotic independent. We then proposed combination methods to enhance the power performance of the tests. Detailed simulation studies and a real data analysis are conducted to illustrate the effectiveness of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10721v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Well-Posedness and Stability of the Stochastic OGTT Model</title>
      <link>https://arxiv.org/abs/2412.10987</link>
      <description>arXiv:2412.10987v1 Announce Type: cross 
Abstract: Oral Glucose Tolerance Test (OGTT) is one of many way to produce data in the study of the diabetes dynamic. In a recent paper [1.]:\textit{ Estimating insulin sensitivity and $ \beta $-cell function from the oral glucose tolerance test: validation of a new insulin sensitivity and secretion (ISS) model, \textit{J. American Physiological Society },(2024)},Ha J., Chung S.T., and al. proposed a comprehensive OGTT model under the form of a dynamic system. But their model was fully deterministic. Yet, our natural environment interacts with noise; thus taking into account that inherent perturbation could potentially improve the model, which in turn could lead to a better estimation of the parameters of the system. The current paper endeavors to explore the OGTT model proposed by Ha and al. but this time with the addition of white noise perturbations to the system.\
  The work is organized as follow: we first establish the existence and uniqueness of a global positive solution to the proposed stochastic model. Then follows the study the statistical stability of the model by examining the existence of an invariant measure to the perturbed system. Once done with the well-posedness and the stability of the stochastic model, we then examine a Maximum Likelihood Estimation (MLE) scheme to estimate the parameters involved in the model. The paper ends with a brief discussion on potential future developments follows by an appendix whose goal is to make the paper as self contained as possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10987v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Bekima</dc:creator>
    </item>
    <item>
      <title>Learning Massive-scale Partial Correlation Networks in Clinical Multi-omics Studies with HP-ACCORD</title>
      <link>https://arxiv.org/abs/2412.11554</link>
      <description>arXiv:2412.11554v1 Announce Type: cross 
Abstract: Graphical model estimation from modern multi-omics data requires a balance between statistical estimation performance and computational scalability. We introduce a novel pseudolikelihood-based graphical model framework that reparameterizes the target precision matrix while preserving sparsity pattern and estimates it by minimizing an $\ell_1$-penalized empirical risk based on a new loss function. The proposed estimator maintains estimation and selection consistency in various metrics under high-dimensional assumptions. The associated optimization problem allows for a provably fast computation algorithm using a novel operator-splitting approach and communication-avoiding distributed matrix multiplication. A high-performance computing implementation of our framework was tested in simulated data with up to one million variables demonstrating complex dependency structures akin to biological networks. Leveraging this scalability, we estimated partial correlation network from a dual-omic liver cancer data set. The co-expression network estimated from the ultrahigh-dimensional data showed superior specificity in prioritizing key transcription factors and co-activators by excluding the impact of epigenomic regulation, demonstrating the value of computational scalability in multi-omic data analysis. %derived from the gene expression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11554v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungdong Lee, Joshua Bang, Youngrae Kim, Hyungwon Choi, Sang-Yun Oh, Joong-Ho Won</dc:creator>
    </item>
    <item>
      <title>Dual Unscented Kalman Filter Architecture for Sensor Fusion in Water Networks Leak Localization</title>
      <link>https://arxiv.org/abs/2412.11687</link>
      <description>arXiv:2412.11687v1 Announce Type: cross 
Abstract: Leakage in water systems results in significant daily water losses, degrading service quality, increasing costs, and aggravating environmental problems. Most leak localization methods rely solely on pressure data, missing valuable information from other sensor types. This article proposes a hydraulic state estimation methodology based on a dual Unscented Kalman Filter (UKF) approach, which enhances the estimation of both nodal hydraulic heads, critical in localization tasks, and pipe flows, useful for operational purposes. The approach enables the fusion of different sensor types, such as pressure, flow and demand meters. The strategy is evaluated in well-known open source case studies, namely Modena and L-TOWN, showing improvements over other state-of-the-art estimation approaches in terms of interpolation accuracy, as well as more precise leak localization performance in L-TOWN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11687v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Romero-Ben, Paul Irofti, Florin Stoican, Vicen\c{c} Puig</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v1 Announce Type: cross 
Abstract: Tree-based models for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data and thus lead to inconsistent inference. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. This strategy parametrizes the tree-based sampling model according to the allocation of probability mass based on the observed data, and yet under appropriate specification, the resulting inference remains valid. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and in particular to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from using the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>Causal Invariance Learning via Efficient Optimization of a Nonconvex Objective</title>
      <link>https://arxiv.org/abs/2412.11850</link>
      <description>arXiv:2412.11850v1 Announce Type: cross 
Abstract: Data from multiple environments offer valuable opportunities to uncover causal relationships among variables. Leveraging the assumption that the causal outcome model remains invariant across heterogeneous environments, state-of-the-art methods attempt to identify causal outcome models by learning invariant prediction models and rely on exhaustive searches over all (exponentially many) covariate subsets. These approaches present two major challenges: 1) determining the conditions under which the invariant prediction model aligns with the causal outcome model, and 2) devising computationally efficient causal discovery algorithms that scale polynomially, instead of exponentially, with the number of covariates. To address both challenges, we focus on the additive intervention regime and propose nearly necessary and sufficient conditions for ensuring that the invariant prediction model matches the causal outcome model. Exploiting the essentially necessary identifiability conditions, we introduce Negative Weight Distributionally Robust Optimization NegDRO a nonconvex continuous minimax optimization whose global optimizer recovers the causal outcome model. Unlike standard group DRO problems that maximize over the simplex, NegDRO allows negative weights on environment losses, which break the convexity. Despite its nonconvexity, we demonstrate that a standard gradient method converges to the causal outcome model, and we establish the convergence rate with respect to the sample size and the number of iterations. Our algorithm avoids exhaustive search, making it scalable especially when the number of covariates is large. The numerical results further validate the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11850v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yifan Hu, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>A clarification on the links between potential outcomes and do-interventions</title>
      <link>https://arxiv.org/abs/2309.05997</link>
      <description>arXiv:2309.05997v4 Announce Type: replace 
Abstract: Most of the scientific literature on causal modeling considers the structural framework of Pearl and the potential-outcome framework of Rubin to be formally equivalent, and therefore interchangeably uses do-interventions and the potential-outcome subscript notation to write counterfactual outcomes. In this paper, we agnostically superimpose the two causal models to specify under which mathematical conditions structural counterfactual outcomes and potential outcomes need to, do not need to, can, or cannot be equal (almost surely or law). Our comparison reminds that a structural causal model and a Rubin causal model compatible with the same observations do not have to coincide, and highlights real-world problems where they even cannot correspond. Then, we examine common claims and practices from the causal-inference literature in the light of these results. In doing so, we aim at clarifying the relationship between the two causal frameworks, and the interpretation of their respective counterfactuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05997v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas de Lara (UT3, IECL)</dc:creator>
    </item>
    <item>
      <title>Density estimation using the perceptron</title>
      <link>https://arxiv.org/abs/2312.17701</link>
      <description>arXiv:2312.17701v3 Announce Type: replace 
Abstract: We propose a new density estimation algorithm. Given $n$ i.i.d. observations from a distribution belonging to a class of densities on $\mathbb{R}^d$, our estimator outputs any density in the class whose ``perceptron discrepancy'' with the empirical distribution is at most $O(\sqrt{d/n})$. The perceptron discrepancy is defined as the largest difference in mass two distribution place on any halfspace. It is shown that this estimator achieves the expected total variation distance to the truth that is almost minimax optimal over the class of densities with bounded Sobolev norm and Gaussian mixtures. This suggests that the regularity of the prior distribution could be an explanation for the efficiency of the ubiquitous step in machine learning that replaces optimization over large function spaces with simpler parametric classes (such as discriminators of GANs). We also show that replacing the perceptron discrepancy with the generalized energy distance of Szekely and Rizzo (2013) further improves total variation loss. The generalized energy distance between empirical distributions is easily computable and differentiable, which makes it especially useful for fitting generative models. To the best of our knowledge, it is the first ``simple'' distance with such properties with minimax statistical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17701v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik R\'obert Gerber, Tianze Jiang, Yury Polyanskiy, Rui Sun</dc:creator>
    </item>
    <item>
      <title>On the asymptotic properties of product-PCA under the high-dimensional setting</title>
      <link>https://arxiv.org/abs/2407.19725</link>
      <description>arXiv:2407.19725v2 Announce Type: replace 
Abstract: Principal component analysis (PCA) is a widely used dimension reduction method, but its performance is known to be non-robust to outliers. Recently, product-PCA (PPCA) has been shown to possess the efficiency-loss free ordering-robustness property: (i) in the absence of outliers, PPCA and PCA share the same asymptotic distributions; (ii), in the presence of outliers, PPCA is more ordering-robust than PCA in estimating the leading eigenspace. PPCA is thus different from the conventional robust PCA methods, and may deserve further investigations. In this article, we study the high-dimensional statistical properties of the PPCA eigenvalues via the techniques of random matrix theory. In particular, we derive the critical value for being distant spiked eigenvalues, the limiting values of the sample spiked eigenvalues, and the limiting spectral distribution of PPCA. Similar to the case of PCA, the explicit forms of the asymptotic properties of PPCA become available under the special case of the simple spiked model. These results enable us to more clearly understand the superiorities of PPCA in comparison with PCA. Numerical studies are conducted to verify our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19725v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Hung, Chi-Chun Yeh, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>Characterizing extremal dependence on a hyperplane</title>
      <link>https://arxiv.org/abs/2411.00573</link>
      <description>arXiv:2411.00573v2 Announce Type: replace 
Abstract: Quantifying the risks of extreme scenarios requires understanding the tail behaviours of variables of interest. While the tails of individual variables can be characterized parametrically, the extremal dependence across variables can be complex and its modeling remains one of the core problems in extreme value analysis. Notably, existing measures for extremal dependence, such as angular components and spectral random vectors, reside on nonlinear supports, such that statistical models and methods designed for linear vector spaces cannot be readily applied. In this paper, we show that the extremal dependence of $d$ asymptotically dependent variables can be characterized by a class of random vectors residing on a $(d-1)$-dimensional hyperplane. This translates the analyses of multivariate extremes to that on a linear vector space, opening up the potentials for the application of existing statistical techniques, particularly in statistical learning and dimension reduction. As an example, we show that a lower-dimensional approximation of multivariate extremes can be achieved through principal component analysis on the hyperplane. Additionally, through this framework, the widely used H\"usler-Reiss family for modelling extremes is characterized by the Gaussian family residing on the hyperplane, thereby justifying its status as the Gaussian counterpart for extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00573v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phyllis Wan</dc:creator>
    </item>
    <item>
      <title>E-backtesting</title>
      <link>https://arxiv.org/abs/2209.00991</link>
      <description>arXiv:2209.00991v5 Announce Type: replace-cross 
Abstract: In the recent Basel Accords, the Expected Shortfall (ES) replaces the Value-at-Risk (VaR) as the standard risk measure for market risk in the banking sector, making it the most important risk measure in financial regulation. One of the most challenging tasks in risk modeling practice is to backtest ES forecasts provided by financial institutions. To design a model-free backtesting procedure for ES, we make use of the recently developed techniques of e-values and e-processes. Backtest e-statistics are introduced to formulate e-processes for risk measure forecasts, and unique forms of backtest e-statistics for VaR and ES are characterized using recent results on identification functions. For a given backtest e-statistic, a few criteria for optimally constructing the e-processes are studied. The proposed method can be naturally applied to many other risk measures and statistical quantities. We conduct extensive simulation studies and data analysis to illustrate the advantages of the model-free backtesting method, and compare it with the ones in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00991v5</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiuqi Wang, Ruodu Wang, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Extremes in High Dimensions: Methods and Scalable Algorithms</title>
      <link>https://arxiv.org/abs/2303.04258</link>
      <description>arXiv:2303.04258v2 Announce Type: replace-cross 
Abstract: Extreme value theory for univariate and low-dimensional observations has been explored in considerable detail, but the field is still in an early stage regarding high-dimensional settings. This paper focuses on H\"usler-Reiss models, a popular class of models for multivariate extremes similar to multivariate Gaussian distributions, and their domain of attraction. We develop estimators for the model parameters based on score matching, and we equip these estimators with theories and exceptionally scalable algorithms. Simulations and applications to weather extremes demonstrate the fact that the estimators can estimate a large number of parameters reliably and fast; for example, we show that H\"usler-Reiss models with thousands of parameters can be fitted within a couple of minutes on a standard laptop. More generally speaking, our work relates extreme value theory to modern concepts of high-dimensional statistics and convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04258v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Lederer, Marco Oesting</dc:creator>
    </item>
    <item>
      <title>An Identification and Dimensionality Robust Test for Instrumental Variables Models</title>
      <link>https://arxiv.org/abs/2311.14892</link>
      <description>arXiv:2311.14892v2 Announce Type: replace-cross 
Abstract: Using modifications of Lindeberg's interpolation technique, I propose a new identification-robust test for the structural parameter in a heteroskedastic instrumental variables model. While my analysis allows the number of instruments to be much larger than the sample size, it does not require many instruments, making my test applicable in settings that have not been well studied. Instead, the proposed test statistic has a limiting chi-squared distribution so long as an auxiliary parameter can be consistently estimated. This is possible using machine learning methods even when the number of instruments is much larger than the sample size. To improve power, a simple combination with the sup-score statistic of Belloni et al. (2012) is proposed. I point out that first-stage F-statistics calculated on LASSO selected variables may be misleading indicators of identification strength and demonstrate favorable performance of my proposed methods in both empirical data and simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14892v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manu Navjeevan</dc:creator>
    </item>
    <item>
      <title>Policy Learning for Optimal Dynamic Treatment Regimes with Observational Data</title>
      <link>https://arxiv.org/abs/2404.00221</link>
      <description>arXiv:2404.00221v5 Announce Type: replace-cross 
Abstract: Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00221v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Expansion of net correlations in terms of partial correlations</title>
      <link>https://arxiv.org/abs/2404.01734</link>
      <description>arXiv:2404.01734v2 Announce Type: replace-cross 
Abstract: The marginal correlation between two variables is a measure of their linear dependence. The two original variables need not interact directly, because marginal correlation may arise from the mediation of other variables in the system. The underlying network of direct interactions can be captured by a weighted graphical model. The connection between two variables can be weighted by their partial correlation, defined as the residual correlation left after accounting for the linear effects of mediating variables. While matrix inversion can be used to obtain marginal correlations from partial correlations, in large systems this approach does not reveal how the former emerge from the latter. Here we present an expansion of marginal correlations in terms of partial correlations, which shows that the effect of mediating variables can be quantified by the weight of the paths in the graphical model that connect the original pair of variables. The expansion is proved to converge for arbitrary probability distributions. The graphical interpretation reveals a close connection between the topology of the graph and the marginal correlations. Moreover, the expansion shows how marginal correlations change when some variables are severed from the graph, and how partial correlations change when some variables are marginalised out from the description. It also establishes the minimum number of latent variables required to replicate the exact effect of a collection of variables that are marginalised out, ensuring that the partial and marginal correlations of the remaining variables remain unchanged. Notably, the number of latent variables may be significantly smaller than the number of variables that they effectively replicate. Finally, for Gaussian variables, marginal correlations are shown to be related to the efficacy with which information propagates along the paths in the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01734v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bautista Arenaza, Sebasti\'an Risau-Gusman, In\'es Samengo</dc:creator>
    </item>
    <item>
      <title>State-Space Systems as Dynamic Generative Models</title>
      <link>https://arxiv.org/abs/2404.08717</link>
      <description>arXiv:2404.08717v2 Announce Type: replace-cross 
Abstract: A probabilistic framework to study the dependence structure induced by deterministic discrete-time state-space systems between input and output processes is introduced. General sufficient conditions are formulated under which output processes exist and are unique once an input process has been fixed, a property that in the deterministic state-space literature is known as the echo state property. When those conditions are satisfied, the given state-space system becomes a generative model for probabilistic dependences between two sequence spaces. Moreover, those conditions guarantee that the output depends continuously on the input when using the Wasserstein metric. The output processes whose existence is proved are shown to be causal in a specific sense and to generalize those studied in purely deterministic situations. The results in this paper constitute a significant stochastic generalization of sufficient conditions for the deterministic echo state property to hold, in the sense that the stochastic echo state property can be satisfied under contractivity conditions that are strictly weaker than those in deterministic situations. This means that state-space systems can induce a purely probabilistic dependence structure between input and output sequence spaces even when there is no functional relation between those two spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08717v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan-Pablo Ortega, Florian Rossmannek</dc:creator>
    </item>
    <item>
      <title>Investigating Self-Supervised Image Denoising with Denaturation</title>
      <link>https://arxiv.org/abs/2405.01124</link>
      <description>arXiv:2405.01124v5 Announce Type: replace-cross 
Abstract: Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01124v5</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Waida, Kimihiro Yamazaki, Atsushi Tokuhisa, Mutsuyo Wada, Yuichiro Wada</dc:creator>
    </item>
    <item>
      <title>Analysis of Corrected Graph Convolutions</title>
      <link>https://arxiv.org/abs/2405.13987</link>
      <description>arXiv:2405.13987v2 Announce Type: replace-cross 
Abstract: Machine learning for node classification on graphs is a prominent area driven by applications such as recommendation systems. State-of-the-art models often use multiple graph convolutions on the data, as empirical evidence suggests they can enhance performance. However, it has been shown empirically and theoretically, that too many graph convolutions can degrade performance significantly, a phenomenon known as oversmoothing. In this paper, we provide a rigorous theoretical analysis, based on the two-class contextual stochastic block model (CSBM), of the performance of vanilla graph convolution from which we remove the principal eigenvector to avoid oversmoothing. We perform a spectral analysis for $k$ rounds of corrected graph convolutions, and we provide results for partial and exact classification. For partial classification, we show that each round of convolution can reduce the misclassification error exponentially up to a saturation level, after which performance does not worsen. We also extend this analysis to the multi-class setting with features distributed according to a Gaussian mixture model. For exact classification, we show that the separability threshold can be improved exponentially up to $O({\log{n}}/{\log\log{n}})$ corrected convolutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13987v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Robert Wang, Aseem Baranwal, Kimon Fountoulakis</dc:creator>
    </item>
    <item>
      <title>Prior Knowledge Accelerate Variance Computing</title>
      <link>https://arxiv.org/abs/2410.21922</link>
      <description>arXiv:2410.21922v5 Announce Type: replace-cross 
Abstract: Variance is a basic metric to evaluate the degree of data dispersion, and it is also frequently used in the realm of statistics. However, due to the computing variance and the large dataset being time-consuming, there is an urge to accelerate this computing process. The paper suggests a new method to reduce the time of this computation, it assumes a scenario in which we already know the variance of the original dataset, and the whole variance of this merge dataset could be expressed in the form of addition between the original variance and a remainder term. When we want to calculate the total variance after this adds up, the method only needs to calculate the remainder to get the result instead of recalculating the total variance again, which we named this type of method PKA(Prior Knowledge Acceleration). The paper mathematically proves the effectiveness of PKA in variance calculation, and the conditions for this method to accelerate properly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21922v5</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Li</dc:creator>
    </item>
  </channel>
</rss>
