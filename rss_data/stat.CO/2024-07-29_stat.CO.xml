<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Approximate learning of parsimonious Bayesian context trees</title>
      <link>https://arxiv.org/abs/2407.19236</link>
      <description>arXiv:2407.19236v1 Announce Type: new 
Abstract: Models for categorical sequences typically assume exchangeable or first-order dependent sequence elements. These are common assumptions, for example, in models of computer malware traces and protein sequences. Although such simplifying assumptions lead to computational tractability, these models fail to capture long-range, complex dependence structures that may be harnessed for greater predictive power. To this end, a Bayesian modelling framework is proposed to parsimoniously capture rich dependence structures in categorical sequences, with memory efficiency suitable for real-time processing of data streams. Parsimonious Bayesian context trees are introduced as a form of variable-order Markov model with conjugate prior distributions. The novel framework requires fewer parameters than fixed-order Markov models by dropping redundant dependencies and clustering sequential contexts. Approximate inference on the context tree structure is performed via a computationally efficient model-based agglomerative clustering procedure. The proposed framework is tested on synthetic and real-world data examples, and it outperforms existing sequence models when fitted to real protein sequences and honeypot computer terminal sessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19236v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniyar Ghani, Nicholas A. Heard, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>CASTRO -- Efficient constrained sampling method for material and chemical experimental design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v2 Announce Type: replace 
Abstract: The exploration of multicomponent material composition space requires significant time and financial investments, necessitating efficient use of resources for statistically relevant compositions. This article introduces a novel methodology, implemented in the open-source CASTRO (ConstrAined Sequential laTin hypeRcube sampling methOd) software package, to overcome equality-mixture constraints and ensure comprehensive design space coverage. Our approach leverages Latin hypercube sampling (LHS) and LHS with multidimensional uniformity (LHSMDU) using a divide-and-conquer strategy to manage high-dimensional problems effectively. By incorporating previous experimental knowledge within a limited budget, our method strategically recommends a feasible number of experiments to explore the design space. We validate our methodology with two examples: a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional mixture constraints, yielding specific component distributions. Our constrained sequential LHS or LHSMDU approach enables thorough design space exploration, proving robustness for experimental design. This research not only advances material science but also offers promising solutions for efficiency challenges in pharmaceuticals and chemicals. CASTRO and the case studies are available for free download on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v2</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v2 Announce Type: replace-cross 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive a theoretical result that guarantees posterior consistency, based on an application of a classic theorem of Doob to our RKHS setting. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2404.05445</link>
      <description>arXiv:2404.05445v2 Announce Type: replace-cross 
Abstract: Imaging is a standard example of an inverse problem, where the task of reconstructing a ground truth from a noisy measurement is ill-posed. Recent state-of-the-art approaches for imaging use deep learning, spearheaded by unrolled and end-to-end models and trained on various image datasets. However, many such methods require the availability of ground truth data, which may be unavailable or expensive, leading to a fundamental barrier that can not be bypassed by choice of architecture. Unsupervised learning presents an alternative paradigm that bypasses this requirement, as they can be learned directly on noisy data and do not require any ground truths. A principled Bayesian approach to unsupervised learning is to maximize the marginal likelihood with respect to the given noisy measurements, which is intrinsically linked to classical variational regularization. We propose an unsupervised approach using maximum marginal likelihood estimation to train a convex neural network-based image regularization term directly on noisy measurements, improving upon previous work in both model expressiveness and dataset size. Experiments demonstrate that the proposed method produces priors that are near competitive when compared to the analogous supervised training method for various image corruption operators, maintaining significantly better generalization properties when compared to end-to-end methods. Moreover, we provide a detailed theoretical analysis of the convergence properties of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05445v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
  </channel>
</rss>
