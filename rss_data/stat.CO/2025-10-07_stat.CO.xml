<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 01:46:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model</title>
      <link>https://arxiv.org/abs/2510.03587</link>
      <description>arXiv:2510.03587v1 Announce Type: new 
Abstract: Bayesian inference for doubly-intractable probabilistic graphical models typically involves variations of the exchange algorithm or approximate Markov chain Monte Carlo (MCMC) samplers. However, existing methods for both classes of algorithms require either perfect samplers or sequential samplers for complex models, which are often either not available, or suffer from poor mixing, especially in high dimensions. We develop a method that does not require perfect or sequential sampling, and can be applied to both classes of methods: exact and approximate MCMC. The key to our approach is to utilize the tractable independence model underlying an intractable probabilistic graphical model for the purpose of constructing a finite sample unbiased Monte Carlo (and not MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out to be crucial for scalability in high dimensions. The method is demonstrated on the Ising model. Gradient-based alternatives to construct a proposal, such as Langevin and Hamiltonian Monte Carlo approaches, also arise as a natural corollary to our general procedure, and are demonstrated as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03587v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Chen, Antik Chakraborty, Anindya Bhadra</dc:creator>
    </item>
    <item>
      <title>Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped</title>
      <link>https://arxiv.org/abs/2510.03949</link>
      <description>arXiv:2510.03949v2 Announce Type: new 
Abstract: Simulating the kinetic Langevin dynamics is a popular approach for sampling from distributions, where only their unnormalized densities are available. Various discretizations of the kinetic Langevin dynamics have been considered, where the resulting algorithm is collectively referred to as the kinetic Langevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically, the stochastic exponential Euler discretization, or exponential integrator for short, has previously been studied under strongly log-concave and log-Lipschitz smooth potentials via the synchronous Wasserstein coupling strategy. Existing analyses, however, impose restrictions on the parameters that do not explain the behavior of KLMC under various choices of parameters. In particular, all known results fail to hold in the overdamped regime, suggesting that the exponential integrator degenerates in the overdamped limit. In this work, we revisit the synchronous Wasserstein coupling analysis of KLMC with the exponential integrator. Our refined analysis results in Wasserstein contractions and bounds on the asymptotic bias that hold under weaker restrictions on the parameters, which assert that the exponential integrator is capable of stably simulating the kinetic Langevin dynamics in the overdamped regime, as long as proper time acceleration is applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03949v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Samuel Gruffaz, Ji Won Park, Alain Oliviero Durmus</dc:creator>
    </item>
    <item>
      <title>Green's Function-Based Thin Plate Splines via Karhunen-Lo\`eve Expansion for Bayesian Spatial Modeling</title>
      <link>https://arxiv.org/abs/2510.04256</link>
      <description>arXiv:2510.04256v1 Announce Type: new 
Abstract: Gaussian random field is an ubiquitous model for spatial phenomena in diverse scientific disciplines. Its approximation is often crucial for computational feasibility in simulation, inference, and uncertainty quantification. The Karhunen-Lo\`eve Expansion provides a theoretically optimal basis for representing a Gaussian random field as a sum of deterministic orthonormal functions weighted by uncorrelated random variables. While this is a well-established method for dimension reduction and approximation of (spatial) stochastic process, its practical application depends on the explicit or implicit definition of the covariance structure. In this work we propose a novel approach to approximating Gaussian random field by explicitly constructing its covariance function from a regularized thin plate splines kernel. In a numerical analysis, the regularized thin plate splines kernel model, under a Bayesian approach, correctly capture the spatial correlation in the different proposed scenarios. Furthermore, the penalty term effectively shrinks most basis function coefficients toward zero, the eigenvalues decay and cumulative variance show that the proposed model efficiently reduces data dimensionality by capturing most of the variance with only a few basis functions. More importantly, from the numerical analysis we can suggest its strong potential for use beyond the Matern correlation function. In a real application, it behaves well when modeling the NO2 concentrations measured at monitoring stations throughout Germany. It has good predictive performance when assessed using the posterior medians and also demonstrate best predictive performance compared with another popular method to approximate a Gaussian random field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04256v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joaquin Cavieres, Sebastian Krumscheid</dc:creator>
    </item>
    <item>
      <title>spd-metrics-id: A Python Package for SPD-Aware Distance Metrics in Connectome Fingerprinting and Beyond</title>
      <link>https://arxiv.org/abs/2510.04438</link>
      <description>arXiv:2510.04438v1 Announce Type: new 
Abstract: We present spd-metrics-id, a Python package for computing distances and divergences between symmetric positive-definite (SPD) matrices. Unlike traditional toolkits that focus on specific applications, spd-metrics-id provides a unified, extensible, and reproducible framework for SPD distance computation. The package supports a wide variety of geometry-aware metrics, including Alpha-z Bures-Wasserstein, Alpha-Procrustes, affine-invariant Riemannian, log-Euclidean, and others, and is accessible both via a command-line interface and a Python API. Reproducibility is ensured through Docker images and Zenodo archiving. We illustrate usage through a connectome fingerprinting example, but the package is broadly applicable to covariance analysis, diffusion tensor imaging, and other domains requiring SPD matrix comparison. The package is openly available at https://pypi.org/project/spd-metrics-id/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04438v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaosar Uddin</dc:creator>
    </item>
    <item>
      <title>Constrained Dikin-Langevin diffusion for polyhedra</title>
      <link>https://arxiv.org/abs/2510.04582</link>
      <description>arXiv:2510.04582v2 Announce Type: new 
Abstract: Interior-point geometry offers a straightforward approach to constrained sampling and optimization on polyhedra, eliminating reflections and ad hoc projections. We exploit the Dikin log-barrier to define a Dikin--Langevin diffusion whose drift and noise are modulated by the inverse barrier Hessian. In continuous time, we establish a boundary no-flux property; trajectories started in the interior remain in $U$ almost surely, so feasibility is maintained by construction. For computation, we adopt a discretize-then-correct design: an Euler--Maruyama proposal with state-dependent covariance, followed by a Metropolis--Hastings correction that targets the exact constrained law and reduces to a Dikin random walk when $f$ is constant.
  Numerically, the unadjusted diffusion exhibits the expected first-order step size bias, while the MH-adjusted variant delivers strong convergence diagnostics on anisotropic, box-constrained Gaussians (rank-normalized split-$\hat{R}$ concentrated near $1$) and higher inter-well transition counts on a bimodal target, indicating superior cross-well mobility. Taken together, these results demonstrate that coupling calibrated stochasticity with interior-point preconditioning provides a practical, reflection-free approach to sampling and optimization over polyhedral domains, offering clear advantages near faces, corners, and in nonconvex landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04582v2</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Chok, Domenic Petzinna</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for High-Dimensional Linear Regression via Adaptive Shrinkage</title>
      <link>https://arxiv.org/abs/2510.03449</link>
      <description>arXiv:2510.03449v1 Announce Type: cross 
Abstract: We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for Transfer, a Bayesian multi-source transfer learning framework for high-dimensional linear regression. The proposed analytical framework leverages global-local shrinkage priors together with Bayesian source selection to balance information sharing and regularization. We show how Bayesian source selection allows for the extraction of the most useful data sources, while discounting biasing information that may lead to negative transfer. In this framework, both source selection and sparse regression are jointly accounted for in prediction and inference via Bayesian model averaging. The structure of our model admits efficient posterior simulation via a Gibbs sampling algorithm allowing full posterior inference for the target regression coefficients, making BLAST both computationally practical and inferentially straightforward. Our method achieves more accurate posterior inference for the target than regularization approaches based on target data alone, while offering competitive predictive performance and superior uncertainty quantification compared to current state-of-the-art transfer learning methods. We validate its effectiveness through extensive simulation studies and illustrate its analytical properties when applied to a case study on the estimation of tumor mutational burden from gene expression, using data from The Cancer Genome Atlas (TCGA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03449v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parsa Jamshidian, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>Efficient Log-Rank Updates for Random Survival Forests</title>
      <link>https://arxiv.org/abs/2510.03665</link>
      <description>arXiv:2510.03665v1 Announce Type: cross 
Abstract: Random survival forests are widely used for estimating covariate-conditional survival functions under right-censoring. Their standard log-rank splitting criterion is typically recomputed at each candidate split. This O(M) cost per split, with M the number of distinct event times in a node, creates a bottleneck for large cohort datasets with long follow-up. We revisit approximations proposed by LeBlanc and Crowley (1995) and develop simple constant-time updates for the log-rank criterion. The method is implemented in grf and substantially reduces training time on large datasets while preserving predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03665v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, James Yang, Michael LeBlanc</dc:creator>
    </item>
    <item>
      <title>The analogy theorem in Hoare logic</title>
      <link>https://arxiv.org/abs/2510.03685</link>
      <description>arXiv:2510.03685v1 Announce Type: cross 
Abstract: The introduction of machine learning methods has led to significant advances in automation, optimization, and discoveries in various fields of science and technology. However, their widespread application faces a fundamental limitation: the transfer of models between data domains generally lacks a rigorous mathematical justification. The key problem is the lack of formal criteria to guarantee that a model trained on one type of data will retain its properties on another.This paper proposes a solution to this problem by formalizing the concept of analogy between data sets and models using first-order logic and Hoare logic.We formulate and rigorously prove a theorem that sets out the necessary and sufficient conditions for analogy in the task of knowledge transfer between machine learning models. Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03685v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.LO</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikitin Nikita</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v3 Announce Type: replace 
Abstract: In the context of macroeconomic/financial time series, the FARS package provides a comprehensive framework in R for the construction of conditional densities of the variable of interest based on the factor-augmented quantile regressions (FA-QRs) methodology, with the factors extracted from multi-level dynamic factor models (ML-DFMs) with potential overlapping group-specific factors. Furthermore, the package also allows the construction of measures of risk as well as modeling and designing economic scenarios based on the conditional densities. In particular, the package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the FA-QRs together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; and (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v3</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Mixtures of Gaussian Process Experts with SMC$^2$</title>
      <link>https://arxiv.org/abs/2208.12830</link>
      <description>arXiv:2208.12830v3 Announce Type: replace-cross 
Abstract: Gaussian processes are a key component of many flexible statistical and machine learning models. However, they exhibit cubic computational complexity and high memory constraints due to the need of inverting and storing a full covariance matrix. To circumvent this, mixtures of Gaussian process experts have been considered where data points are assigned to independent experts, reducing the complexity by allowing inference based on smaller, local covariance matrices. Moreover, mixtures of Gaussian process experts substantially enrich the model's flexibility, allowing for behaviors such as non-stationarity, heteroscedasticity, and discontinuities. In this work, we construct a novel inference approach based on nested sequential Monte Carlo samplers to simultaneously infer both the gating network and Gaussian process expert parameters. This greatly improves inference compared to importance sampling, particularly in settings when a stationary Gaussian process is inappropriate, while still being thoroughly parallelizable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12830v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teemu H\"ark\"onen, Sara Wade, Kody Law, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Reproducing kernel methods for machine learning, PDEs, and statistics</title>
      <link>https://arxiv.org/abs/2402.07084</link>
      <description>arXiv:2402.07084v3 Announce Type: replace-cross 
Abstract: This monograph develops a unified, application-driven framework for kernel methods grounded in reproducing kernel Hilbert spaces (RKHS) and optimal transport (OT). Part I lays the theoretical and numerical foundations on positive-definite kernels; discrete and continuous RKHS; kernel engineering and scaling maps; error assessment via kernel discrepancy/maximum mean discrepancy (MMD); and a systematic operator view of kernels. In this viewpoint, projection, gradient, divergence, and Laplace-Beltrami operators are built directly from kernels, enabling discrete analogues of differential operators and variational tools that connect learning with PDE-style modeling. Part II turns to practice across four domains. In machine learning, we treat supervised and unsupervised tasks, then develop RKHS-based generative modeling, contrasting density and projection approaches and enhancing them with OT and scalable, combinatorial assignments. We introduce clustering strategies that reduce computational burden and support large-scale regression and transport. In physics-informed modeling, we present mesh-free kernel discretizations for elliptic and time-dependent PDEs, discuss automatic differentiation, and propose high-order discrete approximations. In reinforcement learning, we formulate kernel Q-learning and non-parametric HJB methods, and show how kernel operators yield sample-efficient baselines on continuous-state, discrete-action tasks. In mathematical finance, we build nonparametric time-series models and market generators, study benchmarking and extrapolation for pricing, and apply the framework to stress testing and portfolio methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07084v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Generalized promotion time cure model: A new modeling framework to identify cell-type-specific genes and improve survival prognosis</title>
      <link>https://arxiv.org/abs/2509.01001</link>
      <description>arXiv:2509.01001v2 Announce Type: replace-cross 
Abstract: Single-cell technologies provide an unprecedented opportunity for dissecting the interplay between the cancer cells and the associated tumor microenvironment, and the produced high-dimensional omics data should also augment existing survival modeling approaches for identifying tumor cell type-specific genes predictive of cancer patient survival. However, there is no statistical model to integrate multiscale data including individual-level survival data, multicellular-level cell composition data and cellular-level single-cell omics covariates. We propose a class of Bayesian generalized promotion time cure models (GPTCMs) for the multiscale data integration to identify cell-type-specific genes and improve cancer prognosis. We demonstrate with simulations in both low- and high-dimensional settings that the proposed Bayesian GPTCMs are able to identify cell-type-associated covariates and improve survival prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01001v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhao, Fatih K{\i}z{\i}laslan, Shixiong Wang, Manuela Zucknick</dc:creator>
    </item>
  </channel>
</rss>
