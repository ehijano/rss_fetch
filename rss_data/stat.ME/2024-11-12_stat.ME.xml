<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2024 05:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simulation Studies For Goodness-of-Fit and Two-Sample Methods For Univariate Data</title>
      <link>https://arxiv.org/abs/2411.05839</link>
      <description>arXiv:2411.05839v1 Announce Type: new 
Abstract: We present the results of a large number of simulation studies regarding the power of various goodness-of-fit as well as nonparametric two-sample tests for univariate data. This includes both continuous and discrete data. In general no single method can be relied upon to provide good power, any one method may be quite good for some combination of null hypothesis and alternative and may fail badly for another. Based on the results of these studies we propose a fairly small number of methods chosen such that for any of the case studies included here at least one of the methods has good power.
  The studies were carried out using the R packages R2sample and Rgof, available from CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05839v1</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Rolke</dc:creator>
    </item>
    <item>
      <title>Rational Expectations Nonparametric Empirical Bayes</title>
      <link>https://arxiv.org/abs/2411.06129</link>
      <description>arXiv:2411.06129v1 Announce Type: new 
Abstract: We examine the uniqueness of the posterior distribution within an Empirical Bayes framework using a discretized prior. To achieve this, we impose Rational Expectations conditions on the prior, focusing on coherence and stability properties. We derive the conditions necessary for posterior uniqueness when observations are drawn from either discrete or continuous distributions. Additionally, we discuss the properties of our discretized prior as an approximation of the true underlying prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06129v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentino Dardanoni, Stefano Demichelis</dc:creator>
    </item>
    <item>
      <title>It's About Time: What A/B Test Metrics Estimate</title>
      <link>https://arxiv.org/abs/2411.06150</link>
      <description>arXiv:2411.06150v1 Announce Type: new 
Abstract: Online controlled experiments, or A/B tests, are large-scale randomized trials in digital environments. This paper investigates the estimands of the difference-in-means estimator in these experiments, focusing on scenarios with repeated measurements on users. We compare cumulative metrics that use all post-exposure data for each user to windowed metrics that measure each user over a fixed time window. We analyze the estimands and highlight trade-offs between the two types of metrics. Our findings reveal that while cumulative metrics eliminate the need for pre-defined measurement windows, they result in estimands that are more intricately tied to the experiment intake and runtime. This complexity can lead to counter-intuitive practical consequences, such as decreased statistical power with more observations. However, cumulative metrics offer earlier results and can quickly detect strong initial signals. We conclude that neither metric type is universally superior. The optimal choice depends on the temporal profile of the treatment effect, the distribution of exposure, and the stopping time of the experiment. This research provides insights for experimenters to make informed decisions about how to define metrics based on their specific experimental contexts and objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06150v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Ankargren, Mattias Fr{\aa}nberg, M{\aa}rten Schultzberg</dc:creator>
    </item>
    <item>
      <title>Efficient subsampling for high-dimensional data</title>
      <link>https://arxiv.org/abs/2411.06298</link>
      <description>arXiv:2411.06298v1 Announce Type: new 
Abstract: In the field of big data analytics, the search for efficient subdata selection methods that enable robust statistical inferences with minimal computational resources is of high importance. A procedure prior to subdata selection could perform variable selection, as only a subset of a large number of variables is active. We propose an approach when both the size of the full dataset and the number of variables are large. This approach firstly identifies the active variables by applying a procedure inspired by random LASSO (Least Absolute Shrinkage and Selection Operator) and then selects subdata based on leverage scores to build a predictive model. Our proposed approach outperforms approaches that already exists in the current literature, including the usage of the full dataset, in both variable selection and prediction, while also exhibiting significant improvements in computing time. Simulation experiments as well as a real data application are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06298v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasilis Chasiotis, Lin Wang, Dimitris Karlis</dc:creator>
    </item>
    <item>
      <title>Stabilized Inverse Probability Weighting via Isotonic Calibration</title>
      <link>https://arxiv.org/abs/2411.06342</link>
      <description>arXiv:2411.06342v1 Announce Type: new 
Abstract: Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06342v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ziming Lin, Marco Carone, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Analysis of spatially clustered survival data with unobserved covariates using SBART</title>
      <link>https://arxiv.org/abs/2411.06591</link>
      <description>arXiv:2411.06591v1 Announce Type: new 
Abstract: Usual parametric and semi-parametric regression methods are inappropriate and inadequate for large clustered survival studies when the appropriate functional forms of the covariates and their interactions in hazard functions are unknown, and random cluster effects and cluster-level covariates are spatially correlated. We present a general nonparametric method for such studies under the Bayesian ensemble learning paradigm called Soft Bayesian Additive Regression Trees. Our methodological and computational challenges include large number of clusters, variable cluster sizes, and proper statistical augmentation of the unobservable cluster-level covariate using a data registry different from the main survival study. We use an innovative 3-step approach based on latent variables to address our computational challenges. We illustrate our method and its advantages over existing methods by assessing the impacts of intervention in some county-level and patient-level covariates to mitigate existing racial disparity in breast cancer survival in 67 Florida counties (clusters) using two different data resources. Florida Cancer Registry (FCR) is used to obtain clustered survival data with patient-level covariates, and the Behavioral Risk Factor Surveillance Survey (BRFSS) is used to obtain further data information on an unobservable county-level covariate of Screening Mammography Utilization (SMU).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06591v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Durbadal Ghosh, Debajyoti Sinha, Antonio R. Linero, George Rust</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation for Partially Observed McKean-Vlasov Diffusions</title>
      <link>https://arxiv.org/abs/2411.06716</link>
      <description>arXiv:2411.06716v1 Announce Type: new 
Abstract: In this article we consider likelihood-based estimation of static parameters for a class of partially observed McKean-Vlasov (POMV) diffusion process with discrete-time observations over a fixed time interval. In particular, using the framework of [5] we develop a new randomized multilevel Monte Carlo method for estimating the parameters, based upon Markovian stochastic approximation methodology. New Markov chain Monte Carlo algorithms for the POMV model are introduced facilitating the application of [5]. We prove, under assumptions, that the expectation of our estimator is biased, but with expected small and controllable bias. Our approach is implemented on several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06716v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajay Jasra, Mohamed Maama, Raul Tempone</dc:creator>
    </item>
    <item>
      <title>BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments</title>
      <link>https://arxiv.org/abs/2411.06913</link>
      <description>arXiv:2411.06913v1 Announce Type: new 
Abstract: Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between exposure and outcome. An IV must affect the outcome exclusively through the exposure and be unconfounded with the outcome. We present a framework for relaxing either or both of these strong assumptions with tuneable and interpretable budget constraints. Our algorithm returns a feasible set of causal effects that can be identified exactly given relevant covariance parameters. The feasible set may be disconnected but is a finite union of convex subsets. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We also adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06913v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Penn (King's College London), Lee M. Gunderson (University College London), Gecia Bravo-Hermsdorff (University College London), Ricardo Silva (University College London), David S. Watson (King's College London)</dc:creator>
    </item>
    <item>
      <title>Estimating abilities with an Elo-informed growth model</title>
      <link>https://arxiv.org/abs/2411.07028</link>
      <description>arXiv:2411.07028v1 Announce Type: new 
Abstract: An intelligent tutoring system (ITS) aims to provide instructions and exercises tailored to the ability of a student. To do this, the ITS needs to estimate the ability based on student input. Rather than including frequent full-scale tests to update our ability estimate, we want to base estimates on the outcomes of practice exercises that are part of the learning process. A challenge with this approach is that the ability changes as the student learns, which makes traditional item response theory (IRT) models inappropriate. Most IRT models estimate an ability based on a test result, and assume that the ability is constant throughout a test.
  We review some existing methods for measuring abilities that change throughout the measurement period, and propose a new method which we call the Elo-informed growth model. This method assumes that the abilities for a group of respondents who are all in the same stage of the learning process follow a distribution that can be estimated. The method does not assume a particular shape of the growth curve. It performs better than the standard Elo algorithm when the measured outcomes are far apart in time, or when the ability change is rapid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07028v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Sigfrid, Ellinor Fackle-Fornius, Frank Miller</dc:creator>
    </item>
    <item>
      <title>Self-separated and self-connected models for mediator and outcome missingness in mediation analysis</title>
      <link>https://arxiv.org/abs/2411.07221</link>
      <description>arXiv:2411.07221v1 Announce Type: new 
Abstract: Missing data is a common problem that challenges the study of effects of treatments. In the context of mediation analysis, this paper addresses missingness in the two key variables, mediator and outcome, focusing on identification. We consider self-separated missingness models where identification is achieved by conditional independence assumptions only and self-connected missingness models where identification relies on so-called shadow variables. The first class is somewhat limited as it is constrained by the need to remove a certain number of connections from the model. The second class turns out to include substantial variation in the position of the shadow variable in the causal structure (vis-a-vis the mediator and outcome) and the corresponding implications for the model. In constructing the models, to improve plausibility, we pay close attention to allowing, where possible, dependencies due to unobserved causes of the missingness. In this exploration, we develop theory where needed. This results in templates for identification in this mediation setting, generally useful identification techniques, and perhaps most significantly, synthesis and substantial expansion of shadow variable theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07221v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trang Quynh Nguyen, Razieh Nabi, Fan Yang, Elizabeth A. Stuart</dc:creator>
    </item>
    <item>
      <title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
      <link>https://arxiv.org/abs/2411.05869</link>
      <description>arXiv:2411.05869v1 Announce Type: cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Unlike many other machine learning methods, GPs include an implicit characterization of uncertainty, making them extremely useful across many areas of science, technology, and engineering. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05869v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi</dc:creator>
    </item>
    <item>
      <title>An Adaptive Online Smoother with Closed-Form Solutions and Information-Theoretic Lag Selection for Conditional Gaussian Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2411.05870</link>
      <description>arXiv:2411.05870v1 Announce Type: cross 
Abstract: Data assimilation (DA) combines partial observations with a dynamical model to improve state estimation. Filter-based DA uses only past and present data and is the prerequisite for real-time forecasts. Smoother-based DA exploits both past and future observations. It aims to fill in missing data, provide more accurate estimations, and develop high-quality datasets. However, the standard smoothing procedure requires using all historical state estimations, which is storage-demanding, especially for high-dimensional systems. This paper develops an adaptive-lag online smoother for a large class of complex dynamical systems with strong nonlinear and non-Gaussian features, which has important applications to many real-world problems. The adaptive lag allows the DA to utilize only observations within a nearby window, significantly reducing computational storage. Online lag adjustment is essential for tackling turbulent systems, where temporal autocorrelation varies significantly over time due to intermittency, extreme events, and nonlinearity. Based on the uncertainty reduction in the estimated state, an information criterion is developed to systematically determine the adaptive lag. Notably, the mathematical structure of these systems facilitates the use of closed analytic formulae to calculate the online smoother and the adaptive lag, avoiding empirical tunings as in ensemble-based DA methods. The adaptive online smoother is applied to studying three important scientific problems. First, it helps detect online causal relationships between state variables. Second, its advantage of computational storage is illustrated via Lagrangian DA, a high-dimensional nonlinear problem. Finally, the adaptive smoother advances online parameter estimation with partial observations, emphasizing the role of the observed extreme events in accelerating convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05870v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Andreou, Nan Chen, Yingda Li</dc:creator>
    </item>
    <item>
      <title>Deep Nonparametric Conditional Independence Tests for Images</title>
      <link>https://arxiv.org/abs/2411.06140</link>
      <description>arXiv:2411.06140v1 Announce Type: cross 
Abstract: Conditional independence tests (CITs) test for conditional dependence between random variables. As existing CITs are limited in their applicability to complex, high-dimensional variables such as images, we introduce deep nonparametric CITs (DNCITs). The DNCITs combine embedding maps, which extract feature representations of high-dimensional variables, with nonparametric CITs applicable to these feature representations. For the embedding maps, we derive general properties on their parameter estimators to obtain valid DNCITs and show that these properties include embedding maps learned through (conditional) unsupervised or transfer learning. For the nonparametric CITs, appropriate tests are selected and adapted to be applicable to feature representations. Through simulations, we investigate the performance of the DNCITs for different embedding maps and nonparametric CITs under varying confounder dimensions and confounder relationships. We apply the DNCITs to brain MRI scans and behavioral traits, given confounders, of healthy individuals from the UK Biobank (UKB), confirming null results from a number of ambiguous personality neuroscience studies with a larger data set and with our more powerful tests. In addition, in a confounder control study, we apply the DNCITs to brain MRI scans and a confounder set to test for sufficient confounder control, leading to a potential reduction in the confounder dimension under improved confounder control compared to existing state-of-the-art confounder control studies for the UKB. Finally, we provide an R package implementing the DNCITs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06140v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Simnacher, Xiangnan Xu, Hani Park, Christoph Lippert, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>Amortized Bayesian Local Interpolation NetworK: Fast covariance parameter estimation for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2411.06324</link>
      <description>arXiv:2411.06324v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) are a ubiquitous tool for geostatistical modeling with high levels of flexibility and interpretability, and the ability to make predictions at unseen spatial locations through a process called Kriging. Estimation of Kriging weights relies on the inversion of the process' covariance matrix, creating a computational bottleneck for large spatial datasets. In this paper, we propose an Amortized Bayesian Local Interpolation NetworK (A-BLINK) for fast covariance parameter estimation, which uses two pre-trained deep neural networks to learn a mapping from spatial location coordinates and covariance function parameters to Kriging weights and the spatial variance, respectively. The fast prediction time of these networks allows us to bypass the matrix inversion step, creating large computational speedups over competing methods in both frequentist and Bayesian settings, and also provides full posterior inference and predictions using Markov chain Monte Carlo sampling methods. We show significant increases in computational efficiency over comparable scalable GP methodology in an extensive simulation study with lower parameter estimation error. The efficacy of our approach is also demonstrated using a temperature dataset of US climate normals for 1991--2020 based on over 7,000 weather stations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06324v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon R. Feng, Reetam Majumder, Brian J. Reich, Mohamed A. Abba</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning from Multimodal Biological Observations</title>
      <link>https://arxiv.org/abs/2411.06518</link>
      <description>arXiv:2411.06518v1 Announce Type: cross 
Abstract: Prevalent in biological applications (e.g., human phenotype measurements), multimodal datasets can provide valuable insights into the underlying biological mechanisms. However, current machine learning models designed to analyze such datasets still lack interpretability and theoretical guarantees, which are essential to biological applications. Recent advances in causal representation learning have shown promise in uncovering the interpretable latent causal variables with formal theoretical certificates. Unfortunately, existing works for multimodal distributions either rely on restrictive parametric assumptions or provide rather coarse identification results, limiting their applicability to biological research which favors a detailed understanding of the mechanisms.
  In this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biological datasets. Theoretically, we consider a flexible nonparametric latent distribution (c.f., parametric assumptions in prior work) permitting causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from prior work. Our key theoretical ingredient is the structural sparsity of the causal connections among distinct modalities, which, as we will discuss, is natural for a large collection of biological systems. Empirically, we propose a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established medical research, validating our theoretical and methodological framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06518v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Algebraic and Statistical Properties of the Partially Regularized Ordinary Least Squares Interpolator</title>
      <link>https://arxiv.org/abs/2411.06593</link>
      <description>arXiv:2411.06593v1 Announce Type: cross 
Abstract: Modern deep learning has revealed a surprising statistical phenomenon known as benign overfitting, with high-dimensional linear regression being a prominent example. This paper contributes to ongoing research on the ordinary least squares (OLS) interpolator, focusing on the partial regression setting, where only a subset of coefficients is implicitly regularized. On the algebraic front, we extend Cochran's formula and the leave-one-out residual formula for the partial regularization framework. On the stochastic front, we leverage our algebraic results to design several homoskedastic variance estimators under the Gauss-Markov model. These estimators serve as a basis for conducting statistical inference, albeit with slight conservatism in their performance. Through simulations, we study the finite-sample properties of these variance estimators across various generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06593v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Letian Yang, Dennis Shen</dc:creator>
    </item>
    <item>
      <title>SequentialSamplingModels.jl: Simulating and Evaluating Cognitive Models of Response Times in Julia</title>
      <link>https://arxiv.org/abs/2411.06631</link>
      <description>arXiv:2411.06631v1 Announce Type: cross 
Abstract: Sequential sampling models (SSMs) are a widely used framework describing decision-making as a stochastic, dynamic process of evidence accumulation. SSMs popularity across cognitive science has driven the development of various software packages that lower the barrier for simulating, estimating, and comparing existing SSMs. Here, we present a software tool, SequentialSamplingModels.jl (SSM.jl), designed to make SSM simulations more accessible to Julia users, and to integrate with the Julia ecosystem. We demonstrate the basic use of SSM.jl for simulation, plotting, and Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06631v1</guid>
      <category>cs.MS</category>
      <category>q-bio.NC</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kiant\'e Fernandez, Dominique Makowski, Christopher Fisher</dc:creator>
    </item>
    <item>
      <title>Score function-based tests for ultrahigh-dimensional linear models</title>
      <link>https://arxiv.org/abs/2212.08446</link>
      <description>arXiv:2212.08446v2 Announce Type: replace 
Abstract: In this paper, we investigate score function-based tests to check the significance of an ultrahigh-dimensional sub-vector of the model coefficients when the nuisance parameter vector is also ultrahigh-dimensional in linear models. We first reanalyze and extend a recently proposed score function-based test to derive, under weaker conditions, its limiting distributions under the null and local alternative hypotheses. As it may fail to work when the correlation between testing covariates and nuisance covariates is high, we propose an orthogonalized score function-based test with two merits: debiasing to make the non-degenerate error term degenerate and reducing the asymptotic variance to enhance power performance. Simulations evaluate the finite-sample performances of the proposed tests, and a real data analysis illustrates its application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.08446v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichao Yang, Xu Guo, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>A mixture transition distribution modeling for higher-order circular Markov processes</title>
      <link>https://arxiv.org/abs/2304.00874</link>
      <description>arXiv:2304.00874v5 Announce Type: replace 
Abstract: The stationary higher-order Markov process for circular data is considered. We employ the mixture transition distribution (MTD) model to express the transition density of the process on the circle. The underlying circular transition distribution is based on Wehrly and Johnson's bivariate joint circular models. The structures of the circular autocorrelation function together with the circular partial autocorrelation function are found to be similar to those of the autocorrelation and partial autocorrelation functions of the real-valued autoregressive process when the underlying binding density has zero sine moments. The validity of the model is assessed by applying it to some Monte Carlo simulations and real directional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00874v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroaki Ogata, Takayuki Shiohama</dc:creator>
    </item>
    <item>
      <title>Truly Multivariate Structured Additive Distributional Regression</title>
      <link>https://arxiv.org/abs/2306.02711</link>
      <description>arXiv:2306.02711v2 Announce Type: replace 
Abstract: Generalized additive models for location, scale and shape (GAMLSS) are a popular extension to mean regression models where each parameter of an arbitrary distribution is modelled through covariates. While such models have been developed for univariate and bivariate responses, the truly multivariate case remains extremely challenging for both computational and theoretical reasons. Alternative approaches to GAMLSS may allow for higher dimensional response vectors to be modelled jointly but often assume a fixed dependence structure not depending on covariates or are limited with respect to modelling flexibility or computational aspects. We contribute to this gap in the literature and propose a truly multivariate distributional model, which allows one to benefit from the flexibility of GAMLSS even when the response has dimension larger than two or three. Building on copula regression, we model the dependence structure of the response through a Gaussian copula, while the marginal distributions can vary across components. Our model is highly parameterized but estimation becomes feasible with Bayesian inference employing shrinkage priors. We demonstrate the competitiveness of our approach in a simulation study and illustrate how it complements existing models along the examples of childhood malnutrition and a yet unexplored data set on traffic detection in Berlin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02711v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, Nadja Klein</dc:creator>
    </item>
    <item>
      <title>Guidance on Individualized Treatment Rule Estimation in High Dimensions</title>
      <link>https://arxiv.org/abs/2306.16402</link>
      <description>arXiv:2306.16402v2 Announce Type: replace 
Abstract: Individualized treatment rules, cornerstones of precision medicine, inform patient treatment decisions with the goal of optimizing patient outcomes. These rules are generally unknown functions of patients' pre-treatment covariates, meaning they must be estimated from clinical or observational study data. Myriad methods have been developed to learn these rules, and these procedures are demonstrably successful in traditional asymptotic settings with moderate number of covariates. The finite-sample performance of these methods in high-dimensional covariate settings, which are increasingly the norm in modern clinical trials, has not been well characterized, however. We perform a comprehensive comparison of state-of-the-art individualized treatment rule estimators, assessing performance on the basis of the estimators' accuracy, interpretability, and computational efficacy. Sixteen data-generating processes with continuous outcomes and binary treatment assignments are considered, reflecting a diversity of randomized and observational studies. We summarize our findings and provide succinct advice to practitioners needing to estimate individualized treatment rules in high dimensions. All code is made publicly available, facilitating modifications and extensions to our simulation study. A novel pre-treatment covariate filtering procedure is also proposed and is shown to improve estimators' accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16402v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Boileau, Ning Leng, Sandrine Dudoit</dc:creator>
    </item>
    <item>
      <title>Randomization-Based Inference for Average Treatment Effect in Inexactly Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2308.02005</link>
      <description>arXiv:2308.02005v3 Announce Type: replace 
Abstract: Matching is a widely used causal inference study design in observational studies. It seeks to mimic a randomized experiment by forming matched sets of treated and control units based on proximity in covariates. Ideally, treated units are exactly matched with controls for the covariates, and randomization-based inference for the treatment effect can then be conducted as in a randomized experiment under the ignorability assumption. However, matching is typically inexact when continuous covariates or many covariates exist. Previous studies have routinely ignored inexact matching in the downstream randomization-based inference as long as some covariate balance criteria are satisfied. Some recent studies found that this routine practice can cause severe bias. They proposed new inference methods for correcting for bias due to inexact matching. However, these inference methods focus on the constant treatment effect (i.e., Fisher's sharp null) and are not directly applicable to the average treatment effect (i.e., Neyman's weak null). To address this problem, we propose a new framework - inverse post-matching probability weighting (IPPW) - for randomization-based average treatment effect inference under inexact matching. Compared with the routinely used randomization-based inference framework based on the difference-in-means estimator, our proposed IPPW framework can substantially reduce bias due to inexact matching and improve the coverage rate. We have also developed an open-source R package RIIM (Randomization-Based Inference under Inexact Matching) for implementing our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02005v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Zhu, Jeffrey Zhang, Zijian Guo, Siyu Heng</dc:creator>
    </item>
    <item>
      <title>Scalable simulation-based inference for implicitly defined models using a metamodel for Monte Carlo log-likelihood estimator</title>
      <link>https://arxiv.org/abs/2311.09446</link>
      <description>arXiv:2311.09446v2 Announce Type: replace 
Abstract: Models implicitly defined through a random simulator of a process have become widely used in scientific and industrial applications in recent years. However, simulation-based inference methods for such implicit models, like approximate Bayesian computation (ABC), often scale poorly as data size increases. We develop a scalable inference method for implicitly defined models using a metamodel for the Monte Carlo log-likelihood estimator derived from simulations. This metamodel characterizes both statistical and simulation-based randomness in the distribution of the log-likelihood estimator across different parameter values. Our metamodel-based method quantifies uncertainty in parameter estimation in a principled manner, leveraging the local asymptotic normality of the mean function of the log-likelihood estimator. We apply this method to construct accurate confidence intervals for parameters of partially observed Markov process models where the Monte Carlo log-likelihood estimator is obtained using the bootstrap particle filter. We numerically demonstrate that our method enables accurate and highly scalable parameter inference across several examples, including a mechanistic compartment model for infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09446v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonha Park</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Randomized Trials with Partial Clustering</title>
      <link>https://arxiv.org/abs/2406.04505</link>
      <description>arXiv:2406.04505v2 Announce Type: replace 
Abstract: Clustering and dependence are common in trials. For example, in some cluster randomized trials (CRTs), pre-existing clusters are enrolled, randomized, and serve as the basis of intervention delivery. Such CRTs are "fully clustered": participants are dependent within clusters. In contrast, "partially clustered" trials contain a mix of participants that are dependent within clusters and participants that are completely independent. One example of this design is a trial where participants are artificially grouped together for the purposes of randomization only; then, for intervention participants, the groups are the basis for intervention delivery, while control participants are un-grouped. Another example is an individually randomized group treatment trial (IRGTT) where participants are individually randomized and, post-randomization, intervention participants are grouped for intervention delivery, while the control participants remain un-grouped. For the three trial designs, we use causal models to non-parametrically describe the data generating process and formalize the observed data dependence structure. We show that despite the different randomization approach, both designs can be represented with the same dependence structure, enabling the use of the same statistical methods for estimation and inference of causal effects. We propose a novel implementation of targeted minimum loss-based estimation (TMLE) for these trials. TMLE is model-robust, leverages covariate adjustment and machine learning, and estimates many causal effects. In simulations, TMLE achieved comparable higher statistical power than alternatives for partially clustered designs. Finally, application to real data from the SEARCH-IPT trial resulted in 20-57% efficiency gains, demonstrating the consequences of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04505v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Nugent, Elijah Kakande, Gabriel Chamie, Jane Kabami, Asiphas Owaraganise, Diane V. Havlir, Moses Kamya, Laura Balzer</dc:creator>
    </item>
    <item>
      <title>Boosted Conformal Prediction Intervals</title>
      <link>https://arxiv.org/abs/2406.07449</link>
      <description>arXiv:2406.07449v2 Announce Type: replace 
Abstract: This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07449v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ran Xie, Rina Foygel Barber, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Conformal prediction after efficiency-oriented model selection</title>
      <link>https://arxiv.org/abs/2408.07066</link>
      <description>arXiv:2408.07066v2 Announce Type: replace 
Abstract: Given a family of pretrained models and a hold-out set, how can we construct a valid conformal prediction set while selecting a model that minimizes the width of the set? If we use the same hold-out data set both to select a model (the model that yields the smallest conformal prediction sets) and then to construct a conformal prediction set based on that selected model, we suffer a loss of coverage due to selection bias. Alternatively, we could further splitting the data to perform selection and calibration separately, but this comes at a steep cost if the size of the dataset is limited. In this paper, we address the challenge of constructing a valid prediction set after efficiency-oriented model selection. Our novel methods can be implemented efficiently and admit finite-sample validity guarantees without invoking additional sample-splitting. We show that our methods yield prediction sets with asymptotically optimal size under certain notion of continuity for the model class. The improved efficiency of the prediction sets constructed by our methods are further demonstrated through applications to synthetic datasets in various settings and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07066v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiting Liang, Wanrong Zhu, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Sensitivity of MCMC-based analyses to small-data removal</title>
      <link>https://arxiv.org/abs/2408.07240</link>
      <description>arXiv:2408.07240v2 Announce Type: replace 
Abstract: If the conclusion of a data analysis is sensitive to dropping very few data points, that conclusion might hinge on the particular data at hand rather than representing a more broadly applicable truth. How could we check whether this sensitivity holds? One idea is to consider every small subset of data, drop it from the dataset, and re-run our analysis. But running MCMC to approximate a Bayesian posterior is already very expensive; running multiple times is prohibitive, and the number of re-runs needed here is combinatorially large. Recent work proposes a fast and accurate approximation to find the worst-case dropped data subset, but that work was developed for problems based on estimating equations -- and does not directly handle Bayesian posterior approximations using MCMC. We make two principal contributions in the present work. We adapt the existing data-dropping approximation to estimators computed via MCMC. Observing that Monte Carlo errors induce variability in the approximation, we use a variant of the bootstrap to quantify this uncertainty. We demonstrate how to use our approximation in practice to determine whether there is non-robustness in a problem. Empirically, our method is accurate in simple models, such as linear regression. In models with complicated structure, such as hierarchical models, the performance of our method is mixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07240v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tin D. Nguyen, Ryan Giordano, Rachael Meager, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Approximations to worst-case data dropping: unmasking failure modes</title>
      <link>https://arxiv.org/abs/2408.09008</link>
      <description>arXiv:2408.09008v2 Announce Type: replace 
Abstract: A data analyst might worry about generalization if dropping a very small fraction of data points from a study could change its substantive conclusions. Finding the worst-case data subset to drop poses a combinatorial optimization problem. To overcome this intractability, recent works propose using additive approximations, which treat the contribution of a collection of data points as the sum of their individual contributions, and greedy approximations, which iteratively select the point with the highest impact to drop and re-run the data analysis without that point [Broderick et al., 2020, Kuschnig et al., 2021]. We identify that, even in a setting as simple as OLS linear regression, many of these approximations can break down in realistic data arrangements. Several of our examples reflect masking, where one outlier may hide or conceal the effect of another outlier. Based on the failures we identify, we provide recommendations for users and suggest directions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09008v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Y. Huang, David R. Burt, Tin D. Nguyen, Yunyi Shen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Directional data analysis using the spherical Cauchy and the Poisson kernel-based distribution</title>
      <link>https://arxiv.org/abs/2409.03292</link>
      <description>arXiv:2409.03292v5 Announce Type: replace 
Abstract: In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton-Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03292v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michail Tsagris, Panagiotis Papastamoulis, Shogo Kato</dc:creator>
    </item>
    <item>
      <title>Econometric Inference for High Dimensional Predictive Regressions</title>
      <link>https://arxiv.org/abs/2409.10030</link>
      <description>arXiv:2409.10030v2 Announce Type: replace 
Abstract: LASSO introduces shrinkage bias into estimated coefficients, which can adversely affect the desirable asymptotic normality and invalidate the standard inferential procedure based on the $t$-statistic. The desparsified LASSO has emerged as a well-known remedy for this issue. In the context of high dimensional predictive regression, the desparsified LASSO faces an additional challenge: the Stambaugh bias arising from nonstationary regressors. To restore the standard inferential procedure, we propose a novel estimator called IVX-desparsified LASSO (XDlasso). XDlasso eliminates the shrinkage bias and the Stambaugh bias simultaneously and does not require prior knowledge about the identities of nonstationary and stationary regressors. We establish the asymptotic properties of XDlasso for hypothesis testing, and our theoretical findings are supported by Monte Carlo simulations. Applying our method to real-world applications from the FRED-MD database -- which includes a rich set of control variables -- we investigate two important empirical questions: (i) the predictability of the U.S. stock returns based on the earnings-price ratio, and (ii) the predictability of the U.S. inflation using the unemployment rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10030v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Gao, Ji Hyung Lee, Ziwei Mei, Zhentao Shi</dc:creator>
    </item>
    <item>
      <title>Agnostic Characterization of Interference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2410.13142</link>
      <description>arXiv:2410.13142v2 Announce Type: replace 
Abstract: We give an approach for characterizing interference by lower bounding the number of units whose outcome depends on certain groups of treated individuals, such as depending on the treatment of others, or others who are at least a certain distance away. The approach is applicable to randomized experiments with binary-valued outcomes. Asymptotically conservative point estimates and one-sided confidence intervals may be constructed with no assumptions beyond the known randomization design, allowing the approach to be used when interference is poorly understood, or when an observed network might only be a crude proxy for the underlying social mechanisms. Point estimates are equal to Hajek-weighted comparisons of units with differing levels of treatment exposure. Empirically, we find that the size of our interval estimates is competitive with (and often smaller than) those of the EATE, an assumption-lean treatment effect, suggesting that the proposed estimands may be intrinsically easier to estimate than treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13142v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Choi</dc:creator>
    </item>
    <item>
      <title>When Frictions are Fractional: Rough Noise in High-Frequency Data</title>
      <link>https://arxiv.org/abs/2106.16149</link>
      <description>arXiv:2106.16149v4 Announce Type: replace-cross 
Abstract: The analysis of high-frequency financial data is often impeded by the presence of noise. This article is motivated by intraday return data in which market microstructure noise appears to be rough, that is, best captured by a continuous-time stochastic process that locally behaves as fractional Brownian motion. Assuming that the underlying efficient price process follows a continuous It\^o semimartingale, we derive consistent estimators and asymptotic confidence intervals for the roughness parameter of the noise and the integrated price and noise volatilities, in all cases where these quantities are identifiable. In addition to desirable features such as serial dependence of increments, compatibility between different sampling frequencies and diurnal effects, the rough noise model can further explain divergence rates in volatility signature plots that vary considerably over time and between assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.16149v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Thomas Delerue, Guoying Li</dc:creator>
    </item>
    <item>
      <title>Short-time expansion of characteristic functions in a rough volatility setting with applications</title>
      <link>https://arxiv.org/abs/2208.00830</link>
      <description>arXiv:2208.00830v2 Announce Type: replace-cross 
Abstract: We derive a higher-order asymptotic expansion of the conditional characteristic function of the increment of an It\^o semimartingale over a shrinking time interval. The spot characteristics of the It\^o semimartingale are allowed to have dynamics of general form. In particular, their paths can be rough, that is, exhibit local behavior like that of a fractional Brownian motion, while at the same time have jumps with arbitrary degree of activity. The expansion result shows the distinct roles played by the different features of the spot characteristics dynamics. As an application of our result, we construct a nonparametric estimator of the Hurst parameter of the diffusive volatility process from portfolios of short-dated options written on an underlying asset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00830v2</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Viktor Todorov</dc:creator>
    </item>
    <item>
      <title>Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model</title>
      <link>https://arxiv.org/abs/2306.14318</link>
      <description>arXiv:2306.14318v5 Announce Type: replace-cross 
Abstract: The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter's (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14318v5</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2024.100870</arxiv:DOI>
      <dc:creator>Michael Tsyrulnikov, Arseniy Sotskiy</dc:creator>
    </item>
    <item>
      <title>Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel</title>
      <link>https://arxiv.org/abs/2311.01762</link>
      <description>arXiv:2311.01762v2 Announce Type: replace-cross 
Abstract: Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes solving a system of linear equations, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to achieve both zero training error in combination with good generalization, and a double descent behavior, phenomena that do not occur for KRR with constant bandwidth but are known to appear for neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01762v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar Allerbo</dc:creator>
    </item>
    <item>
      <title>Wasserstein-based Minimax Estimation of Dependence in Multivariate Regularly Varying Extremes</title>
      <link>https://arxiv.org/abs/2312.09862</link>
      <description>arXiv:2312.09862v2 Announce Type: replace-cross 
Abstract: We present the first minimax risk bounds for estimators of the spectral measure in multivariate linear factor models, where observations are linear combinations of regularly varying latent factors. Non-asymptotic convergence rates are derived for the multivariate Peak-over-Threshold estimator in terms of the $p$-th order Wasserstein distance, and information-theoretic lower bounds for the minimax risks are established. The convergence rate of the estimator is shown to be minimax optimal under a class of Pareto-type models analogous to the standard class used in the setting of one-dimensional observations known as the Hall-Welsh class. When the estimator is minimax inefficient, a novel two-step estimator is introduced and demonstrated to attain the minimax lower bound. Our analysis bridges the gaps in understanding trade-offs between estimation bias and variance in multivariate extreme value theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09862v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuhui Zhang, Jose Blanchet, Youssef Marzouk, Viet Anh Nguyen, Sven Wang</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: joint estimation and corrected two-stage approaches</title>
      <link>https://arxiv.org/abs/2405.20418</link>
      <description>arXiv:2405.20418v2 Announce Type: replace-cross 
Abstract: Predicting cancer-associated clinical events is challenging in oncology. In Multiple Myeloma (MM), a cancer of plasma cells, disease progression is determined by changes in biomarkers, such as serum concentration of the paraprotein secreted by plasma cells (M-protein). Therefore, the time-dependent behaviour of M-protein and the transition across lines of therapy (LoT) that may be a consequence of disease progression should be accounted for in statistical models to predict relevant clinical outcomes. Furthermore, it is important to understand the contribution of the patterns of longitudinal biomarkers, upon each LoT initiation, to time-to-death or time-to-next-LoT. Motivated by these challenges, we propose a Bayesian joint model for trajectories of multiple longitudinal biomarkers, such as M-protein, and the competing risks of death and transition to next LoT. Additionally, we explore two estimation approaches for our joint model: simultaneous estimation of all parameters (joint estimation) and sequential estimation of parameters using a corrected two-stage strategy aiming to reduce computational time. Our proposed model and estimation methods are applied to a retrospective cohort study from a real-world database of patients diagnosed with MM in the US from January 2015 to February 2022. We split the data into training and test sets in order to validate the joint model using both estimation approaches and make dynamic predictions of times until clinical events of interest, informed by longitudinally measured biomarkers and baseline variables available up to the time of prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20418v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Spyros Roumpanis, Sean Yiu, Felipe Castro, Jochen Schulze, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>Self-Organizing State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v4 Announce Type: replace-cross 
Abstract: We consider a state-space model (SSM) parametrized by some parameter $\theta$ and aim at performing joint parameter and state inference. A popular idea to carry out this task is to replace $\theta$ by a Markov chain $(\theta_t)_{t\geq 0}$ and then to apply a filtering algorithm to the extended, or self-organizing SSM (SO-SSM). However, the practical implementation of this idea in a theoretically justified way has remained an open problem. In this paper we fill this gap by introducing constructions of $(\theta_t)_{t\geq 0}$ that ensure the validity of the SO-SSM for joint parameter and state inference. Notably, we show that such SO-SSMs can be defined even if $\|\mathrm{Var}(\theta_{t}|\theta_{t-1})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This result is important since these models can be efficiently approximated using a particle filter. While SO-SSMs have been introduced for online inference, the development of iterated filtering (IF) has shown that they can also serve for computing the maximum likelihood estimator of an SSM. We also derive constructions of $(\theta_t)_{t\geq 0}$ and theoretical guarantees tailored to these specific applications of SO-SSMs and introduce new IF algorithms. From a practical point of view, the algorithms we develop are simple to implement and only require minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v4</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v2 Announce Type: replace-cross 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity--especially exclusion restrictions--is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can dramatically accelerate this process and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We contend that multi-step and role-playing prompting strategies are effective for simulating the endogenous decision-making processes of economic agents and for navigating language models through the realm of real-world scenarios. We apply our method to three well-known examples in economics: returns to schooling, supply and demand, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
  </channel>
</rss>
