<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 May 2025 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Model Checks in a Kernel Ridge Regression Framework</title>
      <link>https://arxiv.org/abs/2505.01161</link>
      <description>arXiv:2505.01161v1 Announce Type: new 
Abstract: We propose new reproducing kernel-based tests for model checking in conditional moment restriction models. By regressing estimated residuals on kernel functions via kernel ridge regression (KRR), we obtain a coefficient function in a reproducing kernel Hilbert space (RKHS) that is zero if and only if the model is correctly specified. We introduce two classes of test statistics: (i) projection-based tests, using RKHS inner products to capture global deviations, and (ii) random location tests, evaluating the KRR estimator at randomly chosen covariate points to detect local departures. The tests are consistent against fixed alternatives and sensitive to local alternatives at the $n^{-1/2}$ rate. When nuisance parameters are estimated, Neyman orthogonality projections ensure valid inference without repeated estimation in bootstrap samples. The random location tests are interpretable and can visualize model misspecification. Simulations show strong power and size control, especially in higher dimensions, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01161v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhao Li</dc:creator>
    </item>
    <item>
      <title>Detecting multiple change points in linear models with heteroscedastic errors</title>
      <link>https://arxiv.org/abs/2505.01296</link>
      <description>arXiv:2505.01296v1 Announce Type: new 
Abstract: The problem of detecting change points in the regression parameters of a linear regression model with errors and covariates exhibiting heteroscedasticity is considered. Asymptotic results for weighted functionals of the cumulative sum (CUSUM) processes of model residuals are established when the model errors are weakly dependent and non-stationary, allowing for either abrupt or smooth changes in their variance. These theoretical results illuminate how to adapt standard change point test statistics for linear models to this setting. We studied such adapted change-point tests in simulation experiments, along with a finite sample adjustment to the proposed testing procedures. The results suggest that these methods perform well in practice for detecting multiple change points in the linear model parameters and controlling the Type I error rate in the presence of heteroscedasticity. We illustrate the use of these approaches in applications to test for instability in predictive regression models and explanatory asset pricing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01296v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lajos Horvath, Gregory Rice, Yuqian Zhao</dc:creator>
    </item>
    <item>
      <title>Proper Correlation Coefficients for Nominal Random Variables</title>
      <link>https://arxiv.org/abs/2505.00785</link>
      <description>arXiv:2505.00785v1 Announce Type: cross 
Abstract: This paper develops an intuitive concept of perfect dependence between two variables of which at least one has a nominal scale that is attainable for all marginal distributions and proposes a set of dependence measures that are 1 if and only if this perfect dependence is satisfied. The advantages of these dependence measures relative to classical dependence measures like contingency coefficients, Goodman-Kruskal's lambda and tau and the so-called uncertainty coefficient are twofold. Firstly, they are defined if one of the variables is real-valued and exhibits continuities. Secondly, they satisfy the property of attainability. That is, they can take all values in the interval [0,1] irrespective of the marginals involved. Both properties are not shared by the classical dependence measures which need two discrete marginal distributions and can in some situations yield values close to 0 even though the dependence is strong or even perfect.
  Additionally, I provide a consistent estimator for one of the new dependence measures together with its asymptotic distribution under independence as well as in the general case. This allows to construct confidence intervals and an independence test, whose finite sample performance I subsequently examine in a simulation study. Finally, I illustrate the use of the new dependence measure in two applications on the dependence between the variables country and income or country and religion, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00785v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v1 Announce Type: cross 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>Predicting the Price of Gold in the Financial Markets Using Hybrid Models</title>
      <link>https://arxiv.org/abs/2505.01402</link>
      <description>arXiv:2505.01402v1 Announce Type: cross 
Abstract: Predicting the price that has the least error and can provide the best and highest accuracy has been one of the most challenging issues and one of the most critical concerns among capital market activists and researchers. Therefore, a model that can solve problems and provide results with high accuracy is one of the topics of interest among researchers. In this project, using time series prediction models such as ARIMA to estimate the price, variables, and indicators related to technical analysis show the behavior of traders involved in involving psychological factors for the model. By linking all of these variables to stepwise regression, we identify the best variables influencing the prediction of the variable. Finally, we enter the selected variables as inputs to the artificial neural network. In other words, we want to call this whole prediction process the "ARIMA_Stepwise Regression_Neural Network" model and try to predict the price of gold in international financial markets. This approach is expected to be able to be used to predict the types of stocks, commodities, currency pairs, financial market indicators, and other items used in local and international financial markets. Moreover, a comparison between the results of this method and time series methods is also expressed. Finally, based on the results, it can be seen that the resulting hybrid model has the highest accuracy compared to the time series method, regression, and stepwise regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01402v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammadhossein Rashidi, Mohammad Modarres</dc:creator>
    </item>
    <item>
      <title>Quasi-Score Matching Estimation for Spatial Autoregressive Model with Random Weights Matrix and Regressors</title>
      <link>https://arxiv.org/abs/2305.19721</link>
      <description>arXiv:2305.19721v2 Announce Type: replace 
Abstract: With the rapid advancements in technology for data collection, the application of the spatial autoregressive (SAR) model has become increasingly prevalent in real-world analysis, particularly when dealing with large datasets. However, the commonly used quasi-maximum likelihood estimation (QMLE) for the SAR model is not computationally scalable to handle the data with a large size. In addition, when establishing the asymptotic properties of the parameter estimators of the SAR model, both weights matrix and regressors are assumed to be nonstochastic in classical spatial econometrics, which is perhaps not realistic in real applications. Motivated by the machine learning literature, this paper proposes quasi-score matching estimation for the SAR model. This new estimation approach is developed based on the likelihood, but significantly reduces the computational complexity of the QMLE. The asymptotic properties of parameter estimators under the random weights matrix and regressors are established, which provides a new theoretical framework for the asymptotic inference of the SAR-type models. The usefulness of the quasi-score matching estimation and its asymptotic inference is illustrated via extensive simulation studies and a case study of an anti-conflict social network experiment for middle school students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19721v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liang, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Julia as a universal platform for statistical software development</title>
      <link>https://arxiv.org/abs/2404.09309</link>
      <description>arXiv:2404.09309v4 Announce Type: replace 
Abstract: The julia package integrates the Julia programming language into Stata. Users can transfer data between Stata and Julia, issue Julia commands to analyze and plot, and pass results back to Stata. Julia's econometric ecosystem is not as mature as Stata's or R's or Python's. But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. For example, the boottest program for wild bootstrap-based inference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman 2021) can use the same Julia back end. And the program reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects while calling a Julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear fixed-effect models that cannot otherwise be fit in Stata--though preliminarily, as the Julia package for that purpose is immature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09309v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Roodman</dc:creator>
    </item>
    <item>
      <title>Transmission Channel Analysis in Dynamic Models</title>
      <link>https://arxiv.org/abs/2405.18987</link>
      <description>arXiv:2405.18987v3 Announce Type: replace 
Abstract: We propose a framework for analysing transmission channels in a large class of dynamic models. We formulate our approach both using graph theory and potential outcomes, which we show to be equivalent. Our method, labelled Transmission Channel Analysis (TCA), allows for the decomposition of total effects captured by impulse response functions into the effects flowing through transmission channels, thereby providing a quantitative assessment of the strength of various well-defined channels. We establish that this requires no additional identification assumptions beyond the identification of the structural shock whose effects the researcher wants to decompose. Additionally, we prove that impulse response functions are sufficient statistics for the computation of transmission effects. We demonstrate the empirical relevance of TCA for policy evaluation by decomposing the effects of policy shocks arising from a variety of popular macroeconomic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18987v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Wegner, Lenard Lieb, Stephan Smeekes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Cluster-robust jackknife and bootstrap inference for logistic regression models</title>
      <link>https://arxiv.org/abs/2406.00650</link>
      <description>arXiv:2406.00650v2 Announce Type: replace 
Abstract: We study cluster-robust inference for logistic regression (logit) models. Inference based on the most commonly-used cluster-robust variance matrix estimator (CRVE) can be very unreliable. We study several alternatives. Conceptually the simplest of these, but also the most computationally demanding, involves jackknifing at the cluster level. We also propose a linearized version of the cluster-jackknife variance matrix estimator as well as linearized versions of the wild cluster bootstrap. The linearizations are based on empirical scores and are computationally efficient. Our results can readily be generalized to other binary response models. We also discuss a new Stata software package called logitjack which implements these procedures. Simulation results strongly favor the new methods, and two empirical examples suggest that it can be important to use them in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00650v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James G. MacKinnon, Morten {\O}rregaard Nielsen, Matthew D. Webb</dc:creator>
    </item>
    <item>
      <title>Demystifying and avoiding the OLS "weighting problem": Unmodeled heterogeneity and straightforward solutions</title>
      <link>https://arxiv.org/abs/2403.03299</link>
      <description>arXiv:2403.03299v4 Announce Type: replace-cross 
Abstract: Researchers frequently estimate treatment effects by regressing outcomes (Y) on treatment (D) and covariates (X). Even without unobserved confounding, the coefficient on D yields a conditional-variance-weighted average of strata-wise effects, not the average treatment effect. Scholars have proposed characterizing the severity of these weights, evaluating resulting biases, or changing investigators' target estimand to the conditional-variance-weighted effect. We aim to demystify these weights, clarifying how they arise, what they represent, and how to avoid them. Specifically, these weights reflect misspecification bias from unmodeled treatment-effect heterogeneity. Rather than diagnosing or tolerating them, we recommend avoiding the issue altogether, by relaxing the standard regression assumption of "single linearity" to one of "separate linearity" (of each potential outcome in the covariates), accommodating heterogeneity. Numerous methods--including regression imputation (g-computation), interacted regression, and mean balancing weights--satisfy this assumption. In many settings, the efficiency cost to avoiding this weighting problem altogether will be modest and worthwhile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03299v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanvi Shinkre, Chad Hazlett</dc:creator>
    </item>
  </channel>
</rss>
