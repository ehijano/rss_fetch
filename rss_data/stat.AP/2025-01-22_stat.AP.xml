<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Custom Loss Functions in Fuel Moisture Modeling</title>
      <link>https://arxiv.org/abs/2501.10401</link>
      <description>arXiv:2501.10401v1 Announce Type: new 
Abstract: Fuel moisture content (FMC) is a key predictor for wildfire rate of spread (ROS). Machine learning models of FMC are being used more in recent years, augmenting or replacing traditional physics-based approaches. Wildfire rate of spread (ROS) has a highly nonlinear relationship with FMC, where small differences in dry fuels lead to large differences in ROS. In this study, custom loss functions that place more weight on dry fuels were examined with a variety of machine learning models of FMC. The models were evaluated with a spatiotemporal cross-validation procedure to examine whether the custom loss functions led to more accurate forecasts of ROS. Results show that the custom loss functions improved accuracy for ROS forecasts by a small amount. Further research would be needed to establish whether the improvement in ROS forecasts leads to more accurate real-time wildfire simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10401v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathon Hirschi</dc:creator>
    </item>
    <item>
      <title>Do we actually understand the impact of renewables on electricity prices? A causal inference approach</title>
      <link>https://arxiv.org/abs/2501.10423</link>
      <description>arXiv:2501.10423v1 Announce Type: new 
Abstract: The energy transition is profoundly reshaping electricity market dynamics. It makes it essential to understand how renewable energy generation actually impacts electricity prices, among all other market drivers. These insights are critical to design policies and market interventions that ensure affordable, reliable, and sustainable energy systems. However, identifying causal effects from observational data is a major challenge, requiring innovative causal inference approaches that go beyond conventional regression analysis only. We build upon the state of the art by developing and applying a local partially linear double machine learning approach. Its application yields the first robust causal evidence on the distinct and non-linear effects of wind and solar power generation on UK wholesale electricity prices, revealing key insights that have eluded previous analyses. We find that, over 2018-2024, wind power generation has a U-shaped effect on prices: at low penetration levels, a 1 GWh increase in energy generation reduces prices by up to 7 GBP/MWh, but this effect gets close to none at mid-penetration levels (20-30%) before intensifying again. Solar power places substantial downward pressure on prices at very low penetration levels (up to 9 GBP/MWh per 1 GWh increase in energy generation), though its impact weakens quite rapidly. We also uncover a critical trend where the price-reducing effects of both wind and solar power have become more pronounced over time (from 2018 to 2024), highlighting their growing influence on electricity markets amid rising penetration. Our study provides both novel analysis approaches and actionable insights to guide policymakers in appraising the way renewables impact electricity markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10423v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Cacciarelli, Pierre Pinson, Filip Panagiotopoulos, David Dixon, Lizzie Blaxland</dc:creator>
    </item>
    <item>
      <title>A Generalized Benford Framework for Threat Identification in Counter-Intelligence</title>
      <link>https://arxiv.org/abs/2501.10460</link>
      <description>arXiv:2501.10460v1 Announce Type: new 
Abstract: In this paper, we develop a framework of 'Benford models' for counter-intelligence investigations which analyze frequency data of a suspect's visits to physical locations, online websites, and communication channels. We accomplish this by establishing the Benford measure for continuous &amp; bounded domains, generalizing the accumulated percentage differences between sites in the frequency data with the log-determinant of 'Benford Matrices,' employing an estimator to determine a 'Benford Test Statistic,' and identifying maximal values of that test statistic across all permutations of included sites in our data. This framework is intended to complement outlier analysis models by finding where hidden Benford patterns 'break' in frequency data and telling investigators which sites they should investigate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10460v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Tarter (Undergraduate, James Madison University)</dc:creator>
    </item>
    <item>
      <title>Lead Times in Flux: Analyzing Airbnb Booking Dynamics During Global Upheavals (2018-2022)</title>
      <link>https://arxiv.org/abs/2501.10535</link>
      <description>arXiv:2501.10535v1 Announce Type: new 
Abstract: Short-term shifts in booking behaviors can disrupt forecasting in the travel and hospitality industry, especially during global crises. Traditional metrics like average or median lead times often overlook important distribution changes. This study introduces a normalized L1 (Manhattan) distance to assess Airbnb booking lead time divergences from 2018 to 2022, focusing on the COVID-19 pandemic across four major U.S. cities. We identify a two-phase disruption: an abrupt change at the pandemic's onset followed by partial recovery with persistent deviations from pre-2018 patterns. Our method reveals changes in travelers' planning horizons that standard statistics miss, highlighting the need to analyze the entire lead-time distribution for more accurate demand forecasting and pricing strategies. The normalized L1 metric provides valuable insights for tourism stakeholders navigating ongoing market volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10535v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Erica Savage, Peter Coles</dc:creator>
    </item>
    <item>
      <title>The Epistemic Value of Novel Predictive Success in Scientific and Criminal Investigations: A Bayesian Explanation</title>
      <link>https://arxiv.org/abs/2501.11104</link>
      <description>arXiv:2501.11104v1 Announce Type: new 
Abstract: Because there are similarities between the evaluation of alternative stories in criminal trials and the evaluation of scientific theories, scholars have looked to literature in epistemology and the philosophy of science for insights on the evaluation of evidence in criminal trials. The philosophical literature is divided, however, on a key point -- the epistemic value of novel predictive success. This article uses a Bayesian network analysis to explore, in the context of a criminal case, the circumstances in which "new evidence" discovered after a theory is propounded, can provide stronger (or weaker) support for the theory than "old evidence" that was accommodated by the theory. It argues that insights from analysis of the strength of "new" and "old" evidence in the criminal case can be applied more generally when assessing the relative merits of prediction and accommodation in scientific theory development, and are thus helpful in addressing the longstanding philosophic controversy over this issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11104v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Mortera, William C. Thompson</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal characterisation of underwater noise through semantic trajectories</title>
      <link>https://arxiv.org/abs/2501.11131</link>
      <description>arXiv:2501.11131v1 Announce Type: new 
Abstract: Underwater noise pollution from human activities, particularly shipping, has been recognised as a serious threat to marine life. The sound generated by vessels can have various adverse effects on fish and aquatic ecosystems in general. In this setting, the estimation and analysis of the underwater noise produced by vessels is an important challenge for the preservation of the marine environment. In this paper we propose a model for the spatio-temporal characterisation of the underwater noise generated by vessels. The approach is based on the reconstruction of the vessels' trajectories from Automatic Identification System (AIS) data and on their deployment in a spatio-temporal database. Trajectories are enriched with semantic information like the acoustic characteristics of the vessels' engines or the activity performed by the vessels. We define a model for underwater noise propagation and use the trajectories' information to infer how noise propagates in the area of interest. We develop our approach for the case study of the fishery activities in the Northern Adriatic sea, an area of the Mediterranean sea which is well known to be highly exploited. We implement our approach using MobilityDB, an open source geospatial trajectory data management and analysis platform, which offers spatio-temporal operators and indexes improving the efficiency of our system. We use this platform to conduct various analyses of the underwater noise generated in the Northern Adriatic Sea, aiming at estimating the impact of fishing activities on underwater noise pollution and at demonstrating the flexibility and expressiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11131v1</guid>
      <category>stat.AP</category>
      <category>cs.DB</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Rovinelli, Davide Rocchesso, Marta Simeoni, Esteban Zim\'anyi, Alessandra Raffaet\`a</dc:creator>
    </item>
    <item>
      <title>High-dimensional point forecast combinations for emergency department demand</title>
      <link>https://arxiv.org/abs/2501.11315</link>
      <description>arXiv:2501.11315v1 Announce Type: new 
Abstract: Current work on forecasting emergency department (ED) admissions focuses on disease aggregates or singular disease types. However, given differences in the dynamics of individual diseases, it is unlikely that any single forecasting model would accurately account for each disease and for all time, leading to significant forecast model uncertainty. Yet, forecasting models for ED admissions to-date do not explore the utility of forecast combinations to improve forecast accuracy and stability. It is also unknown whether improvements in forecast accuracy can be yield from (1) incorporating a large number of environmental and anthropogenic covariates or (2) forecasting total ED causes by aggregating cause-specific ED forecasts. To address this gap, we propose high-dimensional forecast combination schemes to combine a large number of forecasting individual models for forecasting cause-specific ED admissions over multiple causes and forecast horizons. We use time series data of ED admissions with an extensive set of explanatory lagged variables at the national level, including meteorological/ambient air pollutant variables and ED admissions of all 16 causes studied. We show that the simple forecast combinations yield forecast accuracies of around 3.81%-23.54% across causes. Furthermore, forecast combinations outperform individual forecasting models, in more than 50% of scenarios (across all ED admission categories and horizons) in a statistically significant manner. Inclusion of high-dimensional covariates and aggregating cause-specific forecasts to provide all-cause ED forecasts provided modest improvements in forecast accuracy. Forecasting cause-specific ED admissions can provide fine-scale forward guidance on resource optimization and pandemic preparedness and forecast combinations can be used to hedge against model uncertainty when forecasting across a wide range of admission categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11315v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peihong Guo, Wen Ye Loh, Kenwin Maung, Esther Li Wen Choo, Borame Lee Dickens, Kelvin Bryan Tan, John Abishgenadan, Pei Ma, Jue Tao Lim</dc:creator>
    </item>
    <item>
      <title>Devising PoPStat: A Metric Bridging Population Pyramids with Global Disease Mortality</title>
      <link>https://arxiv.org/abs/2501.11514</link>
      <description>arXiv:2501.11514v1 Announce Type: new 
Abstract: Understanding the relationship between population dynamics and disease-specific mortality is central to evidence-based health policy. This study introduces two novel metrics, PoPDivergence and PoPStat, one to quantify the difference between population pyramids and the other to assess the strength and nature of their association with the mortality of a given disease. PoPDivergence, based on Kullback-Leibler divergence, measures deviations between a countrys population pyramid and a reference pyramid. PoPStat is the correlation between these deviations and the log form of disease-specific mortality rates. The reference population is selected by a brute-force optimization that maximizes this correlation. Utilizing mortality data from the Global Burden of Disease 2021 and population statistics from the United Nations, we applied these metrics to 371 diseases across 204 countries. Results reveal that PoPStat outperforms traditional indicators such as median age, GDP per capita, and Human Development Index in explaining the mortality of most diseases. Noncommunicable diseases (NCDs) like neurological disorders and cancers, communicable diseases (CDs) like neglected tropical diseases, and maternal and neonatal diseases were tightly bound to the underlying demographic attributes whereas NCDs like diabetes, CDs like respiratory infections and injuries including self-harm and interpersonal violence were weakly associated with population pyramid shapes. Notably, except for diabetes, the NCD mortality burden was shared by constrictive population pyramids, while mortality of communicable diseases, maternal and neonatal causes and injuries were largely borne by expansive pyramids. Therefore, PoPStat provides insights into demographic determinants of health and empirical support for models on epidemiological transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11514v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tharaka Fonseka, Buddhi Wijenayake, Athulya Ratnayake, Inosha Alwis, Supun Manathunga, Roshan Godaliyadda, Vijitha Herath, Isuru Pamuditha, Parakrama Ekanayake, Samath Dharmarathne</dc:creator>
    </item>
    <item>
      <title>Changes over time in the 100-year return value of climate model variables</title>
      <link>https://arxiv.org/abs/2501.11650</link>
      <description>arXiv:2501.11650v1 Announce Type: new 
Abstract: We assess evidence for changes in tail characteristics of wind, solar irradiance and temperature variables output from CMIP6 global climate models (GCMs) due to climate forcing. We estimate global and climate zone annual maximum and annual means for period (2015, 2100) from daily output of seven GCMs for daily wind speed, maximum wind speed, solar irradiance and near-surface temperature. We calculate corresponding annualised data for individual locations within neighbourhoods of the North Atlantic and Celtic Sea region. We consider output for three climate scenarios and multiple climate ensembles. We estimate non-stationary extreme value models for annual extremes, and non-homogeneous Gaussian regressions for annual means, using Bayesian inference. We use estimated statistical models to quantify the distribution of (i) the change in 100-year return value for annual extremes, and (2) the change in annual mean, over the period (2025, 2125). To summarise results, we estimate linear mixed effects models for observed variation of (i) and (ii). Evidence for changes in the 100-year return value for annual maxima of solar irradiance and temperature is much stronger than for wind variables over time and with climate scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11650v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Callum Leach, Kevin Ewans, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>A Bayesian Learning Model for Joint Risk Prediction of Alcohol and Cannabis Use Disorders</title>
      <link>https://arxiv.org/abs/2501.12278</link>
      <description>arXiv:2501.12278v1 Announce Type: new 
Abstract: Substance use disorders (SUDs) are a serious public health concern in the United States. Alcohol and cannabis are two of the most widely used substances. For adolescent/youth users of alcohol or cannabis, we propose a joint Bayesian learning model to predict their risks of developing alcohol use disorder (AUD) and cannabis use disorder (CUD) in adulthood based on their personal risk factors. The model is trained on nationally representative longitudinal data from Add Health (n = 12503). It consists of sub-models that predict the two SUDs for three groups of users-those who use alcohol only, cannabis only, and both substances - based on shared as well as unique risk factors. The model comprises of ten predictors. We externally validate the model on two independent datasets. The areas under the receiver operating characteristic curves for AUD and CUD, respectively, are: (a) 0.719 and 0.690 based on 5-fold cross-validation, (b) 0.748 and 0.710 based on validation dataset 1, and (c) 0.650 and 0.750 based on validation dataset 2. A simulation study shows that the proposed joint modeling approach generally performs better than separate univariate modeling of the corresponding dependent outcomes in terms of predictive accuracy. Our model may help in identifying adolescent substance users at high risk of developing SUD in adulthood, who can then be helped with appropriate intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12278v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajapaksha Mudalige Dhanushka S. Rajapaksha, Tingfang Wang, Thanthirige Lakshika M. Ruberu, Joseph M. Boden, Pankaj K. Choudhary, Swati Biswas</dc:creator>
    </item>
    <item>
      <title>Recovering Unobserved Network Links from Aggregated Relational Data: Discussions on Bayesian Latent Surface Modeling and Penalized Regression</title>
      <link>https://arxiv.org/abs/2501.10675</link>
      <description>arXiv:2501.10675v1 Announce Type: cross 
Abstract: Accurate network data are essential in fields such as economics, sociology, and computer science. Aggregated Relational Data (ARD) provides a way to capture network structures using partial data. This article compares two main frameworks for recovering network links from ARD: Bayesian Latent Surface Modeling (BLSM) and Frequentist Penalized Regression (FPR). Using simulation studies and real-world applications, we evaluate their theoretical properties, computational efficiency, and practical utility in domains like financial risk assessment and epidemiology. Key findings emphasize the importance of trait design, privacy considerations, and hybrid modeling approaches to improve scalability and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10675v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-hsuan Tseng</dc:creator>
    </item>
    <item>
      <title>Estimation of Linear models from Coarsened Observations Estimation of Linear models Estimation from Coarsened Observations A Method of Moments Approach</title>
      <link>https://arxiv.org/abs/2501.10726</link>
      <description>arXiv:2501.10726v1 Announce Type: cross 
Abstract: In the last few decades, the study of ordinal data in which the variable of interest is not exactly observed but only known to be in a specific ordinal category has become important. In Psychometrics such variables are analysed under the heading of item response models (IRM). In Econometrics, subjective well-being (SWB) and self-assessed health (SAH) studies, and in marketing research, Ordered Probit, Ordered Logit, and Interval Regression models are common research platforms. To emphasize that the problem is not specific to a specific discipline we will use the neutral term coarsened observation. For single-equation models estimation of the latent linear model by Maximum Likelihood (ML) is routine. But, for higher -dimensional multivariate models it is computationally cumbersome as estimation requires the evaluation of multivariate normal distribution functions on a large scale. Our proposed alternative estimation method, based on the Generalized Method of Moments (GMM), circumvents this multivariate integration problem. The method is based on the assumed zero correlations between explanatory variables and generalized residuals. This is more general than ML but coincides with ML if the error distribution is multivariate normal. It can be implemented by repeated application of standard techniques. GMM provides a simpler and faster approach than the usual ML approach. It is applicable to multiple -equation models with -dimensional error correlation matrices and response categories for the equation. It also yields a simple method to estimate polyserial and polychoric correlations. Comparison of our method with the outcomes of the Stata ML procedure cmp yields estimates that are not statistically different, while estimation by our method requires only a fraction of the computing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10726v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bernard M. S. van Praag, J. Peter Hop, William H. Greene</dc:creator>
    </item>
    <item>
      <title>Penalized generalized linear mixed models for longitudinal outcomes in genetic association studies</title>
      <link>https://arxiv.org/abs/2501.11083</link>
      <description>arXiv:2501.11083v1 Announce Type: cross 
Abstract: This work is motivated by analyses of longitudinal data collected from participants in the Quebec Longitudinal Study of Child Development (QLSCD) and the Quebec Newborn Twin Study (QNTS) to identify important genetic predictors for emotional and behavioral difficulties in childhood and adolescence. We propose a lasso penalized mixed model for continuous and binary longitudinal traits that allows the inclusion of multiple random effects to account for random individual effects not attributable to the genetic similarity between individuals. Through simulation studies, we show that replacing the estimated genetic relatedness matrix (GRM) by a sparse matrix introduces bias in the variance components estimates, but that the obtained computational gain is major while the impact on the performance of the penalized model to retrieve important predictors is negligible. We compare the performance of the proposed penalized mixed model to a standard lasso and to a univariate mixed model association test and show that the proposed model always identifies causal predictors with greater precision. Finally, we show an application of the proposed methodology to predict three externalizing behavorial scores in the combined QLSCD and QNTS longitudinal cohorts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11083v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien St-Pierre, Sahir Rai Bhatnagar, Massimiliano Orri, Michel Boivin, Jos\'ee Dupuis, Karim Oualkacha</dc:creator>
    </item>
    <item>
      <title>B-Call: Integrating Ideological Position and Political Cohesion in Legislative Voting Models</title>
      <link>https://arxiv.org/abs/2501.11084</link>
      <description>arXiv:2501.11084v1 Announce Type: cross 
Abstract: This paper combines two significant areas of political science research: measuring individual ideological position and cohesion. Although both approaches help analyze legislative behaviors, no unified model currently integrates these dimensions. To fill this gap, the paper proposes a methodology called B-Call that combines ideological positioning with voting cohesion, treating votes as random variables. The model is empirically validated using roll-call data from the United States, Brazil, and Chile legislatures, which represent diverse legislative dynamics. The analysis aims to capture the complexities of voting and legislative behaviors, resulting in a two-dimensional indicator. This study addresses gaps in current legislative voting models, particularly in contexts with limited party control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11084v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Reutter, Sergio Toro, Lucas Valenzuela, Daniel Alcatruz, Macarena Valenzuela</dc:creator>
    </item>
    <item>
      <title>The Expected Peak-to-Average Power Ratio of White Gaussian Noise in Sampled I/Q Data</title>
      <link>https://arxiv.org/abs/2501.11261</link>
      <description>arXiv:2501.11261v1 Announce Type: cross 
Abstract: One of the fundamental endeavors in radio frequency (RF) metrology is to measure the power of signals, where a common aim is to estimate the peak-to-average power ratio (PAPR), which quantifies the ratio of the maximum (peak) to the mean value. For a finite number of discrete-time samples of baseband in-phase and quadrature (I/Q) white Gaussian noise (WGN) that are independent and identically distributed with zero mean, we derive a closed-form, exact formula for mean PAPR that is well-approximated by the natural logarithm of the number of samples plus Euler's constant. Additionally, we give related theoretical results for the mean crest factor. After comparing our main result to previously published approximate formulas, we examine how violations of the WGN assumptions in sampled I/Q data result in deviations from the expected value of PAPR. Finally, utilizing a measured RF I/Q acquisition, we illustrate how our formula for mean PAPR can be applied to spectral analysis with spectrograms to verify when measured RF emissions are WGN in a given frequency band.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11261v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Wunderlich, Aric Sanders</dc:creator>
    </item>
    <item>
      <title>Second-order Asymptotic Analysis of Tail Probabilities of Randomly Weighted Sums: With Applications to a Bidimensional Discrete-time Risk Model</title>
      <link>https://arxiv.org/abs/2501.11573</link>
      <description>arXiv:2501.11573v1 Announce Type: cross 
Abstract: Motivated by a bidimensional discrete-time risk model in insurance, we study the second-order asymptotics for two kinds of tail probabilities of the stochastic discounted value of aggregate net losses including two business lines. These are essentially modeled as randomly weighted sums, in which it is assumed that the primary random variables form a sequence of real-valued, independent and identically distributed random pairs following a common bivariate Farlie-Gumbel-Morgenstern distribution and the random weights are bounded, nonnegative and arbitrarily dependent, but independent of the primary random variables. Under the assumption that two marginal distributions of the primary random variables are second-order subexponential, we first obtain the second-order asymptotics for the joint and sum tail probabilities, which generalizes and strengthens some known ones in the literature. Furthermore, by directly applying the obtained results to the above bidimensional risk model, we establish the second-order asymptotic formulas for the corresponding tail probabilities. Compared with the first-order one, our numerical simulation shows that second-order asymptotics are much more precise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11573v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingzhen Geng, Yang Liu, Shijie Wang</dc:creator>
    </item>
    <item>
      <title>Loss of earning capacity in Denmark -- an actuarial perspective</title>
      <link>https://arxiv.org/abs/2501.11578</link>
      <description>arXiv:2501.11578v1 Announce Type: cross 
Abstract: We describe challenges and opportunities related to risk assessment and mitigation for loss of earning capacity insurance with a special focus on Denmark. The presence of public benefits, claim settlement processes, and prevention initiatives introduces significant intricacy to the risk landscape. Accommodating this requires the development of innovative approaches from researchers and practitioners alike. Actuaries are uniquely positioned to lead the way, leveraging their domain knowledge and mathematical-statistical expertise to develop equitable, data-driven solutions that mitigate risk and enhance societal well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11578v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. Furrer, O. L. Sandqvist</dc:creator>
    </item>
    <item>
      <title>Risk-Adjusted learning curve assessment using comparative probability metrics</title>
      <link>https://arxiv.org/abs/2501.11637</link>
      <description>arXiv:2501.11637v1 Announce Type: cross 
Abstract: Surgical learning curves are graphical tools used to evaluate a trainee's progress in the early stages of their career and determine whether they have achieved proficiency after completing a specified number of surgeries. Cumulative sum (CUSUM) techniques are commonly used to assess learning curves due to their simplicity, but they face criticism for relying on fixed performance thresholds and lacking interpretability. This paper introduces a risk-adjusted surgical learning curve assessment (SLCA) method that focuses on estimation rather than hypothesis testing, as seen in CUSUM methods. The method is designed to accommodate right-skewed outcomes, such as surgery durations, characterized by the Weibull distribution. To evaluate the learning process, the SLCA approach estimates comparative probability metrics that assess the likelihood of a clinically important difference between the trainee's performance and a standard. Expecting improvement over time, we use weighted estimating equations to give greater weight to recent outcomes. Compared to CUSUM methods, SLCA offers enhanced interpretability, avoids reliance on externally defined performance levels, and emphasizes assessing clinical equivalence or noninferiority. We demonstrate the method's effectiveness through a colorectal surgery dataset case study and a numerical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11637v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Ahmadi Nadi, Stefan Steiner, Nathaniel Stevens</dc:creator>
    </item>
    <item>
      <title>Investigating the performance of the Phase II Hotelling T2 chart when monitoring multivariate time series observations</title>
      <link>https://arxiv.org/abs/2501.11649</link>
      <description>arXiv:2501.11649v1 Announce Type: cross 
Abstract: Thanks to high-tech measurement systems like sensors, data are often collected with high frequency in modern industrial processes. This phenomenon could potentially produce autocorrelated and cross-correlated measurements. It has been shown that if this issue is not properly accounted for while designing the control charts, many false alarms may be observed, disrupting the monitoring process efficiency. There are generally two recommended ways to monitor autocorrelated data: fitting a time series model and then monitoring the residuals or directly monitoring the original observations. Although residual charts are popular in the literature because they offer advantages such as ease of implementation, questions have been raised about their efficiency due to the loss of information. This paper develops the methodology for applying Hotelling's T2 chart directly to monitoring multivariate autocorrelated and cross-correlated observations. To model such data, we use the multivariate vector autoregressive time series model of order p &gt;= 1, denoted as VAR(p). We compare the performance of the T2 chart based on the original observations and the residual-based T2 chart using the well-known metric average run length and a newly introduced criterion called first-to-signal. The results indicate that the proposed method consistently outperforms the alternative chart. We also illustrate the proposed method using two examples: one from a steel sheet rolling process (VAR(1) observations) and another from a chemical process (VAR(3) observations).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11649v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Ahmadi Nadi, Giovanni Celano, Stefan Steiner</dc:creator>
    </item>
    <item>
      <title>Hypergraph Representations of scRNA-seq Data for Improved Clustering with Random Walks</title>
      <link>https://arxiv.org/abs/2501.11760</link>
      <description>arXiv:2501.11760v1 Announce Type: cross 
Abstract: Analysis of single-cell RNA sequencing data is often conducted through network projections such as coexpression networks, primarily due to the abundant availability of network analysis tools for downstream tasks. However, this approach has several limitations: loss of higher-order information, inefficient data representation caused by converting a sparse dataset to a fully connected network, and overestimation of coexpression due to zero-inflation. To address these limitations, we propose conceptualizing scRNA-seq expression data as hypergraphs, which are generalized graphs in which the hyperedges can connect more than two vertices. In the context of scRNA-seq data, the hypergraph nodes represent cells and the edges represent genes. Each hyperedge connects all cells where its corresponding gene is actively expressed and records the expression of the gene across different cells. This hypergraph conceptualization enables us to explore multi-way relationships beyond the pairwise interactions in coexpression networks without loss of information. We propose two novel clustering methods: (1) the Dual-Importance Preference Hypergraph Walk (DIPHW) and (2) the Coexpression and Memory-Integrated Dual-Importance Preference Hypergraph Walk (CoMem-DIPHW). They outperform established methods on both simulated and real scRNA-seq datasets. The improvement brought by our proposed methods is especially significant when data modularity is weak. Furthermore, CoMem-DIPHW incorporates the gene coexpression network, cell coexpression network, and the cell-gene expression hypergraph from the single-cell abundance counts data altogether for embedding computation. This approach accounts for both the local level information from single-cell level gene expression and the global level information from the pairwise similarity in the two coexpression networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11760v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan He, Daniel I. Bolnick, Samuel V. Scarpino, Tina Eliassi-Rad</dc:creator>
    </item>
    <item>
      <title>Dynamic Risk-Adjusted Monitoring of Time Between Events: Applications of NHPP in Pipeline Accident Surveillance</title>
      <link>https://arxiv.org/abs/2501.11809</link>
      <description>arXiv:2501.11809v1 Announce Type: cross 
Abstract: Monitoring time between events (TBE) is a critical task in industrial settings. Traditional Statistical Process Monitoring (SPM) methods often assume that TBE variables follow an exponential distribution, which implies a constant failure intensity. While this assumption may hold for products with homogeneous quality, it is less appropriate for complex systems, such as repairable systems, where failure mechanisms evolve over time due to degradation or aging. In such cases, the Non-Homogeneous Poisson Process (NHPP), which accommodates time-varying failure intensity, is a more suitable model. Furthermore, failure patterns in complex systems are frequently influenced by risk factors, including environmental conditions and human interventions, and system failures often incur restoration costs. This work introduces a novel approach: a risk-adjusted control chart based on the NHPP model, specifically designed to monitor the ratio of cost to TBE, referred to as the average cost per time unit (AC). The proposed method is evaluated through extensive simulations, demonstrating its superior performance. Additionally, the chart is applied to monitor pipeline accidents over time, accounting for the impact of various risk factors. These results highlight the effectiveness of the developed chart in enhancing monitoring capabilities for complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11809v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hussam Ahmad, Adel Ahmadi Nadi, Mohammad Amini, Subhabrata Chakraborti</dc:creator>
    </item>
    <item>
      <title>Bayesian Despeckling of Structured Sources</title>
      <link>https://arxiv.org/abs/2501.11860</link>
      <description>arXiv:2501.11860v1 Announce Type: cross 
Abstract: Speckle noise is a fundamental challenge in coherent imaging systems, significantly degrading image quality. Over the past decades, numerous despeckling algorithms have been developed for applications such as Synthetic Aperture Radar (SAR) and digital holography. In this paper, we aim to establish a theoretically grounded approach to despeckling. We propose a method applicable to general structured stationary stochastic sources. We demonstrate the effectiveness of the proposed method on piecewise constant sources. Additionally, we theoretically derive a lower bound on the despeckling performance for such sources. The proposed depseckler applied to the 1-Markov structured sources achieves better reconstruction performance with no strong simplification of the ground truth signal model or speckle noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11860v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Zafari, Shirin Jalali</dc:creator>
    </item>
    <item>
      <title>Saturation in Snapshot Compressive Imaging</title>
      <link>https://arxiv.org/abs/2501.11869</link>
      <description>arXiv:2501.11869v1 Announce Type: cross 
Abstract: Snapshot Compressive Imaging (SCI) maps three-dimensional (3D) data cubes, such as videos or hyperspectral images, into two-dimensional (2D) measurements via optical modulation, enabling efficient data acquisition and reconstruction. Recent advances have shown the potential of mask optimization to enhance SCI performance, but most studies overlook nonlinear distortions caused by saturation in practical systems. Saturation occurs when high-intensity measurements exceed the sensor's dynamic range, leading to information loss that standard reconstruction algorithms cannot fully recover. This paper addresses the challenge of optimizing binary masks in SCI under saturation. We theoretically characterize the performance of compression-based SCI recovery in the presence of saturation and leverage these insights to optimize masks for such conditions. Our analysis reveals trade-offs between mask statistics and reconstruction quality in saturated systems. Experimental results using a Plug-and-Play (PnP) style network validate the theory, demonstrating improved recovery performance and robustness to saturation with our optimized binary masks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11869v1</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyu Zhao, Shirin Jalali</dc:creator>
    </item>
    <item>
      <title>Opinion dynamics in bounded confidence models with manipulative agents: Moving the Overton window</title>
      <link>https://arxiv.org/abs/2501.12198</link>
      <description>arXiv:2501.12198v1 Announce Type: cross 
Abstract: This paper focuses on the opinion dynamics under the influence of manipulative agents. This type of agents is characterized by the fact that their opinions follow a trajectory that does not respond to the dynamics of the model, although it does influence the rest of the normal agents. Simulation has been implemented to study how one manipulative group modifies the natural dynamics of some opinion models of bounded confidence. It is studied what strategies based on the number of manipulative agents and their common opinion trajectory can be carried out by a manipulative group to influence normal agents and attract them to their opinions. In certain weighted models, some effects are observed in which normal agents move in the opposite direction to the manipulator group. Moreover, the conditions which ensure the influence of a manipulative group on a group of normal agents over time are also established for the Hegselmann-Krause model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12198v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Bautista</dc:creator>
    </item>
    <item>
      <title>Embracing Uncertainty in "Small Data" Problems: Estimating Earthquakes from Historical Anecdotes</title>
      <link>https://arxiv.org/abs/2106.07797</link>
      <description>arXiv:2106.07797v3 Announce Type: replace 
Abstract: Seismic risk estimates will be vastly improved with an increased understanding of historical (and pre-historical) seismic events. However the only existing data for these events is anecdotal and sparse. To address this we developed a framework based on Bayesian inference to estimate the location and magnitude of pre-instrumental earthquakes. We present a careful analysis of results obtained from this procedure which justifies the sampling algorithm, its convergence to the resultant posterior distribution, and yields estimates on uncertainties in the relevant quantities. Using a priori estimates on the posterior and numerical approximations of the Hessian, we demonstrate that the 1852 Banda Sea earthquake and tsunami is indeed well-understood given certain explicit hypotheses. Using the same techniques we also find that the 1820 south Sulawesi event may best be explained by a dual fault rupture, best attributed to the Kalatoa fault potentially conjoining the Flores thrust and Walanae/Selayar fault.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.07797v3</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan E. Glatt-Holtz, Ronald A. Harris, Andrew J. Holbrook, Justin A. Krometis, Yonatan Kurniawan, Hayden Ringer, Jared P. Whitehead</dc:creator>
    </item>
    <item>
      <title>A multistate approach to disability insurance reserving with information delays</title>
      <link>https://arxiv.org/abs/2312.14324</link>
      <description>arXiv:2312.14324v2 Announce Type: replace 
Abstract: Disability insurance claims are often affected by lengthy reporting delays and adjudication processes. The classic multistate life insurance modeling framework is ill-suited to handle such information delays since the cash flow and available information can no longer be based on the biometric multistate process determining the contractual payments. We propose a new individual reserving model for disability insurance schemes which describes the claim evolution in real-time. Under suitable independence assumptions between the available information and the underlying biometric multistate process, we show that these new reserves may be calculated as natural modifications of the classic reserves. We propose suitable parametric estimators for the model constituents and a real data application shows the practical relevance of our concepts and results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14324v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Lunding Sandqvist</dc:creator>
    </item>
    <item>
      <title>Combining straight-line and map-based distances to investigate the connection between proximity to healthy foods and disease</title>
      <link>https://arxiv.org/abs/2405.16385</link>
      <description>arXiv:2405.16385v2 Announce Type: replace 
Abstract: Healthy foods are essential for a healthy life, but accessing healthy food can be more challenging for some people than others. This disparity in food access may lead to disparities in well-being, potentially with disproportionate rates of diseases in communities that face more challenges in accessing healthy food (i.e., low-access communities). Identifying low-access, high-risk communities for targeted interventions is a public health priority, but current methods to quantify food access rely on distance measures that are either computationally simple (like the length of the shortest straight-line route) or accurate (like the length of the shortest map-based driving route), but not both. We propose a multiple imputation approach to combine these distance measures, allowing researchers to harness the computational ease of one with the accuracy of the other. The approach incorporates straight-line distances for all neighborhoods and map-based distances for just a subset, offering comparable estimates to the "gold standard" model using map-based distances for all neighborhoods and improved efficiency over the "complete case" model using map-based distances for just the subset. Through the adoption of a measurement error framework, information from the straight-line distances can be leveraged to compute informative placeholders (i.e., impute) for any neighborhoods without map-based distances. Using simulations and data for the Piedmont Triad region of North Carolina, we quantify and compare the associations between two health outcomes (diabetes and obesity) and neighborhood-level access to healthy foods. The imputation procedure also makes it possible to predict the full landscape of food access in an area without requiring map-based measurements for all neighborhoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16385v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Ashley E. Mullan, Lucy D'Agostino McGowan, Staci A. Hepler</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference General Procedures for A Single-subject Test Study</title>
      <link>https://arxiv.org/abs/2408.15419</link>
      <description>arXiv:2408.15419v4 Announce Type: replace 
Abstract: Abnormality detection in identifying a single-subject which deviates from the majority of a control group dataset is a fundamental problem. Typically, the control group is characterised using standard Normal statistics, and the detection of a single abnormal subject is in that context. However, in many situations, the control group cannot be described by Normal statistics, making standard statistical methods inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST) designed to mitigate the effects of skewness under the assumption that the dataset of the control group comes from the skewed Student \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in accuracy. BIGPAST can reduce model misspecification errors under the skewed Student \( t \) assumption. We apply BIGPAST to a Magnetoencephalography (MEG) dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in a single-subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15419v4</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>Reconstructing East Asian Temperatures from 1368 to 1911 Using Historical Documents, Climate Models, and Data Assimilation</title>
      <link>https://arxiv.org/abs/2410.21790</link>
      <description>arXiv:2410.21790v2 Announce Type: replace 
Abstract: We propose a novel approach for reconstructing annual temperatures in East Asia from 1368 to 1911, leveraging the Reconstructed East Asian Climate Historical Encoded Series (REACHES). The lack of instrumental data during this period poses significant challenges to understanding past climate conditions. REACHES digitizes historical documents from the Ming and Qing dynasties of China, converting qualitative descriptions into a four-level ordinal temperature scale. However, these index-based data are biased toward abnormal or extreme weather phenomena, leading to data gaps that likely correspond to normal conditions. To address this bias and reconstruct historical temperatures at any point within East Asia, including locations without direct historical data, we employ a three-tiered statistical framework. First, we perform kriging to interpolate temperature data across East Asia, adopting a zero-mean assumption to handle missing information. Next, we utilize the Last Millennium Ensemble (LME) reanalysis data and apply quantile mapping to calibrate the kriged REACHES data to Celsius temperature scales. Finally, we introduce a novel Bayesian data assimilation method that integrates the kriged Celsius data with LME simulations to enhance reconstruction accuracy. We model the LME data at each geographic location using a flexible nonstationary autoregressive time series model and employ regularized maximum likelihood estimation with a fused lasso penalty. The resulting dynamic distribution serves as a prior, which is refined via Kalman filtering by incorporating the kriged Celsius REACHES data to yield posterior temperature estimates. This comprehensive integration of historical documentation, contemporary climate models, and advanced statistical methods improves the accuracy of historical temperature reconstructions and provides a crucial resource for future environmental and climate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21790v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Sun, Kuan-hui Elaine Lin, Wan-Ling Tseng, Pao K. Wang, Hsin-Cheng Huang</dc:creator>
    </item>
    <item>
      <title>Reliability Acceptance Sampling Plans under Progressive Type-I Interval Censoring Schemes in Presence of Dependent Competing Risks</title>
      <link>https://arxiv.org/abs/2411.01324</link>
      <description>arXiv:2411.01324v2 Announce Type: replace 
Abstract: We discuss the development of reliability acceptance sampling plans under progressive Type-I interval censoring schemes in the presence of competing causes of failure. We consider a general framework to accommodate the presence of independent or dependent competing risks and derive the expression for the Fisher information matrix under this framework. We also discuss the asymptotic properties of the maximum likelihood estimators, which are essential in obtaining the sampling plans. Subsequently, we specialize in a frailty model, which allows us to accommodate the dependence among the potential causes of failure. The frailty model provides an independent competing risks model as a limiting case. We then present the traditional sampling plans for both independent and dependent competing risks models using producer and consumer risks. We also consider the design of optimal PIC-I schemes in this context and use a c optimal design criterion, which helps us to obtain more useful reliability acceptance sampling plans in the presence of budgetary constraints. We conduct a comprehensive numerical experiment to examine the impact of the level of dependence among the potential failure times on the resulting sampling plans. We demonstrate an application of the developed methodology using a real-life example and perform a simulation study to study the finite sample properties of the developed sampling plans. The methodology developed in this article has the potential to improve the design of optimal censoring schemes in the presence of competing risks while taking into account budgetary constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01324v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rathin Das, Soumya Roy, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>Modeling Alzheimer's Disease: Bayesian Copula Graphical Model from Demographic, Cognitive, and Neuroimaging Data</title>
      <link>https://arxiv.org/abs/2411.07745</link>
      <description>arXiv:2411.07745v2 Announce Type: replace 
Abstract: The early detection of Alzheimer's disease (AD) requires an understanding of the relationships between a wide range of features. Conditional independencies and partial correlations are suitable measures for these relationships, because they can identify the effects of confounding and mediating variables. This article presents a Bayesian approach to Gaussian copula graphical models (GCGMs) in order to estimate these conditional dependencies and partial correlations. This approach has two key advantages. First, it includes binary, discrete, and continuous variables. Second, it quantifies the uncertainty of the estimates. Despite these advantages, Bayesian GCGMs have not been applied to AD research yet. In this study, we design a GCGM to find the conditional dependencies and partial correlations among brain-region specific gray matter volume and glucose uptake, amyloid-beta levels, demographic information, and cognitive test scores. We applied our model to 1022 participants, including healthy and cognitively impaired, across different stages of AD. We found that aging reduces cognition through three indirect pathways: hippocampal volume loss, posterior cingulate cortex (PCC) volume loss, and amyloid-beta accumulation. We found a positive partial correlation between being woman and cognition, but also discovered four indirect pathways that dampen this association in women: lower hippocampal volume, lower PCC volume, more amyloid-beta accumulation, and less education. We found limited relations between brain-region specific glucose uptake and cognition, but discovered that the hippocampus and PCC volumes are related to cognition. These results show that the use of GCGMs offers valuable insights into AD pathogenesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07745v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lucas Vogels, Reza Mohammadi, Marit Schoonhoven, S. Ilker Birbil, Martin Dyrba</dc:creator>
    </item>
    <item>
      <title>Statistical Issues in the Diagnosis of Shaken Baby Syndrome/Abusive Head Trauma</title>
      <link>https://arxiv.org/abs/2412.10648</link>
      <description>arXiv:2412.10648v2 Announce Type: replace 
Abstract: The diagnosis of Shaken Baby Syndrome/Abusive Head Trauma (SBS/AHT) is fraught with controversy due to critical statistical deficiencies in the data underpinning these diagnoses. This paper examines the reliability and scientific foundation of SBS/AHT through a statistical lens, highlighting the lack of independently verified ground truth, contextual biases, data circularity, and diagnostic heterogeneity. These issues render current methodologies inadequate and complicate evaluations of diagnostic accuracy, particularly when legal determinations are integrated into medical assessments. Without empirical evidence validating the specificity of symptoms like subdural hematoma, retinal hemorrhage, and brain swelling, the diagnosis remains untested and its foundational validity unproven. We recommend that physicians focus on reporting observed clinical signs and avoid making determinations of abuse, which should remain within the legal domain. Addressing these challenges requires comprehensive, high-quality data collection encompassing contextual, medical, and legal information to evaluate the accuracy, repeatability, and reproducibility of SBS/AHT diagnoses. These efforts are essential to protect vulnerable children while ensuring fairness and accuracy in legal proceedings involving allegations of abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10648v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar</dc:creator>
    </item>
    <item>
      <title>Reconstructing community dynamics from limited observations</title>
      <link>https://arxiv.org/abs/2501.03820</link>
      <description>arXiv:2501.03820v2 Announce Type: replace 
Abstract: Ecosystems tend to fluctuate around stable equilibria in response to internal dynamics and environmental factors. Occasionally, they enter an unstable tipping region and collapse into an alternative stable state. Our understanding of how ecological communities vary over time and respond to perturbations depends on our ability to quantify and predict these dynamics. However, the scarcity of long, dense time series data poses a severe bottleneck for characterising community dynamics using existing methods. We overcome this limitation by combining information across multiple short time series using Bayesian inference. By decomposing dynamics into deterministic and stochastic components using Gaussian process priors, we predict stable and tipping regions along the community landscape and quantify resilience while addressing uncertainty. After validation with simulated and real ecological time series, we use the model to question common assumptions underlying classical potential analysis and re-evaluate the stability of previously proposed "tipping elements" in the human gut microbiota.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03820v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chandler Ross, Ville Laitinen, Moein Khalighi, Jarkko Saloj\"arvi, Willem de Vos, Guilhem Sommeria-Klein, Leo Lahti</dc:creator>
    </item>
    <item>
      <title>On the unification of zero-adjusted cure survival models</title>
      <link>https://arxiv.org/abs/1901.09214</link>
      <description>arXiv:1901.09214v3 Announce Type: replace-cross 
Abstract: This paper proposes a unified version of survival models that accounts for both zero-adjustment and cure proportions in various latent competing causes, useful in data where survival times may be zero or cure proportions are present. These models are particularly relevant in scenarios like childbirth duration in sub-Saharan Africa. Different competing cause distributions were considered, including Binomial, Geometric, Poisson, and Negative Binomial. The model's maximum likelihood point estimators and asymptotic confidence intervals were evaluated through simulation, demonstrating improved accuracy with larger sample sizes. The model best fits real obstetric data when assuming geometrically distributed causes. This flexible model, capable of considering different distributions for the lifetime of susceptible individuals and competing causes, is an effective tool for adjusting survival data, indicating broad application potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:1901.09214v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Louzada, Pedro Luiz Ramos, Hayala C. C. Souza, Lawal Oyeneyin, Gleici da Silva Castro Perdona</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</title>
      <link>https://arxiv.org/abs/2201.01357</link>
      <description>arXiv:2201.01357v5 Announce Type: replace-cross 
Abstract: Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.01357v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Kosuke Imai, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Asymptotics of numerical integration for two-level mixed models</title>
      <link>https://arxiv.org/abs/2202.07864</link>
      <description>arXiv:2202.07864v2 Announce Type: replace-cross 
Abstract: We study mixed models with a single grouping factor, where inference about unknown parameters requires optimizing a marginal likelihood defined by an intractable integral. Low-dimensional numerical integration techniques are regularly used to approximate these integrals, with inferences about parameters based on the resulting approximate marginal likelihood. For a generic class of mixed models that satisfy explicit regularity conditions, we derive the stochastic relative error rate incurred for both the likelihood and maximum likelihood estimator when adaptive numerical integration is used to approximate the marginal likelihood. We then specialize the analysis to well-specified generalized linear mixed models having exponential family response and multivariate Gaussian random effects, verifying that the regularity conditions hold, and hence that the convergence rates apply. We also prove that for models with likelihoods satisfying very weak concentration conditions that the maximum likelihood estimators from non-adaptive numerical integration approximations of the marginal likelihood are not consistent, further motivating adaptive numerical integration as the preferred tool for inference in mixed models. Code to reproduce the simulations in this paper is provided at https://github.com/awstringer1/aq-theory-paper-code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stringer, Blair Bilodeau, Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>Evaluating the impact of outcome delay on the efficiency of two-arm group-sequential trials</title>
      <link>https://arxiv.org/abs/2306.04430</link>
      <description>arXiv:2306.04430v2 Announce Type: replace-cross 
Abstract: Adaptive designs(AD) are a broad class of trial designs that allow preplanned modifications based on patient data providing improved efficiency and flexibility. However, a delay in observing the primary outcome variable can harm this added efficiency. In this paper, we aim to ascertain the size of such outcome delay that results in the realised efficiency gains of ADs becoming negligible compared to classical fixed sample RCTs. We measure the impact of delay by developing formulae for the no. of overruns in 2 arm GSDs with normal data, assuming different recruitment models. The efficiency of a GSD is usually measured in terms of the expected sample size (ESS), with GSDs generally reducing the ESS compared to a standard RCT. Our formulae measures the efficiency gain from a GSD in terms of ESS reduction that is lost due to delay. We assess whether careful choice of design (e.g., altering the spacing of the IAs) can help recover the benefits of GSDs in presence of delay. We also analyse the efficiency of GSDs with respect to time to complete the trial. Comparing the expected efficiency gains, with and without consideration of delay, it is evident GSDs suffer considerable losses due to delay. Even a small delay can have a significant impact on the trial's efficiency. In contrast, even in the presence of substantial delay, a GSD will have a smaller expected time to trial completion in comparison to a simple RCT. Greater efficiency is lost with increase in the no. of stages. The timing of IAs also can impact the efficiency of a GSDs with delay. Particularly, for unequally spaced IAs, conducting IAs too early in the trial can be harmful for the design with delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04430v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aritra Mukherjee, Michael J. Grayling, James M. S. Wason</dc:creator>
    </item>
    <item>
      <title>Finite mixture copulas for modeling dependence in longitudinal count data</title>
      <link>https://arxiv.org/abs/2403.10165</link>
      <description>arXiv:2403.10165v2 Announce Type: replace-cross 
Abstract: Dependence modeling of multivariate count data has garnered significant attention in recent years. Multivariate elliptical copulas are typically preferred in statistical literature to analyze dependence between repeated measurements of longitudinal data since they allow for different choices of the correlation structure. But these copulas lack in flexibility to model dependence and inference is only feasible under parametric restrictions. In this article, we propose employing finite mixtures of elliptical copulas to better capture the intricate and hidden temporal dependencies present in discrete longitudinal data. Our approach allows for the utilization of different correlation matrices within each component of the mixture copula. We theoretically explore the dependence properties of finite mixtures of copulas before employing them to construct regression models for count longitudinal data. Inference for this proposed class of models is based on a composite likelihood approach, and we evaluate the finite sample performance of parameter estimates through extensive simulation studies. To validate our models, we extend traditional techniques and introduce the t-plot method to accommodate finite mixtures of elliptical copulas. Finally, we apply our models to analyze the temporal dependence within two real-world longitudinal datasets and demonstrate their superiority over standard elliptical copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10165v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Augmented Doubly Robust Post-Imputation Inference for Proteomic Data</title>
      <link>https://arxiv.org/abs/2403.15802</link>
      <description>arXiv:2403.15802v2 Announce Type: replace-cross 
Abstract: Quantitative measurements produced by mass spectrometry proteomics experiments offer a direct way to explore the role of proteins in molecular mechanisms. However, analysis of such data is challenging due to the large proportion of missing values. A common strategy to address this issue is to utilize an imputed dataset, which often introduces systematic bias into downstream analyses if the imputation errors are ignored. In this paper, we propose a statistical framework inspired by doubly robust estimators that offers valid and efficient inference for proteomic data. Our framework combines powerful machine learning tools, such as variational autoencoders, to augment the imputation quality with high-dimensional peptide data, and a parametric model to estimate the propensity score for debiasing imputed outcomes. Our estimator is compatible with the double machine learning framework and has provable properties. In application to both single-cell and bulk-cell proteomic data our method utilizes the imputed data to gain additional, meaningful discoveries and yet maintains good control of false positives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15802v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haeun Moon, Jin-Hong Du, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference on Dose-Response Curves Without the Positivity Condition</title>
      <link>https://arxiv.org/abs/2405.09003</link>
      <description>arXiv:2405.09003v2 Announce Type: replace-cross 
Abstract: Existing statistical methods in causal inference often assume the positivity condition, where every individual has some chance of receiving any treatment level regardless of covariates. This assumption could be violated in observational studies with continuous treatments. In this paper, we develop identification and estimation theories for causal effects with continuous treatments (i.e., dose-response curves) without relying on the positivity condition. Our approach identifies and estimates the derivative of the treatment effect for each observed sample, integrating it to the treatment level of interest to mitigate bias from the lack of positivity. The method is grounded in a weaker assumption, satisfied by additive confounding models. We propose a fast and reliable numerical recipe for computing our integral estimator in practice and derive its asymptotic properties. To enable valid inference on the dose-response curve and its derivative, we use the nonparametric bootstrap and establish its consistency. The performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09003v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen, Alexander Giessing</dc:creator>
    </item>
    <item>
      <title>A Calibrated Sensitivity Analysis for Weighted Causal Decompositions</title>
      <link>https://arxiv.org/abs/2407.00139</link>
      <description>arXiv:2407.00139v2 Announce Type: replace-cross 
Abstract: Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other, intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter reformulation that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental acceptance on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00139v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy A. Shen, Elina Visoki, Ran Barzilay, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Exploring Learning Rate Selection in Generalised Bayesian Inference using Posterior Predictive Checks</title>
      <link>https://arxiv.org/abs/2410.01475</link>
      <description>arXiv:2410.01475v2 Announce Type: replace-cross 
Abstract: Generalised Bayesian Inference (GBI) attempts to address model misspecification in a standard Bayesian setup by tempering the likelihood. The likelihood is raised to a fractional power, called the learning rate, which reduces its importance in the posterior and has been established as a method to address certain kinds of model misspecification. Posterior Predictive Checks (PPC) attempt to detect model misspecification by locating a diagnostic, computed on the observed data, within the posterior predictive distribution of the diagnostic. This can be used to construct a hypothesis test where a small $p$-value indicates potential misfit. The recent Embedded Diachronic Sense Change (EDiSC) model suffers from misspecification and benefits from likelihood tempering. Using EDiSC as a case study, this exploratory work examines whether PPC could be used in a novel way to set the learning rate in a GBI setup. Specifically, the learning rate selected is the lowest value for which a hypothesis test using the log likelihood diagnostic is not rejected at the 10% level. The experimental results are promising, though not definitive, and indicate the need for further research along the lines suggested here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01475v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Schyan Zafar, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>Improve the Precision of Area Under the Curve Estimation for Recurrent Events Through Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2410.24163</link>
      <description>arXiv:2410.24163v2 Announce Type: replace-cross 
Abstract: The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, capturing a totality of evidence in relation to disease progression. While the Lin-Wei-Yang-Ying (LWYY) model is commonly used for analyzing recurrent events, it relies on the proportional rate assumption between treatment arms, which might be violated in practice. In contrast, the AUC under MCFs does not depend on such proportionality assumptions and offers a clinically interpretable measure of treatment effect. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24163v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tuo Wang, Yanyao Yi, Ting Ye, Jun Shao, Yu Du</dc:creator>
    </item>
    <item>
      <title>Zero-Coupon Treasury Rates and Returns using the Volatility Index</title>
      <link>https://arxiv.org/abs/2411.03699</link>
      <description>arXiv:2411.03699v4 Announce Type: replace-cross 
Abstract: We study a multivariate autoregressive stochastic volatility model for the first 3 principal components (level, slope, curvature) of 10 series of zero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this model using monthly data from 1990. Unlike classic models with hidden stochastic volatility, here it is observed as VIX: the volatility index for the S&amp;P 500 stock market index. Surprisingly, this stock index volatility works for Treasury bonds, too. Next, we prove long-term stability and the Law of Large Numbers. We express total returns of zero-coupon bonds using these principal components. We prove the Law of Large Numbers for these returns. All results are done for discrete and continuous time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03699v4</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Democratic Resilience and Sociotechnical Shocks</title>
      <link>https://arxiv.org/abs/2501.04796</link>
      <description>arXiv:2501.04796v2 Announce Type: replace-cross 
Abstract: We focus on the potential fragility of democratic elections given modern information-communication technologies (ICT) in the Web 2.0 era. Our work provides an explanation for the cascading attrition of public officials recently in the United States and offers potential policy interventions from a dynamic system's perspective. We propose that micro-level heterogeneity across individuals within crucial institutions leads to vulnerabilities of election support systems at the macro scale. Our analysis provides comparative statistics to measure the fragility of systems against targeted harassment, disinformation campaigns, and other adversarial manipulations that are now cheaper to scale and deploy. Our analysis also informs policy interventions that seek to retain public officials and increase voter turnout. We show how limited resources (for example, salary incentives to public officials and targeted interventions to increase voter turnout) can be allocated at the population level to improve these outcomes and maximally enhance democratic resilience. On the one hand, structural and individual heterogeneity cause systemic fragility that adversarial actors can exploit, but also provide opportunities for effective interventions that offer significant global improvements from limited and localized actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04796v2</guid>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Amin Rahimian, Michael P. Colaresi</dc:creator>
    </item>
  </channel>
</rss>
