<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2024 04:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quand rechercher c'est faire des vagues : Dans et {\`a} partir des images algorithmiques</title>
      <link>https://arxiv.org/abs/2404.03923</link>
      <description>arXiv:2404.03923v1 Announce Type: new 
Abstract: In Search of the Wave is a computer-generated film made in 2013, highlighting the computation of images through computer simulation, and through text and voice. Originating from a screening of the film at the Gustave Eiffel University, the article presents a reflection on research-creation in and from algorithmic images. Fundamentally, what is it in this research-creation -- especially in research on algorithmic imagery -- that can be set in motion? Without fully distinguishing between what would be research on one hand and creation on the other, we focus on characterizing forms, aesthetics, or theories that contribute to possible shifts. The inventory of these possibilities is precisely the challenge of the text: from mathematics to image and visualization, from the birth of generative aesthetics to the coding related to pioneering works (recoding), or from indexing new aesthetics to new forms of critical production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03923v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Dispositifs de recherche-cr{\'e}ation. Dialogue entre recherche universitaire et cr{\'e}ation artistique, Delatour France, 2024, 978-2-75210471-7</arxiv:journal_reference>
      <dc:creator>Ga\"etan Robillard (UP8)</dc:creator>
    </item>
    <item>
      <title>Theoretical and Empirical Analysis of a Fast Algorithm for Extracting Polygons from Signed Distance Bounds</title>
      <link>https://arxiv.org/abs/2111.05778</link>
      <description>arXiv:2111.05778v2 Announce Type: replace 
Abstract: Recently there has been renewed interest in signed distance bound representations due to their unique properties for 3D shape modelling. This is especially the case for deep learning-based bounds. However, it is beneficial to work with polygons in most computer-graphics applications. Thus, in this paper we introduce and investigate an asymptotically fast method for transforming signed distance bounds into polygon meshes. This is achieved by combining the principles of sphere tracing (or ray marching) with traditional polygonization techniques, such as Marching Cubes. We provide theoretical and experimental evidence that this approach is of the $O(N^2\log N)$ computational complexity for a polygonization grid with $N^3$ cells. The algorithm is tested on both a set of primitive shapes as well as signed distance bounds generated from point clouds by machine learning (and represented as neural networks). Given its speed, implementation simplicity and portability, we argue that it could prove useful during the modelling stage as well as in shape compression for storage.
  The code is available here: https://github.com/nenadmarkus/gridhopping</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.05778v2</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3390/a17040137</arxiv:DOI>
      <dc:creator>Nenad Marku\v{s}, Mirko Su\v{z}njevi\'c</dc:creator>
    </item>
    <item>
      <title>Neural Field Convolutions by Repeated Differentiation</title>
      <link>https://arxiv.org/abs/2304.01834</link>
      <description>arXiv:2304.01834v4 Announce Type: replace-cross 
Abstract: Neural fields are evolving towards a general-purpose continuous representation for visual computing. Yet, despite their numerous appealing properties, they are hardly amenable to signal processing. As a remedy, we present a method to perform general continuous convolutions with general continuous signals such as neural fields. Observing that piecewise polynomial kernels reduce to a sparse set of Dirac deltas after repeated differentiation, we leverage convolution identities and train a repeated integral field to efficiently execute large-scale convolutions. We demonstrate our approach on a variety of data modalities and spatially-varying kernels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01834v4</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3618340</arxiv:DOI>
      <dc:creator>Ntumba Elie Nsampi, Adarsh Djeacoumar, Hans-Peter Seidel, Tobias Ritschel, Thomas Leimk\"uhler</dc:creator>
    </item>
    <item>
      <title>Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis</title>
      <link>https://arxiv.org/abs/2312.16812</link>
      <description>arXiv:2312.16812v2 Announce Type: replace-cross 
Abstract: Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16812v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Li, Zhang Chen, Zhong Li, Yi Xu</dc:creator>
    </item>
  </channel>
</rss>
