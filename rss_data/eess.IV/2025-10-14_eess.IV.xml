<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 01:45:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery</title>
      <link>https://arxiv.org/abs/2510.09736</link>
      <description>arXiv:2510.09736v1 Announce Type: new 
Abstract: The Mar Menor, Europe's largest coastal lagoon, located in Spain, has undergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is essential to anticipate harmful algal blooms and guide mitigation. Traditional in situ measurements are spatially and temporally limited. Satellite-based approaches provide a more comprehensive view, enabling scalable, long-term, and transferable monitoring. This study aims to overcome limitations of chlorophyll monitoring, often restricted to surface estimates or limited temporal coverage, by developing a reliable methodology to predict and map Chl-a across the water column of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based ground truth to create models capable of high-resolution, depth-specific monitoring, enhancing early-warning capabilities for eutrophication. Nearly a decade of Sentinel 2 images was atmospherically corrected using C2RCC processors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m). Multiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater Perceptron Networks, and ensembles-were trained and validated using cross-validation. Systematic band-combination experiments and spatial aggregation strategies were tested to optimize prediction. Results show depth-dependent performance. At the surface, C2X-Complex with XGBoost and ensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models reached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 = 0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully reproduced known eutrophication events (e.g., 2016 crisis, 2025 surge), confirming robustness. The study delivers an end-to-end, validated methodology for depth-specific Chl-amapping. Its integration of multispectral band combinations, buoy calibration, and ML/DL modeling offers a transferable framework for other turbid coastal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09736v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>physics.ao-ph</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Mart\'inez-Ibarra, Aurora Gonz\'alez-Vidal, Adri\'an C\'anovas-Rodr\'iguez, Antonio F. Skarmeta</dc:creator>
    </item>
    <item>
      <title>Generative Latent Video Compression</title>
      <link>https://arxiv.org/abs/2510.09987</link>
      <description>arXiv:2510.09987v1 Announce Type: new 
Abstract: Perceptual optimization is widely recognized as essential for neural compression, yet balancing the rate-distortion-perception tradeoff remains challenging. This difficulty is especially pronounced in video compression, where frame-wise quality fluctuations often cause perceptually optimized neural video codecs to suffer from flickering artifacts. In this paper, inspired by the success of latent generative models, we present Generative Latent Video Compression (GLVC), an effective framework for perceptual video compression. GLVC employs a pretrained continuous tokenizer to project video frames into a perceptually aligned latent space, thereby offloading perceptual constraints from the rate-distortion optimization. We redesign the codec architecture explicitly for the latent domain, drawing on extensive insights from prior neural video codecs, and further equip it with innovations such as unified intra/inter coding and a recurrent memory mechanism. Experimental results across multiple benchmarks show that GLVC achieves state-of-the-art performance in terms of DISTS and LPIPS metrics. Notably, our user study confirms GLVC rivals the latest neural video codecs at nearly half their rate while maintaining stable temporal coherence, marking a step toward practical perceptual video compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09987v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongyu Guo, Zhaoyang Jia, Jiahao Li, Xiaoyi Zhang, Bin Li, Yan Lu</dc:creator>
    </item>
    <item>
      <title>Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided Framework</title>
      <link>https://arxiv.org/abs/2510.10492</link>
      <description>arXiv:2510.10492v1 Announce Type: new 
Abstract: This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10492v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shanzhi Yin, Bolin Chen, Xinju Wu, Ru-Ling Liao, Jie Chen, Shiqi Wang, Yan Ye</dc:creator>
    </item>
    <item>
      <title>JND-Guided Light-Weight Neural Pre-Filter for Perceptual Image Coding</title>
      <link>https://arxiv.org/abs/2510.10648</link>
      <description>arXiv:2510.10648v1 Announce Type: new 
Abstract: Just Noticeable Distortion (JND)-guided pre-filter is a promising technique for improving the perceptual compression efficiency of image coding. However, existing methods are often computationally expensive, and the field lacks standardized benchmarks for fair comparison. To address these challenges, this paper introduces a twofold contribution. First, we develop and open-source FJNDF-Pytorch, a unified benchmark for frequency-domain JND-Guided pre-filters. Second, leveraging this platform, we propose a complete learning framework for a novel, lightweight Convolutional Neural Network (CNN). Experimental results demonstrate that our proposed method achieves state-of-the-art compression efficiency, consistently outperforming competitors across multiple datasets and encoders. In terms of computational cost, our model is exceptionally lightweight, requiring only 7.15 GFLOPs to process a 1080p image, which is merely 14.1% of the cost of recent lightweight network. Our work presents a robust, state-of-the-art solution that excels in both performance and efficiency, supported by a reproducible research platform. The open-source implementation is available at https://github.com/viplab-fudan/FJNDF-Pytorch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10648v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenlong He, Zijing Dong, Min Li, Zhijian Hao, Leilei Huang, Xiaoyang Zeng, Yibo Fan</dc:creator>
    </item>
    <item>
      <title>Bit Allocation Transfer for Perceptual Quality Enhancement of VVC Intra Coding</title>
      <link>https://arxiv.org/abs/2510.10970</link>
      <description>arXiv:2510.10970v1 Announce Type: new 
Abstract: Mainstream image and video coding standards -- including state-of-the-art codecs like H.266/VVC, AVS3, and AV1 -- adopt a block-based hybrid coding framework. While this framework facilitates straightforward optimization for Peak Signal-to-Noise Ratio (PSNR), it struggles to effectively optimize perceptually-aligned metrics such as Multi-Scale Structural Similarity (MS-SSIM). To address this challenge, this paper proposes a low-complexity method to enhance perceptual quality in VVC intra coding by transferring bit allocation knowledge from end-to-end image compression. We introduce a lightweight model trained with perceptual losses to generate a quantization step map. This map implicitly captures block-level perceptual importance, enabling efficient derivation of a QP map for VVC. Experiments on Kodak and CLIC datasets demonstrate significant advantages, both in execution time and perceptual metric performance, with more than 11% BD-rate reduction in terms of MS-SSIM. Our scheme provides an efficient, practical pathway for perceptual enhancement of traditional codecs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10970v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runyu Yang, Ivan V. Baji\'c</dc:creator>
    </item>
    <item>
      <title>Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types</title>
      <link>https://arxiv.org/abs/2510.11182</link>
      <description>arXiv:2510.11182v1 Announce Type: new 
Abstract: Deep learning is expected to aid pathologists by automating tasks such as tumour segmentation. We aimed to develop one universal tumour segmentation model for histopathological images and examine its performance in different cancer types. The model was developed using over 20 000 whole-slide images from over 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma. Performance was validated in pre-planned analyses on external cohorts with over 3 000 patients across six cancer types. Exploratory analyses included over 1 500 additional patients from The Cancer Genome Atlas. Average Dice coefficient was over 80% in all validation cohorts with en bloc resection specimens and in The Cancer Genome Atlas cohorts. No loss of performance was observed when comparing the universal model with models specialised on single cancer types. In conclusion, extensive and rigorous evaluations demonstrate that generic tumour segmentation by a single model is possible across cancer types, patient populations, sample preparations, and slide scanners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11182v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ole-Johan Skrede, Manohar Pradhan, Maria Xepapadakis Isaksen, Tarjei Sveinsgjerd Hveem, Ljiljana Vlatkovic, Arild Nesbakken, Kristina Lindemann, Gunnar B Kristensen, Jenneke Kasius, Alain G Zeimet, Odd Terje Brustugun, Lill-Tove Rasmussen Busund, Elin H Richardsen, Erik Skaaheim Haug, Bj{\o}rn Brennhovd, Emma Rewcastle, Melinda Lillesand, Vebj{\o}rn Kvikstad, Emiel Janssen, David J Kerr, Knut Liest{\o}l, Fritz Albregtsen, Andreas Kleppe</dc:creator>
    </item>
    <item>
      <title>GADA: Graph Attention-based Detection Aggregation for Ultrasound Video Classification</title>
      <link>https://arxiv.org/abs/2510.11437</link>
      <description>arXiv:2510.11437v1 Announce Type: new 
Abstract: Medical ultrasound video analysis is challenging due to variable sequence lengths, subtle spatial cues, and the need for interpretable video-level assessment. We introduce GADA, a Graph Attention-based Detection Aggregation framework that reformulates video classification as a graph reasoning problem over spatially localized regions of interest. Rather than relying on 3D CNNs or full-frame analysis, GADA detects pathology-relevant regions across frames and represents them as nodes in a spatiotemporal graph, with edges encoding spatial and temporal dependencies. A graph attention network aggregates these node-level predictions through edge-aware attention to generate a compact, discriminative video-level output. Evaluated on a large-scale, multi-center clinical lung ultrasound dataset, GADA outperforms conventional baselines on two pathology video classification tasks while providing interpretable region- and frame-level attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11437v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Chen, Naveen Balaraju, Jochen Kruecker, Balasundar Raju, Alvin Chen</dc:creator>
    </item>
    <item>
      <title>Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection</title>
      <link>https://arxiv.org/abs/2510.09836</link>
      <description>arXiv:2510.09836v1 Announce Type: cross 
Abstract: This paper investigates the use of synthetic face data to enhance Single-Morphing Attack Detection (S-MAD), addressing the limitations of availability of large-scale datasets of bona fide images due to privacy concerns. Various morphing tools and cross-dataset evaluation schemes were utilized to conduct this study. An incremental testing protocol was implemented to assess the generalization capabilities as more and more synthetic images were added. The results of the experiments show that generalization can be improved by carefully incorporating a controlled number of synthetic images into existing datasets or by gradually adding bona fide images during training. However, indiscriminate use of synthetic data can lead to sub-optimal performance. Evenmore, the use of only synthetic data (morphed and non-morphed images) achieves the highest Equal Error Rate (EER), which means in operational scenarios the best option is not relying only on synthetic data for S-MAD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09836v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Benavente-Rios, Juan Ruiz Rodriguez, Gustavo Gatica</dc:creator>
    </item>
    <item>
      <title>Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals</title>
      <link>https://arxiv.org/abs/2510.09945</link>
      <description>arXiv:2510.09945v1 Announce Type: cross 
Abstract: Segmentation models achieve high accuracy on benchmarks but often fail in real-world domains by relying on spurious correlations instead of true object boundaries. We propose a human-in-the-loop interactive framework that enables interventional learning through targeted human corrections of segmentation outputs. Our approach treats human corrections as interventional signals that show when reliance on superficial features (e.g., color or texture) is inappropriate. The system learns from these interventions by propagating correction-informed edits across visually similar images, effectively steering the model toward robust, semantically meaningful features rather than dataset-specific artifacts. Unlike traditional annotation approaches that simply provide more training data, our method explicitly identifies when and why the model fails and then systematically corrects these failure modes across the entire dataset. Through iterative human feedback, the system develops increasingly robust representations that generalize better to novel domains and resist artifactual correlations. We demonstrate that our framework improves segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on challenging cubemap data and yields 3-4$\times$ reductions in annotation effort compared to standard retraining, while maintaining competitive performance on benchmark datasets. This work provides a practical framework for researchers and practitioners seeking to build segmentation systems that are accurate, robust to dataset biases, data-efficient, and adaptable to real-world domains such as urban climate monitoring and autonomous driving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09945v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pouya Shaeri, Ryan T. Woo, Yasaman Mohammadpour, Ariane Middel</dc:creator>
    </item>
    <item>
      <title>Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making</title>
      <link>https://arxiv.org/abs/2510.09981</link>
      <description>arXiv:2510.09981v1 Announce Type: cross 
Abstract: Accurate, scalable traffic monitoring is critical for real-time and long-term transportation management, particularly during disruptions such as natural disasters, large construction projects, or major policy changes like New York City's first-in-the-nation congestion pricing program. However, widespread sensor deployment remains limited due to high installation, maintenance, and data management costs. While traffic cameras offer a cost-effective alternative, existing video analytics struggle with dynamic camera viewpoints and massive data volumes from large camera networks. This study presents an end-to-end AI-based framework leveraging existing traffic camera infrastructure for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11 model, trained on localized urban scenes, extracts multimodal traffic density and classification metrics in real time. To address inconsistencies from non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based viewpoint normalization method. A domain-specific large language model was also integrated to process massive data from a 24/7 video stream to generate frequent, automated summaries of evolving traffic patterns, a task far exceeding manual capabilities. We validated the system using over 9 million images from roughly 1,000 traffic cameras during the early rollout of NYC congestion pricing in 2025. Results show a 9% decline in weekday passenger vehicle density within the Congestion Relief Zone, early truck volume reductions with signs of rebound, and consistent increases in pedestrian and cyclist activity at corridor and zonal scales. Experiments showed that example-based prompts improved LLM's numerical accuracy and reduced hallucinations. These findings demonstrate the framework's potential as a practical, infrastructure-ready solution for large-scale, policy-relevant traffic monitoring with minimal human intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09981v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Zuo, Donglin Zhou, Jingqin Gao, Kaan Ozbay</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models</title>
      <link>https://arxiv.org/abs/2510.10108</link>
      <description>arXiv:2510.10108v1 Announce Type: cross 
Abstract: Accurate fire and smoke detection is critical for safety and disaster response, yet existing vision-based methods face challenges in balancing efficiency and reliability. Compact deep learning models such as YOLOv5n and YOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT devices, but their reduced capacity often results in false positives and missed detections. Conventional post-detection methods such as Non-Maximum Suppression and Soft-NMS rely only on spatial overlap, which can suppress true positives or retain false alarms in cluttered or ambiguous fire scenes. To address these limitations, we propose an uncertainty aware post-detection framework that rescales detection confidences using both statistical uncertainty and domain relevant visual cues. A lightweight Confidence Refinement Network integrates uncertainty estimates with color, edge, and texture features to adjust detection scores without modifying the base model. Experiments on the D-Fire dataset demonstrate improved precision, recall, and mean average precision compared to existing baselines, with only modest computational overhead. These results highlight the effectiveness of post-detection rescoring in enhancing the robustness of compact deep learning models for real-world fire and smoke detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10108v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aniruddha Srinivas Joshi, Godwyn James William, Shreyas Srinivas Joshi</dc:creator>
    </item>
    <item>
      <title>YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments</title>
      <link>https://arxiv.org/abs/2510.10141</link>
      <description>arXiv:2510.10141v1 Announce Type: cross 
Abstract: Litchi is a high-value fruit, yet traditional manual selection methods are increasingly inadequate for modern production demands. Integrating UAV-based aerial imagery with deep learning offers a promising solution to enhance efficiency and reduce costs. This paper introduces YOLOv11-Litchi, a lightweight and robust detection model specifically designed for UAV-based litchi detection. Built upon the YOLOv11 framework, the proposed model addresses key challenges such as small target size, large model parameters hindering deployment, and frequent target occlusion. To tackle these issues, three major innovations are incorporated: a multi-scale residual module to improve contextual feature extraction across scales, a lightweight feature fusion method to reduce model size and computational costs while maintaining high accuracy, and a litchi occlusion detection head to mitigate occlusion effects by emphasizing target regions and suppressing background interference. Experimental results validate the model's effectiveness. YOLOv11-Litchi achieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline - while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%. Additionally, the model achieves a frame rate of 57.2 FPS, meeting real-time detection requirements. These findings demonstrate the suitability of YOLOv11-Litchi for UAV-based litchi detection in complex orchard environments, showcasing its potential for broader applications in precision agriculture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10141v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxing Peng, Haopei Xie, Weijia Lia, Huanai Liuc, Ximing Li</dc:creator>
    </item>
    <item>
      <title>Guided Image Feature Matching using Feature Spatial Order</title>
      <link>https://arxiv.org/abs/2510.10414</link>
      <description>arXiv:2510.10414v1 Announce Type: cross 
Abstract: Image feature matching plays a vital role in many computer vision tasks. Although many image feature detection and matching techniques have been proposed over the past few decades, it is still time-consuming to match feature points in two images, especially for images with a large number of detected features. Feature spatial order can estimate the probability that a pair of features is correct. Since it is a completely independent concept from epipolar geometry, it can be used to complement epipolar geometry in guiding feature match in a target region so as to improve matching efficiency. In this paper, we integrate the concept of feature spatial order into a progressive matching framework. We use some of the initially matched features to build a computational model of feature spatial order and employs it to calculates the possible spatial range of subsequent feature matches, thus filtering out unnecessary feature matches. We also integrate it with epipolar geometry to further improve matching efficiency and accuracy. Since the spatial order of feature points is affected by image rotation, we propose a suitable image alignment method from the fundamental matrix of epipolar geometry to remove the effect of image rotation. To verify the feasibility of the proposed method, we conduct a series of experiments, including a standard benchmark dataset, self-generated simulated images, and real images. The results demonstrate that our proposed method is significantly more efficient and has more accurate feature matching than the traditional method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10414v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chin-Hung Teng, Ben-Jian Dong</dc:creator>
    </item>
    <item>
      <title>SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model</title>
      <link>https://arxiv.org/abs/2510.10910</link>
      <description>arXiv:2510.10910v1 Announce Type: cross 
Abstract: With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10910v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honghui Yuan, Keiji Yanai</dc:creator>
    </item>
    <item>
      <title>Efficient Edge Test-Time Adaptation via Latent Feature Coordinate Correction</title>
      <link>https://arxiv.org/abs/2510.11068</link>
      <description>arXiv:2510.11068v1 Announce Type: cross 
Abstract: Edge devices face significant challenges due to limited computational resources and distribution shifts, making efficient and adaptable machine learning essential. Existing test-time adaptation (TTA) methods often rely on gradient-based optimization or batch processing, which are inherently unsuitable for resource-constrained edge scenarios due to their reliance on backpropagation and high computational demands. Gradient-free alternatives address these issues but often suffer from limited learning capacity, lack flexibility, or impose architectural constraints. To overcome these limitations, we propose a novel single-instance TTA method tailored for edge devices (TED), which employs forward-only coordinate optimization in the principal subspace of latent using the covariance matrix adaptation evolution strategy (CMA-ES). By updating a compact low-dimensional vector, TED not only enhances output confidence but also aligns the latent representation closer to the source latent distribution within the latent principal subspace. This is achieved without backpropagation, keeping the model parameters frozen, and enabling efficient, forgetting-free adaptation with minimal memory and computational overhead. Experiments on image classification and keyword spotting tasks across the ImageNet and Google Speech Commands series datasets demonstrate that TED achieves state-of-the-art performance while $\textit{reducing computational complexity by up to 63 times}$, offering a practical and scalable solution for real-world edge applications. Furthermore, we successfully $\textit{deployed TED on the ZYNQ-7020 platform}$, demonstrating its feasibility and effectiveness for resource-constrained edge devices in real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11068v1</guid>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Luo, Jie Liu, Kecheng Chen, Junyi Yang, Bo Ding, Arindam Basu, Haoliang Li</dc:creator>
    </item>
    <item>
      <title>Rethinking Medical Anomaly Detection in Brain MRI: An Image Quality Assessment Perspective</title>
      <link>https://arxiv.org/abs/2408.08228</link>
      <description>arXiv:2408.08228v2 Announce Type: replace 
Abstract: Reconstruction-based methods, particularly those leveraging autoencoders, have been widely adopted for anomaly detection task in brain MRI. Unlike most existing works try to improve the task accuracy through architectural or algorithmic innovations, we tackle this task from image quality assessment (IQA) perspective, an under-explored direction in the field. Due to the limitations of conventional metrics such as l1 in capturing the nuanced differences in reconstructed images for medical anomaly detection, we propose fusion quality, a novel metric that wisely integrates the structure-level sensitivity of Structural Similarity Index Measure (SSIM) with the pixel-level precision of l1. The metric offers a more comprehensive assessment of reconstruction quality, considering intensity (subtractive property of l1 and divisive property of SSIM), contrast, and structural similarity. Furthermore, the proposed metric makes subtle regional variations more impactful in the final assessment. Thus, considering the inherent divisive properties of SSIM, we design an average intensity ratio (AIR)-based data transformation that amplifies the divisive discrepancies between normal and abnormal regions, thereby enhancing anomaly detection. By fusing the aforementioned two components, we devise the IQA approach. Experimental results on two distinct brain MRI datasets show that our IQA approach significantly enhances medical anomaly detection performance when integrated with state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08228v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Pan, Jun Xia, Zheyu Yan, Guoyue Xu, Yifan Qin, Xueyang Li, Yawen Wu, Zhenge Jia, Jianxu Chen, Yiyu Shi</dc:creator>
    </item>
    <item>
      <title>Cell as Point: One-Stage Framework for Efficient Cell Tracking</title>
      <link>https://arxiv.org/abs/2411.14833</link>
      <description>arXiv:2411.14833v3 Announce Type: replace 
Abstract: Conventional multi-stage cell tracking approaches rely heavily on detection or segmentation in each frame as a prerequisite, requiring substantial resources for high-quality segmentation masks and increasing the overall prediction time. To address these limitations, we propose CAP, a novel end-to-end one-stage framework that reimagines cell tracking by treating Cell as Point. Unlike traditional methods, CAP eliminates the need for explicit detection or segmentation, instead jointly tracking cells for sequences in one stage by leveraging the inherent correlations among their trajectories. This simplification reduces both labeling requirements and pipeline complexity. However, directly processing the entire sequence in one stage poses challenges related to data imbalance in capturing cell division events and long sequence inference. To solve these challenges, CAP introduces two key innovations: (1) adaptive event-guided (AEG) sampling, which prioritizes cell division events to mitigate the occurrence imbalance of cell events, and (2) the rolling-as-window (RAW) inference strategy, which ensures continuous and stable tracking of newly emerging cells over extended sequences. By removing the dependency on segmentation-based preprocessing while addressing the challenges of imbalanced occurrence of cell events and long-sequence tracking, CAP demonstrates promising cell tracking performance and is 8 to 32 times more efficient than existing methods. The code and model checkpoints will be available soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14833v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaxuan Song, Jianan Fan, Heng Huang, Mei Chen, Weidong Cai</dc:creator>
    </item>
    <item>
      <title>MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2505.11797</link>
      <description>arXiv:2505.11797v2 Announce Type: replace 
Abstract: Medical image segmentation has traditionally relied on convolutional neural networks (CNNs) and Transformer-based models. CNNs, however, are constrained by limited receptive fields, while Transformers face scalability challenges due to quadratic computational complexity. To over-come these issues, recent studies have explored alternative architectures. The Mamba model, a selective state-space design, achieves near-linear complexity and effectively captures long-range dependencies. Its vision-oriented variant, the Visual State Space (VSS) model, extends these strengths to image feature learning. In parallel, the Kolmogorov-Arnold Network (KAN) enhanc-es nonlinear expressiveness by replacing fixed activation functions with learnable ones. Moti-vated by these advances, we propose the VSS-Enhanced KAN (VKAN) module, which integrates VSS with the Expanded Field Convolutional KAN (EFC-KAN) as a replacement for Transformer modules, thereby strengthening feature extraction. We further embed VKAN into a U-Net frame-work, resulting in MedVKAN, an efficient medical image segmentation model. Extensive exper-iments on five public datasets demonstrate that MedVKAN achieves state-of-the-art performance on four datasets and ranks second on the remaining one. These results underscore the effective-ness of combining Mamba and KAN while introducing a novel and computationally efficient feature extraction framework. The source code is available at: https://github.com/beginner-cjh/MedVKAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11797v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.bspc.2025.108821</arxiv:DOI>
      <dc:creator>Hancan Zhu, Jinhao Chen, Guanghua He</dc:creator>
    </item>
    <item>
      <title>OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates</title>
      <link>https://arxiv.org/abs/2505.16091</link>
      <description>arXiv:2505.16091v5 Announce Type: replace 
Abstract: Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models are available at https://github.com/jp-guo/OSCAR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16091v5</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</dc:creator>
    </item>
    <item>
      <title>AI-powered skin spectral imaging enables instant sepsis diagnosis and outcome prediction in critically ill patients</title>
      <link>https://arxiv.org/abs/2408.09873</link>
      <description>arXiv:2408.09873v2 Announce Type: replace-cross 
Abstract: With sepsis remaining a leading cause of mortality, early identification of patients with sepsis and those at high risk of death is a challenge of high socioeconomic importance. Given the potential of hyperspectral imaging (HSI) to monitor microcirculatory alterations, we propose a deep learning approach to automated sepsis diagnosis and mortality prediction using a single HSI cube acquired within seconds. In a prospective observational study, we collected HSI data from the palms and fingers of more than 480 intensive care unit patients. Neural networks applied to HSI measurements predicted sepsis and mortality with areas under the receiver operating characteristic curve (AUROCs) of 0.80 and 0.72, respectively. Performance improved substantially with additional clinical data, reaching AUROCs of 0.94 for sepsis and 0.83 for mortality. We conclude that deep learning-based HSI analysis enables rapid and noninvasive prediction of sepsis and mortality, with a potential clinical value for enhancing diagnosis and treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09873v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1126/sciadv.adw1968</arxiv:DOI>
      <arxiv:journal_reference>Sci. Adv. 11, eadw1968 (2025)</arxiv:journal_reference>
      <dc:creator>Silvia Seidlitz, Katharina H\"olzl, Ayca von Garrel, Jan Sellner, Stephan Katzenschlager, Tobias H\"olle, Dania Fischer, Maik von der Forst, Felix C. F. Schmitt, Alexander Studier-Fischer, Markus A. Weigand, Lena Maier-Hein, Maximilian Dietrich</dc:creator>
    </item>
    <item>
      <title>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</title>
      <link>https://arxiv.org/abs/2410.09768</link>
      <description>arXiv:2410.09768v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09768v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shanzhi Yin, Zihan Zhang, Bolin Chen, Shiqi Wang, Yan Ye</dc:creator>
    </item>
    <item>
      <title>The IBEX Imaging Knowledge-Base: A Community Resource Enabling Adoption and Development of Immunofluoresence Imaging Methods</title>
      <link>https://arxiv.org/abs/2412.12965</link>
      <description>arXiv:2412.12965v2 Announce Type: replace-cross 
Abstract: The iterative bleaching extends multiplexity (IBEX) Knowledge-Base is a central portal for researchers adopting IBEX and related 2D and 3D immunofluorescence imaging methods. The design of the Knowledge-Base is modeled after efforts in the open-source software community and includes three facets: a development platform (GitHub), static website, and service for data archiving. The Knowledge-Base facilitates the practice of open science throughout the research life cycle by providing validation data for recommended and non-recommended reagents, such as primary and secondary antibodies. In addition to reporting negative data, the Knowledge-Base empowers method adoption and evolution by providing a venue for sharing protocols, videos, datasets, software, and publications. A dedicated discussion forum fosters a sense of community among researchers while addressing questions not covered in published manuscripts. Together, scientists from around the world are advancing scientific discovery at a faster pace, reducing wasted time and effort, and instilling greater confidence in the resulting data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12965v2</guid>
      <category>q-bio.TO</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziv Yaniv (Bioinformatics and Computational Bioscience Branch, National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD, USA), Ifeanyichukwu U. Anidi (Critical Care Medicine and Pulmonary Branch, National Heart, Lung and Blood Institute, National Institutes of Health, Bethesda, MD, USA), Leanne Arakkal (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Armando J. Arroyo-Mej\'ias (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Rebecca T. Beuschel (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Katy B\"orner (Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA), Colin J. Chu (UCL Institute of Ophthalmology and NIHR Moorfields Biomedical Research Centre, London, UK), Beatrice Clark (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Menna R. Clatworthy (Cambridge Institute for Therapeutic Immunology and Infectious Diseases, University of Cambridge Department of Medicine, Molecular Immunity Unit, Laboratory of Molecular Biology, Cambridge, UK), Jake Colautti (McMaster Immunology Research Centre, Schroeder Allergy and Immunology Research Institute, Department of Medicine, Faculty of Health Sciences, McMaster University, Hamilton, ON, Canada), Fabian Coscia (Max-Delbrueck-Center for Molecular Medicine in the Helmholtz Association), Joshua Croteau (Department of Business Development, BioLegend Inc., San Diego, CA, USA), Saven Denha (McMaster Immunology Research Centre, Schroeder Allergy and Immunology Research Institute, Department of Medicine, Faculty of Health Sciences, McMaster University, Hamilton, ON, Canada), Rose Dever (Functional Immunogenomics Unit, National Institute of Arthritis and Musculoskeletal and Skin Diseases, National Institutes of Health, Bethesda, MD, USA), Walderez O. Dutra (Laboratory of Cell-Cell Interactions, Department of Morphology, Institute of Biological Sciences, Universidade Federal de Minas Gerais, Belo Horizonte, MG, Brazil), Sonja Fritzsche (Max-Delbrueck-Center for Molecular Medicine in the Helmholtz Association, Humboldt-Universitat zu Berlin, Institute of Biology, Berlin, Germany), Spencer Fullam (Division of Rheumatology, Rush University Medical Center, Chicago, IL, USA), Michael Y. Gerner (Department of Immunology, University of Washington School of Medicine, Seattle, WA, USA), Anita Gola (Robin Chemers Neustein Laboratory of Mammalian Cell Biology and Development, The Rockefeller University, New York, NY, USA), Kenneth J. Gollob (Center for Research in Immuno-oncology), Jonathan M. Hernandez (Surgical Oncology Program, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA), Jyh Liang Hor (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Hiroshi Ichise (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Zhixin Jing (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Danny Jonigk (Institute of Pathology, Aachen Medical University, RWTH Aachen, Aachen, Germany, German Center for Lung Research), Evelyn Kandov (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Wolfgang Kastenm\"uller (W\"urzburg Institute of Systems Immunology, Max Planck Research Group at the Julius-Maximilians-Universit\"at W\"urzburg, W\"urzburg, Germany), Joshua F. E. Koenig (McMaster Immunology Research Centre, Schroeder Allergy and Immunology Research Institute, Department of Medicine, Faculty of Health Sciences, McMaster University, Hamilton, ON, Canada), Aanandita Kothurkar (UCL Institute of Ophthalmology and NIHR Moorfields Biomedical Research Centre, London, UK), Rosa K. Kortekaas (Department of Medicine, McMaster University, Firestone Institute for Respiratory Health, St Joseph's Healthcare, Hamilton, ON, Canada), Alexandra Y. Kreins (Infection Immunity and Inflammation Research and Teaching Department, University College London Great Ormond Street Institute of Child Health, London, UK), Ian T. Lamborn (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Yuri Lin (Surgical Oncology Program, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA), Katia Luciano Pereira Morais (Center for Research in Immuno-oncology), Aleksandra Lunich (Critical Care Medicine and Pulmonary Branch, National Heart, Lung and Blood Institute, National Institutes of Health, Bethesda, MD, USA), Jean C. S. Luz (Viral Vector Laboratory, Cancer Institute of S\~ao Paulo, University of S\~ao Paulo, SP, Brazil), Ryan B. MacDonald (UCL Institute of Ophthalmology and NIHR Moorfields Biomedical Research Centre, London, UK), Chen Makranz (Neuro-Oncology Branch, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA), Vivien I. Maltez (Division of Allergy, Immunology and Rheumatology, Department of Pediatrics, University of California San Diego, La Jolla, CA, USA), John E. McDonough (Department of Medicine, McMaster University, Firestone Institute for Respiratory Health, St Joseph's Healthcare, Hamilton, ON, Canada), Ryan V. Moriarty (Department of Cellular and Developmental Biology, Northwestern University, Chicago, IL, USA), Juan M. Ocampo-Godinez (Infection Immunity and Inflammation Research and Teaching Department, University College London Great Ormond Street Institute of Child Health, London, UK, Laboratorio de Bioingenier\'ia de Tejidos, Departamento de Estudios de Posgrado e Investigaci\'on, Universidad Nacional Aut\'onoma de M\'exico, Mexico City, Mexico), Vitoria M. Olyntho (McMaster Immunology Research Centre, Schroeder Allergy and Immunology Research Institute, Department of Medicine, Faculty of Health Sciences, McMaster University, Hamilton, ON, Canada), Annette Oxenius (Institute of Microbiology, ETH Zurich, Zurich, Switzerland), Kartika Padhan (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA, Center for Advanced Tissue Imaging Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Kirsten Remmert (Surgical Oncology Program, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA), Nathan Richoz (Cambridge Institute for Therapeutic Immunology and Infectious Diseases, University of Cambridge Department of Medicine, Molecular Immunity Unit, Laboratory of Molecular Biology, Cambridge, UK), Edward C. Schrom (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Wanjing Shang (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Lihong Shi (Laboratory of Immune System Biology, National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD, USA), Rochelle M. Shih (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Emily Speranza (Florida Research and Innovation Center, Cleveland Clinic Lerner Research Institute, Port Saint Lucie, FL, USA), Salome Stierli (Institute of Anatomy, University of Zurich, Zurich, Switzerland), Sarah A. Teichmann (Cambridge Stem Cell Institute, Jeffrey Cheah Biomedical Centre, Puddicombe Way, Cambridge Biomedical Campus, Cambridge, UK), Tibor Z. Veres (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Megan Vierhout (McMaster Immunology Research Centre, Schroeder Allergy and Immunology Research Institute, Department of Medicine, Faculty of Health Sciences, McMaster University, Hamilton, ON, Canada, Department of Medicine, McMaster University, Firestone Institute for Respiratory Health, St Joseph's Healthcare, Hamilton, ON, Canada), Brianna T. Wachter (Laboratory of Clinical Immunology and Microbiology, National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD, USA), Adam K. Wade-Vallance (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Margaret Williams (Critical Care Medicine and Pulmonary Branch, National Heart, Lung and Blood Institute, National Institutes of Health, Bethesda, MD, USA), Nathan Zangger (Institute of Microbiology, ETH Zurich, Zurich, Switzerland), Ronald N. Germain (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA, Center for Advanced Tissue Imaging Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA), Andrea J. Radtke (Lymphocyte Biology Section, Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA, Center for Advanced Tissue Imaging Laboratory of Immune System Biology, NIAID, NIH, Bethesda, MD, USA, Leica Microsystems, Wetzlar, Germany)</dc:creator>
    </item>
    <item>
      <title>A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory</title>
      <link>https://arxiv.org/abs/2506.08793</link>
      <description>arXiv:2506.08793v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel partial differential equation (PDE) framework for single-image dehazing. We embed the atmospheric scattering model into a PDE featuring edge-preserving diffusion and a nonlocal operator to maintain both local details and global structures. A key innovation is an adaptive regularization mechanism guided by the dark channel prior, which adjusts smoothing strength based on haze density. The framework's mathematical well-posedness is rigorously established by proving the existence and uniqueness of its weak solution in $H_0^1(\Omega)$. An efficient, GPU-accelerated fixed-point solver is used for implementation. Experiments confirm our method achieves effective haze removal while preserving high image fidelity, offering a principled alternative to purely data-driven techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08793v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liubing Hu, Pu Wang, Guangwei Gao, Chunyan Wang, Zhuoran Zheng</dc:creator>
    </item>
    <item>
      <title>TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity</title>
      <link>https://arxiv.org/abs/2506.23484</link>
      <description>arXiv:2506.23484v3 Announce Type: replace-cross 
Abstract: AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23484v3</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhuo Chen, Zehua Ma, Han Fang, Weiming Zhang, Nenghai Yu</dc:creator>
    </item>
    <item>
      <title>RoHOI: Robustness Benchmark for Human-Object Interaction Detection</title>
      <link>https://arxiv.org/abs/2507.09111</link>
      <description>arXiv:2507.09111v3 Announce Type: replace-cross 
Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate predictions. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusions, and noise. Our benchmark, RoHOI, includes 20 corruption types based on the HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the HOI field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, thus dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show that our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code are available at https://github.com/KratosWen/RoHOI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09111v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen</dc:creator>
    </item>
    <item>
      <title>Local MAP Sampling for Diffusion Models</title>
      <link>https://arxiv.org/abs/2510.07343</link>
      <description>arXiv:2510.07343v2 Announce Type: replace-cross 
Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $&gt;1.5$ dB improvements on inverse scattering benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07343v2</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaorong Zhang, Rob Brekelmans, Greg Ver Steeg</dc:creator>
    </item>
  </channel>
</rss>
