<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 May 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Empirical Validation of Functional Multidimensional Scaling via Numerical Simulation and Real-World Application</title>
      <link>https://arxiv.org/abs/2505.13998</link>
      <description>arXiv:2505.13998v1 Announce Type: new 
Abstract: This article presents an empirical validation of the functional multidimensional scaling model, a novel approach that improves the smoothness of time-varying dissimilarities in a low-dimensional space, embedding a modified Adam stochastic gradient descent method. We conduct a numerical simulation study to evaluate the feasibility of the functional multidimensional scaling model under various controlled scenarios and to assess the goodness of the approximation of the estimators with a curvilinear search method, demonstrating its robustness and scalability in dynamic structures. To further explore its effectiveness in practice, we implement the functional multidimensional scaling model in a real-world case with stock market data, revealing strong clustering capabilities in visualization. The experiments in this article indicate that the functional multidimensional scaling model performs robustly on synthetic benchmarks and provides meaningful insights of high-dimensional and time-varying data in the real world, reinforcing its value in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13998v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liting Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian Network Method for Deaggregation: Identification of Tropical Cyclones Driving Coastal Hazards</title>
      <link>https://arxiv.org/abs/2505.14374</link>
      <description>arXiv:2505.14374v1 Announce Type: new 
Abstract: Bayesian networks (BN) have advantages in visualizing causal relationships and performing probabilistic inference analysis, making them ideal tools for coastal hazard analysis and characterizing the compound mechanisms of coastal hazards. Meanwhile, the Joint Probability Method (JPM) has served as the primary probabilistic assessment approach used to develop hazard curves for tropical cyclone (TC) induced coastal hazards in the past decades. To develop hazard curves that can capture the breadth of TC-induced coastal hazards, a large number of synthetic TCs need to be simulated, which is computationally expensive. Given that low exceedance probability (LEP) coastal hazards are likely to result in the most significant damage to coastal communities, it is practical to focus efforts on identifying and understanding TC scenarios that are dominant contributors to LEP coastal hazards. This study developed a BN-based framework incorporating existing JPM for multiple TC-induced coastal hazards deaggregation. Copula-based models capture dependence among TC atmospheric parameters and generate CPTs for corresponding BN nodes. Machine learning surrogates model the relationship between TC parameters and coastal hazards, providing conditional probability tables (CPTs) for hazard nodes. Case studies are applied to the Greater New Orleans region in Louisiana (USA). Deaggregation is a method for identifying dominant scenarios for a given hazard, which was first established in the field of probabilistic seismic hazard analysis. The objective of this study is to leverage BN to develop a deaggregation method of multiple LEP coastal hazards to better understand the dominant drivers of coastal hazards to refine storm parameter set selection to more comprehensively represent multiple forcings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14374v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Liu, Meredith L. Carr, Norberto C. Nadal-Caraballo, Madison C. Yawn, Michelle T. Bensi</dc:creator>
    </item>
    <item>
      <title>Semi-parametric efficient estimation of small genetic effects in large-scale population cohorts</title>
      <link>https://arxiv.org/abs/2505.14675</link>
      <description>arXiv:2505.14675v1 Announce Type: new 
Abstract: Population genetics seeks to quantify DNA variant associations with traits or diseases, as well as interactions among variants and with environmental factors. Computing millions of estimates in large cohorts in which small effect sizes are expected, necessitates minimising model-misspecification bias to control false discoveries. We present TarGene, a unified statistical workflow for the semi-parametric efficient and double robust estimation of genetic effects including k-point interactions among categorical variables in the presence of confounding and weak population dependence. k-point interactions, or Average Interaction Effects (AIEs), are a direct generalisation of the usual average treatment effect (ATE). We estimate AIEs with cross-validated and/or weighted versions of Targeted Minimum Loss-based Estimators (TMLE) and One-Step Estimators (OSE). The effect of dependence among data units on variance estimates is corrected by using sieve plateau variance estimators based on genetic relatedness across the units. We present extensive realistic simulations to demonstrate power, coverage, and control of type I error. Our motivating application is the targeted estimation of genetic effects on trait, including two-point and higher-order gene-gene and gene-environment interactions, in large-scale genomic databases such as UK Biobank and All of Us. All cross-validated and/or weighted TMLE and OSE for the AIE k-point interaction, as well as ATEs, conditional ATEs and functions thereof, are implemented in the general purpose Julia package TMLE.jl. For high-throughput applications in population genomics, we provide the open-source Nextflow pipeline and software TarGene which integrates seamlessly with modern high-performance and cloud computing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14675v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Labayle, Breeshey Roskams-Hieter, Joshua Slaughter, Kelsey Tetley-Campbell, Mark J. van der Laan, Chris P. Ponting, Sjoerd Viktor Beentjes, Ava Khamseh</dc:creator>
    </item>
    <item>
      <title>The Problematic Hoover Index</title>
      <link>https://arxiv.org/abs/2505.13486</link>
      <description>arXiv:2505.13486v1 Announce Type: cross 
Abstract: The Hoover index H, derived from the distribution of population density, has a long history in population geography. But it is prone to misinterpretation and serious measurement artifacts, some of which have been recognized for years. Here its problems, old and new, are put in a common framework and quantitatively dissected with thought-experiments, experimental reaggregation over a thousand-fold range, modeling, and simulation. H has long been interpreted as urbanization, which simple examples show is incorrect. Values of H near zero are taken as an ideal or primordial state, but the mechanism and distributions of settlement growth show that H can be expected in the range 0.4-0.7 and remain nearly constant across continents, centuries of time, and wide ranges of population density. Technically, the measurement of H is confounded by choices of aggregation scale, inclusion of unpopulated areas, and truncated input data, which produce wide swings in its value. Spatially isotropic modeling and simulation reproduce its trends with aggregation scale and time, nullifying many frontier- or nation-specific interpretations. I suggest methods to recognize and address its major artifacts and point out several trends with valid interpretations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13486v1</guid>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robin W. Spencer</dc:creator>
    </item>
    <item>
      <title>Zero-Shot Forecasting Mortality Rates: A Global Study</title>
      <link>https://arxiv.org/abs/2505.13521</link>
      <description>arXiv:2505.13521v1 Announce Type: cross 
Abstract: This study explores the potential of zero-shot time series forecasting, an innovative approach leveraging pre-trained foundation models, to forecast mortality rates without task-specific fine-tuning. We evaluate two state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional and machine learning-based methods across three forecasting horizons (5, 10, and 20 years) using data from 50 countries and 111 age groups. In our investigations, zero-shot models showed varying results: while CHRONOS delivered competitive shorter-term forecasts, outperforming traditional methods like ARIMA and the Lee-Carter model, TimesFM consistently underperformed. Fine-tuning CHRONOS on mortality data significantly improved long-term accuracy. A Random Forest model, trained on mortality data, achieved the best overall performance. These findings underscore the potential of zero-shot forecasting while highlighting the need for careful model selection and domain-specific adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13521v1</guid>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabor Petnehazi, Laith Al Shaggah, Jozsef Gall, Bernadett Aradi</dc:creator>
    </item>
    <item>
      <title>Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient</title>
      <link>https://arxiv.org/abs/2505.13846</link>
      <description>arXiv:2505.13846v1 Announce Type: cross 
Abstract: In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13846v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoru Shinoda, Hideaki Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals</title>
      <link>https://arxiv.org/abs/2505.14164</link>
      <description>arXiv:2505.14164v1 Announce Type: cross 
Abstract: Density regression models allow a comprehensive understanding of data by modeling the complete conditional probability distribution. While flexible estimation approaches such as normalizing flows (NF) work particularly well in multiple dimensions, interpreting the input-output relationship of such models is often difficult, due to the black-box character of deep learning models. In contrast, existing statistical methods for multivariate outcomes such as multivariate conditional transformation models (MCTM) are restricted in flexibility and are often not expressive enough to represent complex multivariate probability distributions. In this paper, we combine MCTM with state-of-the-art and autoregressive NF to leverage the transparency of MCTM for modeling interpretable feature effects on the marginal distributions in the first step and the flexibility of neural-network-based NF techniques to account for complex and non-linear relationships in the joint data distribution. We demonstrate our method's versatility in various numerical experiments and compare it with MCTM and other NF models on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14164v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcel Arpogaus, Thomas Kneib, Thomas Nagler, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces</title>
      <link>https://arxiv.org/abs/2505.14320</link>
      <description>arXiv:2505.14320v1 Announce Type: cross 
Abstract: Facial recognition technology (FRT) is increasingly used in criminal investigations, yet most evaluations of its accuracy rely on high-quality images, unlike those often encountered by law enforcement. This study examines how five common forms of image degradation--contrast, brightness, motion blur, pose shift, and resolution--affect FRT accuracy and fairness across demographic groups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace, we simulate degraded images and evaluate performance using Deepface with ArcFace loss in 1:n identification tasks. We perform an experiment and find that false positive rates peak near baseline image quality, while false negatives increase as degradation intensifies--especially with blur and low resolution. Error rates are consistently higher for women and Black individuals, with Black females most affected. These disparities raise concerns about fairness and reliability when FRT is used in real-world investigative contexts. Nevertheless, even under the most challenging conditions and for the most affected subgroups, FRT accuracy remains substantially higher than that of many traditional forensic methods. This suggests that, if appropriately validated and regulated, FRT should be considered a valuable investigative tool. However, algorithmic accuracy alone is not sufficient: we must also evaluate how FRT is used in practice, including user-driven data manipulation. Such cases underscore the need for transparency and oversight in FRT deployment to ensure both fairness and forensic validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14320v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar (James), Hon Kiu (James),  To, Arush Mehrotra</dc:creator>
    </item>
    <item>
      <title>Complexity of frequency fluctuations and the interpretive style in the bass viola da gamba</title>
      <link>https://arxiv.org/abs/2505.14448</link>
      <description>arXiv:2505.14448v1 Announce Type: cross 
Abstract: Audio signals in a set of musical pieces are modeled as a complex network for studying the relationship between the complexity of frequency fluctuations and the interpretive style of the bass viola da gamba. Based on interdisciplinary scientific and music approaches, we compute the spectral decomposition and translated its frequency components to a network of sounds. We applied a best fit analysis for identifying the statistical distributions that describe more precisely the behavior of such frequencies and computed the centrality measures and identify cliques for characterizing such a network. Findings suggested statistical regularities in the type of statistical distribution that best describes frequency fluctuations. The centrality measure confirmed the most influential and stable group of sounds in a piece of music, meanwhile the identification of the largest clique indicated functional groups of sounds that interact closely for identifying the emergence of complex frequency fluctuations. Therefore, by modeling the sound as a complex network, we can clearly associate the presence of large-scale statistical regularities with the presence of similar frequency fluctuations related to different musical events played by a same musician.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14448v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Lugo, Martha G. Alatriste-Contreras, Rafael S\'anchez-Guevara</dc:creator>
    </item>
    <item>
      <title>Mixed additive modelling of global alien species co-invasions of plants and insects</title>
      <link>https://arxiv.org/abs/2304.00654</link>
      <description>arXiv:2304.00654v3 Announce Type: replace 
Abstract: Alien species refer to non-native species introduced by humans into an ecosystem, which can cause harm to the environment, economy, or human health. Although there is considerable literature on the subject, the presence of confounding factors has so far prevented a comprehensive picture of the relative importance of various drivers of such invasions. In this manuscript, we aim to develop and apply a general mixed additive relational event model to describe the pattern of global invasions of alien species. The diffusion of alien species can be regarded as a relational event, where the species -- the sender -- reaches a region -- the receiver -- at a specific time in history. We use the First Record Database, which contains all co-invasions by insects and plants between 1880 and 2005. A relational event model (REM) is employed to describe the underlying hazard of each species-region pair. Besides potentially time-varying, exogenous, and endogenous covariates, the mixed additive REM incorporates time-varying and random effects, allowing for taxa-specific baseline rates while accounting for the potential synergistic effect between plants and insects in the invasion process. Our efficient inference procedure relies on case-control sampling, yielding the same likelihood as that of a degenerate logistic regression. We propose fitting the mixed additive REM via a generalised additive model with random effects as 0-dimensional splines. The resulting computational efficiency means that complex models for large dynamic networks can be estimated in seconds on a standard computer. Furthermore, we present a framework for testing the goodness-of-fit of our mixed additive REM for the invasions by vascular plants and insects by means of cumulative martingale-residuals. Implementation is performed through the R package mgcv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00654v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, R\=uta Juozaitien\.e, Ernst-Jan Camiel Wit</dc:creator>
    </item>
    <item>
      <title>Evaluating Organizational Effectiveness: A New Strategy to Leverage Multisite Randomized Trials for Valid Assessment</title>
      <link>https://arxiv.org/abs/2407.18360</link>
      <description>arXiv:2407.18360v3 Announce Type: replace 
Abstract: In education, health, and human services, an intervention program is usually implemented by many local organizations. Determining which organizations are more effective is essential for theoretically characterizing effective practices and for intervening to enhance the capacity of ineffective organizations. In multisite randomized trials, site-specific intention-to-treat (ITT) effects are likely invalid indicators for organizational effectiveness and may lead to inequitable decisions. This is because sites differ in their local ecological conditions including client composition, alternative programs, and community context. Applying the potential outcomes framework, this study proposes a mathematical definition for the relative effectiveness of an organization. The estimand contrasts the performance of a focal organization with those that share the features of its local ecological conditions. The identification relies on relatively weak assumptions by leveraging observed control group outcomes that capture the confounding impacts of alternative programs and community context. We propose a two-step mixed-effects modeling (2SME) procedure. Simulations demonstrate significant improvements when compared with site-specific ITT analyses or analyses that only adjust for between-site differences in the observed baseline participant composition. We illustrate its use through an evaluation of the relative effectiveness of individual Job Corps centers by reanalyzing data from the National Job Corps Study, a multisite randomized trial that included 100 Job Corps centers nationwide serving disadvantaged youths. The new strategy promises to alleviate consequential misclassifications of some of the most effective Job Corps centers as least effective and vice versa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18360v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanglei Hong (University of Chicago), Jonah Deutsch (Mathematica), Peter Kress (Mathematica), Jose Eos Trinidad (University of California-Berkeley), Zhengyan Xu (University of Pennsylvania)</dc:creator>
    </item>
    <item>
      <title>The Neglected Error: False Negatives and the Case for Validating Eliminations</title>
      <link>https://arxiv.org/abs/2412.05398</link>
      <description>arXiv:2412.05398v2 Announce Type: replace 
Abstract: This article examines the overlooked risk of false negative errors arising from eliminations in forensic firearm comparisons. While recent reforms in forensic science have focused on reducing false positives, eliminations--often based on class characteristics or intuitive judgments--receive little empirical scrutiny despite their potential to exclude true sources. In cases involving a closed pool of suspects, eliminations can function as de facto identifications, introducing serious risk of error. A review of existing validity studies reveals that many report only false positive rates, failing to provide a complete assessment of method accuracy. This asymmetry is reinforced by professional guidelines, such as those from AFTE, and echoed in major government reports, including those from NAS and PCAST. The article argues that eliminations, like identifications, must be validated through rigorous testing and reported with transparent error rates. It further cautions against the use of "common sense" eliminations in the absence of empirical support and highlights the dangers of contextual bias when examiners are aware of investigative constraints. Five policy recommendations are proposed to improve the scientific treatment and legal interpretation of eliminations, including balanced reporting of false positive and false negative rates, validation of intuitive judgments, and clear warnings against using eliminations to infer guilt in closed-pool scenarios. Without reform, eliminations will continue to escape scrutiny, perpetuating unmeasured error and undermining the integrity of forensic conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05398v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar</dc:creator>
    </item>
    <item>
      <title>Automated flood detection from Sentinel-1 GRD time series using Bayesian analysis for change point problems</title>
      <link>https://arxiv.org/abs/2504.19526</link>
      <description>arXiv:2504.19526v2 Announce Type: replace 
Abstract: Flood monitoring using satellite imagery faces significant limitations in existing methods, particularly regarding training data requirements and distinguishing temporary inundation from permanent water bodies. This paper presents a novel approach using Bayesian analysis for change point problems (BCP) applied to Sentinel-1 SAR time series data, which automatically detects temporal discontinuities in backscatter patterns. Our method processes raw SAR data directly with minimal preprocessing and operates without requiring training data or ancillary information, offering significant advantages for operational flood mapping in diverse geographical contexts. We validate our approach using the UrbanSARFloods benchmark dataset across three distinct geographical settings (Weihui, China; Jubba, Somalia; and NovaKakhovka, Ukraine). Our BCP approach achieves F1 scores ranging from 0.41 to 0.76 (IoU: 0.25-0.61), significantly outperforming both Otsu's thresholding (F1: 0.03-0.12, IoU: 0.02-0.08) and Siamese convolutional neural network approaches (F1: 0.08-0.34, IoU: 0.05-0.24). Further analysis reveals exceptional performance in open areas with F1 scores of 0.47-0.81 (IoU: 0.31-0.68) and high recall (0.36-0.84), contrasted with substantially lower performance in urban areas, indicating a common challenge across current flood detection methods in complex environments. The proposed method's training-free nature enables immediate deployment to new regions without model retraining or adaptation, while its ability to differentiate flood inundation from permanent water bodies without ancillary data represents a significant methodological advancement in SAR-based flood detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19526v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Tomohiro Tanaka, Nifat Sultana</dc:creator>
    </item>
    <item>
      <title>RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate Markers Applied to Vaccinology</title>
      <link>https://arxiv.org/abs/2502.03030</link>
      <description>arXiv:2502.03030v2 Announce Type: replace-cross 
Abstract: In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralising antibody response. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03030v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Hughes, Layla Parast, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>Enhancing Gradient-based Discrete Sampling via Parallel Tempering</title>
      <link>https://arxiv.org/abs/2502.19240</link>
      <description>arXiv:2502.19240v2 Announce Type: replace-cross 
Abstract: While gradient-based discrete samplers are effective in sampling from complex distributions, they are susceptible to getting trapped in local minima, particularly in high-dimensional, multimodal discrete distributions, owing to the discontinuities inherent in these landscapes. To circumvent this issue, we combine parallel tempering, also known as replica exchange, with the discrete Langevin proposal and develop the Parallel Tempering enhanced Discrete Langevin Proposal (PTDLP), which are simulated at a series of temperatures. Significant energy differences prompt sample swaps, which are governed by a Metropolis criterion specifically designed for discrete sampling to ensure detailed balance is maintained. Additionally, we introduce an automatic scheme to determine the optimal temperature schedule and the number of chains, ensuring adaptability across diverse tasks with minimal tuning. Theoretically, we establish that our algorithm converges non-asymptotically to the target energy and exhibits faster mixing compared to a single chain. Empirical results further emphasize the superiority of our method in sampling from complex, multimodal discrete distributions, including synthetic problems, restricted Boltzmann machines, and deep energy-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19240v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luxu Liang, Yuhang Jia, Feng Zhou</dc:creator>
    </item>
    <item>
      <title>Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios</title>
      <link>https://arxiv.org/abs/2504.15268</link>
      <description>arXiv:2504.15268v3 Announce Type: replace-cross 
Abstract: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, thus enabling flexible, granular, and realistic scenarios. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15268v3</guid>
      <category>q-fin.RM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JD Opdyke</dc:creator>
    </item>
    <item>
      <title>Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems</title>
      <link>https://arxiv.org/abs/2505.10311</link>
      <description>arXiv:2505.10311v3 Announce Type: replace-cross 
Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic Gaussian diffusion processes due to the required inversion of covariance matrices in the denoising score matching training objective \cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion models, a novel framework based on stochastic differential equations that learns the Whitened Score function instead of the standard score. This approach circumvents covariance inversion, extending score-based DMs by enabling stable training of DMs on arbitrary Gaussian forward noising processes. WS DMs establish equivalence with flow matching for arbitrary Gaussian noise, allow for tailored spectral inductive biases, and provide strong Bayesian priors for imaging inverse problems with structured noise. We experiment with a variety of computational imaging tasks using the CIFAR and CelebA ($64\times64$) datasets and demonstrate that WS diffusion priors trained on anisotropic Gaussian noising processes consistently outperform conventional diffusion priors based on isotropic Gaussian noise. Our code is open-sourced at \href{https://github.com/jeffreyalido/wsdiffusion}{\texttt{github.com/jeffreyalido/wsdiffusion}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10311v3</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian</dc:creator>
    </item>
  </channel>
</rss>
