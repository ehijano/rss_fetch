<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:36:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust Maximum Likelihood Updating</title>
      <link>https://arxiv.org/abs/2504.17151</link>
      <description>arXiv:2504.17151v2 Announce Type: replace 
Abstract: There is a large body of evidence that decision makers frequently depart from Bayesian updating. This paper introduces a model, robust maximum likelihood (RML) updating, where deviations from Bayesian updating are due to multiple priors/ambiguity. Using the decision maker's prior and posteriors as the primitives of the analysis, I axiomatically characterize a representation where the decision maker's probability assessment can be described by a benchmark prior, which is interpreted as an initial best guess, and a set of plausible priors, which represents all the priors that cannot be ruled out. When new information is received, the decision maker revises her benchmark prior within the set of plausible priors via the maximum likelihood principle in a way that ensures maximally dynamically consistent behavior, and updates the new benchmark prior using Bayes' rule. I demonstrate how the set of plausible priors can be uniquely identified by comparing ex ante and ex post beliefs and show how most commonly observed updating biases can be accommodated within the model in a unified framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17151v2</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elchin Suleymanov</dc:creator>
    </item>
    <item>
      <title>Log-concave functions and transformations thereof</title>
      <link>https://arxiv.org/abs/2512.07768</link>
      <description>arXiv:2512.07768v2 Announce Type: replace 
Abstract: I summarize Bagnoli and Bergstrom (2005)'s review on log-concave functions, make several corrections, and augment the discussion with further results that can be useful in obtaining monotone hazard rate. I also provide an application of monopoly pricing, where strict logconcavity of demand curve implies strict concavity of the revenue function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07768v2</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dihan Zou</dc:creator>
    </item>
    <item>
      <title>Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis</title>
      <link>https://arxiv.org/abs/2510.01115</link>
      <description>arXiv:2510.01115v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01115v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Evan Heus, Rick Bookstaber, Dhruv Sharma</dc:creator>
    </item>
    <item>
      <title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title>
      <link>https://arxiv.org/abs/2512.07901</link>
      <description>arXiv:2512.07901v2 Announce Type: replace-cross 
Abstract: Von Neumann founded both game theory and the theory of self-reproducing automata, but the two programs never merged. This paper provides the synthesis. The Theory of Strategic Evolution analyzes strategic replicators: entities that optimize under resource constraints and spawn copies of themselves. We introduce Games with Endogenous Players (GEPs), where lineages (not instances) are the fundamental strategic units, and define Evolutionarily Stable Distributions of Intelligence (ESDIs) as the resulting equilibrium concept.
  The central mathematical object is a hierarchy of strategic layers linked by cross-level gain matrices. Under a small-gain condition (spectral radius less than one), the system admits a global Lyapunov function at every finite depth. We prove closure under meta-selection: adding governance levels, innovation, or constitutional evolution preserves the dynamical structure. The Alignment Impossibility Theorem shows that unrestricted self-modification destroys this structure; stable alignment requires bounded modification classes.
  Applications include AI deployment dynamics, market concentration, and institutional design. The framework shows why personality engineering fails under selection pressure and identifies constitutional constraints necessary for stable multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07901v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin Vallier</dc:creator>
    </item>
    <item>
      <title>Spatial-Network Treatment Effects: A Continuous Functional Approach</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v3 Announce Type: replace-cross 
Abstract: This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
