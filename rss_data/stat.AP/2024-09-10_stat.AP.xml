<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:46:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Enhancing Electrocardiography Data Classification Confidence: A Robust Gaussian Process Approach (MuyGPs)</title>
      <link>https://arxiv.org/abs/2409.04642</link>
      <description>arXiv:2409.04642v1 Announce Type: new 
Abstract: Analyzing electrocardiography (ECG) data is essential for diagnosing and monitoring various heart diseases. The clinical adoption of automated methods requires accurate confidence measurements, which are largely absent from existing classification methods. In this paper, we present a robust Gaussian Process classification hyperparameter training model (MuyGPs) for discerning normal heartbeat signals from the signals affected by different arrhythmias and myocardial infarction. We compare the performance of MuyGPs with traditional Gaussian process classifier as well as conventional machine learning models, such as, Random Forest, Extra Trees, k-Nearest Neighbors and Convolutional Neural Network. Comparing these models reveals MuyGPs as the most performant model for making confident predictions on individual patient ECGs. Furthermore, we explore the posterior distribution obtained from the Gaussian process to interpret the prediction and quantify uncertainty. In addition, we provide a guideline on obtaining the prediction confidence of the machine learning models and quantitatively compare the uncertainty measures of these models. Particularly, we identify a class of less-accurate (ambiguous) signals for further diagnosis by an expert.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04642v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ukamaka V. Nnyaba, Hewan M. Shemtaga, David W. Collins, Amanda L. Muyskens, Benjamin W. Priest, Nedret Billor</dc:creator>
    </item>
    <item>
      <title>A Multi-objective Economic Statistical Design of the CUSUM chart: NSGA II Approach</title>
      <link>https://arxiv.org/abs/2409.04673</link>
      <description>arXiv:2409.04673v1 Announce Type: new 
Abstract: This paper presents an approach for the economic statistical design of the Cumulative Sum (CUSUM) control chart in a multi-objective optimization framework. The proposed methodology integrates economic considerations with statistical aspects to optimize the design parameters like the sample size ($n$), sampling interval ($h$), and decision interval ($H$) of the CUSUM chart. The Non-dominated Sorting Genetic Algorithm II (NSGA II) is employed to solve the multi-objective optimization problem, aiming to minimize both the average cost per cycle ($C_E$) and the out-of-control Average Run Length ($ARL_\delta$) simultaneously. The effectiveness of the proposed approach is demonstrated through a numerical example by determining the optimized CUSUM chart parameters using NSGA II. Additionally, sensitivity analysis is conducted to assess the impact of variations in input parameters. The corresponding results indicate that the proposed methodology significantly reduces the expected cost per cycle by about 43\% when compared to the findings of the article by M. Lee in the year 2011. A more extensive comparison with respect to both $C_E$ and $ARL_\delta$ has also been provided for justifying the methodology proposed in this article. This highlights the practical relevance and potential of this study for the right application of the technique of the CUSUM chart for process control purposes in industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04673v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Sandeep, Arup Ranjan Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Privacy enhanced collaborative inference in the Cox proportional hazards model for distributed data</title>
      <link>https://arxiv.org/abs/2409.04716</link>
      <description>arXiv:2409.04716v1 Announce Type: new 
Abstract: Data sharing barriers are paramount challenges arising from multicenter clinical studies where multiple data sources are stored in a distributed fashion at different local study sites. Particularly in the case of time-to-event analysis when global risk sets are needed for the Cox proportional hazards model, access to a centralized database is typically necessary. Merging such data sources into a common data storage for a centralized statistical analysis requires a data use agreement, which is often time-consuming. Furthermore, the construction and distribution of risk sets to participating clinical centers for subsequent calculations may pose a risk of revealing individual-level information. We propose a new collaborative Cox model that eliminates the need for accessing the centralized database and constructing global risk sets but needs only the sharing of summary statistics with significantly smaller dimensions than risk sets. Thus, the proposed collaborative inference enjoys maximal protection of data privacy. We show theoretically and numerically that the new distributed proportional hazards model approach has little loss of statistical power when compared to the centralized method that requires merging the entire data. We present a renewable sieve method to establish large-sample properties for the proposed method. We illustrate its performance through simulation experiments and a real-world data example from patients with kidney transplantation in the Organ Procurement and Transplantation Network (OPTN) to understand the factors associated with the 5-year death-censored graft failure (DCGF) for patients who underwent kidney transplants in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04716v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengtong Hu, Xu Shi, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Moving from Machine Learning to Statistics: the case of Expected Points in American football</title>
      <link>https://arxiv.org/abs/2409.04889</link>
      <description>arXiv:2409.04889v1 Announce Type: new 
Abstract: Expected points is a value function fundamental to player evaluation and strategic in-game decision-making across sports analytics, particularly in American football. To estimate expected points, football analysts use machine learning tools, which are not equipped to handle certain challenges. They suffer from selection bias, display counter-intuitive artifacts of overfitting, do not quantify uncertainty in point estimates, and do not account for the strong dependence structure of observational football data. These issues are not unique to American football or even sports analytics; they are general problems analysts encounter across various statistical applications, particularly when using machine learning in lieu of traditional statistical models. We explore these issues in detail and devise expected points models that account for them. We also introduce a widely applicable novel methodological approach to mitigate overfitting, using a catalytic prior to smooth our machine learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04889v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ryan Yee, Sameer K. Deshpande, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Estimating velocities of infectious disease spread through spatio-temporal log-Gaussian Cox point processes</title>
      <link>https://arxiv.org/abs/2409.05036</link>
      <description>arXiv:2409.05036v1 Announce Type: new 
Abstract: Understanding the spread of infectious diseases such as COVID-19 is crucial for informed decision-making and resource allocation. A critical component of disease behavior is the velocity with which disease spreads, defined as the rate of change between time and space. In this paper, we propose a spatio-temporal modeling approach to determine the velocities of infectious disease spread. Our approach assumes that the locations and times of people infected can be considered as a spatio-temporal point pattern that arises as a realization of a spatio-temporal log-Gaussian Cox process. The intensity of this process is estimated using fast Bayesian inference by employing the integrated nested Laplace approximation (INLA) and the Stochastic Partial Differential Equations (SPDE) approaches. The velocity is then calculated using finite differences that approximate the derivatives of the intensity function. Finally, the directions and magnitudes of the velocities can be mapped at specific times to examine better the spread of the disease throughout the region. We demonstrate our method by analyzing COVID-19 spread in Cali, Colombia, during the 2020-2021 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05036v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Jorge Mateu, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Rating Players of Counter-Strike: Global Offensive Based on Plus/Minus value</title>
      <link>https://arxiv.org/abs/2409.05052</link>
      <description>arXiv:2409.05052v1 Announce Type: new 
Abstract: We propose a player rating mechanism for Counter-Strike: Global Offensive (CS ), a popular e-sport, by analyzing players' Plus/Minus values. The Plus/Minus value represents the average point difference between a player's team and the opponent's team across all matches the player has participated in. Using models such as regularized linear regression, logistic regression, and Bayesian linear models, we examine the relationship between player participation and team point differences. The most commonly used metric in the CS community is "Rating 2.0," which focuses solely on individual performance and does not account for indirect contributions to team success. Our approach introduces a new rating system that evaluates both direct and indirect contributions of players, prioritizing those who make a tangible impact on match outcomes rather than those with the highest individual scores. This rating system could help teams distribute rewards more fairly and improve player recruitment. We believe this methodology will positively influence not only the CS community but also the broader e-sports industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05052v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyu Xu, Sarat Moka</dc:creator>
    </item>
    <item>
      <title>Enhancing Empathic Accuracy: Penalized Functional Alignment Method to Correct Misalignment in Emotional Perception</title>
      <link>https://arxiv.org/abs/2409.05343</link>
      <description>arXiv:2409.05343v1 Announce Type: new 
Abstract: Empathic accuracy (EA) is the ability of one person to accurately understand thoughts and feelings of another person, which is crucial for social and psychological interactions. Traditionally, EA is measured by comparing perceivers` real-time ratings of emotional states with the target`s self--evaluation. However, these analyses often ignore or simplify misalignments between ratings (such as assuming a fixed delay), leading to biased EA measures. We introduce a novel alignment method that accommodates diverse misalignment patterns, using the square--oot velocity representation to decompose ratings into amplitude and phase components. Additionally, we incorporate a regularization term to prevent excessive alignment by constraining temporal shifts within plausible human perception bounds. The overall alignment method is implemented effectively through a constrained dynamic programming algorithm. We demonstrate the superior performance of our method through simulations and real-world applications to video and music datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05343v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linh H Nghiem, Jing Cao, Chul Moon</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Framework for Estimating Aircraft Fuel Consumption Based on Flight Trajectories</title>
      <link>https://arxiv.org/abs/2409.05429</link>
      <description>arXiv:2409.05429v2 Announce Type: new 
Abstract: Accurate calculation of aircraft fuel consumption plays an irreplaceable role in flight operations, optimization, and pollutant accounting. Calculating aircraft fuel consumption accurately is tricky because it changes based on different flying conditions and physical factors. Utilizing flight surveillance data, this study developed a comprehensive mathematical framework and established a link between flight dynamics and fuel consumption, providing a set of high-precision, high-resolution fuel calculation methods. It also allows other practitioners to select data sources according to specific needs through this framework. The methodology begins by addressing the functional aspects of interval fuel consumption. We apply spectral transformation techniques to mine Automatic Dependent Surveillance-Broadcast (ADS-B) data, identifying key aspects of the flight profile and establishing their theoretical relationships with fuel consumption. Subsequently, a deep neural network with tunable parameters is used to fit this multivariate function, facilitating high-precision calculations of interval fuel consumption. Furthermore, a second-order smooth monotonic interpolation method was constructed along with a novel estimation method for instantaneous fuel consumption. Numerical results have validated the effectiveness of the model. Using ADS-B and Aircraft Communications Addressing and Reporting System (ACARS) data from 2023 for testing, the average error of interval fuel consumption can be reduced to as low as $3.31\%$, and the error in the integral sense of instantaneous fuel consumption is $8.86\%$. These results establish this model as the state of the art, achieving the lowest estimation errors in aircraft fuel consumption calculations to date.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05429v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linfeng Zhang, Alex Bian, Changmin Jiang, Lingxiao Wu</dc:creator>
    </item>
    <item>
      <title>Modeling the Spatial Distributions of Macro Base Stations with Homogeneous Density: Theory and Application to Real Networks</title>
      <link>https://arxiv.org/abs/2409.05468</link>
      <description>arXiv:2409.05468v1 Announce Type: new 
Abstract: Stochastic geometry is a highly studied field in telecommunications as in many other scientific fields. In the last ten years in particular, theoretical knowledge has evolved a lot, whether for the calculation of metrics to characterize interference, coverage, energy or spectral efficiency, or exposure to electromagnetic fields. Many spatial point process models have been developed but are often left aside because of their unfamiliarity, their lack of tractability in favor of the Poisson point process or the regular lattice, easier to use. This article is intended to be a short guide presenting a complete and simple methodology to follow to infer a real stationary macro antenna network using tractable spatial models. The focus is mainly on repulsive point processes and in particular on determinantal point processes which are among the most tractable repulsive point processes. This methodology is applied on Belgian and French cell towers. The results show that for all stationary distributions in France and Belgium, the best inference model is the $\beta$-Ginibre point process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05468v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>"Modeling the spatial distributions of macro base stations with homogeneous density: Theory and application to real networks," in Proc. of the European Cooperation in Science and Technology (EURO-COST CA20120 TD (22) 0174), Feb. 2022</arxiv:journal_reference>
      <dc:creator>Q. Gontier, C. Tsigros, F. Horlin, J. Wiart, C. Oestges, P. De Doncker</dc:creator>
    </item>
    <item>
      <title>Analyzing and Forecasting the Success in the Men's Ice Hockey World (Junior) Championships Using a Dynamic Ranking Model</title>
      <link>https://arxiv.org/abs/2409.05714</link>
      <description>arXiv:2409.05714v1 Announce Type: new 
Abstract: What factors contribute to the success of national teams in the Men's Ice Hockey World Championships and the Men's Ice Hockey World Junior Championships? This study examines whether hosting the tournament provides a home advantage; the influence of past tournament performances; the impact of players' physical characteristics such as height, weight, and age; and the value of experience from the World Championships compared to the NHL and other leagues. We employ a dynamic ranking model based on the Plackett-Luce distribution with time-varying strength parameters driven by the score. Additionally, we conduct a forecasting analysis to predict the probabilities of winning the tournament, earning a medal, and advancing to the playoff phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05714v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Establishing the Parallels and Differences Between Right-Censored and Missing Covariates</title>
      <link>https://arxiv.org/abs/2409.04684</link>
      <description>arXiv:2409.04684v1 Announce Type: cross 
Abstract: While right-censored time-to-event outcomes have been studied for decades, handling time-to-event covariates, also known as right-censored covariates, is now of growing interest. So far, the literature has treated right-censored covariates as distinct from missing covariates, overlooking the potential applicability of estimators to both scenarios. We bridge this gap by establishing connections between right-censored and missing covariates under various assumptions about censoring and missingness, allowing us to identify parallels and differences to determine when estimators can be used in both contexts. These connections reveal adaptations to five estimators for right-censored covariates in the unexplored area of informative covariate right-censoring and to formulate a new estimator for this setting, where the event time depends on the censoring time. We establish the asymptotic properties of the six estimators, evaluate their robustness under incorrect distributional assumptions, and establish their comparative efficiency. We conducted a simulation study to confirm our theoretical results, and then applied all estimators to a Huntington disease observational study to analyze cognitive impairments as a function of time to clinical diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04684v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus E. Vazquez, Marissa C. Ashner, Yanyuan Ma, Karen Marder, Tanya P. Garcia</dc:creator>
    </item>
    <item>
      <title>Data driven synthetic wavefront generation for boundary layer data</title>
      <link>https://arxiv.org/abs/2409.04873</link>
      <description>arXiv:2409.04873v1 Announce Type: cross 
Abstract: Disturbances such as atmospheric turbulence and aero-optic effects lead to wavefront aberrations, which degrade performance in imaging and laser propagation applications. Adaptive optics (AO) provide a method to mitigate these effects by pre-compensating the wavefront before propagation. However, development and testing of AO systems requires wavefront aberration data, which is difficult and expensive to obtain. Simulation methods can be used to generate such data less expensively. For atmospheric turbulence, the Kolmogorov-Taylor model provides a well-defined power spectrum that can be combined with the well-known angular spectrum method to generate synthetic phase screens. However, as aero-optics cannot be similarly generalized, this process cannot be applied to aero-optically relevant phenomena. In this paper, we introduce ReVAR (Re-Whitened Vector Auto-Regression), a novel algorithm for data-driven aero-optic phase screen generation. ReVAR trains on an input time-series of spatially and temporally correlated wavefront images from experiment and then generates synthetic data that captures the statistics present in the experimental data. The first training step of ReVAR distills the input images to a set of prediction weights and residuals. A further step we call re-whitening uses a spatial principal component analysis (PCA) to whiten these residuals. ReVAR then uses a white noise generator and inverts the previous transformation to construct synthetic time-series of data. This algorithm is computationally efficient, able to generate arbitrarily long synthetic time-series, and produces high-quality results when tested on turbulent boundary layer (TBL) data measured from a wind tunnel experiment. Using measured data for training, the temporal power spectral density (TPSD) of data generated using ReVAR closely matches the TPSD of the experimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04873v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Utley, Gregery Buzzard, Charles Bouman, Matthew Kemnetz</dc:creator>
    </item>
    <item>
      <title>A response-adaptive multi-arm design for continuous endpoints based on a weighted information measure</title>
      <link>https://arxiv.org/abs/2409.04970</link>
      <description>arXiv:2409.04970v1 Announce Type: cross 
Abstract: Multi-arm trials are gaining interest in practice given the statistical and logistical advantages that they can offer. The standard approach is to use a fixed (throughout the trial) allocation ratio, but there is a call for making it adaptive and skewing the allocation of patients towards better performing arms. However, among other challenges, it is well-known that these approaches might suffer from lower statistical power. We present a response-adaptive design for continuous endpoints which explicitly allows to control the trade-off between the number of patients allocated to the 'optimal' arm and the statistical power. Such a balance is achieved through the calibration of a tuning parameter, and we explore various strategies to effectively select it. The proposed criterion is based on a context-dependent information measure which gives a greater weight to those treatment arms which have characteristics close to a pre-specified clinical target. We also introduce a simulation-based hypothesis testing procedure which focuses on selecting the target arm, discussing strategies to effectively control the type-I error rate. The potential advantage of the proposed criterion over currently used alternatives is evaluated in simulations, and its practical implementation is illustrated in the context of early Phase IIa proof-of-concept oncology clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04970v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianmarco Caruso, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Forecasting Age Distribution of Deaths: Cumulative Distribution Function Transformation</title>
      <link>https://arxiv.org/abs/2409.04981</link>
      <description>arXiv:2409.04981v1 Announce Type: cross 
Abstract: Like density functions, period life-table death counts are nonnegative and have a constrained integral, and thus live in a constrained nonlinear space. Implementing established modelling and forecasting methods without obeying these constraints can be problematic for such nonlinear data. We introduce cumulative distribution function transformation to forecast the life-table death counts. Using the Japanese life-table death counts obtained from the Japanese Mortality Database (2024), we evaluate the point and interval forecast accuracies of the proposed approach, which compares favourably to an existing compositional data analytic approach. The improved forecast accuracy of life-table death counts is of great interest to demographers for estimating age-specific survival probabilities and life expectancy and actuaries for determining temporary annuity prices for different ages and maturities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04981v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Inference for Large Scale Regression Models with Dependent Errors</title>
      <link>https://arxiv.org/abs/2409.05160</link>
      <description>arXiv:2409.05160v1 Announce Type: cross 
Abstract: The exponential growth in data sizes and storage costs has brought considerable challenges to the data science community, requiring solutions to run learning methods on such data. While machine learning has scaled to achieve predictive accuracy in big data settings, statistical inference and uncertainty quantification tools are still lagging. Priority scientific fields collect vast data to understand phenomena typically studied with statistical methods like regression. In this setting, regression parameter estimation can benefit from efficient computational procedures, but the main challenge lies in computing error process parameters with complex covariance structures. Identifying and estimating these structures is essential for inference and often used for uncertainty quantification in machine learning with Gaussian Processes. However, estimating these structures becomes burdensome as data scales, requiring approximations that compromise the reliability of outputs. These approximations are even more unreliable when complexities like long-range dependencies or missing data are present. This work defines and proves the statistical properties of the Generalized Method of Wavelet Moments with Exogenous variables (GMWMX), a highly scalable, stable, and statistically valid method for estimating and delivering inference for linear models using stochastic processes in the presence of data complexities like latent dependence structures and missing data. Applied examples from Earth Sciences and extensive simulations highlight the advantages of the GMWMX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05160v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lionel Voirol, Haotian Xu, Yuming Zhang, Luca Insolia, Roberto Molinari, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials</title>
      <link>https://arxiv.org/abs/2409.05271</link>
      <description>arXiv:2409.05271v1 Announce Type: cross 
Abstract: The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited, largely due to the challenges associated with complex statistical modeling, the lack of practical tools, and the cognitive burden on experts required to quantify their uncertainty using probabilistic language. Additionally, existing methods do not address prior-posterior coherence, i.e., does the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflect the expert's actual posterior beliefs? We propose a new elicitation approach that seeks to ensure prior-posterior coherence and reduce the expert's cognitive burden. This is achieved by eliciting responses about the expert's envisioned posterior judgments under various potential data outcomes and inferring the prior distribution by minimizing the discrepancies between these responses and the expected responses obtained from the posterior distribution. The feasibility and potential value of the new approach are illustrated through an application to a real trial currently underway.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05271v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong</dc:creator>
    </item>
    <item>
      <title>Mixed additive modelling of global alien species co-invasions of plants and insects</title>
      <link>https://arxiv.org/abs/2304.00654</link>
      <description>arXiv:2304.00654v2 Announce Type: replace 
Abstract: Alien species refer to non-native species introduced into an ecosystem, potentially causing harm to the environment, economy, or human health. Presence of confounding factors has so far prevented a comprehensive picture of relative importance of various drivers of such invasions. In this manuscript, we aim to develop and apply a general mixed additive Relational Event Model (REM) to describe the pattern of global invasions of alien species. An alien species invasion can be seen as a relational event, where the species - sender - reaches a region - receiver - at a specific time in history. We consider the First Record Database and focus on co-invasions by insects and plants between 1880 and 2005. REM aims to describe underlying hazard of each sender-receiver pair. Besides potentially time-varying, exogenous, and endogenous covariates, our mixed additive REM incorporates time-varying and random effects. Our efficient inference procedure relies on case-control sampling, yielding the same likelihood as that of a degenerate logistic regression. Resulting computational efficiency means that complex models for large dynamic networks can be estimated in seconds on a standard computer. We also present a framework for testing the goodness-of-fit of REMs via cumulative martingale-residuals. Implementation is performed through R package mgcv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00654v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, R\=uta Juozaitien\.e, Ernst-Jan Camiel Wit</dc:creator>
    </item>
    <item>
      <title>Optimal Structured Matrix Approximation for Robustness to Incomplete Biosequence Data</title>
      <link>https://arxiv.org/abs/2310.15375</link>
      <description>arXiv:2310.15375v2 Announce Type: replace 
Abstract: We propose a general method for optimally approximating an arbitrary matrix $\mathbf{M}$ by a structured matrix $\mathbf{T}$ (circulant, Toeplitz/Hankel, etc.) and examine its use for estimating the spectra of genomic linkage disequilibrium matrices. This application is prototypical of a variety of genomic and proteomic problems that demand robustness to incomplete biosequence information. We perform a simulation study and corroborative test of our method using real genomic data from the Mouse Genome Database. The results confirm the predicted utility of the method and provide strong evidence of its potential value to a wide range of bioinformatics applications. Our optimal general matrix approximation method is expected to be of independent interest to an even broader range of applications in applied mathematics and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15375v2</guid>
      <category>stat.AP</category>
      <category>cs.DS</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TCBB.2024.3420903</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Computational Biology and Bioinformatics. 2024. (Early Access)</arxiv:journal_reference>
      <dc:creator>Chris Salahub, Jeffrey Uhlmann</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference General Procedures for A Single-subject Test Study</title>
      <link>https://arxiv.org/abs/2408.15419</link>
      <description>arXiv:2408.15419v2 Announce Type: replace 
Abstract: Abnormality detection in the identification of a single-subject which deviates from the majority of the dataset that comes from a control group is a critical problem. A common approach is to assume that the control group can be characterised in terms of standard Normal statistics and the detection of single abnormal subject is in that context. But in many situations the control group can not be described in terms of Gaussian statistics and the use of standard statistics is inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST), designed to mitigate the effects of skewness under the assumption that the dataset of control group comes from the skewed Student's \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through a series of simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in terms of accuracy. This is because BIGPAST can effectively reduce model misspecification errors under the skewed Student's \( t \) assumption. We apply BIGPAST to a MEG dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in the single-subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15419v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>Distribution-Based Sub-Population Selection (DSPS): A Method for in-Silico Reproduction of Clinical Trials Outcomes</title>
      <link>https://arxiv.org/abs/2409.00232</link>
      <description>arXiv:2409.00232v2 Announce Type: replace 
Abstract: Background and Objective: Diabetes presents a significant challenge to healthcare due to the negative impact of poor blood sugar control on health and associated complications. Computer simulation platforms, notably exemplified by the UVA/Padova Type 1 Diabetes simulator, has emerged as a promising tool for advancing diabetes treatments by simulating patient responses in a virtual environment. The UVA Virtual Lab (UVLab) is a new simulation platform to mimic the metabolic behavior of people with Type 2 diabetes (T2D) with a large population of 6062 virtual subjects. Methods: The work introduces the Distribution-Based Population Selection (DSPS) method, a systematic approach to identifying virtual subsets that mimic the clinical behavior observed in real trials. The method transforms the sub-population selection task into a Linear Programing problem, enabling the identification of the largest representative virtual cohort. This selection process centers on key clinical outcomes in diabetes research, such as HbA1c and Fasting plasma Glucose (FPG), ensuring that the statistical properties (moments) of the selected virtual sub-population closely resemble those observed in real-word clinical trial. Results: DSPS method was applied to the insulin degludec (IDeg) arm of a phase 3 clinical trial. This method was used to select a sub-population of virtual subjects that closely mirrored the clinical trial data across multiple key metrics, including glycemic efficacy, insulin dosages, and cumulative hypoglycemia events over a 26-week period. Conclusion: The DSPS algorithm is able to select virtual sub-population within UVLab to reproduce and predict the outcomes of a clinical trial. This statistical method can bridge the gap between large population simulation platforms and previously conducted clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00232v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Ganji, Anas El Fathi, Chiara Fabris, Dayu Lv, Boris Kovatchev, Marc Breton</dc:creator>
    </item>
    <item>
      <title>Tonal coarticulation revisited: functional covariance analysis to investigate the planning of co-articulated tones by Standard Chinese speakers</title>
      <link>https://arxiv.org/abs/2409.01194</link>
      <description>arXiv:2409.01194v2 Announce Type: replace 
Abstract: We aim to explain whether a stress memory task has a significant impact on tonal coarticulation. We contribute a novel approach to analyse tonal coarticulation in phonetics, where several f0 contours are compared with respect to their vibrations at higher resolution, something that in statistical terms is called variation of the second order. We identify speech recording frequency curves as functional observations and harness inspiration from the mathematical fields of functional data analysis and optimal transport. By leveraging results from these two disciplines, we make one key observation:we identify the time and frequency covariance functions as crucial features for capturing the finer effects of tonal coarticulation. This observation leads us to propose a 2 steps approach where the mean functions are modelled via Generalized Additive Models, and the residuals of such models are investigated for any structure nested at covariance level. If such structure exist, we describe the variation manifested by the covariances through covariance principal component analysis. The 2-steps approach allows to uncover any variation not explained by generalized additive modelling, as well as fill a known shortcoming of these models into incorporating complex correlation structures in the data. The proposed method is illustrated on an articulatory dataset contrasting the pronunciation non-sensical bi-syllabic combinations in the presence of a short-memory challenge</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01194v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valentina Masarotto, Yiya Chen</dc:creator>
    </item>
    <item>
      <title>Intrinsic Bayesian Cram\'er-Rao Bound with an Application to Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2311.04748</link>
      <description>arXiv:2311.04748v3 Announce Type: replace-cross 
Abstract: This paper presents a new performance bound for estimation problems where the parameter to estimate lies in a Riemannian manifold (a smooth manifold endowed with a Riemannian metric) and follows a given prior distribution. In this setup, the chosen Riemannian metric induces a geometry for the parameter manifold, as well as an intrinsic notion of the estimation error measure. Performance bound for such error measure were previously obtained in the non-Bayesian case (when the unknown parameter is assumed to deterministic), and referred to as \textit{intrinsic} Cram\'er-Rao bound. The presented result then appears either as: \textit{a}) an extension of the intrinsic Cram\'er-Rao bound to the Bayesian estimation framework; \textit{b}) a generalization of the Van-Trees inequality (Bayesian Cram\'er-Rao bound) that accounts for the aforementioned geometric structures. In a second part, we leverage this formalism to study the problem of covariance matrix estimation when the data follow a Gaussian distribution, and whose covariance matrix is drawn from an inverse Wishart distribution. Performance bounds for this problem are obtained for both the mean squared error (Euclidean metric) and the natural Riemannian distance for Hermitian positive definite matrices (affine invariant metric). Numerical simulation illustrate that assessing the error with the affine invariant metric is revealing of interesting properties of the maximum a posteriori and minimum mean square error estimator, which are not observed when using the Euclidean metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04748v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</dc:creator>
    </item>
    <item>
      <title>Democratizing Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2402.13768</link>
      <description>arXiv:2402.13768v5 Announce Type: replace-cross 
Abstract: Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.
  In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13768v5</guid>
      <category>cs.MS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Linus Seelinger, Anne Reinarz, Mikkel B. Lykkegaard, Robert Akers, Amal M. A. Alghamdi, David Aristoff, Wolfgang Bangerth, Jean B\'en\'ezech, Matteo Diez, Kurt Frey, John D. Jakeman, Jakob S. J{\o}rgensen, Ki-Tae Kim, Benjamin M. Kent, Massimiliano Martinelli, Matthew Parno, Riccardo Pellegrini, Noemi Petra, Nicolai A. B. Riis, Katherine Rosenfeld, Andrea Serani, Lorenzo Tamellini, Umberto Villa, Tim J. Dodwell, Robert Scheichl</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v4 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>A Metric-based Principal Curve Approach for Learning One-dimensional Manifold</title>
      <link>https://arxiv.org/abs/2405.12390</link>
      <description>arXiv:2405.12390v3 Announce Type: replace-cross 
Abstract: Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12390v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Sisi Shao</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification under Noisy Constraints, with Applications to Raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v2 Announce Type: replace-cross 
Abstract: We consider statistical inference problems under uncertain equality constraints, and provide asymptotically valid uncertainty estimates for inferred parameters. The proposed approach leverages the implicit function theorem and primal-dual optimality conditions for a particular problem class. The motivating application is multi-dimensional raking, where observations are adjusted to match marginals; for example, adjusting estimated deaths across race, county, and cause in order to match state all-race all-cause totals. We review raking from a convex optimization perspective, providing explicit primal-dual formulations, algorithms, and optimality conditions for a wide array of raking applications, which are then leveraged to obtain the uncertainty estimates. Empirical results show that the approach obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and of marginal draws through the entire raking process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Upselling in Online Fantasy Sports</title>
      <link>https://arxiv.org/abs/2409.00629</link>
      <description>arXiv:2409.00629v2 Announce Type: replace-cross 
Abstract: This study explores the impact of upselling on user engagement. We model users' deposit behaviour on the fantasy sports platform Dream11. Subsequently, we develop an experimental framework to evaluate the effect of upselling using an intensity parameter. Our live experiments on user deposit behaviour reveal decreased user recall with heightened upselling intensity. Our findings indicate that increased upselling intensity improves user deposit metrics and concurrently diminishes user satisfaction and conversion rates. We conduct robust counterfactual analysis and train causal meta-learners to personalise users' upselling intensity levels to reach an optimal trade-off point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00629v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aayush Chaudhary</dc:creator>
    </item>
  </channel>
</rss>
