<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2024 04:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Boosting Earth System Model Outputs And Saving PetaBytes in their Storage Using Exascale Climate Emulators</title>
      <link>https://arxiv.org/abs/2408.04440</link>
      <description>arXiv:2408.04440v1 Announce Type: new 
Abstract: We present the design and scalable implementation of an exascale climate emulator for addressing the escalating computational and storage requirements of high-resolution Earth System Model simulations. We utilize the spherical harmonic transform to stochastically model spatio-temporal variations in climate data. This provides tunable spatio-temporal resolution and significantly improves the fidelity and granularity of climate emulation, achieving an ultra-high spatial resolution of 0.034 (approximately 3.5 km) in space. Our emulator, trained on 318 billion hourly temperature data points from a 35-year and 31 billion daily data points from an 83-year global simulation ensemble, generates statistically consistent climate emulations. We extend linear solver software to mixed-precision arithmetic GPUs, applying different precisions within a single solver to adapt to different correlation strengths. The PaRSEC runtime system supports efficient parallel matrix operations by optimizing the dynamic balance between computation, communication, and memory requirements. Our BLAS3-rich code is optimized for systems equipped with four different families and generations of GPUs, scaling well to achieve 0.976 EFlop/s on 9,025 nodes (36,100 AMD MI250X multichip module (MCM) GPUs) of Frontier (nearly full system), 0.739 EFlop/s on 1,936 nodes (7,744 Grace-Hopper Superchips (GH200)) of Alps, 0.243 EFlop/s on 1,024 nodes (4,096 A100 GPUs) of Leonardo, and 0.375 EFlop/s on 3,072 nodes (18,432 V100 GPUs) of Summit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04440v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sameh Abdulah, Allison H. Baker, George Bosilca, Qinglei Cao, Stefano Castruccio, Marc G. Genton, David E. Keyes, Zubair Khalid, Hatem Ltaief, Yan Song, Georgiy L. Stenchikov, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Testing for a general changepoint in psychometric studies: changes detection and sample size planning</title>
      <link>https://arxiv.org/abs/2408.04056</link>
      <description>arXiv:2408.04056v1 Announce Type: cross 
Abstract: This paper introduces a new method for change detection in psychometric studies based on the recently introduced pseudo Score statistic, for which the sampling distribution under the alternative hypothesis has been determined. Our approach has the advantage of simplicity in its computation, eliminating the need for resampling or simulations to obtain critical values. Additionally, it comes with a known null/alternative distribution, facilitating easy calculations for power levels and sample size planning. The paper indeed also discusses the topic of power analysis in segmented regression, namely the estimation of sample size or power level when the study data being collected focuses on a covariate expected to affect the mean response via a piecewise relationship with an unknown breakpoint. We run simulation results showing that our method outperforms other Tests for a Change Point (TFCP) with both normally distributed and binary data and carry out a real SAT Critical reading data analysis. The proposed test contributes to the framework of psychometric research, and it is available on the Comprehensive R Archive Network (CRAN) and in a more user-friendly Shiny App, both illustrated at the end of the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04056v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nicoletta D'Angelo</dc:creator>
    </item>
    <item>
      <title>BayesFBHborrow: An R Package for Bayesian borrowing for time-to-event data from a flexible baseline hazard</title>
      <link>https://arxiv.org/abs/2408.04327</link>
      <description>arXiv:2408.04327v1 Announce Type: cross 
Abstract: There is currently a focus on statistical methods which can use external trial information to help accelerate the discovery, development and delivery of medicine. Bayesian methods facilitate borrowing which is "dynamic" in the sense that the similarity of the data helps to determine how much information is used. We propose a Bayesian semiparameteric model, which allows the baseline hazard to take any form through an ensemble average. We introduce priors to smooth the posterior baseline hazard improving both model estimation and borrowing characteristics. A "lump-and-smear" borrowing prior accounts for non-exchangable historical data and helps reduce the maximum type I error in the presence of prior-data conflict. In this article, we present BayesFBHborrow, an R package, which enables the user to perform Bayesian borrowing with a historical control dataset in a semiparameteric time-to-event model. User-defined hyperparameters smooth an ensemble averaged posterior baseline hazard. The model offers the specification of lump-and-smear priors on the commensurability parameter where the associated hyperparameters can be chosen according to the users tolerance for difference between the log baseline hazards. We demonstrate the performance of our Bayesian flexible baseline hazard model on a simulated and real world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04327v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophia Axillus, Alex Lewin, Darren Scott</dc:creator>
    </item>
    <item>
      <title>Analysing symbolic data by pseudo-marginal methods</title>
      <link>https://arxiv.org/abs/2408.04419</link>
      <description>arXiv:2408.04419v1 Announce Type: cross 
Abstract: Symbolic data analysis (SDA) aggregates large individual-level datasets into a small number of distributional summaries, such as random rectangles or random histograms. Inference is carried out using these summaries in place of the original dataset, resulting in computational gains at the loss of some information. In likelihood-based SDA, the likelihood function is characterised by an integral with a large exponent, which limits the method's utility as for typical models the integral unavailable in closed form. In addition, the likelihood function is known to produce biased parameter estimates in some circumstances. Our article develops a Bayesian framework for SDA methods in these settings that resolves the issues resulting from integral intractability and biased parameter estimation using pseudo-marginal Markov chain Monte Carlo methods. We develop an exact but computationally expensive method based on path sampling and the block-Poisson estimator, and a much faster, but approximate, method based on Taylor expansion. Through simulation and real-data examples we demonstrate the performance of the developed methods, showing large reductions in computation time compared to the full-data analysis, with only a small loss of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04419v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Yang, Matias Quiroz, Boris Beranger, Robert Kohn, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>Statistical visualisation for tidy and geospatial data in R via kernel smoothing methods in the eks package</title>
      <link>https://arxiv.org/abs/2203.01686</link>
      <description>arXiv:2203.01686v4 Announce Type: replace 
Abstract: Kernel smoothers are essential tools for data analysis due to their ability to convey complex statistical information with concise graphical visualisations. Their inclusion in the base distribution and in the many user-contributed add-on packages of the R statistical analysis environment caters well to many practitioners. Though there remain some important gaps for specialised data, most notably for tidy and geospatial data. The proposed eks package fills in these gaps. In addition to kernel density estimation, this package also caters for more complex data analysis situations, such as density derivative estimation, density-based classification (supervised learning) and mean shift clustering (unsupervised learning). We illustrate with experimental data how to obtain and to interpret the statistical visualisations for these kernel smoothing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.01686v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
  </channel>
</rss>
