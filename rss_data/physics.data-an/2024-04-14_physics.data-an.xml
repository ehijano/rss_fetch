<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PINNACLE: PINN Adaptive ColLocation and Experimental points selection</title>
      <link>https://arxiv.org/abs/2404.07662</link>
      <description>arXiv:2404.07662v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interaction among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07662v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Kang Ruey Lau, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low</dc:creator>
    </item>
    <item>
      <title>An explicit form of the fundamental solution of the master equation for a jump-diffusion Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2301.13567</link>
      <description>arXiv:2301.13567v3 Announce Type: replace-cross 
Abstract: An integro-differential equation for the probability density of the generalized stochastic Ornstein-Uhlenbeck process with jump diffusion is considered. It is shown that for a certain ratio between the intensity of jumps and the speed of reversion, the fundamental solution can be found explicitly. The properties of this solution are investigated. The fundamental solution allows one to obtain explicit formulas for the densities at each moment of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13567v3</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga S. Rozanova, Nikolai A. Krutov</dc:creator>
    </item>
    <item>
      <title>Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data</title>
      <link>https://arxiv.org/abs/2307.16036</link>
      <description>arXiv:2307.16036v4 Announce Type: replace-cross 
Abstract: Researchers in the field of ultra-intense laser science are beginning to embrace machine learning methods. In this study we consider three different machine learning methods -- a two-hidden layer neural network, Support Vector Regression and Gaussian Process Regression -- and compare how well they can learn from a synthetic data set for proton acceleration in the Target Normal Sheath Acceleration regime. The synthetic data set was generated from a previously published theoretical model by Fuchs et al. 2005 that we modified. Once trained, these machine learning methods can assist with efforts to maximize the peak proton energy, or with the more general problem of configuring the laser system to produce a proton energy spectrum with desired characteristics. In our study we focus on both the accuracy of the machine learning methods and the performance on one GPU including the memory consumption. Although it is arguably the least sophisticated machine learning model we considered, Support Vector Regression performed very well in our tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16036v4</guid>
      <category>physics.plasm-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronak Desai, Thomas Zhang, Ricky Oropeza, John J. Felice, Joseph R. Smith, Alona Kryshchenko, Chris Orban, Michael L. Dexter, Anil K. Patnaik</dc:creator>
    </item>
  </channel>
</rss>
