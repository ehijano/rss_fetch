<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 01:32:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multivariate Long-term Profile Monitoring with Application to the KW51 Railway Bridge</title>
      <link>https://arxiv.org/abs/2506.20295</link>
      <description>arXiv:2506.20295v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) plays a pivotal role in modern civil engineering, providing critical insights into the health and integrity of infrastructure systems. This work presents a novel multivariate long-term profile monitoring approach to eliminate fluctuations in the measured response quantities, e.g., caused by environmental influences or measurement error. Our methodology addresses critical challenges in SHM and combines supervised methods with unsupervised, principal component analysis-based approaches in a single overarching framework, offering both flexibility and robustness in handling real-world large and/or sparse sensor data streams. We propose a function-on-function regression framework, which leverages functional data analysis for multivariate sensor data and integrates nonlinear modeling techniques, mitigating covariate-induced variations that can obscure structural changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20295v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Alexander Mendler, Sven Knoth, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Adaptive Supergeo Design: A Scalable Framework for Geographic Marketing Experiments</title>
      <link>https://arxiv.org/abs/2506.20499</link>
      <description>arXiv:2506.20499v1 Announce Type: new 
Abstract: Geographic experiments are a gold-standard for measuring incremental return on ad spend (iROAS) at scale, yet their design is challenging: the unit count is small, heterogeneity is large, and the optimal Supergeo partitioning problem is NP-hard. We introduce Adaptive Supergeo Design (ASD), a two-stage framework that renders Supergeo designs practical for thousands of markets. A bespoke graph-neural network first learns geo-embeddings and proposes a concise candidate set of 'supergeos'; a CP-SAT solver then selects a partition that balances both baseline outcomes and pre-treatment covariates believed to modify the treatment effect. We prove that ASD's objective value is within (1+epsilon) of the global optimum under mild community-structure assumptions. In simulations with up to 1,000 Designated Market Areas ASD completes in minutes on standard hardware, retains every media dollar, and cuts iROAS bias substantively relative to existing methods. ASD therefore turns geo-lift testing into a routine, scalable component of media planning while preserving statistical rigour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20499v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>Hurricane Impact Index for Assessing Direct and Indirect Hazards in Central America</title>
      <link>https://arxiv.org/abs/2506.19858</link>
      <description>arXiv:2506.19858v1 Announce Type: cross 
Abstract: Hurricanes rank among the most destructive natural hazards. They are complex phenomena that can cause both direct damage along their path and indirect impacts due to heavy rainfall and strong winds, with effects varying according to regional topography. In this paper, we propose a Hurricane Impact Index to assess both direct and indirect hazards, and we demonstrate its applicability to the Central American region. The index is constructed so that we can decompose these effects across multiple dimensions of time and space, enabling a detailed analysis of the intensity and distribution of hurricane impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19858v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manrique Camacho, Amanda Cede\~no, Luis A. Barboza, Shu Wei Chou-Chen, Mario J. G\'omez, Hugo G. Hidalgo</dc:creator>
    </item>
    <item>
      <title>Introducing RobustiPy: An efficient next generation multiversal library with model selection, averaging, resampling, and explainable artificial intelligence</title>
      <link>https://arxiv.org/abs/2506.19958</link>
      <description>arXiv:2506.19958v1 Announce Type: cross 
Abstract: We present RobustiPy, a next generation Python-based framework for model uncertainty quantification and multiverse analysis, released under the GNU GPL v3.0. Through the integration of efficient bootstrap-based confidence intervals, combinatorial exploration of dependent-variable specifications, model selection and averaging, and two complementary joint-inference routines, RobustiPy transcends existing uncertainty-quantification tools. Its design further supports rigorous out-of-sample evaluation and apportions the predictive contribution of each covariate. We deploy the library across five carefully constructed simulations and ten empirically grounded case studies drawn from high-impact literature and teaching examples, including a novel re-analysis of "unexplained discrepancies" in famous prior work. To illustrate its performance, we time-profile RobustiPy over roughly 672 million simulated linear regressions. These applications showcase how RobustiPy not only accelerates robust inference but also deepens our interpretive insight into model sensitivity across the vast analytical multiverse within which scientists operate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19958v1</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Valdenegro Ibarra, Jiani Yan, Duiyi Dai, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>Causal mediation analysis for longitudinal and survival data in continuous time using Bayesian non-parametric joint models</title>
      <link>https://arxiv.org/abs/2506.20058</link>
      <description>arXiv:2506.20058v1 Announce Type: cross 
Abstract: Observational cohort data is an important source of information for understanding the causal effects of treatments on survival and the degree to which these effects are mediated through changes in disease-related risk factors. However, these analyses are often complicated by irregular data collection intervals and the presence of longitudinal confounders and mediators. We propose a causal mediation framework that jointly models longitudinal exposures, confounders, mediators, and time-to-event outcomes as continuous functions of age. This framework for longitudinal covariate trajectories enables statistical inference even at ages where the subject's covariate measurements are unavailable. The observed data distribution in our framework is modeled using an enriched Dirichlet process mixture (EDPM) model. Using data from the Atherosclerosis Risk in Communities cohort study, we apply our methods to assess how medication -- prescribed to target cardiovascular disease (CVD) risk factors -- affects the time-to-CVD death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20058v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bhandari, Michael J. Daniels, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>E-STGCN: Extreme Spatiotemporal Graph Convolutional Networks for Air Quality Forecasting</title>
      <link>https://arxiv.org/abs/2411.12258</link>
      <description>arXiv:2411.12258v2 Announce Type: replace 
Abstract: Modeling and forecasting air quality is crucial for effective air pollution management and protecting public health. Air quality data, characterized by nonlinearity, nonstationarity, and spatiotemporal correlations, often include extreme pollutant levels in severely polluted cities (e.g., Delhi, the capital of India). This is ignored by various geometric deep learning models, such as Spatiotemporal Graph Convolutional Networks (STGCN), which are otherwise effective for spatiotemporal forecasting. This study develops an extreme value theory (EVT) guided modified STGCN model (E-STGCN) for air pollution data to incorporate extreme behavior across pollutant concentrations. E-STGCN combines graph convolutional networks for spatial modeling and EVT-guided long short-term memory units for temporal sequence learning. Along with spatial and temporal components, it incorporates a generalized Pareto distribution to capture the extreme behavior of different air pollutants and embed this information into the learning process. The proposal is then applied to analyze air pollution data of 37 monitoring stations across Delhi, India. The forecasting performance for different test horizons is compared to benchmark forecasters (both temporal and spatiotemporal). It is found that E-STGCN has consistent performance across all seasons. The robustness of our results has also been evaluated empirically. Moreover, combined with conformal prediction, E-STGCN can produce probabilistic prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12258v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhurima Panja, Tanujit Chakraborty, Anubhab Biswas, Soudeep Deb</dc:creator>
    </item>
    <item>
      <title>The Impact of Meteorological Factors on Crop Price Volatility in India: Case studies of Soybean and Brinjal</title>
      <link>https://arxiv.org/abs/2503.11690</link>
      <description>arXiv:2503.11690v3 Announce Type: replace 
Abstract: Climate is an evolving complex system with dynamic interactions and non-linear feedback mechanisms, shaping environmental and socio-economic outcomes. Crop production is highly sensitive to climatic fluctuations (and many other environmental, social and governance factors). This paper studies the price volatility of agricultural crops as influenced by meteorological variables, which is critical for agricultural planning, sustainable finance and policy-making. As case studies, we choose the two Indian states: Madhya Pradesh (for Soybean) and Odisha (for Brinjal/Eggplant). We employ an Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model to estimate the conditional volatility of the log returns from 2012 to 2024. We further explore the cross-correlations between price volatility and the meteorological variables followed by a Granger-causal test to analyze the causal effect of meteorological variables on the volatility. The Seasonal Auto-Regressive Integrated Moving Average with Exogenous Regressors (SARIMAX) and Long Short-Term Memory (LSTM) models are implemented as simple machine learning models of price volatility with meteorological factors as exogenous variables. Finally, to capture spatial dependencies in volatility across districts, we extend the analysis using a Conditional Autoregressive (CAR) model to construct monthly volatility surfaces that reflect both local price risk as well as geographic dependence. We believe, this paper will illustrate the usefulness of simple machine learning models in agricultural finance, and help the farmers to make informed decisions by considering climate patterns and making beneficial decisions with regard to crop rotation or allocations. In general, incorporating meteorological factors to assess agricultural performance could help to understand and reduce price volatility and possibly lead to economic stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11690v3</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashok Kumar, Abbinav Sankar Kailasam, Anish Rai, Manya Khanna, Sudeep Shukla, Sourish Das, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic Programming for Robot Manipulation Under Uncertainty</title>
      <link>https://arxiv.org/abs/2403.14488</link>
      <description>arXiv:2403.14488v3 Announce Type: replace-cross 
Abstract: Manipulation tasks require robots to reason about cause and effect when interacting with objects. Yet, many data-driven approaches lack causal semantics and thus only consider correlations. We introduce COBRA-PPM, a novel causal Bayesian reasoning architecture that combines causal Bayesian networks and probabilistic programming to perform interventional inference for robot manipulation under uncertainty. We demonstrate its capabilities through high-fidelity Gazebo-based experiments on an exemplar block stacking task, where it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%) and performs greedy next-best action selection with a 94.2% task success rate. We further demonstrate sim2real transfer on a domestic robot, showing effectiveness in handling real-world uncertainty from sensor noise and stochastic actions. Our generalised and extensible framework supports a wide range of manipulation scenarios and lays a foundation for future work at the intersection of robotics and causality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14488v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze</dc:creator>
    </item>
    <item>
      <title>Data Quality in Crowdsourcing and Spamming Behavior Detection</title>
      <link>https://arxiv.org/abs/2404.17582</link>
      <description>arXiv:2404.17582v2 Announce Type: replace-cross 
Abstract: As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators' consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency, and two metrics are developed to measure crowd workers' credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17582v2</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Ba, Michelle V. Mancenido, Erin K. Chiou, Rong Pan</dc:creator>
    </item>
  </channel>
</rss>
