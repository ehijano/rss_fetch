<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 02:32:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Quantifying the Speed-Up from Non-Reversibility in MCMC Tempering Algorithms</title>
      <link>https://arxiv.org/abs/2501.16506</link>
      <description>arXiv:2501.16506v1 Announce Type: new 
Abstract: We investigate the increase in efficiency of simulated and parallel tempering MCMC algorithms when using non-reversible updates to give them "momentum". By making a connection to a certain simple discrete Markov chain, we show that, under appropriate assumptions, the non-reversible algorithms still exhibit diffusive behaviour, just on a different time scale. We use this to argue that the optimally scaled versions of the non-reversible algorithms are indeed more efficient than the optimally scaled versions of their traditional reversible counterparts, but only by a modest speed-up factor of about 42%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16506v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gareth O. Roberts, Jeffrey S. Rosenthal</dc:creator>
    </item>
    <item>
      <title>Symmetric Perceptrons, Number Partitioning and Lattices</title>
      <link>https://arxiv.org/abs/2501.16517</link>
      <description>arXiv:2501.16517v1 Announce Type: new 
Abstract: The symmetric binary perceptron ($\mathrm{SBP}_{\kappa}$) problem with parameter $\kappa : \mathbb{R}_{\geq1} \to [0,1]$ is an average-case search problem defined as follows: given a random Gaussian matrix $\mathbf{A} \sim \mathcal{N}(0,1)^{n \times m}$ as input where $m \geq n$, output a vector $\mathbf{x} \in \{-1,1\}^m$ such that $$|| \mathbf{A} \mathbf{x} ||_{\infty} \leq \kappa(m/n) \cdot \sqrt{m}~.$$ The number partitioning problem ($\mathrm{NPP}_{\kappa}$) corresponds to the special case of setting $n=1$. There is considerable evidence that both problems exhibit large computational-statistical gaps.
  In this work, we show (nearly) tight average-case hardness for these problems, assuming the worst-case hardness of standard approximate shortest vector problems on lattices.
  For $\mathrm{SBP}$, for large $n$, the best that efficient algorithms have been able to achieve is $\kappa(x) = \Theta(1/\sqrt{x})$ (Bansal and Spencer, Random Structures and Algorithms 2020), which is a far cry from the statistical bound. The problem has been extensively studied in the TCS and statistics communities, and Gamarnik, Kizildag, Perkins and Xu (FOCS 2022) conjecture that Bansal-Spencer is tight: namely, $\kappa(x) = \widetilde{\Theta}(1/\sqrt{x})$ is the optimal value achieved by computationally efficient algorithms. We prove their conjecture assuming the worst-case hardness of approximating the shortest vector problem on lattices.
  For $\mathrm{NPP}$, Karmarkar and Karp's classical differencing algorithm achieves $\kappa(m) = 2^{-O(\log^2 m)}~.$ We prove that Karmarkar-Karp is nearly tight: namely, no polynomial-time algorithm can achieve $\kappa(m) = 2^{-\Omega(\log^3 m)}$, once again assuming the worst-case subexponential hardness of approximating the shortest vector problem on lattices to within a subexponential factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16517v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neekon Vafa, Vinod Vaikuntanathan</dc:creator>
    </item>
    <item>
      <title>Ancestral Inference and Learning for Branching Processes in Random Environments</title>
      <link>https://arxiv.org/abs/2501.16526</link>
      <description>arXiv:2501.16526v1 Announce Type: new 
Abstract: Ancestral inference for branching processes in random environments involves determining the ancestor distribution parameters using the population sizes of descendant generations. In this paper, we introduce a new methodology for ancestral inference utilizing the generalized method of moments. We demonstrate that the estimator's behavior is critically influenced by the coefficient of variation of the environment sequence. Furthermore, despite the process's evolution being heavily dependent on the offspring means of various generations, we show that the joint limiting distribution of the ancestor and offspring estimators of the mean, under appropriate centering and scaling, decouple and converge to independent Gaussian random variables when the ratio of the number of generations to the logarithm of the number of replicates converges to zero. Additionally, we provide estimators for the limiting variance and illustrate our findings through numerical experiments and data from Polymerase Chain Reaction experiments and COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16526v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoran Jiang, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>Consistent support recovery for high-dimensional diffusions</title>
      <link>https://arxiv.org/abs/2501.16703</link>
      <description>arXiv:2501.16703v1 Announce Type: new 
Abstract: Statistical inference for stochastic processes has advanced significantly due to applications in diverse fields, but challenges remain in high-dimensional settings where parameters are allowed to grow with the sample size. This paper analyzes a d-dimensional ergodic diffusion process under sparsity constraints, focusing on the adaptive Lasso estimator, which improves variable selection and bias over the standard Lasso. We derive conditions under which the adaptive Lasso achieves support recovery property and asymptotic normality for the drift parameter, with a focus on linear models. Explicit parameter relationships guide tuning for optimal performance, and a marginal estimator is proposed for p&gt;&gt;d scenarios under partial orthogonality assumption. Numerical studies confirm the adaptive Lasso's superiority over standard Lasso and MLE in accuracy and support recovery, providing robust solutions for high-dimensional stochastic processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16703v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmytro Marushkevych, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>The empirical median for estimating the common mean of heteroscedastic random variables</title>
      <link>https://arxiv.org/abs/2501.16956</link>
      <description>arXiv:2501.16956v1 Announce Type: new 
Abstract: We study the problem of mean estimation in the heteroscedastic setting. In particular, we consider symmetric random variables having the same location parameter and different and unknown scale parameters. Our goal is then to estimate their unknown common location parameter. It is an elementary topic but yet a not very well-studied one since we always make the assumption that the random variables are independent and identically distributed. In this paper, we study the median estimator and we establish upper and lower bounds on its estimation error that are of the same order and that generalize and improve recent results of Devroye et al. and Xia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16956v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirine Louati</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties and drift parameter estimations of the ergodic double Heston model based on continuous-time observations</title>
      <link>https://arxiv.org/abs/2501.17100</link>
      <description>arXiv:2501.17100v1 Announce Type: new 
Abstract: The double Heston model is one of the most popular option pricing models in financial theory. It is applied to several issues such that risk management and volatility surface calibration. This paper deals with the problem of global parameter estimations in this model. Our main stochastic results are about the stationarity and the ergodicity of the double Heston process. The statistical part of this paper is about the maximum likelihood and the conditional least squares estimations based on continuous-time observations; then for each estimation method, we study the asymptotic properties of the resulted estimators in the ergodic case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17100v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Houssem Dahbi, Hamdi Fathallah</dc:creator>
    </item>
    <item>
      <title>Hard edge asymptotics of correlation functions between singular values and eigenvalues</title>
      <link>https://arxiv.org/abs/2501.15765</link>
      <description>arXiv:2501.15765v1 Announce Type: cross 
Abstract: Any square complex matrix of size $n\times n$ can be partially characterized by its $n$ eigenvalues and/or $n$ singular values. While no one-to-one correspondence exists between those two kinds of values on a deterministic level, for random complex matrices drawn from a bi-unitarily invariant ensemble, a bijection exists between the underlying singular value ensemble and the corresponding eigenvalue ensemble. This enabled the recent finding of an explicit formula for the joint probability density between $1$ eigenvalue and $k$ singular values, coined $1,k$-point function. We derive here the large $n$ asymptotic of the $1,k$-point function around the origin (hard edge) for a large subclass of bi-unitarily invariant ensembles called P\'olya ensembles. This subclass contains all Meijer-G ensembles and, in particular, Muttalib-Borodin ensembles and the classical Wishart-Laguerre (complex Ginibre), Jacobi (truncated unitary), Cauchy-Lorentz ensembles. We show that the latter three ensembles share the same asymptotic of the $1,k$-point function around the origin. In the case of Jacobi ensembles, there exists another hard edge for the singular values, namely the upper edge of their support, which corresponds to a soft edge for the singular value (soft-hard edge). We give the explicit large $n$ asymptotic of the $1,k$-point function around this soft-hard edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15765v1</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Allard</dc:creator>
    </item>
    <item>
      <title>A New Proof for the Linear Filtering and Smoothing Equations, and Asymptotic Expansion of Nonlinear Filtering</title>
      <link>https://arxiv.org/abs/2501.16333</link>
      <description>arXiv:2501.16333v1 Announce Type: cross 
Abstract: In this paper, we propose a new approach to the linear filtering and smoothing problem and demonstrate its applicability to nonlinear filtering. For the linear case, our main theorem provides an explicit expression for the conditional distribution of the hidden process given the observations, leading to a novel derivation of the linear filtering and smoothing equations. Moreover, the theorem offers an efficient framework for computing the asymptotic expansion of nonlinear filtering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16333v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kurisaki</dc:creator>
    </item>
    <item>
      <title>Comparison theorems for the minimum eigenvalue of a random positive-semidefinite matrix</title>
      <link>https://arxiv.org/abs/2501.16578</link>
      <description>arXiv:2501.16578v1 Announce Type: cross 
Abstract: This paper establishes a new comparison principle for the minimum eigenvalue of a sum of independent random positive-semidefinite matrices. The principle states that the minimum eigenvalue of the matrix sum is controlled by the minimum eigenvalue of a Gaussian random matrix that inherits its statistics from the summands. This methodology is powerful because of the vast arsenal of tools for treating Gaussian random matrices. As applications, the paper presents short, conceptual proofs of some old and new results in high-dimensional statistics. It also settles a long-standing open question in computational linear algebra about the injectivity properties of very sparse random matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16578v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel A. Tropp</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v1 Announce Type: cross 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For \emph{hierarchical} multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches \cite{Pocock2011winratio,Buyse2010}. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. These methods are straightforward to implement, making them accessible to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Error estimation and adaptive tuning for unregularized robust M-estimator</title>
      <link>https://arxiv.org/abs/2312.13257</link>
      <description>arXiv:2312.13257v2 Announce Type: replace 
Abstract: We consider unregularized robust M-estimators for linear models under Gaussian design and heavy-tailed noise, in the proportional asymptotics regime where the sample size n and the number of features p are both increasing such that $p/n \to \gamma\in (0,1)$. An estimator of the out-of-sample error of a robust M-estimator is analyzed and proved to be consistent for a large family of loss functions that includes the Huber loss. As an application of this result, we propose an adaptive tuning procedure of the scale parameter $\lambda&gt;0$ of a given loss function $\rho$: choosing $\hat \lambda$ in a given interval $I$ that minimizes the out-of-sample error estimate of the M-estimator constructed with loss $\rho_\lambda(\cdot) = \lambda^2 \rho(\cdot/\lambda)$ leads to the optimal out-of-sample error over $I$. The proof relies on a smoothing argument: the unregularized M-estimation objective function is perturbed, or smoothed, with a Ridge penalty that vanishes as $n\to+\infty$, and shows that the unregularized M-estimator of interest inherits properties of its smoothed version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13257v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>Bubble Modeling and Tagging: A Stochastic Nonlinear Autoregression Approach</title>
      <link>https://arxiv.org/abs/2401.07038</link>
      <description>arXiv:2401.07038v2 Announce Type: replace 
Abstract: Economic and financial time series can feature locally explosive behavior when a bubble is formed. The economic or financial bubble, especially its dynamics, is an intriguing topic that has been attracting longstanding attention. To illustrate the dynamics of the local explosion itself, the paper presents a novel, simple, yet useful time series model, called the stochastic nonlinear autoregressive model, which is always strictly stationary and geometrically ergodic and can create long swings or persistence observed in many macroeconomic variables. When a nonlinear autoregressive coefficient is outside of a certain range, the model has periodically explosive behaviors and can then be used to portray the bubble dynamics. Further, the quasi-maximum likelihood estimation (QMLE) of our model is considered, and its strong consistency and asymptotic normality are established under minimal assumptions on innovation. A new model diagnostic checking statistic is developed for model fitting adequacy. In addition, two methods for bubble tagging are proposed, one from the residual perspective and the other from the null-state perspective. Monte Carlo simulation studies are conducted to assess the performances of the QMLE and the two bubble tagging methods in finite samples. Finally, the usefulness of the model is illustrated by an empirical application to the monthly Hang Seng Index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07038v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xuanling Yang, Dong Li, Ting Zhang</dc:creator>
    </item>
    <item>
      <title>Causal clustering: design of cluster experiments under network interference</title>
      <link>https://arxiv.org/abs/2310.14983</link>
      <description>arXiv:2310.14983v3 Announce Type: replace-cross 
Abstract: This paper studies the design of cluster experiments to estimate the global treatment effect in the presence of network spillovers. We provide a framework to choose the clustering that minimizes the worst-case mean-squared error of the estimated global effect. We show that optimal clustering solves a novel penalized min-cut optimization problem computed via off-the-shelf semi-definite programming algorithms. Our analysis also characterizes simple conditions to choose between any two cluster designs, including choosing between a cluster or individual-level randomization. We illustrate the method's properties using unique network data from the universe of Facebook's users and existing data from a field experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14983v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davide Viviano, Lihua Lei, Guido Imbens, Brian Karrer, Okke Schrijvers, Liang Shi</dc:creator>
    </item>
    <item>
      <title>Two-sample inference for sparse functional data</title>
      <link>https://arxiv.org/abs/2312.07727</link>
      <description>arXiv:2312.07727v3 Announce Type: replace-cross 
Abstract: We propose a novel test procedure for comparing mean functions across two groups within the reproducing kernel Hilbert space (RKHS) framework. Our proposed method is adept at handling sparsely and irregularly sampled functional data when observation times are random for each subject. Conventional approaches, which are built upon functional principal components analysis, usually assume a homogeneous covariance structure across groups. Nonetheless, justifying this assumption in real-world scenarios can be challenging. To eliminate the need for a homogeneous covariance structure, we first develop a linear approximation for the mean estimator under the RKHS framework; this approximation is a sum of i.i.d. random elements, which naturally leads to the desirable pointwise limiting distributions. Moreover, we establish weak convergence for the mean estimator, allowing us to construct a test statistic for the mean difference. Our method is easily implementable and outperforms some conventional tests in controlling type I errors across various settings. We demonstrate the finite sample performance of our approach through extensive simulations and two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07727v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
  </channel>
</rss>
