<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Acceptance or Rejection of Lots while Minimizing and Controlling Type I and Type II Errors</title>
      <link>https://arxiv.org/abs/2503.14514</link>
      <description>arXiv:2503.14514v1 Announce Type: new 
Abstract: The double hypothesis test (DHT) is a test that allows controlling Type I (producer) and Type II (consumer) errors. It is possible to say whether the batch has a defect rate, p, between 1.5 and 2%, or between 2 and 5%, or between 5 and 10%, and so on, until finding a required value for this probability. Using the two probabilities side by side, the Type I error for the lower probability distribution and the Type II error for the higher probability distribution, both can be controlled and minimized. It can be applied in the development or manufacturing process of a batch of components, or in the case of purchasing from a supplier, when the percentage of defects (p) is unknown, considering the technology and/or process available to obtain them. The power of the test is amplified by the joint application of the Limit of Successive Failures (LSF) related to the Renewal Theory. To enable the choice of the most appropriate algorithm for each application. Four distributions are proposed for the Bernoulli event sequence, including their computational efforts: Binomial, Binomial approximated by Poisson, and Binomial approximated by Gaussian (with two variants). Fuzzy logic rules are also applied to facilitate decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14514v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edson Luiz Ursini, Elaine Cristina Catapani Poletti, Loreno Menezes da Silveira, Jos\'e Roberto Emiliano Leite</dc:creator>
    </item>
    <item>
      <title>Bayesian Sociality Models: A Scalable and Flexible Alternative for Network Analysis</title>
      <link>https://arxiv.org/abs/2503.14697</link>
      <description>arXiv:2503.14697v1 Announce Type: new 
Abstract: Bayesian sociality models provide a scalable and flexible alternative for network analysis, capturing degree heterogeneity through actor-specific parameters while mitigating the identifiability challenges of latent space models. This paper develops a comprehensive Bayesian inference framework, leveraging Markov chain Monte Carlo and variational inference to assess their efficiency-accuracy trade-offs. Through empirical and simulation studies, we demonstrate the model's robustness in goodness-of-fit, predictive performance, clustering, and other key network analysis tasks. The Bayesian paradigm further enhances uncertainty quantification and interpretability, positioning sociality models as a powerful and generalizable tool for modern network science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14697v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carlo Mart\'inez</dc:creator>
    </item>
    <item>
      <title>PSInference: A Package to Draw Inference for Released Plug-in Sampling Single Synthetic Dataset</title>
      <link>https://arxiv.org/abs/2503.14711</link>
      <description>arXiv:2503.14711v1 Announce Type: new 
Abstract: The development and generation of synthetic data are becoming increasingly vital in the field of statistical disclosure control. The PSInference package provides tools to perform exact inferential analysis on singly imputed synthetic data generated through Plug-in Sampling assuming that the original dataset follows a multivariate normal distribution. Includes functions to test the synthetic data's covariance structure, covering aspects like generalized variance, sphericity, independence between subsets of variables, and regression of one set of variables on another. This package addresses the gap in the existing software by providing exact inferential methods suitable for cases where only a single synthetic dataset is released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14711v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Moura, Mina Norouzirad, Vitor Augusto, Miguel Fonseca</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for case-control point pattern data in spatial epidemiology with the \texttt{inlabru} R package</title>
      <link>https://arxiv.org/abs/2503.14954</link>
      <description>arXiv:2503.14954v1 Announce Type: new 
Abstract: The analysis of case-control point pattern data is an important problem in spatial epidemiology. The spatial variation of cases if often compared to that of a set of controls to assess spatial risk variation as well as the detection of risk factors and exposure to putative pollution sources using spatial regression models. The intensities of the point patterns of cases and controls are estimated using log-Gaussian Cox models, so that fixed and spatial random effects can be included. Bayesian inference is conducted via the integrated Nested Laplace approximation (INLA) method using the inlabru R package. In this way, potential risk factors can be assessed by including them as fixed effects while residual spatial variation is considered as a Gaussian process with Mat\'ern covariance. In addition, exposure to pollution sources is modeled using different smooth terms. The proposed methods have been applied to the Chorley-Ribble dataset, that records the locations of lung and larynx cancer cases as well as the location of an disused old incinerator in the area of Lancashire (England, United Kingdom). Taking the locations of lung cancer as controls, the spatial variation of both types of cases has been estimated and the increase of larynx cases in the vicinity of the incinerator has been assessed. The results are similar to those found in the literature. In a nutshell, a framework for Bayesian analysis of multivariate case-control point patterns within an epidemiological framework has been presented. Models to assess spatial variation and the effect of risk factors and pollution sources can be fit with ease with the inlabru R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14954v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Palm\'i-Perales, Finn Lindgren, Virgilio G\'omez-Rubio</dc:creator>
    </item>
    <item>
      <title>Benchmarking Brain Connectivity Graph Inference: A Novel Validation Approach</title>
      <link>https://arxiv.org/abs/2503.15012</link>
      <description>arXiv:2503.15012v1 Announce Type: new 
Abstract: Inferring a binary connectivity graph from resting-state fMRI data for a single subject requires making several methodological choices and assumptions that can significantly affect the results. In this study, we investigate the robustness of existing edge detection methods when relaxing a common assumption: the sparsity of the graph. We propose a new pipeline to generate synthetic data and to benchmark the state of the art in graph inference. Simulated correlation matrices are designed to have a set of given zeros and a constraint on the signal-to-noise ratio. We compare approaches based on covariance or precision matrices, emphasizing their implications for connectivity inference. This framework allows us to assess the sensitivity of connectivity estimations and edge detection methods to different parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15012v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Chevaux (STATIFY), Ali Fahkar (STATIFY), K\'evin Polisano (SVH), Ir\`ene Gannaz (G-SCOP\_GROG, G-SCOP), Sophie Achard (STATIFY, LJK)</dc:creator>
    </item>
    <item>
      <title>A Bivariate Poisson-Gamma Distribution: Statistical Properties and Practical Applications</title>
      <link>https://arxiv.org/abs/2503.15062</link>
      <description>arXiv:2503.15062v1 Announce Type: new 
Abstract: Although the specification of bivariate probability models using a collection of assumed conditional distributions is not a novel concept, it has received considerable attention in the last decade. In this study, a bivariate distribution-the bivariate Poisson-Gamma conditional distribution-is introduced, combining both univariate continuous and discrete distributions. This work explores aspects of this model's structure and statistical inference that have not been studied before. This paper contributes to the field of statistical modeling and distribution theory through the use of maximum likelihood estimation, along with simulations and analyses of real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15062v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Indranil Ghosh, Mina Norouzirad, Filipe J. Marques</dc:creator>
    </item>
    <item>
      <title>Sparse canonical correlation analysis for multiple measurements with latent trajectories</title>
      <link>https://arxiv.org/abs/2503.15140</link>
      <description>arXiv:2503.15140v1 Announce Type: new 
Abstract: Canonical Correlation Analysis, CCA, is a widely used multivariate method in omics research for integrating high dimensional datasets. CCA identifies hidden links by deriving linear projections of features maximally correlating datasets. For standard CCA, observations must be independent of each other. As a result, it cannot properly deal with repeated measurements.
  Current CCA extensions dealing with these challenges either perform CCA on summarized data or estimate correlations for each measurement. While these techniques factor in the correlation between measurements, they are sub-optimal for high dimensional analysis and exploiting this datas longitudinal qualities.
  We propose a novel extension of sparse CCA that incorporates time dynamics at the latent level through longitudinal models. This approach addresses the correlation of repeated measurements while drawing latent paths, focusing on dynamics in the correlation structures. To aid interpretability and computational efficiency, we implement a penalty to enforce fixed sparsity levels.
  We estimate these trajectories fitting longitudinal models to the low dimensional latent variables, leveraging the clustered structure of high dimensional datasets, thus exploring shared longitudinal latent mechanisms. Furthermore, modeling time in the latent space significantly reduces computational burden. We validate our models performance using simulated data and show its real world applicability with data from the Human Microbiome Project.
  Our CCA method for repeated measurements enables efficient estimation of canonical correlations across measurements for clustered data. Compared to existing methods, ours substantially reduces computational time in high dimensional analyses as well as provides longitudinal trajectories that yield interpretable and insightful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15140v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuria Senar, Aeilko H. Zwinderman, Michel H. Hof and</dc:creator>
    </item>
    <item>
      <title>A time-to-event three-outcome design for randomized phase II cancer trials</title>
      <link>https://arxiv.org/abs/2503.15418</link>
      <description>arXiv:2503.15418v1 Announce Type: new 
Abstract: Tumor response, a binary variable, has historically been the main measure of antitumor activity for many cancer phase II single-arm trials. Simon two-stage designs are often used. Sargent et al. proposed a three-outcome trial design in this setting which requires smaller sample sizes. For many new, molecularly targeted therapies, however, tumor response may not be the most reliable endpoint for measuring anti-tumor activity. Increasingly, time-to-event endpoints, such as progression-free survival (PFS), are used in the phase II setting. When such endpoints are the primary measure of efficacy, a randomized concurrently controlled study design is usually required. Given limited resources for phase II, studies are often underpowered with relatively large type I and II error rates, and it is sometimes unavoidable to have a "gray" decision zone after phase II where a clear decision regarding further development actions cannot be made without additional information. Compared with an underpowered standard two-outcome study, a three-outcome design prompts clinical trialists to contemplate the likelihood of landing in the "gray" zone at the trial design stage and choose study design parameters more appropriately. We propose a three-outcome design, with or without interim analyses, for randomized comparative phase II trials when a time-to-event endpoint is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15418v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghua Shan</dc:creator>
    </item>
    <item>
      <title>An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts</title>
      <link>https://arxiv.org/abs/2503.15436</link>
      <description>arXiv:2503.15436v1 Announce Type: new 
Abstract: Despite the accelerating presence of exploratory causal analysis in modern science and medicine, the available non-experimental methods for validating causal models are not well characterized. One of the most popular methods is to evaluate the stability of model features after resampling the data, similar to resampling methods for estimating confidence intervals in statistics. Many aspects of this approach have received little to no attention, however, such as whether the choice of resampling method should depend on the sample size, algorithms being used, or algorithm tuning parameters. We present theoretical results proving that certain resampling methods closely emulate the assignment of specific values to algorithm tuning parameters. We also report the results of extensive simulation experiments, which verify the theoretical result and provide substantial data to aid researchers in further characterizing resampling in the context of causal discovery analysis. Together, the theoretical work and simulation results provide specific guidance on how resampling methods and tuning parameters should be selected in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15436v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwick Banerjee, Bryan Andrews, Erich Kummerfeld</dc:creator>
    </item>
    <item>
      <title>The Hardness of Validating Observational Studies with Experimental Data</title>
      <link>https://arxiv.org/abs/2503.14795</link>
      <description>arXiv:2503.14795v1 Announce Type: cross 
Abstract: Observational data is often readily available in large quantities, but can lead to biased causal effect estimates due to the presence of unobserved confounding. Recent works attempt to remove this bias by supplementing observational data with experimental data, which, when available, is typically on a smaller scale due to the time and cost involved in running a randomised controlled trial. In this work, we prove a theorem that places fundamental limits on this ``best of both worlds'' approach. Using the framework of impossible inference, we show that although it is possible to use experimental data to \emph{falsify} causal effect estimates from observational data, in general it is not possible to \emph{validate} such estimates. Our theorem proves that while experimental data can be used to detect bias in observational studies, without additional assumptions on the smoothness of the correction function, it can not be used to remove it. We provide a practical example of such an assumption, developing a novel Gaussian Process based approach to construct intervals which contain the true treatment effect with high probability, both inside and outside of the support of the experimental data. We demonstrate our methodology on both simulated and semi-synthetic datasets and make the \href{https://github.com/Jakefawkes/Obs_and_exp_data}{code available}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14795v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Fawkes, Michael O'Riordan, Athanasios Vlontzos, Oriol Corcoll, Ciar\'an Mark Gilligan-Lee</dc:creator>
    </item>
    <item>
      <title>A Note on Local Linear Regression for Time Series in Banach Spaces</title>
      <link>https://arxiv.org/abs/2503.15039</link>
      <description>arXiv:2503.15039v1 Announce Type: cross 
Abstract: This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15039v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs</dc:creator>
    </item>
    <item>
      <title>Global Group Fairness in Federated Learning via Function Tracking</title>
      <link>https://arxiv.org/abs/2503.15163</link>
      <description>arXiv:2503.15163v1 Announce Type: cross 
Abstract: We investigate group fairness regularizers in federated learning, aiming to train a globally fair model in a distributed setting. Ensuring global fairness in distributed training presents unique challenges, as fairness regularizers typically involve probability metrics between distributions across all clients and are not naturally separable by client. To address this, we introduce a function-tracking scheme for the global fairness regularizer based on a Maximum Mean Discrepancy (MMD), which incurs a small communication overhead. This scheme seamlessly integrates into most federated learning algorithms while preserving rigorous convergence guarantees, as demonstrated in the context of FedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD regularization enables straightforward analysis through a change of kernel, leveraging an intuitive interpretation of kernel convolution. Numerical experiments confirm our theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15163v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yves Rychener, Daniel Kuhn, Yifan Hu</dc:creator>
    </item>
    <item>
      <title>A Framework for Statistical Inference via Randomized Algorithms</title>
      <link>https://arxiv.org/abs/2307.11255</link>
      <description>arXiv:2307.11255v4 Announce Type: replace 
Abstract: Randomized algorithms, such as randomized sketching or stochastic optimization, are a promising approach to ease the computational burden in analyzing large datasets. However, randomized algorithms also produce non-deterministic outputs, leading to the problem of evaluating their accuracy. In this paper, we develop a statistical inference framework for quantifying the uncertainty of the outputs of randomized algorithms.
  Our key conclusion is that one can perform statistical inference for the target of a sequence of randomized algorithms as long as in the limit, their outputs fluctuate around the target according to any (possibly unknown) probability distribution. In this setting, we develop appropriate statistical inference methods -- sub-randomization, multi-run plug-in and multi-run aggregation -- by estimating the unknown parameters of the limiting distribution either using multiple runs of the randomized algorithm, or by tailored estimates.
  As illustrations, we develop methods for statistical inference when using stochastic optimization (such as Polyak-Ruppert averaging in stochastic gradient descent and stochastic optimization with momentum). We also illustrate our methods in inference for least squares parameters via randomized sketching, by characterizing the limiting distributions of sketching estimates in a possibly growing dimensional case. We also characterize the computation and communication cost of our methods, showing that in certain cases, they add negligible overhead. The results are supported via a broad range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11255v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhixiang Zhang, Sokbae Lee, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Semiparametric Copula Estimation for Spatially Correlated Multivariate Mixed Outcomes: Analyzing Visual Sightings of Fin Whales from Line Transect Survey</title>
      <link>https://arxiv.org/abs/2312.12710</link>
      <description>arXiv:2312.12710v2 Announce Type: replace 
Abstract: For marine biologists, ascertaining the dependence structures between marine species and marine environments, such as sea surface temperature and ocean depth, is imperative for defining ecosystem functioning and providing insights into the dynamics of marine ecosystems. However, obtained data include not only continuous but also discrete data, such as binaries and counts (referred to as mixed outcomes), as well as spatial correlations, both of which make conventional multivariate analysis tools impractical. To solve this issue, we propose semiparametric Bayesian inference and develop an efficient algorithm for computing the posterior of the dependence structure based on the rank likelihood under a latent multivariate spatial Gaussian process using the Markov chain Monte Carlo method. To alleviate the computational intractability caused by the Gaussian process, we also provide a scalable implementation that leverages the nearest-neighbor Gaussian process. Extensive numerical experiments reveal that the proposed method reliably infers the dependence structures of spatially correlated mixed outcomes. Finally, we apply the proposed method to a dataset collected during an international synoptic krill survey in the Scotia Sea of the Antarctic Peninsula to infer the dependence structure between fin whales (Balaenoptera physalus), krill biomass, and relevant oceanographic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12710v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomotaka Momozaki, Tomoyuki Nakagawa, Shonosuke Sugasawa, Hiroko Kato Solvang</dc:creator>
    </item>
    <item>
      <title>Statistical ranking with dynamic covariates</title>
      <link>https://arxiv.org/abs/2406.16507</link>
      <description>arXiv:2406.16507v3 Announce Type: replace 
Abstract: We introduce a general covariate-assisted statistical ranking model within the Plackett--Luce framework. Unlike previous studies focusing on individual effects with fixed covariates, our model allows covariates to vary across comparisons. This added flexibility enhances model fitting yet brings significant challenges in analysis. This paper addresses these challenges in the context of maximum likelihood estimation (MLE). We first provide sufficient and necessary conditions for both model identifiability and the unique existence of the MLE. Then, we develop an efficient alternating maximization algorithm to compute the MLE. Under suitable assumptions on the design of comparison graphs and covariates, we establish a uniform consistency result for the MLE, with convergence rates determined by the asymptotic graph connectivity. We also construct random designs where the proposed assumptions hold almost surely. Numerical studies are conducted to support our findings and demonstrate the model's application to real-world datasets, including horse racing and tennis competitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16507v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinjun Dong, Ruijian Han, Binyan Jiang, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Evaluating Treatment Benefit Predictors using Observational Data: Contending with Identification and Confounding Bias</title>
      <link>https://arxiv.org/abs/2407.05585</link>
      <description>arXiv:2407.05585v3 Announce Type: replace 
Abstract: A treatment benefit predictor (TBP) maps patient characteristics into an estimate of the treatment benefit for that patient, which can support optimizing treatment decisions. However, evaluating the predictive performance of a TBP is challenging, as this often must be conducted in a sample where treatment assignment is not random. After briefly reviewing the metrics for evaluating TBPs, we show conceptually how to evaluate a pre-specified TBP using observational data from the target population for a binary treatment decision at a single time point. We exemplify with a particular measure of discrimination (the concentration of benefit index) and a particular measure of calibration (the moderate calibration curve). The population-level definitions of these metrics involve the latent (counterfactual) treatment benefit variable, but we show identification by re-expressing the respective estimands in terms of the distribution of observable data only. We also show that in the absence of full confounding control, bias propagates in a more complex manner than when targeting more commonly encountered estimands (such as the average treatment effect). Our findings reveal the patterns of biases are often unpredictable and general intuition about the direction of bias in causal effect estimates does not hold in the present context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05585v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Xia, Mohsen Sadatsafavi, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Iterative Trace Minimization for the Reconciliation of Very Short Hierarchical Time Series</title>
      <link>https://arxiv.org/abs/2409.18550</link>
      <description>arXiv:2409.18550v2 Announce Type: replace 
Abstract: Time series often appear in an additive hierarchical structure. In such cases, time series on higher levels are the sums of their subordinate time series. This hierarchical structure places a natural constraint on forecasts. However, univariate forecasting techniques are incapable of ensuring this forecast coherence. An obvious solution is to forecast only bottom time series and obtain higher level forecasts through aggregation. This approach is also known as the bottom-up approach. In their seminal paper, \citep{Wickramasuriya2019} propose an optimal reconciliation approach named MinT. It tries to minimize the trace of the underlying covariance matrix of all forecast errors. The MinT algorithm has demonstrated superior performance to the bottom-up and other approaches and enjoys great popularity. This paper provides a simulation study examining the performance of MinT for very short time series and larger hierarchical structures. This scenario makes the covariance estimation required by MinT difficult. A novel iterative approach is introduced which significantly reduces the number of estimated parameters. This approach is capable of improving forecast accuracy further. The application of MinTit is also demonstrated with a case study at the hand of a semiconductor dataset based on data provided by the World Semiconductor Trade Statistics (WSTS), a premier provider of semiconductor market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18550v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Steinmeister, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Simplified vine copula models: state of science and affairs</title>
      <link>https://arxiv.org/abs/2410.16806</link>
      <description>arXiv:2410.16806v3 Announce Type: replace 
Abstract: Vine copula models have become highly popular practical tools for modeling multivariate dependencies. To maintain tractability, a commonly employed simplifying assumption is that conditional copulas remain unchanged by the conditioning variables. This assumption has sparked a somewhat polarizing debate within the copula community. The fact that much of this dispute occurs outside the public record has placed the field in an unfortunate position, impeding scientific progress. In this article, I will review what we know about the flexibility and limitations of simplified vine copula models, explore the broader implications, and offer my own, hopefully reconciling, perspective on the issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16806v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>On Distributional Discrepancy for Experimental Design with General Assignment Probabilities</title>
      <link>https://arxiv.org/abs/2411.02956</link>
      <description>arXiv:2411.02956v2 Announce Type: replace 
Abstract: We investigate experimental design for randomized controlled trials (RCTs) with both equal and unequal treatment-control assignment probabilities. Our work makes progress on the connection between the distributional discrepancy minimization (DDM) problem introduced by Harshaw et al. (2024) and the design of RCTs. We make two main contributions: First, we prove that approximating the optimal solution of the DDM problem within a certain constant error is NP-hard. Second, we introduce a new Multiplicative Weights Update (MWU) algorithm for the DDM problem, which improves the Gram-Schmidt walk algorithm used by Harshaw et al. (2024) when assignment probabilities are unequal. Building on the framework of Harshaw et al. (2024) and our MWU algorithm, we then develop the MWU design, which reduces the worst-case mean squared error in estimating the average treatment effect. Finally, we present a comprehensive simulation study comparing our design with commonly used designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02956v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anup B. Rao, Peng Zhang</dc:creator>
    </item>
    <item>
      <title>Your copula is a classifier in disguise: classification-based copula density estimation</title>
      <link>https://arxiv.org/abs/2411.03014</link>
      <description>arXiv:2411.03014v2 Announce Type: replace 
Abstract: We propose reinterpreting copula density estimation as a discriminative task. Under this novel estimation scheme, we train a classifier to distinguish samples from the joint density from those of the product of independent marginals, recovering the copula density in the process. We derive equivalences between well-known copula classes and classification problems naturally arising in our interpretation. Furthermore, we show our estimator achieves theoretical guarantees akin to maximum likelihood estimation. By identifying a connection with density ratio estimation, we benefit from the rich literature and models available for such problems. Empirically, we demonstrate the applicability of our approach by estimating copulas of real and high-dimensional datasets, outperforming competing copula estimators in density evaluation as well as sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03014v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Huk, Mark Steel, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.19796</link>
      <description>arXiv:2502.19796v2 Announce Type: replace 
Abstract: Updating $\textit{a priori}$ information given some observed data is the core tenet of Bayesian inference. Bayesian transfer learning extends this idea by incorporating information from a related dataset to improve the inference on the observed data which may have been collected under slightly different settings. The use of related information can be useful when the observed data is scarce, for example. Current Bayesian transfer learning methods that are based on the so-called $\textit{power prior}$ can adaptively transfer information from related data. Unfortunately, it is not always clear under which scenario Bayesian transfer learning performs best or even if it will improve Bayesian inference. Additionally, current power prior methods rely on conjugacy to evaluate the posterior of interest. We propose using leave-one-out cross validation on the target dataset as a means of evaluating Bayesian transfer learning methods. Further, we introduce a new framework, $\textit{transfer sequential Monte Carlo}$, for power prior approaches that efficiently chooses the transfer parameter while avoiding the need for conjugate priors. We assess the performance of our proposed methods in two comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19796v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>fabisearch: A Package for Change Point Detection in and Visualization of the Network Structure of Multivariate High-Dimensional Time Series in R</title>
      <link>https://arxiv.org/abs/2207.02986</link>
      <description>arXiv:2207.02986v4 Announce Type: replace-cross 
Abstract: Change point detection is a commonly used technique in time series analysis, capturing the dynamic nature in which many real-world processes function. With the ever increasing troves of multivariate high-dimensional time series data, especially in neuroimaging and finance, there is a clear need for scalable and data-driven change point detection methods. Currently, change point detection methods for multivariate high-dimensional data are scarce, with even less available in high-level, easily accessible software packages. To this end, we introduce the R package fabisearch, available on the Comprehensive R Archive Network (CRAN), which implements the factorized binary search (FaBiSearch) methodology. FaBiSearch is a novel statistical method for detecting change points in the network structure of multivariate high-dimensional time series which employs non-negative matrix factorization (NMF), an unsupervised dimension reduction and clustering technique. Given the high computational cost of NMF, we implement the method in C++ code and use parallelization to reduce computation time. Further, we also utilize a new binary search algorithm to efficiently identify multiple change points and provide a new method for network estimation for data between change points. We show the functionality of the package and the practicality of the method by applying it to a neuroimaging and a finance data set. Lastly, we provide an interactive, 3-dimensional, brain-specific network visualization capability in a flexible, stand-alone function. This function can be conveniently used with any node coordinate atlas, and nodes can be color coded according to community membership (if applicable). The output is an elegantly displayed network laid over a cortical surface, which can be rotated in the 3-dimensional space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02986v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2024.127321</arxiv:DOI>
      <dc:creator>Martin Ondrus, Ivor Cribben</dc:creator>
    </item>
    <item>
      <title>Bernstein-type Inequalities and Nonparametric Estimation under Near-Epoch Dependence</title>
      <link>https://arxiv.org/abs/2208.11433</link>
      <description>arXiv:2208.11433v4 Announce Type: replace-cross 
Abstract: The major contributions of this paper lie in two aspects. Firstly, we focus on deriving Bernstein-type inequalities for both geometric and algebraic irregularly-spaced NED random fields, which contain time series as special case. Furthermore, by introducing the idea of "effective dimension" to the index set of random field, our results reflect that the sharpness of inequalities are only associated with this "effective dimension". Up to the best of our knowledge, our paper may be the first one reflecting this phenomenon. Hence, the first contribution of this paper can be more or less regarded as an update of the pioneering work from \citeA{xu2018sieve}. Additionally, as a corollary of our first contribution, a Bernstein-type inequality for geometric irregularly-spaced $\alpha$-mixing random fields is also obtained. The second aspect of our contributions is that, based on the inequalities mentioned above, we show the $L_{\infty}$ convergence rate of the many interesting kernel-based nonparametric estimators. To do this, two deviation inequalities for the supreme of empirical process are derived under NED and $\alpha$-mixing conditions respectively. Then, for irregularly-spaced NED random fields, we prove the attainability of optimal rate for local linear estimator of nonparametric regression, which refreshes another pioneering work on this topic, \citeA{jenish2012nonparametric}. Subsequently, we analyze the uniform convergence rate of uni-modal regression under the same NED conditions as well. Furthermore, by following the guide of \citeA{rigollet2009optimal}, we also prove that the kernel-based plug-in density level set estimator could be optimal up to a logarithm factor. Meanwhile, when the data is collected from $\alpha$-mixing random fields, we also derive the uniform convergence rate of a simple local polynomial density estimator \cite{cattaneo2020simple}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11433v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan, Martin Spindler</dc:creator>
    </item>
    <item>
      <title>On the Approximability of Stationary Processes using the ARMA Model</title>
      <link>https://arxiv.org/abs/2408.10610</link>
      <description>arXiv:2408.10610v2 Announce Type: replace-cross 
Abstract: Within the theoretical literature on stationary random variables, pure Moving Average models and pure Autoregressive models have a rich body of work, but the corresponding literature on Autoregressive Moving Average (ARMA) models is very sparse. We attempt to fill certain gaps in this sparse line of work. Central to our observations is the spectral lemma connecting supnorm based function approximation on the unit circle to random variable approximation. This method allows us to provide quantitative approximation bounds in contrast with the qualitative boundedness and stability guarantees associated with unit root tests. Using the spectral lemma we first identify a class of stationary processes where approximation guarantees are feasible. This turns a known heuristic argument motivating ARMA models based on rational approximations into a rigorous result. Second, we identify an idealized stationary random process for which we conjecture that a good ARMA approximation is not possible. Third, we calculate exact approximation bounds for an example process, and a constructive proof that, for a given order, Pad\'e approximations do not always correspond to the best ARMA approximation. Unlike prior literature, our approach uses the generating function of the random process rather than the spectral measure, and further our results focus on approximation error of the random variable rather than the prediction error as in some classical infimum results by Szego, Kolmogorov, and Wiener.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10610v2</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand Ganesh, Babhrubahan Bose, Anand Rajagopalan</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Semiparametrically Efficient Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2502.17741</link>
      <description>arXiv:2502.17741v2 Announce Type: replace-cross 
Abstract: We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\{X_i, Y_i \}_{i=1}^n$ and an unlabeled dataset $\{ X_i \}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17741v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichun Xu, Daniela Witten, Ali Shojaie</dc:creator>
    </item>
  </channel>
</rss>
