<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Aug 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Thermomechanical Hybrid Incompressible Material Point Method</title>
      <link>https://arxiv.org/abs/2408.07276</link>
      <description>arXiv:2408.07276v1 Announce Type: new 
Abstract: We present a novel hybrid incompressible flow/material point method solver for simulating the combustion of flammable solids. Our approach utilizes a sparse grid representation of solid materials in the material point method portion of the solver and a hybrid Eulerian/FLIP solver for the incompressible portion. We utilize these components to simulate the motion of heated air and particulate matter as they interact with flammable solids, causing combustion-related damage. We include a novel particle sampling strategy to increase Eulerian flow accuracy near regions of high temperature. We also support control of the flame front propagation speed and the rate of solid combustion in an artistically directable manner. Solid combustion is modeled with temperature-dependent elastoplastic constitutive modeling. We demonstrate the efficacy of our method on various real-world three-dimensional problems, including a burning match, incense sticks, and a wood log in a fireplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07276v1</guid>
      <category>cs.GR</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victoria Kala, Jingyu Chen, David Hyde, Alexey Stomakhin, Joseph Teran</dc:creator>
    </item>
    <item>
      <title>Generative Photomontage</title>
      <link>https://arxiv.org/abs/2408.07116</link>
      <description>arXiv:2408.07116v1 Announce Type: cross 
Abstract: Text-to-image models are powerful tools for image creation. However, the generation process is akin to a dice roll and makes it difficult to achieve a single image that captures everything a user wants. In this paper, we propose a framework for creating the desired image by compositing it from various parts of generated images, in essence forming a Generative Photomontage. Given a stack of images generated by ControlNet using the same input condition and different seeds, we let users select desired parts from the generated results using a brush stroke interface. We introduce a novel technique that takes in the user's brush strokes, segments the generated images using a graph-based optimization in diffusion feature space, and then composites the segmented regions via a new feature-space blending method. Our method faithfully preserves the user-selected regions while compositing them harmoniously. We demonstrate that our flexible framework can be used for many applications, including generating new appearance combinations, fixing incorrect shapes and artifacts, and improving prompt alignment. We show compelling results for each application and demonstrate that our method outperforms existing image blending methods and various baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07116v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean J. Liu, Nupur Kumari, Ariel Shamir, Jun-Yan Zhu</dc:creator>
    </item>
    <item>
      <title>RoCoSDF: Row-Column Scanned Neural Signed Distance Fields for Freehand 3D Ultrasound Imaging Shape Reconstruction</title>
      <link>https://arxiv.org/abs/2408.07325</link>
      <description>arXiv:2408.07325v1 Announce Type: cross 
Abstract: The reconstruction of high-quality shape geometry is crucial for developing freehand 3D ultrasound imaging. However, the shape reconstruction of multi-view ultrasound data remains challenging due to the elevation distortion caused by thick transducer probes. In this paper, we present a novel learning-based framework RoCoSDF, which can effectively generate an implicit surface through continuous shape representations derived from row-column scanned datasets. In RoCoSDF, we encode the datasets from different views into the corresponding neural signed distance function (SDF) and then operate all SDFs in a normalized 3D space to restore the actual surface contour. Without requiring pre-training on large-scale ground truth shapes, our approach can synthesize a smooth and continuous signed distance field from multi-view SDFs to implicitly represent the actual geometry. Furthermore, two regularizers are introduced to facilitate shape refinement by constraining the SDF near the surface. The experiments on twelve shapes data acquired by two ultrasound transducer probes validate that RoCoSDF can effectively reconstruct accurate geometric shapes from multi-view ultrasound data, which outperforms current reconstruction methods. Code is available at https://github.com/chenhbo/RoCoSDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07325v1</guid>
      <category>eess.IV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongbo Chen, Yuchong Gao, Shuhang Zhang, Jiangjie Wu, Yuexin Ma, Rui Zheng</dc:creator>
    </item>
    <item>
      <title>Inverse Rendering of Fusion Plasmas: Inferring Plasma Composition from Imaging Systems</title>
      <link>https://arxiv.org/abs/2408.07555</link>
      <description>arXiv:2408.07555v1 Announce Type: cross 
Abstract: In this work, we develop a differentiable rendering pipeline for visualising plasma emission within tokamaks, and estimating the gradients of the emission and estimating other physical quantities. Unlike prior work, we are able to leverage arbitrary representations of plasma quantities and easily incorporate them into a non-linear optimisation framework. The efficiency of our method enables not only estimation of a physically plausible image of plasma, but also recovery of the neutral Deuterium distribution from imaging and midplane measurements alone. We demonstrate our method with three different levels of complexity showing first that a poloidal neutrals density distribution can be recovered from imaging alone, second that the distributions of neutral Deuterium, electron density and electron temperature can be recovered jointly, and finally, that this can be done in the presence of realistic imaging systems that incorporate sensor cropping and quantisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07555v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekin \"Ozt\"urk, Rob Akers, Stanislas Pamela, The MAST Team, Pieter Peers, Abhijeet Ghosh</dc:creator>
    </item>
    <item>
      <title>Creating Data Art: Authentic Learning and Visualisation Exhibition</title>
      <link>https://arxiv.org/abs/2408.07590</link>
      <description>arXiv:2408.07590v1 Announce Type: cross 
Abstract: We present an authentic learning task designed for computing students, centred on the creation of data-art visualisations from chosen datasets for a public exhibition. This exhibition was showcased in the cinema foyer for two weeks in June, providing a real-world platform for students to display their work. Over the course of two years, we implemented this active learning task with two different cohorts of students. In this paper, we share our experiences and insights from these activities, highlighting the impact on student engagement and learning outcomes. We also provide a detailed description of the seven individual tasks that learners must perform: topic and data selection and analysis, research and art inspiration, design conceptualisation, proposed solution, visualisation creation, exhibition curation, and reflection. By integrating these tasks, students not only develop technical skills but also gain practical experience in presenting their work to a public audience, bridging the gap between academic learning and professional practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07590v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan C. Roberts</dc:creator>
    </item>
    <item>
      <title>Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality</title>
      <link>https://arxiv.org/abs/2406.12544</link>
      <description>arXiv:2406.12544v2 Announce Type: replace-cross 
Abstract: In human interaction, gestures serve various functions such as marking speech rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impact of gestures on explanations provided by virtual agents remains underexplored. A user study was carried out to investigate how different types of gestures influence perceived interaction quality and listener understanding. This study addresses the effect of gestures in explanation by developing an embodied virtual explainer integrating both beat gestures and iconic gestures to enhance its automatically generated verbal explanations. Our model combines beat gestures generated by a learned speech-driven synthesis module with manually captured iconic gestures, supporting the agent's verbal expressions about the board game Quarto! as an explanation scenario. Findings indicate that neither the use of iconic gestures alone nor their combination with beat gestures outperforms the baseline or beat-only conditions in terms of understanding. Nonetheless, compared to prior research, the embodied agent significantly enhances understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12544v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amelie Sophie Robrecht, Hendric Voss, Lisa Gottschalk, Stefan Kopp</dc:creator>
    </item>
    <item>
      <title>Content and Style Aware Audio-Driven Facial Animation</title>
      <link>https://arxiv.org/abs/2408.07005</link>
      <description>arXiv:2408.07005v2 Announce Type: replace-cross 
Abstract: Audio-driven 3D facial animation has several virtual humans applications for content creation and editing. While several existing methods provide solutions for speech-driven animation, precise control over content (what) and style (how) of the final performance is still challenging. We propose a novel approach that takes as input an audio, and the corresponding text to extract temporally-aligned content and disentangled style representations, in order to provide controls over 3D facial animation. Our method is trained in two stages, that evolves from audio prominent styles (how it sounds) to visual prominent styles (how it looks). We leverage a high-resource audio dataset in stage I to learn styles that control speech generation in a self-supervised learning framework, and then fine-tune this model with low-resource audio/3D mesh pairs in stage II to control 3D vertex generation. We employ a non-autoregressive seq2seq formulation to model sentence-level dependencies, and better mouth articulations. Our method provides flexibility that the style of a reference audio and the content of a source audio can be combined to enable audio style transfer. Similarly, the content can be modified, e.g. muting or swapping words, that enables style-preserving content editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07005v2</guid>
      <category>cs.SD</category>
      <category>cs.GR</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qingju Liu, Hyeongwoo Kim, Gaurav Bharaj</dc:creator>
    </item>
  </channel>
</rss>
