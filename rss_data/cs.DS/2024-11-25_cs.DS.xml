<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Nov 2024 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Construction and Preliminary Validation of a Dynamic Programming Concept Inventory</title>
      <link>https://arxiv.org/abs/2411.14655</link>
      <description>arXiv:2411.14655v1 Announce Type: new 
Abstract: Concept inventories are standardized assessments that evaluate student understanding of key concepts within academic disciplines. While prevalent across STEM fields, their development lags for advanced computer science topics like dynamic programming (DP) -- an algorithmic technique that poses significant conceptual challenges for undergraduates. To fill this gap, we developed and validated a Dynamic Programming Concept Inventory (DPCI). We detail the iterative process used to formulate multiple-choice questions targeting known student misconceptions about DP concepts identified through prior research studies. We discuss key decisions, tradeoffs, and challenges faced in crafting probing questions to subtly reveal these conceptual misunderstandings. We conducted a preliminary psychometric validation by administering the DPCI to 172 undergraduate CS students finding our questions to be of appropriate difficulty and effectively discriminating between differing levels of student understanding. Taken together, our validated DPCI will enable instructors to accurately assess student mastery of DP. Moreover, our approach for devising a concept inventory for an advanced theoretical computer science concept can guide future efforts to create assessments for other under-evaluated areas currently lacking coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14655v1</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthew Ferland, Varun Nagaraj Rao, Arushi Arora, Drew van der Poel, Michael Luu, Randy Huynh, Freddy Reiber, Sandra Ossman, Seth Poulsen, Michael Shindler</dc:creator>
    </item>
    <item>
      <title>Approximating the Held-Karp Bound for Metric TSP in Nearly Linear Work and Polylogarithmic Depth</title>
      <link>https://arxiv.org/abs/2411.14745</link>
      <description>arXiv:2411.14745v1 Announce Type: new 
Abstract: We present a nearly linear work parallel algorithm for approximating the Held-Karp bound for the Metric TSP problem. Given an edge-weighted undirected graph $G=(V,E)$ on $m$ edges and $\epsilon&gt;0$, it returns a $(1+\epsilon)$-approximation to the Held-Karp bound with high probability, in $\tilde{O}(m/\epsilon^4)$ work and $\tilde{O}(1/\epsilon^4)$ depth. While a nearly linear time sequential algorithm was known for almost a decade (Chekuri and Quanrud'17), it was not known how to simultaneously achieve nearly linear work alongside polylogarithmic depth. Using a reduction by Chalermsook et al.'22, we also give a parallel algorithm for computing a $(1+\epsilon)$-approximate fractional solution to the $k$-edge-connected spanning subgraph (kECSS) problem, with the same complexity.
  To obtain these results, we introduce a notion of core-sequences for the parallel Multiplicative Weights Update (MWU) framework (Luby-Nisan'93, Young'01). For the Metric TSP and kECSS problems, core-sequences enable us to exploit the structure of approximate minimum cuts to reduce the cost per iteration and/or the number of iterations. The acceleration technique via core-sequences is generic and of independent interest. In particular, it improves the best-known iteration complexity of MWU algorithms for packing/covering LPs from $poly(\log nnz(A))$ to polylogarithmic in the product of cardinalities of the core-sequence sets where $A$ is the constraint matrix of the LP. For certain implicitly defined LPs such as the kECSS LP, this yields an exponential improvement in depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14745v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuan Khye Koh, Omri Weinstein, Sorrachai Yingchareonthawornchai</dc:creator>
    </item>
    <item>
      <title>Distributed Model Checking in Graphs Classes of Bounded Expansion</title>
      <link>https://arxiv.org/abs/2411.14825</link>
      <description>arXiv:2411.14825v1 Announce Type: new 
Abstract: We show that for every first-order logic (FO) formula $\varphi$, and every graph class $\mathcal{G}$ of bounded expansion, there exists a distributed (deterministic) algorithm that, for every $n$-node graph $G\in\mathcal{G}$ of diameter $D$, decides whether $G\models \varphi$ in $O(D+\log n)$ rounds under the standard CONGEST model. Graphs of bounded expansion encompass many classes of sparse graphs such as planar graphs, bounded-treedepth graphs, bounded-treewidth graphs, bounded-degree graphs, and graphs excluding a fixed graph $H$ as a minor or topological minor. Note that our algorithm is optimal up to a logarithmic additional term, as even a simple FO formula such as "there are two vertices of degree 3" already on trees requires $\Omega(D)$ rounds in CONGEST.
  Our result extends to solving optimization problems expressed in FO (e.g., $k$-vertex cover of minimum weight), as well as to counting the number of solutions of a problem expressible in a fragment of FO (e.g., counting triangles), still running in $O(D+\log n)$ rounds under the CONGEST model. This exemplifies the contrast between sparse graphs and general graphs as far as CONGEST algorithms are concerned. For instance, Drucker, Kuhn, and Oshman [PODC 2014] showed that the problem of deciding whether a general graph contains a 4-cycle requires $\Theta(\sqrt{n}/\log n)$ rounds in CONGEST. For counting triangles, the best known algorithm of Chang, Pettie, and Zhang [SODA 2019] takes $\tilde{O}(\sqrt{n})$ rounds.
  Finally, our result extends to distributed certification. We show that, for every FO formula~$\varphi$, and every graph class of bounded expansion, there exists a certification scheme for $\varphi$ using certificates on $O(\log n)$ bits. This significantly generalizes the recent result of Feuilloley, Bousquet, and Pierron [PODC 2022], which held solely for graphs of bounded treedepth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14825v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fedor V. Fomin, Pierre Fraigniaud, Petr A. Golovach, Pedro Montealegre, Ivan Rapaport, Ioan Todinca</dc:creator>
    </item>
    <item>
      <title>Approximating Prize-Collecting Variants of TSP</title>
      <link>https://arxiv.org/abs/2411.14994</link>
      <description>arXiv:2411.14994v1 Announce Type: new 
Abstract: We present an approximation algorithm for the Prize-collecting Ordered Traveling Salesman Problem (PCOTSP), which simultaneously generalizes the Prize-collecting TSP and the Ordered TSP. The Prize-collecting TSP is well-studied and has a long history, with the current best approximation factor slightly below $1.6$, shown by Blauth, Klein and N\"agele [IPCO 2024]. The best approximation ratio for Ordered TSP is $\frac{3}{2}+\frac{1}{e}$, presented by B\"{o}hm, Friggstad, M\"{o}mke, Spoerhase [SODA 2025] and Armbruster, Mnich, N\"{a}gele [Approx 2024]. The former also present a factor 2.2131 approximation algorithm for Multi-Path-TSP.
  By carefully tuning the techniques of the latest results on the aforementioned problems and leveraging the unique properties of our problem, we present a 2.097-approximation algorithm for PCOTSP. A key idea in our result is to first sample a set of trees, and then probabilistically pick up some vertices, while using the pruning ideas of Blauth, Klein, N\"{a}gele [IPCO 2024] on other vertices to get cheaper parity correction; the sampling probability and the penalty paid by the LP playing a crucial part in both cases. A straightforward adaptation of the aforementioned pruning ideas would only give minuscule improvements over standard parity correction methods. Instead, we use the specific characteristics of our problem together with properties gained from running a simple combinatorial algorithm to bring the approximation factor below 2.1. Our techniques extend to Prize-collecting Multi-Path TSP, building on results from B\"{o}hm, Friggstad, M\"{o}mke, Spoerhase [SODA 2025], leading to a 2.41-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14994v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morteza Alimi, Tobias M\"omke, Michael Ruderer</dc:creator>
    </item>
    <item>
      <title>Sparsifying Suprema of Gaussian Processes</title>
      <link>https://arxiv.org/abs/2411.14664</link>
      <description>arXiv:2411.14664v1 Announce Type: cross 
Abstract: We give a dimension-independent sparsification result for suprema of centered Gaussian processes: Let $T$ be any (possibly infinite) bounded set of vectors in $\mathbb{R}^n$, and let $\{{\boldsymbol{X}}_t\}_{t\in T}$ be the canonical Gaussian process on $T$. We show that there is an $O_\varepsilon(1)$-size subset $S \subseteq T$ and a set of real values $\{c_s\}_{s \in S}$ such that $\sup_{s \in S} \{{\boldsymbol{X}}_s + c_s\}$ is an $\varepsilon$-approximator of $\sup_{t \in T} {\boldsymbol{X}}_t$. Notably, the size of $S$ is completely independent of both the size of $T$ and of the ambient dimension $n$.
  We use this to show that every norm is essentially a junta when viewed as a function over Gaussian space: Given any norm $\nu(x)$ on $\mathbb{R}^n$, there is another norm $\psi(x)$ which depends only on the projection of $x$ along $O_\varepsilon(1)$ directions, for which $\psi({\boldsymbol{g}})$ is a multiplicative $(1 \pm \varepsilon)$-approximation of $\nu({\boldsymbol{g}})$ with probability $1-\varepsilon$ for ${\boldsymbol{g}} \sim N(0,I_n)$.
  We also use our sparsification result for suprema of centered Gaussian processes to give a sparsification lemma for convex sets of bounded geometric width: Any intersection of (possibly infinitely many) halfspaces in $\mathbb{R}^n$ that are at distance $O(1)$ from the origin is $\varepsilon$-close, under $N(0,I_n)$, to an intersection of only $O_\varepsilon(1)$ many halfspaces.
  We describe applications to agnostic learning and tolerant property testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14664v1</guid>
      <category>stat.ML</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anindya De, Shivam Nadimpalli, Ryan O'Donnell, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>Quantum Algorithm for the Multiple String Matching Problem</title>
      <link>https://arxiv.org/abs/2411.14850</link>
      <description>arXiv:2411.14850v1 Announce Type: cross 
Abstract: Let us consider the Multiple String Matching Problem. In this problem, we consider a long string, denoted by $t$, of length $n$. This string is referred to as a text. We also consider a sequence of $m$ strings, denoted by $S$, which we refer to as a dictionary. The total length of all strings from the dictionary is represented by the variable L. The objective is to identify all instances of strings from the dictionary within the text. The standard classical solution to this problem is Aho-Corasick Algorithm that has $O(n+L)$ query and time complexity. At the same time, the classical lower bound for the problem is the same $\Omega(n+L)$. We propose a quantum algorithm with $O(n+\sqrt{mL\log n}+m\log n)$ query complexity and $O(n+\sqrt{mL\log n}\log b+m\log n)=O^*(n+\sqrt{mL})$ time complexity, where $b$ is the maximal length of strings from the dictionary. This improvement is particularly significant in the case of dictionaries comprising long words. Our algorithm's complexity is equal to the quantum lower bound $O(n + \sqrt{mL})$, up to a log factor. In some sense, our algorithm can be viewed as a quantum analogue of the Aho-Corasick algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14850v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamil Khadiev, Danil Serov</dc:creator>
    </item>
    <item>
      <title>Sampling from convex sets with a cold start using multiscale decompositions</title>
      <link>https://arxiv.org/abs/2211.04439</link>
      <description>arXiv:2211.04439v4 Announce Type: replace 
Abstract: Running a random walk in a convex body $K\subseteq\mathbb{R}^n$ is a standard approach to sample approximately uniformly from the body. The requirement is that from a suitable initial distribution, the distribution of the walk comes close to the uniform distribution $\pi_K$ on $K$ after a number of steps polynomial in $n$ and the aspect ratio $R/r$ (i.e., when $rB_2 \subseteq K \subseteq RB_{2}$).
  Proofs of rapid mixing of such walks often require the probability density $\eta_0$ of the initial distribution with respect to $\pi_K$ to be at most $\mathrm{poly}(n)$: this is called a "warm start". Achieving a warm start often requires non-trivial pre-processing before starting the random walk. This motivates proving rapid mixing from a "cold start", wherein $\eta_0$ can be as high as $\exp(\mathrm{poly}(n))$. Unlike warm starts, a cold start is usually trivial to achieve. However, a random walk need not mix rapidly from a cold start: an example being the well-known "ball walk". On the other hand, Lov\'asz and Vempala proved that the "hit-and-run" random walk mixes rapidly from a cold start. For the related coordinate hit-and-run (CHR) walk, which has been found to be promising in computational experiments, rapid mixing from a warm start was proved only recently but the question of rapid mixing from a cold start remained open.
  We construct a family of random walks inspired by classical decompositions of subsets of $\mathbb{R}^n$ into countably many axis-aligned dyadic cubes. We show that even with a cold start, the mixing times of these walks are bounded by a polynomial in $n$ and the aspect ratio. Our main technical ingredient is an isoperimetric inequality for $K$ for a metric that magnifies distances between points close to the boundary of $K$. As a corollary, we show that the CHR walk also mixes rapidly both from a cold start and from a point not too close to the boundary of $K$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04439v4</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>math.PR</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hariharan Narayanan, Amit Rajaraman, Piyush Srivastava</dc:creator>
    </item>
    <item>
      <title>Coloring tournaments with few colors: Algorithms and complexity</title>
      <link>https://arxiv.org/abs/2305.02922</link>
      <description>arXiv:2305.02922v3 Announce Type: replace 
Abstract: A $k$-coloring of a tournament is a partition of its vertices into $k$ acyclic sets. Deciding if a tournament is 2-colorable is NP-hard. A natural problem, akin to that of coloring a 3-colorable graph with few colors, is to color a 2-colorable tournament with few colors. This problem does not seem to have been addressed before, although it is a special case of coloring a 2-colorable 3-uniform hypergraph with few colors, which is a well-studied problem with super-constant lower bounds.
  We present a new efficient decomposition lemma for tournaments, which we use to design polynomial-time algorithms to color various classes of tournaments with few colors, notably, to color a 2-colorable tournament with ten colors. We also use this lemma to prove equivalence between the problems of coloring 3-colorable tournaments and coloring 3-colorable graphs with constantly many colors. For the classes of tournaments considered, we complement our upper bounds with strengthened lower bounds, painting a comprehensive picture of the algorithmic and complexity aspects of coloring tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02922v3</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Klingelhoefer, Alantha Newman</dc:creator>
    </item>
    <item>
      <title>Testing Convex Truncation</title>
      <link>https://arxiv.org/abs/2305.03146</link>
      <description>arXiv:2305.03146v2 Announce Type: replace 
Abstract: We study the basic statistical problem of testing whether normally distributed $n$-dimensional data has been truncated, i.e. altered by only retaining points that lie in some unknown truncation set $S \subseteq \mathbb{R}^n$. As our main algorithmic results,
  (1) We give a computationally efficient $O(n)$-sample algorithm that can distinguish the standard normal distribution $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown and arbitrary convex set $S$.
  (2) We give a different computationally efficient $O(n)$-sample algorithm that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown and arbitrary mixture of symmetric convex sets.
  These results stand in sharp contrast with known results for learning or testing convex bodies with respect to the normal distribution or learning convex-truncated normal distributions, where state-of-the-art algorithms require essentially $n^{\sqrt{n}}$ samples. An easy argument shows that no finite number of samples suffices to distinguish $N(0,I_n)$ from an unknown and arbitrary mixture of general (not necessarily symmetric) convex sets, so no common generalization of results (1) and (2) above is possible.
  We also prove that any algorithm (computationally efficient or otherwise) that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown symmetric convex set must use $\Omega(n)$ samples. This shows that the sample complexity of each of our algorithms is optimal up to a constant factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03146v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anindya De, Shivam Nadimpalli, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>Fast and Simple Sorting Using Partial Information</title>
      <link>https://arxiv.org/abs/2404.04552</link>
      <description>arXiv:2404.04552v3 Announce Type: replace 
Abstract: We consider the problem of sorting $n$ items, given the outcomes of $m$ pre-existing comparisons. We present a simple and natural deterministic algorithm that runs in $O(m+\log T)$ time and does $O(\log T)$ comparisons, where $T$ is the number of total orders consistent with the pre-existing comparisons.
  Our running time and comparison bounds are best possible up to constant factors, thus resolving a problem that has been studied intensely since 1976 (Fredman, Theoretical Computer Science). The best previous algorithm with a bound of $O(\lg T)$ on the number of comparisons has a time bound of $O(n^{2.5})$ and is more complicated.
  Our algorithm combines three classic algorithms: topological sort, heapsort with the right kind of heap, and efficient search in a sorted list. It outputs the items in sorted order one by one. It can be modified to stop early, thereby solving the important and more general top-$k$ sorting problem: Given $k$ and the outcomes of some pre-existing comparisons, output the smallest $k$ items in sorted order. The modified algorithm solves the top-$k$ sorting problem in minimum time and comparisons, to within constant factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04552v3</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Richard Hlad\'ik, John Iacono, Vaclav Rozhon, Robert Tarjan, Jakub T\v{e}tek</dc:creator>
    </item>
    <item>
      <title>Detecting Low-Degree Truncation</title>
      <link>https://arxiv.org/abs/2402.08133</link>
      <description>arXiv:2402.08133v2 Announce Type: replace-cross 
Abstract: We consider the following basic, and very broad, statistical problem: Given a known high-dimensional distribution ${\cal D}$ over $\mathbb{R}^n$ and a collection of data points in $\mathbb{R}^n$, distinguish between the two possibilities that (i) the data was drawn from ${\cal D}$, versus (ii) the data was drawn from ${\cal D}|_S$, i.e. from ${\cal D}$ subject to truncation by an unknown truncation set $S \subseteq \mathbb{R}^n$.
  We study this problem in the setting where ${\cal D}$ is a high-dimensional i.i.d. product distribution and $S$ is an unknown degree-$d$ polynomial threshold function (one of the most well-studied types of Boolean-valued function over $\mathbb{R}^n$). Our main results are an efficient algorithm when ${\cal D}$ is a hypercontractive distribution, and a matching lower bound:
  $\bullet$ For any constant $d$, we give a polynomial-time algorithm which successfully distinguishes ${\cal D}$ from ${\cal D}|_S$ using $O(n^{d/2})$ samples (subject to mild technical conditions on ${\cal D}$ and $S$);
  $\bullet$ Even for the simplest case of ${\cal D}$ being the uniform distribution over $\{+1, -1\}^n$, we show that for any constant $d$, any distinguishing algorithm for degree-$d$ polynomial threshold functions must use $\Omega(n^{d/2})$ samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08133v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anindya De, Huan Li, Shivam Nadimpalli, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>Polynomial Bounds of CFLOBDDs against BDDs</title>
      <link>https://arxiv.org/abs/2406.01525</link>
      <description>arXiv:2406.01525v2 Announce Type: replace-cross 
Abstract: Binary Decision Diagrams (BDDs) are widely used for the representation of Boolean functions. Context-Free-Language Ordered Decision Diagrams (CFLOBDDs) are a plug-compatible replacement for BDDs -- roughly, they are BDDs augmented with a certain form of procedure call. A natural question to ask is, ``For a given family of Boolean functions $F$, what is the relationship between the size of a BDD for $f \in F$ and the size of a CFLOBDD for $f$?'' Sistla et al. established that there are best-case families of functions, which demonstrate an inherently exponential separation between CFLOBDDs and BDDs. They showed that there are families of functions $\{ f_n \}$ for which, for all $n = 2^k$, the CFLOBDD for $f_n$ (using a particular variable order) is exponentially more succinct than any BDD for $f_n$ (i.e., using any variable order). However, they did not give a worst-case bound -- i.e., they left open the question, ``Is there a family of functions $\{ g_i \}$ for which the size of a CFLOBDD for $g_i$ must be substantially larger than a BDD for $g_i$?'' For instance, it could be that there is a family of functions for which the BDDs are exponentially more succinct than any corresponding CFLOBDDs.
  This paper studies such questions, and answers the second question posed above in the negative. In particular, we show that by using the same variable ordering in the CFLOBDD that is used in the BDD, the size of a CFLOBDD for any function $h$ cannot be far worse than the size of the BDD for $h$. The bound that relates their sizes is polynomial: If BDD $B$ for function $h$ is of size $|B|$ and uses variable ordering $\textit{Ord}$, then the size of the CFLOBDD $C$ for $h$ that also uses $\textit{Ord}$ is bounded by $O(|B|^3)$.
  The paper also shows that the bound is tight: there is a family of functions for which $|C|$ grows as $\Omega(|B|^3)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01525v2</guid>
      <category>cs.SC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xusheng Zhi (University of Wisconsin-Madison,Peking University), Thomas Reps (University of Wisconsin-Madison)</dc:creator>
    </item>
    <item>
      <title>Revisiting Tree Canonization using polynomials</title>
      <link>https://arxiv.org/abs/2408.10338</link>
      <description>arXiv:2408.10338v2 Announce Type: replace-cross 
Abstract: Graph Isomorphism (GI) is a fundamental algorithmic problem. Amongst graph classes for which the computational complexity of GI has been resolved, trees are arguably the most fundamental. Tree Isomorphism is complete for deterministic logspace, a tiny subclass of polynomial time, by Lindell's result. Over three decades ago, he devised a deterministic logspace algorithm that computes a string which is a canon for the input tree -- two trees are isomorphic precisely when their canons are identical.
  Inspired by Miller-Reif's reduction of Tree Isomorphism to Polynomial Identity Testing, we present a new logspace algorithm for tree canonization fundamentally different from Lindell's algorithm. Our algorithm computes a univariate polynomial as canon for an input tree, based on the classical Eisenstein's criterion for the irreducibility of univariate polynomials. This can be implemented in logspace by invoking the well known Buss et al. algorithm for arithmetic formula evaluation. However, we have included in the appendix a simpler self-contained proof showing that arithmetic formula evaluation is in logspace.
  This algorithm is conceptually very simple, avoiding the delicate case analysis and complex recursion that constitute the core of Lindell's algorithm. We illustrate the adaptability of our algorithm by extending it to a couple of other classes of graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10338v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Arvind, Samir Datta, Salman Faris, Asif Khan</dc:creator>
    </item>
    <item>
      <title>Weak Poincar\'e Inequalities, Simulated Annealing, and Sampling from Spherical Spin Glasses</title>
      <link>https://arxiv.org/abs/2411.09075</link>
      <description>arXiv:2411.09075v2 Announce Type: replace-cross 
Abstract: There has been a recent surge of powerful tools to show rapid mixing of Markov chains, via functional inequalities such as Poincar\'e inequalities. In many situations, Markov chains fail to mix rapidly from a worst-case initialization, yet are expected to approximately sample from a random initialization. For example, this occurs if the target distribution has metastable states, small clusters accounting for a vanishing fraction of the mass that are essentially disconnected from the bulk of the measure. Under such conditions, a Poincar\'e inequality cannot hold, necessitating new tools to prove sampling guarantees.
  We develop a framework to analyze simulated annealing, based on establishing so-called weak Poincar\'e inequalities. These inequalities imply mixing from a suitably warm start, and simulated annealing provides a way to chain such warm starts together into a sampling algorithm. We further identify a local-to-global principle to prove weak Poincar\'e inequalities, mirroring the spectral independence and localization schemes frameworks for analyzing mixing times of Markov chains.
  As our main application, we prove that simulated annealing samples from the Gibbs measure of a spherical spin glass for inverse temperatures up to a natural threshold, matching recent algorithms based on algorithmic stochastic localization. This provides the first Markov chain sampling guarantee that holds beyond the uniqueness threshold for spherical spin glasses, where mixing from a worst-case initialization is provably slow due to the presence of metastable states. As an ingredient in our proof, we prove bounds on the operator norm of the covariance matrix of spherical spin glasses in the full replica-symmetric regime.
  Additionally, we resolve a question related to sampling using data-based initializations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09075v2</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brice Huang, Sidhanth Mohanty, Amit Rajaraman, David X. Wu</dc:creator>
    </item>
    <item>
      <title>Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals</title>
      <link>https://arxiv.org/abs/2411.14349</link>
      <description>arXiv:2411.14349v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning an arbitrarily-biased ReLU activation (or neuron) over Gaussian marginals with the squared loss objective. Despite the ReLU neuron being the basic building block of modern neural networks, we still do not understand the basic algorithmic question of whether one arbitrary ReLU neuron is learnable in the non-realizable setting. In particular, all existing polynomial time algorithms only provide approximation guarantees for the better-behaved unbiased setting or restricted bias setting.
  Our main result is a polynomial time statistical query (SQ) algorithm that gives the first constant factor approximation for arbitrary bias. It outputs a ReLU activation that achieves a loss of $O(\mathrm{OPT}) + \varepsilon$ in time $\mathrm{poly}(d,1/\varepsilon)$, where $\mathrm{OPT}$ is the loss obtained by the optimal ReLU activation. Our algorithm presents an interesting departure from existing algorithms, which are all based on gradient descent and thus fall within the class of correlational statistical query (CSQ) algorithms. We complement our algorithmic result by showing that no polynomial time CSQ algorithm can achieve a constant factor approximation. Together, these results shed light on the intrinsic limitation of gradient descent, while identifying arguably the simplest setting (a single neuron) where there is a separation between SQ and CSQ algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14349v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anxin Guo, Aravindan Vijayaraghavan</dc:creator>
    </item>
  </channel>
</rss>
