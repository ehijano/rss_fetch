<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:53:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ranking and Invariants for Lower-Bound Inference in Quantitative Verification of Probabilistic Programs</title>
      <link>https://arxiv.org/abs/2504.04132</link>
      <description>arXiv:2504.04132v1 Announce Type: new 
Abstract: Quantitative properties of probabilistic programs are often characterised by the least fixed point of a monotone function $K$. Giving lower bounds of the least fixed point is crucial for quantitative verification. We propose a new method for obtaining lower bounds of the least fixed point. Drawing inspiration from the verification of non-probabilistic programs, we explore the relationship between the uniqueness of fixed points and program termination, and then develop a framework for lower-bound verification. We introduce a generalisation of ranking supermartingales, which serves as witnesses to the uniqueness of fixed points. Our method can be applied to a wide range of quantitative properties, including the weakest preexpectation, expected runtime, and higher moments of runtime. We provide a template-based algorithm for the automated verification of lower bounds. Our implementation demonstrates the effectiveness of the proposed method via an experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04132v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Kura, Hiroshi Unno, Takeshi Tsukada</dc:creator>
    </item>
    <item>
      <title>A Categorical Foundation of Rough Sets</title>
      <link>https://arxiv.org/abs/2504.04218</link>
      <description>arXiv:2504.04218v1 Announce Type: new 
Abstract: Rough sets are approximations of concrete sets. The theory of rough sets has been used widely for data-mining. While it is well-known that adjunctions are underlying in rough approximations, such adjunctions are not enough for characterization of rough sets. This paper provides a way to characterize rough sets in terms of category theory. We reformulate rough sets as adjunctions between preordered sets in a general way. Our formulation of rough sets can enjoy benefits of adjunctions and category theory. Especially, our characterization is closed under composition. We can also explain the notions of attribute reduction and data insertion in our theory. It is novel that our theory enables us to guess decision rules for unknown data. If we change the answer set, we can get a refinement of rough sets without any difficulty. Our refined rough sets lead rough fuzzy sets or more general approximations of functions. Moreover, our theory of rough sets can be generalized in the manner of enriched category theory. The derived enriched theory covers the usual theory of fuzzy rough sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04218v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yoshihiko Kakutani</dc:creator>
    </item>
    <item>
      <title>PVASS Reachability is Decidable</title>
      <link>https://arxiv.org/abs/2504.05015</link>
      <description>arXiv:2504.05015v1 Announce Type: new 
Abstract: Reachability in pushdown vector addition systems with states (PVASS) is among the longest standing open problems in Theoretical Computer Science. We show that the problem is decidable in full generality. Our decision procedure is similar in spirit to the KLMST algorithm for VASS reachability, but works over objects that support an elaborate form of procedure summarization as known from pushdown reachability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05015v1</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roland Guttenberg, Eren Keskin, Roland Meyer</dc:creator>
    </item>
    <item>
      <title>Quantitative Supermartingale Certificates</title>
      <link>https://arxiv.org/abs/2504.05065</link>
      <description>arXiv:2504.05065v1 Announce Type: new 
Abstract: We introduce a general methodology for quantitative model checking and control synthesis with supermartingale certificates. We show that every specification that is invariant to time shifts admits a stochastic invariant that bounds its probability from below; for systems with general state space, the stochastic invariant bounds this probability as closely as desired; for systems with finite state space, it quantifies it exactly. Our result enables the extension of every certificate for the almost-sure satisfaction of shift-invariant specifications to its quantitative counterpart, ensuring completeness up to an approximation in the general case and exactness in the finite-state case. This generalises and unifies existing supermartingale certificates for quantitative verification and control under reachability, safety, reach-avoidance, and stability specifications, as well as asymptotic bounds on accrued costs and rewards. Furthermore, our result provides the first supermartingale certificate for computing upper and lower bounds on the probability of satisfying $\omega$-regular and linear temporal logic specifications. We present an algorithm for quantitative $\omega$-regular verification and control synthesis based on our method and demonstrate its practical efficacy on several infinite-state examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05065v1</guid>
      <category>cs.LO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Abate, Mirco Giacobbe, Diptarko Roy</dc:creator>
    </item>
    <item>
      <title>From Sound Workflow Nets to LTLf Declarative Specifications by Casting Three Spells</title>
      <link>https://arxiv.org/abs/2504.05114</link>
      <description>arXiv:2504.05114v2 Announce Type: new 
Abstract: In process management, effective behavior modeling is essential for understanding execution dynamics and identifying potential issues. Two complementary paradigms have emerged in the pursuit of this objective: the imperative approach, representing all allowed runs of a system in a graph-based model, and the declarative one, specifying the rules that a run must not violate in a constraint-based specification. Extensive studies have been conducted on the synergy and comparisons of the two paradigms. To date, though, whether a declarative specification could be systematically derived from an imperative model such that the original behavior was fully preserved (and if so, how) remained an unanswered question. In this paper, we propose a three-fold contribution. (1) We introduce a systematic approach to synthesize declarative process specifications from safe and sound Workflow nets. (2) We prove behavioral equivalence of the input net with the output specification, alongside related guarantees. (3) We experimentally demonstrate the scalability and compactness of our encoding through tests conducted with synthetic and real-world testbeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05114v2</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Barbaro, Giovanni Varricchione, Marco Montali, Claudio Di Ciccio</dc:creator>
    </item>
    <item>
      <title>Handling the Selection Monad (Full Version)</title>
      <link>https://arxiv.org/abs/2504.03890</link>
      <description>arXiv:2504.03890v1 Announce Type: cross 
Abstract: The selection monad on a set consists of selection functions. These select an element from the set, based on a loss (dually, reward) function giving the loss resulting from a choice of an element. Abadi and Plotkin used the monad to model a language with operations making choices of computations taking account of the loss that would arise from each choice. However, their choices were optimal, and they asked if they could instead be programmer provided.
  In this work, we present a novel design enabling programmers to do so. We present a version of algebraic effect handlers enriched by computational ideas inspired by the selection monad. Specifically, as well as the usual delimited continuations, our new kind of handlers additionally have access to choice continuations, that give the possible future losses. In this way programmers can write operations implementing optimisation algorithms that are aware of the losses arising from their possible choices.
  We give an operational semantics for a higher-order model language $\lambda C$, and establish desirable properties including progress, type soundness, and termination for a subset with a mild hierarchical constraint on allowable operation types. We give this subset a selection monad denotational semantics, and prove soundness and adequacy results. We also present a Haskell implementation and give a variety of programming examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03890v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gordon Plotkin, Ningning Xie</dc:creator>
    </item>
    <item>
      <title>Fractal Origin of the Continuum: A Hypothesis on Process-Relative Definability</title>
      <link>https://arxiv.org/abs/2504.04171</link>
      <description>arXiv:2504.04171v1 Announce Type: cross 
Abstract: We propose a new constructive model of the real continuum based on the notion of fractal definability. Rather than assuming the continuum as a completed uncountable totality, we view it as the cumulative result of a vast space of stratified formal systems, each defining a countable layer of real numbers via constructive means. The union of all such definable layers across all admissible chains yields a set of continuum cardinality, yet no single system or definability path suffices to capture it in full.
  This leads to the Fractal Origin Hypothesis: the apparent uncountability of the real line arises not from actual infinity, but from the meta-theoretical continuity of definability itself. Our framework models the continuum as a process-relative totality, grounded in syntax and layered formal growth. We develop this idea through a formal analysis of definability hierarchies and show that the resulting universe of constructible reals is countable-by-construction (that is, each element is definable within some finite syntactic system, but no single procedure enumerates all of them uniformly) yet inaccessible to any uniform enumeration. The continuum, in this view, is not a static set but a stratified semantic horizon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04171v1</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav Semenov</dc:creator>
    </item>
    <item>
      <title>Separating domains from algebraic domains</title>
      <link>https://arxiv.org/abs/2504.04189</link>
      <description>arXiv:2504.04189v2 Announce Type: cross 
Abstract: We prove that every domain that fails to be algebraic admits the unit interval $[0, 1]$ as its Scott-continuous retract. As a result, every countable domain is algebraic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04189v2</guid>
      <category>math.GN</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaodong Jia, Qingguo Li, Wei Luan</dc:creator>
    </item>
    <item>
      <title>Meta-Mathematics of Computational Complexity Theory</title>
      <link>https://arxiv.org/abs/2504.04416</link>
      <description>arXiv:2504.04416v1 Announce Type: cross 
Abstract: We survey results on the formalization and independence of mathematical statements related to major open problems in computational complexity theory. Our primary focus is on recent findings concerning the (un)provability of complexity bounds within theories of bounded arithmetic. This includes the techniques employed and related open problems, such as the (non)existence of a feasible proof that P = NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04416v1</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3726856.3726862</arxiv:DOI>
      <dc:creator>Igor C. Oliveira</dc:creator>
    </item>
    <item>
      <title>On the Nature of Fractal Numbers and the Classical Continuum Hypothesis (CH)</title>
      <link>https://arxiv.org/abs/2504.04637</link>
      <description>arXiv:2504.04637v2 Announce Type: cross 
Abstract: We propose a reinterpretation of the continuum grounded in the stratified structure of definability rather than classical cardinality. In this framework, a real number is not an abstract point on the number line, but an object expressible at some level Fn of a formal hierarchy. We introduce the notion of "fractal numbers" -- entities defined not within a fixed set-theoretic universe, but through layered expressibility across constructive systems. This reconceptualizes irrationality as a relative property, depending on definability depth, and replaces the binary dichotomy between countable and uncountable sets with a gradated spectrum of definability classes. We show that the classical Continuum Hypothesis loses its force in this context: between aleph_0 and c lies not a single cardinal jump, but a stratified sequence of definitional stages, each forming a countable yet irreducible approximation to the continuum. We argue that the real line should not be seen as a completed totality but as an evolving architecture of formal expressibility. We conclude with a discussion of rational invariants, the relativity of irrationality, and the emergence of a fractal metric for definitional density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04637v2</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav Semenov</dc:creator>
    </item>
    <item>
      <title>Ineffectiveness for Search and Undecidability of PCSP Meta-Problems</title>
      <link>https://arxiv.org/abs/2504.04639</link>
      <description>arXiv:2504.04639v1 Announce Type: cross 
Abstract: It is an open question whether the search and decision versions of promise CSPs are equivalent. Most known algorithms for PCSPs solve only their \emph{decision} variant, and it is unknown whether they can be adapted to solve \emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle a PCSP by finding a solution to a relaxation of some integer program. We prove that rounding those solutions to a proper search certificate can be as hard as any problem in the class TFNP. In other words, these algorithms are ineffective for search. Building on the algebraic approach to PCSPs, we find sufficient conditions that imply ineffectiveness for search. Our tools are tailored to algorithms that are characterized by minions in a suitable way, and can also be used to prove undecidability results for meta-problems. This way, we show that the families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.
  Using the same techniques we also analyze several algebraic conditions that are known to guarantee the tractability of finite-template CSPs. We prove that several meta-problems related to cyclic polymorphims and WNUs are undecidable for PCSPs. In particular, there is no algorithm deciding whether a finite PCSP template (1) admits cyclic a polymorphism, (2) admits a WNU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04639v1</guid>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Larrauri</dc:creator>
    </item>
    <item>
      <title>HypRL: Reinforcement Learning of Control Policies for Hyperproperties</title>
      <link>https://arxiv.org/abs/2504.04675</link>
      <description>arXiv:2504.04675v2 Announce Type: cross 
Abstract: We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04675v2</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour</dc:creator>
    </item>
    <item>
      <title>A Customized SAT-based Solver for Graph Coloring</title>
      <link>https://arxiv.org/abs/2504.04821</link>
      <description>arXiv:2504.04821v1 Announce Type: cross 
Abstract: We introduce ZykovColor, a novel SAT-based algorithm to solve the graph coloring problem working on top of an encoding that mimics the Zykov tree. Our method is based on an approach of H\'ebrard and Katsirelos (2020) that employs a propagator to enforce transitivity constraints, incorporate lower bounds for search tree pruning, and enable inferred propagations. We leverage the recently introduced IPASIR-UP interface for CaDiCal to implement these techniques with a SAT solver. Furthermore, we propose new features that take advantage of the underlying SAT solver. These include modifying the integrated decision strategy with vertex domination hints and using incremental bottom-up search that allows to reuse learned clauses from previous calls. Additionally, we integrate a more efficient clique computation to improve the lower bounds during the search. We validate the effectiveness of each new feature through an experimental analysis. ZykovColor outperforms other state-of-the-art graph coloring implementations on the DIMACS benchmark set. Further experiments on random Erd\H{o}s-R\'enyi graphs show that our new approach dominates state-of-the-art SAT-based methods for both very sparse and highly dense graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04821v1</guid>
      <category>cs.DM</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timo Brand, Daniel Faber, Stephan Held, Petra Mutzel</dc:creator>
    </item>
    <item>
      <title>Lemmanaid: Neuro-Symbolic Lemma Conjecturing</title>
      <link>https://arxiv.org/abs/2504.04942</link>
      <description>arXiv:2504.04942v1 Announce Type: cross 
Abstract: Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Our results indicate that neural and symbolic techniques are complementary. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04942v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yousef Alhessi, S\'olr\'un Halla Einarsd\'ottir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone</dc:creator>
    </item>
    <item>
      <title>Syndicate: Synergistic Synthesis of Ranking Function and Invariants for Termination Analysis</title>
      <link>https://arxiv.org/abs/2404.05951</link>
      <description>arXiv:2404.05951v2 Announce Type: replace 
Abstract: Synthesizing ranking functions is a common technique for proving the termination of loops. A ranking function must be bounded and decrease by a specified amount with each iteration for all reachable program states. However, the set of reachable program states is often unknown, and loop invariants are typically used to overapproximate it. So, proving the termination of a loop requires searching for both a ranking function and a loop invariant. Existing ranking function-based termination analysis techniques can be broadly categorized as (i) those that synthesize the ranking function and invariants independently, (ii) those that combine invariant synthesis with ranking function synthesis into a single query, and (iii) those that offer limited feedback from ranking function synthesis to guide invariant synthesis. These approaches either suffer from having too large a search space or inefficiently exploring the smaller, individual search spaces. In this work, we present a novel termination analysis framework Syndicate, which exploits bi-directional feedback to guide the searches for both ranking functions and invariants, increasing the number of programs that can be proven to terminate and reduces the average time needed to prove termination compared to baselines. Syndicate is general and allows different instantiations of templates, subprocedures, and parameters, offering users the flexibility to optimize the ranking function synthesis. Depending on the templates used, Syndicate is relatively complete and efficient, outperforming existing techniques that achieve at most one of these guarantees. Notably, despite a simpler approach compared to the baselines, Syndicate's performance is either comparable to or better than existing tools in terms of the number of benchmarks proved and average runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05951v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasmin Sarita, Avaljot Singh, Shaurya Gomber, Gagandeep Singh, Mahesh Vishwanathan</dc:creator>
    </item>
    <item>
      <title>Normal Nested Answer Set Programs: Syntactics, Semantics and Logical Calculi</title>
      <link>https://arxiv.org/abs/2412.06407</link>
      <description>arXiv:2412.06407v2 Announce Type: replace 
Abstract: Nested answer set programming (NASP; Lifschitz et al., 1999) generalizes answer set programming (ASP) by admitting nested expressions in rule bodies and heads, and thus, NASP aims at exploiting program succinctness. Yet, although NASP expressiveness is undoubtedly superior to ASP one, the former's reasoning capabilities remain unexplored. This reality seems subsequent to the next existing wide-ranging gap: normal nested programs (NNPs) are not known, or in other words, the nested normal-disjunctive boundary is unidentified thus far. Such an unfavorable situation is yet antagonistic to that of ASP as its normal programs (NPs) have been vital for propelling ASP. We will fill such a gap by defining the NNPs, their semantics and their associated nested logical calculi. Besides, while the unique known way to compute nested programs is unfold them back, we propose to do so in their original form.
  Firstly, we give the syntax of NNPs. For that, we initially define the positive-Horn nested-expressions and then an NNP rule as one whose head (resp. body) is a positive-Horn (resp. general) nested-expression. Secondly, we set up the semantics of NNPs by lifting to the nesting level, classical NP notions including: answer set, minimal and least model, closedness, supported-ness, immediate consequence operator and program consistency. We besides show that NNP restricted to ASP coincides with NP. Thirdly, we introduce nested logical calculi, concretely, nested unit-resolution and nested hyper unit-resolution, proving that they recover unit-resolution and hyper unit-resolution in the ASP setting. We also show how both nested logical calculi allow to process the least model of not-free NNP programs. To end, we demonstrate that computing answer sets of (resp. not-free) NNP programs is (resp. P-complete) NP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06407v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo E. Imaz</dc:creator>
    </item>
    <item>
      <title>A Complete Mental Temporal Logic for Intelligent Agent</title>
      <link>https://arxiv.org/abs/2503.24078</link>
      <description>arXiv:2503.24078v2 Announce Type: replace 
Abstract: In this paper, we present a complete mental temporal logic, called BPICTL, which generalizes CTL by introducing mental modalities. A sound and complete inference system of BPICTL is given. We prove the finite model property of BPICTL. Furthermore, we present a model checking algorithm for BPICTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24078v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zining Cao</dc:creator>
    </item>
    <item>
      <title>Continuous reasoning for adaptive container image distribution in the cloud-edge continuum</title>
      <link>https://arxiv.org/abs/2407.12605</link>
      <description>arXiv:2407.12605v2 Announce Type: replace-cross 
Abstract: Cloud-edge computing requires applications to operate across diverse infrastructures, often triggered by cyber-physical events. Containers offer a lightweight deployment option but pulling images from central repositories can cause delays. This article presents a novel declarative approach and open-source prototype for replicating container images across the cloud-edge continuum. Considering resource availability, network QoS, and storage costs, we leverage logic programming to (i) determine optimal initial placements via Answer Set Programming (ASP) and (ii) adapt placements using Prolog-based continuous reasoning. We evaluate our solution through simulations, showcasing how combining ASP and Prolog continuous reasoning can balance cost optimisation and prompt decision-making in placement adaptation at increasing infrastructure sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12605v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10586-025-05253-9</arxiv:DOI>
      <arxiv:journal_reference>Cluster Computing, 2025</arxiv:journal_reference>
      <dc:creator>Damiano Azzolini, Stefano Forti, Antonio Ielo</dc:creator>
    </item>
    <item>
      <title>Verifying Solutions to Semantics-Guided Synthesis Problems</title>
      <link>https://arxiv.org/abs/2408.15475</link>
      <description>arXiv:2408.15475v2 Announce Type: replace-cross 
Abstract: Semantics-Guided Synthesis (SemGuS) provides a framework to specify synthesis problems in a solver-agnostic and domain-agnostic way, by allowing a user to provide both the syntax and semantics of the language in which the desired program should be synthesized. Because synthesis and verification are closely intertwined, the SemGuS framework raises the problem of how to verify programs in a solver and domain-agnostic way.
  We prove that the problem of verifying whether a program is a valid solution to a SemGuS problem can be reduced to proving validity of a query in the `CLP calculus, a fixed-point logic that generalizes Constrained Horn Clauses and co-Constrained Horn Clauses. Our encoding into `CLP allows us to further classify the SemGuS verification problems into ones that are reducible to validity of (i) first-order-logic formulas, (ii) Constrained Horn Clauses, (iii) co-Constrained Horn Clauses, and (iv) `CLP queries. Furthermore, our encoding shines light on some limitations of the SemGuS framework, such as its inability to model nondeterminism and reactive synthesis. We thus propose a modification to SemGuS that makes it more expressive, and for which verifying solutions is exactly equivalent to proving validity of a query in the `CLP calculus. Our implementation of SemGuS verifiers based on the above encoding can verify instances that were not even encodable in previous work. Furthermore, we use our SemGuS verifiers within an enumeration-based SemGuS solver to correctly synthesize solutions to SemGuS problems that no previous SemGuS synthesizer could solve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15475v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlie Murphy, Keith Johnson, Thomas Reps, Loris D'Antoni</dc:creator>
    </item>
    <item>
      <title>From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems</title>
      <link>https://arxiv.org/abs/2410.18921</link>
      <description>arXiv:2410.18921v2 Announce Type: replace-cross 
Abstract: Consider the math problem: "Lily received 3 cookies from her best friend yesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies. How many cookies does Lily have now?" Many large language models (LLMs) in previous research approach this problem by calculating the answer "1" using the equation "3 - 5 + 3." However, from a human perspective, we recognize the inherent flaw in this problem: Lily cannot eat 5 cookies if she initially only had 3. This discrepancy prompts a key question: Are current LLMs merely Blind Solver that apply mathematical operations without deeper reasoning, or can they function as Logical Thinker capable of identifying logical inconsistencies?
  To explore this question, we propose a benchmark dataset, FaultyMath, which includes faulty math problems of rich diversity: i) multiple mathematical categories, e.g., algebra, geometry, number theory, etc., ii) varying levels of difficulty, and iii) different origins of faultiness -- ranging from violations of common sense and ambiguous statements to mathematical contradictions and more. We evaluate a broad spectrum of LLMs, including open-source, closed-source, and math-specialized models, using FaultyMath across three dimensions: (i) How accurately can the models detect faulty math problems without being explicitly prompted to do so? (ii) When provided with hints -- either correct or misleading -- about the validity of the problems, to what extent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy are the explanations generated by LLMs when they recognize a math problem as flawed? Through extensive experimentation and detailed analysis, our results demonstrate that existing LLMs largely function as Blind Solver and fall short of the reasoning capabilities required to perform as Logical Thinker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18921v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A M Muntasir Rahman, Junyi Ye, Wei Yao, Sierra S. Liu, Jesse Yu, Jonathan Yu, Wenpeng Yin, Guiling Wang</dc:creator>
    </item>
  </channel>
</rss>
