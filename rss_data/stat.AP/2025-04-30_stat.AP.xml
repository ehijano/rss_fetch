<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Apr 2025 04:01:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatio-temporal data fusion of censored threshold exceedances</title>
      <link>https://arxiv.org/abs/2504.20268</link>
      <description>arXiv:2504.20268v1 Announce Type: new 
Abstract: Data fusion models are widely used in air quality monitoring to integrate in situ and remote-sensing data, offering spatially complete and temporally detailed estimates. However, traditional Gaussian-based models often underestimate extreme pollution values, leading to biased risk assessments. To address this, we present a Bayesian hierarchical data fusion framework rooted in extreme value theory, using the Dirac-delta generalised Pareto distribution to jointly account for threshold and non-threshold exceedances while preserving the temporal structure of extreme events. Our model is used to describe and predict censored threshold exceedances of PM2.5 pollution in the Greater London region by using remote sensing observations from the EAC4 dataset, a reanalysis product from the Copernicus Atmospheric Monitoring Service (CAMS), and in situ observation stations from the automatic urban and rural network (AURN) ran by the UK government. Some of our approach's key innovations include combining data with varying spatio-temporal resolutions and fully accounting for parameter uncertainties. Results show that our model outperforms Gaussian-based alternatives and standalone remote-sensing data in predicting threshold exceedances at the majority of observation sites and can even result in improved spatial patterns of PM2.5 pollution than those discernible from the remote-sensing data. Moreover, our approach captures greater variability and spatial patterns, such as higher PM2.5 concentrations near coastal areas, which are not evident in remote-sensing data alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20268v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Daniela Cuba, Craig Wilkie, Marian Scott, Daniela Castro-Camilo</dc:creator>
    </item>
    <item>
      <title>Addressing Data Scarcity in UBEM Validation: Application of Survey Sampling Techniques</title>
      <link>https://arxiv.org/abs/2504.20531</link>
      <description>arXiv:2504.20531v1 Announce Type: new 
Abstract: Urban Building Energy Models (UBEM) are vital for enhancing energy efficiency and sustainability in urban planning. However, data scarcity often challenges their validation, particularly the lack of hourly measured data and the variety of building samples. This study addresses this issue by applying bias adjustment techniques from survey research to improve UBEM validation robustness with incomplete measured data. Error estimation tests are conducted using various levels of missingness, and three bias adjustment methods are employed: multivariate imputation, cell weighting and raking weighting. Key findings indicate that using incomplete data in UBEM validation without adjustment is not advisable, while bias adjustment techniques significantly enhance the robustness of validation, providing more reliable model validity estimates. Cell weighting is preferable in this study due to its reliance on joint distributions of auxiliary variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20531v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunxiao Wang (CSTB, CEEP), Bruno Duplessis (CEEP), Eric Peirano (CSTB), Pascal Schetelat (CSTB), Peter Riederer (CSTB)</dc:creator>
    </item>
    <item>
      <title>Bootstrap Prediction and Confidence Bands for Frequency Response Functions in Posturography</title>
      <link>https://arxiv.org/abs/2504.20588</link>
      <description>arXiv:2504.20588v1 Announce Type: new 
Abstract: The frequency response function (FRF) is an established way to describe the outcome of experiments in posture control literature. The FRF is an empirical transfer function between an input stimulus and the induced body segment sway profile, represented as a vector of complex values associated with a vector of frequencies. For this reason, testing the components of the FRF independently with Bonferroni correction can result in a too-conservative approach. Performing statistics on scalar values defined on the FRF, e.g., comparing the averages, implies an arbitrary decision by the experimenter. This work proposes bootstrap prediction and confidence bands as general methods to evaluate the outcome of posture control experiments, overcoming the foretold limitations of previously used approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20588v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s40799-025-00808-2</arxiv:DOI>
      <arxiv:journal_reference>Lippi V. Bootstrap Prediction and Confidence Bands for Frequency Response Functions in Posturography (2025) Experimental Techniques. 10.1007/s40799-025-00808-2</arxiv:journal_reference>
      <dc:creator>Vittorio Lippi</dc:creator>
    </item>
    <item>
      <title>Flexible extreme thresholds through generalised Bayesian model averaging</title>
      <link>https://arxiv.org/abs/2504.20216</link>
      <description>arXiv:2504.20216v1 Announce Type: cross 
Abstract: Insurance products frequently cover significant claims arising from a variety of sources. To model losses from these products accurately, actuarial models must account for high-severity claims. A widely used strategy is to apply a mixture model, fitting one distribution to losses below a given threshold and modeling excess losses using extreme value theory. However, selecting an appropriate threshold remains an open question with no universally agreed-upon solution. Bayesian Model Averaging (BMA) provides a promising alternative by enabling the simultaneous consideration of multiple thresholds. In this paper, we show that an error integration BMA algorithm can effectively detect heterogeneous optimal thresholds that adapt to predictive variables through the combination of mixture models. This method enhances model accuracy by capturing the full loss distribution and lessening sensitivity to threshold choice. We validate the proposed approach using simulation studies and an application to an automobile claims dataset from a Canadian insurer. As a special case, we also study the homogeneous setting, where a single optimal threshold is selected, and compare it to automatic selection algorithms based on goodness-of-fit tests applied to an actuarial dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Jessup, M\'elina Mailhot, Mathieu Pigeon</dc:creator>
    </item>
    <item>
      <title>Financial Data Analysis with Robust Federated Logistic Regression</title>
      <link>https://arxiv.org/abs/2504.20250</link>
      <description>arXiv:2504.20250v1 Announce Type: cross 
Abstract: In this study, we focus on the analysis of financial data in a federated setting, wherein data is distributed across multiple clients or locations, and the raw data never leaves the local devices. Our primary focus is not only on the development of efficient learning frameworks (for protecting user data privacy) in the field of federated learning but also on the importance of designing models that are easier to interpret. In addition, we care about the robustness of the framework to outliers. To achieve these goals, we propose a robust federated logistic regression-based framework that strives to strike a balance between these goals. To verify the feasibility of our proposed framework, we carefully evaluate its performance not only on independently identically distributed (IID) data but also on non-IID data, especially in scenarios involving outliers. Extensive numerical results collected from multiple public datasets demonstrate that our proposed method can achieve comparable performance to those of classical centralized algorithms, such as Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary and multi-class classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20250v1</guid>
      <category>cs.LG</category>
      <category>q-fin.GN</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Yang, Nikhil Krishnan, Sanjeev R. Kulkarni</dc:creator>
    </item>
    <item>
      <title>Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi</title>
      <link>https://arxiv.org/abs/2504.20276</link>
      <description>arXiv:2504.20276v1 Announce Type: cross 
Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20276v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dandan Chen Kaptur, Yue Huang, Xuejun Ryan Ji, Yanhui Guo, Bradley Kaptur</dc:creator>
    </item>
    <item>
      <title>Sparse mixed linear modeling with anchor-based guidance for high-entropy alloy discovery</title>
      <link>https://arxiv.org/abs/2504.20354</link>
      <description>arXiv:2504.20354v1 Announce Type: cross 
Abstract: High-entropy alloys have attracted attention for their exceptional mechanical properties and thermal stability. However, the combinatorial explosion in the number of possible elemental compositions renders traditional trial-and-error experimental approaches highly inefficient for materials discovery. To solve this problem, machine learning techniques have been increasingly employed for property prediction and high-throughput screening. Nevertheless, highly accurate nonlinear models often suffer from a lack of interpretability, which is a major limitation. In this study, we focus on local data structures that emerge from the greedy search behavior inherent to experimental data acquisition. By introducing a linear and low-dimensional mixture regression model, we strike a balance between predictive performance and model interpretability. In addition, we develop an algorithm that simultaneously performs prediction and feature selection by considering multiple candidate descriptors. Through a case study on high-entropy alloys, this study introduces a method that combines anchor-guided clustering and sparse linear modeling to address biased data structures arising from greedy exploration in materials science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20354v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Murakami, Seiji Miura, Akihiro Endo, Satoshi Minamoto</dc:creator>
    </item>
    <item>
      <title>Scaling and shape of financial returns distributions modeled as conditionally independent random variables</title>
      <link>https://arxiv.org/abs/2504.20488</link>
      <description>arXiv:2504.20488v1 Announce Type: cross 
Abstract: We show that assuming that the returns are independent when conditioned on the value of their variance (volatility), which itself varies in time randomly, then the distribution of returns is well described by the statistics of the sum of conditionally independent random variables. In particular, we show that the distribution of returns can be cast in a simple scaling form, and that its functional form is directly related to the distribution of the volatilities. This approach explains the presence of power-law tails in the returns as a direct consequence of the presence of a power law tail in the distribution of volatilities. It also provides the form of the distribution of Bitcoin returns, which behaves as a stretched exponential, as a consequence of the fact that the Bitcoin volatilities distribution is also closely described by a stretched exponential. We test our predictions with data from the S\&amp;P 500 index, Apple and Paramount stocks; and Bitcoin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20488v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hern\'an Larralde, Roberto Mota Navarro</dc:creator>
    </item>
    <item>
      <title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
      <link>https://arxiv.org/abs/2504.20586</link>
      <description>arXiv:2504.20586v1 Announce Type: cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20586v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Periklis Liaskovitis, Marios Visvardis, Efthymios Efstathiou</dc:creator>
    </item>
    <item>
      <title>Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law</title>
      <link>https://arxiv.org/abs/2408.05231</link>
      <description>arXiv:2408.05231v3 Announce Type: replace 
Abstract: A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05231v3</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/24518492251331375</arxiv:DOI>
      <dc:creator>Bogdan {\L}obodzi\'nski</dc:creator>
    </item>
    <item>
      <title>The Longitudinal Health, Income, and Employment Model (LHIEM): a discrete-time microsimulation model for policy analysis</title>
      <link>https://arxiv.org/abs/2502.02812</link>
      <description>arXiv:2502.02812v2 Announce Type: replace 
Abstract: Dynamic microsimulation has long been recognized as a powerful tool for policy analysis, but in fact most major health policy simulations lack path dependency, a critical feature for evaluating policies that depend on accumulated outcomes such as retirement savings, wealth, or debt. We propose the Longitudinal Health, Income and Employment Model (LHIEM), a path-dependent discrete-time microsimulation that predicts annual health care expenditures, family income, and health status for the U.S. population over a multi-year period. LHIEM advances the population from year to year as a Markov chain with modules capturing the particular dynamics of each predictive attribute. LHIEM was designed to assess a health care financing proposal that would allow individuals to borrow from the U.S. government to cover health care costs, requiring careful tracking of medical expenditures and medical debt over time. However, LHIEM is flexible enough to be used for a range of modeling needs related to predicting health care spending and income over time. In this paper, we present the details of the model and all dynamic modules, and include a case study to demonstrate how LHIEM can be used to evaluate proposed policy changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02812v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.18564/jasss.5591</arxiv:DOI>
      <arxiv:journal_reference>Journal of Artificial Societies and Social Simulation, 28(2), 2025, 1</arxiv:journal_reference>
      <dc:creator>Adrienne M. Propp, Raffaele Vardavas, Carter C. Price, Kandice A. Kapinos</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for the semi-automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v4 Announce Type: replace-cross 
Abstract: This article presents a free and open source toolkit that supports the semi-automated checking of research outputs (SACRO) for privacy disclosure within secure data environments. SACRO is a framework that applies best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. SACRO is designed to assist human checkers rather than seeking to replace them as with current automated rules-based approaches. The toolkit is composed of a lightweight Python package that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This package adds functionality to (i) automatically identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply optional disclosure mitigation strategies as requested; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow and maintain auditable records. This creates an explicit change in the dynamics so that SDC is something done with researchers rather than to them, and enables more efficient communication with checkers. A graphical user interface supports human checkers by displaying the requested output and results of the checks in an immediately accessible format, highlighting identified issues, potential mitigation options, and tracking decisions made. The major analytical programming languages used by researchers (Python, R, and Stata) are supported by providing front-end packages that interface with the core Python back-end. Source code, packages, and documentation are available under MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v4</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects from observational data under truncation by death using survival-incorporated quantiles</title>
      <link>https://arxiv.org/abs/2407.00846</link>
      <description>arXiv:2407.00846v2 Announce Type: replace-cross 
Abstract: The issue of "truncation by death" commonly arises in clinical research: subjects may die before their follow-up assessment, resulting in undefined clinical outcomes. To address this issue, we focus on survival-incorporated quantiles -- quantiles of a composite outcome combining death and clinical outcomes -- to summarize the effect of treatment. Using inverse probability of treatment weighting (IPTW), we propose an estimator for survival-incorporated quantiles from observational data, applicable to settings of both point treatment and time-varying treatments. We establish consistency and asymptotic normality of the estimator under both the true and estimated propensity scores. While the variance properties of IPTW estimators for the mean have been studied, to our knowledge, this article is the first to show that the IPTW quantile estimator using the estimated propensity score yields lower asymptotic variance than the IPTW quantile estimator using the true propensity score. Extensive simulations show that survival-incorporated quantiles provide a simple and useful summary measure and confirm that using the estimated propensity score reduces the root mean square error. We apply our method to estimate the effect of statins on the change in cognitive function, incorporating death, using data from the Long Life Family Study (LLFS) -- a multicenter observational study of 4953 older adults with familial longevity. Our results indicate no significant difference in cognitive decline between statin users and non-users with a similar age- and sex-distribution at baseline. This study not only contributes to understand the cognitive effects of statins but also provides insights into analyzing clinical outcomes in the presence of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00846v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingyan Xiang, Paola Sebastiani, Thomas Perls, Stacy L. Andersen, Svetlana Ukraintseva, Mikael Thinggaard, Judith J. Lok</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Contingency Tables</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v4 Announce Type: replace-cross 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release "redundant" outputs, in the sense that some quantities can be estimated in multiple ways by combining different combinations of privatized values. Indeed, the DP 2020 Decennial Census products published by the U.S. Census Bureau consist of such redundant noisy counts. When redundancy is present, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained by combining different values result in the same estimate) and we show that the minimum variance processing is a linear projection. However, standard projection algorithms are too computationally expensive in terms of both memory and execution time for applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. Finally, we apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v4</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v2 Announce Type: replace-cross 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in evaluating estimator performance, particularly in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy to provide a more robust assessment of efficiency. To compute BRE, we use interquartile range (IQR) overlap to measure precision and apply a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data illustrate that BRE maintains theoretically consistency and interpretability, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
  </channel>
</rss>
