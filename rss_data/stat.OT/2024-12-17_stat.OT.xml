<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2024 03:01:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Scientific Realism vs. Anti-Realism: Toward a Common Ground</title>
      <link>https://arxiv.org/abs/2412.10643</link>
      <description>arXiv:2412.10643v1 Announce Type: new 
Abstract: The debate between scientific realism and anti-realism remains at a stalemate, with reconciliation seeming hopeless. Yet, important work remains: to seek a common ground, even if only to uncover deeper points of disagreement. I develop the idea that everyone values some truths, and use it to benefit both sides of the debate. More specifically, many anti-realists, such as instrumentalists, have yet to seriously engage with Sober's call to justify their preferred version of Ockham's razor through a positive epistemology. Meanwhile, realists face a similar challenge: providing a non-circular explanation of how their version of Ockham's razor connects to truth. Drawing insights from fields that study scientific inference -- statistics and machine learning -- I propose a common ground that addresses these challenges for both sides. This common ground also isolates a distinctively epistemic root of the irreconcilability in the realism debate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10643v1</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Deep Learning-based Approaches for State Space Models: A Selective Review</title>
      <link>https://arxiv.org/abs/2412.11211</link>
      <description>arXiv:2412.11211v1 Announce Type: cross 
Abstract: State-space models (SSMs) offer a powerful framework for dynamical system analysis, wherein the temporal dynamics of the system are assumed to be captured through the evolution of the latent states, which govern the values of the observations. This paper provides a selective review of recent advancements in deep neural network-based approaches for SSMs, and presents a unified perspective for discrete time deep state space models and continuous time ones such as latent neural Ordinary Differential and Stochastic Differential Equations. It starts with an overview of the classical maximum likelihood based approach for learning SSMs, reviews variational autoencoder as a general learning pipeline for neural network-based approaches in the presence of latent variables, and discusses in detail representative deep learning models that fall under the SSM framework. Very recent developments, where SSMs are used as standalone architectural modules for improving efficiency in sequence modeling, are also examined. Finally, examples involving mixed frequency and irregularly-spaced time series data are presented to demonstrate the advantage of SSMs in these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11211v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.OT</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahe Lin, George Michailidis</dc:creator>
    </item>
  </channel>
</rss>
