<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Aug 2024 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Automata-Based Enhancements to RPL: Pioneering Load-Balancing and Traffic Management in IoT</title>
      <link>https://arxiv.org/abs/2408.08373</link>
      <description>arXiv:2408.08373v1 Announce Type: new 
Abstract: The Internet of Things (IoT) signifies a revolutionary technological advancement, enhancing various applications through device interconnectivity while introducing significant challenges due to these devices' limited hardware and communication capabilities. To navigate these complexities, the Internet Engineering Task Force (IETF) has tailored the Routing Protocol for Low-Power and Lossy Networks (RPL) to meet the unique demands of IoT environments. However, RPL struggles with traffic congestion and load distribution issues, negatively impacting network performance and reliability. This paper presents a novel enhancement to RPL by integrating learning automata designed to optimize network traffic distribution. This enhanced protocol, the Learning Automata-based Load-Aware RPL (LALARPL), dynamically adjusts routing decisions based on real-time network conditions, achieving more effective load balancing and significantly reducing network congestion. Extensive simulations reveal that this approach outperforms existing methodologies, leading to notable improvements in packet delivery rates, end-to-end delay, and energy efficiency. The findings highlight the potential of our approach to enhance IoT network operations and extend the lifespan of network components. The effectiveness of learning automata in refining routing processes within RPL offers valuable insights that may drive future advancements in IoT networking, aiming for more robust, efficient, and sustainable network architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08373v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohammadhossein Homaei</dc:creator>
    </item>
    <item>
      <title>Generative AI for Energy Harvesting Internet of Things Network: Fundamental, Applications, and Opportunities</title>
      <link>https://arxiv.org/abs/2408.08496</link>
      <description>arXiv:2408.08496v1 Announce Type: new 
Abstract: Internet of Things (IoT) devices are typically powered by small-sized batteries with limited energy storage capacity, requiring regular replacement or recharging. To reduce costs and maintain connectivity in IoT networks, energy harvesting technologies are regarded as a promising solution. Notably, due to its robust analytical and generative capabilities, generative artificial intelligence (GenAI) has demonstrated significant potential in optimizing energy harvesting networks. Therefore, we discuss key applications of GenAI in improving energy harvesting wireless networks for IoT in this article. Specifically, we first review the key technologies of GenAI and the architecture of energy harvesting wireless networks. Then, we show how GenAI can address different problems to improve the performance of the energy harvesting wireless networks. Subsequently, we present a case study of unmanned aerial vehicle (UAV)-enabled data collection and energy transfer. The case study shows distinctively the necessity of energy harvesting technology and verify the effectiveness of GenAI-based methods. Finally, we discuss some important open directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08496v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwen Xie, Geng Sun, Jiahui Li, Jiacheng Wang, Hongyang Du, Dusit Niyato, Octavia A. Dobre</dc:creator>
    </item>
    <item>
      <title>RPLUW/M: Enabling RPL on the Internet of Underwater Things</title>
      <link>https://arxiv.org/abs/2408.08607</link>
      <description>arXiv:2408.08607v1 Announce Type: new 
Abstract: With the widespread use of the Internet of Things, underwater control and monitoring systems for purposes such as ocean data sampling, natural disaster prevention, underwater surveillance, submarine exploration, and the like have become a popular and challenging topic in computers. So far, various topology control and routing solutions have been proposed for these networks. However, as technology expands and applications grow, so does the need for a stable underwater communication platform. On the other hand, underwater communication is associated with challenges such as node mobility, long propagation delays, low bandwidth, limited resources, and high error rates. In this research, for the first time, a topology control platform based on the RPL tree is modelled by applying its structural changes underwater. The proposed RPLUW methods in the case of RPLUWM fixed nodes are introduced to support the mobility of nodes underwater. Flexible objective functions, utilisation of decision-making systems, and application of control schedules in these methods have increased network life, reduced overhead, and increased node efficiency. The simulation results of the proposed method, in comparison with recent methods in this field, show an increase in network efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08607v1</guid>
      <category>cs.NI</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohammadhossein Homaei</dc:creator>
    </item>
    <item>
      <title>Virtual Reality Traffic Prioritization for Wi-Fi Quality of Service Improvement using Machine Learning Classification Techniques</title>
      <link>https://arxiv.org/abs/2408.08617</link>
      <description>arXiv:2408.08617v1 Announce Type: new 
Abstract: The increase in the demand for eXtended Reality (XR)/Virtual Reality (VR) services in the recent years, poses a great challenge for Wi-Fi networks to maintain the strict latency requirements. In VR over Wi-Fi, latency is a significant issue. In fact, VR users expect instantaneous responses to their interactions, and any noticeable delay can disrupt user experience. Such disruptions can cause motion sickness, and users might end up quitting the service. Differentiating interactive VR traffic from Non-VR traffic within a Wi-Fi network can aim to decrease latency for VR users and improve Wi-Fi Quality of Service (QoS) with giving priority to VR users in the access point (AP) and efficiently handle VR traffic. In this paper, we propose a machine learning-based approach for identifying interactive VR traffic in a Cloud-Edge VR scenario. The correlation between downlink and uplink is crucial in our study. First, we extract features from single-user traffic characteristics and then, we compare six common classification techniques (i.e., Logistic Regression, Support Vector Machines, k-Nearest Neighbors, Decision Trees, Random Forest, and Naive Bayes). For each classifier, a process of hyperparameter tuning and feature selection, namely permutation importance is applied. The model created is evaluated using datasets generated by different VR applications, including both single and multi-user cases. Then, a Wi-Fi network simulator is used to analyze the VR traffic identification and prioritization QoS improvements. Our simulation results show that we successfully reduce VR traffic delays by a factor of 4.2x compared to scenarios without prioritization, while incurring only a 2.3x increase in delay for background (BG) traffic related to Non-VR services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08617v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jnca.2024.103939</arxiv:DOI>
      <arxiv:journal_reference>Journal of Network and Computer Applications, Volume 230, October 2024, 103939</arxiv:journal_reference>
      <dc:creator>Seyedeh Soheila Shaabanzadeh (Universitat Polit\`ecnica de Catalunya), Marc Carrascosa-Zamacois (Universitat Pompeu Fabra), Juan S\'anchez-Gonz\'alez (Universitat Polit\`ecnica de Catalunya), Costas Michaelides (Universitat Pompeu Fabra), Boris Bellalta (Universitat Pompeu Fabra)</dc:creator>
    </item>
    <item>
      <title>Rethinking Generative Semantic Communication for Multi-User Systems with Multi-Modal LLM</title>
      <link>https://arxiv.org/abs/2408.08765</link>
      <description>arXiv:2408.08765v1 Announce Type: new 
Abstract: The surge in connected devices in 6G with typical massive access scenarios, such as smart agriculture, and smart cities, poses significant challenges to unsustainable traditional communication with limited radio resources and already high system complexity. Fortunately, the booming artificial intelligence technology and the growing computational power of devices offer a promising 6G enabler: semantic communication (SemCom). However, existing deep learning-based SemCom paradigms struggle to extend to multi-user scenarios due to their rigid end-to-end training approach. Consequently, to truly empower 6G networks with this critical technology, this article rethinks generative SemCom for multi-user system with multi-modal large language model (MLLM), and propose a novel framework called "M2GSC". In this framework, the MLLM, which serves as shared knowledge base (SKB), plays three critical roles for complex tasks, spawning a series of benefits such as semantic encoding standardization and semantic decoding personalization. Meanwhile, to enhance the performance of M2GSC framework and to advance its implementation in 6G, we highlight three research directions on M2GSC framework, namely, upgrading SKB to closed loop agent, adaptive semantic encoding offloading, and streamlined semantic decoding offloading. Finally, a case study is conducted to demonstrate the preliminary validation on the effectiveness of the M2GSC framework in terms of streamlined decoding offloading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08765v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanting Yang, Zehui Xiong, Shiwen Mao, Tony Q. S. Quek, Ping Zhang, Merouane Debbah, Rahim Tafazolli</dc:creator>
    </item>
    <item>
      <title>An FPTAS for Shortest-Longest Path Problem</title>
      <link>https://arxiv.org/abs/2404.13488</link>
      <description>arXiv:2404.13488v2 Announce Type: replace 
Abstract: Motivated by multi-domain Service Function Chain (SFC) orchestration, we define the Shortest-Longest Path (SLP) problem, prove its hardness, and design an efficient Fully Polynomial Time Approximation Scheme (FPTAS) using the scaling and rounding technique to compute an approximation solution with provable performance guarantee. The SLP problem and its solution algorithm have theoretical significance in multicriteria optimization and also have application potential in QoS routing and multi-domain network resource allocation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13488v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianwei Zhang</dc:creator>
    </item>
    <item>
      <title>FIGRET: Fine-Grained Robustness-Enhanced Traffic Engineering</title>
      <link>https://arxiv.org/abs/2405.04932</link>
      <description>arXiv:2405.04932v3 Announce Type: replace 
Abstract: Traffic Engineering (TE) is critical for improving network performance and reliability. A key challenge in TE is the management of sudden traffic bursts. Existing TE schemes either do not handle traffic bursts or uniformly guard against traffic bursts, thereby facing difficulties in achieving a balance between normal-case performance and burst-case performance. To address this issue, we introduce FIGRET, a Fine-Grained Robustness-Enhanced TE scheme. FIGRET offers a novel approach to TE by providing varying levels of robustness enhancements, customized according to the distinct traffic characteristics of various source-destination pairs. By leveraging a burst-aware loss function and deep learning techniques, FIGRET is capable of generating high-quality TE solutions efficiently. Our evaluations of real-world production networks, including Wide Area Networks and data centers, demonstrate that FIGRET significantly outperforms existing TE schemes. Compared to the TE scheme currently deployed in Jupiter data center networks of Google, FIGRET achieves a 9\%-34\% reduction in average Maximum Link Utilization and improves solution speed by $35\times$-$1800 \times$. Against DOTE, a state-of-the-art deep learning-based TE method, FIGRET substantially lowers the occurrence of significant congestion events triggered by traffic bursts by 41\%-53.9\% in topologies with high traffic dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04932v3</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3651890.3672258</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM SIGCOMM 2024 Conference</arxiv:journal_reference>
      <dc:creator>Ximeng Liu, Shizhen Zhao, Yong Cui, Xinbing Wang</dc:creator>
    </item>
    <item>
      <title>Centralized Network Utility Maximization with Accelerated Gradient Method</title>
      <link>https://arxiv.org/abs/2408.08034</link>
      <description>arXiv:2408.08034v2 Announce Type: replace 
Abstract: Network utility maximization (NUM) is a well-studied problem for network traffic management and resource allocation. Because of the inherent decentralization and complexity of networks, most researches develop decentralized NUM algorithms. In recent years, the Software Defined Networking (SDN) architecture has been widely used, especially in cloud networks and inter-datacenter networks managed by large enterprises, promoting the design of centralized NUM algorithms. To cope with the large and increasing number of flows in such SDN networks, existing researches about centralized NUM focus on the scalability of the algorithm with respect to the number of flows, however the efficiency is ignored. In this paper, we focus on the SDN scenario, and derive a centralized, efficient and scalable algorithm for the NUM problem. By the designing of a smooth utility function and a smooth penalty function, we formulate the NUM problem with a smooth objective function, which enables the use of Nesterov's accelerated gradient method. We prove that the proposed method has $O(d/t^2)$ convergence rate, which is the fastest with respect to the number of iterations $t$, and our method is scalable with respect to the number of flows $d$ in the network. Experiments show that our method obtains accurate solutions with less iterations, and achieves close-to-optimal network utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08034v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>2022 IEEE 30th International Conference on Network Protocols (ICNP), pp. 1-11</arxiv:journal_reference>
      <dc:creator>Ying Tian, Zhiliang Wang, Xia Yin, Xingang Shi, Jiahai Yang, Han Zhang</dc:creator>
    </item>
  </channel>
</rss>
