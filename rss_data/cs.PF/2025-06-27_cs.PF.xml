<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:04:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalable GPU Performance Variability Analysis framework</title>
      <link>https://arxiv.org/abs/2506.20674</link>
      <description>arXiv:2506.20674v1 Announce Type: cross 
Abstract: Analyzing large-scale performance logs from GPU profilers often requires terabytes of memory and hours of runtime, even for basic summaries. These constraints prevent timely insight and hinder the integration of performance analytics into automated workflows. Existing analysis tools typically process data sequentially, making them ill-suited for HPC workflows with growing trace complexity and volume. We introduce a distributed data analysis framework that scales with dataset size and compute availability. Rather than treating the dataset as a single entity, our system partitions it into independently analyzable shards and processes them concurrently across MPI ranks. This design reduces per-node memory pressure, avoids central bottlenecks, and enables low-latency exploration of high-dimensional trace data. We apply the framework to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate its ability to diagnose performance variability, and uncover the impact of memory transfer latency on GPU kernel behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20674v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ankur Lahiry, Ayush Pokharel, Seth Ockerman, Amal Gueroudji, Line Pouchard, Tanzima Z. Islam</dc:creator>
    </item>
    <item>
      <title>Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions</title>
      <link>https://arxiv.org/abs/2506.20677</link>
      <description>arXiv:2506.20677v1 Announce Type: cross 
Abstract: Sorting is an essential operation in computer science with direct consequences on the performance of large scale data systems, real-time systems, and embedded computation. However, no sorting algorithm is optimal under all distributions of data. The new adaptive hybrid sorting paradigm proposed in this paper is the paradigm that automatically selects the most effective sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time monitoring of patterns in input data. The architecture begins by having a feature extraction module to compute significant parameters such as data volume, value range and entropy. These parameters are sent to a decision engine involving Finite State Machine and XGBoost classifier to aid smart and effective in choosing the optimal sorting strategy. It implements Counting Sort on small key ranges, Radix Sort on large range structured input with low-entropy keys and QuickSort on general purpose sorting. The experimental findings of both synthetic and real life dataset confirm that the proposed solution is actually inclined to excel significantly by comparison in execution time, flexibility and the efficiency of conventional static sorting algorithms. The proposed framework provides a scalable, high perhaps and applicable to a wide range of data processing operations like big data analytics, edge computing, and systems with hardware limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20677v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shrinivass Arunachalam Balasubramanian</dc:creator>
    </item>
    <item>
      <title>MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models</title>
      <link>https://arxiv.org/abs/2506.20686</link>
      <description>arXiv:2506.20686v1 Announce Type: cross 
Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20686v1</guid>
      <category>q-bio.BM</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang</dc:creator>
    </item>
    <item>
      <title>GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization</title>
      <link>https://arxiv.org/abs/2506.20807</link>
      <description>arXiv:2506.20807v1 Announce Type: cross 
Abstract: Optimizing GPU kernels for high performance is a complex task, often demanding deep architectural knowledge, extensive profiling, and iterative experimentation. This challenge is amplified when targeting newer or less-documented GPU architectures where traditional development aids are scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a) strategically selecting promising prior code versions as a basis for new iterations; (b) generating hypotheses for optimization experiments, based on existing code and assimilated knowledge from general GPU literature; and (c) autonomously implementing these experiments through code modification and subsequent submission to an external evaluation system, using only observed timing data as performance feedback. We detail how this approach navigates the challenges of the AMD MI300 target architecture and leverages LLMs to compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were embargoed on paper submission date, we present the architectural design, operational workflow, and qualitative insights, highlighting the potential of LLM-driven agents to democratise and accelerate GPU kernel optimization, especially in resource-constrained or rapidly evolving hardware environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20807v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin Andrews, Sam Witteveen</dc:creator>
    </item>
    <item>
      <title>Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe</title>
      <link>https://arxiv.org/abs/2506.20994</link>
      <description>arXiv:2506.20994v1 Announce Type: cross 
Abstract: With the emergence of new high-performance computing (HPC) accelerators, such as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures has become a major challenge for HPC application developers. The increasing hardware diversity in HPC systems often necessitates the development of architecture-specific code, hindering the sustainability of large-scale scientific applications. In this work, we leverage DaCe, a data-centric parallel programming framework, to automate the generation of high-performance kernels. DaCe enables automatic code generation for multicore processors and various accelerators, reducing the burden on developers who would otherwise need to rewrite code for each new architecture. Our study demonstrates DaCe's capabilities by applying its automatic code generation to a critical computational kernel used in Computational Fluid Dynamics (CFD). Specifically, we focus on Neko, a Fortran-based solver that employs the spectral-element method, which relies on small tensor operations. We detail the formulation of this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG) representation and discuss how this approach facilitates high-performance code generation. Additionally, we outline the workflow for seamlessly integrating DaCe's generated code into the Neko solver. Our results highlight the portability and performance of the generated code across multiple platforms, including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive performance results. By demonstrating the potential of automatic code generation, we emphasise the feasibility of using portable solutions to ensure the long-term sustainability of large-scale scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20994v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M{\aa}ns I. Andersson, Martin Karp, Niclas Jansson, Stefano Markidis</dc:creator>
    </item>
    <item>
      <title>Bridding OT and PaaS in Edge-to-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2506.21072</link>
      <description>arXiv:2506.21072v1 Announce Type: cross 
Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides a structured framework for the efficient management and storage of data. It ensures excellent response times while improving security, reliability, data and technology sovereignty, robustness, and energy efficiency, which are crucial for industrial transformation and data sovereignty. This paper illustrates successful deployment, adaptable application management, and various integration components catering to Edge and Cloud environments. It leverages the advantages of the Platform as a Service model and highlights key challenges that have been addressed for specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21072v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Conf{\'e}rence Francophone d'Informatique en Parall{\'e}lisme, Architecture et Syst{\`e}me (COMPAS 2025), INRIA; UNIVERSITE DE BORDEAUX; CNRS, Jun 2025, BORDEAUX, France</arxiv:journal_reference>
      <dc:creator>Carlos J Barrios (LIG, UIS, CITI), Yves Denneulin (LIG, Grenoble INP)</dc:creator>
    </item>
    <item>
      <title>gpu tracker: Python Package for Tracking and Profiling GPU and Other Hardware Utilization in Both Desktop and High-Performance Computing Environments</title>
      <link>https://arxiv.org/abs/2404.01473</link>
      <description>arXiv:2404.01473v3 Announce Type: replace 
Abstract: Over the lifetime of a computing task, determining the maximum usage of random-access memory (RAM) on both the motherboard and on a graphical processing unit (GPU), as well as the utilization percentage of the central processing unit (CPU) and GPU, can be extremely useful for troubleshooting points of failure as well as optimizing memory and processing unit utilization, especially within a high-performance computing (HPC) setting. While there are tools for tracking compute time, CPU utilization, and RAM, including by job management tools themselves, tracking of GPU usage, to our knowledge, does not currently have sufficient solutions, particularly in Unix/Linux operating systems. We present gpu-tracker, a multi-operating system Python package that tracks the computational resource usage of a task while running in the background, including the real compute time that the task takes to complete, its maximum RAM usage, the average and maximum percentage of CPU utilization, the maximum GPU RAM usage, and the average and maximum percentage of GPU utilization for both Nvidia and AMD GPUs. We demonstrate that gpu-tracker can seamlessly track computational resource usage with minimal overhead, both within desktop and HPC execution environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01473v3</guid>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik D. Huckvale, Hunter N. B. Moseley</dc:creator>
    </item>
    <item>
      <title>Anonymized Network Sensing Graph Challenge</title>
      <link>https://arxiv.org/abs/2409.08115</link>
      <description>arXiv:2409.08115v2 Announce Type: replace-cross 
Abstract: The MIT/IEEE/Amazon GraphChallenge encourages community approaches to developing new solutions for analyzing graphs and sparse data derived from social media, sensor feeds, and scientific data to discover relationships between events as they unfold in the field. The anonymized network sensing Graph Challenge seeks to enable large, open, community-based approaches to protecting networks. Many large-scale networking problems can only be solved with community access to very broad data sets with the highest regard for privacy and strong community buy-in. Such approaches often require community-based data sharing. In the broader networking community (commercial, federal, and academia) anonymized source-to-destination traffic matrices with standard data sharing agreements have emerged as a data product that can meet many of these requirements. This challenge provides an opportunity to highlight novel approaches for optimizing the construction and analysis of anonymized traffic matrices using over 100 billion network packets derived from the largest Internet telescope in the world (CAIDA). This challenge specifies the anonymization, construction, and analysis of these traffic matrices. A GraphBLAS reference implementation is provided, but the use of GraphBLAS is not required in this Graph Challenge. As with prior Graph Challenges the goal is to provide a well-defined context for demonstrating innovation. Graph Challenge participants are free to select (with accompanying explanation) the Graph Challenge elements that are appropriate for highlighting their innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08115v2</guid>
      <category>cs.NI</category>
      <category>cs.DM</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <category>math.CO</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/HPEC62836.2024.10938508</arxiv:DOI>
      <dc:creator>Hayden Jananthan, Michael Jones, William Arcand, David Bestor, William Bergeron, Daniel Burrill, Aydin Buluc, Chansup Byun, Timothy Davis, Vijay Gadepally, Daniel Grant, Michael Houle, Matthew Hubbell, Piotr Luszczek, Peter Michaleas, Lauren Milechin, Chasen Milner, Guillermo Morales, Andrew Morris, Julie Mullen, Ritesh Patel, Alex Pentland, Sandeep Pisharody, Andrew Prout, Albert Reuther, Antonio Rosa, Gabriel Wachman, Charles Yee, Jeremy Kepner</dc:creator>
    </item>
  </channel>
</rss>
