<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 05:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Certifiable Boolean Reasoning Is Universal</title>
      <link>https://arxiv.org/abs/2602.05120</link>
      <description>arXiv:2602.05120v1 Announce Type: new 
Abstract: The proliferation of agentic systems has thrust the reasoning capabilities of AI into the forefront of contemporary machine learning. While it is known that there \emph{exist} neural networks which can reason through any Boolean task $f:\{0,1\}^B\to\{0,1\}$, in the sense that they emulate Boolean circuits with fan-in $2$ and fan-out $1$ gates, trained models have been repeatedly demonstrated to fall short of these theoretical ideals. This raises the question: \textit{Can one exhibit a deep learning model which \textbf{certifiably} always reasons and can \textbf{universally} reason through any Boolean task?} Moreover, such a model should ideally require few parameters to solve simple Boolean tasks.
  We answer this question affirmatively by exhibiting a deep learning architecture which parameterizes distributions over Boolean circuits with the guarantee that, for every parameter configuration, a sample is almost surely a valid Boolean circuit (and hence admits an intrinsic circuit-level certificate). We then prove a universality theorem: for any Boolean $f:\{0,1\}^B\to\{0,1\}$, there exists a parameter configuration under which the sampled circuit computes $f$ with arbitrarily high probability. When $f$ is an $\mathcal{O}(\log B)$-junta, the required number of parameters scales linearly with the input dimension $B$.
  Empirically, on a controlled truth-table completion benchmark aligned with our setting, the proposed architecture trains reliably and achieves high exact-match accuracy while preserving the predicted structure: every internal unit is Boolean-valued on $\{0,1\}^B$. Matched MLP baselines reach comparable accuracy, but only about $10\%$ of hidden units admit a Boolean representation; i.e.\ are two-valued over the Boolean cube.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05120v1</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Anastasis Kratsios, Hrad Ghoukasian, Dennis Zvigelsky</dc:creator>
    </item>
    <item>
      <title>Challenges in Solving Sequence-to-Graph Alignment with Co-Linear Structure</title>
      <link>https://arxiv.org/abs/2602.05186</link>
      <description>arXiv:2602.05186v1 Announce Type: new 
Abstract: Sequence alignment is a cornerstone technique in computational biology for assessing similarities and differences among biological sequences. A key variant, sequence-to-graph alignment, plays a crucial role in effectively capturing genetic variations. In this work, we introduce two novel formulations within this framework: the Gap-Sensitive Co-Linear Chaining (Gap-CLC) problem and the Co-Linear Chaining with Errors based on Edit Distance (Edit-CLC) problem, and we investigate their computational complexity. We show that solving the Gap-CLC problem in sub-quadratic time is highly unlikely unless the Strong Exponential Time Hypothesis (SETH) fails -- even when restricted to binary alphabets. Furthermore, we establish that the Edit-CLC problem is NP-hard in the presence of errors within the graph. These findings emphasize that incorporating co-linear structures into sequence-to-graph alignment models fails to reduce computational complexity, highlighting that these models remain at least as computationally challenging to solve as those lacking such prior information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05186v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingfu Li</dc:creator>
    </item>
    <item>
      <title>Graph--Theoretic Analysis of Phase Optimization Complexity in Variational Wave Functions for Heisenberg Antiferromagnets</title>
      <link>https://arxiv.org/abs/2602.04943</link>
      <description>arXiv:2602.04943v1 Announce Type: cross 
Abstract: Despite extensive study, the phase structure of the wavefunctions in frustrated Heisenberg antiferromagnets (HAF) is not yet systematically characterized. In this work, we represent the Hilbert space of an HAF as a weighted graph, which we term the Hilbert graph (HG), whose vertices are spin configurations and whose edges are generated by off-diagonal spin-flip terms of the Heisenberg Hamiltonian, with weights set by products of wavefunction amplitudes. Holding the amplitudes fixed and restricting phases to $\mathbb{Z}_2$ values, the phase-dependent variational energy can be recast as a classical Ising antiferromagnet on the HG, so that phase reconstruction of the ground state reduces to a weighted Max-Cut instance. This shows that phase reconstruction HAF is worst-case NP-hard and provides a direct link between wavefunction sign structure and combinatorial optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04943v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>quant-ph</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahmud Ashraf Shamim, Moshiur Rahman, Mohamed Hibat-Allah, Paulo T Araujo</dc:creator>
    </item>
    <item>
      <title>Reducing the Complexity of Matrix Multiplication to $O(N^2log_2N)$ by an Asymptotically Optimal Quantum Algorithm</title>
      <link>https://arxiv.org/abs/2602.05541</link>
      <description>arXiv:2602.05541v1 Announce Type: cross 
Abstract: Matrix multiplication is a fundamental classical computing operation whose efficiency becomes a major challenge at scale, especially for machine learning applications. Quantum computing, with its inherent parallelism and exponential storage capacity, offers a potential solution to these limitations. This work presents a quantum kernel-based matrix multiplication algorithm (QKMM) that achieves an asymptotically optimal computational complexity of $ O(N^2 \log_2 N) $, outperforming the classical optimal complexity of $ O(N^{2.371552}) $, where $N$ denotes the matrix dimension. Through noiseless and noisy quantum simulation experiments, we demonstrate that the proposed algorithm not only exhibits superior theoretical efficiency but also shows practical advantages in runtime performance and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05541v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Yao, Ding Liu</dc:creator>
    </item>
    <item>
      <title>How do humans succeed in tasks like proving Fermat's Theorem or predicting the Higgs boson?</title>
      <link>https://arxiv.org/abs/2209.09121</link>
      <description>arXiv:2209.09121v2 Announce Type: replace 
Abstract: I discuss issues of inverting feasibly computable functions, optimal discovery algorithms, and the constant overheads in their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.09121v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonid A. Levin</dc:creator>
    </item>
    <item>
      <title>Constructing self-referential instances for the clique problem</title>
      <link>https://arxiv.org/abs/2601.19393</link>
      <description>arXiv:2601.19393v2 Announce Type: replace 
Abstract: In this paper, we propose constructing self-referential instances to reveal the inherent algorithmic hardness of the clique problem. First, we prove the existence of a phase transition phenomenon for the clique problem in the Erd\H{o}s--R\'enyi random graph model and derive an exact location for the transition point. Subsequently, at the transition point, we construct a family of graphs. In this family, each graph shares the same number of vertices, number of edges, and degree sequence, yet both instances containing a $k$-clique and instances without any $k$-clique are included. These two states can be transformed into each other through a symmetric transformation that preserves the degree of every vertex. This property explains why exhaustive search is required in the critical region: an algorithm must search nearly the entire solution space to determine the existence of a solution; otherwise, a counterinstance can be constructed from the original instance using the symmetric transformation. Finally, this paper elaborates on the intrinsic reason for this phenomenon from the independence of the solution space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19393v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Li, Shuli Hu, Xianxian Li, Minghao Yin</dc:creator>
    </item>
    <item>
      <title>Additive Models Explained: A Computational Complexity Approach</title>
      <link>https://arxiv.org/abs/2510.21292</link>
      <description>arXiv:2510.21292v2 Announce Type: replace-cross 
Abstract: Generalized Additive Models (GAMs) are commonly considered *interpretable* within the ML community, as their structure makes the relationship between inputs and outputs relatively understandable. Therefore, it may seem natural to hypothesize that obtaining meaningful explanations for GAMs could be performed efficiently and would not be computationally infeasible. In this work, we challenge this hypothesis by analyzing the *computational complexity* of generating different explanations for various forms of GAMs across multiple contexts. Our analysis reveals a surprisingly diverse landscape of both positive and negative complexity outcomes. Particularly, under standard complexity assumptions such as P!=NP, we establish several key findings: (1) in stark contrast to many other common ML models, the complexity of generating explanations for GAMs is heavily influenced by the structure of the input space; (2) the complexity of explaining GAMs varies significantly with the types of component models used - but interestingly, these differences only emerge under specific input domain settings; (3) significant complexity distinctions appear for obtaining explanations in regression tasks versus classification tasks in GAMs; and (4) expressing complex models like neural networks additively (e.g., as neural additive models) can make them easier to explain, though interestingly, this benefit appears only for certain explanation methods and input domains. Collectively, these results shed light on the feasibility of computing diverse explanations for GAMs, offering a rigorous theoretical picture of the conditions under which such computations are possible or provably hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21292v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahaf Bassan, Michal Moshkovitz, Guy Katz</dc:creator>
    </item>
  </channel>
</rss>
