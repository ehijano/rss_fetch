<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Mar 2025 02:06:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The BIG Argument for AI Safety Cases</title>
      <link>https://arxiv.org/abs/2503.11705</link>
      <description>arXiv:2503.11705v1 Announce Type: new 
Abstract: We present our Balanced, Integrated and Grounded (BIG) argument for assuring the safety of AI systems. The BIG argument adopts a whole-system approach to constructing a safety case for AI systems of varying capability, autonomy and criticality. Whether the AI capability is narrow and constrained or general-purpose and powered by a frontier or foundational model, the BIG argument insists on a meaningful treatment of safety. It respects long-established safety assurance norms such as sensitivity to context, traceability and risk proportionality. Further, it places a particular focus on the novel hazardous behaviours emerging from the advanced capabilities of frontier AI models and the open contexts in which they are rapidly being deployed. These complex issues are considered within a broader AI safety case that approaches assurance from both technical and sociotechnical perspectives. Examples illustrating the use of the BIG argument are provided throughout the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11705v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ibrahim Habli, Richard Hawkins, Colin Paterson, Philippa Ryan, Yan Jia, Mark Sujan, John McDermid</dc:creator>
    </item>
    <item>
      <title>Revisiting the Predictability of Performative, Social Events</title>
      <link>https://arxiv.org/abs/2503.11713</link>
      <description>arXiv:2503.11713v1 Announce Type: new 
Abstract: Social predictions do not passively describe the future; they actively shape it. They inform actions and change individual expectations in ways that influence the likelihood of the predicted outcome. Given these dynamics, to what extent can social events be predicted? This question was discussed throughout the 20th century by authors like Merton, Morgenstern, Simon, and others who considered it a central issue in social science methodology. In this work, we provide a modern answer to this old problem. Using recent ideas from performative prediction and outcome indistinguishability, we establish that one can always efficiently predict social events accurately, regardless of how predictions influence data. While achievable, we also show that these predictions are often undesirable, highlighting the limitations of previous desiderata. We end with a discussion of various avenues forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11713v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan C. Perdomo</dc:creator>
    </item>
    <item>
      <title>Conversation Networks</title>
      <link>https://arxiv.org/abs/2503.11714</link>
      <description>arXiv:2503.11714v2 Announce Type: new 
Abstract: Picture a community torn over a proposed zoning law. Some are angry, others defensive, and misunderstandings abound. On social media, they broadcast insults at one another; every nuanced perspective is reduced to a viral soundbite. Yet, when they meet face-to-face and start speaking, something changes: residents begin listening more than speaking, and people begin testing ideas together. Misunderstandings fade, and trust begins to form. By the end of their discussion, they have not only softened their hostility, but discovered actionable plans that benefit everyone.
  This is the kind of meaningful discourse our society desperately needs. Yet our digital platforms -- designed primarily for maximizing engagement through provocative content -- have pulled us away from these core community endeavours. As a constructive path forward, we introduce the idea of conversation networks as a basis for civic communication infrastructure that combines interoperable digital apps with the thoughtful integration of AI guided by human agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11714v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deb Roy, Lawrence Lessig, Audrey Tang</dc:creator>
    </item>
    <item>
      <title>Survey of City-Wide Homelessness Detection Through Environmental Sensing</title>
      <link>https://arxiv.org/abs/2503.11727</link>
      <description>arXiv:2503.11727v1 Announce Type: new 
Abstract: The growing homelessness crisis in the U.S. presents complex social, economic, and public health challenges, straining shelters, healthcare, and social services while limiting effective interventions. Traditional assessment methods struggle to capture its dynamic, dispersed nature, highlighting the need for scalable, data-driven detection. This survey explores computational approaches across four domains: (1) computer vision and deep learning for identifying encampments and urban indicators, (2) air quality sensing via fixed, mobile, and crowdsourced deployments, (3) IoT and edge computing for real-time urban monitoring, and (4) pedestrian behavior analysis for mobility patterns. Despite advancements, challenges persist in computational constraints, data privacy, accurate environmental measurement, and adaptability. This survey synthesizes recent research, identifies key gaps, and highlights opportunities to enhance homelessness detection, optimize resource allocation, and improve urban planning and social support systems for equitable aid distribution and better neighborhood conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11727v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Gersey, Rose Allegrette, Joshua Lian, Zawad Munshi, Aarti Phatke</dc:creator>
    </item>
    <item>
      <title>LLM Agents for Education: Advances and Applications</title>
      <link>https://arxiv.org/abs/2503.11733</link>
      <description>arXiv:2503.11733v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11733v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S. Yu, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>On Regulating Downstream AI Developers</title>
      <link>https://arxiv.org/abs/2503.11922</link>
      <description>arXiv:2503.11922v1 Announce Type: new 
Abstract: Foundation models - models trained on broad data that can be adapted to a wide range of downstream tasks - can pose significant risks, ranging from intimate image abuse, cyberattacks, to bioterrorism. To reduce these risks, policymakers are starting to impose obligations on the developers of these models. However, downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks by improving a model's capabilities or compromising its safety features. This can make rules on upstream developers ineffective. One way to address this issue could be to impose direct obligations on downstream developers. However, since downstream developers are numerous, diverse, and rapidly growing in number, such direct regulation may be both practically challenging and stifling to innovation. A different approach would be to require upstream developers to mitigate downstream modification risks (e.g. by restricting what modifications can be made). Another approach would be to use alternative policy tools (e.g. clarifying how existing tort law applies to downstream developers or issuing voluntary guidance to help mitigate downstream modification risks). We expect that regulation on upstream developers to mitigate downstream modification risks will be necessary. Although further work is needed, regulation of downstream developers may also be warranted where they retain the ability to increase risk to an unacceptable level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11922v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophie Williams, Jonas Schuett, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance</title>
      <link>https://arxiv.org/abs/2503.11947</link>
      <description>arXiv:2503.11947v1 Announce Type: new 
Abstract: The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11947v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</dc:creator>
    </item>
    <item>
      <title>Privacy Ethics Alignment in AI (PEA-AI): A Stakeholder-Centric Based Framework for Ethcial AI</title>
      <link>https://arxiv.org/abs/2503.11950</link>
      <description>arXiv:2503.11950v1 Announce Type: new 
Abstract: The increasing integration of Artificial Intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups, digital citizens (ages 16-19), parents, educators, and AI professionals, and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from 482 participants through structured surveys, qualitative interviews, and focus groups. The findings reveal distinct privacy expectations- Young users emphasize autonomy and digital freedom, while parents and educators advocate for regulatory oversight and AI literacy programs. AI professionals, in contrast, prioritize the balance between ethical system design and technological efficiency. The data further highlights gaps in AI literacy and transparency, emphasizing the need for comprehensive, stakeholder-driven privacy frameworks that accommodate diverse user needs. Using comparative thematic analysis, this study identifies key tensions in privacy governance and develops the novel Privacy-Ethics Alignment in AI (PEA-AI) model, which structures privacy decision-making as a dynamic negotiation between stakeholders. By systematically analyzing themes such as transparency, user control, risk perception, and parental mediation, this research provides a scalable, adaptive foundation for AI governance, ensuring that privacy protections evolve alongside emerging AI technologies and youth-centric digital interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11950v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</dc:creator>
    </item>
    <item>
      <title>The "recognition," "belief," and "action" regarding conspiracy theories: An empirical study using large-scale samples from Japan and the United States</title>
      <link>https://arxiv.org/abs/2503.12166</link>
      <description>arXiv:2503.12166v1 Announce Type: new 
Abstract: Conspiracy theories present significant societal challenges, shaping political behavior, eroding public trust, and disrupting social cohesion. Addressing their impact requires recognizing that conspiracy engagement is not a singular act but a multi-stage process involving distinct cognitive and behavioral transitions. In this study, we investigate this sequential progression, "recognition," "belief," and "action" (demonstrative action and diffusion action), using nationally representative surveys from the United States (N=13,578) and Japan (N=16,693). Applying a Bayesian hierarchical model, we identify the key social, political, and economic factors that drive engagement at each stage, providing a structured framework for understanding the mechanisms underlying conspiracy theory adoption and dissemination. We find that recognition serves as a crucial gateway determining who transitions to belief, and that demonstrative and diffusion actions are shaped by distinct factors. Demonstrative actions are more prevalent among younger, higher-status individuals with strong political alignments, whereas diffusion actions occur across broader demographics, particularly among those engaged with diverse media channels. Our findings further reveal that early-life economic and cultural capital significantly influence the shape of conspiratorial engagement, emphasizing the role of life-course experiences. These insights highlight the necessity of distinguishing between different forms of conspiracy engagement and highlight the importance of targeted interventions that account for structural, cultural, and psychological factors to mitigate their spread and societal impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12166v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taichi Murayama, Dongwoo Lim, Akira Matsui, Tsukasa Tanihara</dc:creator>
    </item>
    <item>
      <title>ReDefining Code Comprehension: Function Naming as a Mechanism for Evaluating Code Comprehension</title>
      <link>https://arxiv.org/abs/2503.12207</link>
      <description>arXiv:2503.12207v1 Announce Type: new 
Abstract: "Explain in Plain English" (EiPE) questions are widely used to assess code comprehension skills but are challenging to grade automatically. Recent approaches like Code Generation Based Grading (CGBG) leverage large language models (LLMs) to generate code from student explanations and validate its equivalence to the original code using unit tests. However, this approach does not differentiate between high-level, purpose-focused responses and low-level, implementation-focused ones, limiting its effectiveness in assessing comprehension level. We propose a modified approach where students generate function names, emphasizing the function's purpose over implementation details. We evaluate this method in an introductory programming course and analyze it using Item Response Theory (IRT) to understand its effectiveness as exam items and its alignment with traditional EiPE grading standards. We also publish this work as an open source Python package for autograding EiPE questions, providing a scalable solution for adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12207v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David H. Smith IV, Max Fowler, Paul Denny, Craig Zilles</dc:creator>
    </item>
    <item>
      <title>Counting the Trees in the Forest: Evaluating Prompt Segmentation for Classifying Code Comprehension Level</title>
      <link>https://arxiv.org/abs/2503.12216</link>
      <description>arXiv:2503.12216v1 Announce Type: new 
Abstract: Reading and understanding code are fundamental skills for novice programmers, and especially important with the growing prevalence of AI-generated code and the need to evaluate its accuracy and reliability. ``Explain in Plain English'' questions are a widely used approach for assessing code comprehension, but providing automated feedback, particularly on comprehension levels, is a challenging task. This paper introduces a novel method for automatically assessing the comprehension level of responses to ``Explain in Plain English'' questions. Central to this is the ability to distinguish between two response types: multi-structural, where students describe the code line-by-line, and relational, where they explain the code's overall purpose. Using a Large Language Model (LLM) to segment both the student's description and the code, we aim to determine whether the student describes each line individually (many segments) or the code as a whole (fewer segments). We evaluate this approach's effectiveness by comparing segmentation results with human classifications, achieving substantial agreement. We conclude with how this approach, which we release as an open source Python package, could be used as a formative feedback mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12216v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David H. Smith IV, Max Fowler, Paul Denny, Craig Zilles</dc:creator>
    </item>
    <item>
      <title>A novel association and ranking approach identifies factors affecting educational outcomes of STEM majors</title>
      <link>https://arxiv.org/abs/2503.12321</link>
      <description>arXiv:2503.12321v1 Announce Type: new 
Abstract: Improving undergraduate success in STEM requires identifying actionable factors that impact student outcomes, allowing institutions to prioritize key leverage points for change. We examined academic, demographic, and institutional factors that might be associated with graduation rates at two four-year colleges in the northeastern United States using a novel association algorithm called D-basis to rank attributes associated with graduation. Importantly, the data analyzed included tracking data from the National Student Clearinghouse on students who left their original institutions to determine outcomes following transfer.
  Key predictors of successful graduation include performance in introductory STEM courses, the choice of first mathematics class, and flexibility in major selection. High grades in introductory biology, general chemistry, and mathematics courses were strongly correlated with graduation. At the same time, students who switched majors - especially from STEM to non-STEM - had higher overall graduation rates. Additionally, Pell eligibility and demographic factors, though less predictive overall, revealed disparities in time to graduation and retention rates.
  The findings highlight the importance of early academic support in STEM gateway courses and the implementation of institutional policies that provide flexibility in major selection. Enhancing student success in introductory mathematics, biology, and chemistry courses could greatly influence graduation rates. Furthermore, customized mathematics pathways and focused support for STEM courses may assist institutions in optimizing student outcomes. This study offers data-driven insights to guide strategies to increase STEM degree completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12321v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kira Adaricheva, Jonathan T. Brockman, Gillian Z. Elston, Lawrence Hobbie, Skylar Homan, Mohamad Khalefa, Jiyun V. Kim, Rochelle K. Nelson, Sarah Samad, Oren Segal</dc:creator>
    </item>
    <item>
      <title>Synthetic Data for Robust AI Model Development in Regulated Enterprises</title>
      <link>https://arxiv.org/abs/2503.12353</link>
      <description>arXiv:2503.12353v1 Announce Type: new 
Abstract: In today's business landscape, organizations need to find the right balance between using their customers' data ethically to power AI solutions and being compliant regarding data privacy and data usage regulations. In this paper, we discuss synthetic data as a possible solution to this dilemma. Synthetic data is simulated data that mimics the real data. We explore how organizations in heavily regulated industries, such as financial institutions or healthcare organizations, can leverage synthetic data to build robust AI solutions while staying compliant. We demonstrate that synthetic data offers two significant advantages by allowing AI models to learn from more diverse data and by helping organizations stay compliant against data privacy laws with the use of synthetic data instead of customer information. We discuss case studies to show how synthetic data can be effectively used in the finance and healthcare sector while discussing the challenges of using synthetic data and some ethical questions it raises. Our research finds that synthetic data could be a game-changer for AI in regulated industries. The potential can be realized when industry, academia, and regulators collaborate to build solutions. We aim to initiate discussions on the use of synthetic data to build ethical, responsible, and effective AI systems in regulated enterprise industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12353v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE FeedForward Magazine Jan- Mar 2025</arxiv:journal_reference>
      <dc:creator>Aditi Godbole</dc:creator>
    </item>
    <item>
      <title>What is unethical about software? User perceptions in the Netherlands</title>
      <link>https://arxiv.org/abs/2503.12640</link>
      <description>arXiv:2503.12640v1 Announce Type: new 
Abstract: Software has the potential to improve lives. Yet, unethical and uninformed software practices are at the root of an increasing number of ethical concerns. Despite its pervasiveness, few research has analyzed end-users perspectives on the ethical issues of the software they use. We address this gap, and investigate end-user's ethical concerns in software through 19 semi-structured interviews with residents of the Netherlands. We ask a diverse group of users about their ethical concerns when using everyday software applications. We investigate the underlying reasons for their concerns and what solutions they propose to eliminate them. We find that our participants actively worry about privacy, transparency, manipulation, safety and inappropriate content; with privacy and manipulation often being at the center of their worries. Our participants demand software solutions to improve information clarity in applications and provide more control over the user experience. They further expect larger systematic changes within software practices and government regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12640v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yagil Elias, Tom P. Humbert, Lauren Olson, Emitz\'a Guzm\'an</dc:creator>
    </item>
    <item>
      <title>Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching</title>
      <link>https://arxiv.org/abs/2503.11684</link>
      <description>arXiv:2503.11684v1 Announce Type: cross 
Abstract: One of the primary goals of Human-Robot Interaction (HRI) research is to develop robots that can interpret human behavior and adapt their responses accordingly. Adaptive learning models, such as continual and reinforcement learning, play a crucial role in improving robots' ability to interact effectively in real-world settings. However, these models face significant challenges due to the limited availability of real-world data, particularly in sensitive domains like healthcare and well-being. This data scarcity can hinder a robot's ability to adapt to new situations. To address these challenges, causality provides a structured framework for understanding and modeling the underlying relationships between actions, events, and outcomes. By moving beyond mere pattern recognition, causality enables robots to make more explainable and generalizable decisions. This paper presents an exploratory causality-based analysis through a case study of an adaptive robotic coach delivering positive psychology exercises over four weeks in a workplace setting. The robotic coach autonomously adapts to multimodal human behaviors, such as facial valence and speech duration. By conducting both macro- and micro-level causal analyses, this study aims to gain deeper insights into how adaptability can enhance well-being during interactions. Ultimately, this research seeks to advance our understanding of how causality can help overcome challenges in HRI, particularly in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11684v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micol Spitale, Srikar Babu, Serhan Cakmak, Jiaee Cheong, Hatice Gunes</dc:creator>
    </item>
    <item>
      <title>Unlimited Practice Opportunities: Automated Generation of Comprehensive, Personalized Programming Tasks</title>
      <link>https://arxiv.org/abs/2503.11704</link>
      <description>arXiv:2503.11704v1 Announce Type: cross 
Abstract: Generative artificial intelligence (GenAI) offers new possibilities for generating personalized programming exercises, addressing the need for individual practice. However, the task quality along with the student perspective on such generated tasks remains largely unexplored. Therefore, this paper introduces and evaluates a new feature of the so-called Tutor Kai for generating comprehensive programming tasks, including problem descriptions, code skeletons, unit tests, and model solutions. The presented system allows students to freely choose programming concepts and contextual themes for their tasks. To evaluate the system, we conducted a two-phase mixed-methods study comprising (1) an expert rating of 200 automatically generated programming tasks w.r.t. task quality, and (2) a study with 26 computer science students who solved and rated the personalized programming tasks. Results show that experts classified 89.5% of the generated tasks as functional and 92.5% as solvable. However, the system's rate for implementing all requested programming concepts decreased from 94% for single-concept tasks to 40% for tasks addressing three concepts. The student evaluation further revealed high satisfaction with the personalization. Students also reported perceived benefits for learning. The results imply that the new feature has the potential to offer students individual tasks aligned with their context and need for exercise. Tool developers, educators, and, above all, students can benefit from these insights and the system itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11704v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sven Jacobs, Henning Peters, Steffen Jaschke, Natalie Kiesler</dc:creator>
    </item>
    <item>
      <title>PUBLICSPEAK: Hearing the Public with a Probabilistic Framework in Local Government</title>
      <link>https://arxiv.org/abs/2503.11743</link>
      <description>arXiv:2503.11743v1 Announce Type: cross 
Abstract: Local governments around the world are making consequential decisions on behalf of their constituents, and these constituents are responding with requests, advice, and assessments of their officials at public meetings. So many small meetings cannot be covered by traditional newsrooms at scale. We propose PUBLICSPEAK, a probabilistic framework which can utilize meeting structure, domain knowledge, and linguistic information to discover public remarks in local government meetings. We then use our approach to inspect the issues raised by constituents in 7 cities across the United States. We evaluate our approach on a novel dataset of local government meetings and find that PUBLICSPEAK improves over state-of-the-art by 10% on average, and by up to 40%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11743v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianliang Xu, Eva Maxfield Brown, Dustin Dwyer, Sabina Tomkins</dc:creator>
    </item>
    <item>
      <title>Systematic Classification of Studies Investigating Social Media Conversations about Long COVID Using a Novel Zero-Shot Transformer Framework</title>
      <link>https://arxiv.org/abs/2503.11845</link>
      <description>arXiv:2503.11845v1 Announce Type: cross 
Abstract: Long COVID continues to challenge public health by affecting a considerable number of individuals who have recovered from acute SARS-CoV-2 infection yet endure prolonged and often debilitating symptoms. Social media has emerged as a vital resource for those seeking real-time information, peer support, and validating their health concerns related to Long COVID. This paper examines recent works focusing on mining, analyzing, and interpreting user-generated content on social media platforms to capture the broader discourse on persistent post-COVID conditions. A novel transformer-based zero-shot learning approach serves as the foundation for classifying research papers in this area into four primary categories: Clinical or Symptom Characterization, Advanced NLP or Computational Methods, Policy Advocacy or Public Health Communication, and Online Communities and Social Support. This methodology achieved an average confidence of 0.7788, with the minimum and maximum confidence being 0.1566 and 0.9928, respectively. This model showcases the ability of advanced language models to categorize research papers without any training data or predefined classification labels, thus enabling a more rapid and scalable assessment of existing literature. This paper also highlights the multifaceted nature of Long COVID research by demonstrating how advanced computational techniques applied to social media conversations can reveal deeper insights into the experiences, symptoms, and narratives of individuals affected by Long COVID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11845v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur, Niven Francis Da Guia Fernandes, Madje Tobi Marc'Avent Tchona</dc:creator>
    </item>
    <item>
      <title>Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents</title>
      <link>https://arxiv.org/abs/2503.12225</link>
      <description>arXiv:2503.12225v1 Announce Type: cross 
Abstract: This article explores the gaps that can manifest when using a large language model (LLM) to obtain simplified interpretations of data practices from a complex privacy policy. We exemplify these gaps to showcase issues in accuracy, completeness, clarity and representation, while advocating for continued research to realize an LLM's true potential in revolutionizing privacy management through personal assistants and automated compliance checking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12225v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rinku Dewri</dc:creator>
    </item>
    <item>
      <title>Sakshm AI: Advancing AI-Assisted Coding Education for Engineering Students in India Through Socratic Tutoring and Comprehensive Feedback</title>
      <link>https://arxiv.org/abs/2503.12479</link>
      <description>arXiv:2503.12479v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) is reshaping education, particularly in programming, by enhancing problem-solving, enabling personalized feedback, and supporting adaptive learning. Existing AI tools for programming education struggle with key challenges, including the lack of Socratic guidance, direct code generation, limited context retention, minimal adaptive feedback, and the need for prompt engineering. To address these challenges, we introduce Sakshm AI, an intelligent tutoring system for learners across all education levels. It fosters Socratic learning through Disha, its inbuilt AI chatbot, which provides context-aware hints, structured feedback, and adaptive guidance while maintaining conversational memory and supporting language flexibility. This study examines 1170 registered participants, analyzing platform logs, engagement trends, and problem-solving behavior to assess Sakshm AI's impact. Additionally, a structured survey with 45 active users and 25 in-depth interviews was conducted, using thematic encoding to extract qualitative insights. Our findings reveal how AI-driven Socratic guidance influences problem-solving behaviors and engagement, offering key recommendations for optimizing AI-based coding platforms. This research combines quantitative and qualitative insights to inform AI-assisted education, providing a framework for scalable, intelligent tutoring systems that improve learning outcomes. Furthermore, Sakshm AI represents a significant step toward Sustainable Development Goal 4 Quality Education, providing an accessible and structured learning tool for undergraduate students, even without expert guidance. This is one of the first large-scale studies examining AI-assisted programming education across multiple institutions and demographics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12479v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raj Gupta, Harshita Goyal, Dhruv Kumar, Apurv Mehra, Sanchit Sharma, Kashish Mittal, Jagat Sesh Challa</dc:creator>
    </item>
    <item>
      <title>Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model</title>
      <link>https://arxiv.org/abs/2503.12536</link>
      <description>arXiv:2503.12536v1 Announce Type: cross 
Abstract: Image generative models, particularly diffusion-based models, have surged in popularity due to their remarkable ability to synthesize highly realistic images. However, since these models are data-driven, they inherit biases from the training datasets, frequently leading to disproportionate group representations that exacerbate societal inequities. Traditionally, efforts to debiase these models have relied on predefined sensitive attributes, classifiers trained on such attributes, or large language models to steer outputs toward fairness. However, these approaches face notable drawbacks: predefined attributes do not adequately capture complex and continuous variations among groups. To address these issues, we introduce the Debiasing Diffusion Model (DDM), which leverages an indicator to learn latent representations during training, promoting fairness through balanced representations without requiring predefined sensitive attributes. This approach not only demonstrates its effectiveness in scenarios previously addressed by conventional techniques but also enhances fairness without relying on predefined sensitive attributes as conditions. In this paper, we discuss the limitations of prior bias mitigation techniques in diffusion-based models, elaborate on the architecture of the DDM, and validate the effectiveness of our approach through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12536v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin-Chun Huang, Ching Chieh Tsao, Fang-Yi Su, Jung-Hsien Chiang</dc:creator>
    </item>
    <item>
      <title>Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies</title>
      <link>https://arxiv.org/abs/2503.12613</link>
      <description>arXiv:2503.12613v1 Announce Type: cross 
Abstract: Cities are not monolithic; they are arenas of negotiation among groups that hold varying needs, values, and experiences. Conventional methods of urban assessment -- from standardized surveys to AI-driven evaluations -- frequently rely on a single consensus metric (e.g., an average measure of inclusivity or safety). Although such aggregations simplify design decisions, they risk obscuring the distinct perspectives of marginalized populations. In this paper, we present findings from a community-centered study in Montreal involving 35 residents with diverse demographic and social identities, particularly wheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking tasks on 20 urban sites, we observe that disagreements are systematic rather than random, reflecting structural inequalities, differing cultural values, and personal experiences of safety and accessibility.
  Based on these empirical insights, we propose negotiative alignment, an AI framework that treats disagreement as an essential input to be preserved, analyzed, and addressed. Negotiative alignment builds on pluralistic models by dynamically updating stakeholder preferences through multi-agent negotiation mechanisms, ensuring no single perspective is marginalized. We outline how this framework can be integrated into urban analytics -- and other decision-making contexts -- to retain minority viewpoints, adapt to changing stakeholder concerns, and enhance fairness and accountability. The study demonstrates that preserving and engaging with disagreement, rather than striving for an artificial consensus, can produce more equitable and responsive AI-driven outcomes in urban design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12613v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rashid Mushkani, Hugo Berard, Shin Koseki</dc:creator>
    </item>
    <item>
      <title>Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs</title>
      <link>https://arxiv.org/abs/2503.13149</link>
      <description>arXiv:2503.13149v1 Announce Type: cross 
Abstract: We introduce an Item Response Theory (IRT)-based framework to detect and quantify socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods, IRT accounts for item difficulty, improving ideological bias estimation. We fine-tune two LLM families (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct ideological positions and introduce a two-stage approach: (1) modeling response avoidance and (2) estimating perceived bias in answered responses. Our results show that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, challenging prior claims of partisanship. This empirically validated framework enhances AI alignment research and promotes fairer AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13149v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda</dc:creator>
    </item>
    <item>
      <title>Holistic view of the road transportation system based on real-time data sharing mechanism</title>
      <link>https://arxiv.org/abs/2407.03187</link>
      <description>arXiv:2407.03187v4 Announce Type: replace 
Abstract: Traditional manual driving and single-vehicle-based intelligent driving have limitations in real-time and accurate acquisition of the current driving status and intentions of surrounding vehicles, leading to vehicles typically maintaining appropriate safe distances from each other. Yet, accidents still frequently occur, especially in merging areas; meanwhile, it is difficult to comprehensively obtain the conditions of road infrastructure. These limitations not only restrict the further improvement of road capacity but also result in irreparable losses of life and property. To overcome this bottleneck, this paper constructs a space-time global view of the road traffic system based on a real-time sharing mechanism, enabling both road users and managers to timely access the driving intentions of nearby vehicles and the real-time status of road infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03187v4</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Li, Xiang Dong, Junfeng Hao, Ping Yin, Xiaoxue Xu, Maokai Lai, Yuan Li, Ting Peng</dc:creator>
    </item>
    <item>
      <title>Impact of Road Infrastructure and Traffic Scenarios on E-scooterists' Riding and Gaze Behavior</title>
      <link>https://arxiv.org/abs/2407.10310</link>
      <description>arXiv:2407.10310v2 Announce Type: replace 
Abstract: The growing adoption of e-scooters has raised significant safety concerns, particularly due to a surge in injuries and fatalities. This study explores the relationship between road infrastructure, traffic scenarios, and e-scooterists' riding and gaze behaviors to improve road safety and user experience. A naturalistic study was conducted using instrumented e-scooters, capturing gaze patterns, fixation metrics, and head movement data across various road layouts and traffic scenarios. Key findings reveal that bike lanes offer a stable environment with reduced horizontal head movement and focused attention on the road, while shared roads and sidewalks lead to more dispersed gaze and increased head movement, indicating higher uncertainty and complexity. Interactions with other road users, such as navigating intersections, passing buses, riding near cars, and descending on downhill paths, demand greater cognitive load. Intersections require heightened visual focus and spatial awareness, reflected in increased horizontal eye and head movements. Interactions with vehicles prioritize visual scanning over head movement to maintain stability and avoid collisions, while high-speed and downhill riding demand focused attention on obstacles and the road surface. The results provide insights into e-scooter riders' behavior and physiological response analysis, paving the way for safer riding experiences and improved understanding of their needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10310v2</guid>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Transportation &amp; Development (ICTD 2025)</arxiv:journal_reference>
      <dc:creator>Dong Chen, Arman Hosseini, Arik Smith, Zeyang Zheng, David Xiang, Arsalan Heydarian, Omid Shoghli, Bradford Campbell</dc:creator>
    </item>
    <item>
      <title>AI Rules? Characterizing Reddit Community Policies Towards AI-Generated Content</title>
      <link>https://arxiv.org/abs/2410.11698</link>
      <description>arXiv:2410.11698v3 Announce Type: replace 
Abstract: How are Reddit communities responding to AI-generated content? We explored this question through a large-scale analysis of subreddit community rules and their change over time. We collected the metadata and community rules for over $300,000$ public subreddits and measured the prevalence of rules governing AI. We labeled subreddits and AI rules according to existing taxonomies from the HCI literature and a new taxonomy we developed specific to AI rules. While rules about AI are still relatively uncommon, the number of subreddits with these rules more than doubled over the course of a year. AI rules are more common in larger subreddits and communities focused on art or celebrity topics, and less common in those focused on social support. These rules often focus on AI images and evoke, as justification, concerns about quality and authenticity. Overall, our findings illustrate the emergence of varied concerns about AI, in different community contexts. Platform designers and HCI researchers should heed these concerns if they hope to encourage community self-determination in the age of generative AI. We make our datasets public to enable future large-scale studies of community self-governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11698v3</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Travis Lloyd, Jennah Gosciak, Tung Nguyen, Mor Naaman</dc:creator>
    </item>
    <item>
      <title>Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone</title>
      <link>https://arxiv.org/abs/2502.12397</link>
      <description>arXiv:2502.12397v2 Announce Type: replace 
Abstract: Although 85% of sub-Saharan Africa's population is covered by mobile broadband signal, only 37% use the internet, and those who do seldom use the web. The most frequently cited reason for low internet usage is the cost of data. We investigate whether AI can bridge this gap by analyzing 40,350 queries submitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months. Teachers use AI for teaching assistance more frequently than web search. We compare the AI responses to the corresponding top search results for the same queries from the most popular local web search engine, google.com.sl. Only 2% of results for corresponding web searches contain content from in country. Additionally, the average web search result consumes 3,107 times more data than an AI response. Bandwidth alone costs \$2.41 per thousand web search results loaded, while the total cost of AI is \$0.30 per thousand responses. As a result, AI is 87% less expensive than web search. In blinded evaluations, an independent sample of teachers rate AI responses as more relevant, helpful, and correct than web search results. These findings suggest that AI-driven solutions can cost-effectively bridge information gaps in low-connectivity regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12397v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Bj\"orkegren, Jun Ho Choi, Divya Budihal, Dominic Sobhani, Oliver Garrod, Paul Atherton</dc:creator>
    </item>
    <item>
      <title>Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text</title>
      <link>https://arxiv.org/abs/2407.09364</link>
      <description>arXiv:2407.09364v2 Announce Type: replace-cross 
Abstract: The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09364v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3233/FAIA240862</arxiv:DOI>
      <dc:creator>Lucio La Cava, Davide Costa, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>h4rm3l: A language for Composable Jailbreak Attack Synthesis</title>
      <link>https://arxiv.org/abs/2408.04811</link>
      <description>arXiv:2408.04811v3 Announce Type: replace-cross 
Abstract: Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely deployed large language models (LLMs) still have the potential to cause harm to society due to the ineffectiveness of their safety filters, which can be bypassed by prompt transformations called jailbreak attacks. Current approaches to LLM safety assessment, which employ datasets of templated prompts and benchmarking pipelines, fail to cover sufficiently large and diverse sets of jailbreak attacks, leading to the widespread deployment of unsafe LLMs. Recent research showed that novel jailbreak attacks could be derived by composition; however, a formal composable representation for jailbreak attacks, which, among other benefits, could enable the exploration of a large compositional space of jailbreak attacks through program synthesis methods, has not been previously proposed. We introduce h4rm3l, a novel approach that addresses this gap with a human-readable domain-specific language (DSL). Our framework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak attacks as compositions of parameterized string transformation primitives. (2) A synthesizer with bandit algorithms that efficiently generates jailbreak attacks optimized for a target black box LLM. (3) The h4rm3l red-teaming software toolkit that employs the previous two components and an automated harmful LLM behavior classifier that is strongly aligned with human judgment. We demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful novel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and by benchmarking those models against a subset of these synthesized attacks. Our results show that h4rm3l's synthesized attacks are diverse and more successful than existing jailbreak attacks in literature, with success rates exceeding 90% on SOTA LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04811v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning</dc:creator>
    </item>
    <item>
      <title>Cultural Differences and Perverse Incentives in Science Create a Bad Mix: Exploring Country-Level Publication Bias in Select ACM Conferences</title>
      <link>https://arxiv.org/abs/2501.17150</link>
      <description>arXiv:2501.17150v3 Announce Type: replace-cross 
Abstract: In the era of big science, many national governments are helping to build well-funded teams of scientists to serve nationalistic ambitions, providing financial incentives for certain outcomes for purposes other than advancing science. That in turn can impact the behavior of scientists and create distortions in publication rates, frequency, and publication venues targeted. To that end, we provide evidence that indicates significant inequality using standard Gini Index metrics in the publication rates of individual scientists across various groupings (e.g. country, institution type, ranking-level) based on an intensive analysis of thousands of papers published in several well-known ACM conferences (HRI, IUI, KDD, CHI, SIGGRAPH, UIST, and UBICOMP) over 15 years between 2010 to 2024. Furthermore, scientists who were affiliated with the top-5 countries (in terms of research expenditure) were found to be contributing significantly more to the inequality in publication rates than others, which raises a number of questions for the scientific community. We discuss some of those questions later in the paper. We also detected several examples in the dataset of potential serious ethical problems in publications likely caused by such incentive systems. Finally, a topic modeling analysis revealed that some countries are pursuing a much narrower range of scientific topics relative to others, indicating those incentives may also be limiting genuine scientific curiosity. In summary, our findings raise awareness of systems put in place by certain national governments that may be eroding the pursuit of truth through science and gradually undermining the integrity of the global scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17150v3</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aksheytha Chelikavada, Casey C. Bennett</dc:creator>
    </item>
    <item>
      <title>DietGlance: Dietary Monitoring and Personalized Analysis at a Glance with Knowledge-Empowered AI Assistant</title>
      <link>https://arxiv.org/abs/2502.01317</link>
      <description>arXiv:2502.01317v2 Announce Type: replace-cross 
Abstract: Growing awareness of wellness has prompted people to consider whether their dietary patterns align with their health and fitness goals. In response, researchers have introduced various wearable dietary monitoring systems and dietary assessment approaches. However, these solutions are either limited to identifying foods with simple ingredients or insufficient in providing analysis of individual dietary behaviors with domain-specific knowledge. In this paper, we present DietGlance, a system that automatically monitors dietary in daily routines and delivers personalized analysis from knowledge sources. DietGlance first detects ingestive episodes from multimodal inputs using eyeglasses, capturing privacy-preserving meal images of various dishes being consumed. Based on the inferred food items and consumed quantities from these images, DietGlance further provides nutritional analysis and personalized dietary suggestions, empowered by the retrieval augmentation generation module on a reliable nutrition library. A short-term user study (N=33) and a four-week longitudinal study (N=16) demonstrate the usability and effectiveness of DietGlance, offering insights and implications for future AI-assisted dietary monitoring and personalized healthcare intervention systems using eyewear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01317v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihan Jiang, Running Zhao, Lin Lin, Yue Yu, Handi Chen, Xinchen Zhang, Xuhai Xu, Yifang Wang, Xiaojuan Ma, Edith C. H. Ngai</dc:creator>
    </item>
    <item>
      <title>Chasing the Timber Trail: Machine Learning to Reveal Harvest Location Misrepresentation</title>
      <link>https://arxiv.org/abs/2502.14115</link>
      <description>arXiv:2502.14115v2 Announce Type: replace-cross 
Abstract: Illegal logging poses a significant threat to global biodiversity, climate stability, and depresses international prices for legal wood harvesting and responsible forest products trade, affecting livelihoods and communities across the globe. Stable isotope ratio analysis (SIRA) is rapidly becoming an important tool for determining the harvest location of traded, organic, products. The spatial pattern in stable isotope ratio values depends on factors such as atmospheric and environmental conditions and can thus be used for geographic origin identification. We present here the results of a deployed machine learning pipeline where we leverage both isotope values and atmospheric variables to determine timber harvest location. Additionally, the pipeline incorporates uncertainty estimation to facilitate the interpretation of harvest location determination for analysts. We present our experiments on a collection of oak (Quercus spp.) tree samples from its global range. Our pipeline outperforms comparable state-of-the-art models determining geographic harvest origin of commercially traded wood products, and has been used by European enforcement agencies to identify harvest location misrepresentation. We also identify opportunities for further advancement of our framework and how it can be generalized to help identify the origin of falsely labeled organic products throughout the supply chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14115v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shailik Sarkar (Virginia Tech), Raquib Bin Yousuf (Virginia Tech), Linhan Wang (Virginia Tech), Brian Mayer (Virginia Tech), Thomas Mortier (World Forest ID), Victor Deklerck (World Forest ID), Jakub Truszkowski (World Forest ID), John C. Simeone (Simeone Consulting LLC), Marigold Norman (World Forest ID), Jade Saunders (World Forest ID), Chang-Tien Lu (Virginia Tech), Naren Ramakrishnan (Virginia Tech)</dc:creator>
    </item>
    <item>
      <title>Optimizing Product Provenance Verification using Data Valuation Methods</title>
      <link>https://arxiv.org/abs/2502.15177</link>
      <description>arXiv:2502.15177v2 Announce Type: replace-cross 
Abstract: Determining and verifying product provenance remains a critical challenge in global supply chains, particularly as geopolitical conflicts and shifting borders create new incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or agriculture grown on illegally cleared land. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes, has emerged as a powerful tool for geographic origin verification. However, the effectiveness of these models is often constrained by data scarcity and suboptimal dataset selection. In this work, we introduce a novel data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied in SIRA. By prioritizing high-informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. We validate our methodology with extensive experiments, demonstrating its potential to significantly enhance provenance verification, mitigate fraudulent trade practices, and strengthen regulatory enforcement of global supply chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15177v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raquib Bin Yousuf, Hoang Anh Just, Shengzhe Xu, Brian Mayer, Victor Deklerck, Jakub Truszkowski, John C. Simeone, Jade Saunders, Chang-Tien Lu, Ruoxi Jia, Naren Ramakrishnan</dc:creator>
    </item>
    <item>
      <title>Forecasting Open-Weight AI Model Growth on HuggingFace</title>
      <link>https://arxiv.org/abs/2502.15987</link>
      <description>arXiv:2502.15987v2 Announce Type: replace-cross 
Abstract: As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15987v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao</dc:creator>
    </item>
    <item>
      <title>General Scales Unlock AI Evaluation with Explanatory and Predictive Power</title>
      <link>https://arxiv.org/abs/2503.06378</link>
      <description>arXiv:2503.06378v2 Announce Type: replace-cross 
Abstract: Ensuring safe and effective use of AI requires understanding and anticipating its performance on novel tasks, from advanced scientific challenges to transformed workplace activities. So far, benchmarking has guided progress in AI, but it has offered limited explanatory and predictive power for general-purpose AI systems, given the low transferability across diverse tasks. In this paper, we introduce general scales for AI evaluation that can explain what common AI benchmarks really measure, extract ability profiles of AI systems, and predict their performance for new task instances, in- and out-of-distribution. Our fully-automated methodology builds on 18 newly-crafted rubrics that place instance demands on general scales that do not saturate. Illustrated for 15 large language models and 63 tasks, high explanatory power is unleashed from inspecting the demand and ability profiles, bringing insights on the sensitivity and specificity exhibited by different benchmarks, and how knowledge, metacognition and reasoning are affected by model size, chain-of-thought and distillation. Surprisingly, high predictive power at the instance level becomes possible using these demand levels, providing superior estimates over black-box baseline predictors based on embeddings or finetuning, especially in out-of-distribution settings (new tasks and new benchmarks). The scales, rubrics, battery, techniques and results presented here represent a major step for AI evaluation, underpinning the reliable deployment of AI in the years ahead. (Collaborative platform: https://kinds-of-intelligence-cfi.github.io/ADELE.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06378v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lexin Zhou, Lorenzo Pacchiardi, Fernando Mart\'inez-Plumed, Katherine M. Collins, Yael Moros-Daval, Seraphina Zhang, Qinlin Zhao, Yitian Huang, Luning Sun, Jonathan E. Prunty, Zongqian Li, Pablo S\'anchez-Garc\'ia, Kexin Jiang Chen, Pablo A. M. Casares, Jiyun Zu, John Burden, Behzad Mehrbakhsh, David Stillwell, Manuel Cebrian, Jindong Wang, Peter Henderson, Sherry Tongshuang Wu, Patrick C. Kyllonen, Lucy Cheke, Xing Xie, Jos\'e Hern\'andez-Orallo</dc:creator>
    </item>
    <item>
      <title>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy</title>
      <link>https://arxiv.org/abs/2503.09639</link>
      <description>arXiv:2503.09639v2 Announce Type: replace-cross 
Abstract: Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09639v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abe Bohan Hou, Hongru Du, Yichen Wang, Jingyu Zhang, Zixiao Wang, Paul Pu Liang, Daniel Khashabi, Lauren Gardner, Tianxing He</dc:creator>
    </item>
  </channel>
</rss>
