<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:27:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2507.19802</link>
      <description>arXiv:2507.19802v1 Announce Type: new 
Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential algorithmic problem for various other foundational data tasks for AI workloads. Graph-based ANNS indexes have superb empirical trade-offs in indexing cost, query efficiency, and query approximation quality. Most existing graph-based indexes are designed for the static scenario, where there are no updates to the data after the index is constructed. However, full dynamism (insertions, deletions, and searches) is crucial to providing up-to-date responses in applications using vector databases. It is desirable that the index efficiently supports updates and search queries concurrently. Existing dynamic graph-based indexes suffer from at least one of the following problems: (1) the query quality degrades as updates happen; and (2) the graph structure updates used to maintain the index quality upon updates are global and thus expensive. To solve these problems, we propose the CleANN system which consists of three main components: (1) workload-aware linking of diverse search tree descendants to combat distribution shift; (2)query-adaptive on-the-fly neighborhood consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory cleaning to clean up stale information in the data structure and reduce the work spent by the first two components. We evaluate CleANN on 7 diverse datasets on fully dynamic workloads and find that CleANN has query quality at least as good as if the index had been built statically using the corresponding data. In the in-memory setting using 56 hyper-threads, with all types of queries running concurrently, at the same recall level, CleANN achieves 7-1200x throughput improvement on million-scale real-world datasets. To the best of our knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency while maintaining quality under full dynamism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19802v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ziyu Zhang, Yuanhao Wei, Joshua Engels, Julian Shun</dc:creator>
    </item>
    <item>
      <title>TIMEST: Temporal Information Motif Estimator Using Sampling Trees</title>
      <link>https://arxiv.org/abs/2507.20441</link>
      <description>arXiv:2507.20441v1 Announce Type: new 
Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field of graph mining. Edges in real-world networks often have timestamps, so there is a need for temporal motif mining. A temporal motif is a richer structure that imposes timing constraints on the edges of the motif. Temporal motifs have been used to analyze social networks, financial transactions, and biological networks.
  Motif counting in temporal graphs is particularly challenging. A graph with millions of edges can have trillions of temporal motifs, since the same edge can occur with multiple timestamps. There is a combinatorial explosion of possibilities, and state-of-the-art algorithms cannot manage motifs with more than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our approach introduces a temporal spanning tree sampler that leverages weighted sampling to generate substructures of target temporal motifs. This method carefully takes a subset of temporal constraints of the motif that can be jointly and efficiently sampled. TIMEST uses randomized estimation techniques to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation guarantees of TIMEST. We perform an extensive experimental evaluation and show that TIMEST is both faster and more accurate than previous algorithms. Our CPU implementation exhibits an average speedup of 28x over state-of-the-art GPU implementation of the exact algorithm, and 6x speedup over SOTA approximate algorithms while consistently showcasing less than 5% error in most cases. For example, TIMEST can count the number of instances of a financial fraud temporal motif in four minutes with 0.6% error, while exact methods take more than two days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20441v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunjie Pan, Omkar Bhalerao, C. Seshadhri, Nishil Talati</dc:creator>
    </item>
    <item>
      <title>A Functional Data Model and Query Language is All You Need</title>
      <link>https://arxiv.org/abs/2507.20671</link>
      <description>arXiv:2507.20671v1 Announce Type: new 
Abstract: We propose the vision of a functional data model (FDM) and an associated functional query language (FQL). Our proposal has far-reaching consequences: we show a path to come up with a modern QL that solves (almost if not) all problems of SQL (NULL-values, impedance mismatch, SQL injection, missing querying capabilities for updates, etc.). FDM and FQL are much more expressive than the relational model and SQL. In addition, in contrast to SQL, FQL integrates smoothly into existing programming languages. In our approach both QL and PL become the "same thing", thus opening up some interesting holistic optimization opportunities between compilers and databases. In FQL, we also do not need to force application developers to switch to unfamiliar programming paradigms (like SQL or datalog): developers can stick with the abstractions provided by their programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20671v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jens Dittrich</dc:creator>
    </item>
    <item>
      <title>MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation</title>
      <link>https://arxiv.org/abs/2507.20815</link>
      <description>arXiv:2507.20815v1 Announce Type: new 
Abstract: Missing values often limit the usage of data analysis or cause falsification of results. Therefore, methods of missing value imputation (MVI) are of great significance. However, in general, there is no universal, fair MVI method for different tasks. This work thus places MVI in the overall context of data analysis. For this purpose, we present the MVIAnalyzer, a generic framework for a holistic analysis of MVI. It considers the overall process up to the application and analysis of machine learning methods. The associated software is provided and can be used by other researchers for their own analyses. To this end, it further includes a missing value simulation with consideration of relevant parameters. The application of the MVIAnalyzer is demonstrated on data with different characteristics. An evaluation of the results shows the possibilities and limitations of different MVI methods. Since MVI is a very complex topic with different influencing variables, this paper additionally illustrates how the analysis can be supported by visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20815v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerie Restat, Kai Tejkl, Uta St\"orl</dc:creator>
    </item>
    <item>
      <title>Data Cleaning of Data Streams</title>
      <link>https://arxiv.org/abs/2507.20839</link>
      <description>arXiv:2507.20839v1 Announce Type: new 
Abstract: Streaming data can arise from a variety of contexts. Important use cases are continuous sensor measurements such as temperature, light or radiation values. In the process, streaming data may also contain data errors that should be cleaned before further use. Many studies from science and practice focus on data cleaning in a static context. However, in terms of data cleaning, streaming data has particularities that distinguish it from static data. In this paper, we have therefore undertaken an intensive exploration of data cleaning of data streams. We provide a detailed analysis of the applicability of data cleaning to data streams. Our theoretical considerations are evaluated in comprehensive experiments. Using a prototype framework, we show that cleaning is not consistent when working with data streams. An additional contribution is the investigation of requirements for streaming technologies in context of data cleaning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20839v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerie Restat, Niklas Rodenhausen, Carina Antonin, Uta St\"orl</dc:creator>
    </item>
    <item>
      <title>Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems</title>
      <link>https://arxiv.org/abs/2507.19690</link>
      <description>arXiv:2507.19690v1 Announce Type: cross 
Abstract: Though powerful tools for analysis and communication, interactive visualizations often fail to support real-time interaction with large datasets with millions or more records. To highlight and filter data, users indicate values or intervals of interest. Such selections may span multiple components, combine in complex ways, and require optimizations to ensure low-latency updates. We describe Mosaic Selections, a model for representing, managing, and optimizing user selections, in which one or more filter predicates are added to queries that request data for visualizations and input widgets. By analyzing both queries and selection predicates, Mosaic Selections enable automatic optimizations, including pre-aggregating data to rapidly compute selection updates. We contribute a formal description of our selection model and optimization methods, and their implementation in the open-source Mosaic architecture. Benchmark results demonstrate orders-of-magnitude latency improvements for selection-based optimizations over unoptimized queries and existing optimizers for the Vega language. The Mosaic Selection model provides infrastructure for flexible, interoperable filtering across multiple visualizations, alongside automatic optimizations to scale to millions and even billions of records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19690v1</guid>
      <category>cs.HC</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Heer, Dominik Moritz, Ron Pechuk</dc:creator>
    </item>
    <item>
      <title>Integrating Activity Predictions in Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2507.19733</link>
      <description>arXiv:2507.19733v1 Announce Type: cross 
Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, which conflates probability with likelihood and relies on the problematic concept of modal measurements: measurements of future entities. We propose an alternative view, where probabilities are treated as being about process profiles, which better captures the dynamics of real world phenomena. Finally, we demonstrate how our Markov chain based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making. Keywords: predictive analytics, ontology, Markov chains, probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19733v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec Scully, Cameron Stockton, Forrest Hare</dc:creator>
    </item>
    <item>
      <title>Ethereum Conflicts Graphed</title>
      <link>https://arxiv.org/abs/2507.20196</link>
      <description>arXiv:2507.20196v2 Announce Type: cross 
Abstract: Ethereum, a leading blockchain platform, has revolutionized the digital economy by enabling decentralized transactions and the execution of smart contracts. Ethereum transactions form the backbone of its network, facilitating peer-to-peer exchanges and interactions with complex decentralized applications. Smart contracts extend Ethereum's capabilities by automating processes and enabling trustless execution of agreements. Hence, understanding how these smart contracts interact is important in order to facilitate various performance optimizations, such as warming objects before they are being accessed and enabling concurrent execution. Of particular interest to us are the development of the calling graph, as well as the read sets and write sets of invocations within the same block, and the properties of the associated conflict graph that is derived from them. The latter is important for understanding the parallelization potential of smart contracts on Ethereum. We traced upwards of 2 million recent Ethereum blocks using call tracer and prestate tracer, out of a total of 21.4 million blocks at the time of writing. We report on the transactions per block distribution, the structure of call trees in smart contract invocations, the ratio of value-transfer transactions to smart contract invocations, as well as provide a comprehensive study of the structure of blocks' conflict graphs. We find that conflict graphs predominantly show a star like configuration, as well as other noteworthy structural properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20196v2</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dvir David Biton, Roy Friedman, Yaron Hay</dc:creator>
    </item>
    <item>
      <title>The Power of Negation in Higher-Order Datalog</title>
      <link>https://arxiv.org/abs/2507.20251</link>
      <description>arXiv:2507.20251v1 Announce Type: cross 
Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both the well-founded and the stable model semantics, establishing tight connections with complexity classes. We prove that under the well-founded semantics, for all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds without explicit ordering of the input database. The proof of this fact can be performed either by using the powerful existential predicate variables of the language or by using partially applied relations and relation enumeration. Furthermore, we demonstrate that this expressive power is retained within a stratified fragment of the language. Under the stable model semantics, we show that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning and k-NEXP using brave reasoning, again with analogous results for the stratified fragment augmented with choice rules. Our results establish a hierarchy of expressive power, highlighting an interesting trade-off between order and non-determinism in the context of higher-order logic programming: increasing the order of programs under the well-founded semantics can surpass the expressive power of lower-order programs under the stable model semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20251v1</guid>
      <category>cs.PL</category>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelos Charalambidis, Babis Kostopoulos, Christos Nomikos, Panos Rondogiannis</dc:creator>
    </item>
    <item>
      <title>MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)</title>
      <link>https://arxiv.org/abs/2507.20362</link>
      <description>arXiv:2507.20362v1 Announce Type: cross 
Abstract: Location-tracking data from the Automatic Identification System, much of which is publicly available, plays a key role in a range of maritime safety and monitoring applications. However, the data suffers from missing values that hamper downstream applications. Imputing the missing values is challenging because the values of different heterogeneous attributes are updated at diverse rates, resulting in the occurrence of multi-scale dependencies among attributes. Existing imputation methods that assume similar update rates across attributes are unable to capture and exploit such dependencies, limiting their imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based Imputation Network that aims improve imputation accuracy by capturing multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale temporal features for each attribute while preserving their intrinsic heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous graph to explicitly model dependencies between heterogeneous attributes to enable more accurate imputation of missing values through graph propagation. Experimental results on two real-world datasets find that MH-GIN is capable of an average 57% reduction in imputation errors compared to state-of-the-art methods, while maintaining computational efficiency. The source code and implementation details of MH-GIN are publicly available https://github.com/hyLiu1994/MH-GIN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20362v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hengyu Liu, Tianyi Li, Yuqiang He, Kristian Torp, Yushuai Li, Christian S. Jensen</dc:creator>
    </item>
    <item>
      <title>Search-Based Fuzzing For RESTful APIs That Use MongoDB</title>
      <link>https://arxiv.org/abs/2507.20848</link>
      <description>arXiv:2507.20848v1 Announce Type: cross 
Abstract: In RESTful APIs, interactions with a database are a common and crucial aspect. When generating whitebox tests, it is essential to consider the database's state (i.e., the data contained in the database) to achieve higher code coverage and uncover more hidden faults. This article presents novel techniques to enhance search-based software test generation for RESTful APIs interacting with NoSQL databases. Specifically, we target the popular MongoDB database, by dynamically analyzing (via automated code instrumentation) the state of the database during the test generation process. Additionally, to achieve better results, our novel approach allows inserting NoSQL data directly from test cases. This is particularly beneficial when generating the correct sequence of events to set the NoSQL database in an appropriate state is challenging or time-consuming. This method is also advantageous for testing read-only microservices. Our novel techniques are implemented as an extension of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs. Experiments conducted on six RESTful APIs demonstrated significant improvements in code coverage, with increases of up to 18% compared to existing white-box approaches. To better highlight the improvements of our novel techniques, comparisons are also carried out with four state-of-the-art black-box fuzzers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20848v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hernan Ghianni, Man Zhang, Juan P. Galeotti, Andrea Arcuri</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Online Caching: New Upper Bounds</title>
      <link>https://arxiv.org/abs/2410.01760</link>
      <description>arXiv:2410.01760v2 Announce Type: replace 
Abstract: We address the problem of learning-augmented online caching in the scenario when each request is accompanied by a prediction of the next occurrence of the requested page. We improve currently known bounds on the competitive ratio of the BlindOracle algorithm, which evicts a page predicted to be requested last. We also prove a lower bound on the competitive ratio of any randomized algorithm and show that a combination of the BlindOracle with the Marker algorithm achieves a competitive ratio that is optimal up to some constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01760v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Skachkov, Denis Ponomaryov, Yuri Dorn, Alexander Demin</dc:creator>
    </item>
    <item>
      <title>SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in Business Intelligence</title>
      <link>https://arxiv.org/abs/2411.06102</link>
      <description>arXiv:2411.06102v3 Announce Type: replace 
Abstract: With the proliferation of Large Language Models (LLMs) in Business Intelligence (BI), existing solutions face critical challenges in industrial deployments: functionality deficiencies from legacy systems failing to meet evolving LLM-era user demands, interaction limitations from single-round SQL generation paradigms inadequate for multi-round clarification, and cost for domain adaptation arising from cross-domain methods migration.
  We present SiriusBI, a practical LLM-powered BI system addressing the challenges of industrial deployments through three key innovations: (a) An end-to-end architecture integrating multi-module coordination to overcome functionality gaps in legacy systems; (b) A multi-round dialogue with querying mechanism, consisting of semantic completion, knowledge-guided clarification, and proactive querying processes, to resolve interaction constraints in SQL generation; (c) A data-conditioned SQL generation method selection strategy that supports both an efficient one-step Fine-Tuning approach and a two-step method leveraging Semantic Intermediate Representation for low-cost cross-domain applications. Experiments on both real-world datasets and public benchmarks demonstrate the effectiveness of SiriusBI. User studies further confirm that SiriusBI enhances both productivity and user experience.
  As an independent service on Tencent's data platform, SiriusBI is deployed across finance, advertising, and cloud sectors, serving dozens of enterprise clients. It achieves over 93% accuracy in SQL generation and reduces data analysts' query time from minutes to seconds in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06102v3</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Jiang, Haining Xie, Siqi Shen, Yu Shen, Zihan Zhang, Meng Lei, Yifeng Zheng, Yang Li, Chunyou Li, Danqing Huang, Yinjun Wu, Wentao Zhang, Xiaofeng Yang, Bin Cui, Peng Chen</dc:creator>
    </item>
    <item>
      <title>SoftPipe: A Soft-Guided Reinforcement Learning Framework for Automated Data Preparation</title>
      <link>https://arxiv.org/abs/2507.13710</link>
      <description>arXiv:2507.13710v2 Announce Type: replace 
Abstract: Data preparation is a foundational yet notoriously challenging component of the machine learning lifecycle, characterized by a vast combinatorial search space. While reinforcement learning (RL) offers a promising direction, state-of-the-art methods suffer from a critical limitation: to manage the search space, they rely on rigid ``hard constraints'' that prematurely prune the search space and often preclude optimal solutions. To address this, we introduce SoftPipe, a novel RL framework that replaces these constraints with a flexible ``soft guidance'' paradigm. SoftPipe formulates action selection as a Bayesian inference problem. A high-level strategic prior, generated by a Large Language Model (LLM), probabilistically guides exploration. This prior is combined with empirical estimators from two sources through a collaborative process: a fine-grained quality score from a supervised Learning-to-Rank (LTR) model and a long-term value estimate from the agent's Q-function. Through extensive experiments on 18 diverse datasets, we demonstrate that SoftPipe achieves up to a 13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13710v2</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Chang, Chang Liu, Jinbin Huang, Shuyuan Zheng, Rui Mao, Jianbin Qin</dc:creator>
    </item>
    <item>
      <title>TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research</title>
      <link>https://arxiv.org/abs/2503.12730</link>
      <description>arXiv:2503.12730v4 Announce Type: replace-cross 
Abstract: Mechanistic interpretability research faces a gap between analyzing simple circuits in toy tasks and discovering features in large models. To bridge this gap, we propose text-to-SQL generation as an ideal task to study, as it combines the formal structure of toy tasks with real-world complexity. We introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL operations, and train models ranging from 33M to 1B parameters to establish a comprehensive testbed for interpretability. We apply multiple complementary interpretability techniques, including Edge Attribution Patching and Sparse Autoencoders, to identify minimal circuits and components supporting SQL generation. We compare circuits for different SQL subskills, evaluating their minimality, reliability, and identifiability. Finally, we conduct a layerwise logit lens analysis to reveal how models compose SQL queries across layers: from intent recognition to schema resolution to structured generation. Our work provides a robust framework for probing and comparing interpretability methods in a structured, progressively complex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12730v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abir Harrasse, Philip Quirke, Clement Neo, Dhruv Nathawani, Luke Marks, Amir Abdullah</dc:creator>
    </item>
    <item>
      <title>PHast -- Perfect Hashing made fast</title>
      <link>https://arxiv.org/abs/2504.17918</link>
      <description>arXiv:2504.17918v4 Announce Type: replace-cross 
Abstract: Perfect hash functions give unique "names" to arbitrary keys requiring only a few bits per key. This is an essential building block in applications like static hash tables, databases, or bioinformatics. This paper introduces the PHast approach that combines the fastest available queries, very fast construction, and good space consumption (below 2 bits per key). PHast improves bucket-placement which first hashes each key k to a bucket, and then looks for the bucket seed s such that a placement function maps pairs (s,k) in a collision-free way. PHast can use small-range hash functions with linear mapping, fixed-width encoding of seeds, and parallel construction. This is achieved using small overlapping slices of allowed values and bumping to handle unsuccessful seed assignment. A variant we called PHast+ uses additive placement, which enables bit-parallel seed searching, speeding up the construction by an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17918v4</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Beling, Peter Sanders</dc:creator>
    </item>
    <item>
      <title>CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)</title>
      <link>https://arxiv.org/abs/2507.17487</link>
      <description>arXiv:2507.17487v2 Announce Type: replace-cross 
Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where information disclosure is regulated by epistemic dependencies (EDs), a family of logical rules recently proposed for the CQE framework. In particular, we combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground atoms that are entailed by the ontology and can be safely revealed. We focus on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors - an approach that has been shown in other contexts to ensure strong security guarantees with favorable computational behavior. First, we characterize the security of this intersection-based approach and identify a class of EDs (namely, full EDs) for which it remains safe. Then, for a subclass of EDs and for DL-Lite_R ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0 in data complexity by presenting a suitable, detailed first-order rewriting algorithm. Finally, we report on experiments conducted in two different evaluation scenarios, showing the practical feasibility of our rewriting function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17487v2</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lorenzo Marconi, Flavia Ricci, Riccardo Rosati</dc:creator>
    </item>
  </channel>
</rss>
