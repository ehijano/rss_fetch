<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Dec 2025 03:30:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Do Generalized=Gamma Scale Mixtures of Normals Fit Large Image Data-Sets?</title>
      <link>https://arxiv.org/abs/2512.17038</link>
      <description>arXiv:2512.17038v1 Announce Type: new 
Abstract: A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17038v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon Marks, Yash Dave, Zixun Wang, Hannah Chung, Riya Patwa, Simon Cha, Michael Murphy, Alexander Strang</dc:creator>
    </item>
    <item>
      <title>Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models</title>
      <link>https://arxiv.org/abs/2512.17119</link>
      <description>arXiv:2512.17119v1 Announce Type: new 
Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17119v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Pardo, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series</title>
      <link>https://arxiv.org/abs/2512.17758</link>
      <description>arXiv:2512.17758v1 Announce Type: new 
Abstract: We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. Specifically, we leverage functional principal component analysis to efficiently represent a pair of supply and demand curves in a low-dimensional vector space and employ regularized vector autoregressive models for their prediction. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17758v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Koechlin, Filippo Bovera, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2512.17340</link>
      <description>arXiv:2512.17340v1 Announce Type: cross 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17340v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</dc:creator>
    </item>
    <item>
      <title>meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</title>
      <link>https://arxiv.org/abs/2512.17409</link>
      <description>arXiv:2512.17409v1 Announce Type: cross 
Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17409v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05870-6_19</arxiv:DOI>
      <arxiv:journal_reference>MICCAI FAIMI Workshop 2025</arxiv:journal_reference>
      <dc:creator>Dishantkumar Sutariya, Eike Petersen</dc:creator>
    </item>
    <item>
      <title>Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs</title>
      <link>https://arxiv.org/abs/2512.17635</link>
      <description>arXiv:2512.17635v1 Announce Type: cross 
Abstract: Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17635v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Taglieri S\'ao, Olivier Roustant, Geraldo de Freitas Maciel</dc:creator>
    </item>
    <item>
      <title>Perceptions of the Metaverse at the Peak of the Hype Cycle: A Cross-Sectional Study Among Turkish University Students</title>
      <link>https://arxiv.org/abs/2512.17750</link>
      <description>arXiv:2512.17750v1 Announce Type: cross 
Abstract: During the height of the hype in late 2021, the Metaverse drew more attention from around the world than ever before. It promised new ways to interact with people in three-dimensional digital spaces. This cross-sectional study investigates the attitudes, perceptions, and predictors of the willingness to engage with the Metaverse among 381 Turkish university students surveyed in December 2021. The study employs Fisher's Exact Tests and binary logistic regression to assess the influence of demographic characteristics, prior digital experience, and perception-based factors. The results demonstrate that demographic factors, such as gender, educational attainment, faculty association, social media engagement, and previous virtual reality exposure, do not significantly forecast the propensity to participate in the Metaverse. Instead, the main things that affect people's intentions to adopt are how they see things. Belief in the Metaverse's capacity to revolutionize societal frameworks, especially human rights, surfaced as the most significant positive predictor of willingness. Conversely, apprehensions regarding psychological harm, framed as a possible 'cyber syndrome' represented a significant obstacle to participation. Perceptions of technical compatibility and ethical considerations showed complex effects, showing that optimism, uncertainty, and indifference affect willingness in different ways. In general, the results show that early adoption of the Metaverse is based on how people see it, not on their demographics. The research establishes a historically informed benchmark of user skepticism and prudent assessment during the advent of Web 3.0, underscoring the necessity of addressing collective psychological, ethical, and normative issues to promote future engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17750v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehmet Ali Erkan, Halil Eren Ko\c{c}ak</dc:creator>
    </item>
    <item>
      <title>Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</title>
      <link>https://arxiv.org/abs/2512.17850</link>
      <description>arXiv:2512.17850v1 Announce Type: cross 
Abstract: This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17850v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Corey M. Abramson</dc:creator>
    </item>
    <item>
      <title>Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling</title>
      <link>https://arxiv.org/abs/2502.03480</link>
      <description>arXiv:2502.03480v2 Announce Type: replace 
Abstract: Evaluating the predictive performance of species distribution models (SDMs) under realistic deployment scenarios requires careful handling of spatial and temporal dependencies in the data. Cross-validation (CV) is the standard approach for model evaluation, but its design strongly influences the validity of performance estimates. When SDMs are intended for spatial or temporal transfer, random CV can lead to overoptimistic results due to spatial autocorrelation (SAC) among neighboring observations.
  We benchmark four machine learning algorithms (GBM, XGBoost, LightGBM, Random Forest) on two real-world presence-absence datasets, a temperate plant and an anadromous fish, using multiple CV designs: random, spatial, spatio-temporal, environmental, and forward-chaining. Two training data usage strategies (LAST FOLD and RETRAIN) are evaluated, with hyperparameter tuning performed within each CV scheme. Model performance is assessed on independent out-of-time test sets using AUC, MAE, and correlation metrics.
  Random CV overestimates AUC by up to 0.16 and produces MAE values up to 80 percent higher than spatially blocked alternatives. Blocking at the empirical SAC range substantially reduces this bias. Training strategy affects evaluation outcomes: LAST FOLD yields smaller validation-test discrepancies under strong SAC, while RETRAIN achieves higher test AUC when SAC is weaker. Boosted ensemble models consistently perform best under spatially structured CV designs. We recommend a robust SDM workflow based on SAC-aware blocking, blocked hyperparameter tuning, and external temporal validation to improve reliability under spatial and temporal shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03480v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ecoinf.2025.103521</arxiv:DOI>
      <arxiv:journal_reference>Ecological Informatics, Volume 92, Article 103521, 2025</arxiv:journal_reference>
      <dc:creator>Diana Koldasbayeva, Alexey Zaytsev</dc:creator>
    </item>
    <item>
      <title>A Structure-Preserving Assessment of VBPBB for Time Series Imputation Under Periodic Trends, Noise, and Missingness Mechanisms</title>
      <link>https://arxiv.org/abs/2508.19535</link>
      <description>arXiv:2508.19535v3 Announce Type: replace 
Abstract: Incomplete time-series data compromise statistical inference, particularly when the underlying process exhibits periodic structure (e.g., annual or monthly cycles). Conventional imputation procedures rarely account for such temporal dependence, leading to attenuation of seasonal signals and biased estimates. This study proposes and evaluates a structure-preserving multiple imputation framework that augments imputation models with frequency-specific covariates derived via the Variable Bandpass Periodic Block Bootstrap (VBPBB). In controlled simulations, we generate series with annual and monthly components, impose Gaussian noise across low, moderate, and high signal-to-noise regimes, and introduce Missing Completely at Random (MCAR) patterns from 5% to 70% missingness. Dominant periodic components are extracted with VBPBB, resampled to stabilize uncertainty, and incorporated as covariates in Amelia II. Compared with baseline methods that do not model temporal structure, the VBPBB-enhanced approach consistently yields lower imputation error and superior retention of periodic features, with the largest gains observed under high noise and when multiple components are included. These findings demonstrate that explicitly modeling periodic content during imputation improves reconstruction accuracy and preserves time-series structure in the presence of substantial missingness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19535v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Ahmad, Eric J Rose, Michael Roy, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Opening the House: Datasets for Mixed Doubles Curling</title>
      <link>https://arxiv.org/abs/2512.16574</link>
      <description>arXiv:2512.16574v2 Announce Type: replace 
Abstract: We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16574v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robyn Ritchie, Alexandre Leblanc, Thomas Loughin</dc:creator>
    </item>
    <item>
      <title>Efficient Sampling in Disease Surveillance through Subpopulations: Sampling Canaries in the Coal Mine</title>
      <link>https://arxiv.org/abs/2405.10742</link>
      <description>arXiv:2405.10742v2 Announce Type: replace-cross 
Abstract: We consider outbreak detection settings of endemic diseases where the population under study consists of various subpopulations available for stratified surveillance. These subpopulations can for example be based on age cohorts, but may also correspond to other subgroups of the population under study such as international travellers. Rather than sampling uniformly across the population, one may elevate the effectiveness of the detection methodology by optimally choosing a sampling subpopulation. We show (under some assumptions) the relative sampling efficiency between two subpopulations is inversely proportional to the ratio of their respective baseline disease risks. This implies one can increase sampling efficiency by sampling from the subpopulation with higher baseline disease risk. Our results require careful treatment of the power curves of exact binomial tests as a function of their sample size, which are non-monotonic due to the underlying discreteness. A case study of COVID-19 cases in the Netherlands illustrates our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10742v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2025.110384</arxiv:DOI>
      <arxiv:journal_reference>Statistics and Probablity Letters 2025, Vol. 222, 110384</arxiv:journal_reference>
      <dc:creator>Ivo V. Stoepker</dc:creator>
    </item>
    <item>
      <title>Saturation-Aware Snapshot Compressive Imaging: Theory and Algorithm</title>
      <link>https://arxiv.org/abs/2501.11869</link>
      <description>arXiv:2501.11869v3 Announce Type: replace-cross 
Abstract: Snapshot Compressive Imaging (SCI) uses coded masks to compress a 3D data cube into a single 2D snapshot. In practice, multiplexing can push intensities beyond the sensor's dynamic range, producing saturation that violates the linear SCI model and degrades reconstruction. This paper provides the first theoretical characterization of SCI recovery under saturation. We model clipping as an element-wise nonlinearity and derive a finite-sample recovery bound for compression-based SCI that links reconstruction error to mask density and the extent of saturation. The analysis yields a clear design rule: optimal Bernoulli masks use densities below one-half, decreasing further as saturation strengthens. Guided by this principle, we optimize mask patterns and introduce a novel reconstruction framework, Saturation-Aware PnP Net (SAPnet), which explicitly enforces consistency with saturated measurements. Experiments on standard video-SCI benchmarks confirm our theory and demonstrate that SAPnet significantly outperforms existing PnP-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11869v3</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyu Zhao, Shirin Jalali</dc:creator>
    </item>
    <item>
      <title>Non-parametric kernel density estimation of magnitude distribution for the analysis of seismic hazard posed by anthropogenic seismicity</title>
      <link>https://arxiv.org/abs/2503.04393</link>
      <description>arXiv:2503.04393v2 Announce Type: replace-cross 
Abstract: Frequent significant deviations of the observed magnitude distribution of anthropogenic seismicity from the Gutenberg-Richter relation require alternative estimation methods for probabilistic seismic hazard assessments. We evaluate five nonparametric kernel density estimation (KDE) methods on simulated samples drawn from four magnitude distribution models: the exponential, concave and convex bi-exponential, and exponential-Gaussian distributions. The latter three represent deviations from the Gutenberg-Richter relation due to the finite thickness of the seismogenic crust and the effect of characteristic earthquakes. The assumed deviations from exponentiality are never more than those met in practice. The studied KDE methods include Silverman's and Scott's rules with Abramson's bandwidth adaptation, two diffusion-based methods (ISJ and diffKDE), and adaptiveKDE, which formulates the bandwidth estimation as an optimization problem. We assess their performance for magnitudes from 2 to 6 with sample sizes of 400 to 5000, using the mean integrated square error (MISE) over 100,000 simulations. Their suitability in hazard assessments is illustrated by the mean of the mean return period (MRP) for a sample size of 1000. Among the tested methods, diffKDE provides the most accurate cumulative distribution function estimates for larger magnitudes. Even when the data is drawn from an exponential distribution, diffKDE performs comparably to maximum likelihood estimation when the sample size is at least 1000. Given that anthropogenic seismicity often deviates from the exponential model, we recommend using diffKDE for probabilistic seismic hazard assessments whenever a sufficient sample size is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04393v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11600-025-01762-8</arxiv:DOI>
      <dc:creator>Francis Tong, Stanis{\l}aw Lasocki, Beata Orlecka-Sikora</dc:creator>
    </item>
    <item>
      <title>Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</title>
      <link>https://arxiv.org/abs/2512.12783</link>
      <description>arXiv:2512.12783v2 Announce Type: replace-cross 
Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 T\"U\.IK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12783v2</guid>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atalay Denknalbant, Emre Sezdi, Zeki Furkan Kutlu, Polat Goktas</dc:creator>
    </item>
  </channel>
</rss>
