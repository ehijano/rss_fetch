<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Greedy on Preorder is Linear for Preorder Initial Tree</title>
      <link>https://arxiv.org/abs/2407.03666</link>
      <description>arXiv:2407.03666v1 Announce Type: new 
Abstract: The (preorder) traversal conjecture states that starting with an initial tree, the cost to search a sequence $S=(s_1,s_2,\dots,s_n) \in [n]^n$ in a binary search tree (BST) algorithm is $O(n)$, where $S$ is obtained by a preorder traversal of some BST. The sequence $S$ is called a preorder sequence.
  For Splay trees (candidate for dynamic optimality conjecture), the preorder traversal holds only when the initial tree is empty (Levy and Tarjan, WADS 2019). The preorder traversal conjecture for GREEDY (candidate for dynamic optimality conjecture) was known to be $n2^{\alpha(n)^{O(1)}}$ (Chalermsook et al., FOCS 2015), which was recently improved to $O(n2^{\alpha(n)})$ (Chalermsook et al., SODA 2023), here $\alpha(n)$ is the inverse Ackermann function of $n$. For a special case when the initial tree is flat, GREEDY is known to satisfy the traversal conjecture, i.e., $O(n)$ (Chalermsook et al., FOCS 2015).
  In this paper, we show that for every preorder sequence $S$, there exists an initial tree called the preorder initial tree for which GREEDY satisfies the preorder traversal conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03666v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Pareek</dc:creator>
    </item>
    <item>
      <title>Multiway Cuts with a Choice of Representatives</title>
      <link>https://arxiv.org/abs/2407.03877</link>
      <description>arXiv:2407.03877v1 Announce Type: new 
Abstract: In this paper, we study several generalizations of multiway cut where the terminals can be chosen as \emph{representatives} from sets of \emph{candidates} $T_1,\ldots,T_q$. In this setting, one is allowed to choose these representatives so that the minimum-weight cut separating these sets \emph{via their representatives} is as small as possible. We distinguish different cases depending on (A) whether the representative of a candidate set has to be separated from the other candidate sets completely or only from the representatives, and (B) whether there is a single representative for each candidate set or the choice of representative is independent for each pair of candidate sets. For fixed $q$, we give approximation algorithms for each of these problems that match the best known approximation guarantee for multiway cut. Our technical contribution is a new extension of the CKR relaxation that preserves approximation guarantees. For general $q$, we show $o(\log q)$-inapproximability for all cases where the choice of representatives may depend on the pair of candidate sets, as well as for the case where the goal is to separate a fixed node from a single representative from each candidate set. As a positive result, we give a $2$-approximation algorithm for the case where we need to choose a single representative from each candidate set. This is a generalization of the $(2-2/k)$-approximation for k-cut, and we can solve it by relating the tree case to optimization over a gammoid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03877v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.MFCS.2024.18</arxiv:DOI>
      <dc:creator>Krist\'of B\'erczi, Tam\'as Kir\'aly, Daniel P. Szabo</dc:creator>
    </item>
    <item>
      <title>Near-optimal Size Linear Sketches for Hypergraph Cut Sparsifiers</title>
      <link>https://arxiv.org/abs/2407.03934</link>
      <description>arXiv:2407.03934v1 Announce Type: new 
Abstract: A $(1 \pm \epsilon)$-sparsifier of a hypergraph $G(V,E)$ is a (weighted) subgraph that preserves the value of every cut to within a $(1 \pm \epsilon)$-factor. It is known that every hypergraph with $n$ vertices admits a $(1 \pm \epsilon)$-sparsifier with $\tilde{O}(n/\epsilon^2)$ hyperedges. In this work, we explore the task of building such a sparsifier by using only linear measurements (a \emph{linear sketch}) over the hyperedges of $G$, and provide nearly-matching upper and lower bounds for this task.
  Specifically, we show that there is a randomized linear sketch of size $\widetilde{O}(n r \log(m) / \epsilon^2)$ bits which with high probability contains sufficient information to recover a $(1 \pm \epsilon)$ cut-sparsifier with $\tilde{O}(n/\epsilon^2)$ hyperedges for any hypergraph with at most $m$ edges each of which has arity bounded by $r$. This immediately gives a dynamic streaming algorithm for hypergraph cut sparsification with an identical space complexity, improving on the previous best known bound of $\widetilde{O}(n r^2 \log^4(m) / \epsilon^2)$ bits of space (Guha, McGregor, and Tench, PODS 2015). We complement our algorithmic result above with a nearly-matching lower bound. We show that for every $\epsilon \in (0,1)$, one needs $\Omega(nr \log(m/n) / \log(n))$ bits to construct a $(1 \pm \epsilon)$-sparsifier via linear sketching, thus showing that our linear sketch achieves an optimal dependence on both $r$ and $\log(m)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03934v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sanjeev Khanna, Aaron L. Putterman, Madhu Sudan</dc:creator>
    </item>
    <item>
      <title>Fair Repetitive Interval Scheduling</title>
      <link>https://arxiv.org/abs/2407.03987</link>
      <description>arXiv:2407.03987v1 Announce Type: new 
Abstract: Fair resource allocation is undoubtedly a crucial factor in customer satisfaction in several scheduling scenarios. This is especially apparent in repetitive scheduling models where the same set of clients repeatedly submits jobs on a daily basis. In this paper, we aim to analyze a repetitive scheduling system involving a set of $n$ clients and a set of $m$ days. On every day, each client submits a request to process a job exactly within a specific time interval, which may vary from day to day, modeling the scenario where the scheduling is done Just-In-Time (JIT). The daily schedule is executed on a single machine that can process a single job at a time, therefore it is not possible to schedule jobs with intersecting time intervals. Accordingly, a feasible solution corresponds to sets of jobs with disjoint time intervals, one set per day. We define the quality of service (QoS) that a client receives as the number of executed jobs over the $m$ days period. Our objective is to provide a feasible solution where each client has at least $k$ days where his jobs are processed. We prove that this problem is NP-hard even under various natural restrictions such as identical processing times and day-independent due dates. We also provide efficient algorithms for several special cases and analyze the parameterized tractability of the problem with respect to several parameters, providing both parameterized hardness and tractability results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03987v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Heeger, Danny Hermelin, Yuval Itzhaki, Hendrik Molter, Dvir Shabtay</dc:creator>
    </item>
    <item>
      <title>Improved Outerplanarity Bounds for Planar Graphs</title>
      <link>https://arxiv.org/abs/2407.04282</link>
      <description>arXiv:2407.04282v1 Announce Type: new 
Abstract: In this paper, we study the outerplanarity of planar graphs, i.e., the number of times that we must (in a planar embedding that we can initially freely choose) remove the outerface vertices until the graph is empty. It is well-known that there are $n$-vertex graphs with outerplanarity $\tfrac{n}{6}+\Theta(1)$, and not difficult to show that the outerplanarity can never be bigger. We give here improved bounds of the form $\tfrac{n}{2g}+2g+O(1)$, where $g$ is the fence-girth, i.e., the length of the shortest cycle with vertices on both sides. This parameter $g$ is at least the connectivity of the graph, and often bigger; for example, our results imply that planar bipartite graphs have outerplanarity $\tfrac{n}{8}+O(1)$. We also show that the outerplanarity of a planar graph $G$ is at most $\tfrac{1}{2}$diam$(G)+O(\sqrt{n})$, where diam$(G)$ is the diameter of the graph. All our bounds are tight up to smaller-order terms, and a planar embedding that achieves the outerplanarity bound can be found in linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04282v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Therese Biedl, Debajyoti Mondal</dc:creator>
    </item>
    <item>
      <title>A $\frac{4}{3}$-Approximation for the Maximum Leaf Spanning Arborescence Problem in DAGs</title>
      <link>https://arxiv.org/abs/2407.04342</link>
      <description>arXiv:2407.04342v1 Announce Type: new 
Abstract: The Maximum Leaf Spanning Arborescence problem (MLSA) is defined as follows: Given a directed graph $G$ and a vertex $r\in V(G)$ from which every other vertex is reachable, find a spanning arborescence rooted at $r$ maximizing the number of leaves (vertices with out-degree zero). The MLSA has applications in broadcasting, where a message needs to be transferred from a source vertex to all other vertices along the arcs of an arborescence in a given network. In doing so, it is desirable to have as many vertices as possible that only need to receive, but not pass on messages since they are inherently cheaper to build.
  We study polynomial-time approximation algorithms for the MLSA. For general digraphs, the state-of-the-art is a $\min\{\sqrt{\mathrm{OPT}},92\}$-approximation. In the (still APX-hard) special case where the input graph is acyclic, the best known approximation guarantee of $\frac{7}{5}$ is due to Fernandes and Lintzmayer: They prove that any $\alpha$-approximation for the \emph{hereditary $3$-set packing problem}, a special case of weighted $3$-set packing, yields a $\max\{\frac{4}{3},\alpha\}$-approximation for the MLSA in acyclic digraphs (dags), and provide a $\frac{7}{5}$-approximation for the hereditary $3$-set packing problem. In this paper, we obtain a $\frac{4}{3}$-approximation for the hereditary $3$-set packing problem, and, thus, also for the MLSA in dags. In doing so, we manage to leverage the full potential of the reduction provided by Fernandes and Lintzmayer. The algorithm that we study is a simple local search procedure considering swaps of size up to $10$. Its analysis relies on a two-stage charging argument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04342v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meike Neuwohner</dc:creator>
    </item>
    <item>
      <title>Bicriterial Approximation for the Incremental Prize-Collecting Steiner-Tree Problem</title>
      <link>https://arxiv.org/abs/2407.04447</link>
      <description>arXiv:2407.04447v1 Announce Type: new 
Abstract: We consider an incremental variant of the rooted prize-collecting Steiner-tree problem with a growing budget constraint. While no incremental solution exists that simultaneously approximates the optimum for all budgets, we show that a bicriterial $(\alpha,\mu)$-approximation is possible, i.e., a solution that with budget $B+\alpha$ for all $B \in \mathbb{R}_{\geq 0}$ is a multiplicative $\mu$-approximation compared to the optimum solution with budget $B$. For the case that the underlying graph is a tree, we present a polynomial-time density-greedy algorithm that computes a $(\chi,1)$-approximation, where $\chi$ denotes the eccentricity of the root vertex in the underlying graph, and show that this is best possible. An adaptation of the density-greedy algorithm for general graphs is $(\gamma,2)$-competitive where $\gamma$ is the maximal length of a vertex-disjoint path starting in the root. While this algorithm does not run in polynomial time, it can be adapted to a $(\gamma,3)$-competitive algorithm that runs in polynomial time. We further devise a capacity-scaling algorithm that guarantees a $(3\chi,8)$-approximation and, more generally, a $\smash{\bigl((4\ell - 1)\chi, \frac{2^{\ell + 2}}{2^{\ell}-1}\bigr)}$-approximation for every fixed $\ell \in \mathbb{N}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04447v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yann Disser, Svenja M. Griesbach, Max Klimm, Annette Lutz</dc:creator>
    </item>
    <item>
      <title>Rapid Mixing via Coupling Independence for Spin Systems with Unbounded Degree</title>
      <link>https://arxiv.org/abs/2407.04672</link>
      <description>arXiv:2407.04672v1 Announce Type: new 
Abstract: We develop a new framework to prove the mixing or relaxation time for the Glauber dynamics on spin systems with unbounded degree. It works for general spin systems including both $2$-spin and multi-spin systems. As applications for this approach:
  $\bullet$ We prove the optimal $O(n)$ relaxation time for the Glauber dynamics of random $q$-list-coloring on an $n$-vertices triangle-tree graph with maximum degree $\Delta$ such that $q/\Delta &gt; \alpha^\star$, where $\alpha^\star \approx 1.763$ is the unique positive solution of the equation $\alpha = \exp(1/\alpha)$. This improves the $n^{1+o(1)}$ relaxation time for Glauber dynamics obtained by the previous work of Jain, Pham, and Vuong (2022). Besides, our framework can also give a near-linear time sampling algorithm under the same condition.
  $\bullet$ We prove the optimal $O(n)$ relaxation time and near-optimal $\widetilde{O}(n)$ mixing time for the Glauber dynamics on hardcore models with parameter $\lambda$ in $\textit{balanced}$ bipartite graphs such that $\lambda &lt; \lambda_c(\Delta_L)$ for the max degree $\Delta_L$ in left part and the max degree $\Delta_R$ of right part satisfies $\Delta_R = O(\Delta_L)$. This improves the previous result by Chen, Liu, and Yin (2023).
  At the heart of our proof is the notion of $\textit{coupling independence}$ which allows us to consider multiple vertices as a huge single vertex with exponentially large domain and do a "coarse-grained" local-to-global argument on spin systems. The technique works for general (multi) spin systems and helps us obtain some new comparison results for Glauber dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04672v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Chen, Weiming Feng</dc:creator>
    </item>
    <item>
      <title>Near-optimal hierarchical matrix approximation from matrix-vector products</title>
      <link>https://arxiv.org/abs/2407.04686</link>
      <description>arXiv:2407.04686v1 Announce Type: new 
Abstract: We describe a randomized algorithm for producing a near-optimal hierarchical off-diagonal low-rank (HODLR) approximation to an $n\times n$ matrix $\mathbf{A}$, accessible only though matrix-vector products with $\mathbf{A}$ and $\mathbf{A}^{\mathsf{T}}$. We prove that, for the rank-$k$ HODLR approximation problem, our method achieves a $(1+\beta)^{\log(n)}$-optimal approximation in expected Frobenius norm using $O(k\log(n)/\beta^3)$ matrix-vector products. In particular, the algorithm obtains a $(1+\varepsilon)$-optimal approximation with $O(k\log^4(n)/\varepsilon^3)$ matrix-vector products, and for any constant $c$, an $n^c$-optimal approximation with $O(k \log(n))$ matrix-vector products. Apart from matrix-vector products, the additional computational cost of our method is just $O(n \operatorname{poly}(\log(n), k, \beta))$. We complement the upper bound with a lower bound, which shows that any matrix-vector query algorithm requires at least $\Omega(k\log(n) + k/\varepsilon)$ queries to obtain a $(1+\varepsilon)$-optimal approximation.
  Our algorithm can be viewed as a robust version of widely used "peeling" methods for recovering HODLR matrices and is, to the best of our knowledge, the first matrix-vector query algorithm to enjoy theoretical worst-case guarantees for approximation by any hierarchical matrix class. To control the propagation of error between levels of hierarchical approximation, we introduce a new perturbation bound for low-rank approximation, which shows that the widely used Generalized Nystr\"om method enjoys inherent stability when implemented with noisy matrix-vector products. We also introduced a novel randomly perforated matrix sketching method to further control the error in the peeling algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04686v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Feyza Duman Keles, Diana Halikias, Cameron Musco, Christopher Musco, David Persson</dc:creator>
    </item>
    <item>
      <title>Algorithmic Results for Weak Roman Domination Problem in Graphs</title>
      <link>https://arxiv.org/abs/2407.03812</link>
      <description>arXiv:2407.03812v1 Announce Type: cross 
Abstract: Consider a graph $G = (V, E)$ and a function $f: V \rightarrow \{0, 1, 2\}$. A vertex $u$ with $f(u)=0$ is defined as \emph{undefended} by $f$ if it lacks adjacency to any vertex with a positive $f$-value. The function $f$ is said to be a \emph{Weak Roman Dominating function} (WRD function) if, for every vertex $u$ with $f(u) = 0$, there exists a neighbour $v$ of $u$ with $f(v) &gt; 0$ and a new function $f': V \rightarrow \{0, 1, 2\}$ defined in the following way: $f'(u) = 1$, $f'(v) = f(v) - 1$, and $f'(w) = f(w)$, for all vertices $w$ in $V\setminus\{u,v\}$; so that no vertices are undefended by $f'$. The total weight of $f$ is equal to $\sum_{v\in V} f(v)$, and is denoted as $w(f)$. The \emph{Weak Roman Domination Number} denoted by $\gamma_r(G)$, represents $min\{w(f)~\vert~f$ is a WRD function of $G\}$. For a given graph $G$, the problem of finding a WRD function of weight $\gamma_r(G)$ is defined as the \emph{Minimum Weak Roman domination problem}. The problem is already known to be NP-hard for bipartite and chordal graphs. In this paper, we further study the algorithmic complexity of the problem. We prove the NP-hardness of the problem for star convex bipartite graphs and comb convex bipartite graphs, which are subclasses of bipartite graphs. In addition, we show that for the bounded degree star convex bipartite graphs, the problem is efficiently solvable. We also prove the NP-hardness of the problem for split graphs, a subclass of chordal graphs. On the positive side, we give polynomial-time algorithms to solve the problem for $P_4$-sparse graphs. Further, we have presented some approximation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03812v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaustav Paul, Ankit Sharma, Arti Pandey</dc:creator>
    </item>
    <item>
      <title>Quantum spectral method for gradient and Hessian estimation</title>
      <link>https://arxiv.org/abs/2407.03833</link>
      <description>arXiv:2407.03833v1 Announce Type: cross 
Abstract: Gradient descent is one of the most basic algorithms for solving continuous optimization problems. In [Jordan, PRL, 95(5):050501, 2005], Jordan proposed the first quantum algorithm for estimating gradients of functions close to linear, with exponential speedup in the black-box model. This quantum algorithm was greatly enhanced and developed by [Gily\'en, Arunachalam, and Wiebe, SODA, pp. 1425-1444, 2019], providing a quantum algorithm with optimal query complexity $\widetilde{\Theta}(\sqrt{d}/\varepsilon)$ for a class of smooth functions of $d$ variables, where $\varepsilon$ is the accuracy. This is quadratically faster than classical algorithms for the same problem.
  In this work, we continue this research by proposing a new quantum algorithm for another class of functions, namely, analytic functions $f(\boldsymbol{x})$ which are well-defined over the complex field. Given phase oracles to query the real and imaginary parts of $f(\boldsymbol{x})$ respectively, we propose a quantum algorithm that returns an $\varepsilon$-approximation of its gradient with query complexity $\widetilde{O}(1/\varepsilon)$. This achieves exponential speedup over classical algorithms in terms of the dimension $d$. As an extension, we also propose two quantum algorithms for Hessian estimation, aiming to improve quantum analogs of Newton's method. The two algorithms have query complexity $\widetilde{O}(d/\varepsilon)$ and $\widetilde{O}(d^{1.5}/\varepsilon)$, respectively, under different assumptions. Moreover, if the Hessian is promised to be $s$-sparse, we then have two new quantum algorithms with query complexity $\widetilde{O}(s/\varepsilon)$ and $\widetilde{O}(sd/\varepsilon)$, respectively. The former achieves exponential speedup over classical algorithms. We also prove a lower bound of $\widetilde{\Omega}(d)$ for Hessian estimation in the general case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03833v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Zhang, Changpeng Shao</dc:creator>
    </item>
    <item>
      <title>Reconfiguration of Independent Transversals</title>
      <link>https://arxiv.org/abs/2407.04367</link>
      <description>arXiv:2407.04367v1 Announce Type: cross 
Abstract: Given integers $\Delta\ge 2$ and $t\ge 2\Delta$, suppose there is a graph of maximum degree $\Delta$ and a partition of its vertices into blocks of size at least $t$. By a seminal result of Haxell, there must be some independent set of the graph that is transversal to the blocks, a so-called independent transversal. We show that, if moreover $t\ge2\Delta+1$, then every independent transversal can be transformed within the space of independent transversals to any other through a sequence of one-vertex modifications, showing connectivity of the so-called reconfigurability graph of independent transversals.
  This is sharp in that for $t=2\Delta$ (and $\Delta\ge 2$) the connectivity conclusion can fail. In this case we show furthermore that in an essential sense it can only fail for the disjoint union of copies of the complete bipartite graph $K_{\Delta,\Delta}$. This constitutes a qualitative strengthening of Haxell's theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04367v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pjotr Buys, Ross J. Kang, Kenta Ozeki</dc:creator>
    </item>
    <item>
      <title>Improved algorithms for learning quantum Hamiltonians, via flat polynomials</title>
      <link>https://arxiv.org/abs/2407.04540</link>
      <description>arXiv:2407.04540v1 Announce Type: cross 
Abstract: We give an improved algorithm for learning a quantum Hamiltonian given copies of its Gibbs state, that can succeed at any temperature. Specifically, we improve over the work of Bakshi, Liu, Moitra, and Tang [BLMT24], by reducing the sample complexity and runtime dependence to singly exponential in the inverse-temperature parameter, as opposed to doubly exponential. Our main technical contribution is a new flat polynomial approximation to the exponential function, with significantly lower degree than the flat polynomial approximation used in [BLMT24].</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04540v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shyam Narayanan</dc:creator>
    </item>
    <item>
      <title>Optimal Mixing for Randomly Sampling Edge Colorings on Trees Down to the Max Degree</title>
      <link>https://arxiv.org/abs/2407.04576</link>
      <description>arXiv:2407.04576v1 Announce Type: cross 
Abstract: We address the convergence rate of Markov chains for randomly generating an edge coloring of a given tree. Our focus is on the Glauber dynamics which updates the color at a randomly chosen edge in each step. For a tree $T$ with $n$ vertices and maximum degree $\Delta$, when the number of colors $q$ satisfies $q\geq\Delta+2$ then we prove that the Glauber dynamics has an optimal relaxation time of $O(n)$, where the relaxation time is the inverse of the spectral gap. This is optimal in the range of $q$ in terms of $\Delta$ as Dyer, Goldberg, and Jerrum (2006) showed that the relaxation time is $\Omega(n^3)$ when $q=\Delta+1$. For the case $q=\Delta+1$, we show that an alternative Markov chain which updates a pair of neighboring edges has relaxation time $O(n)$. Moreover, for the $\Delta$-regular complete tree we prove $O(n\log^2{n})$ mixing time bounds for the respective Markov chain. Our proofs establish approximate tensorization of variance via a novel inductive approach, where the base case is a tree of height $\ell=O(\Delta^2\log^2{\Delta})$, which we analyze using a canonical paths argument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04576v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlie Carlson, Xiaoyu Chen, Weiming Feng, Eric Vigoda</dc:creator>
    </item>
    <item>
      <title>The Degree of Fairness in Efficient House Allocation</title>
      <link>https://arxiv.org/abs/2407.04664</link>
      <description>arXiv:2407.04664v1 Announce Type: cross 
Abstract: The classic house allocation problem is primarily concerned with finding a matching between a set of agents and a set of houses that guarantees some notion of economic efficiency (e.g. utilitarian welfare). While recent works have shifted focus on achieving fairness (e.g. minimizing the number of envious agents), they often come with notable costs on efficiency notions such as utilitarian or egalitarian welfare. We investigate the trade-offs between these welfare measures and several natural fairness measures that rely on the number of envious agents, the total (aggregate) envy of all agents, and maximum total envy of an agent. In particular, by focusing on envy-free allocations, we first show that, should one exist, finding an envy-free allocation with maximum utilitarian or egalitarian welfare is computationally tractable. We highlight a rather stark contrast between utilitarian and egalitarian welfare by showing that finding utilitarian welfare maximizing allocations that minimize the aforementioned fairness measures can be done in polynomial time while their egalitarian counterparts remain intractable (for the most part) even under binary valuations. We complement our theoretical findings by giving insights into the relationship between the different fairness measures and conducting empirical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04664v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Hosseini, Medha Kumar, Sanjukta Roy</dc:creator>
    </item>
    <item>
      <title>Finding irrelevant vertices in linear time on bounded-genus graphs</title>
      <link>https://arxiv.org/abs/1907.05940</link>
      <description>arXiv:1907.05940v5 Announce Type: replace 
Abstract: The irrelevant vertex technique provides a powerful tool for the design of parameterized algorithms for a wide variety of problems on graphs. A common characteristic of these problems, permitting the application of this technique on surface-embedded graphs, is the fact that every graph of large enough treewidth contains a vertex that is irrelevant, in the sense that its removal yields an equivalent instance of the problem. The straightforward application of this technique yields algorithms with running time that is quadratic in the size of the input graph. This running time is due to the fact that it takes linear time to detect one irrelevant vertex and the total number of irrelevant vertices to be detected is linear as well. Using advanced techniques, sub-quadratic algorithms have been designed for particular problems, even in general graphs. However, designing a general framework for linear-time algorithms has been open, even for the bounded-genus case. In this paper we introduce a general framework that enables finding in linear time an entire set of irrelevant vertices whose removal yields a bounded-treewidth graph, provided that the input graph has bounded genus. Our technique consists in decomposing any surface-embeddable graph into a tree-structured collection of bounded-treewidth subgraphs where detecting globally irrelevant vertices can be done locally and independently. Our method is applicable to a wide variety of known graph containment or graph modification problems where the irrelevant vertex technique applies. Examples include the (Induced) Minor Folio problem, the (Induced) Disjoint Paths problem, and the $\mathcal{F}$-Minor-Deletion problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.05940v5</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr A. Golovach, Stavros G. Kolliopoulos, Giannos Stamoulis, Dimitrios M. Thilikos</dc:creator>
    </item>
    <item>
      <title>Robust Sparse Mean Estimation via Sum of Squares</title>
      <link>https://arxiv.org/abs/2206.03441</link>
      <description>arXiv:2206.03441v2 Announce Type: replace 
Abstract: We study the problem of high-dimensional sparse mean estimation in the presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\mathbb R^d$ with "certifiably bounded" $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m = (k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$ with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our algorithms follow the Sum-of-Squares based, proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively the best possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03441v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering</title>
      <link>https://arxiv.org/abs/2206.05245</link>
      <description>arXiv:2206.05245v2 Announce Type: replace 
Abstract: We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\alpha \in (0, 1/2)$, we are given $m$ points in $\mathbb{R}^n$, $\lfloor \alpha m \rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\widehat \mu$ such that $\| \widehat \mu - \mu \|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with "certifiably bounded" $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\alpha)^{O(1/t)}$ with sample complexity $m = (k\log(n))^{O(t)}/\alpha$ and running time $\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee of $\Theta (\sqrt{\log(1/\alpha)})$ with quasi-polynomial sample and computational complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05245v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>Sparse Suffix and LCP Array: Simple, Direct, Small, and Fast</title>
      <link>https://arxiv.org/abs/2310.09023</link>
      <description>arXiv:2310.09023v2 Announce Type: replace 
Abstract: Sparse suffix sorting is the problem of sorting $b=o(n)$ suffixes of a string of length $n$. Efficient sparse suffix sorting algorithms have existed for more than a decade. Despite the multitude of works and their justified claims for applications in text indexing, the existing algorithms have not been employed by practitioners. Arguably this is because there are no simple, direct, and efficient algorithms for sparse suffix array construction. We provide two new algorithms for constructing the sparse suffix and LCP arrays that are simultaneously simple, direct, small, and fast. In particular, our algorithms are: simple in the sense that they can be implemented using only basic data structures; direct in the sense that the output arrays are not a byproduct of constructing the sparse suffix tree or an LCE data structure; fast in the sense that they run in $\mathcal{O}(n\log b)$ time, in the worst case, or in $\mathcal{O}(n)$ time, when the total number of suffixes with an LCP value greater than $2^{\lfloor \log \frac{n}{b} \rfloor + 1}-1$ is in $\mathcal{O}(b/\log b)$, matching the time of the optimal yet much more complicated algorithms [Gawrychowski and Kociumaka, SODA 2017; Birenzwige et al., SODA 2020]; and small in the sense that they can be implemented using only $8b+o(b)$ machine words. Our algorithms are non-trivial space-efficient adaptations of the Monte Carlo algorithm by I et al. for constructing the sparse suffix tree in $\mathcal{O}(n\log b)$ time [STACS 2014]. We provide extensive experiments to justify our claims on simplicity and on efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09023v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorraine A. K. Ayad, Grigorios Loukides, Solon P. Pissis, Hilde Verbeek</dc:creator>
    </item>
    <item>
      <title>Edge-coloring sparse graphs with $\Delta$ colors in quasilinear time</title>
      <link>https://arxiv.org/abs/2401.13839</link>
      <description>arXiv:2401.13839v4 Announce Type: replace 
Abstract: In this paper we show that every graph $G$ of bounded maximum average degree ${\rm mad}(G)$ and with maximum degree $\Delta$ can be edge-colored using the optimal number of $\Delta$ colors in quasilinear time, whenever $\Delta\ge 2{\rm mad}(G)$. The maximum average degree is within a multiplicative constant of other popular graph sparsity parameters like arboricity, degeneracy or maximum density. Our algorithm extends previous results of Chrobak and Nishizeki [J. Algorithms, 1990] and Bhattacharya, Costa, Panski and Solomon [ESA 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13839v4</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukasz Kowalik</dc:creator>
    </item>
    <item>
      <title>Tight Approximation Bounds on a Simple Algorithm for Minimum Average Search Time in Trees</title>
      <link>https://arxiv.org/abs/2402.05560</link>
      <description>arXiv:2402.05560v2 Announce Type: replace 
Abstract: The graph invariant EPT-sum has cropped up in several unrelated fields in later years: As an objective function for hierarchical clustering, as a more fine-grained version of the classical edge ranking problem, and, specifically when the input is a vertex-weighted tree, as a measure of average/expected search length in a partially ordered set. The EPT-sum of a graph $G$ is defined as the minimum sum of the depth of every leaf in an edge partition tree (EPT), a rooted tree where leaves correspond to vertices in $G$ and internal nodes correspond to edges in $G$.
  A simple algorithm that approximates EPT-sum on trees is given by recursively choosing the most balanced edge in the input tree $G$ to build an EPT of $G$. Due to its fast runtime, this balanced cut algorithm can be used in practice, and has earlier been analysed to give a 1.62-approximation on trees. In this paper, we show that the balanced cut algorithm gives a 1.5-approximation of EPT-sum on trees, which amounts to a tight analysis and answers a question posed by Cicalese et al. in 2014.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05560v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Svein H{\o}gemo</dc:creator>
    </item>
    <item>
      <title>Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover</title>
      <link>https://arxiv.org/abs/2402.08346</link>
      <description>arXiv:2402.08346v2 Announce Type: replace 
Abstract: We investigate fine-grained algorithmic aspects of identification problems in graphs and set systems, with a focus on Locating-Dominating Set and Test Cover. We prove the (tight) conditional lower bounds for these problems when parameterized by treewidth and solution as. Formally, \textsc{Locating-Dominating Set} (respectively, \textsc{Test Cover}) parameterized by the treewidth of the input graph (respectively, of the natural auxiliary graph) does not admit an algorithm running in time $2^{2^{o(tw)}} \cdot poly(n)$ (respectively, $2^{2^{o(tw)}} \cdot poly(|U| + |\mathcal{F}|))$. This result augments the small list of NP-Complete problems that admit double exponential lower bounds when parameterized by treewidth. Then, we first prove that \textsc{Locating-Dominating Set} does not admit an algorithm running in time $2^{o(k^2)} \cdot poly(n)$, nor a polynomial time kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(k)}$ vertices, unless the \ETH\ fails. To the best of our knowledge, \textsc{Locating-Dominating Set} is the first problem that admits such an algorithmic lower-bound (with a quadratic function in the exponent) when parameterized by the solution size. Finally, we prove that \textsc{Test Cover} does not admit an algorithm running in time $2^{2^{o(k)}} \cdot poly(|U| + |\mathcal{F}|)$. This is also a rare example of the problem that admits a double exponential lower bound when parameterized by the solution size.
  We also present algorithms whose running times match the above lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08346v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipayan Chakraborty, Florent Foucaud, Diptapriyo Majumdar, Prafullkumar Tale</dc:creator>
    </item>
    <item>
      <title>LinearAlifold: Linear-Time Consensus Structure Prediction for RNA Alignments</title>
      <link>https://arxiv.org/abs/2206.14794</link>
      <description>arXiv:2206.14794v2 Announce Type: replace-cross 
Abstract: Predicting the consensus structure of a set of aligned RNA homologs is a convenient method to find conserved structures in an RNA genome, which has many applications including viral diagnostics and therapeutics. However, the most commonly used tool for this task, RNAalifold, is prohibitively slow for long sequences, due to a cubic scaling with the sequence length, taking over a day on 400 SARS-CoV-2 and SARS-related genomes (~30,000nt). We present LinearAlifold, a much faster alternative that scales linearly with both the sequence length and the number of sequences, based on our work LinearFold that folds a single RNA in linear time. Our work is orders of magnitude faster than RNAalifold (0.7 hours on the above 400 genomes, or ~36$\times$ speedup) and achieves higher accuracies when compared to a database of known structures. More interestingly, LinearAlifold's prediction on SARS-CoV-2 correlates well with experimentally determined structures, substantially outperforming RNAalifold. Finally, LinearAlifold supports two energy models (Vienna and BL*) and four modes: minimum free energy (MFE), maximum expected accuracy (MEA), ThreshKnot, and stochastic sampling, each of which takes under an hour for hundreds of SARS-CoV variants. Our resource is at: https://github.com/LinearFold/LinearAlifold (code) and http://linearfold.org/linear-alifold (server).</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14794v2</guid>
      <category>q-bio.BM</category>
      <category>cs.DS</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apoorv Malik, Liang Zhang, Milan Gautam, Ning Dai, Sizhen Li, He Zhang, David H. Mathews, Liang Huang</dc:creator>
    </item>
    <item>
      <title>Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes</title>
      <link>https://arxiv.org/abs/2307.07604</link>
      <description>arXiv:2307.07604v4 Announce Type: replace-cross 
Abstract: Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.
  We present a new framework and tools to generate smooth lower bounds on the sample complexity of differentially private algorithms satisfying very weak accuracy. We illustrate the applicability of our method by providing new lower bounds in various settings:
  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).
  2. A lower bound on the additive error of DP algorithms for approximate k-means clustering and general (k,z)-clustering, as a function of the multiplicative error, which is tight for a constant multiplication error.
  3. A lower bound for estimating the top singular vector of a matrix under DP in low-accuracy regimes, which is a special case of DP subspace estimation studied by Singhal and Steinke (NeurIPS 2021).
  Our main technique is to apply a padding-and-permuting transformation to a fingerprinting code. However, rather than proving our results using a black-box access to an existing fingerprinting code (e.g., Tardos' code), we develop a new fingerprinting lemma that is stronger than those of Dwork et al. (FOCS 2015) and Bun et al. (SODA 2017), and prove our lower bounds directly from the lemma. Our lemma, in particular, gives a simpler fingerprinting code construction with optimal rate (up to polylogarithmic factors) that is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07604v4</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naty Peter, Eliad Tsfadia, Jonathan Ullman</dc:creator>
    </item>
    <item>
      <title>Algorithm-agnostic low-rank approximation of operator monotone matrix functions</title>
      <link>https://arxiv.org/abs/2311.14023</link>
      <description>arXiv:2311.14023v2 Announce Type: replace-cross 
Abstract: Low-rank approximation of a matrix function, $f(A)$, is an important task in computational mathematics. Most methods require direct access to $f(A)$, which is often considerably more expensive than accessing $A$. Persson and Kressner (SIMAX 2023) avoid this issue for symmetric positive semidefinite matrices by proposing funNystr\"om, which first constructs a Nystr\"om approximation to $A$ using subspace iteration, and then uses the approximation to directly obtain a low-rank approximation for $f(A)$. They prove that the method yields a near-optimal approximation whenever $f$ is a continuous operator monotone function with $f(0) = 0$.
  We significantly generalize the results of Persson and Kressner beyond subspace iteration. We show that if $\widehat{A}$ is a near-optimal low-rank Nystr\"om approximation to $A$ then $f(\widehat{A})$ is a near-optimal low-rank approximation to $f(A)$, independently of how $\widehat{A}$ is computed. Further, we show sufficient conditions for a basis $Q$ to produce a near-optimal Nystr\"om approximation $\widehat{A} = AQ(Q^T AQ)^{\dagger} Q^T A$. We use these results to establish that many common low-rank approximation methods produce near-optimal Nystr\"om approximations to $A$ and therefore to $f(A)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14023v2</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Persson, Raphael A. Meyer, Christopher Musco</dc:creator>
    </item>
    <item>
      <title>Testing the Fairness-Accuracy Improvability of Algorithms</title>
      <link>https://arxiv.org/abs/2405.04816</link>
      <description>arXiv:2405.04816v3 Announce Type: replace-cross 
Abstract: Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is often unclear whether it is possible to reduce this impact without sacrificing other objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this "necessity" defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this application, we reject the null hypothesis that it is not possible to reduce the algorithm's disparate impact without compromising the accuracy of its predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04816v3</guid>
      <category>econ.EM</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</dc:creator>
    </item>
  </channel>
</rss>
