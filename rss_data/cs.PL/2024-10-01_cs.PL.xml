<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Programming with High-Level Abstractions, Proceedings of the 3rd Workshop on Logic and Practice of Programming</title>
      <link>https://arxiv.org/abs/2409.19211</link>
      <description>arXiv:2409.19211v1 Announce Type: new 
Abstract: This proceedings contains abstracts and position papers for the work presented at the third Logic and Practice of Programming (LPOP) Workshop. The workshop was held online, using zoom, at stonybrook.zoom.us, on December 13, 2022. The workshop focused on core high-level abstractions around sets and logic rules, to help bring them to the general practice of programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19211v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David S. Warren, Yanhong A. Liu</dc:creator>
    </item>
    <item>
      <title>Reasoning About Exceptional Behavior At the Level of Java Bytecode</title>
      <link>https://arxiv.org/abs/2409.20056</link>
      <description>arXiv:2409.20056v1 Announce Type: new 
Abstract: A program's exceptional behavior can substantially complicate its control flow, and hence accurately reasoning about the program's correctness. On the other hand, formally verifying realistic programs is likely to involve exceptions -- a ubiquitous feature in modern programming languages.
  In this paper, we present a novel approach to verify the exceptional behavior of Java programs, which extends our previous work on ByteBack. ByteBack works on a program's bytecode, while providing means to specify the intended behavior at the source-code level; this approach sets ByteBack apart from most state-of-the-art verifiers that target source code. To explicitly model a program's exceptional behavior in a way that is amenable to formal reasoning, we introduce Vimp: a high-level bytecode representation that extends the Soot framework's Grimp with verification-oriented features, thus serving as an intermediate layer between bytecode and the Boogie intermediate verification language. Working on bytecode through this intermediate layer brings flexibility and adaptability to new language versions and variants: as our experiments demonstrate, ByteBack can verify programs involving exceptional behavior in all versions of Java, as well as in Scala and Kotlin (two other popular JVM languages).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20056v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Paganoni, Carlo A. Furia</dc:creator>
    </item>
    <item>
      <title>Verifying Functional Correctness Properties At the Level of Java Bytecode</title>
      <link>https://arxiv.org/abs/2409.20071</link>
      <description>arXiv:2409.20071v1 Announce Type: new 
Abstract: The breakneck evolution of modern programming languages aggravates the development of deductive verification tools, which struggle to timely and fully support all new language features. To address this challenge, we present ByteBack: a verification technique that works on Java bytecode. Compared to high-level languages, intermediate representations such as bytecode offer a much more limited and stable set of features; hence, they may help decouple the verification process from changes in the source-level language.
  ByteBack offers a library to specify functional correctness properties at the level of the source code, so that the bytecode is only used as an intermediate representation that the end user does not need to work with. Then, ByteBack reconstructs some of the information about types and expressions that is erased during compilation into bytecode but is necessary to correctly perform verification. Our experiments with an implementation of ByteBack demonstrate that it can successfully verify bytecode compiled from different versions of Java, and including several modern language features that even state-of-the-art Java verifiers (such as KeY and OpenJML) do not directly support$\unicode{x2013}$thus revealing how ByteBack's approach can help keep up verification technology with language evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20071v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Paganoni, Carlo A. Furia</dc:creator>
    </item>
    <item>
      <title>An Effectively $\Omega(c)$ Language and Runtime</title>
      <link>https://arxiv.org/abs/2409.20494</link>
      <description>arXiv:2409.20494v1 Announce Type: new 
Abstract: The performance of an application/runtime is usually thought of as a continuous function where, the lower the amount of memory/time used on a given workload, then the better the compiler/runtime is. However, in practice, good performance of an application is conceptually more of a binary function -- either the application responds in under, say 100ms, and is fast enough for a user to barely notice, or it takes a noticeable amount of time, leaving the user waiting and potentially abandoning the task. Thus, performance really means how often the application is fast enough to be usable, leading industrial developers to focus on the 95th and 99th percentile latencies as heavily, or moreso, than average response time.
  Unfortunately, tracking and optimizing for these high percentile latencies is difficult and often requires a deep understanding of the application, runtime, GC, and OS interactions. This is further complicated by the fact that tail performance is often only seen occasionally, and is specific to a certain workload or input, making these issues uniquely painful to handle. Our vision is to create a language and runtime that is designed to be $\Omega(c)$ in its performance -- that is, it is designed to have an effectively constant time to execute all operations, there is a constant fixed memory overhead for the application footprint, and the garbage-collector performs a constant amount of work per allocation + a (small) bounded pause for all collection/release operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20494v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Marron</dc:creator>
    </item>
    <item>
      <title>Polynomial Universes and Dependent Types</title>
      <link>https://arxiv.org/abs/2409.19176</link>
      <description>arXiv:2409.19176v1 Announce Type: cross 
Abstract: Awodey, later with Newstead, showed how polynomial pseudomonads $(u,1,\Sigma)$ with extra structure (termed "natural models" by Awodey) hold within them the categorical semantics for dependent type theory. Their work presented these ideas clearly but ultimately led them outside of the category of polynomial functors in order to explain all of the structure possessed by such models of type theory.
  This paper builds off that work -- explicating the categorical semantics of dependent type theory by axiomatizing them \emph{entirely} in the language of polynomial functors. In order to handle the higher-categorical coherences required for such an explanation, we work with polynomial functors internally in the language of Homotopy Type Theory, which allows for higher-dimensional structures such as pseudomonads, etc. to be expressed purely in terms of the structure of a suitably-chosen $\infty$-category of polynomial functors. The move from set theory to Homotopy Type Theory thus has a twofold effect of enabling a simpler exposition of natural models, which is at the same time amenable to formalization in a proof assistant, such as Agda.
  Moreover, the choice to remain firmly within the setting of polynomial functors reveals many additional structures of natural models that were otherwise left implicit or not considered by Awodey \&amp; Newstead. Chief among these, we highlight the fact that every polynomial pseudomonad $(u,1,\Sigma)$ as above that is also equipped with structure to interpret dependent product types gives rise to a self-distributive law $u \triangleleft u\to u \triangleleft u$, which witnesses the usual distributive law of dependent products over dependent sums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19176v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>math.CT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. B. Aberl\'e, David I. Spivak</dc:creator>
    </item>
    <item>
      <title>Proceedings 13th International Workshop on Developments in Computational Models</title>
      <link>https://arxiv.org/abs/2409.19298</link>
      <description>arXiv:2409.19298v1 Announce Type: cross 
Abstract: This volume contains the proceedings of DCM 2023, the 13th International Workshop on Developments in Computational Models held on 2 July 2023 in Rome, Italy. DCM 2023 was organised as a one-day satellite event of FSCD 2023, the 8th International Conference on Formal Structures for Computation and Deduction. The aim of this workshop is to bring together researchers who are currently developing new computation models or new features for traditional computation models, in order to foster their interaction, to provide a forum for presenting new ideas and work in progress, and to enable newcomers to learn about current activities in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19298v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.408</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 408, 2024</arxiv:journal_reference>
      <dc:creator>Sandra Alves (University of Porto), Ian Mackie (London South Bank University)</dc:creator>
    </item>
    <item>
      <title>The Vanilla Sequent Calculus is Call-by-Value</title>
      <link>https://arxiv.org/abs/2409.19722</link>
      <description>arXiv:2409.19722v1 Announce Type: cross 
Abstract: Existing Curry-Howard interpretations of call-by-value evaluation for the $\lambda$-calculus involve classical logic or linear logic, despite the fact that call-by-value was introduced in an intuitionistic setting without linear features.
  This paper shows that the most basic sequent calculus for minimal intuitionistic logic -- dubbed here vanilla -- can naturally be seen as a logical interpretation of call-by-value evaluation. This is obtained by establishing mutual simulations with a well-known formalism for call-by-value evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19722v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Beniamino Accattoli</dc:creator>
    </item>
    <item>
      <title>Pragma driven shared memory parallelism in Zig by supporting OpenMP loop directives</title>
      <link>https://arxiv.org/abs/2409.20148</link>
      <description>arXiv:2409.20148v1 Announce Type: cross 
Abstract: The Zig programming language, which is designed to provide performance and safety as first class concerns, has become popular in recent years. Given that Zig is built upon LLVM, and-so enjoys many of the benefits provided by the ecosystem, including access to a rich set of backends, Zig has significant potential for high performance workloads. However, it is yet to gain acceptance in HPC and one of the reasons for this is that support for the pragma driven shared memory parallelism is missing.
  In this paper we describe enhancing the Zig compiler to add support for OpenMP loop directives. Then exploring performance using NASA's NAS Parallel Benchmark (NPB) suite. We demonstrate that not only does our integration of OpenMP with Zig scale comparatively to Fortran and C reference implementations of NPB, but furthermore Zig provides up to a 1.25 times performance increase compared to Fortran.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20148v1</guid>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Kacs, Joseph Lee, Justs Zarins, Nick Brown</dc:creator>
    </item>
    <item>
      <title>Towards Concurrent Quantitative Separation Logic</title>
      <link>https://arxiv.org/abs/2207.02822</link>
      <description>arXiv:2207.02822v3 Announce Type: replace-cross 
Abstract: In this paper, we develop a novel verification technique to reason about programs featuring concurrency, pointers and randomization. While the integration of concurrency and pointers is well studied, little is known about the combination of all three paradigms. To close this gap, we combine two kinds of separation logic -- Quantitative Separation Logic and Concurrent Separation Logic -- into a new separation logic that enables reasoning about lower bounds of the probability to realise a postcondition by executing such a program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02822v3</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ira Fesefeldt, Joost-Pieter Katoen, Thomas Noll</dc:creator>
    </item>
    <item>
      <title>Multi-Relational Algebra and Its Applications to Data Insights</title>
      <link>https://arxiv.org/abs/2311.04824</link>
      <description>arXiv:2311.04824v5 Announce Type: replace-cross 
Abstract: A range of data insight analytical tasks involves analyzing a large set of tables of different schemas, possibly induced by various groupings, to find salient patterns. This paper presents Multi-Relational Algebra, an extension of the classic Relational Algebra, to facilitate such transformations and their compositions. Multi-Relational Algebra has two main characteristics: (1) Information Unit. The information unit is a slice $(r, X)$, where $r$ is a (region) tuple, and $X$ is a (feature) table. Specifically, a slice can encompass multiple columns, which surpasses the information unit of "a single tuple" or "a group of tuples of one column" in the classic relational algebra, (2) Schema Flexibility. Slices can have varying schemas, not constrained to a single schema. This flexibility further expands the expressive power of the algebra. Through various examples, we show that multi-relational algebra can effortlessly express many complex analytic problems, some of which are beyond the scope of traditional relational analytics. We have implemented and deployed a service for multi-relational analytics. Due to a unified logical design, we are able to conduct systematic optimization for a variety of seemingly different tasks. Our service has garnered interest from numerous internal teams who have developed data-insight applications using it, and serves millions of operators daily.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04824v5</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Wu, Zichen Zhu, Xiangyao Yu, Shaleen Deep, Stratis Viglas, John Cieslewicz, Somesh Jha, Jeffrey F. Naughton</dc:creator>
    </item>
    <item>
      <title>Boosting Few-Pixel Robustness Verification via Covering Verification Designs</title>
      <link>https://arxiv.org/abs/2405.10924</link>
      <description>arXiv:2405.10924v3 Announce Type: replace-cross 
Abstract: Proving local robustness is crucial to increase the reliability of neural networks. While many verifiers prove robustness in $L_\infty$ $\epsilon$-balls, very little work deals with robustness verification in $L_0$ $\epsilon$-balls, capturing robustness to few pixel attacks. This verification introduces a combinatorial challenge, because the space of pixels to perturb is discrete and of exponential size. A previous work relies on covering designs to identify sets for defining $L_\infty$ neighborhoods, which if proven robust imply that the $L_0$ $\epsilon$-ball is robust. However, the number of neighborhoods to verify remains very high, leading to a high analysis time. We propose covering verification designs, a combinatorial design that tailors effective but analysis-incompatible coverings to $L_0$ robustness verification. The challenge is that computing a covering verification design introduces a high time and memory overhead, which is intensified in our setting, where multiple candidate coverings are required to identify how to reduce the overall analysis time. We introduce CoVerD, an $L_0$ robustness verifier that selects between different candidate coverings without constructing them, but by predicting their block size distribution. This prediction relies on a theorem providing closed-form expressions for the mean and variance of this distribution. CoVerD constructs the chosen covering verification design on-the-fly, while keeping the memory consumption minimal and enabling to parallelize the analysis. The experimental results show that CoVerD reduces the verification time on average by up to 5.1x compared to prior work and that it scales to larger $L_0$ $\epsilon$-balls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10924v3</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuval Shapira, Naor Wiesel, Shahar Shabelman, Dana Drachsler-Cohen</dc:creator>
    </item>
  </channel>
</rss>
