<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Sep 2025 04:01:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Digital Engineering Transformation as a Sociotechnical Challenge: Categorization of Barriers and Their Mapping to DoD's Policy Goals</title>
      <link>https://arxiv.org/abs/2509.15461</link>
      <description>arXiv:2509.15461v1 Announce Type: new 
Abstract: Digital Engineering (DE) transformation represents a paradigm shift in systems engineering (SE), aiming to integrate diverse analytical models and digital artifacts into an authoritative source of truth for improved traceability and more efficient system lifecycle management. Despite institutional support, many DE initiatives underperform or fail to realize their intended benefits. We argue that this often results from a limited understanding of the social and technical barriers, and particularly how their interplay shapes transformation outcomes. To address this gap, we document barriers identified in the literature and grounded in sociotechnical systems theory, organized into six dimensions: people, processes, culture, goals, infrastructure, and technology. We then map these barriers to the U.S. Department of Defense's DE policy goals. Our analysis shows that technological investments alone are insufficient, as failures frequently arise from social factors such as workforce readiness, leadership support, and cultural alignment. The mapping also demonstrates that sociotechnical barriers often cascade across dimensions, making their impact on policy goals difficult to trace and complicating implementation. These insights carry practical implications: managers may use the mapping as a diagnostic tool to identify risks and prioritize resources; policymakers may complement strategic mandates with sustained investments and long-term change management; and engineers may view DE not as a threat to job security but as an opportunity for more effective collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15461v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Doulotuzzaman Xames, Taylan G. Topcu</dc:creator>
    </item>
    <item>
      <title>Unpacking Personal(?!) Health Informatics: An Investigation of Awareness, Understanding, And Leveraged Utility in India</title>
      <link>https://arxiv.org/abs/2509.01231</link>
      <description>arXiv:2509.01231v1 Announce Type: cross 
Abstract: Personal Health Informatics (PHI), which leverages digital tools and information systems to support health assessment and self-care, holds promise for empowering individuals and transforming healthcare delivery. However, barriers to its adoption remain underexplored in the Indian context. This study investigates PHI adoption among Indian users and stakeholders using a multi-method approach. An awareness survey (n = 87) examined the usage of wearables and general PHI engagement, followed by semi-structured interviews (n = 22) that explored motivations, usage patterns, and health information sources. Qualitative analysis revealed that while PHI is valued for health monitoring and shared/collective care, its adoption is hindered by factors such as low health literacy, usability challenges, and mistrust in digital health platforms. Further stakeholder interviews and co-design workshops informed the development of a Figma-based prototype, which was evaluated for usability. Based on these findings, we offer design recommendations for an integrated, user-controlled PHI platform featuring accessible analytics and verifiable health information. Our insights highlight the socio-technical challenges of PHI adoption in India and underscore the need for reliable, user-centric solutions to support proactive healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01231v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shyama Sastha Krishnamoorthy Srinivasan, Mohan Kumar, Pushpendra Singh</dc:creator>
    </item>
    <item>
      <title>Classical and Quantum Heuristics for the Binary Paint Shop Problem</title>
      <link>https://arxiv.org/abs/2509.15294</link>
      <description>arXiv:2509.15294v1 Announce Type: cross 
Abstract: The Binary Paint Shop Problem (BPSP) is an $\mathsf{APX}$-hard optimisation problem in automotive manufacturing: given a sequence of $2n$ cars, comprising $n$ distinct models each appearing twice, the task is to decide which of two colours to paint each car so that the two occurrences of each model are painted differently, while minimising consecutive colour swaps. The key performance metric is the paint swap ratio, the average number of colour changes per car, which directly impacts production efficiency and cost. Prior work showed that the Quantum Approximate Optimisation Algorithm (QAOA) at depth $p=7$ achieves a paint swap ratio of $0.393$, outperforming the classical Recursive Greedy (RG) heuristic with an expected ratio of $0.4$ [Phys. Rev. A 104, 012403 (2021)]. More recently, the classical Recursive Star Greedy (RSG) heuristic was conjectured to achieve an expected ratio of $0.361$. In this study, we develop the theoretical foundations for applying QAOA to BPSP through a reduction of BPSP to weighted MaxCut, and use this framework to benchmark two state-of-the-art low-depth QAOA variants, eXpressive QAOA (XQAOA) and Recursive QAOA (RQAOA), at $p=1$ (denoted XQAOA$_1$ and RQAOA$_1$), against the strongest classical heuristics known to date. Across instances ranging from $2^7$ to $2^{12}$ cars, XQAOA$_1$ achieves an average ratio of $0.357$, surpassing RQAOA$_1$ and all classical heuristics, including the conjectured performance of RSG. Surprisingly, RQAOA$_1$ shows diminishing performance as size increases: despite using provably optimal QAOA$_1$ parameters at each recursion, it is outperformed by RSG on most $2^{11}$-car instances and all $2^{12}$-car instances. To our knowledge, this is the first study to report RQAOA$_1$'s performance degradation at scale. In contrast, XQAOA$_1$ remains robust, indicating strong potential to asymptotically surpass all known heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15294v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V Vijendran, Dax Enshan Koh, Ping Koy Lam, Syed M Assad</dc:creator>
    </item>
    <item>
      <title>Leveraging Familiarity with Television to Enrich Older Adults' Engagement and Wellbeing: A Feasibility Study Using Video Probes</title>
      <link>https://arxiv.org/abs/2509.15618</link>
      <description>arXiv:2509.15618v1 Announce Type: cross 
Abstract: The shift away from multigenerational families to nuclear families in India has created a growing need to support older adults living independently. While technology can help address this gap, older adults' limited exposure to newer technology restricts the adoption of such solutions. However, they remain comfortable with long-standing technologies like television (TV). This study explores their daily technology usage and challenges, aiming to determine whether TV can be leveraged to improve their quality of life. We examined how TV systems could be enhanced to assist older adults with tasks such as staying connected, receiving health alerts, and ensuring security. Using a participatory design approach, we developed video probes using the prototype of the TV-based application and interviewed 27 older adults to assess its acceptance and usability. Our findings demonstrate older adults' strong interest in a TV-based solution and a preference for familiar technology to support security, independence, and wellbeing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15618v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757624</arxiv:DOI>
      <dc:creator>Shyama Sastha Krishnamoorthy Srinivasan, Mohan Kumar, Pushpendra Singh</dc:creator>
    </item>
    <item>
      <title>Triplet Loss Based Quantum Encoding for Class Separability</title>
      <link>https://arxiv.org/abs/2509.15705</link>
      <description>arXiv:2509.15705v1 Announce Type: cross 
Abstract: An efficient and data-driven encoding scheme is proposed to enhance the performance of variational quantum classifiers. This encoding is specially designed for complex datasets like images and seeks to help the classification task by producing input states that form well-separated clusters in the Hilbert space according to their classification labels. The encoding circuit is trained using a triplet loss function inspired by classical facial recognition algorithms, and class separability is measured via average trace distances between the encoded density matrices. Benchmark tests performed on various binary classification tasks on MNIST and MedMNIST datasets demonstrate considerable improvement over amplitude encoding with the same VQC structure while requiring a much lower circuit depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15705v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mordacci, Mahul Pandey, Paolo Santini, Michele Amoretti</dc:creator>
    </item>
    <item>
      <title>Impact of Single Rotations and Entanglement Topologies in Quantum Neural Networks</title>
      <link>https://arxiv.org/abs/2509.15722</link>
      <description>arXiv:2509.15722v1 Announce Type: cross 
Abstract: In this work, an analysis of the performance of different Variational Quantum Circuits is presented, investigating how it changes with respect to entanglement topology, adopted gates, and Quantum Machine Learning tasks to be performed. The objective of the analysis is to identify the optimal way to construct circuits for Quantum Neural Networks. In the presented experiments, two types of circuits are used: one with alternating layers of rotations and entanglement, and the other, similar to the first one, but with an additional final layer of rotations. As rotation layers, all combinations of one and two rotation sequences are considered. Four different entanglement topologies are compared: linear, circular, pairwise, and full. Different tasks are considered, namely the generation of probability distributions and images, and image classification. Achieved results are correlated with the expressibility and entanglement capability of the different circuits to understand how these features affect performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15722v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mordacci, Michele Amoretti</dc:creator>
    </item>
    <item>
      <title>Training Variational Quantum Circuits Using Particle Swarm Optimization</title>
      <link>https://arxiv.org/abs/2509.15726</link>
      <description>arXiv:2509.15726v1 Announce Type: cross 
Abstract: In this work, the Particle Swarm Optimization (PSO) algorithm has been used to train various Variational Quantum Circuits (VQCs). This approach is motivated by the fact that commonly used gradient-based optimization methods can suffer from the barren plateaus problem. PSO is a stochastic optimization technique inspired by the collective behavior of a swarm of birds. The dimension of the swarm, the number of iterations of the algorithm, and the number of trainable parameters can be set. In this study, PSO has been used to train the entire structure of VQCs, allowing it to select which quantum gates to apply, the target qubits, and the rotation angle, in case a rotation is chosen. The algorithm is restricted to choosing from four types of gates: Rx, Ry, Rz, and CNOT. The proposed optimization approach has been tested on various datasets of the MedMNIST, which is a collection of biomedical image datasets designed for image classification tasks. Performance has been compared with the results achieved by classical stochastic gradient descent applied to a predefined VQC. The results show that the PSO can achieve comparable or even better classification accuracy across multiple datasets, despite the PSO using a lower number of quantum gates than the VQC used with gradient descent optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15726v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mordacci, Michele Amoretti</dc:creator>
    </item>
    <item>
      <title>Affective Air Quality Dataset: Personal Chemical Emissions from Emotional Videos</title>
      <link>https://arxiv.org/abs/2509.15774</link>
      <description>arXiv:2509.15774v1 Announce Type: cross 
Abstract: Inspired by the role of chemosignals in conveying emotional states, this paper introduces the Affective Air Quality (AAQ) dataset, a novel dataset collected to explore the potential of volatile odor compound and gas sensor data for non-contact emotion detection. This dataset bridges the gap between the realms of breath \&amp; body odor emission (personal chemical emissions) analysis and established practices in affective computing. Comprising 4-channel gas sensor data from 23 participants at two distances from the body (wearable and desktop), alongside emotional ratings elicited by targeted movie clips, the dataset encapsulates initial groundwork to analyze the correlation between personal chemical emissions and varied emotional responses. The AAQ dataset also provides insights drawn from exit interviews, thereby painting a holistic picture of perceptions regarding air quality monitoring and its implications for privacy. By offering this dataset alongside preliminary attempts at emotion recognition models based on it to the broader research community, we seek to advance the development of odor-based affect recognition models that prioritize user privacy and comfort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15774v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jas Brooks, Javier Hernandez, Mary Czerwinski, Judith Amores</dc:creator>
    </item>
    <item>
      <title>TinkerXR: In-Situ, Reality-Aware CAD and 3D Printing Interface for Novices</title>
      <link>https://arxiv.org/abs/2410.06113</link>
      <description>arXiv:2410.06113v4 Announce Type: replace-cross 
Abstract: Despite the growing accessibility of augmented reality (AR) for visualization, existing computer-aided design (CAD) systems remain confined to traditional screens or require complex setups or predefined parameters, limiting immersion and accessibility for novices. We present TinkerXR, an open-source AR interface enabling in-situ design and fabrication through Constructive Solid Geometry (CSG) modeling. TinkerXR operates solely with a headset and 3D printer, allowing users to design directly in and for their physical environments. By leveraging spatial awareness, depth occlusion, recognition of physical constraints, reference objects, and hand movement controls, TinkerXR enhances realism, precision, and ease of use. Its AR-based workflow integrates design and 3D printing with a drag-and-drop interface for printers' virtual twins.
  A user study comparing TinkerXR with Tinkercad shows that TinkerXR offers novices higher accessibility, engagement, and ease of use. Participants highlighted how designing directly in physical space made the process more intuitive. By bridging the gap between digital creation and physical output, TinkerXR aims to transform everyday spaces into expressive creative studios. We release TinkerXR as open source to encourage further exploration of accessible, spatially grounded CAD tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06113v4</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.GR</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>ACM SCF 2025</arxiv:journal_reference>
      <dc:creator>O\u{g}uz Arslan, Artun Akdo\u{g}an, Mustafa Doga Dogan</dc:creator>
    </item>
    <item>
      <title>Towards Supporting QIR: Steps for Adopting the Quantum Intermediate Representation</title>
      <link>https://arxiv.org/abs/2411.18682</link>
      <description>arXiv:2411.18682v2 Announce Type: replace-cross 
Abstract: Intermediate representations (IRs) play a crucial role in the software stack of a quantum computer to facilitate efficient optimizations for executing an application on hardware. One of those IRs is the Quantum Intermediate Representation (QIR), which builds on the classical LLVM compiler infrastructure. In this article, we outline different approaches to how QIR can be adopted. This exploration culminates in a demonstration of what it takes to turn an existing quantum circuit simulator into a QIR runtime and that such a transition is less daunting than it might seem at first. We further show that switching to QIR does not entail any performance deficits compared to the original simulator. On the contrary, the presented steps effortlessly allow adding support for arbitrary classical control flow to any classical simulator. We conclude with an outlook on future directions using QIR. The implemented QIR runtime is available under https://github.com/munich-quantum-toolkit/core.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18682v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannick Stade, Lukas Burgholzer, Robert Wille</dc:creator>
    </item>
  </channel>
</rss>
