<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 01:25:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Identification Power of Combining Experimental and Observational Data for Distributional Treatment Effect Parameters</title>
      <link>https://arxiv.org/abs/2508.12206</link>
      <description>arXiv:2508.12206v1 Announce Type: new 
Abstract: This paper investigates the identification power gained by combining experimental data, in which treatment is randomized, with observational data, in which treatment is self-selected, for distributional treatment effect (DTE) parameters. While experimental data identify average treatment effects, many DTE parameters, such as the distribution of individual treatment effects, are only partially identified. We examine whether, and how, combining the two data sources tightens the identified set for these parameters. For broad classes of DTE parameters, we derive sharp bounds under the combined data and clarify the mechanism through which the data combination tightens the identified set relative to using experimental data alone. Our analysis highlights that self-selection in the observational data is a key source of identification power. We also characterize necessary and sufficient conditions under which the combined data shrink the identified set, showing that such shrinkage generally occurs unless selection-on-observables holds in the observational data. We also propose a linear programming approach to compute the sharp bounds, which can accommodate additional structural restrictions such as mutual stochastic monotonicity of potential outcomes and the generalized Roy model. An empirical application using data on negative campaign advertisements in a U.S. presidential election illustrates the practical relevance of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12206v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>A statistician's guide to weak-instrument-robust inference in instrumental variables regression with illustrations in Python</title>
      <link>https://arxiv.org/abs/2508.12474</link>
      <description>arXiv:2508.12474v1 Announce Type: new 
Abstract: We provide an overview of results relating to estimation and weak-instrument-robust inference in instrumental variables regression. Methods are implemented in the ivmodels software package for Python, which we use to illustrate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12474v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Londschien</dc:creator>
    </item>
    <item>
      <title>Reconstructing Subnational Labor Indicators in Colombia: An Integrated Machine and Deep Learning Approach</title>
      <link>https://arxiv.org/abs/2508.12514</link>
      <description>arXiv:2508.12514v2 Announce Type: new 
Abstract: This study proposes a unified multi-stage framework to reconstruct consistent monthly and annual labor indicators for all 33 Colombian departments from 1993 to 2025. The approach integrates temporal disaggregation, time-series splicing and interpolation, statistical learning, and institutional covariates to estimate seven key variables: employment, unemployment, labor force participation (PEA), inactivity, working-age population (PET), total population, and informality rate, including in regions without direct survey coverage. The framework enforces labor accounting identities, scales results to demographic projections, and aligns all estimates with national benchmarks to ensure internal coherence. Validation against official departmental GEIH aggregates and city-level informality data for the 23 metropolitan areas yields in-sample Mean Absolute Percentage Errors (MAPEs) below 2.3% across indicators, confirming strong predictive performance. To our knowledge, this is the first dataset to provide spatially exhaustive and temporally consistent monthly labor measures for Colombia. By incorporating both quantitative and qualitative dimensions of employment, the panel enhances the empirical foundation for analysing long-term labor market dynamics, identifying regional disparities, and designing targeted policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12514v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaime Vera-Jaramillo</dc:creator>
    </item>
    <item>
      <title>Bayesian Double Machine Learning for Causal Inference</title>
      <link>https://arxiv.org/abs/2508.12688</link>
      <description>arXiv:2508.12688v1 Announce Type: new 
Abstract: This paper proposes a simple, novel, and fully-Bayesian approach for causal inference in partially linear models with high-dimensional control variables. Off-the-shelf machine learning methods can introduce biases in the causal parameter known as regularization-induced confounding. To address this, we propose a Bayesian Double Machine Learning (BDML) method, which modifies a standard Bayesian multivariate regression model and recovers the causal effect of interest from the reduced-form covariance matrix. Our BDML is related to the burgeoning frequentist literature on DML while addressing its limitations in finite-sample inference. Moreover, the BDML is based on a fully generative probability model in the DML context, adhering to the likelihood principle. We show that in high dimensional setups the naive estimator implicitly assumes no selection on observables--unlike our BDML. The BDML exhibits lower asymptotic bias and achieves asymptotic normality and semiparametric efficiency as established by a Bernstein-von Mises theorem, thereby ensuring robustness to misspecification. In simulations, our BDML achieves lower RMSE, better frequentist coverage, and shorter confidence interval width than alternatives from the literature, both Bayesian and frequentist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12688v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francis J. DiTraglia, Laura Liu</dc:creator>
    </item>
    <item>
      <title>Bivariate Distribution Regression; Theory, Estimation and an Application to Intergenerational Mobility</title>
      <link>https://arxiv.org/abs/2508.12716</link>
      <description>arXiv:2508.12716v1 Announce Type: new 
Abstract: We employ distribution regression (DR) to estimate the joint distribution of two outcome variables conditional on chosen covariates. While Bivariate Distribution Regression (BDR) is useful in a variety of settings, it is particularly valuable when some dependence between the outcomes persists after accounting for the impact of the covariates. Our analysis relies on a result from Chernozhukov et al. (2018) which shows that any conditional joint distribution has a local Gaussian representation. We describe how BDR can be implemented and present some associated functionals of interest. As modeling the unexplained dependence is a key feature of BDR, we focus on functionals related to this dependence. We decompose the difference between the joint distributions for different groups into composition, marginal and sorting effects. We provide a similar decomposition for the transition matrices which describe how location in the distribution in one of the outcomes is associated with location in the other. Our theoretical contributions are the derivation of the properties of these estimated functionals and appropriate procedures for inference. Our empirical illustration focuses on intergenerational mobility. Using the Panel Survey of Income Dynamics data, we model the joint distribution of parents' and children's earnings. By comparing the observed distribution with constructed counterfactuals, we isolate the impact of observable and unobservable factors on the observed joint distribution. We also evaluate the forces responsible for the difference between the transition matrices of sons' and daughters'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12716v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Estimation in linear models with clustered data</title>
      <link>https://arxiv.org/abs/2508.12860</link>
      <description>arXiv:2508.12860v1 Announce Type: new 
Abstract: We study linear regression models with clustered data, high-dimensional controls, and a complicated structure of exclusion restrictions. We propose a correctly centered internal IV estimator that accommodates a variety of exclusion restrictions and permits within-cluster dependence. The estimator has a simple leave-out interpretation and remains computationally tractable. We derive a central limit theorem for its quadratic form and propose a robust variance estimator. We also develop inference methods that remain valid under weak identification. Our framework extends classical dynamic panel methods to more general clustered settings. An empirical application of a large-scale fiscal intervention in rural Kenya with spatial interference illustrates the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12860v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Mikusheva, Mikkel S{\o}lvsten, Baiyun Jing</dc:creator>
    </item>
    <item>
      <title>The purpose of an estimator is what it does: Misspecification, estimands, and over-identification</title>
      <link>https://arxiv.org/abs/2508.13076</link>
      <description>arXiv:2508.13076v1 Announce Type: new 
Abstract: In over-identified models, misspecification -- the norm rather than exception -- fundamentally changes what estimators estimate. Different estimators imply different estimands rather than different efficiency for the same target. A review of recent applications of generalized method of moments in the American Economic Review suggests widespread acceptance of this fact: There is little formal specification testing and widespread use of estimators that would be inefficient were the model correct, including the use of "hand-selected" moments and weighting matrices. Motivated by these observations, we review and synthesize recent results on estimation under model misspecification, providing guidelines for transparent and robust empirical research. We also provide a new theoretical result, showing that Hansen's J-statistic measures, asymptotically, the range of estimates achievable at a given standard error. Given the widespread use of inefficient estimators and the resulting researcher degrees of freedom, we thus particularly recommend the broader reporting of J-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13076v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrew, Jiafeng Chen, Otavio Tecchio</dc:creator>
    </item>
    <item>
      <title>Reasonable uncertainty: Confidence intervals in empirical Bayes discrimination detection</title>
      <link>https://arxiv.org/abs/2508.13110</link>
      <description>arXiv:2508.13110v1 Announce Type: new 
Abstract: We revisit empirical Bayes discrimination detection, focusing on uncertainty arising from both partial identification and sampling variability. While prior work has mostly focused on partial identification, we find that some empirical findings are not robust to sampling uncertainty. To better connect statistical evidence to the magnitude of real-world discriminatory behavior, we propose a counterfactual odds-ratio estimand with a attractive properties and interpretation. Our analysis reveals the importance of careful attention to uncertainty quantification and downstream goals in empirical Bayes analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13110v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaying Gu, Nikolaos Ignatiadis, Azeem M. Shaikh</dc:creator>
    </item>
    <item>
      <title>A note on simulation methods for the Dirichlet-Laplace prior</title>
      <link>https://arxiv.org/abs/2508.11982</link>
      <description>arXiv:2508.11982v1 Announce Type: cross 
Abstract: Bhattacharya et al. (2015, Journal of the American Statistical Association 110(512): 1479-1490) introduce a novel prior, the Dirichlet-Laplace (DL) prior, and propose a Markov chain Monte Carlo (MCMC) method to simulate posterior draws under this prior in a conditionally Gaussian setting. The original algorithm samples from conditional distributions in the wrong order, i.e., it does not correctly sample from the joint posterior distribution of all latent variables. This note details the issue and provides two simple solutions: A correction to the original algorithm and a new algorithm based on an alternative, yet equivalent, formulation of the prior. This corrigendum does not affect the theoretical results in Bhattacharya et al. (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11982v1</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2540256</arxiv:DOI>
      <arxiv:journal_reference>Correction. (2025). Journal of the American Statistical Association</arxiv:journal_reference>
      <dc:creator>Luis Gruber, Gregor Kastner, Anirban Bhattacharya, Debdeep Pati, Natesh Pillai, David Dunson</dc:creator>
    </item>
    <item>
      <title>Noisy, Non-Smooth, Non-Convex Estimation of Moment Condition Models</title>
      <link>https://arxiv.org/abs/2301.07196</link>
      <description>arXiv:2301.07196v3 Announce Type: replace 
Abstract: A practical challenge for structural estimation is the requirement to accurately minimize a sample objective function which is often non-smooth, non-convex, or both. This paper proposes a simple algorithm designed to find accurate solutions without performing an exhaustive search. It augments each iteration from a new Gauss-Newton algorithm with a grid search step. A finite sample analysis derives its optimization and statistical properties simultaneously using only econometric assumptions. After a finite number of iterations, the algorithm automatically transitions from global to fast local convergence, producing accurate estimates with high probability. Simulated examples and an empirical application illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07196v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron</dc:creator>
    </item>
    <item>
      <title>The Subtlety of Optimal Paternalism in a Population with Bounded Rationality</title>
      <link>https://arxiv.org/abs/2410.13658</link>
      <description>arXiv:2410.13658v2 Announce Type: replace 
Abstract: We study optimal policy when a paternalistic utilitarian planner has the power to design a discrete choice set for a heterogeneous population with bounded rationality. We show that the policy that most effectively constrains or influences choices depends in a particular multiplicative way on the preferences of the population and on the choice probabilities conditional on preferences that measure the suboptimality of behavior. We first consider the planning problem in abstraction. We then study two settings in which the planner may mandate an action or decentralize decision making. In one setting, we suppose that individuals measure utility with additive random error and maximize mismeasured rather than actual utility. Then optimal planning requires knowledge of the distribution of measurement errors. In the second setting, we consider binary treatment choice under uncertainty when the planner can mandate a treatment conditional on publicly observed personal covariates or can enable individuals to choose their own treatments conditional on private information. We focus on situations where bounded rationality takes the form of deviations between subjective personal beliefs and objective probabilities of uncertain outcomes. To illustrate, we consider clinical decision making in medicine. In toto, our analysis is cautionary. It characterizes the subtle nature of optimal policy, whose determination requires the planner to possess extensive knowledge that is rarely available. We conclude that studies of policy choice by a paternalistic utilitarian planner should view not only the population but also the planner to be boundedly rational.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13658v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles F. Manski, Eytan Sheshinski</dc:creator>
    </item>
    <item>
      <title>Analysis of Multiple Long Run Relations in Panel Data Models with Applications to Financial Ratios</title>
      <link>https://arxiv.org/abs/2506.02135</link>
      <description>arXiv:2506.02135v2 Announce Type: replace 
Abstract: This paper provides a new methodology for the analysis of multiple long run relations in panel data models where the cross section dimension, $n$, is large relative to the time series dimension, $T$. For panel data models with large $n$ researchers have focused on panels with a single long run relationship. The main difficulty has been to eliminate short run dynamics without generating significant uncertainty for identification of the long run. We overcome this problem by using non-overlapping sub-sample time averages as deviations from their full-sample counterpart and estimating the number of long run relations and their coefficients using eigenvalues and eigenvectors of the pooled covariance matrix of these sub-sample deviations. We refer to this procedure as pooled minimum eigenvalue (PME) and show that it applies to unbalanced panels generated from general linear processes with interactive stationary time effects and does not require knowing long run causal linkages. To our knowledge, no other estimation procedure exists for this setting. We show the PME estimator is consistent and asymptotically normal as $n$ and $T \rightarrow \infty$ jointly, such that $T\approx n^{d}$, with $d&gt;0$ for consistency and $d&gt;1/2$ for asymptotic normality. Extensive Monte Carlo studies show that the number of long run relations can be estimated with high precision and the PME estimates of the long run coefficients show small bias and RMSE and have good size and power properties. The utility of our approach is illustrated by investigating long run relationships between key variables in two unbalanced panels one micro and one macro. The micro application uses merged CRSP-Compustat financial data on 2,000 plus US firms over the period 1950-2021. The macro application uses Penn World Table macroeconomic data on up to 177 countries over the period 1950-2019.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02135v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Chudik, M. Hashem Pesaran, Ron P. Smith</dc:creator>
    </item>
  </channel>
</rss>
