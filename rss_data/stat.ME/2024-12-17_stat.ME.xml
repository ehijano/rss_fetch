<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>KenCoh: A Ranked-Based Canonical Coherence</title>
      <link>https://arxiv.org/abs/2412.10521</link>
      <description>arXiv:2412.10521v1 Announce Type: new 
Abstract: In this paper, we consider the problem of characterizing a robust global dependence between two brain regions where each region may contain several voxels or channels. This work is driven by experiments to investigate the dependence between two cortical regions and to identify differences in brain networks between brain states, e.g., alert and drowsy states. The most common approach to explore dependence between two groups of variables (or signals) is via canonical correlation analysis (CCA). However, it is limited to only capturing linear associations and is sensitive to outlier observations. These limitations are crucial because brain network connectivity is likely to be more complex than linear and that brain signals may exhibit heavy-tailed properties. To overcome these limitations, we develop a robust method, Kendall canonical coherence (KenCoh), for learning monotonic connectivity structure among neuronal signals filtered at given frequency bands. Furthermore, we propose the KenCoh-based permutation test to investigate the differences in brain network connectivity between two different states. Our simulation study demonstrates that KenCoh is competitive to the traditional variance-covariance estimator and outperforms the later when the underlying distributions are heavy-tailed. We apply our method to EEG recordings from a virtual-reality driving experiment. Our proposed method led to further insights on the differences of frontal-parietal cross-dependence network when the subject is alert and when the subject is drowsy and that left-parietal channel drives this dependence at the beta-band.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10521v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mara Sherlin D. Talento, Sarbojit Roy, Hernando C. Ombao</dc:creator>
    </item>
    <item>
      <title>Augmented two-stage estimation for treatment crossover in oncology trials: Leveraging external data for improved precision</title>
      <link>https://arxiv.org/abs/2412.10563</link>
      <description>arXiv:2412.10563v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) in oncology often allow control group participants to crossover to experimental treatments, a practice that, while often ethically necessary, complicates the accurate estimation of long-term treatment effects. When crossover rates are high or sample sizes are limited, commonly used methods for crossover adjustment (such as the rank-preserving structural failure time model, inverse probability of censoring weights, and two-stage estimation (TSE)) may produce imprecise estimates. Real-world data (RWD) can be used to develop an external control arm for the RCT, although this approach ignores evidence from trial subjects who did not crossover and ignores evidence from the data obtained prior to crossover for those subjects who did. This paper introduces augmented two-stage estimation (ATSE), a method that combines data from non-switching participants in a RCT with an external dataset, forming a 'hybrid non-switching arm'. With a simulation study, we evaluate the ATSE method's effectiveness compared to TSE crossover adjustment and an external control arm approach. Results indicate that, relative to TSE and the external control arm approach, ATSE can increase precision and may be less susceptible to bias due to unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10563v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harlan Campbell, Jeroen P Jansen, Shannon Cope</dc:creator>
    </item>
    <item>
      <title>The Front-door Criterion in the Potential Outcome Framework</title>
      <link>https://arxiv.org/abs/2412.10600</link>
      <description>arXiv:2412.10600v1 Announce Type: new 
Abstract: In recent years, the front-door criterion (FDC) has been increasingly noticed in economics and social science. However, most economists still resist collecting this tool in their empirical toolkit. This article aims to incorporate the FDC into the framework of the potential outcome model (RCM). It redefines the key assumptions of the FDC with the language of the RCM. These assumptions are more comprehensive and detailed than the original ones in the structure causal model (SCM). The causal connotations of the FDC estimates are elaborated in detail, and the estimation bias caused by violating some key assumptions is theoretically derived. Rigorous simulation data are used to confirm the theoretical derivation. It is proved that the FDC can still provide useful insights into causal relationships even when some key assumptions are violated. The FDC is also comprehensively compared with the instrumental variables (IV) from the perspective of assumptions and causal connotations. The analyses of this paper show that the FDC can serve as a powerful empirical tool. It can provide new insights into causal relationships compared with the conventional methods in social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10600v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zexuan Chen</dc:creator>
    </item>
    <item>
      <title>A Multiprocess State Space Model with Feedback and Switching for Patterns of Clinical Measurements Associated with COVID-19</title>
      <link>https://arxiv.org/abs/2412.10639</link>
      <description>arXiv:2412.10639v1 Announce Type: new 
Abstract: Clinical measurements, such as body temperature, are often collected over time to monitor an individual's underlying health condition. These measurements exhibit complex temporal dynamics, necessitating sophisticated statistical models to capture patterns and detect deviations. We propose a novel multiprocess state space model with feedback and switching mechanisms to analyze the dynamics of clinical measurements. This model captures the evolution of time series through distinct latent processes and incorporates feedback effects in the transition probabilities between latent processes. We develop estimation methods using the EM algorithm, integrated with multiprocess Kalman filtering and multiprocess fixed-interval smoothing. Simulation study shows that the algorithm is efficient and performs well. We apply the proposed model to body temperature measurements from COVID-19-infected hemodialysis patients to examine temporal dynamics and estimate infection and recovery probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10639v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoran Ma, Wensheng Guo, Peter Kotanko, Yuedong Wang</dc:creator>
    </item>
    <item>
      <title>Combining Priors with Experience: Confidence Calibration Based on Binomial Process Modeling</title>
      <link>https://arxiv.org/abs/2412.10658</link>
      <description>arXiv:2412.10658v1 Announce Type: new 
Abstract: Confidence calibration of classification models is a technique to estimate the true posterior probability of the predicted class, which is critical for ensuring reliable decision-making in practical applications. Existing confidence calibration methods mostly use statistical techniques to estimate the calibration curve from data or fit a user-defined calibration function, but often overlook fully mining and utilizing the prior distribution behind the calibration curve. However, a well-informed prior distribution can provide valuable insights beyond the empirical data under the limited data or low-density regions of confidence scores. To fill this gap, this paper proposes a new method that integrates the prior distribution behind the calibration curve with empirical data to estimate a continuous calibration curve, which is realized by modeling the sampling process of calibration data as a binomial process and maximizing the likelihood function of the binomial process. We prove that the calibration curve estimating method is Lipschitz continuous with respect to data distribution and requires a sample size of $3/B$ of that required for histogram binning, where $B$ represents the number of bins. Also, a new calibration metric ($TCE_{bpm}$), which leverages the estimated calibration curve to estimate the true calibration error (TCE), is designed. $TCE_{bpm}$ is proven to be a consistent calibration measure. Furthermore, realistic calibration datasets can be generated by the binomial process modeling from a preset true calibration curve and confidence score distribution, which can serve as a benchmark to measure and compare the discrepancy between existing calibration metrics and the true calibration error. The effectiveness of our calibration method and metric are verified in real-world and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10658v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinzong Dong, Zhaohui Jiang, Dong Pan, Haoyang Yu</dc:creator>
    </item>
    <item>
      <title>Adaptive Nonparametric Perturbations of Parametric Bayesian Models</title>
      <link>https://arxiv.org/abs/2412.10683</link>
      <description>arXiv:2412.10683v1 Announce Type: new 
Abstract: Parametric Bayesian modeling offers a powerful and flexible toolbox for scientific data analysis. Yet the model, however detailed, may still be wrong, and this can make inferences untrustworthy. In this paper we study nonparametrically perturbed parametric (NPP) Bayesian models, in which a parametric Bayesian model is relaxed via a distortion of its likelihood. We analyze the properties of NPP models when the target of inference is the true data distribution or some functional of it, such as in causal inference. We show that NPP models can offer the robustness of nonparametric models while retaining the data efficiency of parametric models, achieving fast convergence when the parametric model is close to true. To efficiently analyze data with an NPP model, we develop a generalized Bayes procedure to approximate its posterior. We demonstrate our method by estimating causal effects of gene expression from single cell RNA sequencing data. NPP modeling offers an efficient approach to robust Bayesian inference and can be used to robustify any parametric Bayesian model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10683v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bohan Wu, Eli N. Weinstein, Sohrab Salehi, Yixin Wang, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Model checking for high dimensional generalized linear models based on random projections</title>
      <link>https://arxiv.org/abs/2412.10721</link>
      <description>arXiv:2412.10721v1 Announce Type: new 
Abstract: Most existing tests in the literature for model checking do not work in high dimension settings due to challenges arising from the "curse of dimensionality", or dependencies on the normality of parameter estimators. To address these challenges, we proposed a new goodness of fit test based on random projections for generalized linear models, when the dimension of covariates may substantially exceed the sample size. The tests only require the convergence rate of parameter estimators to derive the limiting distribution. The growing rate of the dimension is allowed to be of exponential order in relation to the sample size. As random projection converts covariates to one-dimensional space, our tests can detect the local alternative departing from the null at the rate of $n^{-1/2}h^{-1/4}$ where $h$ is the bandwidth, and $n$ is the sample size. This sensitive rate is not related to the dimension of covariates, and thus the "curse of dimensionality" for our tests would be largely alleviated. An interesting and unexpected result is that for randomly chosen projections, the resulting test statistics can be asymptotic independent. We then proposed combination methods to enhance the power performance of the tests. Detailed simulation studies and a real data analysis are conducted to illustrate the effectiveness of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10721v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Multiscale Autoregression on Adaptively Detected Timescales</title>
      <link>https://arxiv.org/abs/2412.10920</link>
      <description>arXiv:2412.10920v1 Announce Type: new 
Abstract: We propose a multiscale approach to time series autoregression, in which linear regressors for the process in question include features of its own path that live on multiple timescales. We take these multiscale features to be the recent averages of the process over multiple timescales, whose number or spans are not known to the analyst and are estimated from the data via a change-point detection technique. The resulting construction, termed Adaptive Multiscale AutoRegression (AMAR) enables adaptive regularisation of linear autoregression of large orders. The AMAR model is designed to offer simplicity and interpretability on the one hand, and modelling flexibility on the other. Our theory permits the longest timescale to increase with the sample size. A simulation study is presented to show the usefulness of our approach. Some possible extensions are also discussed, including the Adaptive Multiscale Vector AutoRegressive model (AMVAR) for multivariate time series, which demonstrates promising performance in the data example on UK and US unemployment rates. The R package amar provides an efficient implementation of the AMAR framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10920v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202023.0017</arxiv:DOI>
      <dc:creator>Rafal Baranowski, Yining Chen, Piotr Fryzlewicz</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation for Nonresponse in Complex Surveys Using Design Weights and Auxiliary Margins</title>
      <link>https://arxiv.org/abs/2412.10988</link>
      <description>arXiv:2412.10988v1 Announce Type: new 
Abstract: Survey data typically have missing values due to unit and item nonresponse. Sometimes, survey organizations know the marginal distributions of certain categorical variables in the survey. As shown in previous work, survey organizations can leverage these distributions in multiple imputation for nonignorable unit nonresponse, generating imputations that result in plausible completed-data estimates for the variables with known margins. However, this prior work does not use the design weights for unit nonrespondents; rather, it relies on a set of fabricated weights for these units. We extend this previous work to utilize the design weights for all sampled units. We illustrate the approach using simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10988v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewei Xu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Minimax Regret Estimation for Generalizing Heterogeneous Treatment Effects with Multisite Data</title>
      <link>https://arxiv.org/abs/2412.11136</link>
      <description>arXiv:2412.11136v1 Announce Type: new 
Abstract: To test scientific theories and develop individualized treatment rules, researchers often wish to learn heterogeneous treatment effects that can be consistently found across diverse populations and contexts. We consider the problem of generalizing heterogeneous treatment effects (HTE) based on data from multiple sites. A key challenge is that a target population may differ from the source sites in unknown and unobservable ways. This means that the estimates from site-specific models lack external validity, and a simple pooled analysis risks bias. We develop a robust CATE (conditional average treatment effect) estimation methodology with multisite data from heterogeneous populations. We propose a minimax-regret framework that learns a generalizable CATE model by minimizing the worst-case regret over a class of target populations whose CATE can be represented as convex combinations of site-specific CATEs. Using robust optimization, the proposed methodology accounts for distribution shifts in both individual covariates and treatment effect heterogeneity across sites. We show that the resulting CATE model has an interpretable closed-form solution, expressed as a weighted average of site-specific CATE models. Thus, researchers can utilize a flexible CATE estimation method within each site and aggregate site-specific estimates to produce the final model. Through simulations and a real-world application, we show that the proposed methodology improves the robustness and generalizability of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11136v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Melody Huang, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>BUPD: A Bayesian under-parameterized basket design with the unit information prior in oncology trials</title>
      <link>https://arxiv.org/abs/2412.11140</link>
      <description>arXiv:2412.11140v1 Announce Type: new 
Abstract: Basket trials in oncology enroll multiple patients with cancer harboring identical gene alterations and evaluate their response to targeted therapies across cancer types. Several existing methods have extended a Bayesian hierarchical model borrowing information on the response rates in different cancer types to account for the heterogeneity of drug effects. However, these methods rely on several pre-specified parameters to account for the heterogeneity of response rates among different cancer types. Here, we propose a novel Bayesian under-parameterized basket design with a unit information prior (BUPD) that uses only one (or two) pre-specified parameters to control the amount of information borrowed among cancer types, considering the heterogeneity of response rates. BUPD adapts the unit information prior approach, originally developed for borrowing information from historical clinical trial data, to enable mutual information borrowing between two cancer types. BUPD enables flexible controls of the type 1 error rate and power by explicitly specifying the strength of borrowing while providing interpretable estimations of response rates. Simulation studies revealed that BUPD reduced the type 1 error rate in scenarios with few ineffective cancer types and improved the power in scenarios with few effective cancer types better than five existing methods. This study also illustrated the efficiency of BUPD using response rates from a real basket trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11140v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryo Kitabayashi, Hiroyuki Sato, Akihiro Hirakawa</dc:creator>
    </item>
    <item>
      <title>Balancing Accuracy and Costs in Cross-Temporal Hierarchies: Investigating Decision-Based and Validation-Based Reconciliation</title>
      <link>https://arxiv.org/abs/2412.11153</link>
      <description>arXiv:2412.11153v1 Announce Type: new 
Abstract: Wind power forecasting is essential for managing daily operations at wind farms and enabling market operators to manage power uncertainty effectively in demand planning. This paper explores advanced cross-temporal forecasting models and their potential to enhance forecasting accuracy. First, we propose a novel approach that leverages validation errors, rather than traditional in-sample errors, for covariance matrix estimation and forecast reconciliation. Second, we introduce decision-based aggregation levels for forecasting and reconciliation where certain horizons are based on the required decisions in practice. Third, we evaluate the forecasting performance of the models not only on their ability to minimize errors but also on their effectiveness in reducing decision costs, such as penalties in ancillary services. Our results show that statistical-based hierarchies tend to adopt less conservative forecasts and reduce revenue losses. On the other hand, decision-based reconciliation offers a more balanced compromise between accuracy and decision cost, making them attractive for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Abolghasemi, Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>P3LS: Point Process Partial Least Squares</title>
      <link>https://arxiv.org/abs/2412.11267</link>
      <description>arXiv:2412.11267v1 Announce Type: new 
Abstract: Many studies collect data that can be considered as a realization of a point process. Included are medical imaging data where photon counts are recorded by a gamma camera from patients being injected with a gamma emitting tracer. It is of interest to develop analytic methods that can help with diagnosis as well as in the training of inexpert radiologists. Partial least squares (PLS) is a popular analytic approach that combines features from linear modeling as well as dimension reduction to provide parsimonious prediction and classification. However, existing PLS methodologies do not include the analysis of point process predictors. In this article, we introduce point process PLS (P3LS) for analyzing latent time-varying intensity functions from collections of inhomogeneous point processes. A novel estimation procedure for $P^3LS$ is developed that utilizes the properties of log-Gaussian Cox processes, and its empirical properties are examined in simulation studies. The method is used to analyze kidney functionality in patients with renal disease in order to aid in the diagnosis of kidney obstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamshid Namdari, Robert T Krafty, Amita K Manatunga</dc:creator>
    </item>
    <item>
      <title>Spatial Cross-Recurrence Quantification Analysis for Multi-Platform Contact Tracing and Epidemiology Research</title>
      <link>https://arxiv.org/abs/2412.11326</link>
      <description>arXiv:2412.11326v1 Announce Type: new 
Abstract: Contact tracing is an essential tool in slowing and containing outbreaks of contagious diseases. Current contact tracing methods range from interviews with public health personnel to Bluetooth pings from smartphones. While all methods offer various benefits, it is difficult for different methods to integrate with one another. Additionally, for contact tracing mobile applications, data privacy is a concern to many as GPS data from users is saved to either a central server or the user's device. The current paper describes a method called spatial cross-recurrence quantification analysis (SpaRQ) that can combine and analyze contact tracing data, regardless of how it has been obtained, and generate a risk profile for the user without storing GPS data. Furthermore, the plots from SpaRQ can be used to investigate the nature of the infectious agent, such as how long it can remain viable in air or on surfaces after an infected person has passed, the chance of infection based on exposure time, and what type of exposure is maximally infective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11326v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. J. Patten</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Functional Principal Components Analysis</title>
      <link>https://arxiv.org/abs/2412.11340</link>
      <description>arXiv:2412.11340v1 Announce Type: new 
Abstract: Functional Principal Components Analysis (FPCA) is one of the most successful and widely used analytic tools for exploration and dimension reduction of functional data. Standard implementations of FPCA estimate the principal components from the data but ignore their sampling variability in subsequent inferences. To address this problem, we propose the Fast Bayesian Functional Principal Components Analysis (Fast BayesFPCA), that treats principal components as parameters on the Stiefel manifold. To ensure efficiency, stability, and scalability we introduce three innovations: (1) project all eigenfunctions onto an orthonormal spline basis, reducing modeling considerations to a smaller-dimensional Stiefel manifold; (2) induce a uniform prior on the Stiefel manifold of the principal component spline coefficients via the polar representation of a matrix with entries following independent standard Normal priors; and (3) constrain sampling using the assumed FPCA structure to improve stability. We demonstrate the application of Fast BayesFPCA to characterize the variability in mealtime glucose from the Dietary Approaches to Stop Hypertension for Diabetes Continuous Glucose Monitoring (DASH4D CGM) study. All relevant STAN code and simulation routines are available as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11340v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Sartini, Xinkai Zhou, Liz Selvin, Scott Zeger, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Analyzing zero-inflated clustered longitudinal ordinal outcomes using GEE-type models with an application to dental fluorosis studies</title>
      <link>https://arxiv.org/abs/2412.11348</link>
      <description>arXiv:2412.11348v1 Announce Type: new 
Abstract: Motivated by the Iowa Fluoride Study (IFS) dataset, which comprises zero-inflated multi-level ordinal responses on tooth fluorosis, we develop an estimation scheme leveraging generalized estimating equations (GEEs) and James-Stein shrinkage. Previous analyses of this cohort study primarily focused on caries (count response) or employed a Bayesian approach to the ordinal fluorosis outcome. This study is based on the expanded dataset that now includes observations for age 23, whereas earlier works were restricted to ages 9, 13, and/or 17 according to the participants' ages at the time of measurement. The adoption of a frequentist perspective enhances the interpretability to a broader audience. Over a choice of several covariance structures, separate models are formulated for the presence (zero versus non-zero score) and severity (non-zero ordinal scores) of fluorosis, which are then integrated through shared regression parameters. This comprehensive framework effectively identifies risk or protective effects of dietary and non-dietary factors on dental fluorosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11348v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoumi Sarkar, Anish Mukherjee, Jeremy Gaskins, Steven Levy, Peihua Qiu, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>Cost-aware Portfolios in a Large Universe of Assets</title>
      <link>https://arxiv.org/abs/2412.11575</link>
      <description>arXiv:2412.11575v1 Announce Type: new 
Abstract: This paper considers the finite horizon portfolio rebalancing problem in terms of mean-variance optimization, where decisions are made based on current information on asset returns and transaction costs. The study's novelty is that the transaction costs are integrated within the optimization problem in a high-dimensional portfolio setting where the number of assets is larger than the sample size. We propose portfolio construction and rebalancing models with nonconvex penalty considering two types of transaction cost, the proportional transaction cost and the quadratic transaction cost. We establish the desired theoretical properties under mild regularity conditions. Monte Carlo simulations and empirical studies using S&amp;P 500 and Russell 2000 stocks show the satisfactory performance of the proposed portfolio and highlight the importance of involving the transaction costs when rebalancing a portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11575v1</guid>
      <category>stat.ME</category>
      <category>q-fin.PM</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingliang Fan, Marcelo C. Medeiros, Hanming Yang, Songshan Yang</dc:creator>
    </item>
    <item>
      <title>Autoregressive hidden Markov models for high-resolution animal movement data</title>
      <link>https://arxiv.org/abs/2412.11612</link>
      <description>arXiv:2412.11612v1 Announce Type: new 
Abstract: New types of high-resolution animal movement data allow for increasingly comprehensive biological inference, but method development to meet the statistical challenges associated with such data is lagging behind. In this contribution, we extend the commonly applied hidden Markov models for step lengths and turning angles to address the specific requirements posed by high-resolution movement data, in particular the very strong within-state correlation induced by the momentum in the movement. The models feature autoregressive components of general order in both the step length and the turning angle variable, with the possibility to automate the selection of the autoregressive degree using a lasso approach. In a simulation study, we identify potential for improved inference when using the new model instead of the commonly applied basic hidden Markov model in cases where there is strong within-state autocorrelation. The practical use of the model is illustrated using high-resolution movement tracks of terns foraging near an anthropogenic structure causing turbulent water flow features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11612v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ferdinand V. Stoye, Annika Hoyer, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>A New Sampling Method Base on Sequential Tests with Fixed Sample Size Upper Limit</title>
      <link>https://arxiv.org/abs/2412.11651</link>
      <description>arXiv:2412.11651v1 Announce Type: new 
Abstract: Sequential inspection is a technique employed to monitor product quality during the production process. For smaller batch sizes, the Acceptable Quality Limit(AQL) inspection theory is typically applied, whereas for larger batch sizes, the Poisson distribution is commonly utilized to determine the sample size and rejection thresholds. However, due to the fact that the rate of defective products is usually low in actual production, using these methods often requires more samples to draw conclusions, resulting in higher inspection time. Based on this, this paper proposes a sequential inspection method with a fixed upper limit of sample size. This approach not only incorporates the Poisson distribution algorithm, allowing for rapid calculation of sample size and rejection thresholds to facilitate planning, but also adapts the concept of sequential inspection to dynamically modify the sampling plan and decision-making process. This method aims to decrease the number of samples required while preserving the inspection's efficacy. Finally, this paper shows through Monte Carlo simulation that compared with the traditional Poisson distribution algorithm, the sequential test method with a fixed sample size upper limit significantly reduces the number of samples compared to the traditional Poisson distribution algorithm, while maintaining effective inspection outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11651v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dihong Huang</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v1 Announce Type: new 
Abstract: Tree-based models for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data and thus lead to inconsistent inference. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. This strategy parametrizes the tree-based sampling model according to the allocation of probability mass based on the observed data, and yet under appropriate specification, the resulting inference remains valid. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and in particular to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from using the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>Variable importance measures for heterogeneous treatment effects with survival outcome</title>
      <link>https://arxiv.org/abs/2412.11790</link>
      <description>arXiv:2412.11790v1 Announce Type: new 
Abstract: Treatment effect heterogeneity plays an important role in many areas of causal inference and within recent years, estimation of the conditional average treatment effect (CATE) has received much attention in the statistical community. While accurate estimation of the CATE-function through flexible machine learning procedures provides a tool for prediction of the individual treatment effect, it does not provide further insight into the driving features of potential treatment effect heterogeneity. Recent papers have addressed this problem by providing variable importance measures for treatment effect heterogeneity. Most of the suggestions have been developed for continuous or binary outcome, while little attention has been given to censored time-to-event outcome. In this paper, we extend the treatment effect variable importance measure (TE-VIM) proposed in Hines et al. (2022) to the survival setting with censored outcome. We derive an estimator for the TE-VIM for two different CATE functions based on the survival function and RMST, respectively. Along with the TE-VIM, we propose a new measure of treatment effect heterogeneity based on the best partially linear projection of the CATE and suggest accompanying estimators for that projection. All estimators are based on semiparametric efficiency theory, and we give conditions under which they are asymptotically linear. The finite sample performance of the derived estimators are investigated in a simulation study. Finally, the estimators are applied and contrasted in two real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11790v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Christoffer Ziersen, Torben Martinussen</dc:creator>
    </item>
    <item>
      <title>Causal Invariance Learning via Efficient Optimization of a Nonconvex Objective</title>
      <link>https://arxiv.org/abs/2412.11850</link>
      <description>arXiv:2412.11850v1 Announce Type: new 
Abstract: Data from multiple environments offer valuable opportunities to uncover causal relationships among variables. Leveraging the assumption that the causal outcome model remains invariant across heterogeneous environments, state-of-the-art methods attempt to identify causal outcome models by learning invariant prediction models and rely on exhaustive searches over all (exponentially many) covariate subsets. These approaches present two major challenges: 1) determining the conditions under which the invariant prediction model aligns with the causal outcome model, and 2) devising computationally efficient causal discovery algorithms that scale polynomially, instead of exponentially, with the number of covariates. To address both challenges, we focus on the additive intervention regime and propose nearly necessary and sufficient conditions for ensuring that the invariant prediction model matches the causal outcome model. Exploiting the essentially necessary identifiability conditions, we introduce Negative Weight Distributionally Robust Optimization NegDRO a nonconvex continuous minimax optimization whose global optimizer recovers the causal outcome model. Unlike standard group DRO problems that maximize over the simplex, NegDRO allows negative weights on environment losses, which break the convexity. Despite its nonconvexity, we demonstrate that a standard gradient method converges to the causal outcome model, and we establish the convergence rate with respect to the sample size and the number of iterations. Our algorithm avoids exhaustive search, making it scalable especially when the number of covariates is large. The numerical results further validate the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11850v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yifan Hu, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Multiplex Dirichlet stochastic block model for clustering multidimensional compositional networks</title>
      <link>https://arxiv.org/abs/2412.11971</link>
      <description>arXiv:2412.11971v1 Announce Type: new 
Abstract: Network data often represent multiple types of relations, which can also denote exchanged quantities, and are typically encompassed in a weighted multiplex. Such data frequently exhibit clustering structures, however, traditional clustering methods are not well-suited for multiplex networks. Additionally, standard methods treat edge weights in their raw form, potentially biasing clustering towards a node's total weight capacity rather than reflecting cluster-related interaction patterns. To address this, we propose transforming edge weights into a compositional format, enabling the analysis of connection strengths in relative terms and removing the impact of nodes' total weights. We introduce a multiplex Dirichlet stochastic block model designed for multiplex networks with compositional layers. This model accounts for sparse compositional networks and enables joint clustering across different types of interactions. We validate the model through a simulation study and apply it to the international export data from the Food and Agriculture Organization of the United Nations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11971v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iuliia Promskaia, Adrian O'Hagan, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Scientific Realism vs. Anti-Realism: Toward a Common Ground</title>
      <link>https://arxiv.org/abs/2412.10643</link>
      <description>arXiv:2412.10643v1 Announce Type: cross 
Abstract: The debate between scientific realism and anti-realism remains at a stalemate, with reconciliation seeming hopeless. Yet, important work remains: to seek a common ground, even if only to uncover deeper points of disagreement. I develop the idea that everyone values some truths, and use it to benefit both sides of the debate. More specifically, many anti-realists, such as instrumentalists, have yet to seriously engage with Sober's call to justify their preferred version of Ockham's razor through a positive epistemology. Meanwhile, realists face a similar challenge: providing a non-circular explanation of how their version of Ockham's razor connects to truth. Drawing insights from fields that study scientific inference -- statistics and machine learning -- I propose a common ground that addresses these challenges for both sides. This common ground also isolates a distinctively epistemic root of the irreconcilability in the realism debate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10643v1</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Posterior asymptotics of high-dimensional spiked covariance model with inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2412.10753</link>
      <description>arXiv:2412.10753v1 Announce Type: cross 
Abstract: We consider Bayesian inference on the spiked eigenstructures of high-dimensional covariance matrices; specifically, we focus on estimating the eigenvalues and corresponding eigenvectors of high-dimensional covariance matrices in which a few eigenvalues are significantly larger than the rest. We impose an inverse-Wishart prior distribution on the unknown covariance matrix and derive the posterior distributions of the eigenvalues and eigenvectors by transforming the posterior distribution of the covariance matrix. We prove that the posterior distribution of the spiked eigenvalues and corresponding eigenvectors converges to the true parameters under the spiked high-dimensional covariance assumption, and also that the posterior distribution of the spiked eigenvector attains the minimax optimality under the single spiked covariance model. Simulation studies and real data analysis demonstrate that our proposed method outperforms all existing methods in quantifying uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10753v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangmin Lee, Sewon Park, Seongmin Kim, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal analysis of extreme winter temperatures in Ireland</title>
      <link>https://arxiv.org/abs/2412.10796</link>
      <description>arXiv:2412.10796v1 Announce Type: cross 
Abstract: We analyse extreme daily minimum temperatures in winter months over the island of Ireland from 1950-2022. We model the marginal distributions of extreme winter minima using a generalised Pareto distribution (GPD), capturing temporal and spatial non-stationarities in the parameters of the GPD. We investigate two independent temporal non-stationarities in extreme winter minima. We model the long-term trend in magnitude of extreme winter minima as well as short-term, large fluctuations in magnitude caused by anomalous behaviour of the jet stream. We measure magnitudes of spatial events with a carefully chosen risk function and fit an r-Pareto process to extreme events exceeding a high-risk threshold. Our analysis is based on synoptic data observations courtesy of Met \'Eireann and the Met Office. We show that the frequency of extreme cold winter events is decreasing over the study period. The magnitude of extreme winter events is also decreasing, indicating that winters are warming, and apparently warming at a faster rate than extreme summer temperatures. We also show that extremely cold winter temperatures are warming at a faster rate than non-extreme winter temperatures. We find that a climate model output previously shown to be informative as a covariate for modelling extremely warm summer temperatures is less effective as a covariate for extremely cold winter temperatures. However, we show that the climate model is useful for informing a non-extreme temperature model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10796v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D\'aire Healy, Jonathan A. Tawn, Peter Thorne, Andrew Parnell</dc:creator>
    </item>
    <item>
      <title>ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2412.11104</link>
      <description>arXiv:2412.11104v1 Announce Type: cross 
Abstract: In causal inference, randomized experiment is a de facto method to overcome various theoretical issues in observational study. However, the experimental design requires expensive costs, so an efficient experimental design is necessary. We propose ABC3, a Bayesian active learning policy for causal inference. We show a policy minimizing an estimation error on conditional average treatment effect is equivalent to minimizing an integrated posterior variance, similar to Cohn criteria \citep{cohn1994active}. We theoretically prove ABC3 also minimizes an imbalance between the treatment and control groups and the type 1 error probability. Imbalance-minimizing characteristic is especially notable as several works have emphasized the importance of achieving balance. Through extensive experiments on real-world data sets, ABC3 achieves the highest efficiency, while empirically showing the theoretical results hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11104v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Taehun Cha, Donghun Lee</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian deep reinforcement learning</title>
      <link>https://arxiv.org/abs/2412.11743</link>
      <description>arXiv:2412.11743v1 Announce Type: cross 
Abstract: Bayesian reinforcement learning (BRL) is a method that merges principles from Bayesian statistics and reinforcement learning to make optimal decisions in uncertain environments. Similar to other model-based RL approaches, it involves two key components: (1) Inferring the posterior distribution of the data generating process (DGP) modeling the true environment and (2) policy learning using the learned posterior. We propose to model the dynamics of the unknown environment through deep generative models assuming Markov dependence. In absence of likelihood functions for these models we train them by learning a generalized predictive-sequential (or prequential) scoring rule (SR) posterior. We use sequential Monte Carlo (SMC) samplers to draw samples from this generalized Bayesian posterior distribution. In conjunction, to achieve scalability in the high dimensional parameter space of the neural networks, we use the gradient based Markov chain Monte Carlo (MCMC) kernels within SMC. To justify the use of the prequential scoring rule posterior we prove a Bernstein-von Misses type theorem. For policy learning, we propose expected Thompson sampling (ETS) to learn the optimal policy by maximizing the expected value function with respect to the posterior distribution. This improves upon traditional Thompson sampling (TS) and its extensions which utilize only one sample drawn from the posterior distribution. This improvement is studied both theoretically and using simulation studies assuming discrete action and state-space. Finally we successfully extend our setup for a challenging problem with continuous action space without theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11743v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreya Sinha Roy, Richard G. Everitt, Christian P. Robert, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>Bayesian Levy-Dynamic Spatio-Temporal Process: Towards Big Data Analysis</title>
      <link>https://arxiv.org/abs/2105.08451</link>
      <description>arXiv:2105.08451v2 Announce Type: replace 
Abstract: In this era of big data, all scientific disciplines are evolving fast to cope up with the enormity of the available information. So is statistics, the queen of science. Big data are particularly relevant to spatio-temporal statistics, thanks to much-improved technology in satellite based remote sensing and Geographical Information Systems. However, none of the existing approaches seem to meet the simultaneous demand of reality emulation and cheap computation. In this article, with the Levy random fields as the starting point, e construct a new Bayesian nonparametric, nonstationary and nonseparable dynamic spatio-temporal model with the additional realistic property that the lagged spatio-temporal correlations converge to zero as the lag tends to infinity. Although our Bayesian model seems to be intricately structured and is variable-dimensional with respect to each time index, we are able to devise a fast and efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for Bayesian inference. Our simulation experiment brings out quite encouraging performance from our Bayesian Levy-dynamic approach. We finally apply our Bayesian Levy-dynamic model and methods to a sea surface temperature dataset consisting of 139,300 data points in space and time. Although not big data in the true sense, this is a large and highly structured data by any standard. Even for this large and complex data, our parallel MCMC algorithm, implemented on 80 processors, generated 110,000 MCMC realizations from the Levy-dynamic posterior within a single day, and the resultant Bayesian posterior predictive analysis turned out to be encouraging. Thus, it is not unreasonable to expect that with significantly more computing resources, it is feasible to analyse terabytes of spatio-temporal data with our new model and methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.08451v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Extremes in High Dimensions: Methods and Scalable Algorithms</title>
      <link>https://arxiv.org/abs/2303.04258</link>
      <description>arXiv:2303.04258v2 Announce Type: replace 
Abstract: Extreme value theory for univariate and low-dimensional observations has been explored in considerable detail, but the field is still in an early stage regarding high-dimensional settings. This paper focuses on H\"usler-Reiss models, a popular class of models for multivariate extremes similar to multivariate Gaussian distributions, and their domain of attraction. We develop estimators for the model parameters based on score matching, and we equip these estimators with theories and exceptionally scalable algorithms. Simulations and applications to weather extremes demonstrate the fact that the estimators can estimate a large number of parameters reliably and fast; for example, we show that H\"usler-Reiss models with thousands of parameters can be fitted within a couple of minutes on a standard laptop. More generally speaking, our work relates extreme value theory to modern concepts of high-dimensional statistics and convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04258v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Lederer, Marco Oesting</dc:creator>
    </item>
    <item>
      <title>Estimating Conditional Average Treatment Effects with Heteroscedasticity by Model Averaging and Matching</title>
      <link>https://arxiv.org/abs/2304.06960</link>
      <description>arXiv:2304.06960v2 Announce Type: replace 
Abstract: We propose a model averaging approach, combined with a partition and matching method to estimate the conditional average treatment effects under heteroskedastic error settings. The proposed approach has asymptotic optimality and consistency of weights and estimator. Numerical studies show that our method has good finite-sample performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06960v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.econlet.2024.111679</arxiv:DOI>
      <arxiv:journal_reference>Economics Letters, 2024, 238: 111679</arxiv:journal_reference>
      <dc:creator>Pengfei Shi, Xinyu Zhang, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Nonparametric Causal Decomposition of Group Disparities</title>
      <link>https://arxiv.org/abs/2306.16591</link>
      <description>arXiv:2306.16591v4 Announce Type: replace 
Abstract: We introduce a new nonparametric causal decomposition approach that identifies the mechanisms by which a treatment variable contributes to a group-based outcome disparity. Our approach distinguishes three mechanisms: group differences in 1) treatment prevalence, 2) average treatment effects, and 3) selection into treatment based on individual-level treatment effects. Our approach reformulates classic Kitagawa-Blinder-Oaxaca decompositions in causal and nonparametric terms, complements causal mediation analysis by explaining group disparities instead of group effects, and isolates conceptually distinct mechanisms conflated in recent random equalization decompositions. In contrast to all prior approaches, our framework uniquely identifies differential selection into treatment as a novel disparity-generating mechanism. Our approach can be used for both the retrospective causal explanation of disparities and the prospective planning of interventions to change disparities. We present both an unconditional and a conditional decomposition, where the latter quantifies the contributions of the treatment within levels of certain covariates. We develop nonparametric estimators that are $\sqrt{n}$-consistent, asymptotically normal, semiparametrically efficient, and multiply robust. We apply our approach to analyze the mechanisms by which college graduation causally contributes to intergenerational income persistence (the disparity in adult income between the children of high- vs low-income parents). Empirically, we demonstrate a previously undiscovered role played by the new selection component in intergenerational income persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16591v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Felix Elwert</dc:creator>
    </item>
    <item>
      <title>Functional conditional volatility modeling with missing data: inference and application to energy commodities</title>
      <link>https://arxiv.org/abs/2310.00356</link>
      <description>arXiv:2310.00356v2 Announce Type: replace 
Abstract: This paper explores the nonparametric estimation of the volatility component in a heteroscedastic scalar-on-function regression model, where the underlying discrete-time process is ergodic and subject to a missing-at-random mechanism. We first propose a simplified estimator for the regression and volatility operators, constructed solely from the observed data. The asymptotic properties of these estimators, including the almost sure uniform consistency rate and asymptotic distribution, are rigorously analyzed. Subsequently, the simplified estimators are employed to impute the missing data in the original process, enhancing the estimation of the regression and volatility components. The asymptotic behavior of these imputed estimators is also thoroughly investigated. A numerical comparison of the simplified and imputed estimators is presented using simulated data. Finally, the methodology is applied to real-world data to model the volatility of daily natural gas returns, utilizing intraday EU/USD exchange rate return curves sampled at a 1-hour frequency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00356v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Abdelbasset Djeniah, Mohamed Chaouch, Amina Angelika Bouchentouf</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Sparse Poisson Regression</title>
      <link>https://arxiv.org/abs/2311.01147</link>
      <description>arXiv:2311.01147v5 Announce Type: replace 
Abstract: We have utilized the non-conjugate VB method for the problem of the sparse Poisson regression model. To provide an approximated conjugacy in the model, the likelihood is approximated by a quadratic function, which provides the conjugacy of the approximation component with the Gaussian prior to the regression coefficient. Three sparsity-enforcing priors are used for this problem. The proposed models are compared with each other and two frequentist sparse Poisson methods (LASSO and SCAD) to evaluate the estimation, prediction and the sparsing performance of the proposed methods. Throughout a simulated data example, the accuracy of the VB methods is computed compared to the corresponding benchmark MCMC methods. It can be observed that the proposed VB methods have provided a good approximation to the posterior distribution of the parameters, while the VB methods are much faster than the MCMC ones. Using several benchmark count response data sets, the prediction performance of the proposed methods is evaluated in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01147v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mitra Kharabati, Morteza Amini, Mohammad Arashi</dc:creator>
    </item>
    <item>
      <title>Bayesian inference on Cox regression models using catalytic prior distributions</title>
      <link>https://arxiv.org/abs/2312.01411</link>
      <description>arXiv:2312.01411v2 Announce Type: replace 
Abstract: The Cox proportional hazards model (Cox model) is a popular model for survival data analysis. When the sample size is small relative to the dimension of the model, the standard maximum partial likelihood inference is often problematic. In this work, we propose the Cox catalytic prior distributions for Bayesian inference on Cox models, which is an extension of a general class of prior distributions originally designed for stabilizing complex parametric models. The Cox catalytic prior is formulated as a weighted likelihood of the regression coefficients based on synthetic data and a surrogate baseline hazard constant. This surrogate hazard can be either provided by the user or estimated from the data, and the synthetic data are generated from the predictive distribution of a fitted simpler model. For point estimation, we derive an approximation of the marginal posterior mode, which can be computed conveniently as a regularized log partial likelihood estimator. We prove that our prior distribution is proper and the resulting estimator is consistent under mild conditions. In simulation studies, our proposed method outperforms standard maximum partial likelihood inference and is on par with existing shrinkage methods. We further illustrate the application of our method to a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01411v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihao Li, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>FDR Control for Online Anomaly Detection</title>
      <link>https://arxiv.org/abs/2312.01969</link>
      <description>arXiv:2312.01969v2 Announce Type: replace 
Abstract: A new online multiple testing procedure is described in the context of anomaly detection, which controls the False Discovery Rate (FDR). An accurate anomaly detector must control the false positive rate at a prescribed level while keeping the false negative rate as low as possible. However in the online context, such a constraint remains highly challenging due to the usual lack of FDR control: the online framework makes it impossible to use classical multiple testing approaches such as the Benjamini-Hochberg (BH) procedure, which would require knowing the entire time series. The developed strategy relies on exploiting the local control of the ``modified FDR'' (mFDR) criterion. It turns out that the local control of mFDR enables global control of the FDR over the full series up to additional modifications of the multiple testing procedures. An important ingredient in this control is the cardinality of the calibration dataset used to compute the empirical p-values. A dedicated strategy for tuning this parameter is designed for achieving the prescribed FDR control over the entire time series. The good statistical performance of the full strategy is analyzed by theoretical guarantees. Its practical behavior is assessed by several simulation experiments which support our conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01969v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Etienne Kr\"onert, Alain C\'elisse, Dalila Hattab</dc:creator>
    </item>
    <item>
      <title>Spectral Deconfounding for High-Dimensional Sparse Additive Models</title>
      <link>https://arxiv.org/abs/2312.02860</link>
      <description>arXiv:2312.02860v3 Announce Type: replace 
Abstract: Many high-dimensional data sets suffer from hidden confounding which affects both the predictors and the response of interest. In such situations, standard regression methods or algorithms lead to biased estimates. This paper substantially extends previous work on spectral deconfounding for high-dimensional linear models to the nonlinear setting and with this, establishes a proof of concept that spectral deconfounding is valid for general nonlinear models. Concretely, we propose an algorithm to estimate high-dimensional sparse additive models in the presence of hidden dense confounding: arguably, this is a simple yet practically useful nonlinear scope. We prove consistency and convergence rates for our method and evaluate it on synthetic data and a genetic data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02860v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Zijian Guo, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Simulation study to evaluate when Plasmode simulation is superior to parametric simulation in estimating the mean squared error of the least squares estimator in linear regression</title>
      <link>https://arxiv.org/abs/2312.04077</link>
      <description>arXiv:2312.04077v3 Announce Type: replace 
Abstract: Simulation is a crucial tool for the evaluation and comparison of statistical methods. How to design fair and neutral simulation studies is therefore of great interest for researchers developing new methods and practitioners confronted with the choice of the most suitable method. The term simulation usually refers to parametric simulation, that is, computer experiments using artificial data made up of pseudo-random numbers. Plasmode simulation, that is, computer experiments using the combination of resampling feature data from a real-life dataset and generating the target variable with a known user-selected outcome-generating model (OGM), is an alternative that is often claimed to produce more realistic data. We compare parametric and Plasmode simulation for the example of estimating the mean squared error (MSE) of the least squares estimator (LSE) in linear regression. If the true underlying data-generating process (DGP) and the OGM were known, parametric simulation would obviously be the best choice in terms of estimating the MSE well. However, in reality, both are usually unknown, so researchers have to make assumptions: in Plasmode simulation for the OGM, in parametric simulation for both DGP and OGM. Most likely, these assumptions do not exactly reflect the truth. Here, we aim to find out how assumptions deviating from the true DGP and the true OGM affect the performance of parametric and Plasmode simulations in the context of MSE estimation for the LSE and in which situations which simulation type is preferable. Our results suggest that the preferable simulation method depends on many factors, including the number of features, and on how and to what extent the assumptions of a parametric simulation differ from the true DGP. Also, the resampling strategy used for Plasmode influences the results. In particular, subsampling with a small sampling proportion can be recommended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04077v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0299989</arxiv:DOI>
      <arxiv:journal_reference>PLOS ONE (2024)</arxiv:journal_reference>
      <dc:creator>Marieke Stolte, Nicholas Schreck, Alla Slynko, Maral Saadati, Axel Benner, J\"org Rahnenf\"uhrer, Andrea Bommert</dc:creator>
    </item>
    <item>
      <title>Doubly regularized generalized linear models for spatial observations with high-dimensional covariates</title>
      <link>https://arxiv.org/abs/2401.15793</link>
      <description>arXiv:2401.15793v2 Announce Type: replace 
Abstract: A discrete spatial lattice can be cast as a network structure over which spatially-correlated outcomes are observed. A second network structure may also capture similarities among measured features, when such information is available. Incorporating the network structures when analyzing such doubly-structured data can improve predictive power, and lead to better identification of important features in the data-generating process. Motivated by applications in spatial disease mapping, we develop a new doubly regularized regression framework to incorporate these network structures for analyzing high-dimensional datasets. Our estimators can be easily implemented with standard convex optimization algorithms. In addition, we describe a procedure to obtain asymptotically valid confidence intervals and hypothesis tests for our model parameters. We show empirically that our framework provides improved predictive accuracy and inferential power compared to existing high-dimensional spatial methods. These advantages hold given fully accurate network information, and also with networks which are partially misspecified or uninformative. The application of the proposed method to modeling COVID-19 mortality data suggests that it can improve prediction of deaths beyond standard spatial models, and that it selects relevant covariates more often.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15793v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjun Sondhi, Si Cheng, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>EM Estimation of the B-Spline Copula with Penalized Pseudo-Likelihood Functions</title>
      <link>https://arxiv.org/abs/2402.07569</link>
      <description>arXiv:2402.07569v2 Announce Type: replace 
Abstract: The B-spline copula function is defined by a linear combination of elements of the normalized B-spline basis. We develop a modified EM algorithm, to maximize the penalized pseudo-likelihood function, wherein we use the smoothly clipped absolute deviation (SCAD) penalty function for the penalization term. We conduct simulation studies to demonstrate the stability of the proposed numerical procedure, show that penalization yields estimates with smaller mean-square errors when the true parameter matrix is sparse, and provide methods for determining tuning parameters and for model selection. We analyze as an example a data set consisting of birth and death rates from 237 countries, available at the website, ''Our World in Data,'' and we estimate the marginal density and distribution functions of those rates together with all parameters of our B-spline copula model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07569v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoling Dou, Satoshi Kuriki, Gwo Dong Lin, Donald Richards</dc:creator>
    </item>
    <item>
      <title>Stochastic gradient descent-based inference for dynamic network models with attractors</title>
      <link>https://arxiv.org/abs/2403.07124</link>
      <description>arXiv:2403.07124v3 Announce Type: replace 
Abstract: In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions. Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory. However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications. We address these issues by (i) introducing a Stochastic gradient descent (SGD) parameter estimation method, (ii) developing a novel approach for uncertainty quantification using SGD, and (iii) extending the model to allow nodes to join and leave over time. Simulation results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks. We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07124v3</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hancong Pan, Xiaojing Zhu, Cantay Caliskan, Dino P. Christenson, Konstantinos Spiliopoulos, Dylan Walker, Eric D. Kolaczyk</dc:creator>
    </item>
    <item>
      <title>Policy Learning for Optimal Dynamic Treatment Regimes with Observational Data</title>
      <link>https://arxiv.org/abs/2404.00221</link>
      <description>arXiv:2404.00221v5 Announce Type: replace 
Abstract: Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00221v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Expansion of net correlations in terms of partial correlations</title>
      <link>https://arxiv.org/abs/2404.01734</link>
      <description>arXiv:2404.01734v2 Announce Type: replace 
Abstract: The marginal correlation between two variables is a measure of their linear dependence. The two original variables need not interact directly, because marginal correlation may arise from the mediation of other variables in the system. The underlying network of direct interactions can be captured by a weighted graphical model. The connection between two variables can be weighted by their partial correlation, defined as the residual correlation left after accounting for the linear effects of mediating variables. While matrix inversion can be used to obtain marginal correlations from partial correlations, in large systems this approach does not reveal how the former emerge from the latter. Here we present an expansion of marginal correlations in terms of partial correlations, which shows that the effect of mediating variables can be quantified by the weight of the paths in the graphical model that connect the original pair of variables. The expansion is proved to converge for arbitrary probability distributions. The graphical interpretation reveals a close connection between the topology of the graph and the marginal correlations. Moreover, the expansion shows how marginal correlations change when some variables are severed from the graph, and how partial correlations change when some variables are marginalised out from the description. It also establishes the minimum number of latent variables required to replicate the exact effect of a collection of variables that are marginalised out, ensuring that the partial and marginal correlations of the remaining variables remain unchanged. Notably, the number of latent variables may be significantly smaller than the number of variables that they effectively replicate. Finally, for Gaussian variables, marginal correlations are shown to be related to the efficacy with which information propagates along the paths in the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01734v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bautista Arenaza, Sebasti\'an Risau-Gusman, In\'es Samengo</dc:creator>
    </item>
    <item>
      <title>Relational event models with global covariates</title>
      <link>https://arxiv.org/abs/2406.09055</link>
      <description>arXiv:2406.09055v2 Announce Type: replace 
Abstract: Bike sharing is an increasingly popular mobility choice as it is a sustainable, healthy and economically viable transportation mode. By interpreting rides between bike stations over time as temporal events connecting two bike stations, relational event models can provide important insights into this phenomenon. The focus of relational event models, as a typical event history model, is normally on dyadic or node-specific covariates, as global covariates are considered nuisance parameters in a partial likelihood approach. As full likelihood approaches are infeasible given the sheer size of the relational process, we propose an innovative sampling approach of temporally shifted non-events to recover important global drivers of the relational process. The method combines nested case-control sampling on a time-shifted version of the event process. This leads to a partial likelihood of the relational event process that is identical to that of a degenerate logistic additive model, enabling efficient estimation of both global and non-global covariate effects. The computational effectiveness of the method is demonstrated through a simulation study. The analysis of around 350,000 bike rides in the Washington D.C. area reveals significant influences of weather and time of day on bike sharing dynamics, besides a number of traditional node-specific and dyadic covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09055v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melania Lembo, R\=uta Juozaitien\.e, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the total treatment effect with multiple outcomes in the presence of terminal events</title>
      <link>https://arxiv.org/abs/2412.09304</link>
      <description>arXiv:2412.09304v2 Announce Type: replace 
Abstract: As standards of care advance, patients are living longer and once-fatal diseases are becoming manageable. Clinical trials increasingly focus on reducing disease burden, which can be quantified by the timing and occurrence of multiple non-fatal clinical events. Most existing methods for the analysis of multiple event-time data require stringent modeling assumptions that can be difficult to verify empirically, leading to treatment efficacy estimates that forego interpretability when the underlying assumptions are not met. Moreover, most existing methods do not appropriately account for informative terminal events, such as premature treatment discontinuation or death, which prevent the occurrence of subsequent events. To address these limitations, we derive and validate estimation and inference procedures for the area under the mean cumulative function (AUMCF), an extension of the restricted mean survival time to the multiple event-time setting. The AUMCF is nonparametric, clinically interpretable, and properly accounts for terminal competing risks. To enable covariate adjustment, we also develop an augmentation estimator that provides efficiency at least equaling, and often exceeding, the unadjusted estimator. The utility and interpretability of the AUMCF are illustrated with extensive simulation studies and through an analysis of multiple heart-failure-related endpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST) clinical trial. Our open-source R package MCC makes conducting AUMCF analyses straightforward and accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09304v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Gronsbell, Zachary R. McCaw, Isabelle-Emmanuella Nogues, Xiangshan Kong, Tianxi Cai, Lu Tian, LJ Wei</dc:creator>
    </item>
    <item>
      <title>E-backtesting</title>
      <link>https://arxiv.org/abs/2209.00991</link>
      <description>arXiv:2209.00991v5 Announce Type: replace-cross 
Abstract: In the recent Basel Accords, the Expected Shortfall (ES) replaces the Value-at-Risk (VaR) as the standard risk measure for market risk in the banking sector, making it the most important risk measure in financial regulation. One of the most challenging tasks in risk modeling practice is to backtest ES forecasts provided by financial institutions. To design a model-free backtesting procedure for ES, we make use of the recently developed techniques of e-values and e-processes. Backtest e-statistics are introduced to formulate e-processes for risk measure forecasts, and unique forms of backtest e-statistics for VaR and ES are characterized using recent results on identification functions. For a given backtest e-statistic, a few criteria for optimally constructing the e-processes are studied. The proposed method can be naturally applied to many other risk measures and statistical quantities. We conduct extensive simulation studies and data analysis to illustrate the advantages of the model-free backtesting method, and compare it with the ones in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00991v5</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiuqi Wang, Ruodu Wang, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Estimating Causal Effects of Discrete and Continuous Treatments with Binary Instruments</title>
      <link>https://arxiv.org/abs/2403.05850</link>
      <description>arXiv:2403.05850v2 Announce Type: replace-cross 
Abstract: We propose an instrumental variable framework for identifying and estimating causal effects of discrete and continuous treatments with binary instruments. The basis of our approach is a local copula representation of the joint distribution of the potential outcomes and unobservables determining treatment assignment. This representation allows us to introduce an identifying assumption, so-called copula invariance, that restricts the local dependence of the copula with respect to the treatment propensity. We show that copula invariance identifies treatment effects for the entire population and other subpopulations such as the treated. The identification results are constructive and lead to practical estimation and inference procedures based on distribution regression. An application to estimating the effect of sleep on well-being uncovers interesting patterns of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05850v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Sukjin Han, Kaspar W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Matrix Completion via Residual Spectral Matching</title>
      <link>https://arxiv.org/abs/2412.10005</link>
      <description>arXiv:2412.10005v2 Announce Type: replace-cross 
Abstract: Noisy matrix completion has attracted significant attention due to its applications in recommendation systems, signal processing and image restoration. Most existing works rely on (weighted) least squares methods under various low-rank constraints. However, minimizing the sum of squared residuals is not always efficient, as it may ignore the potential structural information in the residuals. In this study, we propose a novel residual spectral matching criterion that incorporates not only the numerical but also locational information of residuals. This criterion is the first in noisy matrix completion to adopt the perspective of low-rank perturbation of random matrices and exploit the spectral properties of sparse random matrices. We derive optimal statistical properties by analyzing the spectral properties of sparse random matrices and bounding the effects of low-rank perturbations and partial observations. Additionally, we propose algorithms that efficiently approximate solutions by constructing easily computable pseudo-gradients. The iterative process of the proposed algorithms ensures convergence at a rate consistent with the optimal statistical error bound. Our method and algorithms demonstrate improved numerical performance in both simulated and real data examples, particularly in environments with high noise levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10005v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyuan Chen, Fang Yao</dc:creator>
    </item>
  </channel>
</rss>
