<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>P4sim: Programming Protocol-independent Packet Processors in ns-3</title>
      <link>https://arxiv.org/abs/2503.17554</link>
      <description>arXiv:2503.17554v1 Announce Type: new 
Abstract: Programmable data planes enable users to design data plane algorithms for network devices, providing extensive flexibility for network customization. Programming Protocol-Independent Packet Processors (P4) has become the most widely adopted abstraction, programming language, and framework for data plane programming. However, existing simulation platforms lack high-performance support for P4-based networks. This paper introduces P4sim, a high-performance P4-driven simulation framework built on bmv2 and NS4, seamlessly integrated with ns-3. It improves queue modeling, time scheduling, and P4 architecture support, extending compatibility to V1model, PSA, and PNA. P4sim enables efficient packet processing, accurate time tracking, and seamless interaction between P4-enabled hosts and switches. We evaluate the P4sim in terms of performance and queue management and demonstrate its capabilities using two common use cases: Basic Tunneling and Load Balancing. The results highlight the P4sim as a powerful tool for advancing research and education in programmable networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17554v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Ma, Giang T. Nguyen</dc:creator>
    </item>
    <item>
      <title>Conditional Diffusion Model with OOD Mitigation as High-Dimensional Offline Resource Allocation Planner in Clustered Ad Hoc Networks</title>
      <link>https://arxiv.org/abs/2503.17693</link>
      <description>arXiv:2503.17693v1 Announce Type: new 
Abstract: Due to network delays and scalability limitations, clustered ad hoc networks widely adopt Reinforcement Learning (RL) for on-demand resource allocation. Albeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions struggle to tackle the huge action space, which generally explodes exponentially along with the number of resource allocation units, enduring low sampling efficiency and high interaction cost. In contrast to MFRL, Model-Based RL (MBRL) offers an alternative solution to boost sample efficiency and stabilize the training by explicitly leveraging a learned environment model. However, establishing an accurate dynamic model for complex and noisy environments necessitates a careful balance between model accuracy and computational complexity $\&amp;$ stability. To address these issues, we propose a Conditional Diffusion Model Planner (CDMP) for high-dimensional offline resource allocation in clustered ad hoc networks. By leveraging the astonishing generative capability of Diffusion Models (DMs), our approach enables the accurate modeling of high-quality environmental dynamics while leveraging an inverse dynamics model to plan a superior policy. Beyond simply adopting DMs in offline RL, we further incorporate the CDMP algorithm with a theoretically guaranteed, uncertainty-aware penalty metric, which theoretically and empirically manifests itself in mitigating the Out-of-Distribution (OOD)-induced distribution shift issue underlying scarce training data. Extensive experiments also show that our model outperforms MFRL in average reward and Quality of Service (QoS) while demonstrating comparable performance to other MBRL algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17693v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kechen Meng, Sinuo Zhang, Rongpeng Li, Chan Wang, Ming Lei, Zhifeng Zhao</dc:creator>
    </item>
    <item>
      <title>RAISE: Optimizing RIS Placement to Maximize Task Throughput in Multi-Server Vehicular Edge Computing</title>
      <link>https://arxiv.org/abs/2503.17708</link>
      <description>arXiv:2503.17708v1 Announce Type: new 
Abstract: Given the limited computing capabilities on autonomous vehicles, onboard processing of large volumes of latency-sensitive tasks presents significant challenges. While vehicular edge computing (VEC) has emerged as a solution, offloading data-intensive tasks to roadside servers or other vehicles is hindered by large obstacles like trucks/buses and the surge in service demands during rush hours. To address these challenges, Reconfigurable Intelligent Surface (RIS) can be leveraged to mitigate interference from ground signals and reach more edge servers by elevating RIS adaptively. To this end, we propose RAISE, an optimization framework for RIS placement in multi-server VEC systems. Specifically, RAISE optimizes RIS altitude and tilt angle together with the optimal task assignment to maximize task throughput under deadline constraints. To find a solution, a two-layer optimization approach is proposed, where the inner layer exploits the unimodularity of the task assignment problem to derive the efficient optimal strategy while the outer layer develops a near-optimal hill climbing (HC) algorithm for RIS placement with low complexity. Extensive experiments demonstrate that the proposed RAISE framework consistently outperforms existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17708v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Ma, Zhengru Fang, Longzhi Yuan, Yiqin Deng, Xianhao Chen, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>CP-AgentNet: Autonomous and Explainable Communication Protocol Design Using Generative Agents</title>
      <link>https://arxiv.org/abs/2503.17850</link>
      <description>arXiv:2503.17850v1 Announce Type: new 
Abstract: Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. 2) The decision-making process in DRL models is often opaque, commonly described as a 'black box.' 3) DRL models are data hungry. In response, we propose CP-AgentNet, the first framework designed to use generative agents for developing communication network protocols. This approach addresses these challenges by creating an autonomous system for protocol design, significantly reducing human effort. We developed LLMA (LLM-agents-based multiple access) and CPTCP (CP-Agent-based TCP) for heterogeneous environments. Our comprehensive simulations have demonstrated the efficient coexistence of LLMA and CPTCP with nodes using different types of protocols, as well as enhanced explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17850v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dae Cheol Kwon, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Satellite-Terrestrial Integrated Fog Networks: Architecture, Technologies, and Challenges</title>
      <link>https://arxiv.org/abs/2503.17912</link>
      <description>arXiv:2503.17912v1 Announce Type: new 
Abstract: In the evolution of sixth-generation (6G) mobile communication networks, satellite-terrestrial integrated networks emerge as a promising paradigm, characterized by their wide coverage and reliable transmission capabilities. By integrating with cloud-based terrestrial mobile communication networks, the limitations of low Earth orbit (LEO) satellites, such as insufficient onboard computing capabilities and limited inter-satellite link capacity, can be addressed. In addition, to efficiently respond to the diverse integrated tasks of communication, remote sensing, and navigation, LEO constellations need to be capable of autonomous networking. To this end, this article presents a satellite-terrestrial integrated fog network for 6G. Its system architecture and key technologies are introduced to achieve flexible collaboration between fog satellites and terrestrial cloud computing centers. In particular, key techniques with diverse challenges and their corresponding solutions are discussed, including integrated waveform design and resource management based on fog satellite onboard processing, as well as mobility management and native artificial intelligence based on cloud-fog collaboration. Finally, future challenges and open issues are outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17912v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MWC.011.2400262</arxiv:DOI>
      <dc:creator>Shuo Yuan, Mugen Peng, Yaohua Sun</dc:creator>
    </item>
    <item>
      <title>Cache-Aware Cooperative Multicast Beamforming in Dynamic Satellite-Terrestrial Networks</title>
      <link>https://arxiv.org/abs/2503.17913</link>
      <description>arXiv:2503.17913v1 Announce Type: new 
Abstract: With the burgeoning demand for data-intensive services, satellite-terrestrial networks (STNs) face increasing backhaul link congestion, deteriorating user quality of service (QoS), and escalating power consumption. Cache-aided STNs are acknowledged as a promising paradigm for accelerating content delivery to users and alleviating the load of backhaul links. However, the dynamic nature of low earth orbit (LEO) satellites and the complex interference among satellite beams and terrestrial base stations pose challenges in effectively managing limited edge resources. To address these issues, this paper proposes a method for dynamically scheduling caching and communication resources, aiming to reduce network costs in terms of transmission power consumption and backhaul traffic, while meeting user QoS demands and resource constraints. We formulate a mixed timescale problem to jointly optimize cache placement, LEO satellite beam direction, and cooperative multicast beamforming among satellite beams and base stations. To tackle this intricate problem, we propose a two-stage solution framework, where the primary problem is decoupled into a short-term content delivery subproblem and a long-term cache placement subproblem. The former subproblem is solved by designing an alternating optimization approach with whale optimization and successive convex approximation methods according to the cache placement state, while cache content in STNs is updated using an iterative algorithm that utilizes historical information. Simulation results demonstrate the effectiveness of our proposed algorithms, showcasing their convergence and significantly reducing transmission power consumption and backhaul traffic by up to 52%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17913v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVT.2024.3463548</arxiv:DOI>
      <dc:creator>Shuo Yuan, Yaohua Sun, Mugen Peng</dc:creator>
    </item>
    <item>
      <title>QR-MO: Q-Routing for Multi-Objective Shortest-Path Computation in 5G-MEC Systems</title>
      <link>https://arxiv.org/abs/2503.18122</link>
      <description>arXiv:2503.18122v1 Announce Type: new 
Abstract: Multi-access edge computing (MEC) is a promising technology that provides low-latency processing capabilities. To optimize the network performance in a MEC system, an efficient routing path between a user and a MEC host is essential. The network performance is characterized by multiple attributes, including packet-loss probability, latency, and jitter. A user service may require a particular combination of such attributes, complicating the shortest-path computation. This paper introduces Q-Routing for Multi-Objective shortest-path computation (QR-MO), which simultaneously optimizes multiple attributes. We compare the QR-MO's solutions with the optimal solutions provided by the Multi-objective Dijkstra Algorithm (MDA). The result shows the favorable potential of QR-MO. After 100 episodes, QR-MO achieves 100% accuracy in networks with low to moderate average node degrees, regardless of the size, and over 85% accuracy in networks with high average node degrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18122v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annisa Sarah, Rosario G. Garroppo, Gianfranco Nencioni</dc:creator>
    </item>
    <item>
      <title>Beam Scheduling in Millimeter Wave Networks Using the Whittle Index</title>
      <link>https://arxiv.org/abs/2503.18133</link>
      <description>arXiv:2503.18133v1 Announce Type: new 
Abstract: We address the problem of beam scheduling for downlink transmissions in a single-cell millimeter wave (mmWave) network. The cell contains a mmWave base station (mBS) and its associated users. At the end of each time slot, a packet arrives into the queue of a user at the mBS with a certain probability. A holding cost is incurred for the packets stored in a user's queue at the mBS in every time slot. The number of simultaneous beams that the mBS can form to different users is less than the number of associated users. Also, a cost is incurred whenever a beam is formed from the mBS to a user. In a given time slot, a packet transmitted from the mBS to a user that has been assigned a beam is successfully received (respectively, not received) if the channel quality between the mBS and the user is good (respectively, bad). In every time slot, the mBS needs to assign the available beams to a subset of the users, in order to minimize the long-run expected average cost. This problem can be modeled as a restless multi-armed bandit problem, which is provably hard to solve. We prove the Whittle indexability of the above beam scheduling problem and propose a strategy to compute the Whittle index of each user. In each time slot, our proposed beam scheduling policy assigns beams to the users with the smallest Whittle indices. Using extensive simulations, we show that our proposed Whittle index-based beam scheduling policy significantly outperforms several scheduling policies proposed in prior work in terms of the average cost, average delay, as well as energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18133v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mandar R. Nalavade, Ravindra S. Tomar, Gaurav S. Kasbekar</dc:creator>
    </item>
    <item>
      <title>Charakterystyka odbiciowa inteligentnych matryc antenowych</title>
      <link>https://arxiv.org/abs/2503.18203</link>
      <description>arXiv:2503.18203v1 Announce Type: new 
Abstract: This paper presents the results of reflection characteristics measurements for reconfigurable intelligent surfaces (RIS). The measurements were carried out in a non-ideal environment, i.e. typical for subsequent practical use of RISes. During the experiments, popular matrices implemented in the open-source project OpenSource- RIS were used. The study focused on obtaining two types of reflection characteristics - two- and three-dimensional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18203v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.15199/59.2024.4.58</arxiv:DOI>
      <dc:creator>Pawe{\l} Hatka, Dawid Brz\k{a}ka{\l}a, Marcel Garczyk, Pawe{\l} P{\l}aczkiewicz, Marta Sieradzka, Cyryl Prentki, Tomasz Jaworski, Krzysztof Cicho\'n, Adrian Kliks</dc:creator>
    </item>
    <item>
      <title>AraRACH: Enhancing NextG Random Access Reliability in Programmable Wireless Living Labs</title>
      <link>https://arxiv.org/abs/2503.18218</link>
      <description>arXiv:2503.18218v1 Announce Type: new 
Abstract: The rapid evolution of wireless technologies has intensified interest in open and fully programmable radio access networks for whole-stack research, innovation, and evaluation of emerging solutions. Large-scale wireless living labs, such as ARA, equipped with real-world infrastructure play a vital role in this evolution by enabling researchers to prototype and evaluate advanced algorithms for next-generation wireless systems in outdoor and over-the-air environments benefiting from real-world fidelity and end-to-end programmability. However, at the core of this innovation is the performance in terms of coverage and reliability of these wireless living labs. For instance, interfacing power amplifiers and low noise amplifiers with software-defined radios (SDRs) for experimenting outdoors introduces issues in random access procedure-a process crucial in establishing connectivity between user equipment (UE) and the core network in 5G and 6G systems. Therefore, to ensure seamless connectivity and reliable communications in open-source 5G software stacks such as OpenAirInterface (OAI), we propose a slot-based approach to the 5G random access procedure leveraging full downlink (DL) and uplink (UL) slots instead of using special or mixed slots. We highlight how this approach achieves reliable 5G connectivity over 1 mile-the longest communication range that has been achieved so far in real-world settings using open-source 5G software stacks and the Universal Software Radio Peripheral (USRP) SDRs. We also demonstrate that, in a highly obstructed environment such as an industrial setting, we can increase the probability of a successful random access procedure to 90%-100% when we use at least 9 OFDM symbols to transmit msg2 and msg3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18218v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joshua Ofori Boateng, Tianyi Zhang, Guoying Zu, Taimoor Ul Islam, Sarath Babu, Florian Kaltenberger, Robert Schmidt, Hongwei Zhang, Daji Qiao</dc:creator>
    </item>
    <item>
      <title>Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study</title>
      <link>https://arxiv.org/abs/2503.18487</link>
      <description>arXiv:2503.18487v1 Announce Type: new 
Abstract: Network attack detection is a pivotal technology to identify network anomaly and classify malicious traffic. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network threat detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there is still a lack of comprehensive elaboration how to mine LLMs' potentials in network threat detections, as well as the opportunities and challenges. In this paper, we mainly focus on the classification of malicious traffic from the perspective of LLMs' capability. We present a holistic view of the architecture of LLM-powered network attack detection, including Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in network attack detection: \textit{Classifier, Encoder, and Predictor}. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly $35$\% improvement compared to existing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18487v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, Yong Cui</dc:creator>
    </item>
    <item>
      <title>Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and Power Interference in Multi-User OSaaS Networks</title>
      <link>https://arxiv.org/abs/2503.18495</link>
      <description>arXiv:2503.18495v1 Announce Type: new 
Abstract: We present a framework to identify and mitigate rogue OOK signals and user-generated power interference in a multi-user Optical-Spectrum-as-a-Service network. Experimental tests on the OpenIreland-testbed achieve up to 89% detection rate within 10 seconds of an interference event.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18495v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agastya Raj, Devika Dass, Daniel C. Kilper, Marco Ruffini</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Dynamic Training and Inference for GNN-Based Network Modeling</title>
      <link>https://arxiv.org/abs/2503.18706</link>
      <description>arXiv:2503.18706v1 Announce Type: new 
Abstract: Efficient network modeling is essential for resource optimization and network planning in next-generation large-scale complex networks. Traditional approaches, such as queuing theory-based modeling and packet-based simulators, can be inefficient due to the assumption made and the computational expense, respectively. To address these challenges, we propose an innovative energy-efficient dynamic orchestration of Graph Neural Networks (GNN) based model training and inference framework for context-aware network modeling and predictions. We have developed a low-complexity solution framework, QAG, that is a Quantum approximation optimization (QAO) algorithm for Adaptive orchestration of GNN-based network modeling. We leverage the tripartite graph model to represent a multi-application system with many compute nodes. Thereafter, we apply the constrained graph-cutting using QAO to find the feasible energy-efficient configurations of the GNN-based model and deploying them on the available compute nodes to meet the network modeling application requirements. The proposed QAG scheme closely matches the optimum and offers atleast a 50% energy saving while meeting the application requirements with 60% lower churn-rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18706v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chetna Singhal, Yassine Hadjadj-Aoul</dc:creator>
    </item>
    <item>
      <title>Signal Propagation in RIS-Aided 5G Systems</title>
      <link>https://arxiv.org/abs/2503.18915</link>
      <description>arXiv:2503.18915v1 Announce Type: new 
Abstract: In this paper, we conduct an in-depth analysis of radio signal propagation characteristics within the urban environment of Poznan (Poland). The study specifically addresses the deployment of a 5th generation (5G NR - New Radio) Radio Access Network (RAN), which comprises 8 strategically positioned Base Stations (BSs). These base stations are configured with either Single Input Single Output (SISO) or Multiple Input Multiple Output (MIMO) antenna technologies, contingent upon the specific requirements of the network cells they serve. A key focus of our research is the integration of 15 reflecting arrays, known as Reconfigurable Intelligent Surfaces (RISs), which were installed throughout the study area. These RISs were deployed at various suspension heights to evaluate their impact on radio signal propagation and coverage. By exploring the influence of these RIS matrices, our research sheds light on their potential to significantly enhance signal quality, particularly in urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18915v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/WiMob61911.2024.10770363</arxiv:DOI>
      <arxiv:journal_reference>20th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob 2024), Paris, France, 2024, pp. 443-448</arxiv:journal_reference>
      <dc:creator>Adam Samorzewski, Adrian Kliks</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting</title>
      <link>https://arxiv.org/abs/2503.17410</link>
      <description>arXiv:2503.17410v1 Announce Type: cross 
Abstract: Accurate network traffic forecasting is essential for Internet Service Providers (ISP) to optimize resources, enhance user experience, and mitigate anomalies. This study evaluates state-of-the-art deep learning models on CESNET-TimeSeries24, a recently published, comprehensive real-world network traffic dataset from the ISP network CESNET3 spanning multivariate time series over 40 weeks. Our findings highlight the balance between prediction accuracy and computational efficiency across different levels of network granularity. Additionally, this work establishes a reproducible methodology that facilitates direct comparison of existing approaches, explores their strengths and weaknesses, and provides a benchmark for future studies using this dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17410v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josef Koumar, Timotej Smole\v{n}, Kamil Je\v{r}\'abek, Tom\'a\v{s} \v{C}ejka</dc:creator>
    </item>
    <item>
      <title>A Generative Caching System for Large Language Models</title>
      <link>https://arxiv.org/abs/2503.17603</link>
      <description>arXiv:2503.17603v1 Announce Type: cross 
Abstract: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17603v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi</dc:creator>
    </item>
    <item>
      <title>Detecting and Mitigating DDoS Attacks with AI: A Survey</title>
      <link>https://arxiv.org/abs/2503.17867</link>
      <description>arXiv:2503.17867v1 Announce Type: cross 
Abstract: Distributed Denial of Service attacks represent an active cybersecurity research problem. Recent research shifted from static rule-based defenses towards AI-based detection and mitigation. This comprehensive survey covers several key topics. Preeminently, state-of-the-art AI detection methods are discussed. An in-depth taxonomy based on manual expert hierarchies and an AI-generated dendrogram are provided, thus settling DDoS categorization ambiguities. An important discussion on available datasets follows, covering data format options and their role in training AI detection methods together with adversarial training and examples augmentation. Beyond detection, AI based mitigation techniques are surveyed as well. Finally, multiple open research directions are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17867v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandru Apostu, Silviu Gheorghe, Andrei H\^iji, Nicolae Cleju, Andrei P\u{a}tra\c{s}cu, Cristian Rusu, Radu Ionescu, Paul Irofti</dc:creator>
    </item>
    <item>
      <title>Fast Inverse Model Transformation: Algebraic Framework for Fast Data Plane Verification</title>
      <link>https://arxiv.org/abs/2401.13488</link>
      <description>arXiv:2401.13488v3 Announce Type: replace 
Abstract: Data plane verification (DPV) analyzes routing tables and detects routing abnormalities and policy violations during network operation and planning. Thus, it has become an important tool to harden the networking infrastructure and the computing systems building on top. Substantial advancements have been made in the last decade and state-of-the-art DPV systems can achieve sub-us verification for an update of a single forwarding rule.
  In this paper, we introduce fast inverse model transformation (FIMT), the first theoretical framework to systematically model and analyze centralized DPV systems. FIMT reveals the algebraic structure in the model update process, a key step in fast DPV systems. Thus, it can systematically analyze the correctness of several DPV systems, using algebraic properties. The theory also guides the design and implementation of NeoFlash, a refactored version of Flash with new optimization techniques. Evaluations show that NeoFlash outperforms existing state-of-the-art centralized DPV systems in various datasets and reveal insights to key techniques towards fast DPV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13488v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shenshen Chen, Jian Luo, Dong Guo, Kai Gao, Yang Richard Yang</dc:creator>
    </item>
    <item>
      <title>An Overview and Solution for Democratizing AI Workflows at the Network Edge</title>
      <link>https://arxiv.org/abs/2407.11905</link>
      <description>arXiv:2407.11905v3 Announce Type: replace 
Abstract: With the process of democratization of the network edge, hardware and software for networks are becoming available to the public, overcoming the confines of traditional cloud providers and network operators. This trend, coupled with the increasing importance of AI in 6G and beyond cellular networks, presents opportunities for innovative AI applications and systems at the network edge. While AI models and services are well-managed in cloud systems, achieving similar maturity for serving network needs remains an open challenge. Existing open solutions are emerging and are yet to consider democratization requirements. In this work, we identify key requirements for democratization and propose NAOMI, a solution for democratizing AI/ML workflows at the network edge designed based on those requirements. Guided by the functionality and overlap analysis of the O-RAN AI/ML workflow architecture and MLOps systems, coupled with the survey of open-source AI/ML tools, we develop a modular, scalable, and distributed hardware architecture-independent solution. NAOMI leverages state-of-the-art open-source tools and can be deployed on distributed clusters of heterogeneous devices. The results show that NAOMI performs up to 40% better in deployment time and up to 73% faster in AI/ML workflow execution for larger datasets compared to AI/ML Framework, a representative open network access solution, while performing inference and utilizing resources on par with its counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11905v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrej \v{C}op, Bla\v{z} Bertalani\v{c}, Carolina Fortuna</dc:creator>
    </item>
    <item>
      <title>Neural Representation for Wireless Radiation Field Reconstruction: A 3D Gaussian Splatting Approach</title>
      <link>https://arxiv.org/abs/2412.04832</link>
      <description>arXiv:2412.04832v4 Announce Type: replace 
Abstract: Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a long-standing challenge. This issue has been escalated due to denser network deployment, larger antenna arrays, and broader bandwidth in next-generation networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting (3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. While WRF-GS demonstrates remarkable effectiveness, it faces limitations in capturing high-frequency signal variations caused by complex multipath effects. To overcome these limitations, we propose WRF-GS+, an enhanced framework that integrates electromagnetic wave physics into the neural network design. WRF-GS+ leverages deformable 3D Gaussians to model both static and dynamic components of the WRF, significantly improving its ability to characterize signal variations. In addition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS modeling process and improving computational efficiency. Experimental results demonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial spectrum synthesis, including ray tracing and other deep-learning approaches. Notably, WRF-GS+ achieves state-of-the-art performance in the received signal strength indication (RSSI) and channel state information (CSI) prediction tasks, surpassing existing methods by more than 0.7 dB and 3.36 dB, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04832v4</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaozheng Wen, Jingwen Tong, Yingdong Hu, Zehong Lin, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Adapting Network Information into Semantics for Generalizable and Plug-and-Play Multi-Scenario Network Diagnosis</title>
      <link>https://arxiv.org/abs/2501.16842</link>
      <description>arXiv:2501.16842v2 Announce Type: replace 
Abstract: Leverage large language model (LLM) to refer the fault is considered to be a potential solution for intelligent network fault diagnosis. However, how to represent network information in a paradigm that can be understood by LLMs has always been a core issue that has puzzled scholars in the field of network intelligence. To address this issue, we propose LLM-based Network Semantic Generation (LNSG) algorithm, which integrates semanticization and symbolization methods to uniformly describe the entire multi-modal network information. Based on the LNSG and LLMs, we present NetSemantic, a plug-and-play, data-independent, network information semantic fault diagnosis framework. It enables rapid adaptation to various network environments and provides efficient fault diagnosis capabilities. Experimental results demonstrate that NetSemantic excels in network fault diagnosis across various complex scenarios in a zero-shot manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16842v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiao Tan, Fengxiao Tang, Linfeng Luo, Xiaonan Wang, Zaijing Li, Ming Zhao</dc:creator>
    </item>
    <item>
      <title>Characterizing User Behavior: The Interplay Between Mobility Patterns and Mobile Traffic</title>
      <link>https://arxiv.org/abs/2501.19348</link>
      <description>arXiv:2501.19348v2 Announce Type: replace 
Abstract: Mobile devices have become essential for capturing human activity, and eXtended Data Records (XDRs) offer rich opportunities for detailed user behavior modeling, which is useful for designing personalized digital services. Previous studies have primarily focused on aggregated mobile traffic and mobility analyses, often neglecting individual-level insights. This paper introduces a novel approach that explores the dependency between traffic and mobility behaviors at the user level. By analyzing 13 individual features that encompass traffic patterns and various mobility aspects, we enhance the understanding of how these behaviors interact. Our advanced user modeling framework integrates traffic and mobility behaviors over time, allowing for fine-grained dependencies while maintaining population heterogeneity through user-specific signatures. Furthermore, we develop a Markov model that infers traffic behavior from mobility and vice versa, prioritizing significant dependencies while addressing privacy concerns. Using a week-long XDR dataset from 1,337,719 users across several provinces in Chile, we validate our approach, demonstrating its robustness and applicability in accurately inferring user behavior and matching mobility and traffic profiles across diverse urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19348v2</guid>
      <category>cs.NI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Josiane Kouam, Aline Carneiro Viana, Mariano G. Beir\'o, Leo Ferres, Luca Pappalardo</dc:creator>
    </item>
    <item>
      <title>Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios</title>
      <link>https://arxiv.org/abs/2503.08317</link>
      <description>arXiv:2503.08317v3 Announce Type: replace-cross 
Abstract: Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08317v3</guid>
      <category>cs.RO</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang</dc:creator>
    </item>
    <item>
      <title>Distribution and Purification of Entanglement States in Quantum Networks</title>
      <link>https://arxiv.org/abs/2503.14712</link>
      <description>arXiv:2503.14712v2 Announce Type: replace-cross 
Abstract: We consider problems of distributing high-fidelity entangled states across nodes of a quantum network. We consider a repeater-based network architecture with entanglement swapping (fusion) operations for generating long-distance entanglements, and purification operations that produce high-fidelity states from several lower-fidelity states. The contributions of this paper are two-fold: First, while there have been several works on fidelity-aware routing and incorporating purification into routing for generating EPs, this paper presents the first algorithms for optimal solutions to the high-fidelity EP distribution problem. We provide a dynamic programming algorithm for generating the optimal tree of operations to produce a high-fidelity EP, and an LP-based algorithm for generating an optimal collection of trees. Second, following the EP algorithms, this paper presents the first algorithms for the high-fidelity GHZ-state distribution problem and characterizes its optimality. We evaluate our techniques via simulations over NetSquid, a quantum network simulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14712v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojie Fan, Yukun Yang, Himanshu Gupta, C. R. Ramakrishnan</dc:creator>
    </item>
  </channel>
</rss>
