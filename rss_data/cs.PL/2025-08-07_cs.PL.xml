<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 01:28:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>If-T: A Benchmark for Type Narrowing</title>
      <link>https://arxiv.org/abs/2508.03830</link>
      <description>arXiv:2508.03830v1 Announce Type: new 
Abstract: **Context:**
The design of static type systems that can validate dynamically-typed programs (**gradually**) is an ongoing challenge. A key difficulty is that dynamic code rarely follows datatype-driven design. Programs instead use runtime tests to narrow down the proper usage of incoming data. Type systems for dynamic languages thus need a **type narrowing** mechanism that refines the type environment along individual control paths based on dominating tests, a form of flow-sensitive typing. In order to express refinements, the type system must have some notion of sets and subsets. Since set-theoretic types are computationally and ergonomically complex, the need for type narrowing raises design questions about how to balance precision and performance.  
**Inquiry:**
To date, the design of type narrowing systems has been driven by intuition, past experience, and examples from users in various language communities. There is no standard that captures desirable and undesirable behaviors. Prior formalizations of narrowing are also significantly more complex than a standard type system, and it is unclear how the extra complexity pays off in terms of concrete examples. This paper addresses the problems through If-T, a language-agnostic **design benchmark** for type narrowing that characterizes the abilities of implementations using simple programs that draw attention to fundamental questions. Unlike a traditional performance-focused benchmark, If-T measures a narrowing system's ability to validate correct code and reject incorrect code. Unlike a test suite, systems are not required to fully conform to If-T. Deviations are acceptable provided they are justified by well-reasoned design considerations, such as compile-time performance.  
**Approach:**
If-T is guided by the literature on type narrowing, the documentation of gradual languages such as TypeScript, and experiments with typechecker implementations. We have identified a set of core technical dimensions for type narrowing. For each dimension, the benchmark contains a set of topics and (at least) two characterizing programs per topic: one that should typecheck and one that should not typecheck.  
**Knowledge:**
If-T provides a baseline to measure type narrowing systems. For researchers, it provides criteria to categorize future designs via its collection of positive and negative examples. For language designers, the benchmark demonstrates the payoff of typechecker complexity in terms of concrete examples. Designers can use the examples to decide whether supporting a particular example is worthwhile. Both the benchmark and its implementations are freely available online.  
**Grounding:**
We have implemented the benchmark for five typecheckers: TypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight important differences, such as the ability to track logical implications among program variables and typechecking for user-defined narrowing predicates.  
**Importance:**
Type narrowing is essential for gradual type systems, but the tradeoffs between systems with different complexity have been unclear. If-T clarifies these tradeoffs by illustrating the benefits and limitations of each level of complexity. With If-T as a way to assess implementations in a fair, cross-language manner, future type system designs can strive for a better balance among precision, annotation burden, and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03830v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.22152/programming-journal.org/2025/10/17</arxiv:DOI>
      <arxiv:journal_reference>The Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 2, Article 17</arxiv:journal_reference>
      <dc:creator>Hanwen Guo (University of Utah, USA), Ben Greenman (University of Utah, USA)</dc:creator>
    </item>
    <item>
      <title>A Type System for Data Privacy Compliance in Active Object Languages</title>
      <link>https://arxiv.org/abs/2508.03831</link>
      <description>arXiv:2508.03831v1 Announce Type: new 
Abstract: Data protection laws such as GDPR aim to give users unprecedented control over their personal data. Compliance with these regulations requires systematically considering information flow and interactions among entities handling sensitive data. Privacy-by-design principles advocate embedding data protection into system architectures as a default. However, translating these abstract principles into concrete, explicit methods remains a significant challenge. This paper addresses this gap by proposing a language-based approach to privacy integration, combining static and runtime techniques. By employing type checking and type inference in an active object language, the framework enables the tracking of authorised data flows and the automatic generation of constraints checked at runtime based on user consent. This ensures that personal data is processed in compliance with GDPR constraints. The key contribution of this work is a type system that gather the compliance checks and the changes to users consent and integrates data privacy compliance verification into system execution. The paper demonstrates the feasibility of this approach through a soundness proof and several examples, illustrating how the proposed language addresses common GDPR requirements, such as user consent, purpose limitation, and data subject rights. This work advances the state of the art in privacy-aware system design by offering a systematic and automated method for integrating GDPR compliance into programming languages. This capability has implications for building trustworthy systems in domains such as healthcare or finance, where data privacy is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03831v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.22152/programming-journal.org/2025/10/18</arxiv:DOI>
      <arxiv:journal_reference>The Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 2, Article 18</arxiv:journal_reference>
      <dc:creator>Chinmayi Prabhu Baramashetru (University of Oslo, Norway), Paola Giannini (Universita' del Piemonte Orientale, Italy), Silvia Lizeth Tapia Tarifa (University of Oslo, Norway), Olaf Owe (University of Oslo, Norway)</dc:creator>
    </item>
    <item>
      <title>Generating Inputs for Grammar Mining using Dynamic Symbolic Execution</title>
      <link>https://arxiv.org/abs/2508.03832</link>
      <description>arXiv:2508.03832v1 Announce Type: new 
Abstract: A vast number of software systems include components that parse and process structured input. In addition to programming languages, which are analyzed by compilers or interpreters, there are numerous components that process standardized or proprietary data formats of varying complexity. Even if such components were initially developed and tested based on a specification, such as a grammar, numerous modifications and adaptations over the course of software evolution can make it impossible to precisely determine which inputs they actually accept.  
In this situation, grammar mining can be used to reconstruct the specification in the form of a grammar. Established approaches already produce useful results, provided that sufficient input data is available to fully cover the input language. However, achieving this completeness is a major challenge. In practice, only input data recorded during the operation of the software systems is available. If this data is used for grammar mining, the resulting grammar reflects only the actual processed inputs but not the complete grammar of the input language accepted by the software component. As a result, edge cases or previously supported features that no longer appear in the available input data are missing from the generated grammar.  
This work addresses this challenge by introducing a novel approach for the automatic generation of inputs for grammar mining. Although input generators have already been used for fuzz testing, it remains unclear whether they are also suitable for grammar miners. Building on the grammar miner Mimid, this work presents a fully automated approach to input generation. The approach leverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms to overcome the limitations of DSE regarding structured input parsers. First, the search for new inputs is guided by an iterative expansion that starts with a single-character input and gradually extends it. Second, input generation is structured into a novel three-phase approach, which separates the generation of inputs for parser functions.  
The proposed method was evaluated against a diverse set of eleven benchmark applications from the existing literature. Results demonstrate that the approach achieves precision and recall for extracted grammars close to those derived from state-of-the-art grammar miners such as Mimid. Notably, it successfully uncovers subtle features and edge cases in parsers that are typically missed by such grammar miners. The effectiveness of the method is supported by empirical evidence, showing that it can achieve high performance in various domains without requiring prior input samples.  
This contribution is significant for researchers and practitioners in software engineering, offering an automated, scalable, and precise solution for grammar mining. By eliminating the need for manual input generation, the approach not only reduces workload but also enhances the robustness and comprehensiveness of the extracted grammars. Following this approach, software engineers can reconstruct specification from existing (legacy) parsers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03832v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.22152/programming-journal.org/2025/10/16</arxiv:DOI>
      <arxiv:journal_reference>The Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 2, Article 16</arxiv:journal_reference>
      <dc:creator>Andreas Pointner (University of Applied Sciences Upper Austria, Austria), Josef Pichler (University of Applied Sciences Upper Austria, Austria), Herbert Pr\"ahofer (Johannes Kepler University Linz, Austria)</dc:creator>
    </item>
    <item>
      <title>Weak Memory Model Formalisms: Introduction and Survey</title>
      <link>https://arxiv.org/abs/2508.04115</link>
      <description>arXiv:2508.04115v1 Announce Type: new 
Abstract: Memory consistency models define the order in which accesses to shared memory in a concurrent system may be observed to occur. Such models are a necessity since program order is not a reliable indicator of execution order, due to microarchitectural features or compiler transformations. Concurrent programming, already a challenging task, is thus made even harder when weak memory effects must be addressed. A rigorous specification of weak memory models is therefore essential to make this problem tractable for developers of safety- and security-critical, low-level software.
  In this paper we survey the field of formalisations of weak memory models, including their specification, their effects on execution, and tools and inference systems for reasoning about code. To assist the discussion we also provide an introduction to two styles of formal representation found commonly in the literature (using a much simplified version of Intel's x86 as the example): a step-by-step construction of traces of the system (operational semantics); and with respect to relations between memory events (axiomatic semantics). The survey covers some long-standing hardware features that lead to observable weak behaviours, a description of historical developments in practice and in theory, an overview of computability and complexity results, and outlines current and future directions in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04115v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger C. Su, Robert J. Colvin</dc:creator>
    </item>
    <item>
      <title>Raqlet: Cross-Paradigm Compilation for Recursive Queries</title>
      <link>https://arxiv.org/abs/2508.03978</link>
      <description>arXiv:2508.03978v1 Announce Type: cross 
Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses the fragmentation of recursive querying engines spanning relational (recursive SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for querying graph data within relational and graph databases; however, real-world support remains inconsistent across systems. Raqlet bridges this gap by translating recursive queries across paradigms through leveraging intermediate representations (IRs) grounded in well-defined semantics; it translates Cypher or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog), and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared semantic basis that can serve as a golden reference implementation for language standards, while supporting static analysis and transformations (e.g., magic-set transformation) for performance tuning. Our vision is to make Raqlet a robust platform that enables rapid cross-paradigm prototyping, portable recursive queries, and formal reasoning about recursion even when targeting diverse query execution engines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03978v1</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Shaikhha, Youning Xia, Meisam Tarabkhah, Jazal Saleem, Anna Herlihy</dc:creator>
    </item>
    <item>
      <title>RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution</title>
      <link>https://arxiv.org/abs/2312.08617</link>
      <description>arXiv:2312.08617v5 Announce Type: replace 
Abstract: The automatic generation of RTL code (e.g., Verilog) using natural language instructions and large language models (LLMs) has attracted significant research interest recently. However, most existing approaches heavily rely on commercial LLMs such as ChatGPT, while open-source LLMs tailored for this specific design generation task exhibit notably inferior performance. The absence of high-quality open-source solutions restricts the flexibility and data privacy of this emerging technique. In this study, we present a new customized LLM solution with a modest parameter count of only 7B, achieving better performance than GPT-3.5 on all representative benchmarks for RTL code generation. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark. This remarkable balance between accuracy and efficiency is made possible by leveraging our new RTL code dataset and a customized LLM algorithm, both of which have been made fully open-source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08617v5</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LAD62341.2024.10691788</arxiv:DOI>
      <dc:creator>Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, Zhiyao Xie</dc:creator>
    </item>
    <item>
      <title>Strong Priority and Determinacy in Timed CCS</title>
      <link>https://arxiv.org/abs/2403.04618</link>
      <description>arXiv:2403.04618v4 Announce Type: replace 
Abstract: Building on the standard theory of process algebra with priorities, we identify a new scheduling mechanism, called "constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinacy-by-construction for multi-cast concurrent communication with shared memory. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of "coherent" processes a confluence property for constructive reductions. We show that under some restrictions, called "pivotability", coherence is preserved by the operators of prefix, summation, parallel composition, restriction and hiding. Since this permits memory and sharing, we are able to cover a strictly larger class of processes compared to those in Milner's classical confluence theory for CCS without priorities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04618v4</guid>
      <category>cs.PL</category>
      <category>cs.CL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luigi Liquori, Michael Mendler</dc:creator>
    </item>
    <item>
      <title>ALEA IACTA EST: A Declarative Domain-Specific Language for Manually Performable Random Experiments</title>
      <link>https://arxiv.org/abs/2506.11794</link>
      <description>arXiv:2506.11794v2 Announce Type: replace 
Abstract: Random experiments that are simple and clear enough to be performed by human agents feature prominently in the teaching of elementary stochastics as well as in games.  We present Alea, a domain-specific language for the specification of random experiments.  Alea code can either be analyzed statically to obtain and inspect probability distributions of outcomes, or be executed with a source pseudo-randomness for simulation or as a game assistant.  The language is intended for ease of use by non-expert programmers, in particular students of elementary stochastics, and players and designers of games of chance, by focusing on concepts common to functional programming and basic mathematics.  Both the design of the language and the implementation of runtime environments are work in progress.
</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11794v2</guid>
      <category>cs.PL</category>
      <category>math.PR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.424.4</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 424, 2025, pp. 67-86</arxiv:journal_reference>
      <dc:creator>Baltasar Tranc\'on y Widemann (Brandenburg University of Applied Sciences), Markus Lepper (semantics gGmbH)</dc:creator>
    </item>
    <item>
      <title>IsaMini: Redesigned Isabelle Proof Language for Machine Learning</title>
      <link>https://arxiv.org/abs/2507.18885</link>
      <description>arXiv:2507.18885v2 Announce Type: replace 
Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly Large Language Models (LLMs), to automate formal proofs in proof assistants. This approach holds promise for reducing the dramatic labor costs or computation costs required in proof engineering, which is fundamental to formal verification and other software engineering methods. The paper explores the potential of improving NTP by redesigning the proof language, given that LLMs' capabilities depend highly on representations. We introduce \emph{MiniLang}, a redesigned proof language for Isabelle/HOL incorporating an improved version of Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by improving the success rate on the PISA benchmark by up to 29\% in comparison to generation of Isar proof script. The success rate under one attempt (so-called \emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64 (65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA (71.0\%) achieved by Magnushammer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18885v2</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiyuan Xu, Renxi Wang, Haonan Li, David Sanan, Conrad Watt</dc:creator>
    </item>
  </channel>
</rss>
