<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Note on Fine-Grained Quantum Reductions for Linear Algebraic Problems</title>
      <link>https://arxiv.org/abs/2509.19528</link>
      <description>arXiv:2509.19528v1 Announce Type: cross 
Abstract: We observe that any $T(n)$ time algorithm (quantum or classical) for several central linear algebraic problems, such as computing $\det(A)$, $tr(A^3)$, or $tr(A^{-1})$ for an $n \times n$ integer matrix $A$, yields a $O(T(n)) + \tilde O(n^2)$ time \textit{quantum algorithm} for $n \times n$ matrix-matrix multiplication. That is, on quantum computers, the complexity of these problems is essentially equivalent to that of matrix multiplication. Our results follow by first observing that the Bernstein-Vazirani algorithm gives a direct quantum reduction from matrix multiplication to computing $tr(ABC)$ for $n \times n$ inputs $A,B,C$. We can then reduce $tr(ABC)$ to each of our problems of interest.
  For the above problems, and many others in linear algebra, their fastest known algorithms require $\Theta(n^\omega)$ time, where $\omega \approx 2.37$ is the current exponent of fast matrix multiplication. Our finding shows that any improvements beyond this barrier would lead to faster quantum algorithms for matrix multiplication. Our results complement existing reductions from matrix multiplication in algebraic circuits [BCS13], and reductions that work for standard classical algorithms, but are not tight -- i.e., which roughly show that an $O(n^{3-\delta})$ time algorithm for the problem yields an $O(n^{3-\delta/3})$ matrix multiplication algorithm [WW10].</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19528v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Doney, Cameron Musco</dc:creator>
    </item>
    <item>
      <title>Dequantization and Hardness of Spectral Sum Estimation</title>
      <link>https://arxiv.org/abs/2509.20183</link>
      <description>arXiv:2509.20183v1 Announce Type: cross 
Abstract: We give new dequantization and hardness results for estimating spectral sums of matrices, such as the log-determinant. Recent quantum algorithms have demonstrated that the logarithm of the determinant of sparse, well-conditioned, positive matrices can be approximated to $\varepsilon$-relative accuracy in time polylogarithmic in the dimension $N$, specifically in time $\mathrm{poly}(\mathrm{log}(N), s, \kappa, 1/\varepsilon)$, where $s$ is the sparsity and $\kappa$ the condition number of the input matrix. We provide a simple dequantization of these techniques that preserves the polylogarithmic dependence on the dimension. Our classical algorithm runs in time $\mathrm{polylog}(N)\cdot s^{O(\sqrt{\kappa}\log \kappa/\varepsilon)}$ which constitutes an exponential improvement over previous classical algorithms in certain parameter regimes.
  We complement our classical upper bound with $\mathsf{DQC1}$-completeness results for estimating specific spectral sums such as the trace of the inverse and the trace of matrix powers for log-local Hamiltonians, with parameter scalings analogous to those of known quantum algorithms. Assuming $\mathsf{BPP}\subsetneq\mathsf{DQC1}$, this rules out classical algorithms with the same scalings. It also resolves a main open problem of Cade and Montanaro (TQC 2018) concerning the complexity of Schatten-$p$ norm estimation. We further analyze a block-encoding input model, where instead of a classical description of a sparse matrix, we are given a block-encoding of it. We show $\mathsf{DQC}1$-completeness in a very general way in this model for estimating $\mathrm{tr}[f(A)]$ whenever $f$ and $f^{-1}$ are sufficiently smooth.
  We conclude our work with $\mathsf{BQP}$-hardness and $\mathsf{PP}$-completeness results for high-accuracy log-determinant estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20183v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Edenhofer, Atsuya Hasegawa, Fran\c{c}ois Le Gall</dc:creator>
    </item>
    <item>
      <title>Nonlocal Games and Self-tests in the Presence of Noise</title>
      <link>https://arxiv.org/abs/2509.20350</link>
      <description>arXiv:2509.20350v1 Announce Type: cross 
Abstract: Self-testing is a key characteristic of certain nonlocal games, which allow one to uniquely determine the underlying quantum state and measurement operators used by the players, based solely on their observed input-output correlations [MY04]. Motivated by the limitations of current quantum devices, we study self-testing in the high-noise regime, where the two players are restricted to sharing many copies of a noisy entangled state with an arbitrary constant noise rate. In this setting, many existing self-tests fail to certify any nontrivial structure. We first characterize the maximal winning probabilities of the CHSH game [CHSH69], the Magic Square game [Mer90a], and the 2-out-of-n CHSH game [CRSV18] as functions of the noise rate, under the assumption that players use traceless observables. These results enable the construction of device-independent protocols for estimating the noise rate. Building on this analysis, we show that these three games--together with an additional test enforcing the tracelessness of binary observables--can self-test one, two, and n pairs of anticommuting Pauli operators, respectively. These are the first known self-tests that are robust in the high-noise regime and remain sound even when the players' measurements are noisy. Our proofs rely on Sum-of-Squares (SoS) decompositions and Pauli analysis techniques developed in the contexts of quantum proof systems and quantum learning theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20350v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Honghao Fu, Minglong Qin, Haochen Xu, Penghui Yao</dc:creator>
    </item>
    <item>
      <title>Differentially Private Compression and the Sensitivity of LZ77</title>
      <link>https://arxiv.org/abs/2502.09584</link>
      <description>arXiv:2502.09584v3 Announce Type: replace 
Abstract: We initiate the study of differentially private data-compression schemes motivated by the insecurity of the popular "Compress-Then-Encrypt" framework. Data compression is a useful tool which exploits redundancy in data to reduce storage/bandwidth when files are stored or transmitted. However, if the contents of a file are confidential then the length of a compressed file might leak confidential information about the content of the file itself. Encrypting a compressed file does not eliminate this leakage as data encryption schemes are only designed to hide the content of confidential message instead of the length of the message. In our proposed Differentially Private Compress-Then-Encrypt framework, we add a random positive amount of padding to the compressed file to ensure that any leakage satisfies the rigorous privacy guarantee of $(\epsilon,\delta)$-differential privacy. The amount of padding that needs to be added depends on the sensitivity of the compression scheme to small changes in the input, i.e., to what degree can changing a single character of the input message impact the length of the compressed file. While some popular compression schemes are highly sensitive to small changes in the input, we argue that effective data compression schemes do not necessarily have high sensitivity. Our primary technical contribution is analyzing the fine-grained sensitivity of the LZ77 compression scheme (IEEE Trans. Inf. Theory 1977) which is one of the most common compression schemes used in practice. We show that the global sensitivity of the LZ77 compression scheme has the upper bound $O(W^{2/3}\log n)$ where $W\leq n$ denotes the size of the sliding window. When $W=n$, we show the lower bound $\Omega(n^{2/3}\log^{1/3}n)$ for the global sensitivity of the LZ77 compression scheme which is tight up to a sublogarithmic factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09584v3</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremiah Blocki, Seunghoon Lee, Brayan Sebasti\'an Yepes Garcia</dc:creator>
    </item>
    <item>
      <title>Lines in Every Direction with No ee-Random Points</title>
      <link>https://arxiv.org/abs/2507.05475</link>
      <description>arXiv:2507.05475v2 Announce Type: replace 
Abstract: We prove that in every direction in the Euclidean plane, there exists a line containing no double exponential time random (ee-random) points. This means each point on these lines has an algorithmically predictable location, to the extent that a gambler in an environment with fair payouts can, using double exponential time computing resources, amass unbounded capital placing bets on increasingly precise estimates of the point's location. Our proof relies on effectivizing the construction of the lineal extension of a Kakeya set. This resolves an open question of Lutz and Lutz (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05475v2</guid>
      <category>cs.CC</category>
      <category>math.PR</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Lutz, Spencer Park Martin, Rain White</dc:creator>
    </item>
    <item>
      <title>Smaller Circuits for Bit Addition</title>
      <link>https://arxiv.org/abs/2509.13966</link>
      <description>arXiv:2509.13966v2 Announce Type: replace 
Abstract: Bit addition arises virtually everywhere in digital circuits: arithmetic operations, increment/decrement operators, computing addresses and table indices, and so on. Since bit addition is such a basic task in Boolean circuit synthesis, a lot of research has been done on constructing efficient circuits for various special cases of it. A vast majority of these results are devoted to optimizing the circuit depth (also known as delay).
  In this paper, we investigate the circuit size (also known as area) over the full binary basis of bit addition. Though most of the known circuits are built from Half Adders and Full Adders, we show that, in many interesting scenarios, these circuits have suboptimal size. Namely, we improve an upper bound $5n-3m$ to $4.5n-2m$, where $n$ is the number of input bits and $m$ is the number of output bits. In the regimes where $m$ is small compared to $n$ (for example, for computing the sum of $n$ bits or multiplying two $n$-bit integers), this leads to $10\%$ improvement.
  We complement our theoretical result by an open-source implementation of generators producing circuits for bit addition and multiplication. The generators allow one to produce the corresponding circuits in two lines of code and to compare them to existing designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13966v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Goncharov, Alexander S. Kulikov, Georgie Levtsov</dc:creator>
    </item>
    <item>
      <title>Supersimulators</title>
      <link>https://arxiv.org/abs/2509.17994</link>
      <description>arXiv:2509.17994v2 Announce Type: replace 
Abstract: We prove that every randomized Boolean function admits a supersimulator: a randomized polynomial-size circuit whose output on random inputs cannot be efficiently distinguished from reality with constant advantage, even by polynomially larger distinguishers. Our result builds on the landmark complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009), which, in contrast, provides a simulator that fools smaller distinguishers. We circumvent lower bounds for the simulator size by letting the distinguisher size bound vary with the target function, while remaining below an absolute upper bound independent of the target function. This dependence on the target function arises naturally from our use of an iteration technique originating in the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements thereof, known as multiaccurate and multicalibrated predictors, respectively, as per Hebert-Johnson et al. (2018), have previously been shown to have myriad applications in complexity theory, cryptography, learning theory, and beyond. We first show that a recent multicalibration-based characterization of the computational indistinguishability of product distributions actually requires only (calibrated) multiaccuracy. We then show that supersimulators yield an even tighter result in this application domain, closing a complexity gap present in prior versions of the characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17994v2</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Pranay Tankala</dc:creator>
    </item>
    <item>
      <title>Segmented Operations using Matrix Multiplications</title>
      <link>https://arxiv.org/abs/2506.23906</link>
      <description>arXiv:2506.23906v2 Announce Type: replace-cross 
Abstract: Specialized computational units that perform small matrix multiplications as primitive operations are typically present in modern AI accelerators. However, these Matrix Multiplication Units (MMUs) are often underutilized for many fundamental deep learning operations besides dense matrix multiplications. Coincidentally, the lack of a rigorous theoretical model of computation for such architectures obstructs algorithmic design. In this work, we propose MMV-RAM, a computational model which judiciously extends the Vector-RAM model with an additional MMU. We provide a detailed theoretical analysis and carefully balance the computational power between the matrix and vector units, guided by the circuit complexity lower bound that parity is not in AC{[0]}. Given MMV-RAM, we proceed to algorithm design, starting with two fundamental parallel operations: segmented scan and sum. By expressing them as compositions of elementary parallel primitives (e.g., seg. sum reduces to: scan, compress, and vector differentiation), we can exploit MMUs to perform speculative blocked computations, ultimately leading to provable theoretical speed-ups against vector-only approaches. These results extend to other ubiquitous AI kernels, including dense matrix product, and sparse matrix-vector product. As a case study, we implemented the proposed algorithms on the Ascend 910B AI accelerator, which contains matrix and vector cores. We evaluate these implementations on synthetic and real-world datasets from various applications, including Large Language Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23906v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk, Giuseppe Sorrentino, Anastasios Zouzias</dc:creator>
    </item>
    <item>
      <title>Efficiently learning depth-3 circuits via quantum agnostic boosting</title>
      <link>https://arxiv.org/abs/2509.14461</link>
      <description>arXiv:2509.14461v2 Announce Type: replace-cross 
Abstract: We initiate the study of quantum agnostic learning of phase states with respect to a function class $\mathsf{C}\subseteq \{c:\{0,1\}^n\rightarrow \{0,1\}\}$: given copies of an unknown $n$-qubit state $|\psi\rangle$ which has fidelity $\textsf{opt}$ with a phase state $|\phi_c\rangle=\frac{1}{\sqrt{2^n}}\sum_{x\in \{0,1\}^n}(-1)^{c(x)}|x\rangle$ for some $c\in \mathsf{C}$, output $|\phi\rangle$ which has fidelity $|\langle \phi | \psi \rangle|^2 \geq \textsf{opt}-\varepsilon$. To this end, we give agnostic learning protocols for the following classes: (i) Size-$t$ decision trees which runs in time $\textsf{poly}(n,t,1/\varepsilon)$. This also implies $k$-juntas can be agnostically learned in time $\textsf{poly}(n,2^k,1/\varepsilon)$. (ii) $s$-term DNF formulas in time $\textsf{poly}(n,(s/\varepsilon)^{\log \log (s/\varepsilon) \cdot \log(1/\varepsilon)})$.
  Our main technical contribution is a quantum agnostic boosting protocol which converts a weak agnostic learner, which outputs a parity state $|\phi\rangle$ such that $|\langle \phi|\psi\rangle|^2\geq \textsf{opt}/\textsf{poly}(n)$, into a strong learner which outputs a superposition of parity states $|\phi'\rangle$ such that $|\langle \phi'|\psi\rangle|^2\geq \textsf{opt} - \varepsilon$.
  Using quantum agnostic boosting, we obtain a $n^{O(\log \log n \cdot \log(1/\varepsilon))}$-time algorithm for learning $\textsf{poly}(n)$-sized depth-$3$ circuits (consisting of $\textsf{AND}$, $\textsf{OR}$, $\textsf{NOT}$ gates) in the uniform $\textsf{PAC}$ model given quantum examples, which is near-polynomial time for constant $\varepsilon$. Classically, obtaining an algorithm with a similar complexity has been an open question in the $\textsf{PAC}$ model and our work answers this given quantum examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14461v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srinivasan Arunachalam, Arkopal Dutt, Alexandru Gheorghiu, Michael de Oliveira</dc:creator>
    </item>
  </channel>
</rss>
