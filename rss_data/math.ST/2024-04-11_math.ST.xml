<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From Poisson Observations to Fitted Negative Binomial Distribution</title>
      <link>https://arxiv.org/abs/2404.07457</link>
      <description>arXiv:2404.07457v1 Announce Type: new 
Abstract: The Kolmogorov-Smirnov (KS) test has been widely used for testing whether a random sample comes from a specific distribution, possibly with estimated parameters. If the data come from a Poisson distribution, however, one can hardly tell that they do not come from a negative binomial distribution by running a KS test, even with a large sample size. In this paper, we rigorously justify that the KS test statistic converges to zero almost surely, as the sample size goes to infinity. To prove this result, we demonstrate a notable finding that in this case the maximum likelihood estimates (MLE) for the parameters of the negative binomial distribution converge to infinity and one, respectively and almost surely. Our result highlights a potential limitation of the KS test, as well as other tests based on empirical distribution functions (EDF), in efficiently identifying the true underlying distribution. Our findings and justifications also underscore the importance of careful interpretation and further investigation when identifying the most appropriate distributions in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07457v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Niloufar Dousti Mousavi, Zhou Yu, Jie Yang</dc:creator>
    </item>
    <item>
      <title>A Geometrical Analysis of Kernel Ridge Regression and its Applications</title>
      <link>https://arxiv.org/abs/2404.07709</link>
      <description>arXiv:2404.07709v1 Announce Type: new 
Abstract: We obtain upper bounds for the estimation error of Kernel Ridge Regression (KRR) for all non-negative regularization parameters, offering a geometric perspective on various phenomena in KRR. As applications: 1. We address the multiple descent problem, unifying the proofs of arxiv:1908.10292 and arxiv:1904.12191 for polynomial kernels and we establish multiple descent for the upper bound of estimation error of KRR under sub-Gaussian design and non-asymptotic regimes. 2. For a sub-Gaussian design vector and for non-asymptotic scenario, we prove the Gaussian Equivalent Conjecture. 3. We offer a novel perspective on the linearization of kernel matrices of non-linear kernel, extending it to the power regime for polynomial kernels. 4. Our theory is applicable to data-dependent kernels, providing a convenient and accurate tool for the feature learning regime in deep learning theory. 5. Our theory extends the results in arxiv:2009.14286 under weak moment assumption.
  Our proof is based on three mathematical tools developed in this paper that can be of independent interest: 1. Dvoretzky-Milman theorem for ellipsoids under (very) weak moment assumptions. 2. Restricted Isomorphic Property in Reproducing Kernel Hilbert Spaces with embedding index conditions. 3. A concentration inequality for finite-degree polynomial kernel functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07709v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Gavrilopoulos, Guillaume Lecu\'e, Zong Shang</dc:creator>
    </item>
    <item>
      <title>Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method</title>
      <link>https://arxiv.org/abs/2404.07937</link>
      <description>arXiv:2404.07937v1 Announce Type: new 
Abstract: We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07937v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charis Stamouli, Ingvar Ziemann, George J. Pappas</dc:creator>
    </item>
    <item>
      <title>Surrogate modeling for probability distribution estimation:uniform or adaptive design?</title>
      <link>https://arxiv.org/abs/2404.07323</link>
      <description>arXiv:2404.07323v1 Announce Type: cross 
Abstract: The active learning (AL) technique, one of the state-of-the-art methods for constructing surrogate models, has shown high accuracy and efficiency in forward uncertainty quantification (UQ) analysis. This paper provides a comprehensive study on AL-based global surrogates for computing the full distribution function, i.e., the cumulative distribution function (CDF) and the complementary CDF (CCDF). To this end, we investigate the three essential components for building surrogates, i.e., types of surrogate models, enrichment methods for experimental designs, and stopping criteria. For each component, we choose several representative methods and study their desirable configurations. In addition, we devise a uniform design (i.e., space-filling design) as a baseline for measuring the improvement of using AL. Combining all the representative methods, a total of 1,920 UQ analyses are carried out to solve 16 benchmark examples. The performance of the selected strategies is evaluated based on accuracy and efficiency. In the context of full distribution estimation, this study concludes that (i) AL techniques cannot provide a systematic improvement compared with uniform designs, (ii) the recommended surrogate modeling methods depend on the features of the problems (especially the local nonlinearity), target accuracy, and computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07323v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maijia Su, Ziqi Wang, Oreste Salvatore Bursi, Marco Broccardo</dc:creator>
    </item>
    <item>
      <title>Statistical Independence and the Brockwell Transform -- From an Integral Equation Perspective</title>
      <link>https://arxiv.org/abs/2404.07558</link>
      <description>arXiv:2404.07558v1 Announce Type: cross 
Abstract: Statistical independence is a notion ubiquitous in various fields such as in statistics, probability, number theory and physics. We establish the stability of independence for any pair of random variables by their corresponding Brockwell transforms (Brockwell, 2007) beyond the non-atomic condition that is naturally imposed on their distributions, thereby generalizing the proposition originated by Cai et al. (2022). A central novelty in our work is to formulate the problem as a possibly new type of mathematical inverse problem, which aims to claim that an integral equation has a unique solution in terms of its stochastic kernel -- not under-determined contrary to the usual cases, and also to design a recursive constructive scheme, combined with the property of the quantile function, that solves the aforementioned integral equation in an iterative manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07558v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingzhi Wang</dc:creator>
    </item>
    <item>
      <title>An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization</title>
      <link>https://arxiv.org/abs/2404.07771</link>
      <description>arXiv:2404.07771v1 Announce Type: cross 
Abstract: Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07771v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minshuo Chen, Song Mei, Jianqing Fan, Mengdi Wang</dc:creator>
    </item>
    <item>
      <title>Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2404.07864</link>
      <description>arXiv:2404.07864v1 Announce Type: cross 
Abstract: We consider the problem of localizing change points in high-dimensional linear regression. We propose an Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations. Assuming Gaussian covariates, we give an exact asymptotic characterization of its estimation performance in the limit where the number of samples grows proportionally to the signal dimension. Our algorithm can be tailored to exploit any prior information on the signal, noise, and change points. It also enables uncertainty quantification in the form of an efficiently computable approximate posterior distribution, whose asymptotic form we characterize exactly. We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic data and images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07864v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Arpino, Xiaoqi Liu, Ramji Venkataramanan</dc:creator>
    </item>
    <item>
      <title>Triple Component Matrix Factorization: Untangling Global, Local, and Noisy Components</title>
      <link>https://arxiv.org/abs/2404.07955</link>
      <description>arXiv:2404.07955v1 Announce Type: cross 
Abstract: In this work, we study the problem of common and unique feature extraction from noisy data. When we have N observation matrices from N different and associated sources corrupted by sparse and potentially gross noise, can we recover the common and unique components from these noisy observations? This is a challenging task as the number of parameters to estimate is approximately thrice the number of observations. Despite the difficulty, we propose an intuitive alternating minimization algorithm called triple component matrix factorization (TCMF) to recover the three components exactly. TCMF is distinguished from existing works in literature thanks to two salient features. First, TCMF is a principled method to separate the three components given noisy observations provably. Second, the bulk of the computation in TCMF can be distributed. On the technical side, we formulate the problem as a constrained nonconvex nonsmooth optimization problem. Despite the intricate nature of the problem, we provide a Taylor series characterization of its solution by solving the corresponding Karush-Kuhn-Tucker conditions. Using this characterization, we can show that the alternating minimization algorithm makes significant progress at each iteration and converges into the ground truth at a linear rate. Numerical experiments in video segmentation and anomaly detection highlight the superior feature extraction abilities of TCMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07955v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naichen Shi, Salar Fattahi, Raed Al Kontar</dc:creator>
    </item>
    <item>
      <title>From naive trees to Random Forests: A general approach for proving consistency of tree-based methods</title>
      <link>https://arxiv.org/abs/2404.06850</link>
      <description>arXiv:2404.06850v2 Announce Type: replace 
Abstract: Tree-based methods such as Random Forests are learning algorithms that have become an integral part of the statistical toolbox. The last decade has shed some light on theoretical properties such as their consistency for regression tasks. However, the usual proofs assume normal error terms as well as an additive regression function and are rather technical. We overcome these issues by introducing a simple and catchy technique for proving consistency under quite general assumptions. To this end, we introduce a new class of naive trees, which do the subspacing completely at random and independent of the data. We then give a direct proof of their consistency. Using them to bound the error of more complex tree-based approaches such as univariate and multivariate CARTs, Extra Randomized Trees, or Random Forests, we deduce the consistency of all of them. Since naive trees appear to be too simple for actual application, we further analyze their finite sample properties in a simulation and small benchmark study. We find a slow convergence speed and a rather poor predictive performance. Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06850v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Markus Pauly, Lena Schmid, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>Debiased Inverse Propensity Score Weighting for Estimation of Average Treatment Effects with High-Dimensional Confounders</title>
      <link>https://arxiv.org/abs/2011.08661</link>
      <description>arXiv:2011.08661v3 Announce Type: replace-cross 
Abstract: We consider estimation of average treatment effects given observational data with high-dimensional pretreatment variables. Existing methods for this problem typically assume some form of sparsity for the regression functions. In this work, we introduce a debiased inverse propensity score weighting (DIPW) scheme for average treatment effect estimation that delivers $\sqrt{n}$-consistent estimates when the propensity score follows a sparse logistic regression model; the outcome regression functions are permitted to be arbitrarily complex. We further demonstrate how confidence intervals centred on our estimates may be constructed. Our theoretical results quantify the price to pay for permitting the regression functions to be unestimable, which shows up as an inflation of the variance of the estimator compared to the semiparametric efficient variance by a constant factor, under mild conditions. We also show that when outcome regressions can be estimated faster than a slow $1/\sqrt{ \log n}$ rate, our estimator achieves semiparametric efficiency. As our results accommodate arbitrary outcome regression functions, averages of transformed responses under each treatment may also be estimated at the $\sqrt{n}$ rate. Thus, for example, the variances of the potential outcomes may be estimated. We discuss extensions to estimating linear projections of the heterogeneous treatment effect function and explain how propensity score models with more general link functions may be handled within our framework. An R package \texttt{dipw} implementing our methodology is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.08661v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Assessing quality of selection procedures: Lower bound of false positive rate as a function of inter-rater reliability</title>
      <link>https://arxiv.org/abs/2207.09101</link>
      <description>arXiv:2207.09101v3 Announce Type: replace-cross 
Abstract: Inter-rater reliability (IRR) is one of the commonly used tools for assessing the quality of ratings from multiple raters. However, applicant selection procedures based on ratings from multiple raters usually result in a binary outcome; the applicant is either selected or not. This final outcome is not considered in IRR, which instead focuses on the ratings of the individual subjects or objects. We outline the connection between the ratings' measurement model (used for IRR) and a binary classification framework. We develop a simple way of approximating the probability of correctly selecting the best applicants which allows us to compute error probabilities of the selection procedure (i.e., false positive and false negative rate) or their lower bounds. We draw connections between the inter-rater reliability and the binary classification metrics, showing that binary classification metrics depend solely on the IRR coefficient and proportion of selected applicants. We assess the performance of the approximation in a simulation study and apply it in an example comparing the reliability of multiple grant peer review selection procedures. We also discuss possible other uses of the explored connections in other contexts, such as educational testing, psychological assessment, and health-related measurement and implement the computations in IRR2FPR R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.09101v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Patr\'icia Martinkov\'a</dc:creator>
    </item>
    <item>
      <title>Signature-based validation of real-world economic scenarios</title>
      <link>https://arxiv.org/abs/2208.07251</link>
      <description>arXiv:2208.07251v3 Announce Type: replace-cross 
Abstract: Motivated by insurance applications, we propose a new approach for the validation of real-world economic scenarios. This approach is based on the statistical test developed by Chevyrev and Oberhauser (2022) and relies on the notions of signature and maximum mean distance. This test allows to check whether two samples of stochastic processes paths come from the same distribution. Our contribution is to apply this test to a variety of stochastic processes exhibiting different pathwise properties (H{\"o}lder regularity, autocorrelation, regime switches) and which are relevant for the modelling of stock prices and stock volatility as well as of inflation in view of actuarial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07251v3</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herv\'e Andr\`es (CERMICS), Alexandre Boumezoued (CERMICS, MATHRISK), Benjamin Jourdain (CERMICS, MATHRISK)</dc:creator>
    </item>
    <item>
      <title>A central limit theorem for a sequence of conditionally centered and $\alpha$-mixing random fields</title>
      <link>https://arxiv.org/abs/2301.08942</link>
      <description>arXiv:2301.08942v3 Announce Type: replace-cross 
Abstract: A central limit theorem is established for a sum of random variables belonging to a sequence of random fields. The fields are assumed to have zero mean conditional on the past history and to satisfy certain conditional $\alpha$-mixing conditions in space or time. The limiting normal distribution is obtained for increasing spatial domain or increasing length of the sequence. The applicability of the theorem is demonstrated by examples regarding estimating functions for a space-time point process and a space-time Markov process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08942v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdollah Jalilian, Ganggang Xu, Arnaud Poinas, Rasmus Waagepetersen</dc:creator>
    </item>
    <item>
      <title>Spectral clustering in the Gaussian mixture block model</title>
      <link>https://arxiv.org/abs/2305.00979</link>
      <description>arXiv:2305.00979v3 Announce Type: replace-cross 
Abstract: Gaussian mixture block models are distributions over graphs that strive to model modern networks: to generate a graph from such a model, we associate each vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$ for a pre-specified threshold $\tau$. The different components of the Gaussian mixture represent the fact that there may be different types of nodes with different distributions over features -- for example, in a social network each component represents the different attributes of a distinct community. Natural algorithmic tasks associated with these networks are embedding (recovering the latent feature vectors) and clustering (grouping nodes by their mixture component).
  In this paper we initiate the study of clustering and embedding graphs sampled from high-dimensional Gaussian mixture block models, where the dimension of the latent feature vectors $d\to \infty$ as the size of the network $n \to \infty$. This high-dimensional setting is most appropriate in the context of modern networks, in which we think of the latent feature space as being high-dimensional. We analyze the performance of canonical spectral clustering and embedding algorithms for such graphs in the case of 2-component spherical Gaussian mixtures, and begin to sketch out the information-computation landscape for clustering and embedding in these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00979v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangping Li, Tselil Schramm</dc:creator>
    </item>
    <item>
      <title>Debiasing Welch's Method for Spectral Density Estimation</title>
      <link>https://arxiv.org/abs/2312.13643</link>
      <description>arXiv:2312.13643v2 Announce Type: replace-cross 
Abstract: Welch's method provides an estimator of the power spectral density that is statistically consistent. This is achieved by averaging over periodograms calculated from overlapping segments of a time series. For a finite length time series, while the variance of the estimator decreases as the number of segments increase, the magnitude of the estimator's bias increases: a bias-variance trade-off ensues when setting the segment number. We address this issue by providing a novel method for debiasing Welch's method which maintains the computational complexity and asymptotic consistency, and leads to improved finite-sample performance. Theoretical results are given for fourth-order stationary processes with finite fourth-order moments and absolutely convergent fourth-order cumulant function. The significant bias reduction is demonstrated with numerical simulation and an application to real-world data. Our estimator also permits irregular spacing over frequency and we demonstrate how this may be employed for signal compression and further variance reduction. Code accompanying this work is available in R and python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13643v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lachlan C. Astfalck, Adam M. Sykulski, Edward J. Cripps</dc:creator>
    </item>
    <item>
      <title>Correlation functions between singular values and eigenvalues</title>
      <link>https://arxiv.org/abs/2403.19157</link>
      <description>arXiv:2403.19157v2 Announce Type: replace-cross 
Abstract: Exploiting the explicit bijection between the density of singular values and the density of eigenvalues for bi-unitarily invariant complex random matrix ensembles of finite matrix size we aim at finding the induced probability measure on $j$ eigenvalues and $k$ singular values that we coin $j,k$-point correlation measure. We fully derive all $j,k$-point correlation measures in the simplest cases for matrices of size $n=1$ and $n=2$. For $n&gt;2$, we find a general formula for the $1,1$-point correlation measure. This formula reduces drastically when assuming the singular values are drawn from a polynomial ensemble, yielding an explicit formula in terms of the kernel corresponding to the singular value statistics. These expressions simplify even further when the singular values are drawn from a P\'{o}lya ensemble and extend known results between the eigenvalue and singular value statistics of the corresponding bi-unitarily invariant ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19157v2</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Allard, Mario Kieburg</dc:creator>
    </item>
  </channel>
</rss>
