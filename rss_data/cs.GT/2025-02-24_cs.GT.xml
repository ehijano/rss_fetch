<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>BundleFlow: Deep Menus for Combinatorial Auctions by Diffusion-Based Optimization</title>
      <link>https://arxiv.org/abs/2502.15283</link>
      <description>arXiv:2502.15283v1 Announce Type: new 
Abstract: Differentiable economics -- the use of deep learning for auction design -- has driven progress in the automated design of multi-item auctions with additive or unit-demand valuations. However, little progress has been made for optimal combinatorial auctions (CAs), even for the single bidder case, because we need to overcome the challenge of the bundle space growing exponentially with the number of items. For example, when learning a menu of allocation-price choices for a bidder in a CA, each menu element needs to efficiently and flexibly specify a probability distribution on bundles. In this paper, we solve this problem in the single-bidder CA setting by generating a bundle distribution through an ordinary differential equation (ODE) applied to a tractable initial distribution, drawing inspiration from generative models, especially score-based diffusion models and continuous normalizing flow. Our method, BundleFlow, uses deep learning to find suitable ODE-based transforms of initial distributions, one transform for each menu element, so that the overall menu achieves high expected revenue. Our method achieves 1.11$-$2.23$\times$ higher revenue compared with automated mechanism design baselines on the single-bidder version of CATS, a standard CA testbed, and scales to problems with up to 150 items. Relative to a baseline that also learns allocations in menu elements, our method reduces the training iterations by 3.6$-$9.5$\times$ and cuts training time by about 80% in settings with 50 and 100 items.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15283v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tonghan Wang, Yanchen Jiang, David C. Parkes</dc:creator>
    </item>
    <item>
      <title>Shapley Value-based Approach for Redistributing Revenue of Matchmaking of Private Transactions in Blockchains</title>
      <link>https://arxiv.org/abs/2502.15420</link>
      <description>arXiv:2502.15420v1 Announce Type: new 
Abstract: In the context of blockchain, MEV refers to the maximum value that can be extracted from block production through the inclusion, exclusion, or reordering of transactions. Searchers often participate in order flow auctions (OFAs) to obtain exclusive rights to private transactions, available through entities called matchmakers, also known as order flow providers (OFPs). Most often, redistributing the revenue generated through such auctions among transaction creators is desirable. In this work, we formally introduce the matchmaking problem in MEV, its desirable properties, and associated challenges. Using cooperative game theory, we formalize the notion of fair revenue redistribution in matchmaking and present its potential possibilities and impossibilities. Precisely, we define a characteristic form game, referred to as RST-Game, for the transaction creators. We propose to redistribute the revenue using the Shapley value of RST-Game. We show that the corresponding problem could be SUBEXP (i.e. $2^{o(n)}$, where $n$ is the number of transactions); therefore, approximating the Shapley value is necessary. Further, we propose a randomized algorithm for computing the Shapley value in RST-Game and empirically verify its efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15420v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Rasheed, Parth Desai, Yash Chaurasia, Sujit Gujar</dc:creator>
    </item>
    <item>
      <title>Contract DesignUnderApproximate Best Responses</title>
      <link>https://arxiv.org/abs/2502.15523</link>
      <description>arXiv:2502.15523v1 Announce Type: new 
Abstract: Principal-agent problems model scenarios where a principal incentivizes an agent to take costly, unobservable actions through the provision of payments. Such problems are ubiquitous in several real-world applications, ranging from blockchain to the delegation of machine learning tasks. In this paper, we initiate the study of hidden-action principal-agent problems under approximate best responses, in which the agent may select any action that is not too much suboptimal given the principal's payment scheme (a.k.a. contract). Our main result is a polynomial-time algorithm to compute an optimal contract under approximate best responses. This positive result is perhaps surprising, since, in Stackelberg games, computing an optimal commitment under approximate best responses is computationally intractable. We also investigate the learnability of contracts under approximate best responses, by providing a no-regret learning algorithm for a natural application scenario where the principal has no prior knowledge about the environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15523v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Bacchiocchi, Jiarui Gan, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</dc:creator>
    </item>
    <item>
      <title>Safe Pareto Improvements for Expected Utility Maximizers in Program Games</title>
      <link>https://arxiv.org/abs/2403.05103</link>
      <description>arXiv:2403.05103v5 Announce Type: replace 
Abstract: Agents in mixed-motive coordination problems such as Chicken may fail to coordinate on a Pareto-efficient outcome. Safe Pareto improvements (SPIs) were originally proposed to mitigate miscoordination in cases where players lack probabilistic beliefs as to how their delegates will play a game; delegates are instructed to behave so as to guarantee a Pareto improvement on how they would play by default. More generally, SPIs may be defined as transformations of strategy profiles such that all players are necessarily better off under the transformed profile. In this work, we investigate the extent to which SPIs can reduce downsides of miscoordination between expected utility-maximizing agents. We consider games in which players submit computer programs that can condition their decisions on each other's code, and use this property to construct SPIs using programs capable of renegotiation. We first show that under mild conditions on players' beliefs, each player always prefers to use renegotiation. Next, we show that under similar assumptions, each player always prefers to be willing to renegotiate at least to the point at which they receive the lowest payoff they can attain in any efficient outcome. Thus subjectively optimal play guarantees players at least these payoffs, without the need for coordination on specific Pareto improvements. Lastly, we prove that renegotiation does not guarantee players any improvements on this bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05103v5</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony DiGiovanni, Jesse Clifton, Nicolas Mac\'e</dc:creator>
    </item>
    <item>
      <title>Tractable General Equilibrium</title>
      <link>https://arxiv.org/abs/2502.11449</link>
      <description>arXiv:2502.11449v3 Announce Type: replace 
Abstract: We study Walrasian economies (or general equilibrium models) and their solution concept, the Walrasian equilibrium. A key challenge in this domain is identifying price-adjustment processes that converge to equilibrium. One such process, t\^atonnement, is an auction-like algorithm first proposed in 1874 by L\'eon Walras. While continuous-time variants of t\^atonnement are known to converge to equilibrium in economies satisfying the Weak Axiom of Revealed Preferences (WARP), the process fails to converge in a pathological Walrasian economy known as the Scarf economy. To address these issues, we analyze Walrasian economies using variational inequalities (VIs), an optimization framework. We introduce the class of mirror extragradient algorithms, which, under suitable Lipschitz-continuity-like assumptions, converge to a solution of any VI satisfying the Minty condition in polynomial time. We show that the set of Walrasian equilibria of any balanced economy-which includes among others Arrow-Debreu economies-corresponds to the solution set of an associated VI that satisfies the Minty condition but is generally discontinuous. Applying the mirror extragradient algorithm to this VI we obtain a class of t\^atonnement-like processes, which we call the mirror extrat\^atonnement process. While our VI formulation is generally discontinuous, it is Lipschitz-continuous in variationally stable Walrasian economies with bounded elasticity-including those satisfying WARP and the Scarf economy-thus establishing the polynomial-time convergence of mirror extrat\^atonnement in these economies. We validate our approach through experiments on large Arrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well as the Scarf economy, demonstrating fast convergence in all cases without failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11449v3</guid>
      <category>cs.GT</category>
      <category>cs.CE</category>
      <category>econ.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denizalp Goktas, Amy Greenwald</dc:creator>
    </item>
    <item>
      <title>An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces</title>
      <link>https://arxiv.org/abs/2502.14790</link>
      <description>arXiv:2502.14790v2 Announce Type: replace-cross 
Abstract: We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\mathcal{O}(\beta\sqrt{T\log(1+\lambda)})$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14790v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Terenin, Jeffrey Negrea</dc:creator>
    </item>
  </channel>
</rss>
