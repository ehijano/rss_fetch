<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.CD updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.CD</link>
    <description>nlin.CD updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.CD" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 05:01:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data-Driven Reduced-Complexity Modeling of Fluid Flows: A Community Challenge</title>
      <link>https://arxiv.org/abs/2601.06183</link>
      <description>arXiv:2601.06183v1 Announce Type: cross 
Abstract: We introduce a community challenge designed to facilitate direct comparisons between data-driven methods for compression, forecasting, and sensing of complex aerospace flows. The challenge is organized into three tracks that target these complementary capabilities: compression (compact representations for large datasets), forecasting (predicting future flow states from a finite history), and sensing (inferring unmeasured flow states from limited measurements). Across these tracks, multiple challenges span diverse flow datasets and use cases, each emphasizing different model requirements. The challenge is open to anyone, and we invite broad participation to build a comprehensive and balanced picture of what works and where current methods fall short. To support fair comparisons, we provide standardized success metrics, evaluation tools, and baseline implementations, with one classical and one machine-learning baseline per challenge. Final assessments use blind tests on withheld data. We explicitly encourage negative results and careful analyses of limitations. Outcomes will be disseminated through an AIAA Journal Virtual Collection and invited presentations at AIAA conferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06183v1</guid>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <category>physics.flu-dyn</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver T. Schmidt, Aaron Towne, Adrian Lozano-Duran, Scott T. M. Dawson, Ricardo Vinuesa</dc:creator>
    </item>
    <item>
      <title>Empirical Discovery of Multi-Scale Transfer of Information in Dynamical Systems</title>
      <link>https://arxiv.org/abs/2502.19633</link>
      <description>arXiv:2502.19633v4 Announce Type: replace 
Abstract: In this work, we quantify the time scales and information flow associated with multiscale energy transfer in a weakly turbulent system. This is done through a greedy optimization algorithm which finds the maximum conditional-mutual information across lagged embeddings of time series localized by wavenumber. For our chosen weakly turbulent system, the algorithm finds asymmetries in the information flow across wavenumbers, reflecting what are typically described as forward and inverse cascades. However, our approach goes beyond typical heuristic arguments and provides quantitative insight into the intricate multi-wave mixing dynamics necessary to maintain the steady statistical state characterizing weak turbulence. Our work then provides a novel, detailed, and fully nonlinear statistical analysis of a weakly turbulent system. The flexibility of our approach points to broader applicability in real-world data coming from chaotic or turbulent dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19633v4</guid>
      <category>nlin.CD</category>
      <category>nlin.PS</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher W. Curtis, Erik M. Bollt</dc:creator>
    </item>
    <item>
      <title>Topological Signatures of History-Dependent Invariants in the Lorenz System: Deterministic Probes of Lobe Dynamics</title>
      <link>https://arxiv.org/abs/2512.20390</link>
      <description>arXiv:2512.20390v2 Announce Type: replace 
Abstract: We present a systematic classification of history-dependent dynamical invariants in the Lorenz system, identifying eighteen distinct quantities ($K_1$--$K_{18}$) organized into three regularization classes. These are defined by polynomials vanishing on flow nullclines: $p_{\mathrm{I}} = y - x$, $p_{\mathrm{II}} = y + x(z-\rho)$, and $p_{\mathrm{III}} = xy - \beta z$. Statistical analysis of $N = 4 \times 10^6$ trajectories reveals that Class~III invariants, containing the bilinear product $xy$, function as geometric probes of lobe-switching. In the chaotic regime ($\rho = 28$), Class~III exhibits elevated kurtosis ($\kappa_{\mathrm{III}} \approx 15.8$ vs $\kappa_{\mathrm{I}} \approx 8.2$) and heavy tails, a divergence absent in the pre-chaotic phase ($\rho = 23$). Crucially, the differential response $\Delta S = Q_{\mathrm{III}} - Q_{\mathrm{I}}$ predicts separatrix crossings with a horizon governed by a shifted power law $\Delta t = t_{\min} + C\mathcal{A}^{-n}$. Parametric analysis shows $n$ scales linearly with $\beta$ ($m \approx 0.196$), where $n \approx 1.0$ at $\beta=8/3$ reflects relaxation along the stable manifold. High-precision validation (80-digit arithmetic) confirms conservation to $O(10^{-36})$, establishing Class~III invariants as encoders of the attractor's symbolic dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20390v2</guid>
      <category>nlin.CD</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>B. A. Toledo</dc:creator>
    </item>
    <item>
      <title>Uncovering the Computational Roles of Nonlinearity in Sequence Modeling Using Almost-Linear RNNs</title>
      <link>https://arxiv.org/abs/2506.07919</link>
      <description>arXiv:2506.07919v2 Announce Type: replace-cross 
Abstract: Sequence modeling tasks across domains such as natural language processing, time series forecasting, and control require learning complex input-output mappings. Nonlinear recurrence is theoretically required for universal approximation of sequence-to-sequence functions, yet linear recurrent models often prove surprisingly effective. This raises the question of when nonlinearity is truly required. We present a framework to systematically dissect the functional role of nonlinearity in recurrent networks, identifying when it is computationally necessary and what mechanisms it enables. We address this using Almost Linear Recurrent Neural Networks (AL-RNNs), which allow recurrence nonlinearity to be gradually attenuated and decompose network dynamics into analyzable linear regimes, making computational mechanisms explicit. We illustrate the framework across diverse synthetic and real-world tasks, including classic sequence modeling benchmarks, a neuroscientific stimulus-selection task, and a multi-task suite. We demonstrate how the AL-RNN's piecewise linear structure enables identification of computational primitives such as gating, rule-based integration, and memory-dependent transients, revealing that these operations emerge within predominantly linear backbones. Across tasks, sparse nonlinearity improves interpretability by reducing and localizing nonlinear computations, promotes shared representations in multi-task settings, and reduces computational cost. Moreover, sparse nonlinearity acts as a useful inductive bias: in low-data regimes or when tasks require discrete switching between linear regimes, sparsely nonlinear models often match or exceed fully nonlinear architectures. Our findings provide a principled approach for identifying where nonlinearity is functionally necessary, guiding the design of recurrent architectures that balance performance, efficiency, and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07919v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Brenner, Georgia Koppe</dc:creator>
    </item>
    <item>
      <title>DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing</title>
      <link>https://arxiv.org/abs/2508.13490</link>
      <description>arXiv:2508.13490v2 Announce Type: replace-cross 
Abstract: A primary challenge in using neural networks to approximate nonlinear dynamical systems governed by partial differential equations (PDEs) is transforming these systems into a suitable format, especially when dealing with non-linearizable dynamics or the need for infinite-dimensional spaces for linearization. This paper introduces DyMixOp, a novel neural operator framework for PDEs that integrates insights from complex dynamical systems to address this challenge. Grounded in inertial manifold theory, DyMixOp transforms infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent space, establishing a structured foundation that maintains essential nonlinear interactions and enhances physical interpretability. A key innovation is the Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in turbulence. This transformation effectively captures both fine-scale details and nonlinear interactions, while mitigating spectral bias commonly found in existing neural operators. The framework is further strengthened by a dynamics-informed architecture that connects multiple LGM layers to approximate linear and nonlinear dynamics, reflecting the temporal evolution of dynamical systems. Experimental results across diverse PDE benchmarks demonstrate that DyMixOp achieves state-of-the-art performance, significantly reducing prediction errors, particularly in convection-dominated scenarios reaching up to 86.7\%, while maintaining computational efficiency and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13490v2</guid>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengyu Lai, Yixiao Chen, Hui Xu</dc:creator>
    </item>
  </channel>
</rss>
