<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LSTM-Based Proactive Congestion Management for Internet of Vehicle Networks</title>
      <link>https://arxiv.org/abs/2410.09656</link>
      <description>arXiv:2410.09656v1 Announce Type: new 
Abstract: Vehicle-to-everything (V2X) networks support a variety of safety, entertainment, and commercial applications. This is realized by applying the principles of the Internet of Vehicles (IoV) to facilitate connectivity among vehicles and between vehicles and roadside units (RSUs). Network congestion management is essential for IoVs and it represents a significant concern due to its impact on improving the efficiency of transportation systems and providing reliable communication among vehicles for the timely delivery of safety-critical packets. This paper introduces a framework for proactive congestion management for IoV networks. We generate congestion scenarios and a data set to predict the congestion using LSTM. We present the framework and the packet congestion dataset. Simulation results using SUMO with NS3 demonstrate the effectiveness of the framework for forecasting IoV network congestion and clustering/prioritizing packets employing recurrent neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09656v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aly Sabri Abdalla, Ahmad Al-Kabbany, Ehab F. Badran, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>INA-Infra: An Open and Extensible Infrastructure for Intent-driven Network Automation Research</title>
      <link>https://arxiv.org/abs/2410.09765</link>
      <description>arXiv:2410.09765v1 Announce Type: new 
Abstract: As telecommunications systems progress to support diverse use cases with heterogeneous and dynamic Quality of Service (QoS) requirements, it becomes an increasingly complex task to automatically manage various resources involved -- from radio, compute, to X-haul network, which are distributed from the edge to the cloud. Intent-driven network automation can play an important role in NextG networks to meet this need. Towards this, we have developed INA-Infra, an open, extensible, and end-to-end 5G/beyond 5G network infrastructure with intent-driven network automation and end-to-end network slicing capability. INA-Infra is designed using open-source components and is based on O-RAN architecture. INA-Infra manages the network infrastructure, various resources, and (virtualized / containerized) network functions using Nephio -- a cloud-native intent automation solution. It also incorporates intent-driven intelligent control using a Resource Management rApp and a Network Slicing xApp. We demonstrate that INA-Infra can manage the 5G network in a highly automatic and optimized manner, allowing the mobile network operators to focus on specifying the intents of different traffic classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09765v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen-Bao-Long Tran, Tuan V. Ngo, Mao V. Ngo, Binbin Chen, Jihong Park, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Lessons Learned: A Smart Campus Environment Using LoRaWAN</title>
      <link>https://arxiv.org/abs/2410.09927</link>
      <description>arXiv:2410.09927v1 Announce Type: new 
Abstract: The deployment of LoRaWAN (Long Range Wide Area Network) in dynamic environments, such as smart campuses, presents significant challenges in optimizing network parameters like spreading factor (SF), transmission power (TxPower), and managing mobility while ensuring reliable communication. In this paper, we first introduce the fundamental concepts of short-range and long-range communication protocols, emphasizing the specific requirements and advantages of LoRaWAN in various applications. Next, we discuss smart space solutions that integrate Edge, Fog, and Cloud computing, illustrating how these paradigms work in conjunction with both short-range and long-range communication protocols to enhance data processing and decision-making capabilities in real-time. We then present our insights and lessons learned from the deployment of LoRaWAN across the campus, focusing on the challenges encountered and the strategies employed to address them. This work provides a comprehensive overview of the methodologies applied, the results achieved, and the implications for future research and practical applications in IoT-enabled smart environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09927v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hari Prabhat Gupta</dc:creator>
    </item>
    <item>
      <title>Burst-Mode Digital Signal Processing for Coherent Optical Time-Division Multiple Access</title>
      <link>https://arxiv.org/abs/2410.10080</link>
      <description>arXiv:2410.10080v1 Announce Type: new 
Abstract: As the 50G optical access gradually matures, it is time to discuss Beyond 50G optical access. According to the evolution rules of optical access standards, Beyond 50G optical access data rate may achieve 200Gb/s. Direct detection faces great challenges for Beyond 50G optical access, which makes coherent detection a potential solution. Similar to 50G optical timing-division-multiple access (TDMA), burst-mode digital signal processing (BM-DSP) is also required for Beyond 50G coherent optical TDMA (CO-TDMA). This paper proposes coherent BM-DSP (Co-BM-DSP) based on approximately 10ns designed preambles to process the burst signal for 200G CO-TDMA, which can fast estimate the state of polarization, frequency offset, sampling phase offset, synchronization position, and equalizer coefficients. Meanwhile, for obtaining the equalizer coefficients based on the designed preamble, the channel estimation based on the minimum-mean-square-error criterion is theoretically proven to have a unique solution for ensuring reliability. In conclusion, the proposed Co-BM-DSP based on the designed preambles paves the way for the applications of Beyond 50G CO-TDMA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10080v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ji Zhou, Cheng Li, Haide Wang, Zhiyang Liu, Weiping Liu, Changyuan Yu</dc:creator>
    </item>
    <item>
      <title>VNF Migration with Fast Defragmentation: A GAT-Based Deep Learning Method</title>
      <link>https://arxiv.org/abs/2410.10086</link>
      <description>arXiv:2410.10086v1 Announce Type: new 
Abstract: Network function virtualization (NFV) enhances service flexibility by decoupling network functions from dedicated hardware. To handle time-varying traffic in NFV network, virtualized network function (VNF) migration has been involved to dynamically adjust resource allocation. However, as network functions diversify, different resource types may be underutilized due to bottlenecks, which can be described as multidimensional resource fragmentation. To address this issue, we firstly define a metric to quantify resource fragmentation in NFV networks. Then, we propose a multi-hop graph attention network (MHGAT) model to effectively extract resource features from tailored network layers, which captures the overall network state and produces high-quality strategies rapidly. Building on this, we develop an MHGAT method to implement fast defragmentation and optimize VNF migration. Simulations demonstrate that by fast defragmentation, the MHGAT method improves the acceptance ratio by an average of 12.8%, reduces the overload ratio by an average of 30.6%, and lowers migration loss by an average of 43.3% compared to the state-of-art benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10086v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangyu Zhang, Yuang Chen, Hancheng Lu, Chengdi Lu</dc:creator>
    </item>
    <item>
      <title>WT-CFormer: High-Performance Web Traffic Anomaly Detection Using CNN and Transformer Networks</title>
      <link>https://arxiv.org/abs/2410.10327</link>
      <description>arXiv:2410.10327v1 Announce Type: new 
Abstract: Web traffic (WT) refers to time-series data that captures the volume of data transmitted to and from a web server during a user's visit to a website. Anomalies in web traffic signal unusual fluctuations in this data, making their detection crucial for ensuring network security. Deep neural network approaches for web traffic anomaly detection have achieved cutting-edge classification performance. However, since these methods are still insufficient in terms of classification effectiveness and speed, researching fast and accurate anomaly detection methods is a challenging problem. In this paper, we propose a novel anomaly detection model (WT-CFormer) specifically designed for web traffic, which innovatively use the Transformer to efficiently and accurately extract the temporal features of web traffic and deeply integrates the CNN to extract the spatial features of web traffic to improve the anomaly detection performance. In addition, we conduct a large number of experiments to prove the effectiveness and superiority of WT-CFormer for web traffic anomaly detection. In evaluation experiments, WT-CFormer has the highest performance, obtaining a recall as high as 96.79%, a precision of 97.35%, an F1 score of 97.07%, and an accuracy of 99.43%, which is 7.09%,1.15%, 4.77%, and 0.83% better than the state-of-the-art method, followed by C-LSTM, CTGA, random forest, and KNN algorithms. In addition, we find that the classification performance of WT-CFormer with only 50 training epochs outperforms C-LSTM with 500 training epochs, which greatly improves the convergence performance. Finally, we perform ablation experiments to demonstrate the necessity of each component within WT-CFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10327v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yundi He, Runhua Shi</dc:creator>
    </item>
    <item>
      <title>On Efficient Topology Management in Service-Oriented 6G Networks: An Edge Video Distribution Case Study</title>
      <link>https://arxiv.org/abs/2410.10338</link>
      <description>arXiv:2410.10338v1 Announce Type: new 
Abstract: An efficient topology management in future 6G networks is one of the fundamental challenges for a dynamic network creation based on location services, whereby each autonomous network entity, i.e., a sub-network, can be created for a specific application scenario. In this paper, we study the performance of a novel topology changes management system in a sample 6G network being dynamically organized in autonomous sub-networks. We propose and analyze an algorithm for intelligent prediction of topology changes and provide a comparative analysis with topology monitoring based approach. To this end, we present an industrially relevant case study on edge video distribution, as it is envisioned to be implemented in line with the 3GPP and ETSI MEC (Multi-access Edge Computing) standards. For changes prediction, we implement and analyze a novel topology change prediction algorithm, which can automatically optimize, train and, finally, select the best of different machine learning models available, based on the specific scenario under study. For link change scenario, the results show that three selected ML models exhibit high accuracy in detecting changes in link delay and bandwidth using measured throughput and RTT. ANN demonstrates the best performance in identifying cases with no changes, slightly outperforming random forest and XGBoost. For user mobility scenario, XGBoost is more efficient in learning patterns for topology change prediction while delivering much faster results compared to the more computationally demanding deep learning models, such as LSTM and CNN. In terms of cost efficiency, our ML-based approach represents a significantly cost-effective alternative to traditional monitoring approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10338v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zied Ennaceur, Mounir Bensalem, Admela Jukan, Claus Keuker, Huanzhuo Wu, Rastin Pries</dc:creator>
    </item>
    <item>
      <title>Fast Reroute with Highly Connected Routes Based on Maximum Flow Evaluation</title>
      <link>https://arxiv.org/abs/2410.10528</link>
      <description>arXiv:2410.10528v1 Announce Type: new 
Abstract: Fault-tolerant routing allows the selection of alternative routes to the destination after the route being used fails. Fast Reroute (FRR) is a proactive strategy through which the protocol pre-configures backup routes that are activated when needed. In this work, we propose the MaxFlowRouting algorithm that employs maximum flow evaluation as well as the route size to select routes that are highly connected. The main advantage of the proposed algorithm is that if any component of such a route fails, there are more alternative paths to the destination in comparison with the route computed with Dijkstra's shortest path algorithm. Simulation results are presented in which we compare the two algorithms (Dijkstra's and MaxFlowRouting) for multiple different random graphs (including Erdos-Renyi, Bar\'abasi-Albert, and Watts-Strogatz) and also for the topologies of some of the most important Internet backbones of the U.S.A., Europe, Brazil, and Japan: Internet2, Geant, RNP, and Wide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10528v1</guid>
      <category>cs.NI</category>
      <category>cs.DM</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Okida, Maverson E. Schuze-Rosa, Elias P. Duarte Jr</dc:creator>
    </item>
    <item>
      <title>Soft Tester UE: A Novel Approach for Open RAN Security Testing</title>
      <link>https://arxiv.org/abs/2410.09641</link>
      <description>arXiv:2410.09641v1 Announce Type: cross 
Abstract: With the rise of 5G and open radio access networks (O-RAN), there is a growing demand for customizable experimental platforms dedicated to security testing, as existing testbeds do not prioritize this area. Traditional, hardware-dependent testing methods pose challenges for smaller companies and research institutions. The growing wireless threat landscape highlights the critical need for proactive security testing, as 5G and O-RAN deployments are appealing targets for cybercriminals. To address these challenges, this article introduces the Soft Tester UE (soft T-UE), a software-defined test equipment designed to evaluate the security of 5G and O-RAN deployments via the Uu air interface between the user equipment (UE) and the network. The outcome is to deliver a free, open-source, and expandable test instrument to address the need for both standardized and customizable automated security testing. By extending beyond traditional security metrics, the soft T-UE promotes the development of new security measures and enhances the capability to anticipate and mitigate potential security breaches. The tool's automated testing capabilities are demonstrated through a scenario where the Radio Access Network (RAN) under test is evaluated when it receives fuzzed data when initiating a connection with an UE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09641v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Moore, Aly Sabri Abdalla, Charles Ueltschey, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>Advancing Experimental Platforms for UAV Communications: Insights from AERPAW'S Digital Twin</title>
      <link>https://arxiv.org/abs/2410.09648</link>
      <description>arXiv:2410.09648v1 Announce Type: cross 
Abstract: The rapid evolution of 5G and beyond has advanced space-air-terrestrial networks, with unmanned aerial vehicles (UAVs) offering enhanced coverage, flexible configurations, and cost efficiency. However, deploying UAV-based systems presents challenges including varying propagation conditions and hardware limitations. While simulators and theoretical models have been developed, real-world experimentation is critically important to validate the research. Digital twins, virtual replicas of physical systems, enable emulation that bridge theory and practice. This paper presents our experimental results from AERPAW's digital twin, showcasing its ability to simulate UAV communication scenarios and providing insights into system performance and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09648v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Moore, Aly Sabri Abdalla, Charles Ueltschey, An{\i}l G\"urses, \"Ozg\"ur \"Ozdemir, Mihail L. Sichitiu, \.Ismail G\"uven\c{c}, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>Simulation of fidelity in entanglement-based networks with repeater chains</title>
      <link>https://arxiv.org/abs/2410.09779</link>
      <description>arXiv:2410.09779v1 Announce Type: cross 
Abstract: We implement a simulation environment on top of NetSquid that is specifically designed for estimating the end-to-end fidelity across a path of quantum repeaters or quantum switches. The switch model includes several generalizations which are not currently available in other tools, and are useful for gaining insight into practical and realistic quantum network engineering problems: an arbitrary number of memory registers at the switches, simplicity in including entanglement distillation mechanisms, arbitrary switching topologies, and more accurate models for the depolarization noise. An illustrative case study is presented, namely a comparison in terms of performance between a repeater chain where repeaters can only swap sequentially, and a single switch equipped with multiple memory registers, able to handle multiple swapping requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09779v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David P\'erez Castro, Ana Fern\'andez Vilas, Manuel Fern\'andez-Veiga, Mateo Blanco Rodr\'iguez, Rebeca P. D\'iaz Redondo</dc:creator>
    </item>
    <item>
      <title>A Survey on Performance, Current and Future Usage of Vehicle-To-Everything Communication Standards</title>
      <link>https://arxiv.org/abs/2410.10264</link>
      <description>arXiv:2410.10264v1 Announce Type: cross 
Abstract: Wireless communication between road users is essential for environmental perception, reasoning, and mission planning to enable fully autonomous vehicles, and thus improve road safety and transport efficiency. To enable collaborative driving, the concept of vehicle-to-Everything (V2X) has long been introduced to the industry. Within the last two decades, several communication standards have been developed based on IEEE 802.11p and cellular standards, namely Dedicated Short-Range Communication (DSRC), Intelligent Transportation System G5 (ITS-G5), and Cellular- and New Radio- Vehicle-to-Everything (C-V2X and NR-V2X). However, while there exists a high quantity of available publications concerning V2X and the analysis of the different standards, only few surveys exist that summarize these results. Furthermore, to our knowledge, no survey that provides an analysis about possible future trends and challenges for the global implementation of V2Xexists. Thus, this contribution provides a detailed survey on Vehicle-to-Everything communication standards, their performance, current and future applications, and associated challenges. Based on our research, we have identified several research gaps and provide a picture about the possible future of the Vehicle-to-Everything communication domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10264v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Falk Dettinger, Matthias Wei{\ss}, Daniel Dittler, Johannes St\"umpfle, Maurice Artelt, Michael Weyrich</dc:creator>
    </item>
    <item>
      <title>Continual Deep Reinforcement Learning to Prevent Catastrophic Forgetting in Jamming Mitigation</title>
      <link>https://arxiv.org/abs/2410.10521</link>
      <description>arXiv:2410.10521v1 Announce Type: cross 
Abstract: Deep Reinforcement Learning (DRL) has been highly effective in learning from and adapting to RF environments and thus detecting and mitigating jamming effects to facilitate reliable wireless communications. However, traditional DRL methods are susceptible to catastrophic forgetting (namely forgetting old tasks when learning new ones), especially in dynamic wireless environments where jammer patterns change over time. This paper considers an anti-jamming system and addresses the challenge of catastrophic forgetting in DRL applied to jammer detection and mitigation. First, we demonstrate the impact of catastrophic forgetting in DRL when applied to jammer detection and mitigation tasks, where the network forgets previously learned jammer patterns while adapting to new ones. This catastrophic interference undermines the effectiveness of the system, particularly in scenarios where the environment is non-stationary. We present a method that enables the network to retain knowledge of old jammer patterns while learning to handle new ones. Our approach substantially reduces catastrophic forgetting, allowing the anti-jamming system to learn new tasks without compromising its ability to perform previously learned tasks effectively. Furthermore, we introduce a systematic methodology for sequentially learning tasks in the anti-jamming framework. By leveraging continual DRL techniques based on PackNet, we achieve superior anti-jamming performance compared to standard DRL methods. Our proposed approach not only addresses catastrophic forgetting but also enhances the adaptability and robustness of the system in dynamic jamming environments. We demonstrate the efficacy of our method in preserving knowledge of past jammer patterns, learning new tasks efficiently, and achieving superior anti-jamming performance compared to traditional DRL approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10521v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kemal Davaslioglu, Sastry Kompella, Tugba Erpek, Yalin E. Sagduyu</dc:creator>
    </item>
    <item>
      <title>SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization</title>
      <link>https://arxiv.org/abs/2410.10759</link>
      <description>arXiv:2410.10759v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been a disruptive innovation in recent years, and they play a crucial role in our daily lives due to their ability to understand and generate human-like text. Their capabilities include natural language understanding, information retrieval and search, translation, chatbots, virtual assistance, and many more. However, it is well known that LLMs are massive in terms of the number of parameters. Additionally, the self-attention mechanism in the underlying architecture of LLMs, Transformers, has quadratic complexity in terms of both computation and memory with respect to the input sequence length. For these reasons, LLM inference is resource-intensive, and thus, the throughput of LLM inference is limited, especially for the longer sequences. In this report, we design a collaborative inference architecture between a server and its clients to alleviate the throughput limit. In this design, we consider the available resources on both sides, i.e., the computation and communication costs. We develop a dynamic programming-based algorithm to optimally allocate computation between the server and the client device to increase the server throughput, while not violating the service level agreement (SLA). We show in the experiments that we are able to efficiently distribute the workload allowing for roughly 1/3 reduction in the server workload, while achieving 19 percent improvement over a greedy method. As a result, we are able to demonstrate that, in an environment with different types of LLM inference requests, the throughput of the server is improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10759v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akrit Mudvari, Yuang Jiang, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>Offline to Online Learning for Real-Time Bandwidth Estimation</title>
      <link>https://arxiv.org/abs/2309.13481</link>
      <description>arXiv:2309.13481v3 Announce Type: replace 
Abstract: Real-time video applications require accurate bandwidth estimation (BWE) to maintain user experience across varying network conditions. However, increasing network heterogeneity challenges general-purpose BWE algorithms, necessitating solutions that adapt to end-user environments. While widely adopted, heuristic-based methods are difficult to individualize without extensive domain expertise. Conversely, online reinforcement learning (RL) offers ease of customization but neglects prior domain expertise and suffers from sample inefficiency. Thus, we present Merlin, an imitation learning-based solution that replaces the manual parameter tuning of heuristic-based methods with data-driven updates to streamline end-user personalization. Our key insight is that transforming heuristic-based BWE algorithms into neural networks facilitates data-driven personalization. Merlin utilizes Behavioral Cloning to efficiently learn from offline telemetry logs, capturing heuristic policies without live network interactions. The cloned policy can then be seamlessly tailored to end user network conditions through online finetuning. In real intercontinental videoconferencing calls, Merlin matches our heuristic's policy with no statistically significant differences in user quality of experience (QoE). Finetuning Merlin's control policy to end-user environments enables QoE improvements of up to 7.8% compared to the heuristic policy. Lastly, our IL-based design performs competitively with current state-of-the-art online RL techniques but converges with 80% fewer videoconferencing samples, facilitating practical end-user personalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13481v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aashish Gottipati, Sami Khairy, Gabriel Mittag, Vishak Gopal, Ross Cutler</dc:creator>
    </item>
    <item>
      <title>Socialized Learning: A Survey of the Paradigm Shift for Edge Intelligence in Networked Systems</title>
      <link>https://arxiv.org/abs/2404.13348</link>
      <description>arXiv:2404.13348v2 Announce Type: replace 
Abstract: Amidst the robust impetus from artificial intelligence (AI) and big data, edge intelligence (EI) has emerged as a nascent computing paradigm, synthesizing AI with edge computing (EC) to become an exemplary solution for unleashing the full potential of AI services. Nonetheless, challenges in communication costs, resource allocation, privacy, and security continue to constrain its proficiency in supporting services with diverse requirements. In response to these issues, this paper introduces socialized learning (SL) as a promising solution, further propelling the advancement of EI. SL is a learning paradigm predicated on social principles and behaviors, aimed at amplifying the collaborative capacity and collective intelligence of agents within the EI system. SL not only enhances the system's adaptability but also optimizes communication, and networking processes, essential for distributed intelligence across diverse devices and platforms. Therefore, a combination of SL and EI may greatly facilitate the development of collaborative intelligence in the future network. This paper presents the findings of a literature review on the integration of EI and SL, summarizing the latest achievements in existing research on EI and SL. Subsequently, we delve comprehensively into the limitations of EI and how it could benefit from SL. Special emphasis is placed on the communication challenges and networking strategies and other aspects within these systems, underlining the role of optimized network solutions in improving system efficiency. Based on these discussions, we elaborate in detail on three integrated components: socialized architecture, socialized training, and socialized inference, analyzing their strengths and weaknesses. Finally, we identify some possible future applications of combining SL and EI, discuss open problems and suggest some future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13348v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Wang, Yunfeng Zhao, Chao Qiu, Qinghua Hu, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Evaluating the Impact of Inter-cluster Communications in Edge Computing</title>
      <link>https://arxiv.org/abs/2409.09278</link>
      <description>arXiv:2409.09278v2 Announce Type: replace 
Abstract: Distributed applications based on micro-services in edge computing are becoming increasingly popular due to the rapid evolution of mobile networks. While Kubernetes is the default framework when it comes to orchestrating and managing micro-service-based applications in mobile networks, the requirement to run applications between multiple sites at cloud and edge poses new challenges. Since Kubernetes does not natively provide tools to abstract inter-cluster communications at the application level, inter-cluster communication in edge computing is becoming increasingly critical to the application performance. In this paper, we evaluate for the first time the impact of inter-cluster communication on edge computing performance by using three prominent, open source inter-cluster communication projects and tools, i.e., Submariner, ClusterLink and Skupper. We develop a fully open-source testbed that integrates these tools in a modular fashion, and experimentally benchmark sample applications, including the ML class of applications, on their performance running in the multi-cluster edge computing system under varying networking conditions. We experimentally analyze two classes of envisioned mobile applications, i.e., a) industrial automation, b) vehicle decision drive assist. Our results show that ClusterLink performs best out of the three tools in scenarios with increased payloads, regardless of the underlying networking conditions or transmission direction between clusters. It is closely followed by Skupper, unless request and reply both transport significant amounts of data. Finally, when requesting smaller amounts of data from a service, Submariner slightly outperforms Skupper and ClusterLink regardless of the inter-node networking conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09278v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Michalke, Iulisloi Zacarias, Admela Jukan, Kfir Toledo, Etai Lev-Ran</dc:creator>
    </item>
    <item>
      <title>Performance Comparison of HTTP/3 and HTTP/2: Proxy vs. Non-Proxy Environments</title>
      <link>https://arxiv.org/abs/2409.16267</link>
      <description>arXiv:2409.16267v2 Announce Type: replace 
Abstract: This paper systematically evaluates the performance of QUIC/HTTP3 (H3) and TCP/HTTP2 (H2) protocols in proxy-enhanced environments. H3 leverages features such as UDP-based flow-controlled streams, integrated TLS, multiplexed connections, and connection migration, offering the potential for improved web communication. Despite extensive research, the impact of proxy integration and connection migration remains underexplored. This study addresses this gap by evaluating H3 and H2 across various scenarios, particularly in noisy networks and proxy setups. Our findings show that H3 excels under high loss and high latency conditions, significantly benefiting from its connection migration and multiplexing features, with improvements of up to 88.36% under high-loss and high-latency conditions, and 81.5% under extreme loss conditions, respectively. H3's connection migration remains robust, maintaining stable performance even in proxy-enhanced environments, ensuring seamless network transitions. The proxy has a more neutral impact on H3, while it significantly enhances H2 performance, particularly when paired with BBR, resulting in a 90% improvement in the single-stream file download experiment under severe network impairments. Any improvements observed in H3 under a proxy are minor and do not fundamentally alter H3's performance as they do for H2. Importantly, while H2 with the right congestion control algorithm (CCA) can achieve performance comparable to H3, H3's performance is more robust, as it is less impacted by network conditions, proxy settings, and CCA variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16267v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Liu, John Dehart, Jyoti Parwatikar, Behrooz Farkiani, Patrick Crowley</dc:creator>
    </item>
    <item>
      <title>Manipulating OpenFlow Link Discovery Packet Forwarding for Topology Poisoning</title>
      <link>https://arxiv.org/abs/2408.16940</link>
      <description>arXiv:2408.16940v2 Announce Type: replace-cross 
Abstract: Software-defined networking (SDN) is a centralized, dynamic, and programmable network management technology that enables flexible traffic control and scalability. SDN facilitates network administration through a centralized view of the underlying physical topology; tampering with this topology view can result in catastrophic damage to network management and security. To underscore this issue, we introduce Marionette, a new topology poisoning technique that manipulates OpenFlow link discovery packet forwarding to alter topology information. Our approach exposes an overlooked yet widespread attack vector, distinguishing itself from traditional link fabrication attacks that tamper, spoof, or relay discovery packets at the data plane. Unlike localized attacks observed in existing methods, our technique introduces a globalized topology poisoning attack that leverages control privileges. Marionette implements a reinforcement learning algorithm to compute a poisoned topology target, and injects flow entries to achieve a long-lived stealthy attack. Our evaluation shows that Marionette successfully attacks five open-source controllers and nine OpenFlow-based discovery protocols. Marionette overcomes the state-of-the-art topology poisoning defenses, showcasing a new class of topology poisoning that initiates on the control plane. This security vulnerability was ethically disclosed to OpenDaylight, and CVE-2024-37018 has been assigned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16940v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3658644.3690345</arxiv:DOI>
      <dc:creator>Mingming Chen, Thomas La Porta, Teryl Taylor, Frederico Araujo, Trent Jaeger</dc:creator>
    </item>
  </channel>
</rss>
