<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 01:37:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cross-sectional personal network analysis of adult smoking in rural areas</title>
      <link>https://arxiv.org/abs/2408.14832</link>
      <description>arXiv:2408.14832v1 Announce Type: new 
Abstract: While research on adolescent smoking is extensive, little attention has been given to smoking behaviors among rural middle-aged and older adults. This study examines the role of personal networks and sociodemographic factors in predicting smoking status in a rural Romanian community. Using a link-tracing sampling method, we gathered data from 76 participants out of 83 in Leresti, Arges County. Face-to-face interviews collected sociodemographic data and network information, including smoking status and relational dynamics. We applied multilevel logistic regression models to predict smoking behaviors (current smokers, former smokers, and non-smokers) based on individual characteristics and network influences. Results indicate that social networks significantly influence smoking behaviors. For current smokers, having a smoking family member greatly increased the odds of smoking (OR = 2.51, 95% CI: 1.62, 3.91, p &lt; 0.001). Similarly, non-smoking family members increased the likelihood of being a non-smoker (OR = 1.64, 95% CI: 1.04, 2.61, p &lt; 0.05). Women were less likely to smoke, highlighting sex differences in behavior. These findings emphasize the critical role of social networks in shaping smoking habits, advocating for targeted interventions in rural areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14832v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bianca-Elena Mih\u{a}il\u{a}, Marian-Gabriel H\^ancean, Matja\v{z} Perc, J\"urgen Lerner, Iulian Oan\u{a}, Marius Geant\u{a}, Jos\'e Luis Molina, Cosmina Cioroboiu</dc:creator>
    </item>
    <item>
      <title>Bayesian spatiotemporal modelling of political violence and conflict events using discrete-time Hawkes processes</title>
      <link>https://arxiv.org/abs/2408.14940</link>
      <description>arXiv:2408.14940v1 Announce Type: new 
Abstract: Monitoring of conflict risk in the humanitarian sector is largely based on simple historic averages. To advance our understanding, we propose Hawkes processes, a self-exciting stochastic process used to describe phenomena whereby past events increase the probability of future events occurring. The overarching goal of this work is to assess the potential for using a more statistically rigorous approach to monitor the risk of political violence and conflict events in practice and characterise their temporal and spatial patterns.
  The region of South Asia was selected as an exemplar of how our model can be applied globally. We individually analyse the various types of conflict events for the countries in this region and compare the results. A Bayesian, spatiotemporal variant of the Hawkes process is fitted to data gathered by the Armed Conflict Location and Event Data (ACLED) project to obtain sub-national estimates of conflict risk over time and space. Our model can effectively estimate the risk level of these events within a statistically sound framework, with a more precise understanding of the uncertainty around these estimates than was previously possible. This work enables a better understanding of conflict events which can inform preventative measures.
  We demonstrate the advantages of the Bayesian framework by comparing our results to maximum likelihood estimation. While maximum likelihood gives reasonable point estimates, the Bayesian approach is preferred when possible. Practical examples are presented to demonstrate how the proposed model can be used to monitor conflict risk. Comparing to current practices that rely on historical averages, we also show that our model is more stable and robust to outliers. In this work we aim to support actors in the humanitarian sector in making data-informed decisions, such as the allocation of resources in conflict-prone regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14940v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raiha Browning, Hamish Patten, Judith Rousseau, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>LLOT: application of Laplacian Linear Optimal Transport in spatial transcriptome reconstruction</title>
      <link>https://arxiv.org/abs/2408.15149</link>
      <description>arXiv:2408.15149v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) allows transcriptional profiling, and cell-type annotation of individual cells. However, sample preparation in typical scRNA-seq experiments often homogenizes the samples, thus spatial locations of individual cells are often lost. Although spatial transcriptomic techniques, such as in situ hybridization (ISH) or Slide-seq, can be used to measure gene expression in specific locations in samples, it remains a challenge to measure or infer expression level for every gene at a single-cell resolution in every location in tissues. Existing computational methods show promise in reconstructing these missing data by integrating scRNA-seq data with spatial expression data such as those obtained from spatial transcriptomics. Here we describe Laplacian Linear Optimal Transport (LLOT), an interpretable method to integrate single-cell and spatial transcriptomics data to reconstruct missing information at a whole-genome and single-cell resolution. LLOT iteratively corrects platform effects and employs Laplacian Optimal Transport to decompose each spot in spatial transcriptomics data into a spatially-smooth probabilistic mixture of single cells. We benchmarked LLOT against several other methods on datasets of Drosophila embryo, mouse cerebellum and synthetic datasets generated by scDesign3 in the paper, and another three datasets in the supplementary. The results showed that LLOT consistently outperformed others in reconstructing spatial expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15149v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhao Zhu, Kevin Zhang, Dehan Kong, Zhaolei Zhang</dc:creator>
    </item>
    <item>
      <title>Co-factor analysis of citation networks</title>
      <link>https://arxiv.org/abs/2408.14604</link>
      <description>arXiv:2408.14604v1 Announce Type: cross 
Abstract: One compelling use of citation networks is to characterize papers by their relationships to the surrounding literature. We propose a method to characterize papers by embedding them into two distinct "co-factor" spaces: one describing how papers send citations, and the other describing how papers receive citations. This approach presents several challenges. First, older documents cannot cite newer documents, and thus it is not clear that co-factors are even identifiable. We resolve this challenge by developing a co-factor model for asymmetric adjacency matrices with missing lower triangles and showing that identification is possible. We then frame estimation as a matrix completion problem and develop a specialized implementation of matrix completion because prior implementations are memory bound in our setting. Simulations show that our estimator has promising finite sample properties, and that naive approaches fail to recover latent co-factor structure. We leverage our estimator to investigate 237,794 papers published in statistics journals from 1898 to 2022, resulting in the most comprehensive topic model of the statistics literature to date. We find interpretable co-factors corresponding to many statistical subfields, including time series, variable selection, spatial methods, graphical models, GLM(M)s, causal inference, multiple testing, quantile regression, resampling, semi-parametrics, dimension reduction, and several more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14604v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2394464</arxiv:DOI>
      <dc:creator>Alex Hayes, Karl Rohe</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach for fitting semi-Markov mixture models of cancer latency to individual-level data</title>
      <link>https://arxiv.org/abs/2408.14625</link>
      <description>arXiv:2408.14625v1 Announce Type: cross 
Abstract: Multi-state models of cancer natural history are widely used for designing and evaluating cancer early detection strategies. Calibrating such models against longitudinal data from screened cohorts is challenging, especially when fitting non-Markovian mixture models against individual-level data. Here, we consider a family of semi-Markov mixture models of cancer natural history introduce an efficient data-augmented Markov chain Monte Carlo sampling algorithm for fitting these models to individual-level screening and cancer diagnosis histories. Our fully Bayesian approach supports rigorous uncertainty quantification and model selection through leave-one-out cross-validation, and it enables the estimation of screening-related overdiagnosis rates. We demonstrate the effectiveness of our approach using synthetic data, showing that the sampling algorithm efficiently explores the joint posterior distribution of model parameters and latent variables. Finally, we apply our method to data from the US Breast Cancer Surveillance Consortium and estimate the extent of breast cancer overdiagnosis associated with mammography screening. The sampler and model comparison method are available in the R package baclava.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14625v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, Shannon Holloway, Marc Ryser, Jason Xu</dc:creator>
    </item>
    <item>
      <title>Inspection-Guided Randomization: A Flexible and Transparent Restricted Randomization Framework for Better Experimental Design</title>
      <link>https://arxiv.org/abs/2408.14669</link>
      <description>arXiv:2408.14669v1 Announce Type: cross 
Abstract: Randomized experiments are considered the gold standard for estimating causal effects. However, out of the set of possible randomized assignments, some may be likely to produce poor effect estimates and misleading conclusions. Restricted randomization is an experimental design strategy that filters out undesirable treatment assignments, but its application has primarily been limited to ensuring covariate balance in two-arm studies where the target estimand is the average treatment effect. Other experimental settings with different design desiderata and target effect estimands could also stand to benefit from a restricted randomization approach. We introduce Inspection-Guided Randomization (IGR), a transparent and flexible framework for restricted randomization that filters out undesirable treatment assignments by inspecting assignments against analyst-specified, domain-informed design desiderata. In IGR, the acceptable treatment assignments are locked in ex ante and pre-registered in the trial protocol, thus safeguarding against $p$-hacking and promoting reproducibility. Through illustrative simulation studies motivated by education and behavioral health interventions, we demonstrate how IGR can be used to improve effect estimates compared to benchmark designs in group formation experiments and experiments with interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14669v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maggie Wang, Ren\'e F. Kizilcec, Michael Baiocchi</dc:creator>
    </item>
    <item>
      <title>Inferring ghost cities on the globe in newly developed urban areas based on urban vitality with multi-source data</title>
      <link>https://arxiv.org/abs/2408.15117</link>
      <description>arXiv:2408.15117v1 Announce Type: cross 
Abstract: Due to rapid urbanization over the past 20 years, many newly developed areas have lagged in socio-economic maturity, creating an imbalance with older cities and leading to the rise of "ghost cities." However, due to the complexity of socio-economic factors, no global studies have measured this phenomenon. We propose a unified framework based on urban vitality theory and multi-source data, validated by various data sources. We derived 8841 natural cities globally with an area over 5 square kiloxmeters and divided each into new urban areas (developed after 2005) and old urban areas (developed before 2005). Urban vitality was gauged using the density of road networks, points of interest (POIs), and population density with 1 km resolution across morphological, functional, and social dimensions. By comparing urban vitality in new and old urban areas, we quantify the ghost cities index (GCI) globally using the theory of urban vitality for the first time. The results reveal that the vitality of new urban areas is 7.69% that of old ones. The top 5% (442) of cities were designated as ghost cities, a finding mirrored by news media and other research. This study sheds light on strategies for sustainable global urbanization, crucial for the United Nations' Sustainable Development Goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15117v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yecheng Zhang, Tangqi Tu, Ying long</dc:creator>
    </item>
    <item>
      <title>Risk Twin: Real-time Risk Visualization and Control for Structural Systems</title>
      <link>https://arxiv.org/abs/2403.00283</link>
      <description>arXiv:2403.00283v2 Announce Type: replace 
Abstract: Digital twinning in structural engineering is a rapidly evolving technology that aims to eliminate the gap between physical systems and their digital models through real-time sensing, visualization, and control techniques. Although Digital Twins can offer dynamic insights into physical systems, their accuracy is inevitably compromised by uncertainties in sensing, modeling, simulation, and control. This paper proposes a specialized Digital Twin formulation, named Risk Twin, designed for real-time risk visualization and risk-informed control of structural systems. Integrating structural reliability and Bayesian inference methods with Digital Twinning techniques, Risk Twin can analyze and visualize the reliability indices for structural components in real-time. To facilitate real-time inference and reliability updating, a simulation-free scheme is proposed. This scheme leverages precomputed quantities prepared during an offline phase for rapid inference in the online phase. Proof-of-concept numerical and real-world Risk Twins are constructed to showcase the proposed concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00283v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeyu Wang, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Periodicity in New York State COVID-19 Hospitalizations Leveraged from the Variable Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2404.11006</link>
      <description>arXiv:2404.11006v2 Announce Type: replace 
Abstract: The outbreak of the SARS-CoV-2 virus, which led to an unprecedented global pandemic, has underscored the critical importance of understanding seasonal patterns. This knowledge is fundamental for decision-making in healthcare and public health domains. Investigating the presence, intensity, and precise nature of seasonal trends, as well as these temporal patterns, is essential for forecasting future occurrences, planning interventions, and making informed decisions based on the evolution of events over time. This study employs the Variable Bandpass Periodic Block Bootstrap (VBPBB) to separate and analyze different periodic components by frequency in time series data, focusing on annually correlated (PC) principal components. Bootstrapping, a method used to estimate statistical sampling distributions through random sampling with replacement, is particularly useful in this context. Specifically, block bootstrapping, a model-independent resampling method suitable for time series data, is utilized. Its extensions are aimed at preserving the correlation structures inherent in PC processes. The VBPBB applies a bandpass filter to isolate the relevant PC frequency, thereby minimizing contamination from extraneous frequencies and noise. This approach significantly narrows the confidence intervals, enhancing the precision of estimated sampling distributions for the investigated periodic characteristics. Furthermore, we compared the outcomes of block bootstrapping for periodically correlated time series with VBPBB against those from more traditional bootstrapping methods. Our analysis shows VBPBB provides strong evidence of the existence of an annual seasonal PC pattern in hospitalization rates not detectible by other methods, providing timing and confidence intervals for their impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11006v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Ahmad, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>A Bayesian Classification Trees Approach to Treatment Effect Variation with Noncompliance</title>
      <link>https://arxiv.org/abs/2408.07765</link>
      <description>arXiv:2408.07765v2 Announce Type: replace 
Abstract: Estimating varying treatment effects in randomized trials with noncompliance is inherently challenging since variation comes from two separate sources: variation in the impact itself and variation in the compliance rate. In this setting, existing flexible machine learning methods are highly sensitive to the weak instruments problem, in which the compliance rate is (locally) close to zero. Our main methodological contribution is to present a Bayesian Causal Forest model for binary response variables in scenarios with noncompliance. By repeatedly imputing individuals' compliance types, we can flexibly estimate heterogeneous treatment effects among compliers. Simulation studies demonstrate the usefulness of our approach when compliance and treatment effects are heterogeneous. We apply the method to detect and analyze heterogeneity in the treatment effects in the Illinois Workplace Wellness Study, which not only features heterogeneous and one-sided compliance but also several binary outcomes of interest. We demonstrate the methodology on three outcomes one year after intervention. We confirm a null effect on the presence of a chronic condition, discover meaningful heterogeneity impact of the intervention on metabolic parameters though the average effect is null in classical partial effect estimates, and find substantial heterogeneity in individuals' perception of management prioritization of health and safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07765v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared D. Fisher, David W. Puelz, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Deep R Programming</title>
      <link>https://arxiv.org/abs/2301.01188</link>
      <description>arXiv:2301.01188v4 Announce Type: replace-cross 
Abstract: Deep R Programming is a comprehensive and in-depth introductory course on one of the most popular languages for data science. It equips ambitious students, professionals, and researchers with the knowledge and skills to become independent users of this potent environment so that they can tackle any problem related to data wrangling and analytics, numerical computing, statistics, and machine learning. This textbook is a non-profit project. Its online and PDF versions are freely available at &lt;https://deepr.gagolewski.com/&gt;.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01188v4</guid>
      <category>cs.PL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.7490464</arxiv:DOI>
      <arxiv:journal_reference>Zenodo, Melbourne, ISBN: 978-0-6455719-2-9 (2024) https://deepr.gagolewski.com/</arxiv:journal_reference>
      <dc:creator>Marek Gagolewski</dc:creator>
    </item>
    <item>
      <title>Estimating optical vegetation indices and biophysical variables for temperate forests with Sentinel-1 SAR data using machine learning techniques: A case study for Czechia</title>
      <link>https://arxiv.org/abs/2311.07537</link>
      <description>arXiv:2311.07537v2 Announce Type: replace-cross 
Abstract: Current optical vegetation indices (VIs) for monitoring forest ecosystems are well established and widely used in various applications, but can be limited by atmospheric effects such as clouds. In contrast, synthetic aperture radar (SAR) data can offer insightful and systematic forest monitoring with complete time series (TS) due to signal penetration through clouds and day and night image acquisitions. This study aims to address the limitations of optical satellite data by using SAR data as an alternative for estimating optical VIs for forests through machine learning (ML). While this approach is less direct and likely only feasible through the power of ML, it raises the scientific question of whether enough relevant information is contained in the SAR signal to accurately estimate VIs. This work covers the estimation of TS of four VIs (LAI, FAPAR, EVI and NDVI) using multitemporal Sentinel-1 SAR and ancillary data. The study focused on both healthy and disturbed temperate forest areas in Czechia for the year 2021, while ground truth labels generated from Sentinel-2 multispectral data. This was enabled by creating a paired multi-modal TS dataset in Google Earth Engine (GEE), including temporally and spatially aligned Sentinel-1, Sentinel-2, DEM, weather and land cover datasets. The inclusion of DEM-derived auxiliary features and additional meteorological information, further improved the results. In the comparison of ML models, the traditional ML algorithms, RFR and XGBoost slightly outperformed the AutoML approach, auto-sklearn, for all VIs, achieving high accuracies ($R^2$ between 70-86%) and low errors (0.055-0.29 of MAE). In general, up to 240 measurements per year and a spatial resolution of 20 m can be achieved using estimated SAR-based VIs with high accuracy. A great advantage of the SAR-based VI is the ability to detect abrupt forest changes with sub-weekly temporal accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07537v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Paluba, Bertrand Le Saux, P\v{r}emysl Stych</dc:creator>
    </item>
    <item>
      <title>Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring</title>
      <link>https://arxiv.org/abs/2406.02948</link>
      <description>arXiv:2406.02948v2 Announce Type: replace-cross 
Abstract: Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02948v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huazhen Yu, Lixin Zhang</dc:creator>
    </item>
    <item>
      <title>Enhancing Uplift Modeling in Multi-Treatment Marketing Campaigns: Leveraging Score Ranking and Calibration Techniques</title>
      <link>https://arxiv.org/abs/2408.13628</link>
      <description>arXiv:2408.13628v2 Announce Type: replace-cross 
Abstract: Uplift modeling is essential for optimizing marketing strategies by selecting individuals likely to respond positively to specific marketing campaigns. This importance escalates in multi-treatment marketing campaigns, where diverse treatment is available and we may want to assign the customers to treatment that can make the most impact. While there are existing approaches with convenient frameworks like Causalml, there are potential spaces to enhance the effect of uplift modeling in multi treatment cases. This paper introduces a novel approach to uplift modeling in multi-treatment campaigns, leveraging score ranking and calibration techniques to improve overall performance of the marketing campaign. We review existing uplift models, including Meta Learner frameworks (S, T, X), and their application in real-world scenarios. Additionally, we delve into insights from multi-treatment studies to highlight the complexities and potential advancements in the field. Our methodology incorporates Meta-Learner calibration and a scoring rank-based offer selection strategy. Extensive experiment results with real-world datasets demonstrate the practical benefits and superior performance of our approach. The findings underscore the critical role of integrating score ranking and calibration techniques in refining the performance and reliability of uplift predictions, thereby advancing predictive modeling in marketing analytics and providing actionable insights for practitioners seeking to optimize their campaign strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13628v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoon Tae Park, Ting Xu, Mohamed Anany</dc:creator>
    </item>
  </channel>
</rss>
