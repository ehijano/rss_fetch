<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 05:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Impacts of Increasingly Complex Matchup Models on Baseball Win Probability</title>
      <link>https://arxiv.org/abs/2511.17733</link>
      <description>arXiv:2511.17733v1 Announce Type: new 
Abstract: Baseball is a game of strategic decisions including bullpen usage, pinch-hitting and intentional walks. Managers must adjust their strategies based on the changing state of the game in order to give their team the best chance of winning. In this thesis, we investigate how matchup models -- tools that predict the probabilities of plate appearance outcomes -- impact in-game strategy and ultimately affect win probability. We develop four progressively complex, hierarchical Bayesian models that predict plate appearance outcomes by combining information from both pitchers and batters, their handedness, and recent data, along with base running probabilities calibrated to a player's base-stealing tendencies.
  Using each model within a game-theoretic framework, we approximate subgame perfect Nash equilibria for in-game decisions, including substitutions and intentional walks. Simulations of the 2024 MLB postseason show that more accurate matchup models can yield tangible gains in win probability -- as much as one additional victory per 162-game season. Furthermore, employing the most detailed model to generate win predictions for actual playoff games demonstrates alignment with market expectations, underscoring both the power and potential of advanced matchup modeling for on-field strategy and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17733v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tristan Mott, Caleb Bradshaw, David Grimsman, Christopher Archibald</dc:creator>
    </item>
    <item>
      <title>TimeBoost: Do Ahead-of-Time Auctions Work?</title>
      <link>https://arxiv.org/abs/2511.18328</link>
      <description>arXiv:2511.18328v1 Announce Type: new 
Abstract: We study the performance of the TimeBoost auction, by comparing cumulative fixed time markout of fast lane trades over the TimeBoost interval to bids for the fast lane. Such comparison allows us to assess how well bids predict future extracted value from the time advantage. The correlation between winning bids and markouts is weak across bidders, suggesting that bids are a noisy predictor of extracted value. The correlation slightly improves when comparing paid bids (the second highest bid) instead of winning bids to markouts, which we attribute to the fact that the auction is more of a common value type. In all settings, the relative order of the most frequent bidder performance remains the same, together with their absolute profits. Bids and markouts aggregated over long time intervals exhibit much higher correlation, indicating that bidders detect trends much better than identify when the high arbitrage value is exactly available. One possible explanation for this is the fact that the correlation between previous minute markouts and current minute bids is significant, suggesting that the previous minute markouts is used to predict the next minute value when bidding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18328v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akaki Mamageishvili, Christoph Schlegel, Ko Sunghun, Jinsuk Park, Ali Taslimi</dc:creator>
    </item>
    <item>
      <title>Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part B: Stochastic Stability in Weakly Acyclic Games</title>
      <link>https://arxiv.org/abs/2511.18418</link>
      <description>arXiv:2511.18418v1 Announce Type: new 
Abstract: Reinforcement-based learning dynamics may exhibit several limitations when applied in a distributed setup. In (repeatedly-played) multi-player/action strategic-form games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. Furthermore, strong convergence guarantees (i.e., almost sure convergence or weak convergence) are mostly restricted to two-player games. To address this main limitation of reinforcement-based learning in repeatedly-played strategic-form games, this paper introduces a novel payoff-based learning scheme for distributed optimization in multi-player/action strategic-form games. We present an extension of perturbed learning automata (PLA), namely aspiration-based perturbed learning automata (APLA), in which each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This paper is the second part of this study that analyzes stochastic stability in multi-player/action weakly-acyclic games in the presence of noisy observations. We provide conditions under which convergence is attained (in weak sense) to the set of pure Nash equilibria and payoff-dominant equilibria. To the best of our knowledge, this is the first reinforcement-based learning scheme that addresses convergence in weakly-acyclic games. Lastly, we provide a specialization of the results to the classical Stag-Hunt game, supported by a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18418v1</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios C. Chasparis</dc:creator>
    </item>
    <item>
      <title>Understanding Optimal Portfolios of Strategies for Solving Two-player Zero-sum Games</title>
      <link>https://arxiv.org/abs/2511.18658</link>
      <description>arXiv:2511.18658v1 Announce Type: new 
Abstract: In large-scale games, approximating the opponent's strategy space with a small portfolio of representative strategies is a common and powerful technique. However, the construction of these portfolios often relies on domain-specific knowledge or heuristics with no theoretical guarantees. This paper establishes a formal foundation for portfolio-based strategy approximation. We define the problem of finding an optimal portfolio in two-player zero-sum games and prove that this optimization problem is NP-hard. We demonstrate that several intuitive heuristics-such as using the support of a Nash Equilibrium or building portfolios incrementally - can lead to highly suboptimal solutions. These negative results underscore the problem's difficulty and motivate the need for robust, empirically-validated heuristics. To this end, we introduce an analytical framework to bound portfolio quality and propose a methodology for evaluating heuristic approaches. Our evaluation of several heuristics shows that their success heavily depends on the specific game being solved. Our code is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18658v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karolina Drabent, Ond\v{r}ej Kub\'i\v{c}ek, Viliam Lis\'y</dc:creator>
    </item>
    <item>
      <title>Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute</title>
      <link>https://arxiv.org/abs/2511.19225</link>
      <description>arXiv:2511.19225v1 Announce Type: new 
Abstract: We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19225v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordana Blazek, Frederick C. Harris Jr</dc:creator>
    </item>
    <item>
      <title>On Altruism and Spite in Bimatrix Games</title>
      <link>https://arxiv.org/abs/2511.19307</link>
      <description>arXiv:2511.19307v1 Announce Type: new 
Abstract: One common assumption in game theory is that any player optimizes a utility function that takes into account only its own payoff. However, it has long been observed that in real life players may adopt an altruistic or even spiteful behaviour. As such, there are numerous attempts in the economics literature that strive to explain the fact that players are not entirely selfish, but most of these works do not focus on the algorithmic implications of altruism or spite in games. In this paper, we relax the aforementioned ``self-interest'' assumption, and initiate the study of algorithmic aspects of bimatrix games -- such as the complexity and the quality of their (approximate) Nash equilibria -- under altruism or spite. We provide both a theoretical and an experimental treatment of these topics. Moreover, we demonstrate the potential for learning the degree of an opponent's altruistic/spiteful behaviour, and employing this for opponent selection and transfer of knowledge in bimatrix games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19307v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michail Fasoulakis, Leonidas Bakopoulos, Charilaos Akasiadis, Georgios Chalkiadakis</dc:creator>
    </item>
    <item>
      <title>Disc Game Dynamics: A Latent Space Perspective on Selection and Learning in Games</title>
      <link>https://arxiv.org/abs/2511.19346</link>
      <description>arXiv:2511.19346v1 Announce Type: new 
Abstract: Evolutionary game theory studies populations that change in response to an underlying game. Often, the functional form relating outcome to player attributes or strategy is complex, preventing mathematical progress. In this work, we axiomatically derive a latent space representation for pairwise, symmetric, zero-sum games by seeking a coordinate space in which the optimal training direction for an agent responding to an opponent depends only on their opponent's coordinates. The associated embedding represents the original game as a linear combination of copies of a simple game, the disc game, in a new coordinate space. In this article, we show that disc-game embedding is useful for studying learning dynamics. We demonstrate that a series of classical evolutionary processes simplify to constrained oscillator equations in the latent space. In particular, the continuous replicator equation reduces to a Hamiltonian system of coupled oscillators that exhibit Poincar\'e recurrence. This reduction allows exact, finite-dimensional closure when the underlying game is finite-rank, and optimal approximation otherwise. It also establishes an exact equivalence between the continuous replicator equation and adaptive dynamics in the transformed coordinates. By identifying a minimal rank representation, the disc game embedding offers numerical methods that could decouple the cost of simulation from the number of attributes used to define agents. These results generalize to metapopulation models that mix inhomogeneously, and to any time-differentiable dynamic where the rate of growth of a type, relative to its expected payout, is a nonnegative function of its frequency. We recommend disc-game embedding as an organizing paradigm for learning and selection in response to symmetric two-player zero-sum games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19346v1</guid>
      <category>cs.GT</category>
      <category>math.DS</category>
      <category>math.SP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Lechon-Alonso, Andrew Dennehy, Ruizheng Bai, Nicolas Sanchez, Derek K. Wise, David Sewell, David Rosenbluth, Alexander Strang</dc:creator>
    </item>
    <item>
      <title>Black-Box Lifting and Robustness Theorems for Multi-Agent Contracts</title>
      <link>https://arxiv.org/abs/2511.19358</link>
      <description>arXiv:2511.19358v1 Announce Type: new 
Abstract: Multi-agent contract design has largely evaluated contracts through the lens of pure Nash equilibria (PNE). This focus, however, is not without loss: In general, the principal can strictly gain by recommending a complex, possibly correlated, distribution over actions, while preserving incentive compatibility. In this work, we extend the analysis of multi-agent contracts beyond pure Nash equilibria to encompass more general equilibrium notions, including mixed Nash equilibria as well as (coarse-)correlated equilibria (CCE). The latter, in particular, captures the limiting outcome of agents engaged in learning dynamics.
  Our main result shows that for submodular and, more generally, XOS rewards, such complex recommendations yield at most a constant-factor gain: there exists a contract and a PNE whose utility is within a constant factor of the best CCE achievable by any contract. This provides a black-box lifting: results established against the best PNE automatically apply with respect to the best CCE, with only a constant factor loss. For submodular rewards, we further show how to transform a contract and a PNE of that contract into a new contract such that any of its CCEs gives a constant approximation to the PNE. This yields black-box robustness: up to constant factors, guarantees established for a specific contract and PNE automatically extend to the modified contract and any of its CCEs. We thus expand prior guarantees for multi-agent contracts and lower the barrier to new ones. As an important corollary, we obtain poly-time algorithms for submodular rewards that achieve constant approximations in any CCE, against the best CCE under the best contract. Such worst-case guarantees are provably unattainable for XOS rewards. Finally, we bound the gap between different equilibrium notions for subadditive, supermodular, and general rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19358v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul D\"utting, Tomer Ezra, Michal Feldman, Thomas Kesselheim</dc:creator>
    </item>
    <item>
      <title>Iterative Negotiation and Oversight: A Case Study in Decentralized Air Traffic Management</title>
      <link>https://arxiv.org/abs/2511.17625</link>
      <description>arXiv:2511.17625v1 Announce Type: cross 
Abstract: Achieving consensus among noncooperative agents remains challenging in decentralized multi-agent systems, where agents often have conflicting preferences. Existing coordination methods enable agents to reach consensus without a centralized coordinator, but do not provide formal guarantees on system-level objectives such as efficiency or fairness. To address this limitation, we propose an iterative negotiation and oversight framework that augments a decentralized negotiation mechanism with taxation-like oversight. The framework builds upon the trading auction for consensus, enabling noncooperative agents with conflicting preferences to negotiate through asset trading while preserving valuation privacy. We introduce an oversight mechanism, which implements a taxation-like intervention that guides decentralized negotiation toward system-efficient and equitable outcomes while also regulating how fast the framework converges. We establish theoretical guarantees of finite-time termination and derive bounds linking system efficiency and convergence rate to the level of central intervention. A case study based on the collaborative trajectory options program, a rerouting initiative in U.S. air traffic management, demonstrates that the framework can reliably achieve consensus among noncooperative airspace sector managers, and reveals how the level of intervention regulates the relationship between system efficiency and convergence speed. Taken together, the theoretical and experimental results indicate that the proposed framework provides a general mechanism for decentralized coordination in noncooperative multi-agent systems while safeguarding system-level objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17625v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaehan Im, John-Paul Clarke, Ufuk Topcu, David Fridovich-Keil</dc:creator>
    </item>
    <item>
      <title>Learning the Value of Value Learning</title>
      <link>https://arxiv.org/abs/2511.17714</link>
      <description>arXiv:2511.17714v1 Announce Type: cross 
Abstract: Standard decision frameworks addresses uncertainty about facts but assumes fixed values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yields Pareto-improving Nash bargains. These results show that a framework of rational choice can be extended to model value refinement and its associated benefits. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17714v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex John London, Aydin Mohseni</dc:creator>
    </item>
    <item>
      <title>Complete strategy spaces reveal hidden pathways to cooperation</title>
      <link>https://arxiv.org/abs/2511.17794</link>
      <description>arXiv:2511.17794v1 Announce Type: cross 
Abstract: Understanding how cooperation emerges and persists is a central challenge in evolutionary game theory. Existing models often rely on restricted, hand-picked strategy sets, which can overlook critical behavioural pathways. A recent four-strategy framework showed that cheap talk can promote cooperation through local interactions, yet it remained unclear whether modelled strategies might alter these conclusions. Here, we extend this framework to the complete set of eight strategies that naturally arise from communication and decision-making rules. We show that incorporating the full strategy space dramatically changes the evolutionary landscape. Cooperation becomes both more robust and more versatile, driven by novel pathways absent in the restricted model. In particular, we uncover a previously overlooked mechanism in which suspicious cooperation catalyses a cyclic dynamic that sustains cooperation. Conversely, the assumed role of strategic defection in the biased model is fragile, acting mainly as a spoiler rather than a genuine evolutionary attractor. The complete model further reveals a rich spectrum of long-term behaviours, including stable coexistence among up to seven strategies and time-varying patterns of partial coexistence. These results demonstrate that the full strategy space unlocks hidden routes to cooperative behaviour and highlight the importance of comprehensive modelling when explaining the emergence of cooperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17794v1</guid>
      <category>q-bio.PE</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhao Song, Ndidi Bianca Ogbo, Xinyu Wang, Chen Shen, Matjaz Perc, The Anh Han</dc:creator>
    </item>
    <item>
      <title>How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game</title>
      <link>https://arxiv.org/abs/2511.17990</link>
      <description>arXiv:2511.17990v1 Announce Type: cross 
Abstract: With the rapid advancement of Large Language Models (LLMs), recent studies have drawn attention to their potential for handling not only simple question-answer tasks but also more complex conversational abilities and performing human-like behavioral imitations. In particular, there is considerable interest in how accurately LLMs can reproduce real human emotions and behaviors, as well as whether such reproductions can function effectively in real-world scenarios. However, existing benchmarks focus primarily on knowledge-based assessment and thus fall short of sufficiently reflecting social interactions and strategic dialogue capabilities. To address these limitations, this work proposes a methodology to quantitatively evaluate the human emotional and behavioral imitation and strategic decision-making capabilities of LLMs by employing a Buy and Sell negotiation simulation. Specifically, we assign different personas to multiple LLMs and conduct negotiations between a Buyer and a Seller, comprehensively analyzing outcomes such as win rates, transaction prices, and SHAP values. Our experimental results show that models with higher existing benchmark scores tend to achieve better negotiation performance overall, although some models exhibit diminished performance in scenarios emphasizing emotional or social contexts. Moreover, competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits, suggesting that the assigned persona can lead to significant variations in negotiation strategies and results. Consequently, this study introduces a new evaluation approach for LLMs' social behavior imitation and dialogue strategies, and demonstrates how negotiation simulations can serve as a meaningful complementary metric to measure real-world interaction capabilities-an aspect often overlooked in existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17990v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Jeon, Jaeyoung Suh, Suwan Cho, Dohyeon Kim</dc:creator>
    </item>
    <item>
      <title>The Core in Max-Loss Non-Centroid Clustering Can Be Empty</title>
      <link>https://arxiv.org/abs/2511.19107</link>
      <description>arXiv:2511.19107v1 Announce Type: cross 
Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $\alpha$-core for any $\alpha&lt;2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19107v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Bredereck, Eva Deltl, Leon Kellerhals, Jannik Peters</dc:creator>
    </item>
    <item>
      <title>Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion</title>
      <link>https://arxiv.org/abs/2412.15707</link>
      <description>arXiv:2412.15707v2 Announce Type: replace 
Abstract: This paper examines whether widely used online learning algorithms in pricing can independently reach competitive outcomes or instead foster tacit collusion. This issue has drawn considerable attention from competition regulators as algorithmic pricing becomes more common in digital markets. Understanding when such algorithms lead to equilibrium prices or to supra-competitive prices is critical for buyers, sellers, and policymakers.
  We study the behavior of multi-armed bandit algorithms in repeated price competition. These algorithms only observe profits from the chosen prices, making them realistic models of automated pricing. Our formal analysis shows that an important class of online learning algorithms, called mean-based algorithms, reliably converges to Nash equilibrium in Bertrand competition. This finding is notable because, generally, online learning algorithms do not guarantee convergence. We also run extensive numerical experiments with different bandit algorithms, confirming that most widely used algorithms, including those not mean-based, converge to equilibrium. We observe supra-competitive prices only in specific cases where all sellers implement the same symmetric version of certain algorithms, such as UCB or Q-learning, and this effect diminishes as the number of competitors increases.
  Our results highlight that the risk of algorithmic collusion in competitive markets is often overstated. For most practical implementations of bandit algorithms, sellers' prices converge to competitive levels. Only under very specific and symmetric setups do prices remain above competitive benchmarks, and this effect diminishes with more competitors. These insights support regulators concerned with consumer welfare and managers considering algorithmic pricing tools. They suggest that while vigilance is warranted, fears of widespread algorithm-driven collusion may be exaggerated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15707v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Bichler, Julius Durmann, Matthias Oberlechner</dc:creator>
    </item>
    <item>
      <title>Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values</title>
      <link>https://arxiv.org/abs/2502.00313</link>
      <description>arXiv:2502.00313v2 Announce Type: replace 
Abstract: The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g., intentions or personas) or non-semantic prompting changes (e.g., templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00313v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Hosseini, Samarth Khanna</dc:creator>
    </item>
    <item>
      <title>Tight Bounds On the Distortion of Randomized and Deterministic Distributed Voting</title>
      <link>https://arxiv.org/abs/2509.17134</link>
      <description>arXiv:2509.17134v2 Announce Type: replace 
Abstract: We study metric distortion in distributed voting, where $n$ voters are partitioned into $k$ groups, each selecting a local representative, and a final winner is chosen from these representatives (or from the entire set of candidates). This setting models systems like U.S. presidential elections, where state-level decisions determine the national outcome. We focus on four cost objectives from \citep{anshelevich2022distortion}: $\avgavg$, $\avgmax$, $\maxavg$, and $\maxmax$. We present improved distortion bounds for both deterministic and randomized mechanisms, offering a near-complete characterization of distortion in this model.
  For deterministic mechanisms, we reduce the upper bound for $\avgmax$ from $11$ to $7$, establish a tight lower bound of $5$ for $\maxavg$ (improving on $2+\sqrt{5}$), and tighten the upper bound for $\maxmax$ from $5$ to $3$.
  For randomized mechanisms, we consider two settings: (i) only the second stage is randomized, and (ii) both stages may be randomized. In case (i), we prove tight bounds: $5\!-\!2/k$ for $\avgavg$, $3$ for $\avgmax$ and $\maxmax$, and $5$ for $\maxavg$. In case (ii), we show tight bounds of $3$ for $\maxavg$ and $\maxmax$, and nearly tight bounds for $\avgavg$ and $\avgmax$ within $[3\!-\!2/n,\ 3\!-\!2/(kn^*)]$ and $[3\!-\!2/n,\ 3]$, respectively, where $n^*$ denotes the largest group size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17134v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ali Abam, Davoud Kareshki, Marzieh Nilipour, Mohammad Hossein Paydar, Masoud Seddighin</dc:creator>
    </item>
    <item>
      <title>Transforming Strategic Games into Biform Games: Applications in Allocation Mechanisms and Green Technology Investment</title>
      <link>https://arxiv.org/abs/2511.06858</link>
      <description>arXiv:2511.06858v2 Announce Type: replace 
Abstract: As Aumann stated, cooperation and non-cooperation are different ways of viewing the same game, with the main difference being whether players can reach a binding cooperative agreement. In the real world, many games often coexist competition and cooperation. Based on the above reasons, we propose a method to transform strategic games into a biform game model, which retains the characteristics of cooperative games while considering the ultimate goal of players to maximize their own interests. Furthermore, based on this biform game model, we analyze the impact of two different distribution methods, namely marginalism and egalitarianism, on the game results. As an application, we analyze how food producers seek maximum profits through cooperative pricing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06858v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Shuwen, Luo Enquan, Yang Yanlong</dc:creator>
    </item>
    <item>
      <title>Learning Mean Field Control on Sparse Graphs</title>
      <link>https://arxiv.org/abs/2501.17079</link>
      <description>arXiv:2501.17079v2 Announce Type: replace-cross 
Abstract: Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17079v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Fabian, Kai Cui, Heinz Koeppl</dc:creator>
    </item>
    <item>
      <title>ShortageSim: Simulating Drug Shortages under Information Asymmetry</title>
      <link>https://arxiv.org/abs/2509.01813</link>
      <description>arXiv:2509.01813v2 Announce Type: replace-cross 
Abstract: Drug shortages pose critical risks to patient care and healthcare systems worldwide, yet the effectiveness of regulatory interventions remains poorly understood due to information asymmetries in pharmaceutical supply chains. We propose ShortageSim, which addresses this challenge by providing the first simulation framework that evaluates the impact of regulatory interventions on competition dynamics under information asymmetry. Using Large Language Model (LLM)-based agents, the framework models the strategic decisions of drug manufacturers and institutional buyers in response to shortage alerts given by the regulatory agency. Unlike traditional game theory models that assume perfect rationality and complete information, \name simulates heterogeneous interpretations on regulatory announcements and the resulting decisions. Experiments on a self-processed dataset of historical shortage events show that \name reduces the resolution lag for production disruption cases by up to 84\%, achieving closer alignment to real-world trajectories than the zero-shot baseline. Our framework confirms the effect of regulatory alert in addressing shortages and introduces a new method for understanding competition in multi-stage environments under uncertainty. We open-source \name and a dataset of 2,925 FDA shortage events in https://github.com/Lemutisme/ShortageSim, providing a novel framework for future research on policy design and testing in supply chains under information asymmetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01813v2</guid>
      <category>cs.MA</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxuan Cui, Yilan Jiang, Duo Zhou, Cheng Qian, Yuji Zhang, Qiong Wang</dc:creator>
    </item>
  </channel>
</rss>
