<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Extractive Schema Linking for Text-to-SQL</title>
      <link>https://arxiv.org/abs/2501.17174</link>
      <description>arXiv:2501.17174v1 Announce Type: new 
Abstract: Text-to-SQL is emerging as a practical interface for real world databases. The dominant paradigm for Text-to-SQL is cross-database or schema-independent, supporting application schemas unseen during training. The schema of a database defines the tables, columns, column types and foreign key connections between tables. Real world schemas can be large, containing hundreds of columns, but for any particular query only a small fraction will be relevant. Placing the entire schema in the prompt for an LLM can be impossible for models with smaller token windows and expensive even when the context window is large enough to allow it. Even apart from computational considerations, the accuracy of the model can be improved by focusing the SQL generation on only the relevant portion of the database. Schema linking identifies the portion of the database schema useful for the question. Previous work on schema linking has used graph neural networks, generative LLMs, and cross encoder classifiers. We introduce a new approach to adapt decoder-only LLMs to schema linking that is both computationally more efficient and more accurate than the generative approach. Additionally our extractive approach permits fine-grained control over the precision-recall trade-off for schema linking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17174v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Glass, Mustafa Eyceoz, Dharmashankar Subramanian, Gaetano Rossiello, Long Vu, Alfio Gliozzo</dc:creator>
    </item>
    <item>
      <title>Reqo: A Robust and Explainable Query Optimization Cost Model</title>
      <link>https://arxiv.org/abs/2501.17414</link>
      <description>arXiv:2501.17414v1 Announce Type: new 
Abstract: In recent years, there has been a growing interest in using machine learning (ML) in query optimization to select more efficient plans. Existing learning-based query optimizers use certain model architectures to convert tree-structured query plans into representations suitable for downstream ML tasks. As the design of these architectures significantly impacts cost estimation, we propose a tree model architecture based on Bidirectional Graph Neural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve more accurate cost estimates. The inherent uncertainty of data and model parameters also leads to inaccurate cost estimates, resulting in suboptimal plans and less robust query performance. To address this, we implement a novel learning-to-rank cost model that effectively quantifies the uncertainty in cost estimates using approximate probabilistic ML. This model adaptively integrates quantified uncertainty with estimated costs and learns from comparing pairwise plans, achieving more robust performance. In addition, we propose the first explainability technique specifically designed for learning-based cost models. This technique explains the contribution of any subgraphs in the query plan to the final predicted cost, which can be integrated and trained with any learning-based cost model to significantly boost the model's explainability. By incorporating these innovations, we propose a cost model for a Robust and Explainable Query Optimizer, Reqo, that improves the accuracy, robustness, and explainability of cost estimation, outperforming state-of-the-art approaches in all three dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17414v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baoming Chang, Amin Kamali, Verena Kantere</dc:creator>
    </item>
    <item>
      <title>Comprehensive Evaluation for a Large Scale Knowledge Graph Question Answering Service</title>
      <link>https://arxiv.org/abs/2501.17270</link>
      <description>arXiv:2501.17270v1 Announce Type: cross 
Abstract: Question answering systems for knowledge graph (KGQA), answer factoid questions based on the data in the knowledge graph. KGQA systems are complex because the system has to understand the relations and entities in the knowledge-seeking natural language queries and map them to structured queries against the KG to answer them. In this paper, we introduce Chronos, a comprehensive evaluation framework for KGQA at industry scale. It is designed to evaluate such a multi-component system comprehensively, focusing on (1) end-to-end and component-level metrics, (2) scalable to diverse datasets and (3) a scalable approach to measure the performance of the system prior to release. In this paper, we discuss the unique challenges associated with evaluating KGQA systems at industry scale, review the design of Chronos, and how it addresses these challenges. We will demonstrate how it provides a base for data-driven decisions and discuss the challenges of using it to measure and improve a real-world KGQA system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17270v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saloni Potdar, Daniel Lee, Omar Attia, Varun Embar, De Meng, Ramesh Balaji, Chloe Seivwright, Eric Choi, Mina H. Farid, Yiwen Sun, Yunyao Li</dc:creator>
    </item>
    <item>
      <title>Stable Tree Labelling for Accelerating Distance Queries on Dynamic Road Networks</title>
      <link>https://arxiv.org/abs/2501.17379</link>
      <description>arXiv:2501.17379v1 Announce Type: cross 
Abstract: Finding the shortest-path distance between two arbitrary vertices is an important problem in road networks. Due to real-time traffic conditions, road networks undergo dynamic changes all the time. Current state-of-the-art methods incrementally maintain a distance labelling based on a hierarchy among vertices to support efficient distance computation. However, their labelling sizes are often large and cannot be efficiently maintained. To combat these issues, we present a simple yet efficient labelling method, namely \emph{Stable Tree Labelling} (STL), for answering distance queries on dynamic road networks. We observe that the properties of an underlying hierarchy play an important role in improving and balancing query and update performance. Thus, we introduce the notion of \emph{stable tree hierarchy} which lays the ground for developing efficient maintenance algorithms on dynamic road networks. Based on stable tree hierarchy, STL can be efficiently constructed as a 2-hop labelling. A crucial ingredient of STL is to only store distances within subgraphs in labels, rather than distances in the entire graph, which restricts the labels affected by dynamic changes. We further develop two efficient maintenance algorithms upon STL: \emph{Label Search algorithm} and \emph{Pareto Search algorithm}. Label Search algorithm identifies affected ancestors in a stable tree hierarchy and performs efficient searches to update labels from those ancestors. Pareto Search algorithm explores the interaction between search spaces of different ancestors, and combines searches from multiple ancestors into only two searches for each update, eliminating duplicate graph traversals. The experiments show that our algorithms significantly outperform state-of-the-art dynamic methods in maintaining the labelling and query processing, while requiring an order of magnitude less space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17379v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henning Koehler, Muhammad Farhan, Qing Wang</dc:creator>
    </item>
    <item>
      <title>Matrix Product Sketching via Coordinated Sampling</title>
      <link>https://arxiv.org/abs/2501.17836</link>
      <description>arXiv:2501.17836v1 Announce Type: cross 
Abstract: We revisit the well-studied problem of approximating a matrix product, $\mathbf{A}^T\mathbf{B}$, based on small space sketches $\mathcal{S}(\mathbf{A})$ and $\mathcal{S}(\mathbf{B})$ of $\mathbf{A} \in \R^{n \times d}$ and $\mathbf{B}\in \R^{n \times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\mathbf{A}$ and $\mathbf{B}$ are sparse, methods based on \emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\epsilon\|\mathbf{A}\|_F\|\mathbf{B}\|_F$, coordinated sampling requires sketches of size $O(s/\epsilon^2)$ when $\mathbf{A}$ and $\mathbf{B}$ have at most $s \leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\epsilon^2)$ and $O(m/\epsilon^2)$ for $\mathbf{A}$ and $\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17836v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid Daliri, Juliana Freire, Danrong Li, Christopher Musco</dc:creator>
    </item>
    <item>
      <title>AMIDER: A Multidisciplinary Research Database and Its Application to Promote Open Science</title>
      <link>https://arxiv.org/abs/2408.02246</link>
      <description>arXiv:2408.02246v3 Announce Type: replace 
Abstract: The AMIDER, Advanced Multidisciplinary Integrated-Database for Exploring new Research, is a newly developed research data catalog to demonstrate an advanced database application. AMIDER is characterized as a multidisciplinary database equipped with a user-friendly web application. Its catalog view displays diverse research data at once beyond any limitation of each individual discipline. Some useful functions, such as a selectable data download, data format conversion, and display of data visual information, are also implemented. Further advanced functions, such as visualization of dataset mutual relationship, are also implemented as a preliminary trial. These characteristics and functions are expected to enhance the accessibility to individual research data, even from non-expertized users, and be helpful for collaborations among diverse scientific fields beyond individual disciplines. Multidisciplinary data management is also one of AMIDER's uniqueness, where various metadata schemas can be mapped to a uniform metadata table, and standardized and self-describing data formats are adopted. AMIDER website (https://amider.rois.ac.jp/) had been launched in April 2024. As of July 2024, over 15,000 metadata in various research fields of polar science have been registered in the database, and approximately 500 visitors are viewing the website every day on average. Expansion of the database to further multidisciplinary scientific fields, not only polar science, is planned, and advanced attempts, such as applying Natural Language Processing (NLP) to metadata, have also been considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02246v3</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masayoshi Kozai (Polar Environment Data Science Center, Joint Support-Center for Data Science Research, Research Organization of Information and Systems, Tachikawa, Japan), Yoshimasa Tanaka (Polar Environment Data Science Center, Joint Support-Center for Data Science Research, Research Organization of Information and Systems, Tachikawa, Japan, National Institute of Polar Research, Research Organization of Information and Systems, Tachikawa, Japan), Shuji Abe (International Research Center for Space and Planetary Environmental Science, Kyushu University, Fukuoka, Japan), Yasuyuki Minamiyama (Research Center for Open Science and Data Platform, National Institute of Informatics, Research Organization of Information and Systems, Tokyo, Japan), Atsuki Shinbori (Institute for Space and Earth Environmental Research Center for Integrated Data Science, Nagoya University, Nagoya, Japan), Akira Kadokura (Polar Environment Data Science Center, Joint Support-Center for Data Science Research, Research Organization of Information and Systems, Tachikawa, Japan)</dc:creator>
    </item>
    <item>
      <title>Galley: Modern Query Optimization for Sparse Tensor Programs</title>
      <link>https://arxiv.org/abs/2408.14706</link>
      <description>arXiv:2408.14706v4 Announce Type: replace 
Abstract: The tensor programming abstraction has become a foundational paradigm for modern computing. This framework allows users to write high performance programs for bulk computation via a high-level imperative interface. Recent work has extended this paradigm to sparse tensors (i.e. tensors where most entries are not explicitly represented) with the use of sparse tensor compilers. These systems excel at producing efficient code for computation over sparse tensors, which may be stored in a wide variety of formats. However, they require the user to manually choose the order of operations and the data formats at every step. Unfortunately, these decisions are both highly impactful and complicated, requiring significant effort to manually optimize. In this work, we present Galley, a system for declarative sparse tensor programming. Galley performs cost-based optimization to lower these programs to a logical plan then to a physical plan. It then leverages sparse tensor compilers to execute the physical plan efficiently. We show that Galley achieves high performance on a wide variety of problems including machine learning algorithms, subgraph counting, and iterative graph algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14706v4</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Deeds, Willow Ahrens, Magda Balazinska, Dan Suciu</dc:creator>
    </item>
    <item>
      <title>Equi join query acceleration using algebraic signatures (Published at IADIS'2008 Applied Computing conf.)</title>
      <link>https://arxiv.org/abs/2411.18183</link>
      <description>arXiv:2411.18183v3 Announce Type: replace 
Abstract: Evaluation of join queries is very challenging since they have to deal with an increasing data size. We study the relational join query processing realized by hash tables and we focus on the case of equi join queries. We propose to use a new form of signatures, the algebraic signatures, for fast comparison between values of two attributes in relations participating in an equi join operations. Our technique is efficient especially when the attribute join is a long string. In this paper, we investigate this issue and prove that algebraic signatures combined to known hash join technique constitute an efficient method to accelerate equi join operations. Algebraic signatures allow fast string search. They are descending from the Karp-Rabin signatures. String matching using our algebraic calculus is then several times faster comparing to the fastest known methods, e.g. Boyer Moore.We justify our approach and present an experimental evaluation. We also present a cost analysis for an equi join operation using algebraic signatures. The performance evaluation of our technique shows the improvement of query processing times. We also discuss the reductions of required memory sizes and the disk I/O. The main contribution of this paper is the using of algebraic signatures to accelerate equi join operations especially when the attribute join is a long string and to avoid multiples I/O disk by reduce memory requirement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18183v3</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IADIS Applied Computing, international association for the development of the information society, Apr 2008, algarve, Portugal</arxiv:journal_reference>
      <dc:creator>Riad Mokadem (IRIT-PYRAMIDE, IRIT), Abdelkader Hameurlain (IRIT-PYRAMIDE), Franck Morvan (IRIT-PYRAMIDE, IRIT)</dc:creator>
    </item>
    <item>
      <title>Auditing Yelp's Business Ranking and Review Recommendation Through the Lens of Fairness</title>
      <link>https://arxiv.org/abs/2308.02129</link>
      <description>arXiv:2308.02129v2 Announce Type: replace-cross 
Abstract: Auditing is critical to ensuring the fairness and reliability of decision-making systems. However, auditing a black-box system for bias can be challenging due to the lack of transparency in the model's internal workings. In many web applications, such as Yelp, it is challenging, if not impossible, to manipulate their inputs systematically to identify bias in the output. Yelp connects users and businesses, where users identify new businesses and simultaneously express their experiences through reviews. Yelp recommendation software moderates user-provided content by categorizing it into recommended and not-recommended sections. The recommended reviews, among other attributes, are used by Yelp's ranking algorithm to rank businesses in a neighborhood. Due to Yelp's substantial popularity and its high impact on local businesses' success, understanding the bias of its algorithms is crucial.
  This data-driven study, for the first time, investigates the bias of Yelp's business ranking and review recommendation system. We examine three hypotheses to assess if Yelp's recommendation software shows bias against reviews of less established users with fewer friends and reviews and if Yelp's business ranking algorithm shows bias against restaurants located in specific neighborhoods, particularly in hotspot regions, with specific demographic compositions. Our findings show that reviews of less-established users are disproportionately categorized as not-recommended. We also find a positive association between restaurants' location in hotspot regions and their average exposure. Furthermore, we observed some cases of severe disparity bias in cities where the hotspots are in neighborhoods with less demographic diversity or higher affluence and education levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02129v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Singhal, Javier Pacheco, Seyyed Mohammad Sadegh Moosavi Khorzooghi, Tanushree Debi, Abolfazl Asudeh, Gautam Das, Shirin Nilizadeh</dc:creator>
    </item>
  </channel>
</rss>
