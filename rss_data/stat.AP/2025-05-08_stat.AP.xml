<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 01:42:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Disentangling Complex Systems: IdopNetwork Meets GLMY Homology Theory</title>
      <link>https://arxiv.org/abs/2505.04140</link>
      <description>arXiv:2505.04140v1 Announce Type: new 
Abstract: The study of complex systems has captured widespread attention in recent years, emphasizing the exploration of interactions and emergent properties among system units. Network analysis based on graph theory has emerged as a powerful approach for analyzing network topology and functions, making them widely adopted in complex systems. IdopNetwork is an advanced statistical physics framework that constructs the interaction within complex systems by integrating large-scale omics data. By combining GLMY theory, the structural characteristics of the network topology can be traced, providing deeper insights into the dynamic evolution of the network. This combination not only offers a novel perspective for dissecting the internal regulation of complex systems from a holistic standpoint but also provides significant support for applied fields such as data science, complex disease, and materials science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04140v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuang Wu, Mengmeng Zhang</dc:creator>
    </item>
    <item>
      <title>Forecasting UK Consumer Price Inflation with RaGNAR: Random Generalised Network Autoregressive Processes</title>
      <link>https://arxiv.org/abs/2505.04423</link>
      <description>arXiv:2505.04423v1 Announce Type: new 
Abstract: This article forecasts CPI inflation in the United Kingdom using Random Generalised Network Autoregressive (RaGNAR) Processes. More specifically, we fit Generalised Network Autoregressive (GNAR) Processes to a large set of random networks generated according to the Erd\H{o}s-R\'enyi-Gilbert model and select the best-performing networks each month to compute out-of-sample forecasts. RaGNAR significantly outperforms traditional benchmark models across all horizons. Remarkably, RaGNAR also delivers materially more accurate predictions than the Bank of Englan's four to six month inflation rate forecasts published in their quarterly Monetary Policy Reports. Our results are remarkable not only for their accuracy, but also because of their speed, efficiency and simplicity compared to the Bank's current forecasting processes. RaGNAR's performance improvements manifest both in terms of their root mean squared error and mean absolute percentage error, which measure different, but crucial, aspects of the methods' performance. GNAR processes demonstrably predict future changes to CPI inflation more accurately and quickly than the benchmark models, especially at medium- to long-term forecast horizons, which is of great importance to policymakers charged with setting interest rates. We find that the most robust forecasts are those which combine the predictions from multiple GNAR processes via the use of various model averaging techniques. By analysing the structure of the best-performing graphs, we are also able to identify the key components that influence inflation rates during different periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04423v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy P. Nason, Henry Antonio Palasciano</dc:creator>
    </item>
    <item>
      <title>Bayesian Estimation of Extreme Quantiles and the Exceedance Distribution for Paretian Tails</title>
      <link>https://arxiv.org/abs/2505.04501</link>
      <description>arXiv:2505.04501v1 Announce Type: new 
Abstract: Estimating extreme quantiles is an important task in many applications, including financial risk management and climatology. More important than estimating the quantile itself is to insure zero coverage error, which implies the quantile estimate should, on average, reflect the desired probability of exceedance. In this research, we show that for unconditional distributions isomorphic to the exponential, a Bayesian quantile estimate results in zero coverage error. This compares to the traditional maximum likelihood method, where the coverage error can be significant under small sample sizes even though the quantile estimate is unbiased. More generally, we prove a sufficient condition for an unbiased quantile estimator to result in coverage error. Interestingly, our results hold by virtue of using a Jeffreys prior for the unknown parameters and is independent of the true prior. We also derive an expression for the distribution, and moments, of future exceedances which is vital for risk assessment. We extend our results to the conditional tail of distributions with asymptotic Paretian tails and, in particular, those in the Fr\'echet maximum domain of attraction. We illustrate our results using simulations for a variety of light and heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04501v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Douglas E. Johnston</dc:creator>
    </item>
    <item>
      <title>On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation</title>
      <link>https://arxiv.org/abs/2505.03757</link>
      <description>arXiv:2505.03757v1 Announce Type: cross 
Abstract: Coordinate transformation models often fail to account for nonlinear and spatially dependent distortions, leading to significant residual errors in geospatial applications. Here we propose a residual-based neural correction strategy, in which a neural network learns to model only the systematic distortions left by an initial geometric transformation. By focusing solely on residual patterns, the proposed method reduces model complexity and improves performance, particularly in scenarios with sparse or structured control point configurations. We evaluate the method using both simulated datasets with varying distortion intensities and sampling strategies, as well as under the real-world image georeferencing tasks. Compared with direct neural network coordinate converter and classical transformation models, the residual-based neural correction delivers more accurate and stable results under challenging conditions, while maintaining comparable performance in ideal cases. These findings demonstrate the effectiveness of residual modelling as a lightweight and robust alternative for improving coordinate transformation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03757v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinicius Francisco Rofatto, Luiz Felipe Rodrigues de Almeida, Marcelo Tomio Matsuoka, Ivandro Klein, Mauricio Roberto Veronez, Luiz Gonzaga Da Silveira Junior</dc:creator>
    </item>
    <item>
      <title>Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model</title>
      <link>https://arxiv.org/abs/2505.03893</link>
      <description>arXiv:2505.03893v1 Announce Type: cross 
Abstract: Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approach's clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03893v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muyan Jiang, Yunkai Zhang, Anil Aswani</dc:creator>
    </item>
    <item>
      <title>Evaluating Performance Consistency in Competitive Programming: Educational Implications and Contest Design Insights</title>
      <link>https://arxiv.org/abs/2505.04143</link>
      <description>arXiv:2505.04143v1 Announce Type: cross 
Abstract: Competitive programming (CP) contests are often treated as interchangeable proxies for algorithmic skill, yet the extent to which results at lower contest tiers anticipate performance at higher tiers, and how closely any tier resembles the ubiquitous online-contest circuit, remains unclear. We analyze ten years (2015--2024) of International Collegiate Programming Contest (ICPC) standings, comprising five long-running superregional championships (Africa \&amp; Arab, Asia East, Asia West, North America, and Northern Eurasia), associated local regionals of North America and Northern Eurasia, and the World Finals. For 366 World Finalist teams (2021--2024) we augment the dataset with pre-contest Codeforces ratings. Pairwise rank alignment is measured with Kendall's $\tau$.
  Overall, superregional ranks predict World Final ranks only moderately (weighted $\tau=0.407$), but regional-to-superregional consistency varies widely: Northern Eurasia exhibits the strongest alignment ($\tau=0.521$) while Asia West exhibits the weakest ($\tau=0.188$). Internal consistency within a region can exceed its predictive value for Worlds -- e.g., Northern Eurasia and North America regionals vs. superregionals ($\tau=0.666$ and $\tau=0.577$, respectively). Codeforces ratings correlate more strongly with World Final results ($\tau=0.596$) than any single ICPC tier, suggesting that high-frequency online contests capture decisive skill factors that many superregional sets miss.
  We argue that contest organizers can improve both fairness and pedagogical value by aligning problem style and selection rules with the formats that demonstrably differentiate teams, in particular the Northern-Eurasian model and well-curated online rounds. All data, scripts, and additional analyses are publicly released to facilitate replication and further study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04143v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongtang Luo, Ethan Dickey</dc:creator>
    </item>
    <item>
      <title>Bayesian Approaches to Collaborative Data Analysis with Strict Privacy Restrictions</title>
      <link>https://arxiv.org/abs/2404.14895</link>
      <description>arXiv:2404.14895v2 Announce Type: replace 
Abstract: Collaborative data analysis between countries is crucial for enabling fast responses to increasingly multi-country disease outbreaks. Often, data early in outbreaks are of sensitive nature and subject to strict privacy restrictions. Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as a novel approach solving issues around data privacy and confidentiality. In the present study, we propose two approaches to federated analysis, based on simple Bayesian statistics and exploit this simplicity to make them feasible for rapid collaboration without the risks of data leaks and data reidentification, as they require neither data sharing nor direct communication between devices. The first approach uses summaries from parameters' posteriors previously obtained at a different location to update truncated normal distributions approximating priors of a new model. The second approach uses the entire previously sampled posterior, approximating via a multivariate normal distribution. We test these models on simulated and on real outbreak data to estimate the incubation period of infectious diseases. Results indicate that both approaches can recover incubation period parameters accurately, but they differ in terms of inferential capacity. The posterior summary approach shows higher stability and precision, but it cannot capture posterior correlations, meaning it is inferentially limited. The whole posterior approach can capture correlations, but it shows less stability, and its applicability is limited to fewer prior distributions. We discuss results in terms of the advantages of their simplicity and privacy-preserving properties, and in terms of their limited generalisability to more complex analytical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14895v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Busch-Moreno, Moritz U. G. Kraemer</dc:creator>
    </item>
    <item>
      <title>Machine learning accelerates fuel cell life testing</title>
      <link>https://arxiv.org/abs/2504.18835</link>
      <description>arXiv:2504.18835v2 Announce Type: replace 
Abstract: Accelerated life testing (ALT) can significantly reduce the economic, time, and labor costs of life testing in the process of equipment, device, and material research and development (R&amp;D), and improve R&amp;D efficiency. This paper proposes a performance characterization data prediction (PCDP) method and a life prediction-driven ALT (LP-ALT) method to accelerate the life test of polymer electrolyte membrane fuel cells (PEMFCs). The PCDP method can accurately predict different PCD using only four impedances (real and imaginary) corresponding to a high frequency and a medium frequency, greatly shortening the measurement time of offline PCD and reducing the difficulty of life testing. The test results on an open source life test dataset containing 42 PEMFCs show that compared with the determination coefficient (R^2) results of predicted aging indicators, including limiting current, total mass transport resistance, electrochemically active surface area, and crossover current, obtained based on the measured PCD, the R^2 results of predicted aging indicators based on the predicted PCD is only reduced by 0.04, 0.01, 0.05, and 0.06, respectively. The LP-ALT method can shorten the life test time through early life prediction. Test results on the same open-source life test dataset of PEMFCs show that the acceleration ratio of the LP-ALT method can reach 30 times under the premise of ensuring that the minimum R^2 of the prediction results of different aging indicators, including limiting current, total mass transport resistance, and electrochemically active surface area, is not less than 0.89. Combining the different performance characterization data predicted by the PCDP method and the life prediction of the LP-ALT method, the diagnosis and prognosis of PEMFCs and their components can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18835v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbin Zhao, Hao Liu, Zhihua Deng, Haoyi Jiang, Zhenfei Ling, Zhiyang Liu, Xingkai Wang, Tong Li, Xiaoping Ouyang</dc:creator>
    </item>
    <item>
      <title>A Computationally Efficient Approach to False Discovery Rate Control and Power Maximisation via Randomisation and Mirror Statistic</title>
      <link>https://arxiv.org/abs/2401.12697</link>
      <description>arXiv:2401.12697v2 Announce Type: replace-cross 
Abstract: Simultaneously performing variable selection and inference in high-dimensional regression models is an open challenge in statistics and machine learning. The increasing availability of vast amounts of variables requires the adoption of specific statistical procedures to accurately select the most important predictors in a high-dimensional space, while controlling the false discovery rate (FDR) associated with the variable selection procedure. In this paper, we propose the joint adoption of the Mirror Statistic approach to FDR control, coupled with outcome randomisation to maximise the statistical power of the variable selection procedure, measured through the true positive rate. Through extensive simulations, we show how our proposed strategy allows us to combine the benefits of the two techniques. The Mirror Statistic is a flexible method to control FDR, which only requires mild model assumptions, but requires two sets of independent regression coefficient estimates, usually obtained after splitting the original dataset. Outcome randomisation is an alternative to data splitting that allows to generate two independent outcomes, which can then be used to estimate the coefficients that go into the construction of the Mirror Statistic. The combination of these two approaches provides increased testing power in a number of scenarios, such as highly correlated covariates and high percentages of active variables. Moreover, it is scalable to very high-dimensional problems, since the algorithm has a low memory footprint and only requires a single run on the full dataset, as opposed to iterative alternatives such as multiple data splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12697v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/09622802251329768</arxiv:DOI>
      <arxiv:journal_reference>Statistical Methods in Medical Research. 2025;0(0)</arxiv:journal_reference>
      <dc:creator>Marco Molinari, Magne Thoresen</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Networks with Conditional Dynamics in Edge Addition and Deletion</title>
      <link>https://arxiv.org/abs/2409.08965</link>
      <description>arXiv:2409.08965v2 Announce Type: replace-cross 
Abstract: This study presents a dynamic Bayesian network framework that facilitates intuitive gradual edge changes. We use two conditional dynamics to model the edge addition and deletion, and edge selection separately. Unlike previous research that uses a mixture network approach, which restricts the number of possible edge changes, or structural priors to induce gradual changes, which can lead to unclear network evolution, our model induces more frequent and intuitive edge change dynamics. We employ Markov chain Monte Carlo (MCMC) sampling to estimate the model structures and parameters and demonstrate the model's effectiveness in a portfolio selection application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08965v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lupe S. H. Chan, Amanda M. Y. Chu, Mike K. P. So</dc:creator>
    </item>
    <item>
      <title>DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for Spectrophotometric Calibration</title>
      <link>https://arxiv.org/abs/2412.08809</link>
      <description>arXiv:2412.08809v2 Announce Type: replace-cross 
Abstract: We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky faint DA white dwarf (DA WD) spectrophotometric standards ($16.5 &lt; V &lt; 19.5$) alongside three CALSPEC standards, from 912 \r{A} to 32 $\mu$m. The framework is the first of its kind to jointly infer photometric zeropoints and WD parameters (surface gravity $\log g$, effective temperature $T_{\text{eff}}$, extinction $A_V$, dust relation parameter $R_V$) by simultaneously modelling both photometric and spectroscopic data. We model panchromatic Hubble Space Telescope Wide Field Camera 3 (HST/WFC3) UVIS and IR photometry, HST/STIS UV spectroscopy and ground-based optical spectroscopy to sub-percent precision. Photometric residuals for the sample are the lowest yet yielding $&lt;0.004$ mag RMS on average from the UV to the NIR, achieved by jointly inferring time-dependent changes in system sensitivity and WFC3/IR count-rate nonlinearity. Our GPU-accelerated implementation enables efficient sampling via Hamiltonian Monte Carlo, critical for exploring the high-dimensional posterior space. The hierarchical nature of the model enables population analysis of intrinsic WD and dust parameters. Inferred spectral energy distributions from this model will be essential for calibrating the James Webb Space Telescope as well as next-generation surveys, including Vera Rubin Observatory's Legacy Survey of Space and Time and the Nancy Grace Roman Space Telescope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08809v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin M. Boyd, Gautham Narayan, Kaisey S. Mandel, Matthew Grayling, Abhijit Saha, Tim Axelrod, Thomas Matheson, Edward W. Olszewski, Annalisa Calamida, Aaron Do, Ralph C. Bohlin, Jay B. Holberg, Ivan Hubeny, Susana Deustua, Armin Rest, Christopher W. Stubbs, Aidan Berres, Mai Li, John W. Mackenty, Elena Sabbi</dc:creator>
    </item>
    <item>
      <title>A Sloppy approach to QSP: XAI enabling fit-for-purpose models</title>
      <link>https://arxiv.org/abs/2505.02750</link>
      <description>arXiv:2505.02750v2 Announce Type: replace-cross 
Abstract: Quantitative Systems Pharmacology (QSP) promises to accelerate drug development, enable personalized medicine, and improve the predictability of clinical outcomes. Realizing its full potential depends on effectively managing the complexity of the underlying mathematical models and biological systems. Here, we present and validate a novel QSP workflow grounded in the principles of sloppy modeling, offering a practical and principled strategy for building and deploying models in a QSP pipeline. Our approach begins with a literature-derived model, constructed to be as comprehensive and unbiased as possible by drawing from the collective knowledge of prior research. At the core of the workflow is the Manifold Boundary Approximation Method (MBAM), which simplifies models while preserving their predictive capacity and mechanistic interpretability. Applying MBAM as a context-specific model reduction strategy, we link the simplified representation directly to the downstream predictions of interest. The resulting reduced models are computationally efficient and well-suited to key QSP tasks, including virtual population generation, experimental design, and target discovery. We demonstrate the utility of this workflow through case studies involving the coagulation cascade and SHIV infection. Our analysis suggests several promising next steps for improving the efficacy of bNAb therapies in HIV infected patients within the context of a general-purpose QSP modeling workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02750v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.MN</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah DeTal, Christian N. K. Anderson, Mark K. Transtrum</dc:creator>
    </item>
  </channel>
</rss>
