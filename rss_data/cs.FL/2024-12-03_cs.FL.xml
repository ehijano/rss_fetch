<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>H\"older equivalence of a class of Bara\'nski carpets</title>
      <link>https://arxiv.org/abs/2412.00694</link>
      <description>arXiv:2412.00694v1 Announce Type: cross 
Abstract: The study of Lipschitz equivalence of fractals is a very active topic in recent years, but there are very few results on non-totally disconnected fractals. In this paper, we use a class of finite state automata, called feasible $\Sigma$-automata, to construct pseudo-metric spaces, and then apply them to the classification of self-affine sets. We first recall a notion of neighbor automaton, and we show that an neighbor automaton satisfying the finite type condition is a feasible $\Sigma$-automaton. Secondly, we construct a universal map to show that pseudo-metric spaces induced by different automata can be bi-Lipschitz equivalent. As an application, we obtain a rather general sufficient condition for Bara\'nski carpets to be Lipschitz equivalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00694v1</guid>
      <category>math.MG</category>
      <category>cs.FL</category>
      <category>math.GT</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunjie Zhu, Liang-yi Huang</dc:creator>
    </item>
    <item>
      <title>Learning Elementary Cellular Automata with Transformers</title>
      <link>https://arxiv.org/abs/2412.01417</link>
      <description>arXiv:2412.01417v1 Announce Type: cross 
Abstract: Large Language Models demonstrate remarkable mathematical capabilities but at the same time struggle with abstract reasoning and planning. In this study, we explore whether Transformers can learn to abstract and generalize the rules governing Elementary Cellular Automata. By training Transformers on state sequences generated with random initial conditions and local rules, we show that they can generalize across different Boolean functions of fixed arity, effectively abstracting the underlying rules. While the models achieve high accuracy in next-state prediction, their performance declines sharply in multi-step planning tasks without intermediate context. Our analysis reveals that including future states or rule prediction in the training loss enhances the models' ability to form internal representations of the rules, leading to improved performance in longer planning horizons and autoregressive generation. Furthermore, we confirm that increasing the model's depth plays a crucial role in extended sequential computations required for complex reasoning tasks. This highlights the potential to improve LLM with inclusion of longer horizons in loss function, as well as incorporating recurrence and adaptive computation time for dynamic control of model depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01417v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Burtsev</dc:creator>
    </item>
    <item>
      <title>Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers</title>
      <link>https://arxiv.org/abs/2404.04393</link>
      <description>arXiv:2404.04393v2 Announce Type: replace-cross 
Abstract: Deriving formal bounds on the expressivity of transformers, as well as studying transformers that are constructed to implement known algorithms, are both effective methods for better understanding the computational power of transformers. Towards both ends, we introduce the temporal counting logic $\textsf{K}_\text{t}$[#] alongside the RASP variant $\textsf{C-RASP}$. We show they are equivalent to each other, and that together they are the best-known lower bound on the formal expressivity of future-masked soft attention transformers with unbounded input size. We prove this by showing all $\textsf{K}_\text{t}$[#] formulas can be compiled into these transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04393v2</guid>
      <category>cs.LO</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Yang, David Chiang</dc:creator>
    </item>
  </channel>
</rss>
