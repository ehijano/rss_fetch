<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Apr 2024 19:06:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion</title>
      <link>https://arxiv.org/abs/2404.15283</link>
      <description>arXiv:2404.15283v1 Announce Type: new 
Abstract: Interactions with electronic devices are changing in our daily lives. The day-to-day development brings curiosity to recent technology and challenges its use. The gadgets are becoming cumbersome, and their usage frustrates a segment of society. In specific scenarios, the user cannot use the modalities because of the challenges that bring in, e.g., the usage of touch screen devices by elderly people. The idea of multimodality provides easy access to devices of daily use through various modalities. In this paper, we suggest a solution that allows the operation of a microcontroller-based device using voice and speech. The model implemented will learn from the user's behavior and decide based on prior knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15283v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tauheed Khan Mohd, Ahmad Y Javaid</dc:creator>
    </item>
    <item>
      <title>Complexity of Popularity and Dynamics of Within-Game Achievements in Computer Games</title>
      <link>https://arxiv.org/abs/2404.15295</link>
      <description>arXiv:2404.15295v1 Announce Type: new 
Abstract: Tasks of different nature and difficulty levels are a part of people's lives. In this context, there is a scientific interest in the relationship between the difficulty of the task and the persistence need to accomplish it. Despite the generality of this problem, some tasks can be simulated in the form of games. In this way, we employ data from a large online platform, called Steam, to analyze games and the performance of their players. More specifically, we investigated persistence in completing tasks based on the proportion of players who accomplished game achievements. Overall, we present five major findings. First, the probability distribution for the number of achievements is log-normal distribution. Second, the distribution of game players also follows a log-normal. Third, most games require neither a very high degree of persistence nor a very low one. Fourth, players also prefer games that demand a certain intermediate persistence. Fifth, the proportion of players as a function of the number of achievements declines approximately exponentially. As both the log-normal and the exponential functions are memoryless, they are mathematical forms that describe random effects arising from the nature of the system. Therefore our first two findings describe random processes of fragmenting achievements and players while the last three provide a quantitative measure of the human preference in the pursuit of challenging, achievable, and justifiable tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15295v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1142/S0129183124501122</arxiv:DOI>
      <arxiv:journal_reference>Cunha, L. R., Mendes, L. O., &amp; Mendes, R. S. (2024). Complexity of Popularity and Dynamics of Within-Game Achievements in Computer Games. International Journal of Modern Physics C</arxiv:journal_reference>
      <dc:creator>Leonardo Ribeiro da Cunha, Leonardo Oliveira Mendes, Renio dos Santos Mendes</dc:creator>
    </item>
    <item>
      <title>Development of a Gamification Model for Personalized E-learning</title>
      <link>https://arxiv.org/abs/2404.15301</link>
      <description>arXiv:2404.15301v1 Announce Type: new 
Abstract: This study designed a personality-based gamification model for E-learning systems. It also implemented the model and evaluated the performance of the gamification model implemented. These were with a view to developing a model for gamifying personalization of e-learning systems. Personalization requirements for motivational tendencies based on the Myers-Briggs Type Indicator (MBTI) and gamification elements were elicited for e-learning from existing literature and from education experts using interview and questionnaire. The gamification model for personalized e-learning was designed by mapping motivational tendencies to corresponding gamification elements using set theory and rendered using Unified modelling language (UML) tools. The model was implemented using Hypertext Markup Language for the front end, Hypertext Preprocessor (PHP) for the backend and Structured Query Language (SQL) for database on WordPress. The model was evaluated using appeal, emotion, user-centricity as well as satisfaction as engagement criteria, and clarity, error correction as well as feedback for educational usability. The results collected from the implemented system database and questionnaires administered to learners showed an average appeal rating of 4.3, an emotion rating of 4.5, a user-centricity rating of 4.4, and a satisfaction rating of 4.4 in terms of engagement on a 5.0 scale. The results also showed that clarity, error correction and feedback received an average rating of 3.9, 4.7, and 4.8 respectively on a 5.0 scale concerning educational usability. In addition, when comparing educational usability (4.5) to engagement (4.4), educational usability received slightly higher ratings. The study concluded that the gamification model for personalized e-learning was suitable for increasing learner motivation and engagement within the personalized e-learning environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15301v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afvensu Enoch Ibisu</dc:creator>
    </item>
    <item>
      <title>State Space Paradox of Computational Research in Creativity</title>
      <link>https://arxiv.org/abs/2404.15303</link>
      <description>arXiv:2404.15303v1 Announce Type: new 
Abstract: This paper explores the paradoxical nature of computational creativity, focusing on the inherent limitations of closed digital systems in emulating the open-ended, dynamic process of human creativity. Through a comprehensive analysis, we delve into the concept of the State Space Paradox (SSP) in computational research on creativity, which arises from the attempt to model or replicate creative behaviors within the bounded state spaces of digital systems. Utilizing a combination of procedural and representational paradigms, we examine various computational models and their capabilities to assist or emulate the creative process. Our investigation encompasses rule-based systems, genetic algorithms, case-based reasoning, shape grammars, and data mining, among others, to understand how these methods contribute to or fall short of achieving genuine creativity. The discussion extends to the implications of SSP on the future of creativity-related computer systems, emphasizing the cultural and contextual fluidity of creativity itself and the challenges of producing truly creative outcomes within the constraints of pre-defined algorithmic structures. We argue that while digital systems can provoke sudden mental insights (SMIs) in human observers and potentially support the creative process, their capacity to autonomously break out of their pre-programmed state spaces and achieve originality akin to human creativity remains fundamentally constrained. The paper concludes with reflections on the future directions for research in computational creativity, suggesting that recognizing and embracing the limitations and potentials of digital systems could lead to more nuanced and effective tools for creative assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15303v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\"Omer Akin, Yuning Wu</dc:creator>
    </item>
    <item>
      <title>Facilitating Human Feedback for GenAI Prompt Optimization</title>
      <link>https://arxiv.org/abs/2404.15304</link>
      <description>arXiv:2404.15304v1 Announce Type: new 
Abstract: This study investigates the optimization of Generative AI (GenAI) systems through human feedback, focusing on how varying feedback mechanisms influence the quality of GenAI outputs. We devised a Human-AI training loop where 32 students, divided into two groups, evaluated AI-generated responses based on a single prompt. One group assessed a single output, while the other compared two outputs. Preliminary results from this small-scale experiment suggest that comparative feedback might encourage more nuanced evaluations, highlighting the potential for improved human-AI collaboration in prompt optimization. Future research with larger samples is recommended to validate these findings and further explore effective feedback strategies for GenAI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15304v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Sherson, Florent Vinchon</dc:creator>
    </item>
    <item>
      <title>Automated Assessment of Encouragement and Warmth in Classrooms Leveraging Multimodal Emotional Features and ChatGPT</title>
      <link>https://arxiv.org/abs/2404.15310</link>
      <description>arXiv:2404.15310v1 Announce Type: new 
Abstract: Classroom observation protocols standardize the assessment of teaching effectiveness and facilitate comprehension of classroom interactions. Whereas these protocols offer teachers specific feedback on their teaching practices, the manual coding by human raters is resource-intensive and often unreliable. This has sparked interest in developing AI-driven, cost-effective methods for automating such holistic coding. Our work explores a multimodal approach to automatically estimating encouragement and warmth in classrooms, a key component of the Global Teaching Insights (GTI) study's observation protocol. To this end, we employed facial and speech emotion recognition with sentiment analysis to extract interpretable features from video, audio, and transcript data. The prediction task involved both classification and regression methods. Additionally, in light of recent large language models' remarkable text annotation capabilities, we evaluated ChatGPT's zero-shot performance on this scoring task based on transcripts. We demonstrated our approach on the GTI dataset, comprising 367 16-minute video segments from 92 authentic lesson recordings. The inferences of GPT-4 and the best-trained model yielded correlations of r = .341 and r = .441 with human ratings, respectively. Combining estimates from both models through averaging, an ensemble approach achieved a correlation of r = .513, comparable to human inter-rater reliability. Our model explanation analysis indicated that text sentiment features were the primary contributors to the trained model's decisions. Moreover, GPT-4 could deliver logical and concrete reasoning as potential teacher guidelines. Our findings provide insights into using advanced, multimodal techniques for automated classroom observation, aiming to foster teacher training through frequent and valuable feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15310v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruikun Hou, Tim F\"utterer, Babette B\"uhler, Efe Bozkir, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>An Optimized Framework for Processing Large-scale Polysomnographic Data Incorporating Expert Human Oversight</title>
      <link>https://arxiv.org/abs/2404.15313</link>
      <description>arXiv:2404.15313v1 Announce Type: new 
Abstract: Polysomnographic recordings are essential for diagnosing many sleep disorders, yet their detailed analysis presents considerable challenges. With the rise of machine learning methodologies, researchers have created various algorithms to automatically score and extract clinically relevant features from polysomnography, but less research has been devoted to how exactly the algorithms should be incorporated into the workflow of sleep technologists. This paper presents a sophisticated data collection platform developed under the Sleep Revolution project, to harness polysomnographic data from multiple European centers. A tripartite platform is presented: a user-friendly web platform for uploading three-night polysomnographic recordings, a dedicated splitter that segments these into individual one-night recordings, and an advanced processor that enhances the one-night polysomnography with contemporary automatic scoring algorithms. The platform is evaluated using real-life data and human scorers, whereby scoring time, accuracy and trust are quantified. Additionally, the scorers were interviewed about their trust in the platform, along with the impact of its integration into their workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15313v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benedikt Holm, Gabriel Jouan, Emil Hardarson, Sigr\'i{\dh}ur Sigur{\dh}ardottir, Kenan Hoelke, Conor Murphy, Erna Sif Arnard\'ottir, Mar\'ia \'Oskarsd\'ottir, Anna Sigr\'i{\dh}ur Islind</dc:creator>
    </item>
    <item>
      <title>Quantifying Social Presence in Mixed Reality: A Contemporary Review of Techniques and Innovations</title>
      <link>https://arxiv.org/abs/2404.15325</link>
      <description>arXiv:2404.15325v1 Announce Type: new 
Abstract: This literature review investigates the transformative potential of mixed reality (MR) technology, where we explore the intersection of contemporary technological advancements, modern deep learning recommendation systems, and social psychology frameworks. This interdisciplinary study informs the understanding of MR's role in improving social presence, catalyzing novel social interactions, and enhancing the quality of interpersonal communication in the real world. We also discuss the challenges and barriers blocking the wide-spread adoption of social networking in MR, such as device constraints, privacy and accessibility concerns, and social norms. Through carefully structured, closed-environment experiments with diverse participants of varying levels of digital literacy, we measure the differences in social dynamics, frequency, quality, and duration of interactions, and levels of social anxiety between MR-enhanced, mobile-enhanced, and control condition participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15325v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sparsh Srivastava</dc:creator>
    </item>
    <item>
      <title>Introduction to Eye Tracking: A Hands-On Tutorial for Students and Practitioners</title>
      <link>https://arxiv.org/abs/2404.15435</link>
      <description>arXiv:2404.15435v1 Announce Type: new 
Abstract: Eye-tracking technology is widely used in various application areas such as psychology, neuroscience, marketing, and human-computer interaction, as it is a valuable tool for understanding how people process information and interact with their environment. This tutorial provides a comprehensive introduction to eye tracking, from the basics of eye anatomy and physiology to the principles and applications of different eye-tracking systems. The guide is designed to provide a hands-on learning experience for everyone interested in working with eye-tracking technology. Therefore, we include practical case studies to teach students and professionals how to effectively set up and operate an eye-tracking system. The tutorial covers a variety of eye-tracking systems, calibration techniques, data collection, and analysis methods, including fixations, saccades, pupil diameter, and visual scan path analysis. In addition, we emphasize the importance of considering ethical aspects when conducting eye-tracking research and experiments, especially informed consent and participant privacy. We aim to give the reader a solid understanding of basic eye-tracking principles and the practical skills needed to conduct their experiments. Python-based code snippets and illustrative examples are included in the tutorials and can be downloaded at: https://gitlab.lrz.de/hctl/Eye-Tracking-Tutorial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15435v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enkelejda Kasneci, Hong Gao, Suleyman Ozdel, Virmarie Maquiling, Enkeleda Thaqi, Carrie Lau, Yao Rong, Gjergji Kasneci, Efe Bozkir</dc:creator>
    </item>
    <item>
      <title>Exploring Convergence in Relation using Association Rules Mining: A Case Study in Collaborative Knowledge Production</title>
      <link>https://arxiv.org/abs/2404.15440</link>
      <description>arXiv:2404.15440v1 Announce Type: new 
Abstract: This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes. Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists. By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights. Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production. To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced. This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors. Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data. This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age. Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15440v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiahe Ling, Corey B. Jackson</dc:creator>
    </item>
    <item>
      <title>BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis</title>
      <link>https://arxiv.org/abs/2404.15532</link>
      <description>arXiv:2404.15532v1 Announce Type: new 
Abstract: This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI's potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15532v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>The Ability of Virtual Reality Technologies to Improve Comprehension of Speech Therapy Device Training</title>
      <link>https://arxiv.org/abs/2404.15534</link>
      <description>arXiv:2404.15534v1 Announce Type: new 
Abstract: This study evaluates the usage of virtual reality (VR) technologies as a teaching tool in oral placement therapy, a subset of speech therapy. The researcher distributed instructional videos using traditional lecture and modified three-dimensional video to prompt responses. Data was gathered with a two-part Google Form: In "Section 1: Knowledge Test" participants were asked to determine how well they received the information displayed to them. In "Section 2: Opinion Test" participants were asked diagnostic and subjective questions via Likert scale ranging from 1 ("Strongly Disagree") to 5 ("Strongly Agree") to determine how well they enjoyed viewing the information displayed to them. Averages for Section 1 were 92.00% for the control group (viewing 2D, unmodified video) and 77.88% for the experimental group (viewing 3D, VR video). Almost all participants answered at least 60% of the questions correctly. Averages for 2D and 3D participants were 4.53/5 and 3.82/5, respectively for "positive" prompts. Exactly 50% of participants experiencing VR video preferred the method to a traditional lecture. This study determines that virtual reality is viable as a learning tool, but knowledge obtained is not necessarily as high as using traditional lecture. Further experimentation is required to determine how well oral placement therapists respond to physically interacting with a model instead of only viewing it. Copies of the Google Form used to collect responses, all raw data, and a flowchart outlining each step used to construct the 3D video can be found in the Appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15534v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel E. Killough</dc:creator>
    </item>
    <item>
      <title>Designing AI-Enabled Games to Support Social-Emotional Learning for Children with Autism Spectrum Disorders</title>
      <link>https://arxiv.org/abs/2404.15576</link>
      <description>arXiv:2404.15576v1 Announce Type: new 
Abstract: Children with autism spectrum disorder (ASD) experience challenges in grasping social-emotional cues, which can result in difficulties in recognizing emotions and understanding and responding to social interactions. Social-emotional intervention is an effective method to improve emotional understanding and facial expression recognition among individuals with ASD. Existing work emphasizes the importance of personalizing interventions to meet individual needs and motivate engagement for optimal outcomes in daily settings. We design a social-emotional game for ASD children, which generates personalized stories by leveraging the current advancement of artificial intelligence. Via a co-design process with five domain experts, this work offers several design insights into developing future AI-enabled gamified systems for families with autistic children. We also propose a fine-tuned AI model and a dataset of social stories for different basic emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15576v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yue Lyu, Pengcheng An, Huan Zhang, Keiko Katsuragawa, Jian Zhao</dc:creator>
    </item>
    <item>
      <title>Revealing Aspects of Hawai'i Tourism Using Situated Augmented Reality</title>
      <link>https://arxiv.org/abs/2404.15610</link>
      <description>arXiv:2404.15610v1 Announce Type: new 
Abstract: In this position paper, we present a process artifact that aims to bring awareness to historical context, contemporary issues, and identity harm inflicted by tourism in Hawaii. First, we introduce the historical background and how the work is informed by the positionality of the authors. We discuss how related augmented reality work can inform strategy for building augmented reality experiences that address cultural issues. Then, we present a mockup of the artifact, aimed to bring awareness to 20th century colonialism, recent Kanaka Maoli art exclusion, and cultural prostitution. We describe how we will share the app at the workshop and list topics for discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15610v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karen Abe, Jules Park, Samir Ghosh</dc:creator>
    </item>
    <item>
      <title>MDDD: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition</title>
      <link>https://arxiv.org/abs/2404.15615</link>
      <description>arXiv:2404.15615v1 Announce Type: new 
Abstract: Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces represents a significant area within the field of affective computing. In the present study, we propose a novel non-deep transfer learning method, termed as Manifold-based Domain adaptation with Dynamic Distribution (MDDD). The proposed MDDD includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data undergoes a transformation onto an optimal Grassmann manifold space, enabling dynamic alignment of the source and target domains. This process prioritizes both marginal and conditional distributions according to their significance, ensuring enhanced adaptation efficiency across various types of data. In the classifier learning, the principle of structural risk minimization is integrated to develop robust classification models. This is complemented by dynamic distribution alignment, which refines the classifier iteratively. Additionally, the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process, which leverages the diversity of the classifiers to enhance the overall prediction accuracy. The experimental results indicate that MDDD outperforms traditional non-deep learning methods, achieving an average improvement of 3.54%, and is comparable to deep learning methods. This suggests that MDDD could be a promising method for enhancing the utility and applicability of aBCIs in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15615v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Luo, Jing Zhang, Yingwei Qiu, Li Zhang, Yaohua Hu, Zhuliang Yu, Zhen Liang</dc:creator>
    </item>
    <item>
      <title>Reflections on the Usefulness and Limitations of Tools for Life-Centred Design</title>
      <link>https://arxiv.org/abs/2404.15636</link>
      <description>arXiv:2404.15636v1 Announce Type: new 
Abstract: Life-centred design decenters humans and considers all life and the far-reaching impacts of design decisions. However, little is known about the application of life-centred design tools in practice and their usefulness and limitations for con-sidering more-than-human perspectives. To address this gap, we carried out a se-ries of workshops, reporting on findings from a first-person study involving one design academic and three design practitioners. Using a popular flat-pack chair as a case study, we generatively identified and applied four tools: systems maps, actant maps, product lifecycle maps and behavioural impact canvas. We found that the tools provided a structured approach for practising systems thinking, identifying human and non-human actors, understanding their interconnected-ness, and surfacing gaps in the team's knowledge. Based on the findings, the pa-per proposes a process for implementing life-centred design tools in design pro-jects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15636v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Tomitsch, Katharina Clasen, Estela Duhart, Damien Lutz</dc:creator>
    </item>
    <item>
      <title>Introducing EEG Analyses to Help Personal Music Preference Prediction</title>
      <link>https://arxiv.org/abs/2404.15753</link>
      <description>arXiv:2404.15753v1 Announce Type: new 
Abstract: Nowadays, personalized recommender systems play an increasingly important role in music scenarios in our daily life with the preference prediction ability. However, existing methods mainly rely on users' implicit feedback (e.g., click, dwell time) which ignores the detailed user experience. This paper introduces Electroencephalography (EEG) signals to personal music preferences as a basis for the personalized recommender system. To realize collection in daily life, we use a dry-electrodes portable device to collect data. We perform a user study where participants listen to music and record preferences and moods. Meanwhile, EEG signals are collected with a portable device. Analysis of the collected data indicates a significant relationship between music preference, mood, and EEG signals. Furthermore, we conduct experiments to predict personalized music preference with the features of EEG signals. Experiments show significant improvement in rating prediction and preference classification with the help of EEG. Our work demonstrates the possibility of introducing EEG signals in personal music preference with portable devices. Moreover, our approach is not restricted to the music scenario, and the EEG signals as explicit feedback can be used in personalized recommendation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15753v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu He, Jiayu Li, Weizhi Ma, Min Zhang, Yiqun Liu, Shaoping Ma</dc:creator>
    </item>
    <item>
      <title>Risk or Chance? Large Language Models and Reproducibility in Human-Computer Interaction Research</title>
      <link>https://arxiv.org/abs/2404.15782</link>
      <description>arXiv:2404.15782v1 Announce Type: new 
Abstract: Reproducibility is a major concern across scientific fields. Human-Computer Interaction (HCI), in particular, is subject to diverse reproducibility challenges due to the wide range of research methodologies employed. In this article, we explore how the increasing adoption of Large Language Models (LLMs) across all user experience (UX) design and research activities impacts reproducibility in HCI. In particular, we review upcoming reproducibility challenges through the lenses of analogies from past to future (mis)practices like p-hacking and prompt-hacking, general bias, support in data analysis, documentation and education requirements, and possible pressure on the community. We discuss the risks and chances for each of these lenses with the expectation that a more comprehensive discussion will help shape best practices and contribute to valid and reproducible practices around using LLMs in HCI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15782v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Kosch, Sebastian Feger</dc:creator>
    </item>
    <item>
      <title>MYCloth: Towards Intelligent and Interactive Online T-Shirt Customization based on User's Preference</title>
      <link>https://arxiv.org/abs/2404.15801</link>
      <description>arXiv:2404.15801v1 Announce Type: new 
Abstract: In conventional online T-shirt customization, consumers, \ie, users, can achieve the intended design only after repeated adjustments of the design prototypes presented by sellers in online dialogues. However, this process is prone to limited visual feedback and cumbersome communication, thus detracting from users' customization experience and time. This paper presents an intelligent and interactive online customization system, named \textbf{MYCloth}, aiming to enhance the T-shirt customization experience. Given the user's text input, our MYCloth employs ChatGPT to refine the text prompt and generate the intended paint of the cloth via the Stable Diffusion model. Our MYCloth also enables the user to preview the final outcome via a novel learning-based virtual try-on model. The whole system allows to iteratively adjust the cloth till optimal design is achieved. We verify the system's efficacy through a series of performance evaluations and user studies, highlighting its ability to streamline the online customization process and improve overall satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15801v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yexin Liu, Lin Wang</dc:creator>
    </item>
    <item>
      <title>From Prisons to Programming: Fostering Self-Efficacy via Virtual Web Design Curricula in Prisons and Jails</title>
      <link>https://arxiv.org/abs/2404.15904</link>
      <description>arXiv:2404.15904v1 Announce Type: new 
Abstract: Self-efficacy and digital literacy are key predictors to incarcerated people's success in the modern workplace. While digitization in correctional facilities is expanding, few templates exist for how to design computing curricula that foster self-efficacy and digital literacy in carceral environments. As a result, formerly incarcerated people face increasing social and professional exclusion post-release. We report on a 12-week college-accredited web design class, taught virtually and synchronously, across 5 correctional facilities across the United States. The program brought together men and women from gender-segregated facilities into one classroom to learn fundamentals in HTML, CSS and Javascript, and create websites addressing social issues of their choosing. We conducted surveys with participating students, using dichotomous and open-ended questions, and performed thematic and quantitative analyses of their responses that suggest students' increased self-efficacy. Our study discusses key design choices, needs, and recommendations for furthering computing curricula that foster self-efficacy and digital literacy in carceral settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15904v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642717</arxiv:DOI>
      <dc:creator>Martin Nisser, Marisa Gaetz, Andrew Fishberg, Raechel Soicher, Faraz Faruqi, Joshua Long</dc:creator>
    </item>
    <item>
      <title>Training Attention Skills in Individuals with Neurodevelopmental Disorders using Virtual Reality and Eye-tracking technology</title>
      <link>https://arxiv.org/abs/2404.15960</link>
      <description>arXiv:2404.15960v1 Announce Type: new 
Abstract: Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder, present challenges across various cognitive capacities. Attention deficits are often common in individuals with NDD due to the sensory system dysfunction that characterizes these disorders. Consequently, limited attention capability can affect the overall quality of life and the ability to transfer knowledge from one circumstance to another. The literature has increasingly recognized the potential benefits of virtual reality (VR) in supporting NDD learning and rehabilitation due to its interactive and engaging nature, which is critical for consistent practice. In previous studies, we explored the usage of a VR application called Wildcard to enhance attention skills in persons with NDD. The application has been redesigned in this study, exploiting eye-tracking technology to enable novel and more fine-grade interactions. A four-week experiment with 38 NDD participants was conducted to evaluate its usability and effectiveness in improving Visual Attention Skills. Results show the usability and effectiveness of Wildcard in enhancing attention skills, advocating for continued exploration of VR and eye-tracking technology's potential in NDD interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15960v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Patti, Francesco Vona, Anna Barberio, Marco Domenico Buttiglione, Ivan Crusco, Marco Mores, Franca Garzotto</dc:creator>
    </item>
    <item>
      <title>Shared Boundary Interfaces: can one fit all? A controlled study on virtual reality vs touch-screen interfaces on persons with Neurodevelopmental Disorders</title>
      <link>https://arxiv.org/abs/2404.15970</link>
      <description>arXiv:2404.15970v1 Announce Type: new 
Abstract: Technology presents a significant educational opportunity, particularly in enhancing emotional engagement and expanding learning and educational prospects for individuals with Neurodevelopmental Disorders (NDD). Virtual reality emerges as a promising tool for addressing such disorders, complemented by numerous touchscreen applications that have shown efficacy in fostering education and learning abilities. VR and touchscreen technologies represent diverse interface modalities. This study primarily investigates which interface, VR or touchscreen, more effectively facilitates food education for individuals with NDD. We compared learning outcomes via pre- and post-exposure questionnaires. To this end, we developed GEA, a dual-interface, user-friendly web application for Food Education, adaptable for either immersive use in a head-mounted display (HMD) or non-immersive use on a tablet. A controlled study was conducted to determine which interface better promotes learning. Over three sessions, the experimental group engaged with all GEA games in VR (condition A), while the control group interacted with the same games on a tablet (condition B). Results indicated a significant increase in post-questionnaire scores across subjects, averaging a 46% improvement. This enhancement was notably consistent between groups, with VR and Tablet groups showing 42% and 41% improvements, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15970v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Vona, Eleonora Beccaluva, Marco Mores, Franca Garzotto</dc:creator>
    </item>
    <item>
      <title>Boosting Architectural Generation via Prompts: Report</title>
      <link>https://arxiv.org/abs/2404.15971</link>
      <description>arXiv:2404.15971v1 Announce Type: new 
Abstract: In the realm of AI architectural design, the importance of prompts is becoming increasingly prominent. With advancements in artificial intelligence and large-scale model technology, more design tasks are being delegated to machine learning algorithms. This necessitates a method for designers to guide algorithms in producing their desired designs. Prompts serve as a guiding and motivational mechanism, playing a crucial role in AI-generated architectural design. This paper categorizes and summarizes common vocabulary used in architectural design, discussing how to craft effective prompts and their impact on the quality and creativity of generated results. Through careful prompt design, designers can better control the generated architectural design images, thereby achieving designs that are more aligned with requirements and innovative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15971v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Zhang, Wenwen Liu</dc:creator>
    </item>
    <item>
      <title>A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples</title>
      <link>https://arxiv.org/abs/2404.15974</link>
      <description>arXiv:2404.15974v1 Announce Type: new 
Abstract: The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a substantial amount of time and effort. In this paper, we introduce EasyLAN, a human-computer collaborative tool that helps developers construct LANs. EasyLAN initially generates a LAN containing only one agent based on the description of the desired task. Subsequently, EasyLAN leverages a few training examples to update the LAN. For each example, EasyLAN models the gap between the output and the ground truth and identifies the causes of the errors. These errors are addressed through carefully designed strategies. Users can intervene in EasyLAN's workflow or directly modify the LAN. Eventually, the LAN evolves from a single agent to a network of LLM agents. The experimental results indicate that developers can rapidly construct LANs with good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15974v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihang Pan, Yuxuan Li, Chun Yu, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>The State of the Art in Visual Analytics for 3D Urban Data</title>
      <link>https://arxiv.org/abs/2404.15976</link>
      <description>arXiv:2404.15976v1 Announce Type: new 
Abstract: Urbanization has amplified the importance of three-dimensional structures in urban environments for a wide range of phenomena that are of significant interest to diverse stakeholders. With the growing availability of 3D urban data, numerous studies have focused on developing visual analysis techniques tailored to the unique characteristics of urban environments. However, incorporating the third dimension into visual analytics introduces additional challenges in designing effective visual tools to tackle urban data's diverse complexities. In this paper, we present a survey on visual analytics of 3D urban data. Our work characterizes published works along three main dimensions (why, what, and how), considering use cases, analysis tasks, data, visualizations, and interactions. We provide a fine-grained categorization of published works from visualization journals and conferences, as well as from a myriad of urban domains, including urban planning, architecture, and engineering. By incorporating perspectives from both urban and visualization experts, we identify literature gaps, motivate visualization researchers to understand challenges and opportunities, and indicate future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15976v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Miranda, Thomas Ortner, Gustavo Moreira, Maryam Hosseini, Milena Vuckovic, Filip Biljecki, Claudio Silva, Marcos Lage, Nivan Ferreira</dc:creator>
    </item>
    <item>
      <title>Concept-Guided LLM Agents for Human-AI Safety Codesign</title>
      <link>https://arxiv.org/abs/2404.15317</link>
      <description>arXiv:2404.15317v1 Announce Type: cross 
Abstract: Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15317v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the AAAI-make Spring Symposium, 2024</arxiv:journal_reference>
      <dc:creator>Florian Geissler, Karsten Roscher, Mario Trapp</dc:creator>
    </item>
    <item>
      <title>The largest EEG-based BCI reproducibility study for open science: the MOABB benchmark</title>
      <link>https://arxiv.org/abs/2404.15319</link>
      <description>arXiv:2404.15319v1 Announce Type: cross 
Abstract: Objective. This study conduct an extensive Brain-computer interfaces (BCI) reproducibility analysis on open electroencephalography datasets, aiming to assess existing solutions and establish open and reproducible benchmarks for effective comparison within the field. The need for such benchmark lies in the rapid industrial progress that has given rise to undisclosed proprietary solutions. Furthermore, the scientific literature is dense, often featuring challenging-to-reproduce evaluations, making comparisons between existing approaches arduous.
  Approach. Within an open framework, 30 machine learning pipelines (separated into raw signal: 11, Riemannian: 13, deep learning: 6) are meticulously re-implemented and evaluated across 36 publicly available datasets, including motor imagery (14), P300 (15), and SSVEP (7). The analysis incorporates statistical meta-analysis techniques for results assessment, encompassing execution time and environmental impact considerations.
  Main results. The study yields principled and robust results applicable to various BCI paradigms, emphasizing motor imagery, P300, and SSVEP. Notably, Riemannian approaches utilizing spatial covariance matrices exhibit superior performance, underscoring the necessity for significant data volumes to achieve competitive outcomes with deep learning techniques. The comprehensive results are openly accessible, paving the way for future research to further enhance reproducibility in the BCI domain.
  Significance. The significance of this study lies in its contribution to establishing a rigorous and transparent benchmark for BCI research, offering insights into optimal methodologies and highlighting the importance of reproducibility in driving advancements within the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15319v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sylvain Chevallier, Igor Carrara, Bruno Aristimunha, Pierre Guetschel, Sara Sedlar, Bruna Lopes, Sebastien Velut, Salim Khazem, Thomas Moreau</dc:creator>
    </item>
    <item>
      <title>Comparing Self-Supervised Learning Techniques for Wearable Human Activity Recognition</title>
      <link>https://arxiv.org/abs/2404.15331</link>
      <description>arXiv:2404.15331v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) based on the sensors of mobile/wearable devices aims to detect the physical activities performed by humans in their daily lives. Although supervised learning methods are the most effective in this task, their effectiveness is constrained to using a large amount of labeled data during training. While collecting raw unlabeled data can be relatively easy, annotating data is challenging due to costs, intrusiveness, and time constraints.
  To address these challenges, this paper explores alternative approaches for accurate HAR using a limited amount of labeled data. In particular, we have adapted recent Self-Supervised Learning (SSL) algorithms to the HAR domain and compared their effectiveness. We investigate three state-of-the-art SSL techniques of different families: contrastive, generative, and predictive. Additionally, we evaluate the impact of the underlying neural network on the recognition rate by comparing state-of-the-art CNN and transformer architectures.
  Our results show that a Masked Auto Encoder (MAE) approach significantly outperforms other SSL approaches, including SimCLR, commonly considered one of the best-performing SSL methods in the HAR domain.
  The code and the pre-trained SSL models are publicly available for further research and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15331v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sannara Ek, Riccardo Presotto, Gabriele Civitarese, Fran\c{c}ois Portet, Philippe Lalanda, Claudio Bettini</dc:creator>
    </item>
    <item>
      <title>Evaluating Fast Adaptability of Neural Networks for Brain-Computer Interface</title>
      <link>https://arxiv.org/abs/2404.15350</link>
      <description>arXiv:2404.15350v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) classification is a versatile and portable technique for building non-invasive Brain-computer Interfaces (BCI). However, the classifiers that decode cognitive states from EEG brain data perform poorly when tested on newer domains, such as tasks or individuals absent during model training. Researchers have recently used complex strategies like Model-agnostic meta-learning (MAML) for domain adaptation. Nevertheless, there is a need for an evaluation strategy to evaluate the fast adaptability of the models, as this characteristic is essential for real-life BCI applications for quick calibration. We used motor movement and imaginary signals as input to Convolutional Neural Networks (CNN) based classifier for the experiments. Datasets with EEG signals typically have fewer examples and higher time resolution. Even though batch-normalization is preferred for Convolutional Neural Networks (CNN), we empirically show that layer-normalization can improve the adaptability of CNN-based EEG classifiers with not more than ten fine-tuning steps. In summary, the present work (i) proposes a simple strategy to evaluate fast adaptability, and (ii) empirically demonstrate fast adaptability across individuals as well as across tasks with simple transfer learning as compared to MAML approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15350v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anupam Sharma, Krishna Miyapuram</dc:creator>
    </item>
    <item>
      <title>Integrating Physiological Data with Large Language Models for Empathic Human-AI Interaction</title>
      <link>https://arxiv.org/abs/2404.15351</link>
      <description>arXiv:2404.15351v1 Announce Type: cross 
Abstract: This paper explores enhancing empathy in Large Language Models (LLMs) by integrating them with physiological data. We propose a physiological computing approach that includes developing deep learning models that use physiological data for recognizing psychological states and integrating the predicted states with LLMs for empathic interaction. We showcase the application of this approach in an Empathic LLM (EmLLM) chatbot for stress monitoring and control. We also discuss the results of a pilot study that evaluates this EmLLM chatbot based on its ability to accurately predict user stress, provide human-like responses, and assess the therapeutic alliance with the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15351v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poorvesh Dongre, Majid Behravan, Kunal Gupta, Mark Billinghurst, Denis Gra\v{c}anin</dc:creator>
    </item>
    <item>
      <title>Towards Robust and Interpretable EMG-based Hand Gesture Recognition using Deep Metric Meta Learning</title>
      <link>https://arxiv.org/abs/2404.15360</link>
      <description>arXiv:2404.15360v1 Announce Type: cross 
Abstract: Current electromyography (EMG) pattern recognition (PR) models have been shown to generalize poorly in unconstrained environments, setting back their adoption in applications such as hand gesture control. This problem is often due to limited training data, exacerbated by the use of supervised classification frameworks that are known to be suboptimal in such settings. In this work, we propose a shift to deep metric-based meta-learning in EMG PR to supervise the creation of meaningful and interpretable representations. We use a Siamese Deep Convolutional Neural Network (SDCNN) and contrastive triplet loss to learn an EMG feature embedding space that captures the distribution of the different classes. A nearest-centroid approach is subsequently employed for inference, relying on how closely a test sample aligns with the established data distributions. We derive a robust class proximity-based confidence estimator that leads to a better rejection of incorrect decisions, i.e. false positives, especially when operating beyond the training data domain. We show our approach's efficacy by testing the trained SDCNN's predictions and confidence estimations on unseen data, both in and out of the training domain. The evaluation metrics include the accuracy-rejection curve and the Kullback-Leibler divergence between the confidence distributions of accurate and inaccurate predictions. Outperforming comparable models on both metrics, our results demonstrate that the proposed meta-learning approach improves the classifier's precision in active decisions (after rejection), thus leading to better generalization and applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15360v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Tam, Shriram Tallam Puranam Raghu, \'Etienne Buteau, Erik Scheme, Mounir Boukadoum, Alexandre Campeau-Lecours, Benoit Gosselin</dc:creator>
    </item>
    <item>
      <title>Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency</title>
      <link>https://arxiv.org/abs/2404.15564</link>
      <description>arXiv:2404.15564v1 Announce Type: cross 
Abstract: This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction. We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. We propose two propositions for these two objectives and prove the necessity of evaluating them. We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model. Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15564v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jun Huang, Yan Liu</dc:creator>
    </item>
    <item>
      <title>Generic Approach to Visualization of Time Series Data</title>
      <link>https://arxiv.org/abs/2207.13664</link>
      <description>arXiv:2207.13664v2 Announce Type: replace 
Abstract: Time series is a collection of data instances that are ordered according to a time stamp. Stock prices, temperature, etc are examples of time series data in real life. Time series data are used for forecasting sales, predicting trends. Visualization is the process of visually representing data or the relationship between features of a data either in a two-dimensional plot or a three-dimensional plot. Visualizing the time series data constitutes an important part of the process for working with a time series dataset. Visualizing the data not only helps in the modelling process but it can also be used to identify trends and features that cause those trends. In this work, we take a real-life time series dataset and analyse how the target feature relates to other features of the dataset through visualization. From the work that has been carried out, we present an effective method of visualization for time series data which will be much useful for machine learning modelling with such datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.13664v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sathya Krishnan Suresh, Shunmugapriya P</dc:creator>
    </item>
    <item>
      <title>3D-Mirrorcle: Bridging the Virtual and Real through Depth Alignment in AR Mirror Systems</title>
      <link>https://arxiv.org/abs/2310.13617</link>
      <description>arXiv:2310.13617v2 Announce Type: replace 
Abstract: Smart mirrors have emerged as a new form of augmented reality (AR) interface for home environments. However, due to the parallax in human vision, one major challenge hindering their development is the depth misalignment between the 3D mirror reflection and the 2D screen display. This misalignment causes the display content to appear as if it is floating above the mirror, thereby disrupting the seamless integration of the two components and impacting the overall quality and functionality of the mirror. In this study, we introduce 3D-Mirrorcle, an innovative augmented reality (AR) mirror system that effectively addresses the issue of depth disparity through a hardware-software co-design on a lenticular grating setup. With our implemented real-time position adjustment and depth adaptation algorithms, the screen display can be dynamically aligned to the user's depth perception for a highly realistic and engaging experience. Our method has been validated through a prototype and hands-on user experiments that engaged 36 participants, and the results show significant improvements in terms of accuracy (24.72% $\uparrow$), immersion(31.4% $\uparrow$), and user satisfaction (44.4% $\uparrow$) compared to the existing works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13617v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Liu, Qi Xin, Chenzhuo Xiang, Yu Zhang, Lun Yiu Nie, Yingqing Xu</dc:creator>
    </item>
    <item>
      <title>Open Your Ears and Take a Look: A State-of-the-Art Report on the Integration of Sonification and Visualization</title>
      <link>https://arxiv.org/abs/2402.16558</link>
      <description>arXiv:2402.16558v2 Announce Type: replace 
Abstract: The research communities studying visualization and sonification for data display and analysis share exceptionally similar goals, essentially making data of any kind interpretable to humans. One community does so by using visual representations of data, and the other community employs auditory (non-speech) representations of data. While the two communities have a lot in common, they developed mostly in parallel over the course of the last few decades. With this STAR, we discuss a collection of work that bridges the borders of the two communities, hence a collection of work that aims to integrate the two techniques into one form of audiovisual display, which we argue to be "more than the sum of the two."
  We introduce and motivate a classification system applicable to such audiovisual displays and categorize a corpus of 57 academic publications that appeared between 2011 and 2023 in categories such as reading level, dataset type, or evaluation system, to mention a few. The corpus also enables a meta-analysis of the field, including regularly occurring design patterns such as type of visualization and sonification techniques, or the use of visual and auditory channels, showing an overall diverse field with different designs. An analysis of a co-author network of the field shows individual teams without many interconnections. The body of work covered in this STAR also relates to three adjacent topics: audiovisual monitoring, accessibility, and audiovisual data art. These three topics are discussed individually in addition to the systematically conducted part of this research. The findings of this report may be used by researchers from both fields to understand the potentials and challenges of such integrated designs while hopefully inspiring them to collaborate with experts from the respective other field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16558v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kajetan Enge, Elias Elmquist, Valentina Caiola, Niklas R\"onnberg, Alexander Rind, Michael Iber, Sara Lenzi, Fangfei Lan, Robert H\"oldrich, Wolfgang Aigner</dc:creator>
    </item>
    <item>
      <title>Explainable Multimodal Emotion Reasoning</title>
      <link>https://arxiv.org/abs/2306.15401</link>
      <description>arXiv:2306.15401v5 Announce Type: replace-cross 
Abstract: Multimodal emotion recognition is an active research topic in artificial intelligence. Its main goal is to integrate multi-modalities to identify human emotional states. Current works generally assume accurate emotion labels for benchmark datasets and focus on developing more effective architectures. However, emotions have inherent ambiguity and subjectivity. To obtain more reliable labels, existing datasets usually restrict the label space to some basic categories, then hire multiple annotators and use majority voting to select the most likely label. However, this process may cause some correct but non-candidate or non-majority labels to be ignored. To improve reliability without ignoring subtle emotions, we propose a new task called "Explainable Multimodal Emotion Reasoning (EMER)". In contrast to traditional tasks that focus on predicting emotions, EMER takes a step further by providing explanations for these predictions. Through this task, we can extract more reliable labels since each label has a certain basis. Meanwhile, we use LLMs to disambiguate unimodal descriptions and generate more complete multimodal EMER descriptions. From them, we can extract more subtle labels, providing a promising approach for open-vocabulary emotion recognition. This paper presents our initial efforts, where we introduce a new dataset, establish baselines, and define evaluation metrics. In addition, EMER can also be used as a benchmark dataset to evaluate the audio-video-text understanding capabilities of multimodal LLMs. To facilitate further research, we will make the code and data available at: https://github.com/zeroQiaoba/AffectGPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15401v5</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Lian, Licai Sun, Haiyang Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu, Lan Chen, Jiangyan Yi, Bin Liu, Jianhua Tao</dc:creator>
    </item>
    <item>
      <title>PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans</title>
      <link>https://arxiv.org/abs/2308.13651</link>
      <description>arXiv:2308.13651v3 Announce Type: replace-cross 
Abstract: Nearest neighbors (NN) are traditionally used to compute final decisions, e.g., in Support Vector Machines or k-NN classifiers, and to provide users with explanations for the model's decision. In this paper, we show a novel utility of nearest neighbors: To improve predictions of a frozen, pretrained classifier C. We leverage an image comparator S that (1) compares the input image with NN images from the top-K most probable classes; and (2) uses S's output scores to weight the confidence scores of C. Our method consistently improves fine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120. Also, a human study finds that showing lay users our probable-class nearest neighbors (PCNN) improves their decision accuracy over prior work which only shows only the top-1 class examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13651v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giang Nguyen, Valerie Chen, Mohammad Reza Taesiri, Anh Totti Nguyen</dc:creator>
    </item>
    <item>
      <title>Can LLM-Generated Misinformation Be Detected?</title>
      <link>https://arxiv.org/abs/2309.13788</link>
      <description>arXiv:2309.13788v5 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13788v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Canyu Chen, Kai Shu</dc:creator>
    </item>
  </channel>
</rss>
