<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rejoinder on "Marked spatial point processes: current state and extensions to point processes on linear networks"</title>
      <link>https://arxiv.org/abs/2405.02343</link>
      <description>arXiv:2405.02343v1 Announce Type: new 
Abstract: We are grateful to all discussants for their invaluable comments, suggestions, questions, and contributions to our article. We have attentively reviewed all discussions with keen interest. In this rejoinder, our objective is to address and engage with all points raised by the discussants in a comprehensive and considerate manner. Consistently, we identify the discussants, in alphabetical order, as follows: CJK for Cronie, Jansson, and Konstantinou, DS for Stoyan, GP for Grabarnik and Pommerening, MRS for Myllym\"aki, Rajala, and S\"arkk\"a, and MCvL for van Lieshout throughout this rejoinder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02343v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13253-024-00613-1</arxiv:DOI>
      <arxiv:journal_reference>Eckardt, M. and Moradi, M. (2024). Rejoinder on `Marked Spatial Point Processes: Current State and Extensions to Point Processes on Linear Networks. Journal of Agricultural, Biological and Environmental Statistics</arxiv:journal_reference>
      <dc:creator>Matthias Eckardt, Mehdi Moradi</dc:creator>
    </item>
    <item>
      <title>Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title>
      <link>https://arxiv.org/abs/2405.02529</link>
      <description>arXiv:2405.02529v1 Announce Type: new 
Abstract: As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02529v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</dc:creator>
    </item>
    <item>
      <title>Distributed Iterative Hard Thresholding for Variable Selection in Tobit Models</title>
      <link>https://arxiv.org/abs/2405.02539</link>
      <description>arXiv:2405.02539v1 Announce Type: new 
Abstract: While extensive research has been conducted on high-dimensional data and on regression with left-censored responses, simultaneously addressing these complexities remains challenging, with only a few proposed methods available. In this paper, we utilize the Iterative Hard Thresholding (IHT) algorithm on the Tobit model in such a setting. Theoretical analysis demonstrates that our estimator converges with a near-optimal minimax rate. Additionally, we extend the method to a distributed setting, requiring only a few rounds of communication while retaining the estimation rate of the centralized version. Simulation results show that the IHT algorithm for the Tobit model achieves superior accuracy in predictions and subset selection, with the distributed estimator closely matching that of the centralized estimator. When applied to high-dimensional left-censored HIV viral load data, our method also exhibits similar superiority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02539v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changxin Yang, Zhongyi Zhu, Heng Lian</dc:creator>
    </item>
    <item>
      <title>Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title>
      <link>https://arxiv.org/abs/2405.02551</link>
      <description>arXiv:2405.02551v1 Announce Type: new 
Abstract: Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02551v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>The Analysis of Criminal Recidivism: A Hierarchical Model-Based Approach for the Analysis of Zero-Inflated, Spatially Correlated recurrent events Data</title>
      <link>https://arxiv.org/abs/2405.02666</link>
      <description>arXiv:2405.02666v1 Announce Type: new 
Abstract: The life course perspective in criminology has become prominent last years, offering valuable insights into various patterns of criminal offending and pathways. The study of criminal trajectories aims to understand the beginning, persistence and desistence in crime, providing intriguing explanations about these moments in life. Central to this analysis is the identification of patterns in the frequency of criminal victimization and recidivism, along with the factors that contribute to them. Specifically, this work introduces a new class of models that overcome limitations in traditional methods used to analyze criminal recidivism. These models are designed for recurrent events data characterized by excess of zeros and spatial correlation. They extend the Non-Homogeneous Poisson Process, incorporating spatial dependence in the model through random effects, enabling the analysis of associations among individuals within the same spatial stratum. To deal with the excess of zeros in the data, a zero-inflated Poisson mixed model was incorporated. In addition to parametric models following the Power Law process for baseline intensity functions, we propose flexible semi-parametric versions approximating the intensity function using Bernstein Polynomials. The Bayesian approach offers advantages such as incorporating external evidence and modeling specific correlations between random effects and observed data. The performance of these models was evaluated in a simulation study with various scenarios, and we applied them to analyze criminal recidivism data in the Metropolitan Region of Belo Horizonte, Brazil. The results provide a detailed analysis of high-risk areas for recurrent crimes and the behavior of recidivism rates over time. This research significantly enhances our understanding of criminal trajectories, paving the way for more effective strategies in combating criminal recidivism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02666v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alisson C. C. Silva, F\'abio N. Demarqui, Br\'aulio F. Silva, Marcos O. Prates</dc:creator>
    </item>
    <item>
      <title>Grouping predictors via network-wide metrics</title>
      <link>https://arxiv.org/abs/2405.02715</link>
      <description>arXiv:2405.02715v1 Announce Type: new 
Abstract: When multitudes of features can plausibly be associated with a response, both privacy considerations and model parsimony suggest grouping them to increase the predictive power of a regression model. Specifically, the identification of groups of predictors significantly associated with the response variable eases further downstream analysis and decision-making. This paper proposes a new data analysis methodology that utilizes the high-dimensional predictor space to construct an implicit network with weighted edges %and weights on the edges to identify significant associations between the response and the predictors. Using a population model for groups of predictors defined via network-wide metrics, a new supervised grouping algorithm is proposed to determine the correct group, with probability tending to one as the sample size diverges to infinity. For this reason, we establish several theoretical properties of the estimates of network-wide metrics. A novel model-assisted bootstrap procedure that substantially decreases computational complexity is developed, facilitating the assessment of uncertainty in the estimates of network-wide metrics. The proposed methods account for several challenges that arise in the high-dimensional data setting, including (i) a large number of predictors, (ii) uncertainty regarding the true statistical model, and (iii) model selection variability. The performance of the proposed methods is demonstrated through numerical experiments, data from sports analytics, and breast cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02715v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon Woosuk Park, Anand N. Vidyashankar, Tucker S. McElroy</dc:creator>
    </item>
    <item>
      <title>Estimating Complier Average Causal Effects with Mixtures of Experts</title>
      <link>https://arxiv.org/abs/2405.02779</link>
      <description>arXiv:2405.02779v1 Announce Type: new 
Abstract: Understanding the causal impact of medical interventions is essential in healthcare research, especially through randomized controlled trials (RCTs). Despite their prominence, challenges arise due to discrepancies between treatment allocation and actual intake, influenced by various factors like patient non-adherence or procedural errors. This paper focuses on the Complier Average Causal Effect (CACE), crucial for evaluating treatment efficacy among compliant patients. Existing methodologies often rely on assumptions such as exclusion restriction and monotonicity, which can be problematic in practice. We propose a novel approach, leveraging supervised learning architectures, to estimate CACE without depending on these assumptions. Our method involves a two-step process: first estimating compliance probabilities for patients, then using these probabilities to estimate two nuisance components relevant to CACE calculation. Building upon the principal ignorability assumption, we introduce four root-n consistent, asymptotically normal, CACE estimators, and prove that the underlying mixtures of experts' nuisance components are identifiable. Our causal framework allows our estimation procedures to enjoy reduced mean squared errors when exclusion restriction or monotonicity assumptions hold. Through simulations and application to a breastfeeding promotion RCT, we demonstrate the method's performance and applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02779v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Grolleau, C\'eline B\'eji, Fran\c{c}ois Petit, Rapha\"el Porcher</dc:creator>
    </item>
    <item>
      <title>Modeling frequency distribution above a priority in presence of IBNR</title>
      <link>https://arxiv.org/abs/2405.02871</link>
      <description>arXiv:2405.02871v1 Announce Type: new 
Abstract: In reinsurance, Poisson and Negative binomial distributions are employed for modeling frequency. However, the incomplete data regarding reported incurred claims above a priority level presents challenges in estimation. This paper focuses on frequency estimation using Schnieper's framework for claim numbering. We demonstrate that Schnieper's model is consistent with a Poisson distribution for the total number of claims above a priority at each year of development, providing a robust basis for parameter estimation. Additionally, we explain how to build an alternative assumption based on a Negative binomial distribution, which yields similar results. The study includes a bootstrap procedure to manage uncertainty in parameter estimation and a case study comparing assumptions and evaluating the impact of the bootstrap approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02871v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Baradel</dc:creator>
    </item>
    <item>
      <title>Mixture of partially linear experts</title>
      <link>https://arxiv.org/abs/2405.02905</link>
      <description>arXiv:2405.02905v1 Announce Type: new 
Abstract: In the mixture of experts model, a common assumption is the linearity between a response variable and covariates. While this assumption has theoretical and computational benefits, it may lead to suboptimal estimates by overlooking potential nonlinear relationships among the variables. To address this limitation, we propose a partially linear structure that incorporates unspecified functions to capture nonlinear relationships. We establish the identifiability of the proposed model under mild conditions and introduce a practical estimation algorithm. We present the performance of our approach through numerical studies, including simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02905v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeongsan Hwang, Byungtae Seo, Sangkon Oh</dc:creator>
    </item>
    <item>
      <title>CVXSADes: a stochastic algorithm for constructing optimal exact regression designs with single or multiple objectives</title>
      <link>https://arxiv.org/abs/2405.02983</link>
      <description>arXiv:2405.02983v1 Announce Type: new 
Abstract: We propose an algorithm to construct optimal exact designs (EDs). Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results. To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them. Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps. The first step involves constructing an AD by utilizing the convex nature of many design criteria. The second step employs a simulated annealing algorithm to search for the ED stochastically. Through several applications, we demonstrate the utility of our method for various design problems. Additionally, we show that the design efficiency approaches unity as the number of design points increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02983v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Kuang Yeh, Julie Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Functional Graphical Models with Change-Point Detection</title>
      <link>https://arxiv.org/abs/2405.03041</link>
      <description>arXiv:2405.03041v1 Announce Type: new 
Abstract: Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data. Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series. For this task, we propose a dynamic and Bayesian functional graphical model. Our modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns. We introduce a novel block-structured sparsity prior paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm. Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics. Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods. We apply the proposed approach to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and discovers meaningful edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03041v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chunshan Liu, Daniel R. Kowal, James Doss-Gollin, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Functional Post-Clustering Selective Inference with Applications to EHR Data Analysis</title>
      <link>https://arxiv.org/abs/2405.03042</link>
      <description>arXiv:2405.03042v1 Announce Type: new 
Abstract: In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases. Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters. Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error. In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time. Our method extends classical selective inference methods for cross-sectional data to longitudinal data. We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors. We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03042v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihan Zhu, Xin Gai, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Causal K-Means Clustering</title>
      <link>https://arxiv.org/abs/2405.03083</link>
      <description>arXiv:2405.03083v1 Announce Type: new 
Abstract: Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03083v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwangho Kim, Jisu Kim, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Exact Sampling of Spanning Trees via Fast-forwarded Random Walks</title>
      <link>https://arxiv.org/abs/2405.03096</link>
      <description>arXiv:2405.03096v1 Announce Type: new 
Abstract: Tree graphs are routinely used in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous--Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain sub-graphs. We formalize this phenomenon using the bottlenecks in the random walk's transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03096v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edric Tam, David B. Dunson, Leo L. Duan</dc:creator>
    </item>
    <item>
      <title>Consistent response prediction for multilayer networks on unknown manifolds</title>
      <link>https://arxiv.org/abs/2405.03225</link>
      <description>arXiv:2405.03225v1 Announce Type: new 
Abstract: Our paper deals with a collection of networks on a common set of nodes, where some of the networks are associated with responses. Assuming that the networks correspond to points on a one-dimensional manifold in a higher dimensional ambient space, we propose an algorithm to consistently predict the response at an unlabeled network. Our model involves a specific multiple random network model, namely the common subspace independent edge model, where the networks share a common invariant subspace, and the heterogeneity amongst the networks is captured by a set of low dimensional matrices. Our algorithm estimates these low dimensional matrices that capture the heterogeneity of the networks, learns the underlying manifold by isomap, and consistently predicts the response at an unlabeled network. We provide theoretical justifications for the use of our algorithm, validated by numerical simulations. Finally, we demonstrate the use of our algorithm on larval Drosophila connectome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03225v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aranyak Acharyya, Jes\'us Arroyo Reli\'on, Michael Clayton, Marta Zlatic, Youngser Park, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under the framework of the generalized linear mixed model</title>
      <link>https://arxiv.org/abs/2405.03603</link>
      <description>arXiv:2405.03603v1 Announce Type: new 
Abstract: Publication bias (PB) is one of the serious issues in meta-analysis. Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels. For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly. Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended. However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model. The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model. We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model. An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03603v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Taojun Hu, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2405.03606</link>
      <description>arXiv:2405.03606v1 Announce Type: new 
Abstract: We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03606v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Generalizing Orthogonalization for Models with Non-linearities</title>
      <link>https://arxiv.org/abs/2405.02475</link>
      <description>arXiv:2405.02475v1 Announce Type: cross 
Abstract: The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms' application. It was, for instance, shown that neural networks can deduce racial information solely from a patient's X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the "orthogonalization" or "normalization" of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method's effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02475v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David R\"ugamer, Chris Kolb, Tobias Weber, Lucas Kook, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Stability of a Generalized Debiased Lasso with Applications to Resampling-Based Variable Selection</title>
      <link>https://arxiv.org/abs/2405.03063</link>
      <description>arXiv:2405.03063v1 Announce Type: cross 
Abstract: Suppose that we first apply the Lasso to a design matrix, and then update one of its columns. In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly. In this work, we propose an approximate formula for updating a debiased Lasso coefficient. We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix's columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\ sub-Gaussian row vectors and i.i.d.\ Gaussian noise. Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number. Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes. In contrast, rigorously establishing distributional limit properties (e.g.\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory. As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03063v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Some Statistical and Data Challenges When Building Early-Stage Digital Experimentation and Measurement Capabilities</title>
      <link>https://arxiv.org/abs/2405.03579</link>
      <description>arXiv:2405.03579v1 Announce Type: cross 
Abstract: Digital experimentation and measurement (DEM) capabilities -- the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact -- are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions. Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others. Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era.
  This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities. We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities. We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities. This is done using a ranking under lower uncertainty model, enabling one to construct a business case. We also examine what ingredients are necessary to run digital experiments. In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods. Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs' data efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03579v1</guid>
      <category>stat.AP</category>
      <category>cs.DB</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.25560/110307</arxiv:DOI>
      <dc:creator>C. H. Bryan Liu</dc:creator>
    </item>
    <item>
      <title>Statistical Principles for Platform Trials</title>
      <link>https://arxiv.org/abs/2302.12728</link>
      <description>arXiv:2302.12728v2 Announce Type: replace 
Abstract: While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The False Approval Rate (FAR) is the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).
  While Tukey's (1953) Error Rate Familwise (ERFw) is meant to be applied within a clinical study, Tukey's (1953) Error Rate per Family (ERpF), defined alongside ERFw,is meant to be applied across studies. We show that controlling Error Rate Familwise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate FAR exactly (not conservatively) at 5-per-100 (even for Platform trials).
  In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization. (In the U.S., policy is set by the Congress.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12728v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinping Cui, Emily Ouyang, Yi Liu, Jingjing Schneider, Hong Tian, Bushi Wang, Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease</title>
      <link>https://arxiv.org/abs/2305.06262</link>
      <description>arXiv:2305.06262v3 Announce Type: replace 
Abstract: We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior's cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06262v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica M. Porter, Christopher T. Franck, Stephen Adams</dc:creator>
    </item>
    <item>
      <title>Doubly ranked tests for grouped functional data</title>
      <link>https://arxiv.org/abs/2306.14761</link>
      <description>arXiv:2306.14761v2 Announce Type: replace 
Abstract: Nonparametric tests for functional data are a challenging class of tests to work with because of the potentially high dimensional nature of functional data. One of the main challenges for considering rank-based tests, like the Mann-Whitney or Wilcoxon Rank Sum tests (MWW), is that the unit of observation is a curve. Thus any rank-based test must consider ways of ranking curves. While several procedures, including depth-based methods, have recently been used to create scores for rank-based tests, these scores are not constructed under the null and often introduce additional, uncontrolled for variability. We therefore reconsider the problem of rank-based tests for functional data and develop an alternative approach that incorporates the null hypothesis throughout. Our approach first ranks realizations from the curves at each time point, then summarizes the ranks for each subject using a sufficient statistic we derive, and finally re-ranks the sufficient statistics in a procedure we refer to as a doubly ranked test. As we demonstrate, doubly rank tests are more powerful while maintaining ideal type I error in the two sample, MWW setting. We also extend our framework to more than two samples, developing a Kruskal-Wallis test for functional data which exhibits good test characteristics as well. Finally, we illustrate the use of doubly ranked tests in functional data contexts from material science, climatology, and public health policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14761v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer</dc:creator>
    </item>
    <item>
      <title>A Change-Point Approach to Estimating the Proportion of False Null Hypotheses in Multiple Testing</title>
      <link>https://arxiv.org/abs/2309.10017</link>
      <description>arXiv:2309.10017v2 Announce Type: replace 
Abstract: For estimating the proportion of false null hypotheses in multiple testing, a family of estimators by Storey (2002) is widely used in the applied and statistical literature, with many methods suggested for selecting the parameter $\lambda$. Inspired by change-point concepts, our new approach to the latter problem first approximates the $p$-value plot with a piecewise linear function with a single change-point and then selects the $p$-value at the change-point location as $\lambda$. Simulations show that our method has among the smallest RMSE across various settings, and we extend it to address the estimation in cases of superuniform $p$-values. We provide asymptotic theory for our estimator, relying on the theory of quantile processes. Additionally, we propose an application in the change-point literature and illustrate it using high-dimensional CNV data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10017v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anica Kostic, Piotr Fryzlewicz</dc:creator>
    </item>
    <item>
      <title>Investigating the heterogeneity of "study twins"</title>
      <link>https://arxiv.org/abs/2312.09884</link>
      <description>arXiv:2312.09884v2 Announce Type: replace 
Abstract: Meta-analyses are commonly performed based on random-effects models, while in certain cases one might also argue in favour of a common-effect model. One such case may be given by the example of two "study twins" that are performed according to a common (or at least very similar) protocol. Here we investigate the particular case of meta-analysis of a pair of studies, e.g. summarizing the results of two confirmatory clinical trials in phase III of a clinical development programme. Thereby, we focus on the question of to what extent homogeneity or heterogeneity may be discernible, and include an empirical investigation of published ("twin") pairs of studies. A pair of estimates from two studies only provides very little evidence on homogeneity or heterogeneity of effects, and ad-hoc decision criteria may often be misleading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09884v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>The risks of risk assessment: causal blind spots when using prediction models for treatment decisions</title>
      <link>https://arxiv.org/abs/2402.17366</link>
      <description>arXiv:2402.17366v2 Announce Type: replace 
Abstract: Prediction models are increasingly proposed for guiding treatment decisions, but most fail to address the special role of treatments, leading to inappropriate use. This paper highlights the limitations of using standard prediction models for treatment decision support. We identify `causal blind spots' in three common approaches to handling treatments in prediction modelling: including treatment as a predictor, restricting data based on treatment status and ignoring treatments. When predictions are used to inform treatment decisions, confounders, colliders and mediators, as well as changes in treatment protocols over time may lead to misinformed decision-making. We illustrate potential harmful consequences in several medical applications. We advocate for an extension of guidelines for development, reporting and evaluation of prediction models to ensure that the intended use of the model is matched to an appropriate risk estimand. When prediction models are intended to inform treatment decisions, prediction models should specify upfront the treatment decisions they aim to support and target a prediction estimand in line with that goal. This requires a shift towards developing predictions under the specific treatment options under consideration (`predictions under interventions'). Predictions under interventions need causal reasoning and inference techniques during development and validation. We argue that this will improve the efficacy of prediction models in guiding treatment decisions and prevent potential negative effects on patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17366v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan van Geloven, Ruth H Keogh, Wouter van Amsterdam, Giovanni Cin\`a, Jesse H. Krijthe, Niels Peek, Kim Luijken, Sara Magliacane, Pawe{\l} Morzywo{\l}ek, Thijs van Ommen, Hein Putter, Matthew Sperrin, Junfeng Wang, Daniala L. Weir, Vanessa Didelez</dc:creator>
    </item>
    <item>
      <title>Singularity and Error Analysis of a Simple Quaternion Estimator</title>
      <link>https://arxiv.org/abs/2403.01150</link>
      <description>arXiv:2403.01150v4 Announce Type: replace 
Abstract: A novel single-frame quaternion estimator processing two vector observations is introduced. The singular cases are examined, and appropriate rotational solutions are provided. Additionally, an alternative method involving sequential rotation is introduced to manage these singularities. The simplicity of the estimator enables clear physical insights and a closed-form expression for the bias as a function of the quaternion error covariance matrix. The covariance could be approximated up to second order with respect to the underlying measurement noise assuming arbitrary probability distribution. The current note relaxes the second-order assumption and provides an expression for the error covariance that is exact to the fourth order, under the assumption of Gaussian distribution. A comprehensive derivation of the individual components of the quaternion additive error covariance matrix is presented. This not only provides increased accuracy but also alleviates issues related to singularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01150v4</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitong Peng, Daniel Choukroun</dc:creator>
    </item>
    <item>
      <title>Extreme Treatment Effect: Extrapolating Dose-Response Function Into Extreme Treatment Domain</title>
      <link>https://arxiv.org/abs/2403.11003</link>
      <description>arXiv:2403.11003v2 Announce Type: replace 
Abstract: The potential outcomes framework serves as a fundamental tool for quantifying causal effects. The average dose-response function (also called the effect curve), denoted as (\mu(t)), is typically of interest when dealing with a continuous treatment variable (exposure). The focus of this work is to determine the impact of an extreme level of treatment, potentially beyond the range of observed values--that is, estimating (\mu(t)) for very large (t). Our approach is grounded in the field of statistics known as extreme value theory. We outline key assumptions for the identifiability of the extreme treatment effect. Additionally, we present a novel and consistent estimation procedure that can potentially reduce the dimension of the confounders to at most 3. This is a significant result since typically, the estimation of (\mu(t)) is very challenging due to high-dimensional confounders. In practical applications, our framework proves valuable when assessing the effects of scenarios such as drug overdoses, extreme river discharges, or extremely high temperatures on a variable of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11003v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik</dc:creator>
    </item>
    <item>
      <title>Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso</title>
      <link>https://arxiv.org/abs/2404.11579</link>
      <description>arXiv:2404.11579v2 Announce Type: replace 
Abstract: Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11579v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Shan Yu, Zhengyuan Zhu, Xin Wang</dc:creator>
    </item>
    <item>
      <title>Identifiable causal inference with noisy treatment and no side information</title>
      <link>https://arxiv.org/abs/2306.10614</link>
      <description>arXiv:2306.10614v2 Announce Type: replace-cross 
Abstract: In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method's good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10614v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antti P\"oll\"anen, Pekka Marttinen</dc:creator>
    </item>
    <item>
      <title>Towards Causal Foundation Model: on Duality between Causal Inference and Attention</title>
      <link>https://arxiv.org/abs/2310.00809</link>
      <description>arXiv:2310.00809v2 Announce Type: replace-cross 
Abstract: Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00809v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma</dc:creator>
    </item>
    <item>
      <title>Estimating Counterfactual Matrix Means with Short Panel Data</title>
      <link>https://arxiv.org/abs/2312.07520</link>
      <description>arXiv:2312.07520v2 Announce Type: replace-cross 
Abstract: We develop a new, spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of "matches" between agents of two types, e.g. workers and firms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE) models of outcomes. Given an infinite population of units and a finite number of outcomes, we show our approach identifies all counterfactual outcome means, including those not estimable by existing methods, if a particular graph constructed based on overlaps in observed outcomes between subpopulations is connected. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a semi-synthetic simulation study based on matched employer-employee data, our estimator has lower bias and only slightly higher variance than a TWFE-model-based estimator when estimating average log-wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07520v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Brad Ross</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Model</title>
      <link>https://arxiv.org/abs/2403.07236</link>
      <description>arXiv:2403.07236v5 Announce Type: replace-cross 
Abstract: It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. In this paper, I develop a methodology to partially identify linear combinations of conditional mean outcomes for individual-level outcomes of interest without imposing parametric assumptions when the researcher only has access to aggregate data. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with data that only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07236v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Moon</dc:creator>
    </item>
    <item>
      <title>Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)</title>
      <link>https://arxiv.org/abs/2404.19495</link>
      <description>arXiv:2404.19495v2 Announce Type: replace-cross 
Abstract: Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. The coefficient (bp) serves the two functions effectively and efficiently. It thus serves needs unserved by other indicators, such as raw coefficient (bw) and standardized beta.
  Another premise of the functionalist theory is that "effect" is not a monolithic concept. Rather, it is a collection of concepts, each of which measures a component of the conglomerate called "effect", thereby serving a subfunction. Regression coefficient (b), for example, indicates the unit change in DV associated with a one-unit increase in IV, thereby measuring one aspect called unit effect, aka efficiency. Percentage coefficient (bp) indicates the percentage change in DV associated with a whole scale increase in IV. It is not meant to be an all-encompassing indicator of an all-encompassing concept, but rather a comprehendible and comparable indicator of efficiency, a key aspect of effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19495v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao (Department of Communication, Faculty of Social Science, University of Macau), Dianshi Moses Li (Centre for Empirical Legal Studies, Faculty of Law, University of Macau), Ze Zack Lai (Department of Communication, Faculty of Social Science, University of Macau), Piper Liping Liu (School of Media and Communication, Shenzhen University), Song Harris Ao (Department of Communication, Faculty of Social Science, University of Macau), Fei You (Department of Communication, Faculty of Social Science, University of Macau)</dc:creator>
    </item>
    <item>
      <title>Causal Inference with High-dimensional Discrete Covariates</title>
      <link>https://arxiv.org/abs/2405.00118</link>
      <description>arXiv:2405.00118v2 Announce Type: replace-cross 
Abstract: When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00118v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, Sivaraman Balakrishnan, Yanjun Han, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions</title>
      <link>https://arxiv.org/abs/2405.00727</link>
      <description>arXiv:2405.00727v2 Announce Type: replace-cross 
Abstract: In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter's parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter's efficacy. Fault detection primarily uses the fault component's prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00727v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephan Schmidt, Daniel N. Wilke, Konstantinos C. Gryllias</dc:creator>
    </item>
  </channel>
</rss>
