<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 May 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Complex contagions can outperform simple contagions for network reconstruction with dense networks or saturated dynamics</title>
      <link>https://arxiv.org/abs/2405.00129</link>
      <description>arXiv:2405.00129v1 Announce Type: new 
Abstract: Network scientists often use complex dynamic processes to describe network contagions, but tools for fitting contagion models typically assume simple dynamics. Here, we address this gap by developing a nonparametric method to reconstruct a network and dynamics from a series of node states, using a model that breaks the dichotomy between simple pairwise and complex neighborhood-based contagions. We then show that a network is more easily reconstructed when observed through the lens of complex contagions if it is dense or the dynamic saturates, and that simple contagions are better otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00129v1</guid>
      <category>cs.SI</category>
      <category>q-bio.PE</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas W. Landry, William Thompson, Laurent H\'ebert-Dufresne, Jean-Gabriel Young</dc:creator>
    </item>
    <item>
      <title>Global News Synchrony and Diversity During the Start of the COVID-19 Pandemic</title>
      <link>https://arxiv.org/abs/2405.00280</link>
      <description>arXiv:2405.00280v1 Announce Type: new 
Abstract: News coverage profoundly affects how countries and individuals behave in international relations. Yet, we have little empirical evidence of how news coverage varies across countries. To enable studies of global news coverage, we develop an efficient computational methodology that comprises three components: (i) a transformer model to estimate multilingual news similarity; (ii) a global event identification system that clusters news based on a similarity network of news articles; and (iii) measures of news synchrony across countries and news diversity within a country, based on country-specific distributions of news coverage of the global events. Each component achieves state-of-the art performance, scaling seamlessly to massive datasets of millions of news articles. We apply the methodology to 60 million news articles published globally between January 1 and June 30, 2020, across 124 countries and 10 languages, detecting 4357 news events. We identify the factors explaining diversity and synchrony of news coverage across countries. Our study reveals that news media tend to cover a more diverse set of events in countries with larger Internet penetration, more official languages, larger religious diversity, higher economic inequality, and larger populations. Coverage of news events is more synchronized between countries that not only actively participate in commercial and political relations -- such as, pairs of countries with high bilateral trade volume, and countries that belong to the NATO military alliance or BRICS group of major emerging economies -- but also countries that share certain traits: an official language, high GDP, and high democracy indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00280v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Scott A. Hale, David Jurgens, Mattia Samory, Ethan Zuckerman, Przemyslaw A. Grabowicz</dc:creator>
    </item>
    <item>
      <title>U.S. Election Hardens Hate Universe</title>
      <link>https://arxiv.org/abs/2405.00459</link>
      <description>arXiv:2405.00459v1 Announce Type: new 
Abstract: Local or national politics can trigger potentially dangerous hate in someone. But with a third of the world's population eligible to vote in elections in 2024 alone, we lack understanding of how individual-level hate multiplies up to hate behavior at the collective global scale. Here we show, based on the most recent U.S. election, that offline events are associated with a rapid adaptation of the global online hate universe that hardens (strengthens) both its network-of-networks structure and the 'flavors' of hate content that it collectively produces. Approximately 50 million potential voters in hate communities are drawn closer to each other and to the broad mainstream of approximately 2 billion others. It triggers new hate content at scale around immigration, ethnicity, and antisemitism that aligns with conspiracy theories about Jewish-led replacement before blending in hate around gender identity/sexual orientation, and religion. Telegram acts as a key hardening agent - yet is overlooked by U.S. Congressional hearings and new E.U. legislation. Because the hate universe has remained robust since 2020, anti-hate messaging surrounding not only upcoming elections but also other events like the war in Gaza, should pivot to blending multiple hate 'flavors' while targeting previously untouched social media structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00459v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>nlin.AO</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akshay Verma, Richard Sear, Neil F. Johnson</dc:creator>
    </item>
    <item>
      <title>Re-visiting Skip-Gram Negative Sampling: Dimension Regularization for More Efficient Dissimilarity Preservation in Graph Embeddings</title>
      <link>https://arxiv.org/abs/2405.00172</link>
      <description>arXiv:2405.00172v1 Announce Type: cross 
Abstract: A wide range of graph embedding objectives decompose into two components: one that attracts the embeddings of nodes that are perceived as similar, and another that repels embeddings of nodes that are perceived as dissimilar. Because real-world graphs are sparse and the number of dissimilar pairs grows quadratically with the number of nodes, Skip-Gram Negative Sampling (SGNS) has emerged as a popular and efficient repulsion approach. SGNS repels each node from a sample of dissimilar nodes, as opposed to all dissimilar nodes. In this work, we show that node-wise repulsion is, in aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are much more scalable than node operations. The dimension approach, in addition to being more efficient, yields a simpler geometric interpretation of the repulsion. Our result extends findings from the self-supervised learning literature to the skip-gram model, establishing a connection between skip-gram node contrast and dimension regularization. We show that in the limit of large graphs, under mild regularity conditions, the original node repulsion objective converges to optimization with dimension regularization. We use this observation to propose an algorithm augmentation framework that speeds up any existing algorithm, supervised or unsupervised, using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream performance while dramatically increasing efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00172v1</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Liu, Arjun Seshadri, Tina Eliassi-Rad, Johan Ugander</dc:creator>
    </item>
    <item>
      <title>How founder motivations, goals, and actions influence early trajectories of online communities</title>
      <link>https://arxiv.org/abs/2405.00601</link>
      <description>arXiv:2405.00601v1 Announce Type: cross 
Abstract: Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment. Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored. To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities. We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote. Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth. These differences in founders' early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days. We end with a discussion of the implications for researchers, designers, and founders of online communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00601v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642269</arxiv:DOI>
      <dc:creator>Sanjay R. Kairam, Jeremy Foote</dc:creator>
    </item>
    <item>
      <title>Robustness of graph embedding methods for community detection</title>
      <link>https://arxiv.org/abs/2405.00636</link>
      <description>arXiv:2405.00636v1 Announce Type: cross 
Abstract: This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00636v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi-Feng Wei, Pablo Moriano, Ramakrishnan Kannan</dc:creator>
    </item>
    <item>
      <title>Conceptual structure and the growth of scientific knowledge</title>
      <link>https://arxiv.org/abs/2204.09747</link>
      <description>arXiv:2204.09747v3 Announce Type: replace 
Abstract: How does scientific knowledge grow? This question has occupied a central place in the philosophy of science, stimulating heated debates, but yielding no clear consensus. Many explanations can be understood in terms of whether and how they view the expansion of knowledge as proceeding through the accretion of scientific concepts into larger conceptual structures. Here, we examine these views empirically, performing a large-scale analysis of the physical and social sciences, spanning five decades. Using natural language processing techniques, we create semantic networks of concepts, wherein noun phrases become linked when used in the same paper abstract. For both the physical and social sciences, we observe increasingly rigid conceptual cores (i.e., densely connected sets of highly central nodes) accompanied by the proliferation of periphery concepts (i.e., sparsely connected nodes that are highly connected to the core). Subsequently, we examine the relationship between conceptual structure and the growth of scientific knowledge, finding that scientific works are more innovative in fields with cores that have higher conceptual churn and with larger cores. Furthermore, scientific consensus is associated with reduced conceptual churn and fewer conceptual cores. Overall, our findings suggest that while the organization of scientific concepts is important for the growth of knowledge, the mechanisms vary across time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.09747v3</guid>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara Kedrick, Ekaterina Levitskaya, Russell J. Funk</dc:creator>
    </item>
    <item>
      <title>A Survey of Graph Neural Networks for Social Recommender Systems</title>
      <link>https://arxiv.org/abs/2212.04481</link>
      <description>arXiv:2212.04481v3 Announce Type: replace 
Abstract: Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users' tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes 5 groups of input type notations and 7 groups of input representation notations; (2) architecture taxonomy includes 8 groups of GNN encoder notations, 2 groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04481v3</guid>
      <category>cs.SI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3661821</arxiv:DOI>
      <arxiv:journal_reference>ACM Comput. Surv. (April 2024)</arxiv:journal_reference>
      <dc:creator>Kartik Sharma, Yeon-Chang Lee, Sivagami Nambi, Aditya Salian, Shlok Shah, Sang-Wook Kim, Srijan Kumar</dc:creator>
    </item>
    <item>
      <title>Influence Maximization with Unknown Individual Effect on General Network</title>
      <link>https://arxiv.org/abs/2301.12226</link>
      <description>arXiv:2301.12226v2 Announce Type: replace 
Abstract: The identification of a seed set to maximize information spread in a network is crucial, a concept known as Influence Maximization (IM). Elegant IM algorithms could naturally extend to cases where each node is equipped with specific weight, referred to as individual effect, to measure the node's importance. Prevailing literature has typically assumed that the individual effect remains constant during the cascade process. However, this assumption is not always feasible, as the individual effect of each node is primarily evaluated by the difference between the outputs in the activated and non-activated states, with one of these states always being unobservable after propagation. Moreover, the individual effect is sensitive to the environmental information provided by surrounding nodes. To address these challenges, we extend the consideration of IM to a broader scenario involving general networks with dynamic node individual effects, leveraging causality techniques. In our paper, we address this through the development of a Causal Influence Maximization (CauIM) algorithm. Theoretically, for CauIM, we present the generalized lower bound of influence spread and provide robustness analysis. Empirically, in synthetic and real-world experiments, we demonstrate the effectiveness and robustness of CauIM, along with a novel acceleration technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12226v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyan Su, Zhiheng Zhang, Jiyan Qiu, Jun Li</dc:creator>
    </item>
    <item>
      <title>Grassroots Social Networking: Where People have Agency over their Personal Information and Social Graph</title>
      <link>https://arxiv.org/abs/2306.13941</link>
      <description>arXiv:2306.13941v5 Announce Type: replace-cross 
Abstract: Offering an architecture for social networking in which people have agency over their personal information and social graph is an open challenge. Here we present a grassroots architecture for serverless, permissionless, peer-to-peer social networks termed Grassroots Social Networking that aims to address this challenge. The architecture is geared for people with networked smartphones -- roaming (address-changing) computing devices communicating over an unreliable network (e.g., using UDP). The architecture incorporates (i) a decentralized social graph, where each person controls, maintains and stores only their local neighborhood in the graph; (iii) personal feeds, with authors and followers who create and store the feeds; and (ii) a grassroots dissemination protocol, in which communication among people occurs only along the edges of their social graph. The architecture realizes these components using the blocklace data structure -- a partially-ordered conflict-free counterpart of the totally-ordered conflict-based blockchain. We provide two example Grassroots Social Networking protocols -- Twitter-like and WhatsApp-like -- and address their security (safety, liveness and privacy), spam/bot/deep-fake resistance, and implementation, demonstrating how server-based social networks could be supplanted by a grassroots architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13941v5</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3599696.3612898</arxiv:DOI>
      <dc:creator>Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>Link Prediction Accuracy on Real-World Networks Under Non-Uniform Missing Edge Patterns</title>
      <link>https://arxiv.org/abs/2401.15140</link>
      <description>arXiv:2401.15140v3 Announce Type: replace-cross 
Abstract: Real-world network datasets are typically obtained in ways that fail to capture all edges. The patterns of missing data are often non-uniform as they reflect biases and other shortcomings of different data collection methods. Nevertheless, uniform missing data is a common assumption made when no additional information is available about the underlying missing-edge pattern, and link prediction methods are frequently tested against uniformly missing edges. To investigate the impact of different missing-edge patterns on link prediction accuracy, we employ 9 link prediction algorithms from 4 different families to analyze 20 different missing-edge patterns that we categorize into 5 groups. Our comparative simulation study, spanning 250 real-world network datasets from 6 different domains, provides a detailed picture of the significant variations in the performance of different link prediction algorithms in these different settings. With this study, we aim to provide a guide for future researchers to help them select a link prediction algorithm that is well suited to their sampled network data, considering the data collection process and application domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15140v3</guid>
      <category>math.DS</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xie He, Amir Ghasemian, Eun Lee, Alice Schwarze, Aaron Clauset, Peter J. Mucha</dc:creator>
    </item>
  </channel>
</rss>
