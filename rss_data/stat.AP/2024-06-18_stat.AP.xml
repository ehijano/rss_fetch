<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jun 2024 01:58:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Information-theoretic evaluation of covariate distributions models</title>
      <link>https://arxiv.org/abs/2406.10611</link>
      <description>arXiv:2406.10611v1 Announce Type: new 
Abstract: Statistical modelling of covariate distributions allows to generate virtual populations or to impute missing values in a covariate dataset. Covariate distributions typically have non-Gaussian margins and show nonlinear correlation structures, which simple multivariate Gaussian distributions fail to represent. Prominent non-Gaussian frameworks for covariate distribution modelling are copula-based models and models based on multiple imputation by chained equations (MICE). While both frameworks have already found applications in the life sciences, a systematic investigation of their goodness-of-fit to the theoretical underlying distribution, indicating strengths and weaknesses under different conditions, is still lacking. To bridge this gap, we thoroughly evaluated covariate distribution models in terms of Kullback-Leibler divergence (KL-D), a scale-invariant information-theoretic goodness-of-fit criterion for distributions. Methodologically, we proposed a new approach to construct confidence intervals for KL-D by combining nearest neighbour-based KL-D estimators with subsampling-based uncertainty quantification. In relevant data sets of different sizes and dimensionalities with both continuous and discrete covariates, non-Gaussian models showed consistent improvements in KL-D, compared to simpler Gaussian or scale transform approximations. KL-D estimates were also robust to the inclusion of latent variables and large fractions of missing values. While good generalization behaviour to new data could be seen in copula-based models, MICE shows a trend for overfitting and its performance should always be evaluated on separate test data. Parametric copula models and MICE were found to scale much better with the dataset dimension than nonparametric copula models. These findings corroborate the potential of non-Gaussian models for modelling realistic life science covariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10611v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Hartung, Aleksandra Khatova</dc:creator>
    </item>
    <item>
      <title>Statistical Considerations for Evaluating Treatment Effect under Various Non-proportional Hazard Scenarios</title>
      <link>https://arxiv.org/abs/2406.11043</link>
      <description>arXiv:2406.11043v1 Announce Type: new 
Abstract: We conducted a systematic comparison of statistical methods used for the analysis of time-to-event outcomes under various proportional and nonproportional hazard (NPH) scenarios. Our study used data from recently published oncology trials to compare the Log-rank test, still by far the most widely used option, against some available alternatives, including the MaxCombo test, the Restricted Mean Survival Time Difference (dRMST) test, the Generalized Gamma Model (GGM) and the Generalized F Model (GFM). Power, type I error rate, and time-dependent bias with respect to the RMST difference, survival probability difference, and median survival time were used to evaluate and compare the performance of these methods. In addition to the real data, we simulated three hypothetical scenarios with crossing hazards chosen so that the early and late effects 'cancel out' and used them to evaluate the ability of the aforementioned methods to detect time-specific and overall treatment effects. We implemented novel metrics for assessing the time-dependent bias in treatment effect estimates to provide a more comprehensive evaluation in NPH scenarios. Recommendations under each NPH scenario are provided by examining the type I error rate, power, and time-dependent bias associated with each statistical approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11043v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Erich J. Greene, Ondrej Blaha, Wei Wei</dc:creator>
    </item>
    <item>
      <title>Predictive Probabilities Made Simple: A Fast and Accurate Method for Clinical Trial Decision Making</title>
      <link>https://arxiv.org/abs/2406.11406</link>
      <description>arXiv:2406.11406v1 Announce Type: new 
Abstract: Bayesian predictive probabilities are commonly used for interim monitoring of clinical trials through efficacy and futility stopping rules. Despite their usefulness, calculation of predictive probabilities, particularly in pre-experiment trial simulation, can be a significant challenge. We introduce an approximation for computing predictive probabilities using either a p-value or a posterior probability that significantly reduces this burden. We show the approximation has a high degree of concordance with standard Monte Carlo imputation methods for computing predictive probabilities, and present five simulation studies comparing the approximation to the full predictive probability for a range of primary analysis strategies: dichotomous, time-to-event, and ordinal endpoints, as well as historical borrowing and longitudinal modeling. We find that this faster method of predictive probability approximation works well in all five applications, thus significantly reducing the computational burden of trial simulation, allowing more virtual trials to be simulated to achieve greater precision in estimating trial operating characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11406v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joe Marion, Liz Lorenzi, Cora Allen-Savietta, Scott Berry, Kert Viele</dc:creator>
    </item>
    <item>
      <title>Ride-sharing Determinants: Spatial and Spatio-temporal Bayesian Analysis for Chicago Service in 2022</title>
      <link>https://arxiv.org/abs/2406.11590</link>
      <description>arXiv:2406.11590v1 Announce Type: new 
Abstract: The rapid expansion of ride-sharing services has caused significant disruptions in the transpor-tation industry and fundamentally altered the way individuals move from one place to another. Accurate estimation of ride-sharing improves service utilization and reliability and reduces travel time and traffic congestion. In this study, we employ two Bayesian models to estimate ride-sharing demand in the 77 Chicago community areas. We consider demographic, scoio-economic, transportation factors as well as land-use characteristics as explanatory variables. Our models assume conditional autoregression (CAR) prior for the explanatory variables. Moreover, the Bayesian frameworks estimate both the unstructured random error and the struc-tured errors for the spatial and the spatiotemporal correlation. We assessed the performance of the estimated models and the residuals of the spatial regression model have no left-over spatial structure. For the spatiotemporal model, the squared correlation between actual ride-shares and the fitted values is 0.95. Our analysis revealed that the demographic factors (populations size and registered crimes) positively impact the ride-sharing demand. Additionally, the ride-sharing demand increases with higher income and increase in the economically active propor-tion of the population as well as the residents with no cars. Moreover, the transit availability and the walkability indices are crucial determinants for the ridesharing in Chicago.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11590v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Elkhouly, Taqwa Alhadidi</dc:creator>
    </item>
    <item>
      <title>Statistical Evolution of ODI Cricket: Analyzing Performance Trends and Effect Sizes</title>
      <link>https://arxiv.org/abs/2406.11652</link>
      <description>arXiv:2406.11652v1 Announce Type: new 
Abstract: In the dynamic realm of One Day International (ODI) cricket, the sport has undergone significant transformations over the past four decades. This study digs into the intricate evolution of ODI cricket from 1987 to 2023, analyzing about 4000 matches to uncover pivotal performance indicators such as batting prowess, bowling efficiency, and partnership dynamics. Employing statistical methodologies, including Cohen's effect size, we scrutinize the observed changes that have shaped ODI cricket's landscape. Our findings reveal nuanced trends: while first innings scores have shown stability with sporadic high outliers in recent years, the impact of achieving scores exceeding 300 has notably increased. Furthermore, batting depth and early wickets lost in the first innings continue to significantly influence match outcomes, highlighting strategic shifts in team approaches. We also observe improvements in second innings bowling effectiveness, particularly in wicket-taking ability, underscoring evolving defensive strategies. This research contributes a statistical foundation to comprehensively understand the evolving dynamics of ODI cricket, offering insights crucial for strategic decision-making and further analysis in sports analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11652v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Mullick</dc:creator>
    </item>
    <item>
      <title>Quick and Simple Kernel Differential Equation Regression Estimators for Data with Sparse Design</title>
      <link>https://arxiv.org/abs/2406.10308</link>
      <description>arXiv:2406.10308v1 Announce Type: cross 
Abstract: Local polynomial regression of order at least one often performs poorly in regions of sparse data. Local constant regression is exceptional in this regard, though it is the least accurate method in general, especially at the boundaries of the data. Incorporating information from differential equations which may approximately or exactly hold is one way of extending the sparse design capacity of local constant regression while reducing bias and variance. A nonparametric regression method that exploits first order differential equations is introduced in this paper and applied to noisy mouse tumour growth data. Asymptotic biases and variances of kernel estimators using Taylor polynomials with different degrees are discussed. Model comparison is performed for different estimators through simulation studies under various scenarios which simulate exponential-type growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10308v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunlei Ge, W. John Braun</dc:creator>
    </item>
    <item>
      <title>Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework</title>
      <link>https://arxiv.org/abs/2406.10366</link>
      <description>arXiv:2406.10366v1 Announce Type: cross 
Abstract: Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications -- a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10366v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Binette, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between County-Level Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v1 Announce Type: cross 
Abstract: Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH -- stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH -- stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v1 Announce Type: cross 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title>
      <link>https://arxiv.org/abs/2406.10612</link>
      <description>arXiv:2406.10612v1 Announce Type: cross 
Abstract: A key output of network meta-analysis (NMA) is the relative ranking of the treatments; nevertheless, it has attracted a lot of criticism. This is mainly due to the fact that ranking is an influential output and prone to over-interpretations even when relative effects imply small differences between treatments. To date, common ranking methods rely on metrics that lack a straightforward interpretation, while it is still unclear how to measure their uncertainty. We introduce a novel framework for estimating treatment hierarchies in NMA. At first, we formulate a mathematical expression that defines a treatment choice criterion (TCC) based on clinically important values. This TCC is applied to the study treatment effects to generate paired data indicating treatment preferences or ties. Then, we synthesize the paired data across studies using an extension of the so-called "Bradley-Terry" model. We assign to each treatment a latent variable interpreted as the treatment "ability" and we estimate the ability parameters within a regression model. Higher ability estimates correspond to higher positions in the final ranking. We further extend our model to adjust for covariates that may affect treatment selection. We illustrate the proposed approach and compare it with alternatives in two datasets: a network comparing 18 antidepressants for major depression and a network comparing 6 antihypertensives for the incidence of diabetes. Our approach provides a robust and interpretable treatment hierarchy which accounts for clinically important values and is presented alongside with uncertainty measures. Overall, the proposed framework offers a novel approach for ranking in NMA based on concrete criteria and preserves from over-interpretation of unimportant differences between treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10612v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta R\"ucker, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Entity Representation for Medicinal Synergy Prediction</title>
      <link>https://arxiv.org/abs/2406.10778</link>
      <description>arXiv:2406.10778v1 Announce Type: cross 
Abstract: Medicinal synergy prediction is a powerful tool in drug discovery and development that harnesses the principles of combination therapy to enhance therapeutic outcomes by improving efficacy, reducing toxicity, and preventing drug resistance. While a myriad of computational methods has emerged for predicting synergistic drug combinations, a large portion of them may overlook the intricate, yet critical relationships between various entities in drug interaction networks, such as drugs, cell lines, and diseases. These relationships are complex and multidimensional, requiring sophisticated modeling to capture nuanced interplay that can significantly influence therapeutic efficacy. We introduce a salient deep hypergraph learning method, namely, Heterogeneous Entity Representation for MEdicinal Synergy prediction (HERMES), to predict anti-cancer drug synergy. HERMES integrates heterogeneous data sources, encompassing drug, cell line, and disease information, to provide a comprehensive understanding of the interactions involved. By leveraging advanced hypergraph neural networks with gated residual mechanisms, HERMES can effectively learn complex relationships/interactions within the data. Our results show HERMES demonstrates state-of-the-art performance, particularly in forecasting new drug combinations, significantly surpassing previous methods. This advancement underscores the potential of HERMES to facilitate more effective and precise drug combination predictions, thereby enhancing the development of novel therapeutic strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10778v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Wu, Jun Wen, Mingyuan Yan, Anqi Dong, Can Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification</title>
      <link>https://arxiv.org/abs/2406.10807</link>
      <description>arXiv:2406.10807v2 Announce Type: cross 
Abstract: With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19) pandemic, there are still uncertainties that are yet to be discovered about its spread, future impact, and resurgence. In this paper, we present a three-stage data-driven approach to distill the hidden information about COVID-19. The first stage employs a Bayesian network structure learning method to identify the causal relationships among COVID-19 symptoms and their intrinsic demographic variables. As a second stage, the output from the Bayesian network structure learning, serves as a useful guide to train an unsupervised machine learning (ML) algorithm that uncovers the similarities in patients' symptoms through clustering. The final stage then leverages the labels obtained from clustering to train a demographic symptom identification (DSID) model which predicts a patient's symptom class and the corresponding demographic probability distribution. We applied our method on the COVID-19 dataset obtained from the Centers for Disease Control and Prevention (CDC) in the United States. Results from the experiments show a testing accuracy of 99.99%, as against the 41.15% accuracy of a heuristic ML method. This strongly reveals the viability of our Bayesian network and ML approach in understanding the relationship between the virus symptoms, and providing insights on patients' stratification towards reducing the severity of the virus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10807v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oluwaseun T. Ajayi, Yu Cheng</dc:creator>
    </item>
    <item>
      <title>SynthTree: Co-supervised Local Model Synthesis for Explainable Prediction</title>
      <link>https://arxiv.org/abs/2406.10962</link>
      <description>arXiv:2406.10962v1 Announce Type: cross 
Abstract: Explainable machine learning (XML) has emerged as a major challenge in artificial intelligence (AI). Although black-box models such as Deep Neural Networks and Gradient Boosting often exhibit exceptional predictive accuracy, their lack of interpretability is a notable drawback, particularly in domains requiring transparency and trust. This paper tackles this core AI problem by proposing a novel method to enhance explainability with minimal accuracy loss, using a Mixture of Linear Models (MLM) estimated under the co-supervision of black-box models. We have developed novel methods for estimating MLM by leveraging AI techniques. Specifically, we explore two approaches for partitioning the input space: agglomerative clustering and decision trees. The agglomerative clustering approach provides greater flexibility in model construction, while the decision tree approach further enhances explainability, yielding a decision tree model with linear or logistic regression models at its leaf nodes. Comparative analyses with widely-used and state-of-the-art predictive models demonstrate the effectiveness of our proposed methods. Experimental results show that statistical models can significantly enhance the explainability of AI, thereby broadening their potential for real-world applications. Our findings highlight the critical role that statistical methodologies can play in advancing explainable AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10962v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evgenii Kuriabov, Jia Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modelling of Noisy Gamma Processes: Model Formulation, Identifiability, Model Fitting, and Extensions to Unit-to-Unit Variability</title>
      <link>https://arxiv.org/abs/2406.11216</link>
      <description>arXiv:2406.11216v1 Announce Type: cross 
Abstract: The gamma process is a natural model for monotonic degradation processes. In practice, it is desirable to extend the single gamma process to incorporate measurement error and to construct models for the degradation of several nominally identical units. In this paper, we show how these extensions are easily facilitated through the Bayesian hierarchical modelling framework. Following the precepts of the Bayesian statistical workflow, we show the principled construction of a noisy gamma process model. We also reparameterise the gamma process to simplify the specification of priors and make it obvious how the single gamma process model can be extended to include unit-to-unit variability or covariates. We first fit the noisy gamma process model to a single simulated degradation trace. In doing so, we find an identifiability problem between the volatility of the gamma process and the measurement error when there are only a few noisy degradation observations. However, this lack of identifiability can be resolved by including extra information in the analysis through a stronger prior or extra data that informs one of the non-identifiable parameters, or by borrowing information from multiple units. We then explore extensions of the model to account for unit-to-unit variability and demonstrate them using a crack-propagation data set with added measurement error. Lastly, we perform model selection in a fully Bayesian framework by using cross-validation to approximate the expected log probability density of new observation. We also show how failure time distributions with uncertainty intervals can be calculated for new units or units that are currently under test but are yet to fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Leadbetter, Gabriel Gonzalez Caceres, Aloke Phatak</dc:creator>
    </item>
    <item>
      <title>Background Modeling for Double Higgs Boson Production: Density Ratios and Optimal Transport</title>
      <link>https://arxiv.org/abs/2208.02807</link>
      <description>arXiv:2208.02807v3 Announce Type: replace 
Abstract: We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is therefore a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events where signal is unlikely to appear, and where the background distribution is therefore identifiable. The background distribution can be estimated in these regions, and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02807v3</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tudor Manole, Patrick Bryant, John Alison, Mikael Kuusela, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Scaling-aware rating of count forecasts</title>
      <link>https://arxiv.org/abs/2211.16313</link>
      <description>arXiv:2211.16313v4 Announce Type: replace 
Abstract: Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast's sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric's value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields "the slow movers are performing worse than the fast movers" or vice versa, the na\"ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder's Demand Edge for Retail solution for grocery products in Sainsbury's supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16313v4</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Image-on-Scalar Regression for Population-Scale Neuroimaging Data Analysis</title>
      <link>https://arxiv.org/abs/2404.13204</link>
      <description>arXiv:2404.13204v2 Announce Type: replace 
Abstract: Bayesian Image-on-Scalar Regression (ISR) offers significant advantages for neuroimaging data analysis, including flexibility and the ability to quantify uncertainty. However, its application to large-scale imaging datasets, such as found in the UK Biobank, is hindered by the computational demands of traditional posterior computation methods, as well as the challenge of individual-specific brain masks that deviate from the common mask typically used in standard ISR approaches. To address these challenges, we introduce a novel Bayesian ISR model that is scalable and accommodates inconsistent brain masks across subjects in large-scale imaging studies. Our model leverages Gaussian process priors and integrates salience area indicators to facilitate ISR. We develop a cutting-edge scalable posterior computation algorithm that employs stochastic gradient Langevin dynamics coupled with memory mapping techniques, ensuring that computation time scales linearly with subsample size and memory usage is constrained only by the batch size. Our approach uniquely enables direct spatial posterior inferences on brain activation regions. The efficacy of our method is demonstrated through simulations and analysis of the UK Biobank task fMRI data, encompassing 38,639 subjects and over 120,000 voxels per image, showing that it can achieve a speed increase of 4 to 11 times and enhance statistical power by 8% to 18% compared to traditional Gibbs sampling with zero-imputation in various simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13204v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Xu, Timothy D. Johnson, Thomas E. Nichols, Jian Kang</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily</title>
      <link>https://arxiv.org/abs/2405.14789</link>
      <description>arXiv:2405.14789v2 Announce Type: replace 
Abstract: Researchers have focused on understanding how individual's behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers' belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14789v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungha Um, Tracy Sweet, Samrachana Adhikari</dc:creator>
    </item>
    <item>
      <title>Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics</title>
      <link>https://arxiv.org/abs/2110.11771</link>
      <description>arXiv:2110.11771v3 Announce Type: replace-cross 
Abstract: Motivated by research on gender identity norms and the distribution of the woman's share in a couple's total labor income, we consider functional additive regression models for probability density functions as responses with scalar covariates. To preserve nonnegativity and integration to one under vector space operations, we formulate the model for densities in a Bayes Hilbert space, which allows to not only consider continuous densities, but also, e.g., discrete or mixed densities. Mixed ones occur in our application, as the woman's income share is a continuous variable having discrete point masses at zero and one for single-earner couples. Estimation is based on a gradient boosting algorithm, allowing for potentially numerous flexible covariate effects and model selection. We develop properties of Bayes Hilbert spaces related to subcompositional coherence, yielding (odds-ratio) interpretation of effect functions and simplified estimation for mixed densities via an orthogonal decomposition. Applying our approach to data from the German Socio-Economic Panel Study (SOEP) shows a more symmetric distribution in East German than in West German couples after reunification and a smaller child penalty comparing couples with and without minor children. These West-East differences become smaller, but are persistent over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11771v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva-Maria Maier, Almond St\"ocker, Bernd Fitzenberger, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</title>
      <link>https://arxiv.org/abs/2201.01357</link>
      <description>arXiv:2201.01357v4 Announce Type: replace-cross 
Abstract: Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.01357v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Kosuke Imai, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model</title>
      <link>https://arxiv.org/abs/2306.14318</link>
      <description>arXiv:2306.14318v4 Announce Type: replace-cross 
Abstract: The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter's (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14318v4</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Tsyrulnikov, Arseniy Sotskiy</dc:creator>
    </item>
    <item>
      <title>Spectral co-Clustering in Multi-layer Directed Networks</title>
      <link>https://arxiv.org/abs/2307.10572</link>
      <description>arXiv:2307.10572v2 Announce Type: replace-cross 
Abstract: Modern network analysis often involves multi-layer network data in which the nodes are aligned, and the edges on each layer represent one of the multiple relations among the nodes. Current literature on multi-layer network data is mostly limited to undirected relations. However, direct relations are more common and may introduce extra information. This study focuses on community detection (or clustering) in multi-layer directed networks. To take into account the asymmetry, a novel spectral-co-clustering-based algorithm is developed to detect co-clusters, which capture the sending patterns and receiving patterns of nodes, respectively. Specifically, the eigendecomposition of the debiased sum of Gram matrices over the layer-wise adjacency matrices is computed, followed by the k-means, where the sum of Gram matrices is used to avoid possible cancellation of clusters caused by direct summation. Theoretical analysis of the algorithm under the multi-layer stochastic co-block model is provided, where the common assumption that the cluster number is coupled with the rank of the model is relaxed. After a systematic analysis of the eigenvectors of the population version algorithm, the misclassification rates are derived, which show that multi-layers would bring benefits to the clustering performance. The experimental results of simulated data corroborate the theoretical predictions, and the analysis of a real-world trade network dataset provides interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10572v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.csda.2024.107987</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics &amp; Data Analysis (2024) 107987</arxiv:journal_reference>
      <dc:creator>Wenqing Su, Xiao Guo, Xiangyu Chang, Ying Yang</dc:creator>
    </item>
    <item>
      <title>L0-regularized compressed sensing with Mean-field Coherent Ising Machines</title>
      <link>https://arxiv.org/abs/2405.00366</link>
      <description>arXiv:2405.00366v2 Announce Type: replace-cross 
Abstract: Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM's stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.'s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00366v2</guid>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mastiyage Don Sudeera Hasaranga Gunathilaka, Yoshitaka Inui, Satoshi Kako, Kazushi Mimura, Masato Okada, Yoshihisa Yamamoto, Toru Aonishi</dc:creator>
    </item>
  </channel>
</rss>
