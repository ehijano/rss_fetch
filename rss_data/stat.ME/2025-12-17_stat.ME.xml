<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:35:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parameter Estimation for Partially Observed Stable Continuous-State Branching Processes</title>
      <link>https://arxiv.org/abs/2512.13841</link>
      <description>arXiv:2512.13841v1 Announce Type: new 
Abstract: In this article, we present a novel inference framework for estimating the parameters of Continuous-State Branching Processes (CSBPs). We do so by leveraging their subordinator representation. Our method reformulates the estimation problem by shifting the stochastic dynamics to the associated subordinator, enabling a parametric estimation procedure without requiring additional assumptions. This reformulation allows for efficient numerical recovery of the likelihood function via Laplace transform inversion, even in models where closed-form transition densities are unavailable. In addition to offering a flexible approach to parameter estimation, we propose a dynamic simulation framework that generates discrete-time trajectories of CSBPs using the same subordinator-based structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13841v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo Guti\'errez-Pe\~na, Carlos Octavio P\'erez-Mendoza, Alan Riva Palacio, Arno Siri-J\'egousse</dc:creator>
    </item>
    <item>
      <title>Bond strength uncertainty quantification via confidence intervals for nondestructive evaluation of bonded composites</title>
      <link>https://arxiv.org/abs/2512.13875</link>
      <description>arXiv:2512.13875v1 Announce Type: new 
Abstract: As bonded composite materials are used more frequently for aerospace applications, it is necessary to certify that parts achieve desired levels of certain physical characteristics (e.g., strength) for safety and performance. Nondestructive evaluation (NDE) of adhesively bonded structures enables verification of bond physical characteristics, but uncertainty quantification (UQ) of NDE estimates is crucial for understanding risks, especially for NDE estimates like bond strength. To address the critical need for NDE UQ for adhesive bond strength estimates, we propose an optimization--based approach to computing finite--sample confidence intervals showing the range of bond strengths that could feasibly be produced by the observed data. A statistical inverse model approach is used to compute a confidence interval of specimen interfacial stiffness from swept--frequency ultrasonic phase observations and a method for propagating the interval to bond strength via a known interfacial stiffness regression is proposed. This approach requires innovating the optimization--based confidence interval to handle both a nonlinear forward model and unknown variance and developing a calibration approach to ensure that the final bond strength interval achieves at least the desired coverage level. Using model assumptions in line with current literature, we demonstrate our approach on simulated measurement data using a variety of low to high noise settings under two prototypical parameter settings. Relative to a baseline approach, we show that our method achieves better coverage and smaller intervals in high--noise settings and when a nuisance parameter is near the constraint boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13875v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. Stanley, Peter W. Spaeth, James E. Warner, Matthew R. Webster</dc:creator>
    </item>
    <item>
      <title>A latent variable model for identifying and characterizing food adulteration</title>
      <link>https://arxiv.org/abs/2512.13939</link>
      <description>arXiv:2512.13939v1 Announce Type: new 
Abstract: Recently, growing consumer awareness of food quality and sustainability has led to a rising demand for effective food authentication methods. Vibrational spectroscopy techniques have emerged as a promising tool for collecting large volumes of data to detect food adulteration. However, spectroscopic data pose significant challenges from a statistical viewpoint, highlighting the need for more sophisticated modeling strategies. To address these challenges, in this work we propose a latent variable model specifically tailored for food adulterant detection, while accommodating the features of spectral data. Our proposal offers greater granularity with respect to existing approaches, since it does not only identify adulterated samples but also estimates the level of adulteration, and detects the spectral regions most affected by the adulterant. Consequently, the methodology offers deeper insights, and could facilitate the development of portable and faster instruments for efficient data collection in food authenticity studies. The method is applied to both synthetic and real honey mid-infrared spectroscopy data, delivering precise estimates of the adulteration level and accurately identifying which portions of the spectra are most impacted by the adulterant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13939v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casa, Thomas Brendan Murphy, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Low-rank Covariate Balancing Estimators under Interference</title>
      <link>https://arxiv.org/abs/2512.13944</link>
      <description>arXiv:2512.13944v1 Announce Type: new 
Abstract: A key methodological challenge in observational studies with interference between units is twofold: (1) each unit's outcome may depend on many others' treatments, and (2) treatment assignments may exhibit complex dependencies across units. We develop a general statistical framework for constructing robust causal effect estimators to address these challenges. We first show that, without restricting the patterns of interference, the standard inverse probability weighting (IPW) estimator is the only uniformly unbiased estimator when the propensity score is known. In contrast, no estimator has such a property if the propensity score is unknown. We then introduce a \emph{low-rank structure} of potential outcomes as a broad class of structural assumptions about interference. This framework encompasses common assumptions such as anonymous, nearest-neighbor, and additive interference, while flexibly allowing for more complex study-specific interference assumptions. Under this low-rank assumption, we show how to construct an unbiased weighting estimator for a large class of causal estimands. The proposed weighting estimator does not require knowledge of true propensity scores and is therefore robust to unknown treatment assignment dependencies that often exist in observational studies. If the true propensity score is known, we can obtain an unbiased estimator that is more efficient than the IPW estimator by leveraging a low-rank structure. We establish the finite sample and asymptotic properties of the proposed weighting estimator, develop a data-driven procedure to select among candidate low-rank structures, and validate our approach through simulation and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13944v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Souhardya Sengupta, Kosuke Imai, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations</title>
      <link>https://arxiv.org/abs/2512.13962</link>
      <description>arXiv:2512.13962v1 Announce Type: new 
Abstract: Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13962v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Christoffersen, Keith Humphreys, Alessandro Gasparini, Birzhan Akynkozhayev, Hedvig Kjellstr\"om, Mark Clements</dc:creator>
    </item>
    <item>
      <title>Bayesian Global-Local Regularization</title>
      <link>https://arxiv.org/abs/2512.13992</link>
      <description>arXiv:2512.13992v1 Announce Type: new 
Abstract: We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13992v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Most Powerful Test with Exact Family-Wise Error Rate Control: Necessary Conditions and a Path to Fast Computing</title>
      <link>https://arxiv.org/abs/2512.14131</link>
      <description>arXiv:2512.14131v1 Announce Type: new 
Abstract: Identifying the most powerful test in multiple hypothesis testing under strong family-wise error rate (FWER) control is a fundamental problem in statistical methodology. State-of-the-art approaches formulate this as a constrained optimisation problem, for which a dual problem with strong duality has been established in a general sense. However, a constructive method for solving the dual problem is lacking, leaving a significant computational gap. This paper fills this gap by deriving novel, necessary optimality conditions for the dual optimisation. We show that these conditions motivate an efficient coordinate-wise algorithm for computing the optimal dual solution, which, in turn, provides the most powerful test for the primal problem. We prove the linear convergence of our algorithm, i.e., the computational complexity of our proposed algorithm is proportional to the logarithm of the reciprocal of the target error. To the best of our knowledge, this is the first time such a fast and computationally efficient algorithm has been proposed for finding the most powerful test with family-wise error rate control. The method's superior power is demonstrated through simulation studies, and its practical utility is shown by identifying new, significant findings in both clinical and financial data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14131v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prasanjit Dubey, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Signature-Informed Selection Detection: A Novel Method for Multi-Locus Temporal Population Genetic Model with Recombination</title>
      <link>https://arxiv.org/abs/2512.14353</link>
      <description>arXiv:2512.14353v1 Announce Type: new 
Abstract: In population genetics, there is often interest in inferring selection coefficients. This task becomes more challenging if multiple linked selected loci are considered simultaneously. For such a situation, we propose a novel generalized Bayesian framework where we compute a scoring rule posterior for the selection coefficients in multi-locus temporal population genetics models. As we consider trajectories of allele frequencies over time as our data, we choose to use a signature kernel scoring rule - a kernel scoring rule defined for high-dimensional time-series data using iterated path integrals of a path (called signatures). We can compute an unbiased estimate of the signature kernel score using model simulations. This enables us to sample asymptotically from the signature kernel scoring rule posterior of the selection coefficients using pseudo-marginal MCMC-type algorithms. Through a simulation study, we were able to show the inferential efficacy of our method compared to existing benchmark methods for two and three selected locus scenarios under the standard Wright-Fisher model with recombination and selection. We also consider a negative frequency-dependent selection model for one and two locus scenarios, and also joint inference of selection coefficients and initial haplotype frequencies under the standard Wright-Fisher model. Finally, we illustrate the application of our inferential method for two real-life dataset. More specifically, we consider a data set on Yeast, as well as data from an Evolve and Resequence (E\&amp;R) experiment on {\em Drosophila simulans}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritabrata Dutta, Yuehao Xu, Sherman Khoo, Francesca Basini, Andreas Futschik</dc:creator>
    </item>
    <item>
      <title>On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays</title>
      <link>https://arxiv.org/abs/2512.14378</link>
      <description>arXiv:2512.14378v2 Announce Type: new 
Abstract: Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14378v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanouil Androulakis, Kashinath Chatterjee, Haralambos Evangelaras</dc:creator>
    </item>
    <item>
      <title>Trunc-Opt vine building algorithms</title>
      <link>https://arxiv.org/abs/2512.14399</link>
      <description>arXiv:2512.14399v1 Announce Type: new 
Abstract: Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kov\'acs and Sz\'antai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14399v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>D\'aniel Pfeifer, Edith Alice Kov\'acs</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Interaction Modeling</title>
      <link>https://arxiv.org/abs/2512.14413</link>
      <description>arXiv:2512.14413v1 Announce Type: new 
Abstract: We propose a procedure for sparse regression with pairwise interactions, by generalizing the Univariate Guided Sparse Regression (UniLasso) methodology. A central contribution is our introduction of a concept of univariate (or marginal) interactions. Using this concept, we propose two algorithms -- uniPairs and uniPairs-2stage -- , and evaluate their performance against established methods, including Glinternet and Sprinter. We show that our framework yields sparser models with more interpretable interactions. We also prove support recovery results for our proposal under suitable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14413v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aymen Echarghaoui, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error</title>
      <link>https://arxiv.org/abs/2512.14492</link>
      <description>arXiv:2512.14492v1 Announce Type: new 
Abstract: The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14492v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Slawski</dc:creator>
    </item>
    <item>
      <title>A flexible class of latent variable models for the analysis of antibody response data</title>
      <link>https://arxiv.org/abs/2512.14504</link>
      <description>arXiv:2512.14504v1 Announce Type: new 
Abstract: Existing approaches to modelling antibody concentration data are mostly based on finite mixture models that rely on the assumption that individuals can be divided into two distinct groups: seronegative and seropositive. Here, we challenge this dichotomous modelling assumption and propose a latent variable modelling framework in which the immune status of each individual is represented along a continuum of latent seroreactivity, ranging from minimal to strong immune activation. This formulation provides greater flexibility in capturing age-related changes in antibody distributions while preserving the full information content of quantitative measurements. We show that the proposed class of models can accommodate a great variety of model formulations, both mechanistic and regression-based, and also includes finite mixture models as a special case. We demonstrate the advantages of this approach using malaria serology data and its ability to develop joint analyses across all ages that account for changes in transmission patterns. We conclude by outlining extensions of the proposed modelling framework and its relevance to other omics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14504v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Giorgi, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Rank Correlations</title>
      <link>https://arxiv.org/abs/2512.14609</link>
      <description>arXiv:2512.14609v1 Announce Type: new 
Abstract: Kendall's tau and Spearman's rho are widely used tools for measuring dependence. Surprisingly, when it comes to asymptotic inference for these rank correlations, some fundamental results and methods have not yet been developed, in particular for discrete random variables and in the time series case, and concerning variance estimation in general. Consequently, asymptotic confidence intervals are not available. We provide a comprehensive treatment of asymptotic inference for classical rank correlations, including Kendall's tau, Spearman's rho, Goodman-Kruskal's gamma, Kendall's tau-b, and grade correlation. We derive asymptotic distributions for both iid and time series data, resorting to asymptotic results for U-statistics, and introduce consistent variance estimators. This enables the construction of confidence intervals and tests, generalizes classical results for continuous random variables and leads to corrected versions of widely used tests of independence. We analyze the finite-sample performance of our variance estimators, confidence intervals, and tests in simulations and illustrate their use in case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14609v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc-Oliver Pohle, Jan-Lukas Wermuth, Christian H. Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data</title>
      <link>https://arxiv.org/abs/2512.13712</link>
      <description>arXiv:2512.13712v1 Announce Type: cross 
Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13712v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Guo</dc:creator>
    </item>
    <item>
      <title>Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics</title>
      <link>https://arxiv.org/abs/2512.13997</link>
      <description>arXiv:2512.13997v1 Announce Type: cross 
Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13997v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Wei, Milad Jalali, Danica J. Sutherland</dc:creator>
    </item>
    <item>
      <title>On the Hardness of Conditional Independence Testing In Practice</title>
      <link>https://arxiv.org/abs/2512.14000</link>
      <description>arXiv:2512.14000v1 Announce Type: cross 
Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14000v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng He, Roman Pogodin, Yazhe Li, Namrata Deka, Arthur Gretton, Danica J. Sutherland</dc:creator>
    </item>
    <item>
      <title>Estimating Program Participation with Partial Validation</title>
      <link>https://arxiv.org/abs/2512.14616</link>
      <description>arXiv:2512.14616v1 Announce Type: cross 
Abstract: This paper considers the estimation of binary choice models when survey responses are possibly misclassified but one of the response category can be validated. Partial validation may occur when survey questions about participation include follow-up questions on that particular response category. In this case, we show that the initial two-sided misclassification problem can be transformed into a one-sided one, based on the partially validated responses. Using the updated responses naively for estimation does not solve or mitigate the misclassification bias, and we derive the ensuing asymptotic bias under general conditions. We then show how the partially validated responses can be used to construct a model for participation and propose consistent and asymptotically normal estimators that overcome misclassification error. Monte Carlo simulations are provided to demonstrate the finite sample performance of the proposed and selected existing methods. We provide an empirical illustration on the determinants of health insurance coverage in Ghana. We discuss implications for the design of survey questionnaires that allow researchers to overcome misclassification biases without recourse to relatively costly and often imperfect validation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14616v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Augustine Denteh, Pierre E. Nguimkeu</dc:creator>
    </item>
    <item>
      <title>Causal-ICM: A Data Fusion Framework For Heterogeneous Treatment Effect Estimation With Multi-Task Gaussian Processes</title>
      <link>https://arxiv.org/abs/2405.20957</link>
      <description>arXiv:2405.20957v2 Announce Type: replace 
Abstract: Bridging the gap between internal and external validity is crucial for heterogeneous treatment effect estimation. Randomised controlled trials (RCTs), favoured for their internal validity due to randomisation, often encounter challenges in generalising findings due to strict eligibility criteria. Observational studies, on the other hand, may provide stronger external validity through larger and more representative samples but can suffer from compromised internal validity due to unmeasured confounding. Motivated by these complementary characteristics, we propose a novel Bayesian nonparametric approach, Causal-ICM, leveraging multi-task Gaussian processes to integrate data from both RCTs and observational studies. In particular, we introduce a parameter that controls the degree of borrowing between the datasets and prevents the observational dataset from dominating the estimation. We propose a data-adaptive procedure for choosing the optimal value of the parameter. Causal-ICM outperforms other data fusion methods in point estimation across the covariate support of the observational study and provides principled uncertainty quantification for the estimated treatment effects. We demonstrate the robust performance of Causal-ICM in diverse scenarios through multiple simulation studies and a real-world study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20957v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evangelos Dimitriou, Edwin Fong, Jens Magelund Tarp, Karla Diaz-Ordaz, Brieuc Lehmann</dc:creator>
    </item>
    <item>
      <title>Differential Test Functioning via Robust Scaling</title>
      <link>https://arxiv.org/abs/2409.03502</link>
      <description>arXiv:2409.03502v5 Announce Type: replace 
Abstract: In the item response theory (IRT) literature, differential test functioning (DTF) has been conceptualized in terms of how the test response function differs over groups of respondents. This paper presents an alternative approach to DTF that focusses on how the distribution of the latent trait differs over groups, which is referred to as impact. We propose to compare two estimates of impact, one that naively aggregates over all test items and one that down-weights items that exhibit differential item functioning (DIF). Taking this approach, we make the following three contributions to the literature on DTF. First it is shown that the difference between the two estimates provides a convenient effect size for quantifying the extent to which DIF affects conclusions about impact (as opposed to test scores). Second, we provide a relatively general purpose Wald test of the difference between two estimates of impact. Third, we extend the recent literature on robust scaling to propose a procedure for down-weighting items that is shown to produce consistent estimates of impact whenever fewer than 1/2 of items exhibit DIF. Using simulations and an empirical example from physics education, we show how the proposed effect size and test statistic perform using the proposed robust estimator of impact, as well as estimators that arise from conventional item-by-item tests of DIF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03502v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter F. Halpin</dc:creator>
    </item>
    <item>
      <title>Inference with Randomized Regression Trees</title>
      <link>https://arxiv.org/abs/2412.20535</link>
      <description>arXiv:2412.20535v2 Announce Type: replace 
Abstract: Regression trees are a popular machine learning algorithm that fit piecewise constant models by recursively partitioning the predictor space. This paper focuses on statistical inference for a data-dependent model obtained from a fitted regression tree. We introduce Randomized Regression Trees (RRT), a novel selective inference method that adds independent Gaussian noise to the gain function underlying the splitting rules of classic regression trees.
  The RRT method offers several advantages over existing methods. First, added randomization is used to obtain a closed-form pivot while accounting for the data-dependent tree structure. Second, RRT with a small amount of randomization achieves predictive accuracy similar to a model trained on the entire dataset, while also providing significantly more powerful inference than existing selective inference methods, such as data splitting. Third, RRT yields intervals that automatically adapt to the signal strength in the data. Our empirical analyses highlight these advantages of the RRT method and its ability to convert a purely predictive algorithm into a method capable of performing powerful inference in the non-linear tree model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20535v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Bakshi, Yiling Huang, Snigdha Panigrahi, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>On the Graphical Rules for Recovering the Average Treatment Effect Under Selection Bias</title>
      <link>https://arxiv.org/abs/2502.00924</link>
      <description>arXiv:2502.00924v5 Announce Type: replace 
Abstract: Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are typically coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these existing simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias and recover average treatment effect in these two cases, we propose an alternative set of graphical rules and construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis (i.e., complete-case analysis) yields erroneous conclusions contradictory to the truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00924v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>Inference on the attractor spaces via functional approximation</title>
      <link>https://arxiv.org/abs/2502.06462</link>
      <description>arXiv:2502.06462v2 Announce Type: replace 
Abstract: This paper discusses semiparametric inference on hypotheses on the cointegration and the attractor spaces for $I(1)$ linear processes with moderately large cross-sectional dimension. The approach is based on empirical canonical correlations and functional approximation of Brownian motions, and it can be applied both to the whole system and or to any set of linear combinations of it. The hypotheses of interest are cast in terms of the number of stochastic trends in specified subsystems, and inference is based either on selection criteria or on sequences of tests. This paper derives the limit distribution of these tests in the special one-dimensional case, and discusses asymptotic properties of the derived inference criteria for hypotheses on the attractor space for sequentially diverging sample size and number of basis elements in the functional approximation. Finite sample properties are analyzed via a Monte Carlo study and an empirical illustration on exchange rates is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06462v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimo Franchi, Paolo Paruolo</dc:creator>
    </item>
    <item>
      <title>Modeling and forecasting subnational age distribution of death counts</title>
      <link>https://arxiv.org/abs/2503.16744</link>
      <description>arXiv:2503.16744v2 Announce Type: replace 
Abstract: Existing mortality forecasting methods focus on age-specific mortality rates, which lie in an unconstrained space and overlook the distributional nature of life-table death counts. Few studies have developed and compared forecasting methods that model the shape and dynamics of the age distribution of deaths, especially at the subnational level, where data quality varies greatly. This paper presents several forecasting methods to model and forecast the subnational age distribution of death counts. The age distribution of death counts has many similarities to probability density functions, which are nonnegative and have a constrained integral, and thus live in a constrained nonlinear space. To address the nonlinear nature of objects, we implement a cumulative distribution function transformation that is scale-free and has additional monotonicity. Using subnational Japanese life-table death counts from Japanese Mortality Database (2025), we evaluate the forecast accuracy of the transformation and forecasting methods. The improved forecast accuracy of life-table death counts implemented here will be of great interest to demographers in estimating regional age-specific survival probabilities and life expectancy, and to actuaries for determining annuity prices for various ages and maturities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16744v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Cristian F. Jim\'enez-Var\'on</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v3 Announce Type: replace 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference of model parameters. Specifically, in computational cognitive neuroscience, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle in common application scenarios in which drift rates dynamically vary within trials as a function of exogenous covariates (e.g., brain activity in specific regions or visual fixations). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also considerably outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
    <item>
      <title>Testing for dice control at craps</title>
      <link>https://arxiv.org/abs/2504.13158</link>
      <description>arXiv:2504.13158v3 Announce Type: replace 
Abstract: Dice control involves "setting" the dice and then throwing them carefully, in the hope of influencing the outcomes and gaining an advantage at craps. How does one test for this ability? To specify the alternative hypothesis, we need a statistical model of dice control. Two have been suggested in the gambling literature, namely the Smith-Scott model and the Wong-Shackleford model. Both models are parameterized by $\theta\in[0,1]$, which measures the shooter's level of control. We propose and compare four test statistics: (a) the sample proportion of 7s; (b) the sample proportion of pass-line wins; (c) the sample mean of hand-length observations; and (d) the likelihood ratio statistic for a hand-length sample. We want to test $H_0:\theta = 0$ (no control) versus $H_1:\theta &gt; 0$ (some control). We also want to test $H_0:\theta\le\theta_0$ versus $H_1:\theta&gt;\theta_0$, where $\theta_0$ is the "break-even point." For the tests considered we estimate the power, either by normal approximation or by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13158v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>UNLV Gaming Research &amp; Review Journal 29 (2025), https://oasis.library.unlv.edu/grrj/vol29/iss1/10/</arxiv:journal_reference>
      <dc:creator>Stewart N. Ethier</dc:creator>
    </item>
    <item>
      <title>Undersmoothed LASSO Models for Propensity Score Weighting and Synthetic Negative Control Exposures for Bias Detection</title>
      <link>https://arxiv.org/abs/2506.17760</link>
      <description>arXiv:2506.17760v3 Announce Type: replace 
Abstract: The propensity score (PS) is often used to control for large numbers of covariates in high-dimensional healthcare database studies. The least absolute shrinkage and selection operator (LASSO) has become the most widely used tool for fitting large-scale PS models in these settings. LASSO uses L1 regularized regression to prevent overfitting by shrinking coefficients toward zero (setting some exactly to zero). The degree of regularization is typically selected using cross-validation to minimize out-of-sample prediction error. Both theory and simulations have shown, however, that when using LASSO models for PS weighting, less regularization is needed to minimize bias in PS weighted estimators. This is referred to as undersmoothing the LASSO model, where the optimal degree of undersmoothing can be derived from the target causal parameter's efficient influence function. In many settings, however, the efficient influence function is unknown or difficult to derive. Here, we consider the use of balance metrics as a simple and generally applicable approach to select the degree of undersmoothing when the efficient influence function is unknown. Because LASSO models that are tuned using balance metrics alone are not assured to minimize bias in PS weighted estimators -- as such metrics are blind to the efficient influence function -- we propose a framework to generate synthetic negative control exposures for bias detection. We show that synthetic negative control exposures can identify analyses that likely violate partial exchangeability due to lack of control for measured confounding. Finally, we use a series of numerical studies to investigate the finite sample performance of using balance criteria to undersmooth LASSO PS-weighted estimators, and the use of synthetic negative control exposures to detect biased analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17760v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Wyss, Ben B. Hansen, Georg Hahn, Lars van der Laan, Kueiyu Joshua Lin</dc:creator>
    </item>
    <item>
      <title>Finite-Sample NonParametric Bounds with an Application to the Causal Effect of Workforce Gender Diversity on Firm Performance</title>
      <link>https://arxiv.org/abs/2509.01622</link>
      <description>arXiv:2509.01622v2 Announce Type: replace 
Abstract: Classical Manski bounds identify average treatment effects under minimal assumptions but, in finite samples, they rely on latent outcome expectations being bounded by the sample's own extrema or known population bounds, an assumption often violated in firm-level data with heavy-tailed outcomes. We develop a finite-sample, concentration-driven confidence band (concATE) that replaces this requirement with a Dvoretzky-Kiefer-Wolfowitz tail bound, combines it with delta-method variance, and allocates size via a Bonferroni correction. The band extends to a group-sequential design that controls the family-wise error rate when the first "significant" diversity threshold is data-chosen. Applied to data on 901 listed firms (2015 Q2-2022 Q1), concATE shows that senior-level gender diversity has a significant positive effect on firm value (Tobin's Q) only after crossing substantial representation thresholds: in Growth &amp; Innovation sectors the effect becomes statistically significant at the 5% level once women hold roughly 55% of senior leadership roles, whereas in Defensive sectors a significant impact appears only once female leadership reaches about 60%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01622v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace Lordan, Kaveh Salehzadeh Nobari</dc:creator>
    </item>
    <item>
      <title>Partially Retargeted Balancing Weights for Causal Effect Estimation Under Positivity Violations</title>
      <link>https://arxiv.org/abs/2510.22072</link>
      <description>arXiv:2510.22072v2 Announce Type: replace 
Abstract: Positivity violations, which occur when some subgroups either always or never receive a treatment of interest, pose significant challenges for causal effect estimation with observational data. Recent balancing weight methods have proved to be highly effective in confounding control, however their utility is diminished in the presence of positivity violations, resulting in bias and excess variance. Approaches that deal with positivity violations, on the other hand, work by targeting a modified estimand that may be misaligned with the original research question. To address these challenges, we propose a novel balancing weights approach, which mitigates positivity violations while attempting to retain the original estimand by a targeted relaxation of the balancing constraints. Our proposed weighted estimator is consistent for the original estimand when either 1) the implied propensity score model is correct; or 2) all treatment effect modifiers are balanced to the target population. When these conditions do not hold, our estimator is consistent for a slightly modified treatment effect estimand. Furthermore, our proposed weighted estimator has reduced asymptotic variance when positivity does not hold. We evaluate our approach through applications to synthetic data, an observational study, and when transporting a treatment effect from a randomized trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22072v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martha Barnard, Jared D. Huling, Julian Wolfson</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Network Autoregression Model: A Targeted Minimum Loss Estimation Approach</title>
      <link>https://arxiv.org/abs/2511.06652</link>
      <description>arXiv:2511.06652v2 Announce Type: replace 
Abstract: We study estimation of the average treatment effect (ATE) from a single network in observational settings with interference. The weak cross-unit dependence is modeled via an endogenous peer-effect (network autoregressive) term that induces distance-decaying network dependence, relaxing the common finite-order interference to infinite interference. We propose a targeted minimum loss estimation (TMLE) procedure that removes plug-in bias from an initial estimator. The targeting step yields an adjustment direction that incorporates the network autoregressive structure and assigns heterogeneous, network-dependent weights to units. We find that the asymptotic leading term related to the covariates $\mathbf{X}_i$ can be formulated into a $V$-statistic whose order diverges with the network degrees. A novel limit theory is developed to establish the asymptotic normality under such complex network dependent scenarios. We show that our method can achieve smaller asymptotic variance than existing methods when $\mathbf{X}_i$ is i.i.d. generated and estimated with empirical distribution, and provide theoretical guarantees for estimating the variance. Extensive numerical studies and a live-streaming data analysis are presented to illustrate the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06652v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Wu, Shuyuan Wu, Xinwei Sun, Xuening Zhu</dc:creator>
    </item>
    <item>
      <title>A Recursive Theory of Variational State Estimation: The Dynamic Programming Approach</title>
      <link>https://arxiv.org/abs/2511.11497</link>
      <description>arXiv:2511.11497v2 Announce Type: replace 
Abstract: In this article, variational state estimation is examined from the dynamic programming perspective. This leads to two different value functional recursions depending on whether backward or forward dynamic programming is employed. The result is a theory of variational state estimation that corresponds to the classical theory of Bayesian state estimation. More specifically, in the backward method, the value functional corresponds to a likelihood that is upper bounded by the state likelihood from the Bayesian backward recursion. In the forward method, the value functional corresponds to an unnormalized density that is upper bounded by the unnormalized filtering density. Both methods can be combined to arrive at a variational two-filter formula. Additionally, it is noted that optimal variational filtering is generally of quadratic time-complexity in the sequence length. This motivates the notion of sub-optimal variational filtering, which also lower bounds the evidence but is of linear time-complexity. Another problem is the fact that the value functional recursions are generally intractable. This is briefly discussed and a simple approximation is suggested that retrieves the filter proposed by Courts et. al (2021).The methodology is examined in (i) a jump Gauss-Markov system under a certain factored Markov process approximation, and (ii) in a Gauss-Markov model with log-polynomial likelihoods under a Gauss--Markov constraint on the variational approximation. It is demonstrated that the value functional recursions are tractable in both cases. The resulting estimators are examined in simulation studies and are found to be of adequate quality in comparison to sensible baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11497v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using e-values: Applications for trial monitoring</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v4 Announce Type: replace 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We discuss a nonparametric sequential test and its application to continuous and time-to-event endpoints that derives validity solely from the randomization mechanism. Using a betting framework, these tests constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. These methods provide a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
    <item>
      <title>Model-robust Inference for Seamless II/III Trials with Covariate Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2512.09430</link>
      <description>arXiv:2512.09430v3 Announce Type: replace 
Abstract: Seamless phase II/III trials have become a cornerstone of modern drug development, offering a means to accelerate evaluation while maintaining statistical rigor. However, most existing inference procedures are model-based, designed primarily for continuous outcomes, and often neglect the stratification used in covariate-adaptive randomization (CAR), limiting their practical relevance. In this paper, we propose a unified, model-robust framework for seamless phase II/III trials grounded in generalized linear models (GLMs), enabling valid inference across diverse outcome types, estimands, and CAR schemes. Using Z-estimation, we derive the asymptotic properties of treatment effect estimators and explicitly characterize how their variance depends on the underlying randomization procedure. Based on these results, we develop adjusted Wald tests that, together with Dunnett's multiple-comparison procedure and the inverse chi-square combination method, ensure valid overall Type I error. Extensive simulation studies and a trial example demonstrate that the proposed model-robust tests achieve superior power and reliable inference compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09430v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Yi, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>A fine-grained look at causal effects in causal spaces</title>
      <link>https://arxiv.org/abs/2512.11919</link>
      <description>arXiv:2512.11919v2 Announce Type: replace 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11919v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Yuqing Zhou</dc:creator>
    </item>
    <item>
      <title>Nowcasting using regression on signatures</title>
      <link>https://arxiv.org/abs/2305.10256</link>
      <description>arXiv:2305.10256v2 Announce Type: replace-cross 
Abstract: We introduce a new method of nowcasting using regression on path signatures. Path signatures capture the geometric properties of sequential data. Because signatures embed observations in continuous time, they naturally handle mixed frequencies and missing data. We prove theoretically, and with simulations, that regression on signatures subsumes the linear Kalman filter and retains desirable consistency properties. Nowcasting with signatures is more robust to disruptions in data series than previous methods, making it useful in stressed times (for example, during COVID-19). This approach is performant in nowcasting US GDP growth, and in nowcasting UK unemployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10256v2</guid>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel N. Cohen, Giulia Mantoan, Lars Nesheim, \'Aureo de Paula, Arthur Turrell, Lingyi Yang</dc:creator>
    </item>
    <item>
      <title>Fused $L_{1/2}$ prior for large scale linear inverse problem with Gibbs bouncy particle sampler</title>
      <link>https://arxiv.org/abs/2409.07874</link>
      <description>arXiv:2409.07874v3 Announce Type: replace-cross 
Abstract: In this paper, we study Bayesian approach for solving large scale linear inverse problems arising in various scientific and engineering fields. We propose a fused $L_{1/2}$ prior with edge-preserving and sparsity-promoting properties and show that it can be formulated as a Gaussian mixture Markov random field. Since the density function of this family of prior is neither log-concave nor Lipschitz, gradient-based Markov chain Monte Carlo methods can not be applied to sample the posterior. Thus, we present a Gibbs sampler in which all the conditional posteriors involved have closed form expressions. The Gibbs sampler works well for small size problems but it is computationally intractable for large scale problems due to the need for sample high dimensional Gaussian distribution. To reduce the computation burden, we construct a Gibbs bouncy particle sampler (Gibbs-BPS) based on a piecewise deterministic Markov process. This new sampler combines elements of Gibbs sampler with bouncy particle sampler and its computation complexity is an order of magnitude smaller. We show that the new sampler converges to the target distribution. With computed tomography examples, we demonstrate that the proposed method shows competitive performance with existing popular Bayesian methods and is highly efficient in large scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07874v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan, Qingping Zhou</dc:creator>
    </item>
    <item>
      <title>The Monge optimal transport barycenter problem</title>
      <link>https://arxiv.org/abs/2507.03669</link>
      <description>arXiv:2507.03669v2 Announce Type: replace-cross 
Abstract: A novel methodology is developed for the solution of the data-driven Monge optimal transport barycenter problem, where the pushforward condition is formulated in terms of the statistical independence between two sets of random variables: the factors $z$ and a transformed outcome $y$. Relaxing independence to the uncorrelation between all functions of $z$ and $y$ within suitable finite-dimensional spaces leads to an adversarial formulation, for which the adversarial strategy can be found in closed form through the first principal components of a small-dimensional matrix. The resulting pure minimization problem can be solved very efficiently through gradient descent driven flows in phase space. The methodology extends beyond scenarios where only discrete factors affect the outcome, to multivariate sets of both discrete and continuous factors, for which the corresponding barycenter problems have infinitely many marginals. Corollaries include a new framework for the solution of the Monge optimal transport problem, a procedure for the data-based simulation and estimation of conditional probability densities, and a nonparametric methodology for Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03669v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew D. Lipnick, Esteban G. Tabak, Giulio Trigila, Yating Wang, Xuancheng Ye, Wenjun Zhao</dc:creator>
    </item>
    <item>
      <title>Suicide Mortality in Spain (2010-2022): Temporal Trends, Spatial Patterns, and Risk Factors</title>
      <link>https://arxiv.org/abs/2509.01342</link>
      <description>arXiv:2509.01342v2 Announce Type: replace-cross 
Abstract: Background: Suicide remains a major public health concern worldwide, responsible for more than 700,000 deaths in 2021, accounting for approximately 1.1\% of all global deaths. While many high-income countries have reported declines in age-standardized suicide rates over the past two decades, recent evidence from Spain indicates increasing mortality among women, whereas suicide rates among men have remained relatively stable. To better understand these patterns and their potential underlying determinants, this study examines the spatial and temporal patterns of age-stratified suicide mortality across Spanish provinces from 2010 to 2022, with particular attention to sex-specific differences.
  Methods: Mixed Poisson models were applied to analyze provincial- and temporal-level suicide mortality rates, stratified by age and sex. The models accounted for spatial and temporal confounding effects and examined associations with various socioeconomic and contextual factors, including rurality and unemployment.
  Results: Findings highlight the influence of rurality and unemployment on suicide mortality, with distinct gender-specific patterns. A 10$\%$ increase in the proportion of residents living in rural areas was associated with more than a 5$\%$ rise in male suicide mortality, while a 1$\%$ increase in the annual unemployment rate was linked to a 2.4$\%$ increase in female suicide mortality. Although male suicide rates remained consistently higher than female rates, a notable and steady upward trend was observed in female suicide mortality over the study period.
  Conclusions: The use of sophisticated statistical models permits the detection of underlying patterns, revealing both geographic and temporal disparities in suicide mortality across Spanish provinces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01342v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Adin, G. Retegui, A. S\'anchez Villegas, M. D. Ugarte</dc:creator>
    </item>
    <item>
      <title>Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger</title>
      <link>https://arxiv.org/abs/2510.19077</link>
      <description>arXiv:2510.19077v2 Announce Type: replace-cross 
Abstract: Background: Target trial emulation (TTE) that applies trial design principles to improve the analysis of non-randomized studies is increasingly being used. Applications of TTE to emulate cluster randomized trials (RCTs) have been limited. This study explored how to integrate simulation-guided design into the TTE framework to inform planning of a non-randomized cluster trial. Methods: We performed simulations to prospectively plan data collection of a non-randomized study emulating a village-level cluster RCT when cluster-randomization was infeasible. The planned study will assess the impact of mass distribution of nutritional supplements embedded within an existing immunization program to improve pentavalent vaccination rates among children 12-24 months old in Niger. The design included covariate-constrained random selection of villages for outcome ascertainment at follow-up. Simulations used baseline census data on pentavalent vaccination rates and cluster-level covariates to compare the type I error rate and power of four statistical methods: beta-regression; quasi-binomial regression; inverse probability of treatment weighting (IPTW); and naive Wald test. Results: Of the four analytic methods considered, only IPTW and beta-regression controlled the type I error rate at 0.05, but IPTW yielded poor statistical power. Beta-regression that showed adequate statistical power was chosen as our primary analysis. Conclusions: Adopting simulation-guided design principles within TTE can enable robust planning of a group-level non-randomized study emulating a cluster RCT. Lessons from this study also apply to TTE planning of individually-RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19077v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shomoita Alam, Nathaniel Dyrkton, Susan Shepherd, Ibrahim Sana, Kevin Phelan, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning</title>
      <link>https://arxiv.org/abs/2512.09368</link>
      <description>arXiv:2512.09368v2 Announce Type: replace-cross 
Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/AdvancedAI-ComplexSystem/SmartCity/tree/main/CFLight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09368v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mingyuan Li, Chunyu Liu, Zhuojun Li, Xiao Liu, Guangsheng Yu, Bo Du, Jun Shen, Qiang Wu</dc:creator>
    </item>
    <item>
      <title>Spatial-Network Treatment Effects: A Continuous Functional Approach</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v3 Announce Type: replace-cross 
Abstract: This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
