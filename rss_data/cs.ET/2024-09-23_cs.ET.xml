<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Is 3D chip technology the next growth engine for performance improvement?</title>
      <link>https://arxiv.org/abs/2409.14527</link>
      <description>arXiv:2409.14527v1 Announce Type: new 
Abstract: The semiconductor industry is reaching a fascinating confluence in several evolutionary trends that will likely lead to a number of revolutionary changes in how computer systems are designed, implemented, scaled, and used. Since Moores Law, which has driven the evolution in systems for the last several decades, is imminently approaching real and severe limitations, the ability to create three-dimensional,3D, device stacks appears promising as a way to continue to integrate more devices into a chip.While on the one hand, this nascent ability to make 3D technology can be interpreted as merely an extension of Moores Law, on the other hand, the fact that systems can now be integrated across multiple planes poses some novel opportunities, as well as serious challenges and questions. In this paper, we explore these various challenges and opportunities and discuss structures and systems that are likely to be facilitated by 3D technology. We also describe the ways in which these systems are likely to change. Since 3D technology offers some different value propositions, we expect that some of the most important ways in which 3D technology will likely impact our approach to future systems design, implementation, and usage are not yet obvious to most system designers, and we outline several of them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14527v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IBM Journal of Research and Development, Volume 52, Pages 541-552, 2008/11</arxiv:journal_reference>
      <dc:creator>Philip Emma, Eren Kurshan</dc:creator>
    </item>
    <item>
      <title>Semantic Communication Enabled 6G-NTN Framework: A Novel Denoising and Gateway Hop Integration Mechanism</title>
      <link>https://arxiv.org/abs/2409.14726</link>
      <description>arXiv:2409.14726v1 Announce Type: new 
Abstract: The sixth-generation (6G) non-terrestrial networks (NTNs) are crucial for real-time monitoring in critical applications like disaster relief. However, limited bandwidth, latency, rain attenuation, long propagation delays, and co-channel interference pose challenges to efficient satellite communication. Therefore, semantic communication (SC) has emerged as a promising solution to improve transmission efficiency and address these issues. In this paper, we explore the potential of SC as a bandwidth-efficient, latency-minimizing strategy specifically suited to 6G satellite communications. While existing SC methods have demonstrated efficacy in direct satellite-terrestrial transmissions, they encounter limitations in satellite networks due to distortion accumulation across gateway hop-relays. Additionally, certain ground users (GUs) experience poor signal-to-noise ratios (SNR), making direct satellite communication challenging. To address these issues, we propose a novel framework that optimizes gateway hop-relay selection for GUs with low SNR and integrates gateway-based denoising mechanisms to ensure high-quality-of-service (QoS) in satellite-based SC networks. This approach directly mitigates distortion, leading to significant improvements in satellite service performance by delivering customized services tailored to the unique signal conditions of each GU. Our findings represent a critical advancement in reliable and efficient data transmission from the Earth observation satellites, thereby enabling fast and effective responses to urgent events. Simulation results demonstrate that our proposed strategy significantly enhances overall network performance, outperforming conventional methods by offering tailored communication services based on specific GU conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14726v1</guid>
      <category>cs.ET</category>
      <category>eess.SP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loc X. Nguyen, Sheikh Salman Hassan, Yan Kyaw Tun, Kitae Kim, Zhu Han, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>AmpAgent: An LLM-based Multi-Agent System for Multi-stage Amplifier Schematic Design from Literature for Process and Performance Porting</title>
      <link>https://arxiv.org/abs/2409.14739</link>
      <description>arXiv:2409.14739v1 Announce Type: new 
Abstract: Multi-stage amplifiers are widely applied in analog circuits. However, their large number of components, complex transfer functions, and intricate pole-zero distributions necessitate extensive manpower for derivation and param sizing to ensure their stability. In order to achieve efficient derivation of the transfer function and simplify the difficulty of circuit design, we propose AmpAgent: a multi-agent system based on large language models (LLMs) for efficiently designing such complex amplifiers from literature with process and performance porting. AmpAgent is composed of three agents: Literature Analysis Agent, Mathematics Reasoning Agent and Device Sizing Agent. They are separately responsible for retrieving key information (e.g. formulas and transfer functions) from the literature, decompose the whole circuit's design problem by deriving the key formulas, and address the decomposed problem iteratively.
  AmpAgent was employed in the schematic design of seven types of multi-stage amplifiers with different compensation techniques. In terms of design efficiency, AmpAgent has reduced the number of iterations by 1.32$ \sim $4${\times}$ and execution time by 1.19$ \sim $2.99${\times}$ compared to conventional optimization algorithms, with a success rate increased by 1.03$ \sim $6.79${\times}$. In terms of circuit performance, it has improved by 1.63$ \sim $27.25${\times}$ compared to the original literature. The findings suggest that LLMs could play a crucial role in the field of complex analog circuit schematic design, as well as process and performance porting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14739v1</guid>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengjie Liu, Weiyu Chen, Anlan Peng, Yuan Du, Li Du, Jun Yang</dc:creator>
    </item>
    <item>
      <title>Declarative Integration and Management of Large Language Models through Finite Automata: Application to Automation, Communication, and Ethics</title>
      <link>https://arxiv.org/abs/2409.13693</link>
      <description>arXiv:2409.13693v1 Announce Type: cross 
Abstract: This article introduces an innovative architecture designed to declaratively combine Large Language Models (LLMs) with shared histories, and triggers to identify the most appropriate LLM for a given task. Our approach is general and declarative, relying on the construction of finite automata coupled with an event management system. The developed tool is crafted to facilitate the efficient and complex integration of LLMs with minimal programming effort, especially, but not only, for integrating methods of positive psychology to AI. The flexibility of our technique is demonstrated through applied examples in automation, communication, and ethics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13693v1</guid>
      <category>cs.FL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thierry Petit, Arnault Pachot, Claire Conan-Vrinat, Alexandre Dubarry</dc:creator>
    </item>
    <item>
      <title>Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder</title>
      <link>https://arxiv.org/abs/2409.13747</link>
      <description>arXiv:2409.13747v1 Announce Type: cross 
Abstract: This project, titled "Machine Translation with Large Language Models: Decoder-only vs. Encoder-Decoder," aims to develop a multilingual machine translation (MT) model. Focused on Indian regional languages, especially Telugu, Tamil, and Malayalam, the model seeks to enable accurate and contextually appropriate translations across diverse language pairs. By comparing Decoder-only and Encoder-Decoder architectures, the project aims to optimize translation quality and efficiency, advancing cross-linguistic communication tools.The primary objective is to develop a model capable of delivering high-quality translations that are accurate and contextually appropriate. By leveraging large language models, specifically comparing the effectiveness of Decoder-only and Encoder-Decoder architectures, the project seeks to optimize translation performance and efficiency across multilingual contexts. Through rigorous experimentation and analysis, this project aims to advance the field of machine translation, contributing valuable insights into the effectiveness of different model architectures and paving the way for enhanced cross-linguistic communication tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13747v1</guid>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Abhinav P. M., SujayKumar Reddy M, Oswald Christopher</dc:creator>
    </item>
    <item>
      <title>Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models</title>
      <link>https://arxiv.org/abs/2409.13753</link>
      <description>arXiv:2409.13753v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have increasingly demonstrated the ability to facilitate the development of multi-agent systems that allow the interpretation of thoughts and actions generated by each individual. Promising advancements have also been made in LLM-based interaction with existing worlds, particularly in interacting with simulated environments. This paper aims to integrate both aforementioned topics (agents &amp; world interaction) into a single simulation where multiple agents can work together to solve a problem, modeling how groups of humans can often solve problems better than individuals. By showing whether LLMs demonstrate the synergy of human collaboration, it could lead to advancements in the applications of LLMs. We implemented two simulations: a physical studio apartment with two roommates, and another where agents collaborate to complete a programming task. We provide a multi-agent framework, discuss the performance of the agents in each simulation, and discuss potential future additions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13753v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asher Sprigler, Alexander Drobek, Keagan Weinstock, Wendpanga Tapsoba, Gavin Childress, Andy Dao, Lucas Gral</dc:creator>
    </item>
    <item>
      <title>An Evolutionary Algorithm For the Vehicle Routing Problem with Drones with Interceptions</title>
      <link>https://arxiv.org/abs/2409.14173</link>
      <description>arXiv:2409.14173v1 Announce Type: cross 
Abstract: The use of trucks and drones as a solution to address last-mile delivery challenges is a new and promising research direction explored in this paper. The variation of the problem where the drone can intercept the truck while in movement or at the customer location is part of an optimisation problem called the vehicle routing problem with drones with interception (VRPDi). This paper proposes an evolutionary algorithm to solve the VRPDi. In this variation of the VRPDi, multiple pairs of trucks and drones need to be scheduled. The pairs leave and return to a depot location together or separately to make deliveries to customer nodes. The drone can intercept the truck after the delivery or meet up with the truck at the following customer location. The algorithm was executed on the travelling salesman problem with drones (TSPD) datasets by Bouman et al. (2015), and the performance of the algorithm was compared by benchmarking the results of the VRPDi against the results of the VRP of the same dataset. This comparison showed improvements in total delivery time between 39% and 60%. Further detailed analysis of the algorithm results examined the total delivery time, distance, node delivery scheduling and the degree of diversity during the algorithm execution. This analysis also considered how the algorithm handled the VRPDi constraints. The results of the algorithm were then benchmarked against algorithms in Dillon et al. (2023) and Ernst (2024). The latter solved the problem with a maximum drone distance constraint added to the VRPDi. The analysis and benchmarking of the algorithm results showed that the algorithm satisfactorily solved 50 and 100-nodes problems in a reasonable amount of time, and the solutions found were better than those found by the algorithms in Dillon et al. (2023) and Ernst (2024) for the same problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14173v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Pambo, Jacomine Grobler</dc:creator>
    </item>
    <item>
      <title>Quantum Computing for Automotive Applications: From Algorithms to Applications</title>
      <link>https://arxiv.org/abs/2409.14183</link>
      <description>arXiv:2409.14183v1 Announce Type: cross 
Abstract: Quantum computing could impact various industries, with the automotive industry with many computational challenges, from optimizing supply chains and manufacturing to vehicle engineering, being particularly promising. This chapter investigates state-of-the-art quantum algorithms to enhance efficiency, accuracy, and scalability across the automotive value chain. We explore recent advances in quantum optimization, machine learning, and numerical and chemistry simulations, highlighting their potential and limitations. We identify and discuss key challenges in near-term and fault-tolerant algorithms and their practical use in industrial applications. While quantum algorithms show potential in many application domains, current noisy intermediate-scale quantum hardware limits scale and, thus, business benefits. In the long term, fault-tolerant systems promise theoretical speedups; however, they also require further progress in hardware and software (e.\,g., related to error correction and data loading). We expect that with this progress, significant practical benefits will emerge eventually.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14183v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> BMW Group Quantum Team, Johannes Klepsch, Jernej Rudi Fin\v{z}gar, Florian Kiwit, Leonhard H\"olscher, Marvin Erdmann, Lukas M\"uller, Chandan Kumar, Andre Luckow</dc:creator>
    </item>
    <item>
      <title>Gate Optimization of NEQR Quantum Circuits via PPRM Transformation</title>
      <link>https://arxiv.org/abs/2409.14629</link>
      <description>arXiv:2409.14629v1 Announce Type: cross 
Abstract: Quantum image representation (QIR) is a key challenge in quantum image processing (QIP) due to the large number of pixels in images, which increases the need for quantum gates and qubits. However, current quantum systems face limitations in run-time complexity and available qubits. This work aims to compress the quantum circuits of the Novel Enhanced Quantum Representation (NEQR) scheme by transforming their Exclusive-Or Sum-of-Products (ESOP) expressions into Positive Polarity Reed-Muller (PPRM) equivalents without adding ancillary qubits. Two cases of run-time complexity, exponential and linear, are considered for NEQR circuits with m controlling qubits ($m \rightarrow \infty$), depending on the decomposition of multi-controlled NOT gates. Using nonlinear regression, the proposed transformation is estimated to reduce the exponential complexity from $O(2^m)$ to $O(1.5^m)$, with a compression ratio approaching 100%. For linear complexity, the transformation is estimated to halve the run-time, with a compression ratio approaching 52%. Tests on six 256$\times$256 images show average reductions of 105.5 times for exponential cases and 2.4 times for linear cases, with average compression ratios of 99.05% and 58.91%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14629v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahab Iranmanesh, Hossein Aghababa, Kazim Fouladi</dc:creator>
    </item>
    <item>
      <title>Explainable and Human-Grounded AI for Decision Support Systems: The Theory of Epistemic Quasi-Partnerships</title>
      <link>https://arxiv.org/abs/2409.14839</link>
      <description>arXiv:2409.14839v1 Announce Type: cross 
Abstract: In the context of AI decision support systems (AI-DSS), we argue that meeting the demands of ethical and explainable AI (XAI) is about developing AI-DSS to provide human decision-makers with three types of human-grounded explanations: reasons, counterfactuals, and confidence, an approach we refer to as the RCC approach. We begin by reviewing current empirical XAI literature that investigates the relationship between various methods for generating model explanations (e.g., LIME, SHAP, Anchors), the perceived trustworthiness of the model, and end-user accuracy. We demonstrate how current theories about what constitutes good human-grounded reasons either do not adequately explain this evidence or do not offer sound ethical advice for development. Thus, we offer a novel theory of human-machine interaction: the theory of epistemic quasi-partnerships (EQP). Finally, we motivate adopting EQP and demonstrate how it explains the empirical evidence, offers sound ethical advice, and entails adopting the RCC approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14839v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Dorsch, Maximilian Moll</dc:creator>
    </item>
    <item>
      <title>A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming Sequence Processing</title>
      <link>https://arxiv.org/abs/2409.15022</link>
      <description>arXiv:2409.15022v1 Announce Type: cross 
Abstract: Deep State-Space Models (SSM) demonstrate state-of-the art performance on long-range sequence modeling tasks. While the recurrent structure of SSMs can be efficiently implemented as a convolution or as a parallel scan during training, recurrent token-by-token processing cannot currently be implemented efficiently on GPUs. Here, we demonstrate efficient token-by-token inference of the SSM S4D on Intel's Loihi 2 state-of-the-art neuromorphic processor. We compare this first ever neuromorphic-hardware implementation of an SSM on sMNIST, psMNIST, and sCIFAR to a recurrent and a convolutional implementation of S4D on Jetson Orin Nano (Jetson). While we find Jetson to perform better in an offline sample-by-sample based batched processing mode, Loihi 2 outperforms during token-by-token based processing, where it consumes 1000 times less energy with a 75 times lower latency and a 75 times higher throughput compared to the recurrent implementation of S4D on Jetson. This opens up new avenues towards efficient real-time streaming applications of SSMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15022v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svea Marie Meyer, Philipp Weidel, Philipp Plank, Leobardo Campos-Macias, Sumit Bam Shrestha, Philipp Stratmann, Mathis Richter</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection from a Tensor Train Perspective</title>
      <link>https://arxiv.org/abs/2409.15030</link>
      <description>arXiv:2409.15030v1 Announce Type: cross 
Abstract: We present a series of algorithms in tensor networks for anomaly detection in datasets, by using data compression in a Tensor Train representation. These algorithms consist of preserving the structure of normal data in compression and deleting the structure of anomalous data. The algorithms can be applied to any tensor network representation. We test the effectiveness of the methods with digits and Olivetti faces datasets and a cybersecurity dataset to determine cyber-attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15030v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Mata Ali, Aitor Moreno Fdez. de Leceta, Jorge L\'opez Rubio</dc:creator>
    </item>
    <item>
      <title>MACeIP: A Multimodal Ambient Context-enriched Intelligence Platform in Smart Cities</title>
      <link>https://arxiv.org/abs/2409.15243</link>
      <description>arXiv:2409.15243v1 Announce Type: cross 
Abstract: This paper presents a Multimodal Ambient Context-enriched Intelligence Platform (MACeIP) for Smart Cities, a comprehensive system designed to enhance urban management and citizen engagement. Our platform integrates advanced technologies, including Internet of Things (IoT) sensors, edge and cloud computing, and Multimodal AI, to create a responsive and intelligent urban ecosystem. Key components include Interactive Hubs for citizen interaction, an extensive IoT sensor network, intelligent public asset management, a pedestrian monitoring system, a City Planning Portal, and a Cloud Computing System. We demonstrate the prototype of MACeIP in several cities, focusing on Fredericton, New Brunswick. This work contributes to innovative city development by offering a scalable, efficient, and user-centric approach to urban intelligence and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15243v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Truong Thanh Hung Nguyen, Phuc Truong Loc Nguyen, Monica Wachowicz, Hung Cao</dc:creator>
    </item>
    <item>
      <title>An All-Optical General-Purpose CPU and Optical Computer Architecture</title>
      <link>https://arxiv.org/abs/2403.00045</link>
      <description>arXiv:2403.00045v2 Announce Type: replace 
Abstract: Energy efficiency of electronic digital processors is primarily limited by the energy consumption of electronic communication and interconnects. The industry is almost unanimously pushing towards replacing both long-haul, as well as local chip interconnects, using optics to drastically increase efficiency. In this paper, we explore what comes after the successful migration to optical interconnects, as with this inefficiency solved, the main source of energy consumption will be electronic digital computing, memory and electro-optical conversion. Our approach attempts to address all these issues by introducing efficient all-optical digital computing and memory, which in turn eliminates the need for electro-optical conversions. Here, we demonstrate for the first time a scheme to enable general purpose digital data processing in an integrated form and present our photonic integrated circuit (PIC) implementation. For this demonstration we implemented a URISC architecture capable of running any classical piece of software all-optically and present a comprehensive architectural framework for all-optical computing to go beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00045v2</guid>
      <category>cs.ET</category>
      <category>physics.optics</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JLT.2024.3458459</arxiv:DOI>
      <dc:creator>Michael Kissner, Leonardo Del Bino, Felix P\"asler, Peter Caruana, George Ghalanos</dc:creator>
    </item>
    <item>
      <title>Beyond Diagonal RIS for 6G Non-Terrestrial Networks: Potentials and Challenges</title>
      <link>https://arxiv.org/abs/2406.10691</link>
      <description>arXiv:2406.10691v2 Announce Type: replace 
Abstract: Reconfigurable intelligent surface (RIS) has emerged as a promising technology in both terrestrial and non-terrestrial networks (NTNs) due to its ability to manipulate wireless environments for better connectivity. Significant studies have been focused on conventional RIS with diagonal phase response matrices. This simple RIS architecture, though less expensive, has limited flexibility in engineering the wireless channels. As the latest member of RIS technology, beyond diagonal RIS (BD-RIS) has recently been proposed in terrestrial setups. Due to the interconnected phase response elements (PREs), BD-RIS significantly enhances the control over the wireless environment. This work proposes the potential and challenges of BD-RIS in NTNs. We begin with the motivation and recent advances in BD-RIS. Subsequently, we discuss the fundamentals of BD-RIS and NTNs. We then outline the application of BD-RIS in NTNs, followed by a case study on BD-RIS enabled non-orthogonal multiple access low earth orbit satellite communication. Finally, we highlight challenges and research directions with concluding remarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10691v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wali Ullah Khan, Asad Mahmood, Muhammad Ali Jamshed, Eva Lagunas, Manzoor Ahmed, Symeon Chatzinotas</dc:creator>
    </item>
    <item>
      <title>Circuit decompositions and scheduling for neutral atom devices with limited local addressability</title>
      <link>https://arxiv.org/abs/2307.14996</link>
      <description>arXiv:2307.14996v2 Announce Type: replace-cross 
Abstract: Despite major ongoing advancements in neutral atom hardware technology, there remains limited work in systems-level software tailored to overcoming the challenges of neutral atom quantum computers. In particular, most current neutral atom architectures do not natively support local addressing of single-qubit rotations about an axis in the xy-plane of the Bloch sphere. Instead, these are executed via global beams applied simultaneously to all qubits. While previous neutral atom experimental work has used straightforward synthesis methods to convert short sequences of operations into this native gate set, these methods cannot be incorporated into a systems-level framework nor applied to entire circuits without imposing impractical amounts of serialization. Without sufficient compiler optimizations, decompositions involving global gates will significantly increase circuit depth, gate count, and accumulation of errors. No prior compiler work has addressed this, and adapting existing compilers to solve this problem is nontrivial.
  In this paper, we present an optimized compiler pipeline that translates an input circuit from an arbitrary gate set into a realistic neutral atom native gate set containing global gates. We focus on decomposition and scheduling passes that minimize the final circuit's global gate count and total global rotation amount. As we show, these costs contribute the most to the circuit's duration and overall error, relative to costs incurred by other gate types. Compared to the unoptimized version of our compiler pipeline, minimizing global gate costs gives up to 4.77x speedup in circuit duration. Compared to the closest prior existing work, we achieve up to 53.8x speedup. For large circuits, we observe a few orders of magnitude improvement in circuit fidelities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14996v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalia Nottingham, Michael A. Perlin, Dhirpal Shah, Ryan White, Hannes Bernien, Frederic T. Chong, Jonathan M. Baker</dc:creator>
    </item>
    <item>
      <title>Magic Mirror on the Wall, How to Benchmark Quantum Error Correction Codes, Overall ?</title>
      <link>https://arxiv.org/abs/2402.11105</link>
      <description>arXiv:2402.11105v4 Announce Type: replace-cross 
Abstract: Quantum Error Correction Codes (QECCs) are pivotal in advancing quantum computing by protecting quantum states against the adverse effects of noise and errors. With a variety of QECCs developed, including new developments and modifications of existing ones, selecting an appropriate QECC tailored to specific conditions is crucial. Despite significant improvements in the field of QECCs, a unified methodology for evaluating them on a consistent basis has remained elusive. Addressing this gap, this paper presents the first benchmarking framework for QECCs, introducing a set of universal parameters. By evaluating eight prominent QECCs, we propose a comprehensive suite of eight parameters for their analysis. Our methodology establishes a universal benchmarking approach and highlights the complexity of quantum error correction, indicating that the choice of a QECC depends on the unique requirements and limitations of each scenario. Furthermore, we develop a systematic strategy for selecting QECCs that adapts to the specific requirements of a given scenario, facilitating a tailored approach to quantum error correction. Additionally, we introduce a novel QECC recommendation tool that assesses the characteristics of a given scenario provided by the user, subsequently recommending a spectrum of QECCs from most to least suitable, along with the maximum achievable distance for each code. This tool is designed to be adaptable, allowing for the inclusion of new QECCs and the modification of their parameters with minimal effort, ensuring its relevance in the evolving landscape of quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11105v4</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Avimita Chatterjee, Swaroop Ghosh</dc:creator>
    </item>
    <item>
      <title>SocialEyes: Scaling mobile eye-tracking to multi-person social settings</title>
      <link>https://arxiv.org/abs/2407.06345</link>
      <description>arXiv:2407.06345v2 Announce Type: replace-cross 
Abstract: Eye movements provide a window into human behaviour, attention, and interaction dynamics. Challenges in real-world, multi-person environments have, however, restrained eye-tracking research predominantly to single-person, in-lab settings. We developed a system to stream, record, and analyse synchronised data from multiple mobile eye-tracking devices during collective viewing experiences (e.g., concerts, films, lectures). We implemented lightweight operator interfaces for real-time-monitoring, remote-troubleshooting, and gaze-projection from individual egocentric perspectives to a common coordinate space for shared gaze analysis. We tested the system in a live concert and a film screening with 30 simultaneous viewers during each of two public events (N=60). We observe precise time-synchronisation between devices measured through recorded clock-offsets, and accurate gaze-projection in challenging dynamic scenes. Our novel analysis metrics and visualizations illustrate the potential of collective eye-tracking data for understanding collaborative behaviour and social interaction. This advancement promotes ecological validity in eye-tracking research and paves the way for innovative interactive tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06345v2</guid>
      <category>cs.HC</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shreshth Saxena, Areez Visram, Neil Lobo, Zahid Mirza, Mehak Rafi Khan, Biranugan Pirabaharan, Alexander Nguyen, Lauren K. Fink</dc:creator>
    </item>
    <item>
      <title>The Landscape of GPU-Centric Communication</title>
      <link>https://arxiv.org/abs/2409.09874</link>
      <description>arXiv:2409.09874v2 Announce Type: replace-cross 
Abstract: In recent years, GPUs have become the preferred accelerators for HPC and ML applications due to their parallelism and fast memory bandwidth. While GPUs boost computation, inter-GPU communication can create scalability bottlenecks, especially as the number of GPUs per node and cluster grows. Traditionally, the CPU managed multi-GPU communication, but advancements in GPU-centric communication now challenge this CPU dominance by reducing its involvement, granting GPUs more autonomy in communication tasks, and addressing mismatches in multi-GPU communication and computation.
  This paper provides a landscape of GPU-centric communication, focusing on vendor mechanisms and user-level library supports. It aims to clarify the complexities and diverse options in this field, define the terminology, and categorize existing approaches within and across nodes. The paper discusses vendor-provided mechanisms for communication and memory management in multi-GPU execution and reviews major communication libraries, their benefits, challenges, and performance insights. Then, it explores key research paradigms, future outlooks, and open research questions. By extensively describing GPU-centric communication techniques across the software and hardware stacks, we provide researchers, programmers, engineers, and library designers insights on how to exploit multi-GPU systems at their best.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09874v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Didem Unat, Ilyas Turimbetov, Mohammed Kefah Taha Issa, Do\u{g}an Sa\u{g}bili, Flavio Vella, Daniele De Sensi, Ismayil Ismayilov</dc:creator>
    </item>
    <item>
      <title>Pennsieve: A Collaborative Platform for Translational Neuroscience and Beyond</title>
      <link>https://arxiv.org/abs/2409.10509</link>
      <description>arXiv:2409.10509v2 Announce Type: replace-cross 
Abstract: The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.
  Pennsieve forms the core for major neuroscience research programs including NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10509v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zack Goldblum, Zhongchuan Xu, Haoer Shi, Patryk Orzechowski, Jamaal Spence, Kathryn A Davis, Brian Litt, Nishant Sinha, Joost Wagenaar</dc:creator>
    </item>
  </channel>
</rss>
