<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Apr 2024 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatio-temporal Modeling of Count Data</title>
      <link>https://arxiv.org/abs/2404.02982</link>
      <description>arXiv:2404.02982v1 Announce Type: new 
Abstract: We introduce parsimonious parameterisations for multivariate autoregressive count time series models for spatio-temporal data, including possible regressions on covariates. The number of parameters is reduced by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. Consistency and asymptotic normality of the parameter estimators are obtained under mild assumptions by employing quasi-maximum likelihood methodology. This is used to obtain an asymptotic Wald test for testing the significance of individual or group effects. Several simulations and two data examples support and illustrate the methods proposed in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02982v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Maletz, Konstantinos Fokianos, Roland Fried</dc:creator>
    </item>
    <item>
      <title>General Effect Modelling (GEM) -- Part 1. Method description</title>
      <link>https://arxiv.org/abs/2404.03024</link>
      <description>arXiv:2404.03024v1 Announce Type: new 
Abstract: We present a flexible tool, called General Effect Modelling (GEM), for the analysis of any multivariate data influenced by one or more qualitative (categorical) or quantitative (continuous) input variables. The variables can be design factors or observed values, e.g., age, sex, or income, or they may represent subgroups of the samples found by data exploration. The first step in GEM separates the variation in the multivariate data into effect matrices associated with each of the influencing variables (and possibly interactions between these) by applying a general linear model. The residuals of the model are added to each of the effect matrices and the results are called Effect plus Residual (ER) values. The tables of ER values have the same dimensions as the original multivariate data. The second step of GEM is a multi- or univariate exploration of the ER tables to learn more about the multivariate data in relation to each input variables. The exploration is simplified as it addresses one input variable at the time, or if preferred, a combination of input variables. One example is a study to identify molecular fingerprints associated with a disease that is not influenced by age where individuals at different ages with and without the disease are included in the experiment. This situation can be described as an experiment with two input variables: the targeted disease and the individual age. Through GEM, the effect of age can be removed, thus focusing further analysis on the targeted disease without the influence of the confounding effect of age. ER values can also be the combined effect of several input variables. This publication has three parts: the first part describes the GEM methodology, Part 2 is a consideration of multivariate data and the benefits of treating such data by multivariate analysis, with a focus on omics data, and Part 3 is a case study in Multiple Sclerosis (MS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03024v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellen F{\ae}rgestad Mosleth, Kristian Hovde Liland</dc:creator>
    </item>
    <item>
      <title>General Effect Modelling (GEM) -- Part 2. Multivariate GEM applied to gene expression data of type 2 diabetes detects information that is lost by univariate validation</title>
      <link>https://arxiv.org/abs/2404.03029</link>
      <description>arXiv:2404.03029v1 Announce Type: new 
Abstract: General Effect Modelling (GEM) is an umbrella over different methods that utilise effects in the analyses of data with multiple design variables and multivariate responses. To demonstrate the methodology, we here use GEM in gene expression data where we use GEM to combine data from different cohorts and apply multivariate analysis of the effects of the targeted disease across the cohorts. Omics data are by nature multivariate, yet univariate analysis is the dominating approach used for such data. A major challenge in omics data is that the number of features such as genes, proteins and metabolites are often very large, whereas the number of samples is limited. Furthermore, omics research aims to obtain results that are generically valid across different backgrounds. The present publication applies GEM to address these aspects. First, we emphasise the benefit of multivariate analysis for multivariate data. Then we illustrate the use of GEM to combine data from two different cohorts for multivariate analysis across the cohorts, and we highlight that multivariate analysis can detect information that is lost by univariate validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03029v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellen F{\ae}rgestad Mosleth, Simon Erling Nitter Dankel, Gunnar Mellgren, Francisco Martin Barajas Olmos, Lorena Sofia Orozco, Artem Lysenko, Ragni Ofstad, Most Champa Begum, Harald Martens, Kristian Hovde Liland</dc:creator>
    </item>
    <item>
      <title>General Effect Modelling (GEM) -- Part 3. GEM applied on proteome data of cerebrospinal fluid of multiple sclerosis and clinically isolated syndrome</title>
      <link>https://arxiv.org/abs/2404.03034</link>
      <description>arXiv:2404.03034v1 Announce Type: new 
Abstract: The novel data analytical platform General Effect Modelling (GEM), is an umbrella platform covering different data analytical methods that handle data with multiple design variables (or pseudo design variables) and multivariate responses. GEM is here demonstrated in an analysis of proteome data from cerebrospinal fluid (CSF) from two independent previously published datasets, one data set comprised of persons with relapsing-remitting multiple sclerosis, persons with other neurological disorders and persons without neurological disorders, and one data set had persons with clinically isolated syndrome (CIS), which is the first clinical symptom of MS, and controls. The primary aim of the present publication is to use these data to demonstrate how patient stratification can be utilised by GEM for multivariate analysis. We also emphasize how the findings shed light on important aspects of the molecular mechanism of MS that may otherwise be lost. We identified proteins involved in neural development as significantly lower for MS/CIS than for their respective controls. This information was only seen after stratification of the persons into two groups, which were found to have different inflammatory patterns and the utilisation of this by GEM. Our conclusion from the study of these data is that disrupted neural development may be an early event in CIS and MS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03034v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellen F{\ae}rgestad Mosleth, Kjell-Morten Myhr, Christian Alexander Vedeler, Frode Steingrimsen Berven, Artem Lysenko, Sonia Gavasso, Kristian Hovde Liland</dc:creator>
    </item>
    <item>
      <title>Asymptotically-exact selective inference for quantile regression</title>
      <link>https://arxiv.org/abs/2404.03059</link>
      <description>arXiv:2404.03059v1 Announce Type: new 
Abstract: When analyzing large datasets, it is common to select a model prior to making inferences. For reliable inferences, it is important to make adjustments that account for the model selection process, resulting in selective inferences. Our paper introduces an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and ensures asymptotically-exact selective inferences without making strict distributional assumptions about the response variable. At the core of the pivot is the use of external randomization, which enables us to utilize the full sample for both selection and inference without the need to partition the data into independent data subsets or discard data at either step. On simulated data, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03059v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumeng Wang, Snigdha Panigrahi, Xuming He</dc:creator>
    </item>
    <item>
      <title>A Bayesian factor analysis model for high-dimensional microbiome count data</title>
      <link>https://arxiv.org/abs/2404.03127</link>
      <description>arXiv:2404.03127v1 Announce Type: new 
Abstract: Dimension reduction techniques are among the most essential analytical tools in the analysis of high-dimensional data. Generalized principal component analysis (PCA) is an extension to standard PCA that has been widely used to identify low-dimensional features in high-dimensional discrete data, such as binary, multi-category and count data. For microbiome count data in particular, the multinomial PCA is a natural counterpart of the standard PCA. However, this technique fails to account for the excessive number of zero values, which is frequently observed in microbiome count data. To allow for sparsity, zero-inflated multivariate distributions can be used. We propose a zero-inflated probabilistic PCA model for latent factor analysis. The proposed model is a fully Bayesian factor analysis technique that is appropriate for microbiome count data analysis. In addition, we use the mean-field-type variational family to approximate the marginal likelihood and develop a classification variational approximation algorithm to fit the model. We demonstrate the efficiency of our procedure for predictions based on the latent factors and the model parameters through simulation experiments, showcasing its superiority over competing methods. This efficiency is further illustrated with two real microbiome count datasets. The method is implemented in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03127v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isma\"ila Ba, Maxime Turgeon, Simona Veniamin, Juan Joel, Richard Miller, Morag Graham, Christine Bonner, Charles N. Bernstein, Douglas L. Arnold, Amit Bar-Or, Ruth Ann Marrie, Julia O'Mahony, E. Ann Yeh, Brenda Banwell, Emmanuelle Waubant, Natalie Knox, Gary Van Domselaar, Ali I. Mirza, Heather Armstrong, Saman Muthukumarana, Kevin McGregor</dc:creator>
    </item>
    <item>
      <title>Orthogonal calibration via posterior projections with applications to the Schwarzschild model</title>
      <link>https://arxiv.org/abs/2404.03152</link>
      <description>arXiv:2404.03152v1 Announce Type: new 
Abstract: The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics. Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures. This renders a statistical calibration problem with multivariate outcomes. In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability. Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes. We develop a functional projection approach using the theory of Hilbert spaces. A finite-dimensional analogue of the projection problem is also considered. We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03152v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antik Chakraborty, Jonelle B. Walsh, Louis Strigari, Bani K. Mallick, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Delaunay Weighted Two-sample Test for High-dimensional Data by Incorporating Geometric Information</title>
      <link>https://arxiv.org/abs/2404.03198</link>
      <description>arXiv:2404.03198v1 Announce Type: new 
Abstract: Two-sample hypothesis testing is a fundamental problem with various applications, which faces new challenges in the high-dimensional context. To mitigate the issue of the curse of dimensionality, high-dimensional data are typically assumed to lie on a low-dimensional manifold. To incorporate geometric informtion in the data, we propose to apply the Delaunay triangulation and develop the Delaunay weight to measure the geometric proximity among data points. In contrast to existing similarity measures that only utilize pairwise distances, the Delaunay weight can take both the distance and direction information into account. A detailed computation procedure to approximate the Delaunay weight for the unknown manifold is developed. We further propose a novel nonparametric test statistic using the Delaunay weight matrix to test whether the underlying distributions of two samples are the same or not. Applied on simulated data, the new test exhibits substantial power gain in detecting differences in principal directions between distributions. The proposed test also shows great power on a real dataset of human face images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03198v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaqi Gu, Ruoxu Tan, Guosheng Yin</dc:creator>
    </item>
    <item>
      <title>Multi-task learning via robust regularized clustering with non-convex group penalties</title>
      <link>https://arxiv.org/abs/2404.03250</link>
      <description>arXiv:2404.03250v1 Announce Type: new 
Abstract: Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03250v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akira Okazaki, Shuichi Kawano</dc:creator>
    </item>
    <item>
      <title>Robust inference for linear regression models with possibly skewed error distribution</title>
      <link>https://arxiv.org/abs/2404.03404</link>
      <description>arXiv:2404.03404v1 Announce Type: new 
Abstract: Traditional methods for linear regression generally assume that the underlying error distribution, equivalently the distribution of the responses, is normal. Yet, sometimes real life response data may exhibit a skewed pattern, and assuming normality would not give reliable results in such cases. This is often observed in cases of some biomedical, behavioral, socio-economic and other variables. In this paper, we propose to use the class of skew normal (SN) distributions, which also includes the ordinary normal distribution as its special case, as the model for the errors in a linear regression setup and perform subsequent statistical inference using the popular and robust minimum density power divergence approach to get stable insights in the presence of possible data contamination (e.g., outliers). We provide the asymptotic distribution of the proposed estimator of the regression parameters and also propose robust Wald-type tests of significance for these parameters. We provide an influence function analysis of these estimators and test statistics, and also provide level and power influence functions. Numerical verification including simulation studies and real data analysis is provided to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03404v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amarnath Nandy, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Modeling temporal dependency of longitudinal data: use of multivariate geometric skew-normal copula</title>
      <link>https://arxiv.org/abs/2404.03420</link>
      <description>arXiv:2404.03420v1 Announce Type: new 
Abstract: Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times. On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research. When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements. But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data. Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption. In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data. First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data. The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini &amp; Valle (1996). Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution. We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets. Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03420v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes for the Reluctant Frequentist</title>
      <link>https://arxiv.org/abs/2404.03422</link>
      <description>arXiv:2404.03422v1 Announce Type: new 
Abstract: Empirical Bayes methods offer valuable tools for a large class of compound decision problems. In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation. Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models. A more extensive introductory treatment will eventually be available in \citet{kg24}. The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03422v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Koenker, Jiaying Gu</dc:creator>
    </item>
    <item>
      <title>ALAAMEE: Open-source software for fitting autologistic actor attribute models</title>
      <link>https://arxiv.org/abs/2404.03116</link>
      <description>arXiv:2404.03116v1 Announce Type: cross 
Abstract: The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random graph model (ERGM). ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates. This work introduces ALAAMEE, open-source Python software for estimation, simulation, and goodness-of-fit testing for ALAAM models. ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data. It implements data structures and statistics for undirected, directed, and bipartite networks. We use a simulation study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03116v1</guid>
      <category>stat.CO</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stivala, Peng Wang, Alessandro Lomi</dc:creator>
    </item>
    <item>
      <title>Semiparametric adaptive estimation under informative sampling</title>
      <link>https://arxiv.org/abs/2208.06039</link>
      <description>arXiv:2208.06039v3 Announce Type: replace 
Abstract: In survey sampling, survey data do not necessarily represent the target population, and the samples are often biased. However, information on the survey weights aids in the elimination of selection bias. The Horvitz-Thompson estimator is a well-known unbiased, consistent, and asymptotically normal estimator; however, it is not efficient. Thus, this study derives the semiparametric efficiency bound for various target parameters by considering the survey weight as a random variable and consequently proposes a semiparametric optimal estimator with certain working models on the survey weights. The proposed estimator is consistent, asymptotically normal, and efficient in a class of the regular and asymptotically linear estimators. Further, a limited simulation study is conducted to investigate the finite sample performance of the proposed method. The proposed method is applied to the 1999 Canadian Workplace and Employee Survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.06039v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Morikawa, Yoshikazu Terada, Jae Kwang Kim</dc:creator>
    </item>
    <item>
      <title>Learning from limited temporal data: Dynamically sparse historical functional linear models with applications to Earth science</title>
      <link>https://arxiv.org/abs/2303.06501</link>
      <description>arXiv:2303.06501v2 Announce Type: replace 
Abstract: Scientists and statisticians often want to learn about the complex relationships that connect two time-varying variables. Recent work on sparse functional historical linear models confirms that they are promising for this purpose, but several notable limitations exist. Most importantly, previous works have imposed sparsity on the historical coefficient function, but have not allowed the sparsity, hence lag, to vary with time. We simplify the framework of sparse functional historical linear models by using a rectangular coefficient structure along with Whittaker smoothing, then reduce the assumptions of the previous frameworks by estimating the dynamic time lag from a hierarchical coefficient structure. We motivate our study by aiming to extract the physical rainfall-runoff processes hidden within hydrological data. We show the promise and accuracy of our method using eight simulation studies, further justified by two real sets of hydrological data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06501v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Janssen, Shizhe Meng, Asad Haris, Stefan Schrunner, Jiguo Cao, William J. Welch, Nadja Kunz, Ali A. Ameli</dc:creator>
    </item>
    <item>
      <title>Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters</title>
      <link>https://arxiv.org/abs/2306.08598</link>
      <description>arXiv:2306.08598v4 Announce Type: replace 
Abstract: In the problem of estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce "plug-in bias." Traditional methods addressing this sub-optimal bias-variance trade-offs rely on the influence function (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, posing analytical and computational challenges. In this work, we leverage the targeted maximum likelihood estimation framework to propose a novel method named kernel debiased plug-in estimation (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel Hilbert spaces. We show that KDPE (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08598v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Cho, Yaroslav Mukhin, Kyra Gan, Ivana Malenica</dc:creator>
    </item>
    <item>
      <title>Improved LM Test for Robust Model Specification Searches in Covariance Structure Analysis</title>
      <link>https://arxiv.org/abs/2306.14302</link>
      <description>arXiv:2306.14302v3 Announce Type: replace 
Abstract: Model specification searches and modifications are commonly employed in covariance structure analysis (CSA) or structural equation modeling (SEM) to improve the goodness-of-fit. However, these practices can be susceptible to capitalizing on chance, as a model that fits one sample may not generalize to another sample from the same population. This paper introduces the improved Lagrange Multipliers (LM) test, which provides a reliable method for conducting a thorough model specification search and effectively identifying missing parameters. By leveraging the stepwise bootstrap method in the standard LM and Wald tests, our data-driven approach enhances the accuracy of parameter identification. The results from Monte Carlo simulations and two empirical applications in political science demonstrate the effectiveness of the improved LM test, particularly when dealing with small sample sizes and models with large degrees of freedom. This approach contributes to better statistical fit and addresses the issue of capitalization on chance in model specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14302v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng, Peter M. Bentler</dc:creator>
    </item>
    <item>
      <title>covXtreme : MATLAB software for non-stationary penalised piecewise constant marginal and conditional extreme value models</title>
      <link>https://arxiv.org/abs/2309.17295</link>
      <description>arXiv:2309.17295v2 Announce Type: replace 
Abstract: The covXtreme software provides functionality for estimation of marginal and conditional extreme value models, non-stationary with respect to covariates, and environmental design contours. Generalised Pareto (GP) marginal models of peaks over threshold are estimated, using a piecewise-constant representation for the variation of GP threshold and scale parameters on the (potentially multidimensional) covariate domain of interest. The conditional variation of one or more associated variates, given a large value of a single conditioning variate, is described using the conditional extremes model of Heffernan and Tawn (2004), the slope term of which is also assumed to vary in a piecewise constant manner with covariates. Optimal smoothness of marginal and conditional extreme value model parameters with respect to covariates is estimated using cross-validated roughness-penalised maximum likelihood estimation. Uncertainties in model parameter estimates due to marginal and conditional extreme value threshold choice, and sample size, are quantified using a bootstrap resampling scheme. Estimates of environmental contours using various schemes, including the direct sampling approach of Huseby et al. 2013, are calculated by simulation or numerical integration under fitted models. The software was developed in MATLAB for metocean applications, but is applicable generally to multivariate samples of peaks over threshold. The software and case study data can be downloaded from GitHub, with an accompanying user guide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.17295v2</guid>
      <category>stat.ME</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross Towe, Emma Ross, David Randell, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>A Pareto tail plot without moment restrictions</title>
      <link>https://arxiv.org/abs/2310.01076</link>
      <description>arXiv:2310.01076v2 Announce Type: replace 
Abstract: We propose a mean functional which exists for any probability distributions, and which characterizes the Pareto distribution within the set of distributions with finite left endpoint. This is in sharp contrast to the mean excess plot which is not meaningful for distributions without existing mean, and which has a nonstandard behaviour if the mean is finite, but the second moment does not exist. The construction of the plot is based on the so called principle of a single huge jump, which differentiates between distributions with moderately heavy and super heavy tails. We present an estimator of the tail function based on $U$-statistics and study its large sample properties. The use of the new plot is illustrated by several loss datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01076v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Klar</dc:creator>
    </item>
    <item>
      <title>Identifiability of the Multinomial Processing Tree-IRT model for the Philadelphia Naming Test</title>
      <link>https://arxiv.org/abs/2310.10915</link>
      <description>arXiv:2310.10915v3 Announce Type: replace 
Abstract: Naming tests represent an essential tool in gauging the severity of aphasia and monitoring the trajectory of recovery for individuals afflicted with this debilitating condition. In these assessments, patients are presented with images corresponding to common nouns, and their responses are evaluated for accuracy. The Philadelphia Naming Test (PNT) stands as a paragon in this domain, offering nuanced insights into the type of errors made in responses. In a groundbreaking advancement, Walker et al. (2018) introduced a model rooted in Item Response Theory and multinomial processing trees (MPT-IRT). This innovative approach seeks to unravel the intricate mechanisms underlying the various errors patients make when responding to an item, aiming to pinpoint the specific stage of word production where a patient's capability falters. However, given the sophisticated nature of the IRT-MPT model proposed by Walker et al. (2018), it is imperative to scrutinize both its conceptual as well as its statistical validity. Our endeavor here is to closely examine the model's formulation to ensure its parameters are identifiable as a first step in evaluating its validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10915v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew J. Womack, Daniel Taylor-Rodriguez, Gerasimos Fergadiotis, William D. Hula</dc:creator>
    </item>
    <item>
      <title>Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Streaming Data</title>
      <link>https://arxiv.org/abs/2310.12140</link>
      <description>arXiv:2310.12140v2 Announce Type: replace-cross 
Abstract: Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and instantly update the parameter estimate of interest. In this work we consider model selection and hyperparameter tuning for such online algorithms. We propose a weighted rolling-validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators. Similar to batch cross-validation, it can boost base estimators to achieve a better, adaptive convergence rate. Our theoretical analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in rolling validation in practice and demonstrates its sensitivity even when there is only a slim difference between candidate estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12140v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Random Forest for Dynamic Risk Prediction or Recurrent Events: A Pseudo-Observation Approach</title>
      <link>https://arxiv.org/abs/2312.00770</link>
      <description>arXiv:2312.00770v2 Announce Type: replace-cross 
Abstract: Recurrent events are common in clinical, healthcare, social and behavioral studies. A recent analysis framework for potentially censored recurrent event data is to construct a censored longitudinal data set consisting of times to the first recurrent event in multiple prespecified follow-up windows of length $\tau$. With the staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest are growing in popularity, as they can incorporate information from highly correlated predictors with non-standard relationships. In this paper, we bridge this gap by developing a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent $\tau$-duration follow-up period from a reconstructed censored longitudinal data set. We demonstrate the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a $\tau$-duration follow-up period when compared to the recurrent event modeling framework of Xia et al. (2020) in settings where association between predictors and recurrent event outcomes is complex in nature. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease (Albert et al., 2011).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00770v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abigail Loe, Susan Murray, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Causal hybrid modeling with double machine learning</title>
      <link>https://arxiv.org/abs/2402.13332</link>
      <description>arXiv:2402.13332v2 Announce Type: replace-cross 
Abstract: Hybrid modeling integrates machine learning with scientific knowledge to enhance interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13332v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai-Hendrik Cohrs, Gherardo Varando, Nuno Carvalhais, Markus Reichstein, Gustau Camps-Valls</dc:creator>
    </item>
    <item>
      <title>Asymptotic expansion of the drift estimator for the fractional Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2403.00967</link>
      <description>arXiv:2403.00967v2 Announce Type: replace-cross 
Abstract: We present an asymptotic expansion formula of an estimator for the drift coefficient of the fractional Ornstein-Uhlenbeck process. As the machinery, we apply the general expansion scheme for Wiener functionals recently developed by the authors [26]. The central limit theorem in the principal part of the expansion has the classical scaling T^{1/2}. However, the asymptotic expansion formula is a complex in that the order of the correction term becomes the classical T^{-1/2} for H in (1/2,5/8), but T^{4H-3} for H in [5/8, 3/4).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00967v2</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ciprian A. Tudor, Nakahiro Yoshida</dc:creator>
    </item>
  </channel>
</rss>
