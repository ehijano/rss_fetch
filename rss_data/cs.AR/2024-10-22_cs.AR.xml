<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Efficient IMC Accelerator Design Through Joint Hardware-Workload Co-optimization</title>
      <link>https://arxiv.org/abs/2410.16759</link>
      <description>arXiv:2410.16759v1 Announce Type: new 
Abstract: Designing generalized in-memory computing (IMC) hardware that efficiently supports a variety of workloads requires extensive design space exploration, which is infeasible to perform manually. Optimizing hardware individually for each workload or solely for the largest workload often fails to yield the most efficient generalized solutions. To address this, we propose a joint hardware-workload optimization framework that identifies optimised IMC chip architecture parameters, enabling more efficient, workload-flexible hardware. We show that joint optimization achieves 36%, 36%, 20%, and 69% better energy-latency-area scores for VGG16, ResNet18, AlexNet, and MobileNetV3, respectively, compared to the separate architecture parameters search optimizing for a single largest workload. Additionally, we quantify the performance trade-offs and losses of the resulting generalized IMC hardware compared to workload-specific IMC designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16759v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama</dc:creator>
    </item>
    <item>
      <title>A 0.03${mm}^2$ 100-250MHz Charge-Pump or Amplifier-Less Integrating Sub-Sampling PLL for Ultra-low Power Communication and Computing</title>
      <link>https://arxiv.org/abs/2410.16310</link>
      <description>arXiv:2410.16310v1 Announce Type: cross 
Abstract: Clock generation is an essential part of wireless or wireline communication modules. To facilitate recent advancements in wireline-like communication and in-sensor computing modules at relatively lower data rates, ultra-low power, and accurate clock generation are of the utmost importance. This paper presents a unique implementation of integrating sub-sampling phase locked loop, which alleviates the usage of additional gain elements in the PLL and reduces the noise injection in the system. In this design, the ring oscillator-based PLL can operate a wide frequency range of 100-250MHz while consuming 0.03mm2 of area and 131.8$\mu W$ of power at 250MHz. The area-normalized figure of merit (FOM) of the integrating SSPLL is found to be -236, while showing a reference spur of -43.2dB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16310v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yudhajit Ray, Archisman Ghosh, Shreyas Sen</dc:creator>
    </item>
    <item>
      <title>BETA: Automated Black-box Exploration for Timing Attacks in Processors</title>
      <link>https://arxiv.org/abs/2410.16648</link>
      <description>arXiv:2410.16648v1 Announce Type: cross 
Abstract: Modern processor advancements have introduced security risks, particularly in the form of microarchitectural timing attacks. High-profile attacks such as Meltdown and Spectre have revealed critical flaws, compromising the entire system's security. Recent black-box automated methods have demonstrated their advantages in identifying these vulnerabilities on various commercial processors. However, they often focus on specific attack types or incorporate numerous ineffective test cases, which severely limits the detection scope and efficiency.
  In this paper, we present BETA, a novel black-box framework that harnesses fuzzing to efficiently uncover multifaceted timing vulnerabilities in processors. Our framework employs a two-pronged approach, enhancing both mutation space and exploration efficiency: 1) we introduce an innovative fuzzer that precisely constrains mutation direction for diverse instruction combinations, including opcode, data, address, and execution level; 2) we develop a coverage feedback mechanism based on our instruction classification to discard potentially trivial or redundant test cases. This mechanism significantly expands coverage across a broader spectrum of instruction types. We evaluate the performance and effectiveness of BETA on four processors from Intel and AMD, each featuring distinct microarchitectures. BETA has successfully detected all x86 processor vulnerabilities previously identified by recent black-box methods, as well as 8 previously undiscovered timing vulnerabilities. BETA outperforms the existing state-of-the-art black-box methods, achieving at least 3x faster detection speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16648v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congcong Chen, Jinhua Cui, Jiliang Zhang</dc:creator>
    </item>
    <item>
      <title>Accelerating PoT Quantization on Edge Devices</title>
      <link>https://arxiv.org/abs/2409.20403</link>
      <description>arXiv:2409.20403v2 Announce Type: replace 
Abstract: Non-uniform quantization, such as power-of-two (PoT) quantization, matches data distributions better than uniform quantization, which reduces the quantization error of Deep Neural Networks (DNNs). PoT quantization also allows bit-shift operations to replace multiplications, but there are limited studies on the efficiency of shift-based accelerators for PoT quantization. Furthermore, existing pipelines for accelerating PoT-quantized DNNs on edge devices are not open-source. In this paper, we first design shift-based processing elements (shift-PE) for different PoT quantization methods and evaluate their efficiency using synthetic benchmarks. Then we design a shift-based accelerator using our most efficient shift-PE and propose PoTAcc, an open-source pipeline for end-to-end acceleration of PoT-quantized DNNs on resource-constrained edge devices. Using PoTAcc, we evaluate the performance of our shift-based accelerator across three DNNs. On average, it achieves a 1.23x speedup and 1.24x energy reduction compared to a multiplier-based accelerator, and a 2.46x speedup and 1.83x energy reduction compared to CPU-only execution. Our code is available at https://github.com/gicLAB/PoTAcc</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20403v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rappy Saha, Jude Haris, Jos\'e Cano</dc:creator>
    </item>
  </channel>
</rss>
