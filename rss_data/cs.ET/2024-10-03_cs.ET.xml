<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Automated Curvy Waveguide Routing for Large-Scale Photonic Integrated Circuits</title>
      <link>https://arxiv.org/abs/2410.01260</link>
      <description>arXiv:2410.01260v1 Announce Type: new 
Abstract: As photonic integrated circuit (PIC) designs advance and grow in complexity, largely driven by innovations in photonic computing and interconnects, traditional manual physical design processes have become increasingly cumbersome. Available PIC layout automation tools are mostly schematic-driven, which has not alleviated the burden of manual waveguide planning and layout drawing for engineers. Previous research in automated PIC routing largely relies on off-the-shelf algorithms designed for electrical circuits, which only support high-level route planning to minimize waveguide crossings. It is not customized to handle unique photonics-specific routing constraints and metrics, such as curvy waveguides, bending, port alignment, and insertion loss. These approaches struggle with large-scale PICs and cannot produce real layout geometries without design-rule violations (DRVs). This highlights the pressing need for electronic-photonic design automation (EPDA) tools that can streamline the physical design of modern PICs. In this paper, for the first time, we propose an open-source automated PIC detailed routing tool, dubbed APR, to generate DRV-free PIC layout for large-scale real-world PICs. APR features a grid-based curvy-aware A* engine with adaptive crossing insertion, congestion-aware net ordering and objective, and crossing-waveguide optimization scheme, all tailored to the unique property of PIC. On large-scale real-world photonic computing cores and interconnects, APR generates a DRV-free layout with 14% lower insertion loss and 6.25x speedup than prior methods, paving the way for future advancements in the EPDA toolchain. Our codes are open-sourced at https://github.com/ScopeX-ASU/APR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01260v1</guid>
      <category>cs.ET</category>
      <category>physics.optics</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Zhou, Keren Zhu, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>The Unlikely Hero: Nonideality in Analog Photonic Neural Networks as Built-in Defender Against Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2410.01289</link>
      <description>arXiv:2410.01289v1 Announce Type: new 
Abstract: Electronic-photonic computing systems have emerged as a promising platform for accelerating deep neural network (DNN) workloads. Major efforts have been focused on countering hardware non-idealities and boosting efficiency with various hardware/algorithm co-design methods. However, the adversarial robustness of such photonic analog mixed-signal AI hardware remains unexplored. Though the hardware variations can be mitigated with robustness-driven optimization methods, malicious attacks on the hardware show distinct behaviors from noises, which requires a customized protection method tailored to optical analog hardware. In this work, we rethink the role of conventionally undesired non-idealities in photonic analog accelerators and claim their surprising effects on defending against adversarial weight attacks. Inspired by the protection effects from DNN quantization and pruning, we propose a synergistic defense framework tailored for optical analog hardware that proactively protects sensitive weights via pre-attack unary weight encoding and post-attack vulnerability-aware weight locking. Efficiency-reliability trade-offs are formulated as constrained optimization problems and efficiently solved offline without model re-training costs. Extensive evaluation of various DNN benchmarks with a multi-core photonic accelerator shows that our framework maintains near-ideal on-chip inference accuracy under adversarial bit-flip attacks with merely &lt;3% memory overhead. Our codes are open-sourced at https://github.com/ScopeX-ASU/Unlikely_Hero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01289v1</guid>
      <category>cs.ET</category>
      <category>cs.CR</category>
      <category>physics.optics</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Lu, Ziang Yin, Partho Bhoumik, Sanmitra Banerjee, Krishnendu Chakrabarty, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>ADEPT-Z: Zero-Shot Automated Circuit Topology Search for Pareto-Optimal Photonic Tensor Cores</title>
      <link>https://arxiv.org/abs/2410.01313</link>
      <description>arXiv:2410.01313v1 Announce Type: new 
Abstract: Photonic tensor cores (PTCs) are essential building blocks for optical artificial intelligence (AI) accelerators based on programmable photonic integrated circuits. Most PTC designs today are manually constructed, with low design efficiency and unsatisfying solution quality. This makes it challenging to meet various hardware specifications and keep up with rapidly evolving AI applications. Prior work has explored gradient-based methods to learn a good PTC structure differentiably. However, it suffers from slow training speed and optimization difficulty when handling multiple non-differentiable objectives and constraints. Therefore, in this work, we propose a more flexible and efficient zero-shot multi-objective evolutionary topology search framework ADEPT-Z that explores Pareto-optimal PTC designs with advanced devices in a larger search space. Multiple objectives can be co-optimized while honoring complicated hardware constraints. With only &lt;3 hours of search, we can obtain tens of diverse Pareto-optimal solutions, 100x faster than the prior gradient-based method, outperforming prior manual designs with 2x higher accuracy weighted area-energy efficiency. The code of ADEPT-Z is available at https://github.com/ScopeX-ASU/ADEPT-Z.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01313v1</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>physics.optics</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Jiang, Pingchuan Ma, Meng Zhang, Rena Huang, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>Optimization of a Quantum Subset Sum Oracle</title>
      <link>https://arxiv.org/abs/2410.01775</link>
      <description>arXiv:2410.01775v1 Announce Type: new 
Abstract: We investigate the implementation of an oracle for the Subset Sum problem for quantum search using Grover's algorithm. Our work concerns reducing the number of qubits, gates, and multi-controlled gates required by the oracle. We describe the compilation of a Subset Sum instance into a quantum oracle, using a Python library we developed for Qiskit and have published in GitHub. We then present techniques to conserve qubits and gates along with experiments showing their effectiveness on random instances of Subset Sum. These techniques include moving from fixed to varying-width arithmetic, using partial sums of a set's integers to determine specific integer widths, and sorting the set to obtain provably the most efficient partial sums. We present a new method for computing bit-string comparisons that avoids arbitrarily large multiple-control gates, and we introduce a simple modification to the oracle that allows for approximate solutions to the Subset Sum problem via Grover search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01775v1</guid>
      <category>cs.ET</category>
      <category>cs.CC</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelo Benoit, Sam Schwartz, Ron K. Cytron</dc:creator>
    </item>
    <item>
      <title>DreamGarden: A Designer Assistant for Growing Games from a Single Prompt</title>
      <link>https://arxiv.org/abs/2410.01791</link>
      <description>arXiv:2410.01791v1 Announce Type: cross 
Abstract: Coding assistants are increasingly leveraged in game design, both generating code and making high-level plans. To what degree can these tools align with developer workflows, and what new modes of human-computer interaction can emerge from their use? We present DreamGarden, an AI system capable of assisting with the development of diverse game environments in Unreal Engine. At the core of our method is an LLM-driven planner, capable of breaking down a single, high-level prompt -- a dream, memory, or imagined scenario provided by a human user -- into a hierarchical action plan, which is then distributed across specialized submodules facilitating concrete implementation. This system is presented to the user as a garden of plans and actions, both growing independently and responding to user intervention via seed prompts, pruning, and feedback. Through a user study, we explore design implications of this system, charting courses for future work in semi-autonomous assistants and open-ended simulation design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01791v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey</dc:creator>
    </item>
    <item>
      <title>Thermodynamic Bayesian Inference</title>
      <link>https://arxiv.org/abs/2410.01793</link>
      <description>arXiv:2410.01793v1 Announce Type: cross 
Abstract: A fully Bayesian treatment of complicated predictive models (such as deep neural networks) would enable rigorous uncertainty quantification and the automation of higher-level tasks including model selection. However, the intractability of sampling Bayesian posteriors over many parameters inhibits the use of Bayesian methods where they are most needed. Thermodynamic computing has emerged as a paradigm for accelerating operations used in machine learning, such as matrix inversion, and is based on the mapping of Langevin equations to the dynamics of noisy physical systems. Hence, it is natural to consider the implementation of Langevin sampling algorithms on thermodynamic devices. In this work we propose electronic analog devices that sample from Bayesian posteriors by realizing Langevin dynamics physically. Circuit designs are given for sampling the posterior of a Gaussian-Gaussian model and for Bayesian logistic regression, and are validated by simulations. It is shown, under reasonable assumptions, that the Bayesian posteriors for these models can be sampled in time scaling with $\ln(d)$, where $d$ is dimension. For the Gaussian-Gaussian model, the energy cost is shown to scale with $ d \ln(d)$. These results highlight the potential for fast, energy-efficient Bayesian inference using thermodynamic computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01793v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxwell Aifer, Samuel Duffield, Kaelan Donatella, Denis Melanson, Phoebe Klett, Zach Belateche, Gavin Crooks, Antonio J. Martinez, Patrick J. Coles</dc:creator>
    </item>
    <item>
      <title>Annealing-inspired training of an optical neural network with ternary weights</title>
      <link>https://arxiv.org/abs/2409.01042</link>
      <description>arXiv:2409.01042v2 Announce Type: replace 
Abstract: Artificial neural networks (ANNs) represent a fundamentally connectionnist and distributed approach to computing, and as such they differ from classical computers that utilize the von Neumann architecture. This has revived research interest in new unconventional hardware to enable more efficient implementations of ANNs rather than emulating them on traditional machines. In order to fully leverage the capabilities of this new generation of ANNs, optimization algorithms that take into account hardware limitations and imperfections are necessary. Photonics represents a particularly promising platform, offering scalability, high speed, energy efficiency, and the capability for parallel information processing. Yet, fully fledged implementations of autonomous optical neural networks (ONNs) with in-situ learning remain scarce. In this work, we propose a ternary weight architecture high-dimensional semiconductor laser-based ONN. We introduce a simple method for achieving ternary weights with Boolean hardware, significantly increasing the ONN's information processing capabilities. Furthermore, we design a novel in-situ optimization algorithm that is compatible with, both, Boolean and ternary weights, and provide a detailed hyperparameter study of said algorithm for two different tasks. Our novel algorithm results in benefits, both in terms of convergence speed and performance. Finally, we experimentally characterize the long-term inference stability of our ONN and find that it is extremely stable with a consistency above 99\% over a period of more than 10 hours, addressing one of the main concerns in the field. Our work is of particular relevance in the context of in-situ learning under restricted hardware resources, especially since minimizing the power consumption of auxiliary hardware is crucial to preserving efficiency gains achieved by non-von Neumann ANN implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01042v2</guid>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anas Skalli, Mirko Goldmann, Nasibeh Haghighi, Stephan Reitzenstein, James A. Lott, Daniel Brunner</dc:creator>
    </item>
    <item>
      <title>Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition</title>
      <link>https://arxiv.org/abs/2403.05754</link>
      <description>arXiv:2403.05754v2 Announce Type: replace-cross 
Abstract: With the rapid social progress, novel neural networks with high generalization power and robustness are strongly required to substitute traditional ones nowadays. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired algorithms with outstanding potentials. Hence, influenced by quantum-inspired and pure classical neural networks, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern recognition. We explain the concrete frameworks and illustrate the potential superiority to prevent gradient explosion of our hybrid models. Numerical experiments indicate that the recognition accuracy of our hybrid models is 2\%-3\% higher than that of the traditional quantum-inspired model. Our hybrid models also generalize as well as the pure classical models with different noisy datasets utilized. More importantly, our hybrid models outperform pure classical models notably in robustness, especially in resistance to parameter attacks with various asymmetric noises. Eventually, we discuss the application scenarios of our hybrid models by analyzing their computational complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05754v2</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andi Chen, Hua-Lei Yin, Zeng-Bing Chen, Shengjun Wu</dc:creator>
    </item>
    <item>
      <title>Do Not Trust Power Management: A Survey on Internal Energy-based Attacks Circumventing Trusted Execution Environments Security Properties</title>
      <link>https://arxiv.org/abs/2405.15537</link>
      <description>arXiv:2405.15537v2 Announce Type: replace-cross 
Abstract: Over the past few years, several research groups have introduced innovative hardware designs for Trusted Execution Environments (TEEs), aiming to secure applications against potentially compromised privileged software, including the kernel. Since 2015, a new class of software-enabled hardware attacks leveraging energy management mechanisms has emerged. These internal energy-based attacks comprise fault, side-channel and covert channel attacks. Their aim is to bypass TEE security guarantees and expose sensitive information such as cryptographic keys. They have increased in prevalence in the past few years. Popular TEE implementations, such as ARM TrustZone and Intel SGX, incorporate countermeasures against these attacks. However, these countermeasures either hinder the capabilities of the power management mechanisms or have been shown to provide insufficient system protection. This article presents the first comprehensive knowledge survey of these attacks, along with an evaluation of literature countermeasures. We believe that this study will spur further community efforts towards this increasingly important type of attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15537v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Owen Le Gonidec, Maria M\'endez Real, Guillaume Bouffard, Jean-Christophe Pr\'evotet</dc:creator>
    </item>
    <item>
      <title>AgriLLM: Harnessing Transformers for Farmer Queries</title>
      <link>https://arxiv.org/abs/2407.04721</link>
      <description>arXiv:2407.04721v2 Announce Type: replace-cross 
Abstract: Agriculture, vital for global sustenance, necessitates innovative solutions due to a lack of organized domain experts, particularly in developing countries where many farmers are impoverished and cannot afford expert consulting. Initiatives like Farmers Helpline play a crucial role in such countries, yet challenges such as high operational costs persist. Automating query resolution can alleviate the burden on traditional call centers, providing farmers with immediate and contextually relevant information. The integration of Agriculture and Artificial Intelligence (AI) offers a transformative opportunity to empower farmers and bridge information gaps. Language models like transformers, the rising stars of AI, possess remarkable language understanding capabilities, making them ideal for addressing information gaps in agriculture. This work explores and demonstrates the transformative potential of Large Language Models (LLMs) in automating query resolution for agricultural farmers, leveraging their expertise in deciphering natural language and understanding context. Using a subset of a vast dataset of real-world farmer queries collected in India, our study focuses on approximately 4 million queries from the state of Tamil Nadu, spanning various sectors, seasonal crops, and query types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04721v2</guid>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krish Didwania, Pratinav Seth, Aditya Kasliwal, Amit Agarwal</dc:creator>
    </item>
    <item>
      <title>Social Conjuring: Multi-User Runtime Collaboration with AI in Building Virtual 3D Worlds</title>
      <link>https://arxiv.org/abs/2410.00274</link>
      <description>arXiv:2410.00274v2 Announce Type: replace-cross 
Abstract: Generative artificial intelligence has shown promise in prompting virtual worlds into existence, yet little attention has been given to understanding how this process unfolds as social interaction. We present Social Conjurer, a framework for AI-augmented dynamic 3D scene co-creation, where multiple users collaboratively build and modify virtual worlds in real-time. Through an expanded set of interactions, including social and tool-based engagements as well as spatial reasoning, our framework facilitates the creation of rich, diverse virtual environments. Findings from a preliminary user study (N=12) provide insight into the user experience of this approach, how social contexts shape the prompting of spatial environments, and perspective on social applications of prompt-based 3D co-creation. In addition to highlighting the potential of AI-supported multi-user world creation and offering new pathways for AI-augmented creative processes in VR, this article presents a set of implications for designing human-centered interfaces that incorporate AI models into 3D content generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00274v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amina Kobenova, Cyan DeVeaux, Samyak Parajuli, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier</dc:creator>
    </item>
  </channel>
</rss>
