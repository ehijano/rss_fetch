<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Mar 2024 04:01:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Surgery duration prediction using multi-task feature selection</title>
      <link>https://arxiv.org/abs/2403.09791</link>
      <description>arXiv:2403.09791v1 Announce Type: new 
Abstract: Efficient optimization of operating room (OR) activity poses a significant challenge for hospital managers due to the complex and risky nature of the environment. The traditional "one size fits all" approach to OR scheduling is no longer practical, and personalized medicine is required to meet the diverse needs of patients, care providers, medical procedures, and system constraints within limited resources. This paper aims to introduce a scientific and practical tool for predicting surgery durations and improving OR performance for maximum benefit to patients and the hospital. Previous works used machine-learning models for surgery duration prediction based on preoperative data. The models consider covariates known to the medical staff at the time of scheduling the surgery. Given a large number of covariates, model selection becomes crucial, and the number of covariates used for prediction depends on the available sample size. Our proposed approach utilizes multi-task regression to select a common subset of predicting covariates for all tasks with the same sample size while allowing the model's coefficients to vary between them. A regression task can refer to a single surgeon or operation type or the interaction between them. By considering these diverse factors, our method provides an overall more accurate estimation of the surgery durations, and the selected covariates that enter the model may help to identify the resources required for a specific surgery. We found that when the regression tasks were surgeon-based or based on the pair of operation type and surgeon, our suggested approach outperformed the compared baseline suggested in a previous study. However, our approach failed to reach the baseline for an operation-type-based task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09791v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Yosef Rinott, Orna Tal, Benyamine Abbou, Nadav Rappoport</dc:creator>
    </item>
    <item>
      <title>The reliability of the gender Implicit Association Test (gIAT) for high-ability careers</title>
      <link>https://arxiv.org/abs/2403.10300</link>
      <description>arXiv:2403.10300v1 Announce Type: new 
Abstract: Males outnumber females in many high ability careers, for example, in the fields of science, technology, engineering, and mathematics and professors in academic medicine. These differences are often attributed to implicit, subconscious, bias. One objective of this study was to use statistical p value plots to independently test the ability to support the claim of implicit bias made in a meta analysis of gender bias studies.
  The meta analysis examined correlations between implicit bias measures based on the gender Implicit Association Test, g IAT, and measures of intergroup, female and male, behavior. A second objective was to investigate general intelligence g and vocational, things people, interests as explanatory factors for gender differences in high ability careers.
  The p value plots constructed using data sets from the meta analysis did not support real associations between the tested variables. These findings reinforce the lack of correlation between g IAT, implicit bias, measures and real world gender behaviors in high ability careers.
  More demanding careers, attorneys, engineers, scientists, corporate executives, are recruited from people with higher g. One is dealing with gender groups and the group of high g females is smaller than high g males. Regarding vocational interests, females prefer working with people and males prefer working with things. STEM fields are typically things oriented. One is dealing with gender groups and the group of females who prefer working with things is smaller than the group of males.
  These facts make it predictable that there are more males in high complexity, things careers, STEM, academic medicine positions, than females. Implicit bias gIAT measures have little or no explanatory power for gender differences in high ability careers relative to g and interests in working with things.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10300v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Stanley Young, Warren B. Kindzierski</dc:creator>
    </item>
    <item>
      <title>Probabilistic Models of Profiles for Voting by Evaluation</title>
      <link>https://arxiv.org/abs/2403.10302</link>
      <description>arXiv:2403.10302v1 Announce Type: new 
Abstract: Considering voting rules based on evaluation inputs rather than preference rankings modifies the paradigm of probabilistic studies of voting procedures. This article proposes several simulation models for generating evaluation-based voting inputs. These models can cope with dependent and non identical marginal distributions of the evaluations received by the candidates. A last part is devoted to fitting these models to real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10302v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Rolland, Jean-Baptiste Aubin, Ir\`ene Gannaz, Samuela Leoni</dc:creator>
    </item>
    <item>
      <title>Rapid and Robust construction of an ML-ready peak feature table from X-ray diffraction data using Bayesian peak-top fitting</title>
      <link>https://arxiv.org/abs/2403.09677</link>
      <description>arXiv:2403.09677v1 Announce Type: cross 
Abstract: To advance the development of materials through data-driven scientific methods, appropriate methods for building machine learning (ML)-ready feature tables from measured and computed data must be established. In materials development, X-ray diffraction (XRD) is an effective technique for analysing crystal structures and other microstructural features that have information that can explain material properties. Therefore, the fully automated extraction of peak features from XRD data without the bias of an analyst is a significant challenge. This study aimed to establish an efficient and robust approach for constructing peak feature tables that follow ML standards (ML-ready) from XRD data. We challenge peak feature extraction in the situation where only the peak function profile is known a priori, without knowledge of the measurement material or crystal structure factor. We utilized Bayesian estimation to extract peak features from XRD data and subsequently performed Bayesian regression analysis with feature selection to predict the material property. The proposed method focused only on the tops of peaks within localized regions of interest (ROIs) and extracted peak features quickly and accurately. This process facilitated the rapid extracting of major peak features from the XRD data and the construction of an ML-ready feature table. We then applied Bayesian linear regression to the maximum energy product $(BH)_{max}$, using the extracted peak features as the explanatory variable. The outcomes yielded reasonable and robust regression results. Thus, the findings of this study indicated that \textit{004} peak height and area were important features for predicting $(BH)_{max}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09677v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryo Murakami, Taisuke T. Sasaki, Hideki Yoshikawa, Yoshitaka Matsushita, Keitaro Sodeyama, Tadakatsu Ohkubo, Hiroshi Shinotsuka, Kenji Nagata</dc:creator>
    </item>
    <item>
      <title>Inference for non-probability samples using the calibration approach for quantiles</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v1 Announce Type: cross 
Abstract: Non-probability survey samples are examples of data sources that have become increasingly popular in recent years, also in official statistics. However, statistical inference based on non-probability samples is much more difficult because they are biased and are not representative of the target population (Wu, 2022). In this paper we consider a method of joint calibration for totals (Deville &amp; S\"arndal, 1992) and quantiles (Harms &amp; Duchesne, 2006) and use the proposed approach to extend existing inference methods for non-probability samples, such as inverse probability weighting, mass imputation and doubly robust estimators. By including quantile information in the estimation process non-linear relationships between the target and auxiliary variables can be approximated the way it is done in step-wise (constant) regression. Our simulation study has demonstrated that the estimators in question are more robust against model mis-specification and, as a result, help to reduce bias and improve estimation efficiency. Variance estimation for our proposed approach is also discussed. We show that existing inference methods can be used and that the resulting confidence intervals are at nominal levels. Finally, we applied the proposed methods to estimate the share of vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies. The proposed approaches have been implemented in two R packages (nonprobsvy and jointCalib), which were used to conduct the simulation and empirical study</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak</dc:creator>
    </item>
    <item>
      <title>Inference for Heterogeneous Graphical Models using Doubly High-Dimensional Linear-Mixed Models</title>
      <link>https://arxiv.org/abs/2403.10034</link>
      <description>arXiv:2403.10034v1 Announce Type: cross 
Abstract: Motivated by the problem of inferring the graph structure of functional connectivity networks from multi-level functional magnetic resonance imaging data, we develop a valid inference framework for high-dimensional graphical models that accounts for group-level heterogeneity. We introduce a neighborhood-based method to learn the graph structure and reframe the problem as that of inferring fixed effect parameters in a doubly high-dimensional linear mixed model. Specifically, we propose a LASSO-based estimator and a de-biased LASSO-based inference framework for the fixed effect parameters in the doubly high-dimensional linear mixed model, leveraging random matrix theory to deal with challenges induced by the identical fixed and random effect design matrices arising in our setting. Moreover, we introduce consistent estimators for the variance components to identify subject-specific edges in the inferred graph. To illustrate the generality of the proposed approach, we also adapt our method to account for serial correlation by learning heterogeneous graphs in the setting of a vector autoregressive model. We demonstrate the performance of the proposed framework using real data and benchmark simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10034v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Yue, Eardi Lila, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Finite mixture copulas for modeling dependence in longitudinal count data</title>
      <link>https://arxiv.org/abs/2403.10165</link>
      <description>arXiv:2403.10165v1 Announce Type: cross 
Abstract: Dependence modeling of multivariate count data has been receiving a considerable attention in recent times. Multivariate elliptical copulas are typically preferred in statistical literature to analyze dependence between repeated measurements of longitudinal data since they allow for different choices of the correlation structure. But these copulas lack in flexibility to model dependence and inference is only feasible under parametric restrictions. In this article, we propose the use of finite mixture of elliptical copulas in order to capture complex and hidden temporal dependency of discrete longitudinal data. With guaranteed model identifiability, our approach permits to use different correlation matrices in each component of the mixture copula. We theoretically examine the dependence properties of finite mixture of copulas, before applying them for constructing regression models for count longitudinal data. The inference of the proposed class of models is based on composite likelihood approach and the finite sample performance of the parameter estimates are investigated through extensive simulation studies. For model validation, besides the standard techniques we extended the t-plot method to accommodate finite mixture of elliptical copulas. Finally, our models are applied to analyze the temporal dependency of two real world longitudinal data sets and shown to provide improvements if compared against standard elliptical copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10165v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Multivariate Bayesian models with flexible shared interactions for analyzing spatio-temporal patterns of rare cancers</title>
      <link>https://arxiv.org/abs/2403.10440</link>
      <description>arXiv:2403.10440v1 Announce Type: cross 
Abstract: Rare cancers affect millions of people worldwide each year. However, estimating incidence or mortality rates associated with rare cancers presents important difficulties and poses new statistical methodological challenges. In this paper, we expand the collection of multivariate spatio-temporal models by introducing adaptable shared interactions to enable a comprehensive analysis of both incidence and cancer mortality in rare cancer cases. These models allow the modulation of spatio-temporal interactions between incidence and mortality, allowing for changes in their relationship over time. The new models have been implemented in INLA using r-generic constructions. We conduct a simulation study to evaluate the performance of the new spatio-temporal models in terms of sensitivity and specificity. Results show that multivariate spatio-temporal models with flexible shared interaction outperform conventional multivariate spatio-temporal models with independent interactions. We use these models to analyze incidence and mortality data for pancreatic cancer and leukaemia among males across 142 administrative healthcare districts of Great Britain over a span of nine biennial periods (2002-2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10440v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Multilevel functional distributional models with application to continuous glucose monitoring in diabetes clinical trials</title>
      <link>https://arxiv.org/abs/2403.10514</link>
      <description>arXiv:2403.10514v1 Announce Type: cross 
Abstract: Continuous glucose monitoring (CGM) is a minimally invasive technology that allows continuous monitoring of an individual's blood glucose. We focus on a large clinical trial that collected CGM data every few minutes for 26 weeks and assumes that the basic observation unit is the distribution of CGM observations in a four-week interval. The resulting data structure is multilevel (because each individual has multiple months of data) and distributional (because the data for each four-week interval is represented as a distribution). The scientific goals are to: (1) identify and quantify the effects of factors that affect glycemic control in type 1 diabetes (T1D) patients; and (2) identify and characterize the patients who respond to treatment. To address these goals, we propose a new multilevel functional model that treats the CGM distributions as a response. Methods are motivated by and applied to data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group. Reproducible code for the methods introduced here is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10514v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal methods for estimating subsurface ocean thermal response to tropical cyclones</title>
      <link>https://arxiv.org/abs/2012.15130</link>
      <description>arXiv:2012.15130v5 Announce Type: replace 
Abstract: Tropical cyclones (TCs), driven by heat exchange between the air and sea, pose a substantial risk to many communities around the world. Accurate characterization of the subsurface ocean thermal response to TC passage is crucial for accurate TC intensity forecasts and for an understanding of the role that TCs play in the global climate system. However, that characterization is complicated by the high-noise ocean environment, correlations inherent in spatio-temporal data, relative scarcity of in situ observations, and the entanglement of the TC-induced signal with seasonal signals. We present a general methodological framework that addresses these difficulties, integrating existing techniques in seasonal mean field estimation, Gaussian process modeling, and nonparametric regression into an ANOVA decomposition model. Importantly, we improve upon past work by properly handling seasonality, providing rigorous uncertainty quantification, and treating time as a continuous variable, rather than producing estimates that are binned in time. This ANOVA model is estimated using in situ subsurface temperature profiles from the Argo fleet of autonomous floats through a multi-step procedure, which (1) characterizes the upper ocean seasonal shift during the TC season; (2) models the variability in the temperature observations; (3) fits a thin plate spline using the variability estimates to account for heteroskedasticity and correlation between the observations. This spline fit reveals the ocean thermal response to TC passage. Through this framework, we obtain new scientific insights into the interaction between TCs and the ocean on a global scale, including a three-dimensional characterization of the near-surface and subsurface cooling along the TC storm track and the mixing-induced subsurface warming on the track's right side.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.15130v5</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Addison J. Hu, Mikael Kuusela, Ann B. Lee, Donata Giglio, Kimberly M. Wood</dc:creator>
    </item>
    <item>
      <title>Beyond expected values: Making environmental decisions using value of information analysis when measurement outcome matters</title>
      <link>https://arxiv.org/abs/2309.09452</link>
      <description>arXiv:2309.09452v2 Announce Type: replace 
Abstract: In ecological and environmental contexts, management actions must sometimes be chosen urgently. Value of information (VoI) analysis provides a quantitative toolkit for projecting the improved management outcomes expected after making additional measurements. However, traditional VoI analysis reports metrics as expected values (i.e. risk-neutral). This can be problematic because expected values hide uncertainties in projections. The true value of a measurement will only be known after the measurement's outcome is known, leaving large uncertainty in the measurement's value before it is performed. As a result, the expected value metrics produced in traditional VoI analysis may not align with the priorities of a risk-averse decision-maker who wants to avoid low-value measurement outcomes. In the present work, we introduce four new VoI metrics that can address a decision-maker's risk-aversion to different measurement outcomes. We demonstrate the benefits of the new metrics with two ecological case studies for which traditional VoI analysis has been previously applied. Using the new metrics, we also demonstrate a clear mathematical link between the often-separated environmental decision-making disciplines of VoI and optimal design of experiments. This mathematical link has the potential to catalyse future collaborations between ecologists and statisticians to work together to quantitatively address environmental decision-making questions of fundamental importance. Overall, the introduced VoI metrics complement existing metrics to provide decision-makers with a comprehensive view of the value of, and risks associated with, a proposed monitoring or measurement activity. This is critical for improved environmental outcomes when decisions must be urgently made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09452v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ecolind.2024.111828</arxiv:DOI>
      <arxiv:journal_reference>Ecological Indicators 160 (2024) 111828</arxiv:journal_reference>
      <dc:creator>Morenikeji D. Akinlotan, David J. Warne, Kate J. Helmstedt, Sarah A. Vollert, Iadine Chad\`es, Ryan F. Heneghan, Hui Xiao, Matthew P. Adams</dc:creator>
    </item>
    <item>
      <title>High-Throughput Asset Pricing</title>
      <link>https://arxiv.org/abs/2311.10685</link>
      <description>arXiv:2311.10685v2 Announce Type: replace-cross 
Abstract: We use empirical Bayes (EB) to mine data on 140,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This "high-throughput asset pricing" produces out-of-sample performance comparable to strategies in top finance journals. But unlike the published strategies, the data-mined strategies are free of look-ahead bias. EB predicts that high returns are concentrated in accounting strategies, small stocks, and pre-2004 samples, consistent with limited attention theories. The intuition is seen in the cross-sectional distribution of t-stats, which is far from the null for equal-weighted accounting strategies. High-throughput methods provide a rigorous, unbiased method for documenting asset pricing facts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10685v2</guid>
      <category>q-fin.GN</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen, Chukwuma Dim</dc:creator>
    </item>
    <item>
      <title>Exact Consistency Tests for Gaussian Mixture Filters using Normalized Deviation Squared Statistics</title>
      <link>https://arxiv.org/abs/2312.17420</link>
      <description>arXiv:2312.17420v2 Announce Type: replace-cross 
Abstract: We consider the problem of evaluating dynamic consistency in discrete time probabilistic filters that approximate stochastic system state densities with Gaussian mixtures. Dynamic consistency means that the estimated probability distributions correctly describe the actual uncertainties. As such, the problem of consistency testing naturally arises in applications with regards to estimator tuning and validation. However, due to the general complexity of the density functions involved, straightforward approaches for consistency testing of mixture-based estimators have remained challenging to define and implement. This paper derives a new exact result for Gaussian mixture consistency testing within the framework of normalized deviation squared (NDS) statistics. It is shown that NDS test statistics for generic multivariate Gaussian mixture models exactly follow mixtures of generalized chi-square distributions, for which efficient computational tools are available. The accuracy and utility of the resulting consistency tests are numerically demonstrated on static and dynamic mixture estimation examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17420v2</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nisar Ahmed, Luke Burks, Kailah Cabral, Alyssa Bekai Rose</dc:creator>
    </item>
  </channel>
</rss>
