<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 03:30:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Discrete dualities for some algebras from rough sets</title>
      <link>https://arxiv.org/abs/2601.05843</link>
      <description>arXiv:2601.05843v1 Announce Type: new 
Abstract: A discrete duality is a relationship between classes of algebras and classes of relational systems (frames) resulting in two representation theorems building on the early work of J\'onsson and Tarski, Kripke, and van Benthem. In this section we recall discrete dualities for various types of algebras arising from rough sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05843v1</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivo D\"untsch, Ewa Or{\l}owska</dc:creator>
    </item>
    <item>
      <title>The Modal Logic of Abstraction Refinement</title>
      <link>https://arxiv.org/abs/2601.05897</link>
      <description>arXiv:2601.05897v1 Announce Type: new 
Abstract: Iterative abstraction refinement techniques are one of the most prominent paradigms for the analysis and verification of systems with large or infinite state spaces. This paper investigates the changes of truth values of system properties expressible in computation tree logic (CTL) when abstractions of transition systems are refined. To this end, the paper utilizes modal logic by defining alethic modalities expressing possibility and necessity on top of CTL: The modal operator $\lozenge$ is interpreted as "there is a refinement, in which ..." and $\Box$ is interpreted as "in all refinements, ...".
  Upper and lower bounds for the resulting modal logics of abstraction refinement are provided for three scenarios: 1) when considering all finite abstractions of a transition system, 2) when considering all abstractions of a transition system, and 3) when considering the class of all transition systems. Furthermore, to prove these results, generic techniques to obtain upper bounds of modal logics using novel types of so-called control statements are developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05897v1</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Piribauer, Vinzent Zschuppe</dc:creator>
    </item>
    <item>
      <title>Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning</title>
      <link>https://arxiv.org/abs/2601.05705</link>
      <description>arXiv:2601.05705v1 Announce Type: cross 
Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05705v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Farjami, Luca Redondi, Marco Valentino</dc:creator>
    </item>
    <item>
      <title>Interpolation with Automated First-Order Reasoning</title>
      <link>https://arxiv.org/abs/2507.01577</link>
      <description>arXiv:2507.01577v2 Announce Type: replace 
Abstract: We consider interpolation from the viewpoint of fully automated theorem proving in first-order logic as a general core technique for mechanized knowledge processing. For Craig interpolation, our focus is on the two-stage approach, where first an essentially propositional ground interpolant is calculated that is then lifted to a quantified first-order formula. We discuss two possibilities to obtain a ground interpolant from a proof: with clausal tableaux, and with resolution. Established preprocessing techniques for first-order proving can also be applied for Craig interpolation if they are restricted in specific ways. Equality encodings from automated reasoning justify strengthened variations of Craig interpolation. Contributions to Craig interpolation that emerged from automated reasoning include variations for logics used in databases and logic programming. As an approach to uniform interpolation we introduce second-order quantifier elimination with examples and describe the basic algorithms DLS and SCAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01577v2</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Wernhard</dc:creator>
    </item>
    <item>
      <title>A Lazy, Concurrent Convertibility Checker</title>
      <link>https://arxiv.org/abs/2510.18418</link>
      <description>arXiv:2510.18418v2 Announce Type: replace 
Abstract: Convertibility checking - determining whether two lambda-terms are equal up to reductions - is a crucial component of proof assistants and dependently-typed languages. Practical implementations often use heuristics to quickly conclude that two terms are or are not convertible without reducing them to normal form. However, these heuristics can backfire, triggering huge amounts of unnecessary computation. This paper presents a novel convertibility-checking algorithm that relies crucially on laziness and concurrency} Laziness is used to share computations, while concurrency is used to explore multiple convertibility subproblems in parallel or via fair interleaving. Unlike heuristics-based approaches, our algorithm always finds an easy solution to the convertibility problem, if one exists. The paper presents the algorithm in process calculus style and discusses its mechanized proof of partial correctness, its complexity, and its lightweight experimental evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18418v2</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the ACM on Programming Languages, 2026, 10 (POPL), pp.53:1-53:27</arxiv:journal_reference>
      <dc:creator>Nathana\"elle Courant (CAMBIUM), Xavier Leroy (CAMBIUM)</dc:creator>
    </item>
    <item>
      <title>TAPAAL HyperLTL: A Tool for Checking Hyperproperties of Petri Nets</title>
      <link>https://arxiv.org/abs/2512.14265</link>
      <description>arXiv:2512.14265v2 Announce Type: replace 
Abstract: Petri nets are a modeling formalism capable of describing complex distributed systems and there exists a large number of both academic and industrial tools that enable automatic verification of model properties. Typical questions include reachability analysis and model checking against logics like LTL and CTL. However, these logics fall short when describing properties like non-interference and observational determinism that require simultaneous reasoning about multiple traces of the model and can thus only be expressed as hyperproperties. We introduce, to the best of our knowledge, the first HyperLTL model checker for Petri nets. The tool is fully integrated into the verification framework TAPAAL and we describe the semantics of the hyperlogic, present the tool's architecture and GUI, and evaluate the performance of the HyperLTL verification engine on two benchmarks of problems originating from the computer networking domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14265v2</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Maria Ren\'e Gonzalez, Peter Gj{\o}l Jensen, Stefan Schmid, Ji\v{r}\'i Srba, Martin Zimmermann</dc:creator>
    </item>
    <item>
      <title>Graded String Diagrams for Imprecise Probability and Causal Intervention</title>
      <link>https://arxiv.org/abs/2501.18404</link>
      <description>arXiv:2501.18404v4 Announce Type: replace-cross 
Abstract: We introduce string diagrams for graded symmetric monoidal categories. Our approach includes a definition of graded monoidal theory and the corresponding freely generated syntactic category. Also, we show how an axiomatic presentation for the graded theory may be modularly obtained from one for the grading theory and one for the base category. The Para construction on monoidal actegories is a motivating example for our framework. As a case study, we show how to axiomatise a variant of the graded category ImP, recently introduced by Liell-Cock and Staton to model imprecise probability. This culminates in a representation, as string diagrams with grading wires, of programs with primitives for nondeterministic and probabilistic choices and conditioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18404v4</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ralph Sarkis, Fabio Zanasi</dc:creator>
    </item>
    <item>
      <title>FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs</title>
      <link>https://arxiv.org/abs/2601.03031</link>
      <description>arXiv:2601.03031v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\log^2 N$, FlexProofs can be $6\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03031v2</guid>
      <category>cs.CR</category>
      <category>cs.LO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Liu, Liang Feng Zhang</dc:creator>
    </item>
  </channel>
</rss>
