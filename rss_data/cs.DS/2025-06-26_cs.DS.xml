<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>All-Pairs Shortest Paths with Few Weights per Node</title>
      <link>https://arxiv.org/abs/2506.20017</link>
      <description>arXiv:2506.20017v1 Announce Type: new 
Abstract: We study the central All-Pairs Shortest Paths (APSP) problem under the restriction that there are at most $d$ distinct weights on the outgoing edges from every node. For $d=n$ this is the classical (unrestricted) APSP problem that is hypothesized to require cubic time $n^{3-o(1)}$, and at the other extreme, for $d=1$, it is equivalent to the Node-Weighted APSP problem. We present new algorithms that achieve the following results:
  1. Node-Weighted APSP can be solved in time $\tilde{O}(n^{(3+\omega)/2}) = \tilde{O}(n^{2.686})$, improving on the 15-year-old subcubic bounds $\tilde{O}(n^{(9+\omega)/4}) = \tilde{O}(n^{2.843})$ [Chan; STOC '07] and $\tilde{O}(n^{2.830})$ [Yuster; SODA '09]. This positively resolves the question of whether Node-Weighted APSP is an ``intermediate'' problem in the sense of having complexity $n^{2.5+o(1)}$ if $\omega=2$, in which case it also matches an $n^{2.5-o(1)}$ conditional lower bound.
  2. For up to $d \leq n^{3-\omega-\epsilon}$ distinct weights per node (where $\epsilon &gt; 0$), the problem can be solved in subcubic time $O(n^{3-f(\epsilon)})$ (where $f(\epsilon) &gt; 0$). In particular, assuming that $\omega = 2$, we can tolerate any sublinear number of distinct weights per node $d \leq n^{1-\epsilon}$, whereas previous work [Yuster; SODA '09] could only handle $d \leq n^{1/2-\epsilon}$ in subcubic time. This promotes our understanding of the APSP hypothesis showing that the hardest instances must exhaust a linear number of weights per node. Our result also applies to the All-Pairs Exact Triangle problem, thus generalizing a result of Chan and Lewenstein on "Clustered 3SUM" from arrays to matrices. Notably, our technique constitutes a rare application of additive combinatorics in graph algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20017v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Abboud, Nick Fischer, Ce Jin, Virginia Vassilevska Williams, Zoe Xi</dc:creator>
    </item>
    <item>
      <title>LZSE: an LZ-style compressor supporting $O(\log n)$-time random access</title>
      <link>https://arxiv.org/abs/2506.20107</link>
      <description>arXiv:2506.20107v1 Announce Type: new 
Abstract: An LZ-like factorization of a string is a factorization in which each factor is either a single character or a copy of a substring that occurs earlier in the string. While grammar-based compression schemes support efficient random access with linear space in the size of the compressed representation, such methods are not known for general LZ-like factorizations. This has led to the development of restricted LZ-like schemes such as LZ-End [Kreft and Navarro, 2013] and height-bounded (LZHB) [Bannai et al., 2024], which trade off some compression efficiency for faster access. We introduce LZ-Start-End (LZSE), a new variant of LZ-like factorizations in which each copy factor refers to a contiguous sequence of preceding factors. By its nature, any context-free grammar can easily be converted into an LZSE factorization of equal size. Further, we study the greedy LZSE factorization, in which each copy factor is taken as long as possible. We show how the greedy LZSE factorization can be computed in linear time with respect to the input string length, and that there exists a family of strings for which the size of the greedy LZSE factorization is of strictly lower order than that of the smallest grammar. These imply that our LZSE scheme is stronger than grammar-based compressions in the context of repetitiveness measures. To support fast queries, we propose a data structure for LZSE-compressed strings that permits $O(\log n)$-time random access within space linear in the compressed size, where $n$ is the length of the input string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20107v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Shibata, Yuto Nakashima, Yutaro Yamaguchi, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data</title>
      <link>https://arxiv.org/abs/2506.20141</link>
      <description>arXiv:2506.20141v1 Announce Type: new 
Abstract: The explosive growth of AI research has driven paper submissions at flagship AI conferences to unprecedented levels, necessitating many venues in 2025 (e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author submission limits and to desk-reject any excess papers by simple ID order. While this policy helps reduce reviewer workload, it may unintentionally discard valuable papers and penalize authors' efforts. In this paper, we ask an essential research question on whether it is possible to follow submission limits while minimizing needless rejections. We first formalize the current desk-rejection policies as an optimization problem, and then develop a practical algorithm based on linear programming relaxation and a rounding scheme. Under extensive evaluation on 11 years of real-world ICLR (International Conference on Learning Representations) data, our method preserves up to $19.23\%$ more papers without violating any author limits. Moreover, our algorithm is highly efficient in practice, with all results on ICLR data computed within at most 53.64 seconds. Our work provides a simple and practical desk-rejection strategy that significantly reduces unnecessary rejections, demonstrating strong potential to improve current CS conference submission policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20141v1</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Zhao Song, Jiahao Zhang</dc:creator>
    </item>
    <item>
      <title>Cut-Query Algorithms with Few Rounds</title>
      <link>https://arxiv.org/abs/2506.20412</link>
      <description>arXiv:2506.20412v1 Announce Type: new 
Abstract: In the cut-query model, the algorithm can access the input graph $G=(V,E)$ only via cut queries that report, given a set $S\subseteq V$, the total weight of edges crossing the cut between $S$ and $V\setminus S$. This model was introduced by Rubinstein, Schramm and Weinberg [ITCS'18] and its investigation has so far focused on the number of queries needed to solve optimization problems, such as global minimum cut. We turn attention to the round complexity of cut-query algorithms, and show that several classical problems can be solved in this model with only a constant number of rounds.
  Our main results are algorithms for finding a minimum cut in a graph, that offer different tradeoffs between round complexity and query complexity, where $n=|V|$ and $\delta(G)$ denotes the minimum degree of $G$: (i) $\tilde{O}(n^{4/3})$ cut queries in two rounds in unweighted graphs; (ii) $\tilde{O}(rn^{1+1/r}/\delta(G)^{1/r})$ queries in $2r+1$ rounds for any integer $r\ge 1$ again in unweighted graphs; and (iii) $\tilde{O}(rn^{1+(1+\log_n W)/r})$ queries in $4r+3$ rounds for any $r\ge1$ in weighted graphs. We also provide algorithms that find a minimum $(s,t)$-cut and approximate the maximum cut in a few rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20412v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yotam Kenneth-Mordoch, Robert Krauthgamer</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More</title>
      <link>https://arxiv.org/abs/2506.20030</link>
      <description>arXiv:2506.20030v1 Announce Type: cross 
Abstract: This paper derives polynomial-time approximation schemes for several NP-hard stochastic optimization problems from the algorithmic mechanism design and operations research literatures. The problems we consider involve a principal or seller optimizing with respect to a subsequent choice by an agent or buyer. These include posted pricing for a unit-demand buyer with independent values (Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with independent utilities (Talluri and van Ryzin, 2004), and delegated choice (Khodabakhsh et al., 2024). Our results advance the state of the art for each of these problems. For unit-demand pricing with discrete distributions, our multiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we additionally give a PTAS for the unbounded regular case, improving on the latter paper's QPTAS. For assortment optimization, no constant approximation was previously known. For delegated choice, we improve on both the $3$-approximation for the case with no outside option and the super-constant-approximation with an outside option.
  A key technical insight driving our results is an economically meaningful property we term utility alignment. Informally, a problem is utility aligned if, at optimality, the principal derives most of their utility from realizations where the agent's utility is also high. Utility alignment allows the algorithm designer to focus on maximizing performance on realizations with high agent utility, which is often an algorithmically simpler task. We prove utility alignment results for all the problems mentioned above, including strong results for unit-demand pricing and delegation, as well as a weaker but very broad guarantee that holds for many other problems under very mild conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20030v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Bowers, Marius Garbea, Emmanouil Pountourakis, Samuel Taggart</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations</title>
      <link>https://arxiv.org/abs/2506.20362</link>
      <description>arXiv:2506.20362v1 Announce Type: cross 
Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that bypasses the need for negative sampling by leveraging spectral bootstrapping techniques. Our method integrates Laplacian-based signals into the learning process, allowing the model to effectively capture rich structural representations without relying on contrastive objectives or handcrafted augmentations. By focusing on positive alignment, LaplaceGNN achieves linear scaling while offering a simpler, more efficient, self-supervised alternative for graph neural networks, applicable across diverse domains. Our contributions are twofold: we precompute spectral augmentations through max-min centrality-guided optimization, enabling rich structural supervision without relying on handcrafted augmentations, then we integrate an adversarial bootstrapped training scheme that further strengthens feature learning and robustness. Our extensive experiments on different benchmark datasets show that LaplaceGNN achieves superior performance compared to state-of-the-art self-supervised graph methods, offering a promising direction for efficiently learning expressive graph representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20362v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Bini, Stephane Marchand-Maillet</dc:creator>
    </item>
    <item>
      <title>Deterministic Scheduling of Periodic Messages for Low Latency in Cloud RAN</title>
      <link>https://arxiv.org/abs/1801.07029</link>
      <description>arXiv:1801.07029v5 Announce Type: replace-cross 
Abstract: Cloud-RAN (C-RAN) is a cellular network architecture where processing units, previously attached to antennas, are centralized in data centers. The main challenge in meeting protocol time constraints is minimizing the latency of periodic messages exchanged between antennas and processing units. We demonstrate that statistical multiplexing introduces significant logical latency due to buffering at network nodes to prevent collisions. To address this, we propose a deterministic scheme for periodic message transmission without collisions, eliminating latency caused by buffering.
  We develop several algorithms to compute such schemes for star-routed networks, a common topology where all antennas share a single link. First, we show that deterministic transmission is possible without buffering when routes are short or network load is low. Under high load, we allow buffering at processing units and introduce the Periodic Minimal Latency Scheduling (PMLS) algorithm, adapted from classical scheduling methods. Experimental results indicate that even at full load, PMLS finds deterministic transmission schemes with negligible logical latency, whereas statistical multiplexing incurs substantial delays. Moreover, PMLS runs in polynomial time and scales efficiently to hundreds of antennas. Building on this approach, we also derive low-latency periodic transmission schemes that coexist with additional random network traffic. This article extends previous work presented at ICT.</description>
      <guid isPermaLink="false">oai:arXiv.org:1801.07029v5</guid>
      <category>cs.NI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominique Barth, Ma\"el Guiraud, Yann Strozecki</dc:creator>
    </item>
    <item>
      <title>Computing finite index congruences of finitely presented semigroups and monoids</title>
      <link>https://arxiv.org/abs/2302.06295</link>
      <description>arXiv:2302.06295v4 Announce Type: replace-cross 
Abstract: In this paper, we describe an algorithm for computing the left, right, or 2-sided congruences of a finitely presented semigroup or monoid with finitely many classes, and an alternative algorithm when the finitely presented semigroup or monoid is finite. We compare the two algorithms presented with existing algorithms and implementations. The first algorithm is a generalization of Sims' low-index subgroup algorithm for finding the congruences of a monoid. The second algorithm involves determining the distinct principal congruences, and then finding all of their possible joins. Variations of this algorithm have been suggested in numerous contexts by numerous authors. We show how to utilize the theory of relative Green's relations, and a version of Schreier's Lemma for monoids, to reduce the number of principal congruences that must be generated as the first step of this approach. Both of the algorithms described in this paper are implemented in the GAP package Semigroups, and the first algorithm is available in the C++ library libsemigroups and in its python bindings libsemigroups_pybind11.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06295v4</guid>
      <category>math.RA</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Anagnostopoulou-Merkouri, Reinis Cirpons, James D. Mitchell, Maria Tsalakou</dc:creator>
    </item>
  </channel>
</rss>
