<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:52:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Regression Discontinuity Design with Distribution-Valued Outcomes</title>
      <link>https://arxiv.org/abs/2504.03992</link>
      <description>arXiv:2504.03992v1 Announce Type: new 
Abstract: This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a "local average quantile treatment effect", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03992v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Estimating Demand with Recentered Instruments</title>
      <link>https://arxiv.org/abs/2504.04056</link>
      <description>arXiv:2504.04056v1 Announce Type: new 
Abstract: We develop a new approach to estimating flexible demand models with exogenous supply-side shocks. Our approach avoids conventional assumptions of exogenous product characteristics, putting no restrictions on product entry, despite using instrumental variables that incorporate characteristic variation. The proposed instruments are model-predicted responses of endogenous variables to the exogenous shocks, recentered to avoid bias from endogenous characteristics. We illustrate the approach in a series of Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04056v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Borusyak, Mauricio Caceres Bravo, Peter Hull</dc:creator>
    </item>
    <item>
      <title>Rationalizing dynamic choices</title>
      <link>https://arxiv.org/abs/2504.05251</link>
      <description>arXiv:2504.05251v1 Announce Type: cross 
Abstract: An analyst observes an agent take a sequence of actions. The analyst does not have access to the agent's information and ponders whether the observed actions could be justified through a rational Bayesian model with a known utility function. We show that the observed actions cannot be justified if and only if there is a single deviation argument that leaves the agent better off, regardless of the information. The result is then extended to allow for distributions over possible action sequences. Four applications are presented: monotonicity of rationalization with risk aversion, a potential rejection of the Bayesian model with observable data, feasible outcomes in dynamic information design, and partial identification of preferences without assumptions on information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05251v1</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrique de Oliveira, Rohit Lamba</dc:creator>
    </item>
    <item>
      <title>Eigenvalue-Based Randomness Test for Residual Diagnostics in Panel Data Models</title>
      <link>https://arxiv.org/abs/2504.05297</link>
      <description>arXiv:2504.05297v1 Announce Type: cross 
Abstract: This paper introduces the Eigenvalue-Based Randomness (EBR) test - a novel approach rooted in the Tracy-Widom law from random matrix theory - and applies it to the context of residual analysis in panel data models. Unlike traditional methods, which target specific issues like cross-sectional dependence or autocorrelation, the EBR test simultaneously examines multiple assumptions by analyzing the largest eigenvalue of a symmetrized residual matrix. Monte Carlo simulations demonstrate that the EBR test is particularly robust in detecting not only standard violations such as autocorrelation and linear cross-sectional dependence (CSD) but also more intricate non-linear and non-monotonic dependencies, making it a comprehensive and highly flexible tool for enhancing the reliability of panel data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05297v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Betsab\'e P\'erez Garrido, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>GCov-Based Portmanteau Test</title>
      <link>https://arxiv.org/abs/2312.05373</link>
      <description>arXiv:2312.05373v2 Announce Type: replace 
Abstract: We study nonlinear serial dependence tests for non-Gaussian time series and residuals of dynamic models based on portmanteau statistics involving nonlinear autocovariances. A new test with an asymptotic $\chi^2$ distribution is introduced for testing nonlinear serial dependence (NLSD) in time series. This test is inspired by the Generalized Covariance (GCov) residual-based specification test, recently proposed as a diagnostic tool for semi-parametric dynamic models with i.i.d. non-Gaussian errors. It has a $\chi^2$ distribution when the model is correctly specified and estimated by the GCov estimator. We derive new asymptotic results under local alternatives for testing hypotheses on the parameters of a semi-parametric model. We extend it by introducing a GCov bootstrap test for residual diagnostics,\color{black} which is also available for models estimated by a different method, such as the maximum likelihood estimator under a parametric assumption on the error distribution. \color{black} A simulation study shows that the tests perform well in applications to mixed causal-noncausal autoregressive models. The GCov specification test is used to assess the fit of a mixed causal-noncausal model of aluminum prices with locally explosive patterns, i.e. bubbles and spikes between 2005 and 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05373v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joann Jasiak, Aryan Manafi Neyazi</dc:creator>
    </item>
    <item>
      <title>Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions</title>
      <link>https://arxiv.org/abs/2312.05985</link>
      <description>arXiv:2312.05985v4 Announce Type: replace 
Abstract: To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We provide an R package implementing fused extended two-way fixed effects, and we demonstrate FETWFE in simulation studies and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05985v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Faletto</dc:creator>
    </item>
    <item>
      <title>Regret Analysis in Threshold Policy Design</title>
      <link>https://arxiv.org/abs/2404.11767</link>
      <description>arXiv:2404.11767v2 Announce Type: replace 
Abstract: Threshold policies are decision rules that assign treatments based on whether an observable characteristic exceeds a certain threshold. They are widespread across multiple domains, including welfare programs, taxation, and clinical medicine. This paper examines the problem of designing threshold policies using experimental data, when the goal is to maximize the population welfare. First, I characterize the regret - a measure of policy optimality - of the Empirical Welfare Maximizer (EWM) policy, popular in the literature. Next, I introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the EWM's regret convergence rate under an additional smoothness condition. The two policies are compared by studying how differently their regrets depend on the population distribution, and investigating their finite sample performances through Monte Carlo simulations. In many contexts, the SWM policy guarantees larger welfare than the EWM. An empirical illustration demonstrates how the treatment recommendations of the two policies may differ in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11767v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Crippa</dc:creator>
    </item>
    <item>
      <title>Probabilistic Targeted Factor Analysis</title>
      <link>https://arxiv.org/abs/2412.06688</link>
      <description>arXiv:2412.06688v3 Announce Type: replace 
Abstract: We develop a probabilistic variant of Partial Least Squares (PLS) we call Probabilistic Targeted Factor Analysis (PTFA), which can be used to extract common factors in predictors that are useful to predict a set of predetermined target variables. Along with the technique, we provide an efficient expectation-maximization (EM) algorithm to learn the parameters and forecast the targets of interest. We develop a number of extensions to missing-at-random data, stochastic volatility, factor dynamics, and mixed-frequency data for real-time forecasting. In a simulation exercise, we show that PTFA outperforms PLS at recovering the common underlying factors affecting both features and target variables delivering better in-sample fit, and providing valid forecasts under contamination such as measurement error or outliers. Finally, we provide three applications in Economics and Finance where PTFA outperforms compared with PLS and Principal Component Analysis (PCA) at out-of-sample forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06688v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Miguel C. Herculano, Santiago Montoya-Bland\'on</dc:creator>
    </item>
    <item>
      <title>Bounding the Effect of Persuasion with Monotonicity Assumptions: Reassessing the Impact of TV Debates</title>
      <link>https://arxiv.org/abs/2503.06046</link>
      <description>arXiv:2503.06046v2 Announce Type: replace 
Abstract: Televised debates between presidential candidates are often regarded as the exemplar of persuasive communication. Yet, recent evidence from Le Pennec and Pons (2023) indicates that they may not sway voters as strongly as popular belief suggests. We revisit their findings through the lens of the persuasion rate and introduce a robust framework that does not require exogenous treatment, parallel trends, or credible instruments. Instead, we leverage plausible monotonicity assumptions to partially identify the persuasion rate and related parameters. Our results reaffirm that the sharp upper bounds on the persuasive effects of TV debates remain modest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06046v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Jae Jun, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Weak instrumental variables due to nonlinearities in panel data: A Super Learner Control Function estimator</title>
      <link>https://arxiv.org/abs/2504.03228</link>
      <description>arXiv:2504.03228v2 Announce Type: replace 
Abstract: A triangular structural panel data model with additive separable individual-specific effects is used to model the causal effect of a covariate on an outcome variable when there are unobservable confounders with some of them time-invariant. In this setup, a linear reduced-form equation might be problematic when the conditional mean of the endogenous covariate and the instrumental variables is nonlinear. The reason is that ignoring the nonlinearity could lead to weak instruments As a solution, we propose a triangular simultaneous equation model for panel data with additive separable individual-specific fixed effects composed of a linear structural equation with a nonlinear reduced form equation. The parameter of interest is the structural parameter of the endogenous variable. The identification of this parameter is obtained under the assumption of available exclusion restrictions and using a control function approach. Estimating the parameter of interest is done using an estimator that we call Super Learner Control Function estimator (SLCFE). The estimation procedure is composed of two main steps and sample splitting. We estimate the control function using a super learner using sample splitting. In the following step, we use the estimated control function to control for endogeneity in the structural equation. Sample splitting is done across the individual dimension. We perform a Monte Carlo simulation to test the performance of the estimators proposed. We conclude that the Super Learner Control Function Estimators significantly outperform Within 2SLS estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03228v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monika Avila Marquez</dc:creator>
    </item>
    <item>
      <title>Parametrization, Prior Independence, and the Semiparametric Bernstein-von Mises Theorem for the Partially Linear Model</title>
      <link>https://arxiv.org/abs/2306.03816</link>
      <description>arXiv:2306.03816v5 Announce Type: replace-cross 
Abstract: I prove a semiparametric Bernstein-von Mises theorem for a partially linear regression model with independent priors for the low-dimensional parameter of interest and the infinite-dimensional nuisance parameters. My result avoids a challenging prior invariance condition that arises from a loss of information associated with not knowing the nuisance parameter. The key idea is to employ a feasible reparametrization of the partially linear regression model that reflects the semiparametric structure of the model. This allows a researcher to assume independent priors for the model parameters while automatically accounting for the loss of information associated with not knowing the nuisance parameters. The theorem is verified for uniform wavelet series priors and Mat\'{e}rn Gaussian process priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03816v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher D. Walker</dc:creator>
    </item>
    <item>
      <title>On Sinkhorn's Algorithm and Choice Modeling</title>
      <link>https://arxiv.org/abs/2310.00260</link>
      <description>arXiv:2310.00260v2 Announce Type: replace-cross 
Abstract: For a broad class of models widely used in practice for choice and ranking data based on Luce's choice axiom, including the Bradley--Terry--Luce and Plackett--Luce models, we show that the associated maximum likelihood estimation problems are equivalent to a classic matrix balancing problem with target row and column sums. This perspective opens doors between two seemingly unrelated research areas, and allows us to unify existing algorithms in the choice modeling literature as special instances or analogs of Sinkhorn's celebrated algorithm for matrix balancing. We draw inspirations from these connections and resolve some open problems on the study of Sinkhorn's algorithm. We establish the global linear convergence of Sinkhorn's algorithm for non-negative matrices whenever finite scaling matrices exist, and characterize its linear convergence rate in terms of the algebraic connectivity of a weighted bipartite graph. We further derive the sharp asymptotic rate of linear convergence, which generalizes a classic result of Knight (2008). To our knowledge, these are the first quantitative linear convergence results for Sinkhorn's algorithm for general non-negative matrices and positive marginals. Our results highlight the importance of connectivity and orthogonality structures in matrix balancing and Sinkhorn's algorithm, which could be of independent interest. More broadly, the connections we establish in this paper between matrix balancing and choice modeling could also help motivate further transmission of ideas and lead to interesting results in both disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00260v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaonan Qu, Alfred Galichon, Wenzhi Gao, Johan Ugander</dc:creator>
    </item>
  </channel>
</rss>
