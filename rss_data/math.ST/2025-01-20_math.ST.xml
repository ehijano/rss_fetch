<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 05:02:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Strong Consistency of Sparse K-means Clustering</title>
      <link>https://arxiv.org/abs/2501.09983</link>
      <description>arXiv:2501.09983v1 Announce Type: new 
Abstract: In this paper, we prove the strong consistency of the sparse K-means method proposed by Witten and Tibshirani (2010). We prove the consistency in both risk and clustering for the Euclidean distance. We discuss the characterization of the limit of the clustering under some special cases. For the general distance, we prove the consistency in risk. Our result naturally extends to other models with the same objective function but different constraints such as l0 or l1 penalty in Chang et al. (2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09983v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeungju Kim, Johan Lim</dc:creator>
    </item>
    <item>
      <title>Robust density estimation over star-shaped density classes</title>
      <link>https://arxiv.org/abs/2501.10025</link>
      <description>arXiv:2501.10025v1 Announce Type: new 
Abstract: We establish a novel criterion for comparing the performance of two densities, $g_1$ and $g_2$, within the context of corrupted data. Utilizing this criterion, we propose an algorithm to construct a density estimator within a star-shaped density class, $\mathcal{F}$, under conditions of data corruption. We proceed to derive the minimax upper and lower bounds for density estimation across this star-shaped density class, characterized by densities that are uniformly bounded above and below (in the sup norm), in the presence of adversarially corrupted data. Specifically, we assume that a fraction $\epsilon \leq \frac{1}{3}$ of the $N$ observations are arbitrarily corrupted. We obtain the minimax upper bound $\max\{ \tau_{\overline{J}}^2, \epsilon \} \wedge d^2$. Under certain conditions, we obtain the minimax risk, up to proportionality constants, under the squared $L_2$ loss as $$ \max\left\{ \tau^{*2} \wedge d^2, \epsilon \wedge d^2 \right\}, $$ where $\tau^* := \sup\left\{ \tau : N\tau^2 \leq \log \mathcal{M}_{\mathcal{F}}^{\text{loc}}(\tau, c) \right\}$ for a sufficiently large constant $c$. Here, $\mathcal{M}_{\mathcal{F}}^{\text{loc}}(\tau, c)$ denotes the local entropy of the set $\mathcal{F}$, and $d$ is the $L_2$ diameter of $\mathcal{F}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10025v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaolong Liu, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Vector-Valued Gaussian Processes and their Kernels on a Class of Metric Graphs</title>
      <link>https://arxiv.org/abs/2501.10208</link>
      <description>arXiv:2501.10208v1 Announce Type: new 
Abstract: Despite the increasing importance of stochastic processes on linear networks and graphs, current literature on multivariate (vector-valued) Gaussian random fields on metric graphs is elusive. This paper challenges several aspects related to the construction of proper matrix-valued kernels structures. We start by considering matrix-valued metrics that can be composed with scalar- or matrix-valued functions to implement valid kernels associated with vector-valued Gaussian fields. We then provide conditions for certain classes of matrix-valued functions to be composed with the univariate resistance metric and ensure positive semidefiniteness. Special attention is then devoted to Euclidean trees, where a substantial effort is required given the absence of literature related to multivariate kernels depending on the $\ell_1$ metric. Hence, we provide a foundational contribution to certain classes of matrix-valued positive semidefinite functions depending on the $\ell_1$ metric. This fact is then used to characterise kernels on Euclidean trees with a finite number of leaves. Amongst those, we provide classes of matrix-valued covariance functions that are compactly supported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10208v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tobia Filosi, Emilio Porcu, Xavier Emery, Claudio Agostinelli, Alfredo Alegr\`ia</dc:creator>
    </item>
    <item>
      <title>SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft Semi-multivariate Split Rules</title>
      <link>https://arxiv.org/abs/2501.09900</link>
      <description>arXiv:2501.09900v1 Announce Type: cross 
Abstract: Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained significant popularity due to their remarkable predictive performance and ability to quantify uncertainty. However, standard decision tree models rely on recursive data splits at each decision node, using deterministic decision rules based on a single univariate feature. This approach limits their ability to effectively capture complex decision boundaries, particularly in scenarios involving multiple features, such as spatial domains, or when transitions are either sharp or smoothly varying. In this paper, we introduce a novel probabilistic additive decision tree model that employs a soft split rule. This method enables highly flexible splits that leverage both univariate and multivariate features, while also respecting the geometric properties of the feature domain. Notably, the probabilistic split rule adapts dynamically across decision nodes, allowing the model to account for varying levels of smoothness in the regression function. We demonstrate the utility of the proposed model through comparisons with existing tree-based models on synthetic datasets and a New York City education dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09900v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stamatina Lamprinakou, Huiyan Sang, Bledar A. Konomi, Ligang Lu</dc:creator>
    </item>
    <item>
      <title>Prior distributions for structured semi-orthogonal matrices</title>
      <link>https://arxiv.org/abs/2501.10263</link>
      <description>arXiv:2501.10263v1 Announce Type: cross 
Abstract: Statistical models for multivariate data often include a semi-orthogonal matrix parameter. In many applications, there is reason to expect that the semi-orthogonal matrix parameter satisfies a structural assumption such as sparsity or smoothness. From a Bayesian perspective, these structural assumptions should be incorporated into an analysis through the prior distribution. In this work, we introduce a general approach to constructing prior distributions for structured semi-orthogonal matrices that leads to tractable posterior inference via parameter-expanded Markov chain Monte Carlo. We draw upon recent results from random matrix theory to establish a theoretical basis for the proposed approach. We then introduce specific prior distributions for incorporating sparsity or smoothness and illustrate their use through applications to biological and oceanographic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10263v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jauch, Marie-Christine D\"uker, Peter Hoff</dc:creator>
    </item>
    <item>
      <title>Martingale Approach to Gambler's Ruin Problem for Correlated Random Walks</title>
      <link>https://arxiv.org/abs/2501.10302</link>
      <description>arXiv:2501.10302v1 Announce Type: cross 
Abstract: The gambler's ruin problem for correlated random walks, both with and without delays, is addressed by Optional Stopping Theorem for martingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10302v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Pozdnyakov</dc:creator>
    </item>
    <item>
      <title>Privacy Guarantees in Posterior Sampling under Contamination</title>
      <link>https://arxiv.org/abs/2403.07772</link>
      <description>arXiv:2403.07772v2 Announce Type: replace 
Abstract: In recent years differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms. In this article, we study differential privacy in Bayesian posterior sampling settings. We begin by considering differential privacy in the most common privatization setting in which Laplace or Gaussian noise is simply injected into the output. In an effort to achieve better differential privacy, we consider adopting {\em Huber's contamination model} for use within privacy settings, and replace at random data points with samples from a heavy-tailed distribution ({\em instead} of injecting noise into the output). We derive bounds for the differential privacy level $(\epsilon,\delta)$ of our approach, without the need to impose the restriction of having a bounded observation and parameter space which is commonly used by existing approaches and literature. We further consider for our approach the effect of sample size on the privacy level and the convergence rate of $(\epsilon,\delta)$ to zero. Asymptotically, our contamination approach is fully private at no cost of information loss. We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of the convergence rate, together with some simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07772v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenggang Hu, Louis Aslett, Hongsheng Dai, Murray Pollock, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Logistic lasso regression with nearest neighbors for gradient-based dimension reduction</title>
      <link>https://arxiv.org/abs/2407.08485</link>
      <description>arXiv:2407.08485v2 Announce Type: replace 
Abstract: This paper investigates a new approach to estimate the gradient of the conditional probability given the covariates in the binary classification framework. The proposed approach consists in fitting a localized nearest-neighbor logistic model with $\ell_1$-penalty in order to cope with possibly high-dimensional covariates. Our theoretical analysis shows that the pointwise convergence rate of the gradient estimator is optimal under very mild conditions. Moreover, using an outer product of such gradient estimates at several points in the covariate space, we establish the rate of convergence for estimating the so-called central subspace, a well-known object allowing to carry out dimension reduction within the covariate space. Our implementation uses cross-validation on the misclassification rate to estimate the dimension of this subspace. We find that the proposed approach outperforms existing competitors in synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08485v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Fran\c{c}ois Portier, Gilles Stupfler</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Estimation with Observational Network Data using Machine Learning</title>
      <link>https://arxiv.org/abs/2206.14591</link>
      <description>arXiv:2206.14591v4 Announce Type: replace-cross 
Abstract: Causal inference methods for treatment effect estimation usually assume independent units. However, this assumption is often questionable because units may interact, resulting in spillover effects between them. We develop augmented inverse probability weighting (AIPW) for estimation and inference of the expected average treatment effect (EATE) with observational data from a single (social) network with spillover effects. In contrast to overall effects such as the global average treatment effect (GATE), the EATE measures, in expectation and on average over all units, how the outcome of a unit is causally affected by its own treatment, marginalizing over the spillover effects from other units. We develop cross-fitting theory with plugin machine learning to obtain a semiparametric treatment effect estimator that converges at the parametric rate and asymptotically follows a Gaussian distribution. The asymptotics are developed using the dependency graph rather than the network graph, which makes explicit that we allow for spillover effects beyond immediate neighbors in the network. We apply our AIPW method to the Swiss StudentLife Study data to investigate the effect of hours spent studying on exam performance accounting for the students' social network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14591v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinne Emmenegger, Meta-Lina Spohn, Timon Elmer, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Consistent estimation of generative model representations in the data kernel perspective space</title>
      <link>https://arxiv.org/abs/2409.17308</link>
      <description>arXiv:2409.17308v2 Announce Type: replace-cross 
Abstract: Generative models, such as large language models and text-to-image diffusion models, produce relevant information when presented a query. Different models may produce different information when presented the same query. As the landscape of generative models evolves, it is important to develop techniques to study and analyze differences in model behaviour. In this paper we present novel theoretical results for embedding-based representations of generative models in the context of a set of queries. In particular, we establish sufficient conditions for the consistent estimation of the model embeddings in situations where the query set and the number of models grow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17308v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aranyak Acharyya, Michael W. Trosset, Carey E. Priebe, Hayden S. Helm</dc:creator>
    </item>
  </channel>
</rss>
