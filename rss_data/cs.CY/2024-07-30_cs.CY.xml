<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 31 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Designing an AI-Powered Mentorship Platform for Professional Development: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2407.20233</link>
      <description>arXiv:2407.20233v1 Announce Type: new 
Abstract: This article examines the promising prospects and potential hurdles associated with the development of MentorAI, a conceptual AI-driven mentorship platform for professional growth yet to be actualized. The article explores the essential characteristics and technological underpinnings required for the successful creation and efficacy of the MentorAI platform in providing tailored mentorship experiences. The article highlights the transformative potential of MentorAI on various dimensions of professional growth, such as boosting career progression, nurturing skill development, and supporting a balanced work-life environment for professionals. MentorAI, through its AI-based approach, aspires to offer real-time guidance, resources, and assistance customized to each individual's specific needs and goals. Furthermore, the article examines the core technologies crucial to MentorAI's operation, including artificial intelligence, machine learning, and natural language comprehension. These technologies will empower the platform to process user inputs, deliver context-sensitive responses, and dynamically adjust to user preferences and objectives. The deployment of MentorAI presents potential challenges and ethical concerns, as with any groundbreaking technology. The article outlines critical issues like data protection, security, algorithmic bias, and moral quandaries concerning substituting human mentors with AI systems. Addressing these challenges proactively and deliberately is vital to ensure a positive impact on users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20233v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14445/22312803/IJCTT-V71I4P114</arxiv:DOI>
      <arxiv:journal_reference>IJCTT, 71(4), 108-114, 2023</arxiv:journal_reference>
      <dc:creator>Rahul Bagai, Vaishali Mane</dc:creator>
    </item>
    <item>
      <title>Exploring Factors Affecting Student Learning Satisfaction during COVID-19 in South Korea</title>
      <link>https://arxiv.org/abs/2407.20234</link>
      <description>arXiv:2407.20234v1 Announce Type: new 
Abstract: Understanding students' preferences and learning satisfaction during COVID-19 has focused on learning attributes such as self-efficacy, performance, and engagement. Although existing efforts have constructed statistical models capable of accurately identifying significant factors impacting learning satisfaction, they do not necessarily explain the complex relationships among these factors in depth. This study aimed to understand several facets related to student learning preferences and satisfaction during the pandemic such as individual learner characteristics, instructional design elements and social and environmental factors. Responses from 302 students from Sungkyunkwan University, South Korea were collected between 2021 and 2022. Information gathered included their gender, study major, satisfaction and motivation levels when learning, perceived performance, emotional state and learning environment. Wilcoxon Rank sum test and Explainable Boosting Machine (EBM) were performed to determine significant differences in specific cohorts. The two core findings of the study are as follows:1) Using Wilcoxon Rank Sum test, we can attest with 95% confidence that students who took offline classes had significantly higher learning satisfaction, among other attributes, than those who took online classes, as with STEM versus HASS students; 2) An explainable boosting machine (EBM) model fitted to 95.08% accuracy determined the top five factors affecting students' learning satisfaction as their perceived performance, their perception on participating in class activities, their study majors, their ability to conduct discussions in class and the study space availability at home. Positive perceived performance and ability to discuss with classmates had a positive impact on learning satisfaction, while negative perception on class activities participation had a negative impact on learning satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20234v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwon Han, Chaeeun Ryu, Gayathri Nadarajan</dc:creator>
    </item>
    <item>
      <title>Solve the Refugee Crisis with Data</title>
      <link>https://arxiv.org/abs/2407.20235</link>
      <description>arXiv:2407.20235v1 Announce Type: new 
Abstract: In this study, we addressed the refugee crisis through two main models. For predicting the ultimate number of refugees, we first established a Logistic Regression Model, but due to the limited data points, its prediction accuracy was suboptimal. Consequently, we incorporated Gray Theory to develop the Gary Verhulst Model, which provided scientifically sound and reasonable predictions. Statistical tests comparing both models highlighted the superiority of the Gary Verhulst Model. For formulating refugee allocation schemes, we initially used the Factor Analysis Method but found it too subjective and lacking in rigorous validation measures. We then developed a Refugee Allocation Model based on the Analytic Hierarchy Process (AHP), which absorbed the advantages of the former method. This model underwent extensive validation and passed consistency checks, resulting in an effective and scientific refugee allocation scheme. We also compared our model with the current allocation schemes and proposed improvements. Finally, we discussed the advantages and disadvantages of our models, their applicability, and scalability. Sensitivity analysis was conducted, and directions for future model improvements were identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20235v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Liu</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence from Idea to Implementation. How Can AI Reshape the Education Landscape?</title>
      <link>https://arxiv.org/abs/2407.20236</link>
      <description>arXiv:2407.20236v1 Announce Type: new 
Abstract: This introductory chapter provides an overview of the evolution and impact of Artificial Intelligence technologies in today society. Beginning with a historical context while exploring a few general definitions of AI, the author provides a timeline of the used technologies, highlighting its periods of stagnation, commonly referred to as AI winters, and the subsequent resurgence fueled by relentless enthusiasm and investment. The narrative then transitions to focus on the transformative effects of AI on society at large, with a particular emphasis on educational applications. Through examples, the paper shows how AI technologies have moved from theoretical constructs to practical tools that are reshaping pedagogical approaches and student engagement. The essay concludes by discussing the prospects of AI in education, emphasizing the need for a balanced approach that considers both technological advancements and societal implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20236v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Changing the game. AI in Education, 2023, ISBN 978-606-749-727-4</arxiv:journal_reference>
      <dc:creator>Catalin Vrabie</dc:creator>
    </item>
    <item>
      <title>A Study on Internet of Things in Women and Children Healthcare</title>
      <link>https://arxiv.org/abs/2407.20237</link>
      <description>arXiv:2407.20237v1 Announce Type: new 
Abstract: Individual entities are being connected every day with the advancement of Internet of Things (IoT). IoT contains various application domains and healthcare is one of them indeed. It is receiving a lot of attention recently because of its seamless integration with electronic health (eHealth) and telemedicine. IoT has the capability of collecting patient data incessantly which surely helps in preventive care. Doctors can diagnose their patients early to avoid complications and they can suggest further modifications if needed. As the whole process is automated, risk of errors is reduced. Administrative paperwork and data entry tasks will be automated due to tracking and connectivity. As a result, healthcare providers can engage themselves more in patient care. In traditional healthcare services, an individual used to have access to minimal insights into his own health. Hence, they were less conscious about themselves and depended wholly on the healthcare facilities for unfortunate events. But they can track their vitals, activities and fitness with the aid of connected devices now. Furthermore, they can suggest their preferred user interfaces. This paper describes several methods, practices and prototypes regarding IoT in the field of healthcare for women and children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20237v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Int. J. Mach. Learn. Netw. Collab. Eng. 3 (2019) 01-15. Retrieved from https://mlnce.net/index.php/Home/article/view/63</arxiv:journal_reference>
      <dc:creator>Nishargo Nigar</dc:creator>
    </item>
    <item>
      <title>Impact of COVID-19 post lockdown on eating habits and lifestyle changes among university students in Bangladesh: a web based cross sectional study</title>
      <link>https://arxiv.org/abs/2407.20238</link>
      <description>arXiv:2407.20238v1 Announce Type: new 
Abstract: Background:Since the confinement of the lockdown, universities transferred their teaching and learning activities in online as an all-out intention to prevent the transmission of the infection. This study aimed to determine the significant changes in food habits, physical activity, sleeping hours, shopping habits, Internet use time and mental status of the students and investigate the associations between variables. Methods:The study participants were 307 Undergraduate students, between 18 and 25 years of age completed a structured questionnaire from January 3, 2022 to February 13, 2022. The questionnaire included demographic information of the students, questionnaire of dietary pattern, physical activity, sleep quality index, Shopping practice and Internet use time.Chi-square tests were used to associate the baseline information with lifestyle changes in post lockdown. Results:The study reveals that 21.5% of respondents gained weight, 23.8% lost their weight and 41.7% controlled their weight. Eating of homemade food decreased after lockdown 76.5% and eating of restaurant food increased after lockdown 23.5%. A number of major meals 3-4 meals per day decreased after lockdown 61.9%. Physical exercise significantly increased after lockdown (p=0.001). Sleeping hours per day significantly decreased after lockdown (p=0.001), sleep quality was almost the same and energy level increased more in post lockdown. Respondents felt mentally tired after lockdown 60.9%. Respondents spending time on the Internet in chat rooms was 88.3%. Conclusions: This study represents the significant impact on food habits, mental health, and daily routine of students after lockdown, suggesting that we should maintain a balanced diet, physical exercise to sleep quality and mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20238v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.18203/2394-6040.ijcmph20221519</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Community Medicine and Public Health | June 2022| Vol 9 | Issue 6 | Page 2449-2456</arxiv:journal_reference>
      <dc:creator>Faysal Ahmed Imran (Department of Nursing, Tairunnessa Memorial Medical College and Hospital, Gazipur, Bangladesh), Mst Eshita Khatun (Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh)</dc:creator>
    </item>
    <item>
      <title>Landslide vulnerability analysis using frequency ratio (FR) model: a study on Bandarban district, Bangladesh</title>
      <link>https://arxiv.org/abs/2407.20239</link>
      <description>arXiv:2407.20239v1 Announce Type: new 
Abstract: This study assesses landslide vulnerability in the Chittagong Hill Tracts (CHT), specifically focusing on Bandarban district in Southeast Bangladesh. By employing a multidisciplinary approach, thirteen factors influencing landslides were examined, including terrain features, land use, and environmental variables. Utilizing the FR model and integrating various datasets such as DEM, satellite images, and rainfall data, landslide susceptibility mapping was conducted. The analysis revealed that steep slopes, high elevations, specific aspects, and curvature contribute significantly to landslide susceptibility. Factors like erosion, soil saturation, drainage density, and human activities were also identified as key contributors. The study underscored the impact of land use changes and highlighted the stabilizing effect of vegetation cover. The resulting Landslide Susceptibility Map (LSM) categorized the area into five susceptibility zones. The model demonstrated a prediction accuracy of 76.47%, indicating its effectiveness in forecasting landslide occurrences. Additionally, the study identified significant changes in the study area over three decades, emphasizing the influence of human activities on slope instability. These findings offer valuable insights for policymakers and land-use planners, emphasizing the importance of proactive measures to mitigate landslide risks and ensure community safety. Incorporating these insights into policy frameworks</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20239v1</guid>
      <category>cs.CY</category>
      <category>physics.geo-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nafis Fuad, Javed Meandad, Ashraful Haque, Rukhsar Sultana, Sumaiya Binte Anwar, Sharmin Sultana</dc:creator>
    </item>
    <item>
      <title>Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada</title>
      <link>https://arxiv.org/abs/2407.20240</link>
      <description>arXiv:2407.20240v1 Announce Type: new 
Abstract: The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settlement sector, ensuring human oversight, trustworthiness, and accountability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20240v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isar Nejadgholi, Maryam Molamohammadi</dc:creator>
    </item>
    <item>
      <title>NudgeRank: Digital Algorithmic Nudging for Personalized Health</title>
      <link>https://arxiv.org/abs/2407.20241</link>
      <description>arXiv:2407.20241v1 Announce Type: new 
Abstract: In this paper we describe NudgeRank, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems' 4%. Demonstrating scalability and reliability, NudgeRank operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20241v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671562</arxiv:DOI>
      <dc:creator>Jodi Chiam, Aloysius Lim, Ankur Teredesai</dc:creator>
    </item>
    <item>
      <title>BadRobot: Jailbreaking LLM-based Embodied AI in the Physical World</title>
      <link>https://arxiv.org/abs/2407.20242</link>
      <description>arXiv:2407.20242v1 Announce Type: new 
Abstract: Embodied artificial intelligence (AI) represents an artificial intelligence system that interacts with the physical world through sensors and actuators, seamlessly integrating perception and action. This design enables AI to learn from and operate within complex, real-world environments. Large Language Models (LLMs) deeply explore language instructions, playing a crucial role in devising plans for complex tasks. Consequently, they have progressively shown immense potential in empowering embodied AI, with LLM-based embodied AI emerging as a focal point of research within the community. It is foreseeable that, over the next decade, LLM-based embodied AI robots are expected to proliferate widely, becoming commonplace in homes and industries. However, a critical safety issue that has long been hiding in plain sight is: could LLM-based embodied AI perpetrate harmful behaviors? Our research investigates for the first time how to induce threatening actions in embodied AI, confirming the severe risks posed by these soon-to-be-marketed robots, which starkly contravene Asimov's Three Laws of Robotics and threaten human safety. Specifically, we formulate the concept of embodied AI jailbreaking and expose three critical security vulnerabilities: first, jailbreaking robotics through compromised LLM; second, safety misalignment between action and language spaces; and third, deceptive prompts leading to unaware hazardous behaviors. We also analyze potential mitigation measures and advocate for community awareness regarding the safety of embodied AI applications in the physical world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20242v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Shengshan Hu, Leo Yu Zhang</dc:creator>
    </item>
    <item>
      <title>Assessing AI Rationality: The Random Guesser Test for Sequential Decision-Making Systems</title>
      <link>https://arxiv.org/abs/2407.20276</link>
      <description>arXiv:2407.20276v1 Announce Type: new 
Abstract: We propose a general approach to quantitatively assessing the risk and vulnerability of artificial intelligence (AI) systems to biased decisions. The guiding principle of the proposed approach is that any AI algorithm must outperform a random guesser. This may appear trivial, but empirical results from a simplistic sequential decision-making scenario involving roulette games show that sophisticated AI-based approaches often underperform the random guesser by a significant margin. We highlight that modern recommender systems may exhibit a similar tendency to favor overly low-risk options. We argue that this "random guesser test" can serve as a useful tool for evaluating the rationality of AI actions, and also points towards increasing exploration as a potential improvement to such systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20276v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shun Ide, Allison Blunt, Djallel Bouneffouf</dc:creator>
    </item>
    <item>
      <title>Mapping the Digital Healthcare Revolution</title>
      <link>https://arxiv.org/abs/2407.20300</link>
      <description>arXiv:2407.20300v1 Announce Type: new 
Abstract: This introductory chapter briefly outlines the main theme of this volume, namely, to review the new opportunities and risks of digital healthcare from various disciplinary perspectives. These perspectives include law, public policy, organisational studies, and applied ethics. Based on this interdisciplinary approach, we hope that effective strategies may arise to ensure that benefits of this on-going revolution are deployed in a responsible and sustainable manner. The second part of the chapter comprises a brief review of the four parts and fourteen substantive chapters that comprise this volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20300v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Corrales Compagnucci, Mark Fenwick, Michael Lowery Wilson, Nikolaus Forgo, Till Baernighausen</dc:creator>
    </item>
    <item>
      <title>Legal Aspects of Decentralized and Platform-Driven Economies</title>
      <link>https://arxiv.org/abs/2407.20301</link>
      <description>arXiv:2407.20301v1 Announce Type: new 
Abstract: The sharing economy is sprawling across almost every sector and activity around the world. About a decade ago, there were only a handful of platform driven companies operating on the market. Zipcar, BlaBlaCar and Couchsurfing among them. Then Airbnb and Uber revolutionized the transportation and hospitality industries with a presence in virtually every major city. Access over ownership is the paradigm shift from the traditional business model that grants individuals the use of products or services without the necessity of buying them. Digital platforms, data and algorithm-driven companies as well as decentralized blockchain technologies have tremendous potential. But they are also changing the rules of the game. One of such technologies challenging the legal system are AI systems that will also reshape the current legal framework concerning the liability of operators, users and manufacturers. Therefore, this introductory chapter deals with explaining and describing the legal issues of some of these disruptive technologies. The chapter argues for a more forward-thinking and flexible regulatory structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20301v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Corrales Compagnucci, Toshiyuki Kono, Shinto Teramoto</dc:creator>
    </item>
    <item>
      <title>Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval</title>
      <link>https://arxiv.org/abs/2407.20371</link>
      <description>arXiv:2407.20371v1 Announce Type: new 
Abstract: Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1\% of cases and female-associated names in only 11.1\% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100\% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20371v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kyra Wilson, Aylin Caliskan</dc:creator>
    </item>
    <item>
      <title>The GPT Dilemma: Foundation Models and the Shadow of Dual-Use</title>
      <link>https://arxiv.org/abs/2407.20442</link>
      <description>arXiv:2407.20442v1 Announce Type: new 
Abstract: This paper examines the dual-use challenges of foundation models and the consequent risks they pose for international security. As artificial intelligence (AI) models are increasingly tested and deployed across both civilian and military sectors, distinguishing between these uses becomes more complex, potentially leading to misunderstandings and unintended escalations among states. The broad capabilities of foundation models lower the cost of repurposing civilian models for military uses, making it difficult to discern another state's intentions behind developing and deploying these models. As military capabilities are increasingly augmented by AI, this discernment is crucial in evaluating the extent to which a state poses a military threat. Consequently, the ability to distinguish between military and civilian applications of these models is key to averting potential military escalations. The paper analyzes this issue through four critical factors in the development cycle of foundation models: model inputs, capabilities, system use cases, and system deployment. This framework helps elucidate the points at which ambiguity between civilian and military applications may arise, leading to potential misperceptions. Using the Intermediate-Range Nuclear Forces (INF) Treaty as a case study, this paper proposes several strategies to mitigate the associated risks. These include establishing red lines for military competition, enhancing information-sharing protocols, employing foundation models to promote international transparency, and imposing constraints on specific weapon platforms. By managing dual-use risks effectively, these strategies aim to minimize potential escalations and address the trade-offs accompanying increasingly general AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20442v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Hickey</dc:creator>
    </item>
    <item>
      <title>The Future of International Data Transfers: Managing Legal Risk with a User-Held Data Model</title>
      <link>https://arxiv.org/abs/2407.20514</link>
      <description>arXiv:2407.20514v1 Announce Type: new 
Abstract: The General Data Protection Regulation contains a blanket prohibition on the transfer of personal data outside of the European Economic Area unless strict requirements are met. The rationale for this provision is to protect personal data and data subject rights by restricting data transfers to countries that may not have the same level of protection as the EEA. However, the ubiquitous and permeable character of new technologies such as cloud computing, and the increased inter connectivity between societies, has made international data transfers the norm and not the exception. The Schrems II case and subsequent regulatory developments have further raised the bar for companies to comply with complex and, often, opaque rules. Many firms are, therefore, pursuing technology-based solutions in order to mitigate this new legal risk. These emerging technological alternatives reduce the need for open-ended cross-border transfers and the practical challenges and legal risk that such transfers create after Schrems. This article examines one such alternative, namely a user-held data model. This approach takes advantage of personal data clouds that allows data subjects to store their data locally and in a more decentralised manner, thus decreasing the need for cross-border transfers and offering end-users the possibility of greater control over their data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20514v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulius Jurcys, Marcelo Corrales Compagnucci, Mark Fenwick</dc:creator>
    </item>
    <item>
      <title>A Three Steps Methodological Approach to Legal Governance Validation</title>
      <link>https://arxiv.org/abs/2407.20691</link>
      <description>arXiv:2407.20691v1 Announce Type: new 
Abstract: We present in this position paper a methodology to validate legal governance regulatory models from an empirical approach, as illustrated by means of three diagrams: (i) a scheme drawing the rule and meta-rule of law; (ii) a metamodel for legal governance; (iii) a causal validation scheme for legal compliance. These visualisations refer to different sets of notions corresponding respectively to (i) a general scheme with three dimensions and four clusters, (ii) a meta-model encompassing legal compliance through design (LCtD) and ecological validity, and (iii) the con-struction of an empirical validation model of causal chains. The final aim of the methodology is to build and test smart legal ecosystems (SLE) for Industry 4.0 and 5.0.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20691v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pompeu Casanovas, Mustafa Hashmi, Louis de Koker, Ho-Pun Lam</dc:creator>
    </item>
    <item>
      <title>Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries</title>
      <link>https://arxiv.org/abs/2407.20847</link>
      <description>arXiv:2407.20847v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both. Auditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively. First, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of the risk profile of advanced AI, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight. Secondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20847v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merlin Stein, Milan Gandhi, Theresa Kriecherbauer, Amin Oueslati, Robert Trager</dc:creator>
    </item>
    <item>
      <title>Human-Data Interaction Framework: A Comprehensive Model for a Future Driven by Data and Humans</title>
      <link>https://arxiv.org/abs/2407.21010</link>
      <description>arXiv:2407.21010v1 Announce Type: new 
Abstract: In an age defined by rapid data expansion, the connection between individuals and their digital footprints has become more intricate. The Human-Data Interaction (HDI) framework has become an essential approach to tackling the challenges and ethical issues associated with data governance and utilization in the modern digital world. This paper outlines the fundamental steps required for organizations to seamlessly integrate HDI principles, emphasizing auditing, aligning, formulating considerations, and the need for continuous monitoring and adaptation. Through a thorough audit, organizations can critically assess their current data management practices, trace the data lifecycle from collection to disposal, and evaluate the effectiveness of existing policies, security protocols, and user interfaces. The next step involves aligning these practices with the main HDI principles, such as informed consent, data transparency, user control, algorithm transparency, and ethical data use, to identify gaps that need strategic action. Formulating preliminary considerations includes developing policies and technical solutions to close identified gaps, ensuring that these practices not only meet legal standards, but also promote fairness and accountability in data interactions. The final step, monitoring and adaptation, highlights the need for setting up continuous evaluation mechanisms and being responsive to technological, regulatory, and societal developments, ensuring HDI practices stay up-to-date and effective. Successful implementation of the HDI framework requires multi-disciplinary collaboration, incorporating insights from technology, law, ethics, and user experience design. The paper posits that this comprehensive approach is vital for building trust and legitimacy in digital environments, ultimately leading to more ethical, transparent, and user-centric data interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21010v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Durango, Jose A. Gallud, Victor M. R. Penichet</dc:creator>
    </item>
    <item>
      <title>LAPIS: Language Model-Augmented Police Investigation System</title>
      <link>https://arxiv.org/abs/2407.20248</link>
      <description>arXiv:2407.20248v1 Announce Type: cross 
Abstract: Crime situations are race against time. An AI-assisted criminal investigation system, providing prompt but precise legal counsel is in need for police officers. We introduce LAPIS (Language Model Augmented Police Investigation System), an automated system that assists police officers to perform rational and legal investigative actions. We constructed a finetuning dataset and retrieval knowledgebase specialized in crime investigation legal reasoning task. We extended the dataset's quality by incorporating manual curation efforts done by a group of domain experts. We then finetuned the pretrained weights of a smaller Korean language model to the newly constructed dataset and integrated it with the crime investigation knowledgebase retrieval approach. Experimental results show LAPIS' potential in providing reliable legal guidance for police officers, even better than the proprietary GPT-4 model. Qualitative analysis on the rationales generated by LAPIS demonstrate the model's reasoning ability to leverage the premises and derive legally correct conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20248v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heedou Kim, Dain Kim, Jiwoo Lee, Chanwoong Yoon, Donghee Choi, Mogan Gim, Jaewoo Kang</dc:creator>
    </item>
    <item>
      <title>Survey of Design Paradigms for Social Robots</title>
      <link>https://arxiv.org/abs/2407.20556</link>
      <description>arXiv:2407.20556v1 Announce Type: cross 
Abstract: The demand for social robots in fields like healthcare, education, and entertainment increases due to their emotional adaptation features. These robots leverage multimodal communication, incorporating speech, facial expressions, and gestures to enhance user engagement and emotional support. The understanding of design paradigms of social robots is obstructed by the complexity of the system and the necessity to tune it to a specific task. This article provides a structured review of social robot design paradigms, categorizing them into cognitive architectures, role design models, linguistic models, communication flow, activity system models, and integrated design models. By breaking down the articles on social robot design and application based on these paradigms, we highlight the strengths and areas for improvement in current approaches. We further propose our original integrated design model that combines the most important aspects of the design of social robots. Our approach shows the importance of integrating operational, communicational, and emotional dimensions to create more adaptive and empathetic interactions between robots and humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20556v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rita Frieske, Xiaoyu Mo, Yini Fang, Jay Nieles, Bertram E. Shi</dc:creator>
    </item>
    <item>
      <title>Comparison of Large Language Models for Generating Contextually Relevant Questions</title>
      <link>https://arxiv.org/abs/2407.20578</link>
      <description>arXiv:2407.20578v1 Announce Type: cross 
Abstract: This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three LLMs are compared in their ability to create questions from university slide text without fine-tuning. Questions were obtained in a two-step pipeline: first, answer phrases were extracted from slides using Llama 2-Chat 13B; then, the three models generated questions for each answer. To analyze whether the questions would be suitable in educational applications for students, a survey was conducted with 46 students who evaluated a total of 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL by a small margin, particularly in terms of clarity and question-answer alignment. GPT-3.5 especially excels at tailoring questions to match the input answers. The contribution of this research is the analysis of the capacity of LLMs for Automatic Question Generation in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20578v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivo Lodovico Molina, Valdemar \v{S}v\'abensk\'y, Tsubasa Minematsu, Li Chen, Fumiya Okubo, Atsushi Shimada</dc:creator>
    </item>
    <item>
      <title>PIXELMOD: Improving Soft Moderation of Visual Misleading Information on Twitter</title>
      <link>https://arxiv.org/abs/2407.20987</link>
      <description>arXiv:2407.20987v1 Announce Type: cross 
Abstract: Images are a powerful and immediate vehicle to carry misleading or outright false messages, yet identifying image-based misinformation at scale poses unique challenges. In this paper, we present PIXELMOD, a system that leverages perceptual hashes, vector databases, and optical character recognition (OCR) to efficiently identify images that are candidates to receive soft moderation labels on Twitter. We show that PIXELMOD outperforms existing image similarity approaches when applied to soft moderation, with negligible performance overhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 US Presidential Election, and find that it is able to identify visually misleading images that are candidates for soft moderation with 0.99% false detection and 2.06% false negatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20987v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pujan Paudel, Chen Ling, Jeremy Blackburn, Gianluca Stringhini</dc:creator>
    </item>
    <item>
      <title>The Responsible Development of Automated Student Feedback with Generative AI</title>
      <link>https://arxiv.org/abs/2308.15334</link>
      <description>arXiv:2308.15334v2 Announce Type: replace 
Abstract: Contribution: This paper identifies four critical ethical considerations for implementing generative AI tools to provide automated feedback to students.
  Background: Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly with large language models (LLMs), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies.
  Intended Outcomes: The goal of this work is to enable the use of AI systems to automate mundane assessment and feedback tasks, without introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.
  Application Design: This paper applies an extant ethical framework used for AI and machine learning to the specific challenge of providing automated feedback to student engineers. The task is considered from both a development and maintenance perspective, considering how automated feedback tools will evolve and be used over time.
  Findings: This paper identifies four key ethical considerations for the implementation of automated feedback for students: Participation, Development, Impact on Learning and Evolution over Time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15334v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euan D Lindsay, Mike Zhang, Aditya Johri, Johannes Bjerva</dc:creator>
    </item>
    <item>
      <title>The Potential Impact of AI Innovations on U.S. Occupations</title>
      <link>https://arxiv.org/abs/2312.04714</link>
      <description>arXiv:2312.04714v5 Announce Type: replace 
Abstract: An occupation is comprised of interconnected tasks, and it is these tasks, not occupations themselves, that are affected by AI. To evaluate how tasks may be impacted, previous approaches utilized manual annotations or coarse-grained matching. Leveraging recent advancements in machine learning, we replace coarse-grained matching with more precise deep learning approaches. Introducing the AI Impact (AII) measure, we employ Deep Learning Natural Language Processing to automatically identify AI patents that may impact various occupational tasks at scale. Our methodology relies on a comprehensive dataset of 17,879 task descriptions and quantifies AI's potential impact through analysis of 24,758 AI patents filed with the United States Patent and Trademark Office (USPTO) between 2015 and 2022. Our results reveal that some occupations will potentially be impacted, and that impact is intricately linked to specific skills. These include not only routine tasks (codified as a series of steps), as previously thought, but also non-routine ones (e.g., diagnosing health conditions, programming computers, and tracking flight routes). However, AI's impact on labour is limited by the fact that some of the occupations affected are augmented rather than replaced (e.g., neurologists, software engineers, air traffic controllers), and the sectors affected are experiencing labour shortages (e.g., IT, Healthcare, Transport).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04714v5</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Akbar Septiandri, Marios Constantinides, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination</title>
      <link>https://arxiv.org/abs/2401.05254</link>
      <description>arXiv:2401.05254v3 Announce Type: replace 
Abstract: Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressions are associated with personal life and feelings on Twitter, while on Weibo such discussions are about socio-political topics in the society. These results suggest a West-East difference in the V-shaped relationship between valence and arousal of affective expressions on social media influenced by content differences. Our findings have implications for applications and theories related to cultural differences in affective expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05254v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Young-Min Cho, Dandan Pang, Stuti Thapa, Garrick Sherman, Lyle Ungar, Louis Tay, Sharath Chandra Guntuku</dc:creator>
    </item>
    <item>
      <title>Particip-AI: Anticipating Future AI Use Cases and Impacts with Lay Users</title>
      <link>https://arxiv.org/abs/2403.14791</link>
      <description>arXiv:2403.14791v3 Announce Type: replace 
Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without a comprehensive assessment of risks. As a first step towards democratic risk assessment and design of general purpose AI, we introduce PARTICIP-AI, a carefully designed framework for laypeople to speculate and assess AI use cases and their impacts. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards informing democratic AI development, we run a medium-scale study with inputs from 295 demographically diverse participants. Our analyses show that participants' responses emphasize applications for personal life and society, contrasting with most current AI development's business focus. We also surface diverse set of envisioned harms such as distrust in AI and institutions, complementary to those defined by experts. Furthermore, we found that perceived impact of not developing use cases significantly predicted participants' judgements of whether AI use cases should be developed, and highlighted lay users' concerns of techno-solutionism. We conclude with a discussion on how frameworks like PARTICIP-AI can further guide democratic AI development and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14791v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, Maarten Sap</dc:creator>
    </item>
    <item>
      <title>Large Language Models (LLMs) as Agents for Augmented Democracy</title>
      <link>https://arxiv.org/abs/2405.03452</link>
      <description>arXiv:2405.03452v3 Announce Type: replace 
Abstract: We explore an augmented democracy system built on off-the-shelf LLMs fine-tuned to augment data on citizen's preferences elicited over policies extracted from the government programs of the two main candidates of Brazil's 2022 presidential election. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, we find that LLMs predict out of sample preferences more accurately than a "bundle rule", which would assume that citizens always vote for the proposals of the candidate aligned with their self-reported political orientation. At the population level, we show that a probabilistic sample augmented by an LLM provides a more accurate estimate of the aggregate preferences of a population than the non-augmented probabilistic sample alone. Together, these results indicates that policy preference data augmented using LLMs can capture nuances that transcend party lines and represents a promising avenue of research for data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03452v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1098/rsta</arxiv:DOI>
      <dc:creator>Jairo Gudi\~no-Rosero, Umberto Grandi, C\'esar A. Hidalgo</dc:creator>
    </item>
    <item>
      <title>CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education</title>
      <link>https://arxiv.org/abs/2407.10246</link>
      <description>arXiv:2407.10246v3 Announce Type: replace 
Abstract: The growing enrollments in computer science courses and increase in class sizes necessitate scalable, automated tutoring solutions to adequately support student learning. While Large Language Models (LLMs) like GPT-4 have demonstrated potential in assisting students through question-answering, educators express concerns over student overreliance, miscomprehension of generated code, and the risk of inaccurate answers. Rather than banning these tools outright, we advocate for a constructive approach that harnesses the capabilities of AI while mitigating potential risks. This poster introduces CourseAssist, a novel LLM-based tutoring system tailored for computer science education. Unlike generic LLM systems, CourseAssist uses retrieval-augmented generation, user intent classification, and question decomposition to align AI responses with specific course materials and learning objectives, thereby ensuring pedagogical appropriateness of LLMs in educational settings. We evaluated CourseAssist against a baseline of GPT-4 using a dataset of 50 question-answer pairs from a programming languages course, focusing on the criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation results show that CourseAssist significantly outperforms the baseline, demonstrating its potential to serve as an effective learning assistant. We have also deployed CourseAssist in 6 computer science courses at a large public R1 research university reaching over 500 students. Interviews with 20 student users show that CourseAssist improves computer science instruction by increasing the accessibility of course-specific tutoring help and shortening the feedback loop on their programming assignments. Future work will include extensive pilot testing at more universities and exploring better collaborative relationships between students, educators, and AI that improve computer science learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10246v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ty Feng, Sa Liu, Dipak Ghosal</dc:creator>
    </item>
    <item>
      <title>Modeling the amplification of epidemic spread by misinformed populations</title>
      <link>https://arxiv.org/abs/2402.11351</link>
      <description>arXiv:2402.11351v3 Announce Type: replace-cross 
Abstract: Understanding how misinformation affects the spread of disease is crucial for public health, especially given recent research indicating that misinformation can increase vaccine hesitancy and discourage vaccine uptake. However, it is difficult to investigate the interaction between misinformation and epidemic outcomes due to the dearth of data-informed holistic epidemic models. Here, we employ an epidemic model that incorporates a large, mobility-informed physical contact network as well as the distribution of misinformed individuals across counties derived from social media data. The model allows us to simulate and estimate various scenarios to understand the impact of misinformation on epidemic spreading. Using this model, we present a worst-case scenario in which a heavily misinformed population would result in an additional 14% of the U.S. population becoming infected over the course of the COVID-19 epidemic, compared to a best-case scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11351v3</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew R. DeVerna, Francesco Pierri, Yong-Yeol Ahn, Santo Fortunato, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>An Abundance of Katherines: The Game Theory of Baby Naming</title>
      <link>https://arxiv.org/abs/2404.00732</link>
      <description>arXiv:2404.00732v3 Announce Type: replace-cross 
Abstract: In this paper, we study the highly competitive arena of baby naming. Through making several Extremely Reasonable Assumptions (namely, that parents are myopic, perfectly knowledgeable agents who pick a name based solely on its uniqueness), we create a model which is not only tractable and clean, but also perfectly captures the real world. We then extend our investigation with numerical experiments, as well as analysis of large language model tools. We conclude by discussing avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00732v3</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katy Blumer, Kate Donahue, Katie Fritz, Kate Ivanovich, Katherine Lee, Katie Luo, Cathy Meng, Katie Van Koevering</dc:creator>
    </item>
    <item>
      <title>Large Language Models Assume People are More Rational than We Really are</title>
      <link>https://arxiv.org/abs/2406.17055</link>
      <description>arXiv:2406.17055v3 Announce Type: replace-cross 
Abstract: In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o &amp; 4-Turbo, Llama-3-8B &amp; 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17055v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression</title>
      <link>https://arxiv.org/abs/2406.19528</link>
      <description>arXiv:2406.19528v3 Announce Type: replace-cross 
Abstract: Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM's video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM's capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19528v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678884.3681850</arxiv:DOI>
      <dc:creator>Jiaying Lizzy Liu, Yunlong Wang, Yao Lyu, Yiheng Su, Shuo Niu, Xuhai Orson Xu, Yan Zhang</dc:creator>
    </item>
  </channel>
</rss>
