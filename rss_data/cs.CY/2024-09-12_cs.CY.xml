<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Navigating Design Science Research in mHealth Applications: A Guide to Best Practices</title>
      <link>https://arxiv.org/abs/2409.07470</link>
      <description>arXiv:2409.07470v1 Announce Type: new 
Abstract: The rapid proliferation of mobile devices and advancements in wireless technologies have given rise to a new era of healthcare delivery through mobile health (mHealth) applications. Design Science Research (DSR) is a widely used research paradigm that aims to create and evaluate innovative artifacts to solve real-world problems. This paper presents a comprehensive framework for employing DSR in mHealth application projects to address healthcare challenges and improve patient outcomes. We discussed various DSR principles and methodologies, highlighting their applicability and importance in developing and evaluating mHealth applications. Furthermore, we present several case studies to exemplify the successful implementation of DSR in mHealth projects and provide practical recommendations for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07470v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TEM.2024.3450178</arxiv:DOI>
      <dc:creator>Avnish Singh Jat, Tor-Morten Gr{\o}nli, George Ghinea</dc:creator>
    </item>
    <item>
      <title>AI, Climate, and Transparency: Operationalizing and Improving the AI Act</title>
      <link>https://arxiv.org/abs/2409.07471</link>
      <description>arXiv:2409.07471v1 Announce Type: new 
Abstract: This paper critically examines the AI Act's provisions on climate-related transparency, highlighting significant gaps and challenges in its implementation. We identify key shortcomings, including the exclusion of energy consumption during AI inference, the lack of coverage for indirect greenhouse gas emissions from AI applications, and the lack of standard reporting methodology. The paper proposes a novel interpretation to bring inference-related energy use back within the Act's scope and advocates for public access to climate-related disclosures to foster market accountability and public scrutiny. Cumulative server level energy reporting is recommended as the most suitable method. We also suggests broader policy changes, including sustainability risk assessments and renewable energy targets, to better address AI's environmental impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07471v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolas Alder, Kai Ebert, Ralf Herbrich, Philipp Hacker</dc:creator>
    </item>
    <item>
      <title>Ethical AI Governance: Methods for Evaluating Trustworthy AI</title>
      <link>https://arxiv.org/abs/2409.07473</link>
      <description>arXiv:2409.07473v1 Announce Type: new 
Abstract: Trustworthy Artificial Intelligence (TAI) integrates ethics that align with human values, looking at their influence on AI behaviour and decision-making. Primarily dependent on self-assessment, TAI evaluation aims to ensure ethical standards and safety in AI development and usage. This paper reviews the current TAI evaluation methods in the literature and offers a classification, contributing to understanding self-assessment methods in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07473v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louise McCormack, Malika Bendechache</dc:creator>
    </item>
    <item>
      <title>Cross-Cultural Communication in the Digital Age: An Analysis of Cultural Representation and Inclusivity in Emojis</title>
      <link>https://arxiv.org/abs/2409.07475</link>
      <description>arXiv:2409.07475v1 Announce Type: new 
Abstract: Emojis have become a universal language in the digital world, enabling users to express emotions, ideas, and identities across diverse cultural contexts. As emojis incorporate more cultural symbols and diverse representations, they play a crucial role in cross-cultural communication. This research project aims to analyze the representation of different cultures in emojis, investigate how emojis facilitate cross-cultural communication and promote inclusivity, and explore the impact of emojis on understanding and interpretation in different cultural contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07475v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lingfeng Li (Southeast University, Nanjing, China), Xiangwen Zheng (Southeast University, Nanjing, China)</dc:creator>
    </item>
    <item>
      <title>Responsible AI for Test Equity and Quality: The Duolingo English Test as a Case Study</title>
      <link>https://arxiv.org/abs/2409.07476</link>
      <description>arXiv:2409.07476v1 Announce Type: new 
Abstract: Artificial intelligence (AI) creates opportunities for assessments, such as efficiencies for item generation and scoring of spoken and written responses. At the same time, it poses risks (such as bias in AI-generated item content). Responsible AI (RAI) practices aim to mitigate risks associated with AI. This chapter addresses the critical role of RAI practices in achieving test quality (appropriateness of test score inferences), and test equity (fairness to all test takers). To illustrate, the chapter presents a case study using the Duolingo English Test (DET), an AI-powered, high-stakes English language assessment. The chapter discusses the DET RAI standards, their development and their relationship to domain-agnostic RAI principles. Further, it provides examples of specific RAI practices, showing how these practices meaningfully address the ethical principles of validity and reliability, fairness, privacy and security, and transparency and accountability standards to ensure test equity and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07476v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jill Burstein, Geoffrey T. LaFlair, Kevin Yancey, Alina A. von Davier, Ravit Dotan</dc:creator>
    </item>
    <item>
      <title>Detection and Classification of Twitter Users' Opinions on Drought Crises in Iran Using Machine Learning Techniques</title>
      <link>https://arxiv.org/abs/2409.07611</link>
      <description>arXiv:2409.07611v1 Announce Type: new 
Abstract: The main objective of this research is to identify and classify the opinions of Persian-speaking Twitter users related to drought crises in Iran and subsequently develop a model for detecting these opinions on the platform. To achieve this, a model has been developed using machine learning and text mining methods to detect the opinions of Persian-speaking Twitter users regarding the drought issues in Iran. The statistical population for the research included 42,028 drought-related tweets posted over a one-year period. These tweets were extracted from Twitter using keywords related to the drought crises in Iran. Subsequently, a sample of 2,300 tweets was qualitatively analyzed, labeled, categorized, and examined. Next, a four-category classification of users` opinions regarding drought crises and Iranians' resilience to these crises was identified. Based on these four categories, a machine learning model based on logistic regression was trained to predict and detect various opinions in Twitter posts. The developed model exhibits an accuracy of 66.09% and an F-score of 60%, indicating that this model has good performance for detecting Iranian Twitter users' opinions regarding drought crises. The ability to detect opinions regarding drought crises on platforms like Twitter using machine learning methods can intelligently represent the resilience level of the Iranian society in the face of these crises, and inform policymakers in this area about changes in public opinion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07611v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Somayeh Labafi, Leila Rabiei, Zeinab Rajabi</dc:creator>
    </item>
    <item>
      <title>Scoping Sustainable Collaborative Mixed Reality</title>
      <link>https://arxiv.org/abs/2409.07640</link>
      <description>arXiv:2409.07640v1 Announce Type: new 
Abstract: Mixed Reality (MR) is becoming ubiquitous as it finds its applications in education, healthcare, and other sectors beyond leisure. While MR end devices, such as headsets, have low energy intensity, the total number of devices and resource requirements of the entire MR ecosystem, which includes cloud and edge endpoints, can be significant. The resulting operational and embodied carbon footprint of MR has led to concerns about its environmental implications. Recent research has explored reducing the carbon footprint of MR devices by exploring hardware design space or network optimizations. However, many additional avenues for enhancing MR's sustainability remain open, including energy savings in non-processor components and carbon-aware optimizations in collaborative MR ecosystems. In this paper, we aim to identify key challenges, existing solutions, and promising research directions for improving MR sustainability. We explore adjacent fields of embedded and mobile computing systems for insights and outline MR-specific problems requiring new solutions. We identify the challenges that must be tackled to enable researchers, developers, and users to avail themselves of these opportunities in collaborative MR systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07640v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasra Chandio, Noman Bashir, Tian Guo, Elsa Olivetti, Fatima Anwar</dc:creator>
    </item>
    <item>
      <title>Mapping Technical Safety Research at AI Companies: A literature review and incentives analysis</title>
      <link>https://arxiv.org/abs/2409.07878</link>
      <description>arXiv:2409.07878v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems become more advanced, concerns about large-scale risks from misuse or accidents have grown. This report analyzes the technical research into safe AI development being conducted by three leading AI companies: Anthropic, Google DeepMind, and OpenAI.
  We define safe AI development as developing AI systems that are unlikely to pose large-scale misuse or accident risks. This encompasses a range of technical approaches aimed at ensuring AI systems behave as intended and do not cause unintended harm, even as they are made more capable and autonomous.
  We analyzed all papers published by the three companies from January 2022 to July 2024 that were relevant to safe AI development, and categorized the 61 included papers into eight safety approaches. Additionally, we noted three categories representing nascent approaches explored by academia and civil society, but not currently represented in any papers by the three companies. Our analysis reveals where corporate attention is concentrated and where potential gaps lie.
  Some AI research may stay unpublished for good reasons, such as to not inform adversaries about security techniques they would need to overcome to misuse AI systems. Therefore, we also considered the incentives that AI companies have to research each approach. In particular, we considered reputational effects, regulatory burdens, and whether the approaches could make AI systems more useful.
  We identified three categories where there are currently no or few papers and where we do not expect AI companies to become more incentivized to pursue this research in the future. These are multi-agent safety, model organisms of misalignment, and safety by design. Our findings provide an indication that these approaches may be slow to progress without funding or efforts from government, civil society, philanthropists, or academia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07878v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Delaney, Oliver Guest, Zoe Williams</dc:creator>
    </item>
    <item>
      <title>From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback</title>
      <link>https://arxiv.org/abs/2409.08027</link>
      <description>arXiv:2409.08027v1 Announce Type: new 
Abstract: Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08027v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja K\"aser</dc:creator>
    </item>
    <item>
      <title>Designing a Collaborative Platform for Advancing Supply Chain Transparency</title>
      <link>https://arxiv.org/abs/2409.08104</link>
      <description>arXiv:2409.08104v1 Announce Type: new 
Abstract: Enabling supply chain transparency (SCT) is essential for regulatory compliance and meeting sustainability standards. Multi-tier SCT plays a pivotal role in identifying and mitigating an organization's operational, environmental, and social (ESG) risks. While research observes increasing efforts towards SCT, a minority of companies are currently publishing supply chain information. Using the Design Science Research approach, we develop a collaborative platform for supply chain transparency. We derive design requirements, formulate design principles, and evaluate the artefact with industry experts. Our artefact is initialized with publicly available supply chain data through an automated pipeline designed to onboard future participants to our platform. This work contributes to SCT research by providing insights into the challenges and opportunities of implementing multi-tier SCT and offers a practical solution that encourages organizations to participate in a transparent ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08104v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Hueller, Tim Kuffner, Matthias Schneider, Leo Schuhmann, Virginie Cauderay, Tolga Buz, Vincent Beermann, Falk Uebernickel</dc:creator>
    </item>
    <item>
      <title>Still More Shades of Null: A Benchmark for Responsible Missing Value Imputation</title>
      <link>https://arxiv.org/abs/2409.07510</link>
      <description>arXiv:2409.07510v1 Announce Type: cross 
Abstract: We present Shades-of-NULL, a benchmark for responsible missing value imputation. Our benchmark includes state-of-the-art imputation techniques, and embeds them into the machine learning development lifecycle. We model realistic missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR), to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test). Another key novelty of our work is that we evaluate imputers holistically, based on the predictive performance, fairness and stability of the models that are trained and tested on the data they produce.
  We use Shades-of-NULL to conduct a large-scale empirical study involving 20,952 experimental pipelines, and find that, while there is no single best-performing imputation approach for all missingness types, interesting performance patterns do emerge when comparing imputer performance in simpler vs. more complex missingness scenarios. Further, while predictive performance, fairness and stability can be seen as orthogonal, we identify trade-offs among them that arise due to the combination of missingness scenario, the choice of an imputer, and the architecture of the model trained on the data post-imputation. We make Shades-of-NULL publicly available, and hope to enable researchers to comprehensively and rigorously evaluate new missing value imputation methods on a wide range of evaluation metrics, in plausible and socially meaningful missingness scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07510v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Falaah Arif Khan, Denys Herasymuk, Nazar Protsiv, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Passed the Turing Test: Living in Turing Futures</title>
      <link>https://arxiv.org/abs/2409.07656</link>
      <description>arXiv:2409.07656v1 Announce Type: cross 
Abstract: The world has seen the emergence of machines based on pretrained models, transformers, also known as generative artificial intelligences for their ability to produce various types of content, including text, images, audio, and synthetic data. Without resorting to preprogramming or special tricks, their intelligence grows as they learn from experience, and to ordinary people, they can appear human-like in conversation. This means that they can pass the Turing test, and that we are now living in one of many possible Turing futures where machines can pass for what they are not. However, the learning machines that Turing imagined would pass his imitation tests were machines inspired by the natural development of the low-energy human cortex. They would be raised like human children and naturally learn the ability to deceive an observer. These ``child machines,'' Turing hoped, would be powerful enough to have an impact on society and nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07656v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernardo Gon\c{c}alves</dc:creator>
    </item>
    <item>
      <title>Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM</title>
      <link>https://arxiv.org/abs/2409.07871</link>
      <description>arXiv:2409.07871v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07871v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eike Schneiders, Tina Seabrooke, Joshua Krook, Richard Hyde, Natalie Leesakul, Jeremie Clos, Joel Fischer</dc:creator>
    </item>
    <item>
      <title>CROSS: A Contributor-Project Interaction Lifecycle Model for Open Source Software</title>
      <link>https://arxiv.org/abs/2409.08267</link>
      <description>arXiv:2409.08267v1 Announce Type: cross 
Abstract: Despite the widespread adoption of open source software (OSS), its sustainability remains a critical concern, particularly in light of security vulnerabilities and the often inadequate end-of-service (EoS) processes for OSS projects as they decline. Existing models of OSS community participation, like the Onion model and the episodic contribution model, offer valuable insights but are fundamentally incompatible and fail to provide a comprehensive picture of contributor engagement with OSS projects. This paper addresses these gaps by proposing the CROSS model, a novel contributor-project interaction lifecycle model for open source, which delineates the various lifecycle stages of contributor-project interaction along with the driving and retaining forces pertinent to each stage. By synthesizing existing research on OSS communities, organizational behavior, and human resource development, it explains a range of archetypal cases of contributor engagement and highlights research gaps, especially in EoS/offboarding scenarios. The CROSS model provides a foundation for understanding and enhancing the sustainability of OSS projects, offering a robust foundation for future research and practical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08267v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tapajit Dey, Brian Fitzgerald, Sherae Daniel</dc:creator>
    </item>
    <item>
      <title>Extended p-median problems for balancing service efficiency and equality</title>
      <link>https://arxiv.org/abs/2312.14408</link>
      <description>arXiv:2312.14408v3 Announce Type: replace 
Abstract: This article deals with the location problem for balancing the service efficiency and equality. In public service systems, some individuals may experience envy if they have to travel longer distances to access services compared to others. This envy can be simplified by comparing an individual's travel distance to a service facility against a threshold distance. Four extended p-median problems are proposed, utilizing the total travel distance and total envy to balance service efficiency and spatial equality. The new objective function is designed to be inequity-averse and exhibits several analytical properties that pertain to both service efficiency and equality. The extended problems were extensively tested on two sets of benchmark instances and one set of geographical instances. The experimentation shows that the equality measures, such as the standard deviation, mean absolute deviation, and Gini coefficient between travel distances, can be substantially improved by slightly increasing the travel distance. Additionally, the advantages of the proposed problems were validated through Pareto optimality analysis and comparisons with other location problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14408v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunfeng Kong, Chenchen Lian, Guangli Zhang, Shiyan Zhai</dc:creator>
    </item>
    <item>
      <title>Understanding the concerns and choices of public when using large language models for healthcare</title>
      <link>https://arxiv.org/abs/2401.09090</link>
      <description>arXiv:2401.09090v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown their potential in biomedical fields. However, how the public uses them for healthcare purposes such as medical Q\&amp;A, self-diagnosis, and daily healthcare information seeking is under-investigated. This paper adopts a mixed-methods approach, including surveys (N=214) and interviews (N=17) to investigate how and why the public uses LLMs for healthcare. We found that participants generally believed LLMs as a healthcare tool have gained popularity, and are often used in combination with other information channels such as search engines and online health communities to optimize information quality. Based on the findings, we reflect on the ethical and effective use of LLMs for healthcare and propose future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09090v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunpeng Xiao, Kyrie Zhixuan Zhou, Yueqing Liang, Kai Shu</dc:creator>
    </item>
    <item>
      <title>Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for Business Process Management</title>
      <link>https://arxiv.org/abs/2408.11386</link>
      <description>arXiv:2408.11386v2 Announce Type: replace 
Abstract: To promote sustainable business practices, and to achieve climate neutrality by 2050, the EU has developed the taxonomy of sustainable activities, which describes when exactly business practices can be considered sustainable. While the taxonomy has only been recently established, progressively more companies will have to report how much of their revenue was created via sustainably executed business processes. To help companies prepare to assess whether their business processes comply with the constraints outlined in the taxonomy, we investigate in how far these criteria can be used for conformance checking, that is, assessing in a data-driven manner, whether business process executions adhere to regulatory constraints. For this, we develop a few-shot learning pipeline to characterize the constraints of the taxonomy with the help of an LLM as to the process dimensions they relate to. We find that many constraints of the taxonomy are useable for conformance checking, particularly in the sectors of energy, manufacturing, and transport. This will aid companies in preparing to monitor regulatory compliance with the taxonomy automatically, by characterizing what kind of information they need to extract, and by providing a better understanding of sectors where such an assessment is feasible and where it is not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11386v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Klessascheck, Stephan A. Fahrenkrog-Petersen, Jan Mendling, Luise Pufahl</dc:creator>
    </item>
    <item>
      <title>adF: A Novel System for Measuring Web Fingerprinting through Ads</title>
      <link>https://arxiv.org/abs/2311.08769</link>
      <description>arXiv:2311.08769v2 Announce Type: replace-cross 
Abstract: This paper introduces adF, a novel system for analyzing the vulnerability of different devices, Operating Systems (OSes), and browsers to web fingerprinting. adF performs its measurements from code inserted in ads. We have used our system in several ad campaigns that delivered 5.40 million ad impressions. The collected data allow us to assess the vulnerability of current desktop and mobile devices to web fingerprinting. Based on our results, we estimate that 66% of desktop devices and 40% of mobile devices can be uniquely fingerprinted with our web fingerprinting system. However, the resilience to web fingerprinting varies significantly across browsers and device types, with Chrome on desktops being the most vulnerable configuration.
  To counter web fingerprinting, we propose ShieldF, a simple solution which blocks the reporting by browsers of those attributes that we found in the analysis of our dataset that present the most significant discrimination power. Our experiments reveal that ShieldF outperforms all anti-fingerprinting solutions proposed by major browsers (Chrome, Safari and Firefox) offering an increase in the resilience offered to web fingerprinting up to 62% for some device configurations. ShieldF is available as an add-on for any chromium-based browser. Moreover, it is readily adoptable by browser and mobile app developers. Its widespread use would lead to a significant improvement in the protection offered by browsers and mobile apps to web fingerprinting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08769v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel A. Bermejo-Agueda (Universidad Carlos III de Madrid, uc3m-Santander Big Data Institute), Patricia Callejo (Universidad Carlos III de Madrid, uc3m-Santander Big Data Institute), Rub\'en Cuevas (Universidad Carlos III de Madrid, uc3m-Santander Big Data Institute), \'Angel Cuevas (Universidad Carlos III de Madrid, uc3m-Santander Big Data Institute)</dc:creator>
    </item>
    <item>
      <title>J\"ager: Automated Telephone Call Traceback</title>
      <link>https://arxiv.org/abs/2409.02839</link>
      <description>arXiv:2409.02839v3 Announce Type: replace-cross 
Abstract: Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02839v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658644.3690290</arxiv:DOI>
      <dc:creator>David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro</dc:creator>
    </item>
  </channel>
</rss>
