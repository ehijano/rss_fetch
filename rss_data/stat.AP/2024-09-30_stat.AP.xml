<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 03:26:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Soil organic carbon sequestration potential and policy optimization</title>
      <link>https://arxiv.org/abs/2409.18198</link>
      <description>arXiv:2409.18198v1 Announce Type: new 
Abstract: Land management could help mitigate climate change by sequestering atmospheric carbon dioxide as soil organic carbon (SOC). The impact of a given management change on the SOC content of a given volume of soil is generally unknown, but is likely moderated by features of the land that collectively determine its sequestration potential. To maximize sequestration, management interventions should be preferentially applied to fields with the highest sequestration potential and the lowest cost of application. We present a design-based statistical framework for estimating sequestration potential, average treatment effects, and optimal management policies from a randomized experiment with baseline covariate information. We review the myriad and nested sources of uncertainty that arise in this context and formalize the problem using potential outcomes. We show that a particular regression estimator -- regressing field-level SOC on management indicators and their interactions with covariates -- can help identify effective policies. The regression estimator also gives asymptotically valid inference on average treatment effects under the randomized design -- without modeling assumptions -- and can increase precision and power compared to the difference-in-means $T$-test. We conclude by discussing the saturation hypothesis in relation to sequestration potential, other study designs including observational studies of SOC, models for policy costs, nonparametric inference, and broader policy uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18198v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob Spertus, Philip Stark, Whendee Silver, Eric Slessarev</dc:creator>
    </item>
    <item>
      <title>Algorithmic Forecasting of Extreme Heat Waves</title>
      <link>https://arxiv.org/abs/2409.18305</link>
      <description>arXiv:2409.18305v1 Announce Type: new 
Abstract: This paper provides forecasts of rare and extreme heat waves coupled with estimates of uncertainty. Both rest in part on gaining a better understanding of the similarities and differences between hot days under normal circumstances and rare, extreme heat waves. We analyze AIRS data from the American Pacific Northwest and AIRS data from the Phoenix, Arizona region. Forecasting accuracy is excellent for the Pacific Northwest and is replicated for Phoenix. Uncertainty is estimated with nested conformal prediction sets, provably valid even for finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18305v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard A Berk, Amy Braverman</dc:creator>
    </item>
    <item>
      <title>Predicting failure times of coherent systems</title>
      <link>https://arxiv.org/abs/2409.18625</link>
      <description>arXiv:2409.18625v1 Announce Type: new 
Abstract: The article is focused on studying how to predict the failure times of coherent systems from the early failure times of their components. Both the cases of independent and dependent components are considered by assuming that they are identically distributed (homogeneous components). The heterogeneous components' case can be addressed similarly but more complexly. The present study is for non-repairable systems, but the information obtained could be used to decide if a maintenance action should be carried out at time t. Different cases are considered regarding the information available at time t. We use quantile regression techniques to predict the system failure times and to provide prediction intervals. The theoretical results are applied to specific system structures in some illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18625v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/NAV.22169</arxiv:DOI>
      <arxiv:journal_reference>Naval Research Logistics 2024 Vol. 71 n 5 pp. 627-644</arxiv:journal_reference>
      <dc:creator>Jorge Navarro, Antonio Arriaza, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>A variance-based importance index for systems with dependent components</title>
      <link>https://arxiv.org/abs/2409.18669</link>
      <description>arXiv:2409.18669v1 Announce Type: new 
Abstract: This paper proposes a variance-based measure of importance for coherent systems with dependent and heterogeneous components. The particular cases of independent components and homogeneous components are also considered. We model the dependence structure among the components by the concept of copula. The proposed measure allows us to provide the best estimation of the system lifetime, in terms of the mean squared error, under the assumption that the lifetime of one of its components is known. We include theoretical results that are useful to calculate a closed-form of our measure and to compare two components of a system. We also provide some procedures to approximate the importance measure by Monte Carlo simulation methods. Finally, we illustrate the main results with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18669v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/J.FSS.2023.02.003</arxiv:DOI>
      <arxiv:journal_reference>Fuzzy Sets and Systems 2023 Vol. 467</arxiv:journal_reference>
      <dc:creator>Antonio Arriaza, Jorge Navarro, Miguel Angel Sordo, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>An interpretable and transferable model for shallow landslides detachment combining spatial Poisson point processes and generalized additive models</title>
      <link>https://arxiv.org/abs/2409.18672</link>
      <description>arXiv:2409.18672v1 Announce Type: new 
Abstract: Less than 10 meters deep, shallow landslides are rapidly moving and strongly dangerous slides. In the present work, the probabilistic distribution of the landslide detachment points within a valley is modelled as a spatial Poisson point process, whose intensity depends on geophysical predictors according to a generalized additive model. Modelling the intensity with a generalized additive model jointly allows to obtain good predictive performance and to preserve the interpretability of the effects of the geophysical predictors on the intensity of the process. We propose a novel workflow, based on Random Forests, to select the geophysical predictors entering the model for the intensity. In this context, the statistically significant effects are interpreted as activating or stabilizing factors for landslide detachment. In order to guarantee the transferability of the resulting model, training, validation, and test of the algorithm are performed on mutually disjoint valleys in the Alps of Lombardy (Italy). Finally, the uncertainty around the estimated intensity of the process is quantified via semiparametric bootstrap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18672v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Patan\`e, Teresa Bortolotti, Vasil Yordanov, Ludovico Giorgio Aldo Biagi, Maria Antonia Brovelli, Xuan Quang Truong, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Preservation of some stochastic orders by distortion functions with application to coherent systems with exchangeable components</title>
      <link>https://arxiv.org/abs/2409.18684</link>
      <description>arXiv:2409.18684v1 Announce Type: new 
Abstract: The preservation of stochastic orders by distortion functions has become a topic of increasing interest in the reliability analysis of coherent systems. The reason of this interest is that the reliability function of a coherent system with identically distributed components can be represented as a distortion function of the common reliability function of the components. In this framework, we study the preservation of the excess wealth order, the total time on test transform order, the decreasing mean residual live order, and the quantile mean inactivity time order by distortion functions. The results are applied to study the preservation of these stochastic orders under the formation of coherent systems with exchangeable components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18684v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/ASMB.2565</arxiv:DOI>
      <arxiv:journal_reference>Applied Stochastic Models in Business and Industry 2021 Vol. 37 n 2 pp. 303-317</arxiv:journal_reference>
      <dc:creator>Antonio Arriaza, Miguel Angel Sordo</dc:creator>
    </item>
    <item>
      <title>Incorporation of NAMs dynamic reservoir model into Cox rate-and-state model for monitoring earthquakes in the Groningen gas field</title>
      <link>https://arxiv.org/abs/2409.18837</link>
      <description>arXiv:2409.18837v1 Announce Type: new 
Abstract: Induced seismicity due to fluid extraction or injection has become a critical issue in regions with extensive hydrocarbon production, such as the Groningen gas field in the Netherlands. This study examines the relationship between pore pressure changes from natural gas extraction and the resulting induced earthquakes in Groningen. We employ a Cox process-based rate-state model, integrating pore pressure data from NAM's dynamic reservoir model. By combining geomechanical and statistical approaches, we aim to predict future seismic events and assess the accuracy of our model. Our methodology uses Markov Chain Monte Carlo (MCMC) algorithms to estimate model parameters and forecast earthquake occurrences. The results highlight the significant impact of pressure changes on seismic activity, providing valuable insights for mitigating seismic risks in gas-producing regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18837v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuldyzay Baki</dc:creator>
    </item>
    <item>
      <title>Enhancing Characterization of Near-Surface Velocity Structure(s) in the San Francisco Bay Area: A Stationary and Spatially Varying Approach</title>
      <link>https://arxiv.org/abs/2409.18856</link>
      <description>arXiv:2409.18856v1 Announce Type: new 
Abstract: This study presents the development of two new sedimentary velocity models for the San Francisco Bay Area (SFBA) to improve the near-surface characterization of shear-wave velocity ($V_S$), with the ultimate goal of enhancing the Bay Area community velocity model. A stationary model, based solely on $V_{S30}$, and a spatially varying model incorporating location-specific adjustments were developed using a dataset of 200 measured $V_S$ profiles. Both models were formulated within a hierarchical Bayesian framework, using a parameterization that ensures robust scaling. The spatially varying model includes a slope adjustment term modeled as a Gaussian process to capture site-specific effects based on location. Residual analysis shows that both models are unbiased up to $V_S$ values of 1000 m/sec. Along-depth variability models were also developed using within-profile residuals. Applying the proposed models in the SFBA results in an increase in $V_S$ in the San Jose area and east of the Hayward Fault, supporting simulations that suggest over-amplification in these regions compared to observations. Goodness-of-fit (GOF) comparisons using linear site-response analyses demonstrate that the proposed models outperform the USGS model in capturing near-surface amplification across a broad frequency range. Incorporating along-depth variability further improves GOF scores by reducing over-amplification at high frequencies. These results underscore the importance of integrating community velocity models with detailed sedimentary velocity models to enhance regional seismic hazard assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18856v1</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grigorios Lavrentiadis, Elnaz Seylabi, Feiruo Xia, Hesam Tehrani, Domniki Asimaki, David McCallen</dc:creator>
    </item>
    <item>
      <title>Robust optimization and uncertainty quantification in the nonlinear mechanics of an elevator brake system</title>
      <link>https://arxiv.org/abs/2409.18139</link>
      <description>arXiv:2409.18139v1 Announce Type: cross 
Abstract: This paper deals with nonlinear mechanics of an elevator brake system subjected to uncertainties. A deterministic model that relates the braking force with uncertain parameters is deduced from mechanical equilibrium conditions. In order to take into account parameters variabilities, a parametric probabilistic approach is employed. In this stochastic formalism, the uncertain parameters are modeled as random variables, with distributions specified by the maximum entropy principle. The uncertainties are propagated by the Monte Carlo method, which provides a detailed statistical characterization of the response. This work still considers the optimum design of the brake system, formulating and solving nonlinear optimization problems, with and without the uncertainties effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18139v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>physics.class-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11012-019-00992-7</arxiv:DOI>
      <arxiv:journal_reference>Meccanica, vol. 54, pp. 1057-1069, 2019</arxiv:journal_reference>
      <dc:creator>Piotr Wolszczak, Pawel Lonkwic, Americo Cunha Jr, Grzegorz Litak, Szymon Molski</dc:creator>
    </item>
    <item>
      <title>Bayesian Event Categorization Matrix Approach for Nuclear Detonations</title>
      <link>https://arxiv.org/abs/2409.18227</link>
      <description>arXiv:2409.18227v1 Announce Type: cross 
Abstract: Current efforts to detect nuclear detonations and correctly categorize explosion sources with ground- and space-collected discriminants presents challenges that remain unaddressed by the Event Categorization Matrix (ECM) model. Smaller events (lower yield explosions) often include only sparse observations among few modalities and can therefore lack a complete set of discriminants. The covariance structures can also vary significantly between such observations of event (source-type) categories. Both obstacles are problematic for ``classic'' ECM. Our work addresses this gap and presents a Bayesian update to the previous ECM model, termed B-ECM, which can be trained on partial observations and does not rely on a pooled covariance structure. We further augment ECM with Bayesian Decision Theory so that false negative or false positive rates of an event categorization can be reduced in an intuitive manner. To demonstrate improved categorization rates with B-ECM, we compare an array of B-ECM and classic ECM models with multiple performance metrics that leverage Monte Carlo experiments. We use both synthetic and real data. Our B-ECM models show consistent gains in overall accuracy and a lower false negative rates relative to the classic ECM model. We propose future avenues to improve B-ECM that expand its decision-making and predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18227v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Koermer, Joshua D. Carmichael, Brian J. Williams</dc:creator>
    </item>
    <item>
      <title>Measuring Research Interest Similarity with Transition Probabilities</title>
      <link>https://arxiv.org/abs/2409.18240</link>
      <description>arXiv:2409.18240v1 Announce Type: cross 
Abstract: We propose a method to measure the similarity of papers and authors by simulating a literature search procedure on citation networks, which is an information retrieval inspired conceptualization of similarity. This transition probability (TP) based approach does not require a curated classification system, avoids clustering complications, and provides a continuous measure of similarity. We perform testing scenarios to explore several versions of the general TP concept and the Node2vec machine-learning technique. We found that TP measures outperform Node2vec in mapping the macroscopic structure of fields. The paper provides a general discussion of how to implement TP similarity measurement, with a particular focus on how to utilize publication-level information to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package capable of calculating all the tested metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18240v1</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Attila Varga, Sadamori Kojaku, Filipi Nascimento Silva</dc:creator>
    </item>
    <item>
      <title>Which depth to use to construct functional boxplots?</title>
      <link>https://arxiv.org/abs/2409.18603</link>
      <description>arXiv:2409.18603v1 Announce Type: cross 
Abstract: This paper answers the question of which functional depth to use to construct a boxplot for functional data. It shows that integrated depths, e.g., the popular modified band depth, do not result in well-defined boxplots. Instead, we argue that infimal depths are the only functional depths that provide a valid construction of a functional boxplot. We also show that the properties of the boxplot are completely determined by properties of the one-dimensional depth function used in defining the infimal depth for functional data. Our claims are supported by (i) a motivating example, (ii) theoretical results concerning the properties of the boxplot, and (iii) a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18603v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav Nagy, Tom\'a\v{s} Mrkvi\v{c}ka, Antonio El\'ias</dc:creator>
    </item>
    <item>
      <title>Tail Risk Analysis for Financial Time Series</title>
      <link>https://arxiv.org/abs/2409.18643</link>
      <description>arXiv:2409.18643v1 Announce Type: cross 
Abstract: This book chapter illustrates how to apply extreme value statistics to financial time series data. Such data often exhibits strong serial dependence, which complicates assessment of tail risks. We discuss the two main approches to tail risk estimation, unconditional and conditional quantile forecasting. We use the S&amp;P 500 index as a case study to assess serial (extremal) dependence, perform an unconditional and conditional risk analysis, and apply backtesting methods. Additionally, the chapter explores the impact of serial dependence on multivariate tail dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18643v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Kiriliouk, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Simplifying Random Forests' Probabilistic Forecasts</title>
      <link>https://arxiv.org/abs/2408.12332</link>
      <description>arXiv:2408.12332v2 Announce Type: replace 
Abstract: Since their introduction by Breiman, Random Forests (RFs) have proven to be useful for both classification and regression tasks. The RF prediction of a previously unseen observation can be represented as a weighted sum of all training sample observations. This nearest-neighbor-type representation is useful, among other things, for constructing forecast distributions (Meinshausen, 2006). In this paper, we consider simplifying RF-based forecast distributions by sparsifying them. That is, we focus on a small subset of nearest neighbors while setting the remaining weights to zero. This sparsification step greatly improves the interpretability of RF predictions. It can be applied to any forecasting task without re-training existing RF models. In empirical experiments, we document that the simplified predictions can be similar to or exceed the original ones in terms of forecasting performance. We explore the statistical sources of this finding via a stylized analytical model of RFs. The model suggests that simplification is particularly promising if the unknown true forecast distribution contains many small weights that are estimated imprecisely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12332v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Koster, Fabian Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Can-SAVE: Mass Cancer Risk Prediction via Survival Analysis Variables and EHR</title>
      <link>https://arxiv.org/abs/2309.15039</link>
      <description>arXiv:2309.15039v2 Announce Type: replace-cross 
Abstract: Specific medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects prevent the mass implementation of cancer screening methods. For this reason, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume. This paper presents a novel Can-SAVE cancer risk assessment method combining a survival analysis approach with a gradient-boosting algorithm. It is highly accessible and resource-efficient, utilizing only a sequence of high-level medical events. We tested the proposed method in a long-term retrospective experiment covering more than 1.1 million people and four regions of Russia. The Can-SAVE method significantly exceeds the baselines by the Average Precision metric of 22.8%$\pm$2.7% vs 15.1%$\pm$2.6%. The extensive ablation study also confirmed the proposed method's dominant performance. The experiment supervised by oncologists shows a reliable cancer patient detection rate of up to 84 out of 1000 selected. Such results surpass the medical screening strategies estimates; the typical age-specific Number Needed to Screen is only 9 out of 1000 (for colorectal cancer). Overall, our experiments show a 4.7-6.4 times improvement in cancer detection rate (TOP@1k) compared to the traditional healthcare risk estimation approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15039v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Philonenko, Vladimir Kokh, Pavel Blinov</dc:creator>
    </item>
    <item>
      <title>Manipulating a Continuous Instrumental Variable in an Observational Study of Premature Babies: Algorithm, Partial Identification Bounds, and Inference under Randomization and Biased Randomization Assumptions</title>
      <link>https://arxiv.org/abs/2404.17734</link>
      <description>arXiv:2404.17734v2 Announce Type: replace-cross 
Abstract: Regionalization of intensive care for premature babies refers to a triage system of mothers with high-risk pregnancies to hospitals of varied capabilities based on risks faced by infants. Due to the limited capacity of high-level hospitals, which are equipped with advanced expertise to provide critical care, understanding the effect of delivering premature babies at such hospitals on infant mortality for different subgroups of high-risk mothers could facilitate the design of an efficient perinatal regionalization system. Towards answering this question, Baiocchi et al. (2010) proposed to strengthen an excess-travel-time-based, continuous instrumental variable (IV) in an IV-based, matched-pair design by switching focus to a smaller cohort amenable to being paired with a larger separation in the IV dose. Three elements changed with the strengthened IV: the study cohort, compliance rate and latent complier subgroup. Here, we introduce a non-bipartite, template matching algorithm that embeds data into a target, pair-randomized encouragement trial which maintains fidelity to the original study cohort while strengthening the IV. We then study randomization-based and IV-dependent, biased-randomization-based inference of partial identification bounds for the sample average treatment effect (SATE) in an IV-based matched pair design, which deviates from the usual effect ratio estimand in that the SATE is agnostic to the IV and who is matched to whom, although a strengthened IV design could narrow the partial identification bounds. Based on our proposed strengthened-IV design, we found that delivering at a high-level NICU reduced preterm babies' mortality rate compared to a low-level NICU for $81,766 \times 2 = 163,532$ mothers and their preterm babies and the effect appeared to be minimal among non-black, low-risk mothers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17734v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Min Haeng Cho, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Optical ISAC: Fundamental Performance Limits and Transceiver Design</title>
      <link>https://arxiv.org/abs/2408.11792</link>
      <description>arXiv:2408.11792v4 Announce Type: replace-cross 
Abstract: This paper characterizes the optimal Capacity-Distortion (C-D) tradeoff in an optical point-to-point system with Single-Input Single-Output (SISO) for communication and Single-Input Multiple-Output (SIMO) for sensing within an Integrated Sensing and Communication (ISAC) framework. We consider the optimal Rate-Distortion (R-D) region and explore several Inner (IB) and Outer Bounds (OB). We introduce practical, asymptotically optimal Maximum A Posteriori (MAP) and Maximum Likelihood Estimators (MLE) for target distance, addressing nonlinear measurement-to-state relationships and non-conjugate priors. As the number of sensing antennas increases, these estimators converge to the Bayesian Cram\'er-Rao Bound (BCRB). We also establish that the achievable Rate-Cram\'er-Rao Bound (R-CRB) serves as an OB for the optimal C-D region, valid for both unbiased estimators and asymptotically large numbers of receive antennas. To clarify that the input distribution determines the tradeoff across the Pareto boundary of the C-D region, we propose two algorithms: i) an iterative Blahut-Arimoto Algorithm (BAA)-type method, and ii) a memory-efficient Closed-Form (CF) approach. The CF approach includes a CF optimal distribution for high Optical Signal-to-Noise Ratio (O-SNR) conditions. Additionally, we adapt and refine the Deterministic-Random Tradeoff (DRT) to this optical ISAC context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11792v4</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Ghazavi Khorasgani (5/6GIC, Institute for Communication Systems), Mahtab Mirmohseni (5/6GIC, Institute for Communication Systems), Ahmed Elzanaty (5/6GIC, Institute for Communication Systems)</dc:creator>
    </item>
    <item>
      <title>Reservoir Static Property Estimation Using Nearest-Neighbor Neural Network</title>
      <link>https://arxiv.org/abs/2409.15295</link>
      <description>arXiv:2409.15295v2 Announce Type: replace-cross 
Abstract: This note presents an approach for estimating the spatial distribution of static properties in reservoir modeling using a nearest-neighbor neural network. The method leverages the strengths of neural networks in approximating complex, non-linear functions, particularly for tasks involving spatial interpolation. It incorporates a nearest-neighbor algorithm to capture local spatial relationships between data points and introduces randomization to quantify the uncertainty inherent in the interpolation process. This approach addresses the limitations of traditional geostatistical methods, such as Inverse Distance Weighting (IDW) and Kriging, which often fail to model the complex non-linear dependencies in reservoir data. By integrating spatial proximity and uncertainty quantification, the proposed method can improve the accuracy of static property predictions like porosity and permeability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15295v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhe Wang</dc:creator>
    </item>
  </channel>
</rss>
