<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 02:34:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Risk thresholds for frontier AI</title>
      <link>https://arxiv.org/abs/2406.14713</link>
      <description>arXiv:2406.14713v1 Announce Type: new 
Abstract: Frontier artificial intelligence (AI) systems could pose increasing risks to public safety and security. But what level of risk is acceptable? One increasingly popular approach is to define capability thresholds, which describe AI capabilities beyond which an AI system is deemed to pose too much risk. A more direct approach is to define risk thresholds that simply state how much risk would be too much. For instance, they might state that the likelihood of cybercriminals using an AI system to cause X amount of economic damage must not increase by more than Y percentage points. The main upside of risk thresholds is that they are more principled than capability thresholds, but the main downside is that they are more difficult to evaluate reliably. For this reason, we currently recommend that companies (1) define risk thresholds to provide a principled foundation for their decision-making, (2) use these risk thresholds to help set capability thresholds, and then (3) primarily rely on capability thresholds to make their decisions. Regulators should also explore the area because, ultimately, they are the most legitimate actors to define risk thresholds. If AI risk estimates become more reliable, risk thresholds should arguably play an increasingly direct role in decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14713v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie Koessler, Jonas Schuett, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>Network visualization techniques for story charting</title>
      <link>https://arxiv.org/abs/2406.14734</link>
      <description>arXiv:2406.14734v1 Announce Type: new 
Abstract: Visualization techniques have been widely used to analyze various data types, including text. This paper proposes an approach to analyze a controversial text in Portuguese by applying graph visualization techniques. Specifically, we use a story charting technique that transforms the text into a graph. Each node represents a character or main entities, and each edge represents the interactions between characters. We also present several visualization techniques to gain insights into the story's structure, relationships between the characters, the most important events, and how some key terms are used throughout the book. By using this approach, we can effectively reveal complex patterns and relationships that may not be easily discernible from reading the text. Finally, we discuss the potential applications of our technique in Literary Studies and other fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14734v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joao T. Aparicio, Andreas Karatsoli, Carlos J. Costa</dc:creator>
    </item>
    <item>
      <title>I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education</title>
      <link>https://arxiv.org/abs/2406.14871</link>
      <description>arXiv:2406.14871v1 Announce Type: new 
Abstract: Trust plays a pivotal role in Lecturer-Student-Collaboration, encompassing teaching and research aspects. The advent of Large Language Models (LLMs) in platforms like Open AI's ChatGPT, coupled with their cost-effectiveness and high-quality results, has led to their rapid adoption among university students. However, discerning genuine student input from LLM-generated output poses a challenge for lecturers. This dilemma jeopardizes the trust relationship between lecturers and students, potentially impacting university downstream activities, particularly collaborative research initiatives. Despite attempts to establish guidelines for student LLM use, a clear framework mutually beneficial for lecturers and students in higher education remains elusive. This study addresses the research question: How does the use of LLMs by students impact Informational and Procedural Justice, influencing Team Trust and Expected Team Performance? Methodically, we applied a quantitative construct-based survey, evaluated using techniques of Structural Equation Modelling (PLS- SEM) to examine potential relationships among these constructs. Our findings based on 23 valid respondents from Ndejje University indicate that lecturers are less concerned about the fairness of LLM use per se but are more focused on the transparency of student utilization, which significantly influences Team Trust positively. This research contributes to the global discourse on integrating and regulating LLMs and subsequent models in education. We propose that guidelines should support LLM use while enforcing transparency in Lecturer-Student- Collaboration to foster Team Trust and Performance. The study contributes valuable insights for shaping policies enabling ethical and transparent LLMs usage in education to ensure effectiveness of collaborative learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14871v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Kloker, Matthew Bazanya, Twaha Kateete</dc:creator>
    </item>
    <item>
      <title>Beyond Accidents and Misuse: Decoding the Structural Risk Dynamics of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2406.14873</link>
      <description>arXiv:2406.14873v1 Announce Type: new 
Abstract: The integration of artificial intelligence (AI) across contemporary industries is not just a technological upgrade but a transformation with profound structural implications. This paper explores the concept of structural risks associated with the rapid integration of advanced AI systems across social, economic, and political systems. This framework challenges the conventional perspectives that primarily focus on direct AI threats such as accidents and misuse and suggests that these more proximate risks are interconnected and influenced by a larger sociotechnical system. By analyzing the interactions between technological advancements and social dynamics, this study isolates three primary categories of structural risk: antecedent structural causes, antecedent system causes, and deleterious feedback loops. We present a comprehensive framework to understand the causal chains that drive these risks, highlighting the interdependence between structural forces and the more proximate risks of misuse and system failures. The paper articulates how unchecked AI advancement can reshape power dynamics, trust, and incentive structures, leading to profound and often unpredictable shifts. We introduce a methodological research agenda for mapping, simulating, and gaming these dynamics aimed at preparing policymakers and national security officials for the challenges posed by next-generation AI technologies. The paper concludes with policy recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14873v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kyle A Kilian</dc:creator>
    </item>
    <item>
      <title>AIGC-Chain: A Blockchain-Enabled Full Lifecycle Recording System for AIGC Product Copyright Management</title>
      <link>https://arxiv.org/abs/2406.14966</link>
      <description>arXiv:2406.14966v1 Announce Type: new 
Abstract: As artificial intelligence technology becomes increasingly prevalent, Artificial Intelligence Generated Content (AIGC) is being adopted across various sectors. Although AIGC is playing an increasingly significant role in business and culture, questions surrounding its copyright have sparked widespread debate. The current legal framework for copyright and intellectual property is grounded in the concept of human authorship, but in the creation of AIGC, human creators primarily provide conceptual ideas, with AI independently responsible for the expressive elements. This disconnect creates complexity and difficulty in determining copyright ownership under existing laws. Consequently, it is imperative to reassess the intellectual contributions of all parties involved in the creation of AIGC to ensure a fair allocation of copyright ownership. To address this challenge, we introduce AIGC-Chain, a blockchain-enabled full lifecycle recording system designed to manage the copyright of AIGC products. It is engineered to meticulously document the entire lifecycle of AIGC products, providing a transparent and dependable platform for copyright management. Furthermore, we propose a copyright tracing method based on an Indistinguishable Bloom Filter, named IBFT, which enhances the efficiency of blockchain transaction queries and significantly reduces the risk of fraudulent copyright claims for AIGC products. In this way, auditors can analyze the copyright of AIGC products by reviewing all relevant information retrieved from the blockchain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14966v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajia Jiang, Moting Su, Xiangli Xiao, Yushu Zhang, Yuming Fang</dc:creator>
    </item>
    <item>
      <title>A Highly Granular Temporary Migration Dataset Derived From Mobile Phone Data in Senegal</title>
      <link>https://arxiv.org/abs/2406.15216</link>
      <description>arXiv:2406.15216v1 Announce Type: new 
Abstract: Understanding temporary migration is crucial for addressing various socio-economic and environmental challenges in developing countries. However, traditional surveys often fail to capture such movements effectively, leading to a scarcity of reliable data, particularly in sub-Saharan Africa. This article introduces a detailed and open-access dataset that leverages mobile phone data to capture temporary migration in Senegal with unprecedented spatio-temporal detail. The dataset provides measures of migration flows and stock across 151 locations across the country and for each half-month period from 2013 to 2015, with a specific focus on movements lasting between 20 and 180 days. The article presents a suite of methodological tools that not only include algorithmic methods for the detection of temporary migration events in digital traces, but also addresses key challenges in aggregating individual trajectories into coherent migration statistics. These methodological advancements are not only pivotal for the intrinsic value of the dataset but also adaptable for generating systematic migration statistics from other digital trace datasets in other contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15216v1</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Blanchard, Stefania Rubrichi</dc:creator>
    </item>
    <item>
      <title>ChatGPT as Research Scientist: Probing GPT's Capabilities as a Research Librarian, Research Ethicist, Data Generator and Data Predictor</title>
      <link>https://arxiv.org/abs/2406.14765</link>
      <description>arXiv:2406.14765v1 Announce Type: cross 
Abstract: How good a research scientist is ChatGPT? We systematically probed the capabilities of GPT-3.5 and GPT-4 across four central components of the scientific process: as a Research Librarian, Research Ethicist, Data Generator, and Novel Data Predictor, using psychological science as a testing field. In Study 1 (Research Librarian), unlike human researchers, GPT-3.5 and GPT-4 hallucinated, authoritatively generating fictional references 36.0% and 5.4% of the time, respectively, although GPT-4 exhibited an evolving capacity to acknowledge its fictions. In Study 2 (Research Ethicist), GPT-4 (though not GPT-3.5) proved capable of detecting violations like p-hacking in fictional research protocols, correcting 88.6% of blatantly presented issues, and 72.6% of subtly presented issues. In Study 3 (Data Generator), both models consistently replicated patterns of cultural bias previously discovered in large language corpora, indicating that ChatGPT can simulate known results, an antecedent to usefulness for both data generation and skills like hypothesis generation. Contrastingly, in Study 4 (Novel Data Predictor), neither model was successful at predicting new results absent in their training data, and neither appeared to leverage substantially new information when predicting more versus less novel outcomes. Together, these results suggest that GPT is a flawed but rapidly improving librarian, a decent research ethicist already, capable of data generation in simple domains with known characteristics but poor at predicting novel patterns of empirical data to aid future experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14765v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, Mahzarin R. Banaji</dc:creator>
    </item>
    <item>
      <title>Securing the Future: Proactive Threat Hunting for Sustainable IoT Ecosystems</title>
      <link>https://arxiv.org/abs/2406.14804</link>
      <description>arXiv:2406.14804v1 Announce Type: cross 
Abstract: In the rapidly evolving landscape of the IoT, the security of connected devices has become a paramount concern. This paper explores the concept of proactive threat hunting as a pivotal strategy for enhancing the security and sustainability of IoT systems. Proactive threat hunting is an alternative to traditional reactive security measures that analyses IoT networks continuously and in advance to find and eliminate threats before they occure. By improving the security posture of IoT devices this approach significantly contributes to extending IoT operational lifespan and reduces environmental impact. By integrating security metrics similar to the Common Vulnerability Scoring System (CVSS) into consumer platforms, this paper argues that proactive threat hunting can elevate user awareness about the security of IoT devices. This has the potential to impact consumer choices and encourage a security-conscious mindset in both the manufacturing and user communities. Through a comprehensive analysis, this study demonstrates how proactive threat hunting can contribute to the development of a more secure, sustainable, and user-aware IoT ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14804v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Ghasemshirazi, Ghazaleh Shirvani</dc:creator>
    </item>
    <item>
      <title>OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants</title>
      <link>https://arxiv.org/abs/2406.14883</link>
      <description>arXiv:2406.14883v1 Announce Type: cross 
Abstract: Warning: Contents of this paper may be upsetting.
  Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5x speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14883v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaspreet Ranjit, Brihi Joshi, Rebecca Dorn, Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta</dc:creator>
    </item>
    <item>
      <title>Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition</title>
      <link>https://arxiv.org/abs/2406.14894</link>
      <description>arXiv:2406.14894v1 Announce Type: cross 
Abstract: Verbs form the backbone of language, providing the structure and meaning to sentences. Yet, their intricate semantic nuances pose a longstanding challenge. Understanding verb relations through the concept of lexical entailment is crucial for comprehending sentence meanings and grasping verb dynamics. This work investigates the capabilities of eight Large Language Models in recognizing lexical entailment relations among verbs through differently devised prompting strategies and zero-/few-shot settings over verb pairs from two lexical databases, namely WordNet and HyperLex. Our findings unveil that the models can tackle the lexical entailment recognition task with moderately good performance, although at varying degree of effectiveness and under different conditions. Also, utilizing few-shot prompting can enhance the models' performance. However, perfectly solving the task arises as an unmet challenge for all examined LLMs, which raises an emergence for further research developments on this topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14894v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Candida M. Greco, Lucio La Cava, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>SoK: Attacks on DAOs</title>
      <link>https://arxiv.org/abs/2406.15071</link>
      <description>arXiv:2406.15071v1 Announce Type: cross 
Abstract: Decentralized Autonomous Organizations (DAOs) are blockchain-based organizations that facilitate decentralized governance. Today, DAOs not only hold billions of dollars in their treasury but also govern many of the most popular Decentralized Finance (DeFi) protocols. This paper systematically analyses security threats to DAOs, focusing on the types of attacks they face. We study attacks on DAOs that took place in the past, attacks that have been theorized to be possible, and potential attacks that were uncovered and prevented in audits. For each of these (potential) attacks, we describe and categorize the attack vectors utilized into four categories. This reveals that while many attacks on DAOs take advantage of the less tangible and more complex human nature involved in governance, audits tend to focus on code and protocol vulnerabilities. Thus, additionally, the paper examines empirical data on DAO vulnerabilities, outlines risk factors contributing to these attacks, and suggests mitigation strategies to safeguard against such vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15071v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rainer Feichtinger, Robin Fritsch, Lioba Heimbach, Yann Vonlanthen, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Sound and Fury, Signifying Nothing? Impact of Data Breach Disclosure Laws</title>
      <link>https://arxiv.org/abs/2406.15215</link>
      <description>arXiv:2406.15215v1 Announce Type: cross 
Abstract: Data breach disclosure (DBD) is presumed to improve firms' cybersecurity practices by inducing fear of subsequent revenue loss. This revenue loss, the theory goes, will occur if customers punish an offending firm by refusing to buy from them and is assumed to be the primary mechanism through which DBD laws will change firm behavior ex ante. However, our analysis of a large-scale data breach at a US retailer reveals no evidence of a decline in revenue. Using a difference-in-difference design on revenue data from 302 stores over a 20-week period around the breach disclosure, we found no evidence of a decline either across all stores or when sub-sampling by prior revenue size (to account for any heterogeneity in prior revenue size). Therefore, we posit that the presumed primary mechanism of DBD laws, and thus these laws may be ineffective and merely a lot of "sound and fury, signifying nothing."</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15215v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Zia Hydari, Yangfan Liang, Rahul Telang</dc:creator>
    </item>
    <item>
      <title>Keystroke Dynamics Against Academic Dishonesty in the Age of LLMs</title>
      <link>https://arxiv.org/abs/2406.15335</link>
      <description>arXiv:2406.15335v1 Announce Type: cross 
Abstract: The transition to online examinations and assignments raises significant concerns about academic integrity. Traditional plagiarism detection systems often struggle to identify instances of intelligent cheating, particularly when students utilize advanced generative AI tools to craft their responses. This study proposes a keystroke dynamics-based method to differentiate between bona fide and assisted writing within academic contexts. To facilitate this, a dataset was developed to capture the keystroke patterns of individuals engaged in writing tasks, both with and without the assistance of generative AI. The detector, trained using a modified TypeNet architecture, achieved accuracies ranging from 74.98% to 85.72% in condition-specific scenarios and from 52.24% to 80.54% in condition-agnostic scenarios. The findings highlight significant differences in keystroke dynamics between genuine and assisted writing. The outcomes of this study enhance our understanding of how users interact with generative AI and have implications for improving the reliability of digital educational platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15335v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debnath Kundu, Atharva Mehta, Rajesh Kumar, Naman Lal, Avinash Anand, Apoorv Singh, Rajiv Ratn Shah</dc:creator>
    </item>
    <item>
      <title>The Potential Impact of AI Innovations on U.S. Occupations</title>
      <link>https://arxiv.org/abs/2312.04714</link>
      <description>arXiv:2312.04714v3 Announce Type: replace 
Abstract: An occupation is comprised of interconnected tasks, and it is these tasks, not occupations themselves, that are affected by AI. To evaluate how tasks may be impacted, previous approaches utilized manual annotations or coarse-grained matching. Leveraging recent advancements in machine learning, we replace coarse-grained matching with more precise deep learning approaches. Introducing the AI Impact (AII) measure, we employ Deep Learning Natural Language Processing to automatically identify AI patents that may impact various occupational tasks at scale. Our methodology relies on a comprehensive dataset of 17,879 task descriptions and quantifies AI's potential impact through analysis of 24,758 AI patents filed with the United States Patent and Trademark Office (USPTO) between 2015 and 2022. Our results reveal that some occupations will potentially be impacted, and that impact is intricately linked to specific skills. These include not only routine tasks (codified as a series of steps), as previously thought, but also non-routine ones (e.g., diagnosing health conditions, programming computers, and tracking flight routes). Furthermore, AI's impact on labour is limited by the fact that some of the occupations affected are augmented rather than replaced (e.g., neurologists, software engineers, air traffic controllers), and the sectors affected are experiencing labour shortages (e.g., IT, Healthcare, Transport).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04714v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Akbar Septiandri, Marios Constantinides, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>On-demand Mobility-as-a-Service platform assignment games with guaranteed stable outcomes</title>
      <link>https://arxiv.org/abs/2305.00818</link>
      <description>arXiv:2305.00818v2 Announce Type: replace-cross 
Abstract: Mobility-as-a-Service (MaaS) systems are two-sided markets, with two mutually exclusive sets of agents, i.e., travelers/users and operators, forming a mobility ecosystem in which multiple operators compete or cooperate to serve customers under a governing platform provider. This study proposes a MaaS platform equilibrium model based on many-to-many assignment games incorporating both fixed-route transit services and mobility-on-demand (MOD) services. The matching problem is formulated as a convex multicommodity flow network design problem under congestion that captures the cost of accessing MOD services. The local stability conditions reflect a generalization of Wardrop's principles that include operators' decisions. Due to the presence of congestion, the problem may result in non-stable designs, and a subsidy mechanism from the platform is proposed to guarantee local stability. A new exact solution algorithm to the matching problem is proposed based on a branch and bound framework with a Frank-Wolfe algorithm integrated with Lagrangian relaxation and subgradient optimization, which guarantees the optimality of the matching problem but not stability. A heuristic which integrates stability conditions and subsidy design is proposed, which reaches either an optimal MaaS platform equilibrium solution with global stability, or a feasible locally stable solution that may require subsidy. For the heuristic, a worst-case bound and condition for obtaining an exact solution are both identified. An expanded Sioux Falls network test with 82 nodes and 748 links derives generalizable insights about the model for coopetitive interdependencies between operators sharing the platform, handling congestion effects in MOD services, effects of local stability on investment impacts, and illustrating inequities that may arise under heterogeneous populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00818v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingqing Liu, Joseph Y. J. Chow</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Probabilistic Graph Neural Networks for Road-Level Traffic Accident Prediction</title>
      <link>https://arxiv.org/abs/2309.05072</link>
      <description>arXiv:2309.05072v2 Announce Type: replace-cross 
Abstract: Traffic accidents present substantial challenges to human safety and socioeconomic development in urban areas. Developing a reliable and responsible traffic accident prediction model is crucial to addressing growing public safety concerns and enhancing the safety of urban mobility systems. Traditional methods face limitations at fine spatiotemporal scales due to the sporadic nature of highrisk accidents and the predominance of nonaccident characteristics. Furthermore, while most current models show promising occurrence prediction, they overlook the uncertainties arising from the inherent nature of accidents, and then fail to adequately map the hierarchical ranking of accident risk values for more precise insights. To address these issues, we introduce the Spatiotemporal ZeroInflated Tweedie Graph Neural Network ,STZITDGNN, the first uncertainty-aware probabilistic graph deep learning model in roadlevel traffic accident prediction for multi-steps. This model integrates the interpretability of the statistical Tweedie family model and the expressive power of graph neural networks. Its decoder innovatively employs a compound Tweedie model, a Poisson distribution to model the frequency of accident occurrences and a Gamma distribution to assess injury severity, supplemented by a zeroinflated component to effectively identify exessive non-incident instances. Empirical tests using realworld traffic data from London, UK, demonstrate that the STZITDGNN surpasses other baseline models across multiple benchmarks and metrics, including accident risk value prediction, uncertainty minimisation, nonaccident road identification and accident occurrence accuracy. Our study demonstrates that STZTIDGNN can effectively inform targeted road monitoring, thereby improving urban road safety strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05072v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaowei Gao, Xinke Jiang, Dingyi Zhuang, Huanfa Chen, Shenhao Wang, Stephen Law, James Haworth</dc:creator>
    </item>
    <item>
      <title>Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models</title>
      <link>https://arxiv.org/abs/2401.01301</link>
      <description>arXiv:2401.01301v2 Announce Type: replace-cross 
Abstract: Do large language models (LLMs) know the law? These models are increasingly being used to augment legal practice, education, and research, yet their revolutionary potential is threatened by the presence of hallucinations -- textual output that is not consistent with legal facts. We present the first systematic evidence of these hallucinations, documenting LLMs' varying performance across jurisdictions, courts, time periods, and cases. Our work makes four key contributions. First, we develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. Second, we find that legal hallucinations are alarmingly prevalent, occurring between 58% of the time with ChatGPT 4 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. Third, we illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. Fourth, we provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, our findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01301v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jla/laae003</arxiv:DOI>
      <dc:creator>Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho</dc:creator>
    </item>
  </channel>
</rss>
