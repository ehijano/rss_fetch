<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Review on Edge Large Language Models: Design, Execution, and Applications</title>
      <link>https://arxiv.org/abs/2410.11845</link>
      <description>arXiv:2410.11845v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing with their exceptional capabilities. However, deploying LLMs on resource-constrained edge devices presents significant challenges due to computational limitations, memory constraints, and edge hardware heterogeneity. This survey summarizes recent developments in edge LLMs across their lifecycle, examining resource-efficient designs from pre-deployment techniques to runtime optimizations. Additionally, it explores on-device LLM applications in personal, enterprise, and industrial scenarios. By synthesizing advancements and identifying future directions, this survey aims to provide a comprehensive understanding of state-of-the-art methods for deploying LLMs on edge devices, bridging the gap between their immense potential and edge computing limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11845v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, Jiming Chen</dc:creator>
    </item>
    <item>
      <title>Experimental Validation of User Experience-focused Dynamic Onboard Service Orchestration for Software Defined Vehicles</title>
      <link>https://arxiv.org/abs/2410.11847</link>
      <description>arXiv:2410.11847v1 Announce Type: new 
Abstract: In response to the growing need for dynamic software features in automobiles, Software Defined Vehicles (SDVs) have emerged as a promising solution. They integrate dynamic onboard service management to handle the large variety of user-requested services during vehicle operation. Allocating onboard resources efficiently in this setting is a challenging task, as it requires a balance between maximizing user experience and guaranteeing mixed-criticality Quality-of-Service (QoS) network requirements. Our previous research introduced a dynamic resource-based onboard service orchestration algorithm. This algorithm considers real-time invehicle and V2X network health, along with onboard resource constraints, to globally select degraded modes for onboard applications. It maximizes the overall user experience at all times while being embeddable onboard for on-the-fly decisionmaking. A key enabler of this approach is the introduction of the Automotive eXperience Integrity Level (AXIL), a metric expressing runtime priority for non-safety-critical applications. While initial simulation results demonstrated the algorithm's effectiveness, a comprehensive performance assessment would greatly contribute in validating its industrial feasibility. In this current work, we present experimental results obtained from a dedicated test bench. These results illustrate, validate, and assess the practicality of our proposed solution, providing a solid foundation for the continued advancement of dynamic onboard service orchestration in SDVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11847v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Laclau (Heudiasyc), St\'ephane Bonnet (Heudiasyc), Bertrand Ducourthial (Heudiasyc), Trista Lin (Toulouse INP), Xiaoting Li (Toulouse INP)</dc:creator>
    </item>
    <item>
      <title>Online Energy Optimization in GPUs: A Multi-Armed Bandit Approach</title>
      <link>https://arxiv.org/abs/2410.11855</link>
      <description>arXiv:2410.11855v1 Announce Type: new 
Abstract: Energy consumption has become a critical design metric and a limiting factor in the development of future computing architectures, from small wearable devices to large-scale leadership computing facilities. The predominant methods in energy management optimization are focused on CPUs. However, GPUs are increasingly significant and account for the majority of energy consumption in heterogeneous high performance computing (HPC) systems. Moreover, they typically rely on either purely offline training or a hybrid of offline and online training, which are impractical and lead to energy loss during data collection. Therefore, this paper studies a novel and practical online energy optimization problem for GPUs in HPC scenarios. The problem is challenging due to the inherent performance-energy trade-offs of GPUs, the exploration &amp; exploitation dilemma across frequencies, and the lack of explicit performance counters in GPUs. To address these challenges, we formulate the online energy consumption optimization problem as a multi-armed bandit framework and develop a novel bandit based framework EnergyUCB. EnergyUCB is designed to dynamically adjust GPU core frequencies in real-time, reducing energy consumption with minimal impact on performance. Specifically, the proposed framework EnergyUCB (1) balances the performance-energy trade-off in the reward function, (2) effectively navigates the exploration &amp; exploitation dilemma when adjusting GPU core frequencies online, and (3) leverages the ratio of GPU core utilization to uncore utilization as a real-time GPU performance metric. Experiments on a wide range of real-world HPC benchmarks demonstrate that EnergyUCB can achieve substantial energy savings. The code of EnergyUCB is available at https://github.com/XiongxiaoXu/EnergyUCB-Bandit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11855v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongxiao Xu, Solomon Abera Bekele, Brice Videau, Kai Shu</dc:creator>
    </item>
    <item>
      <title>LLMProxy: Reducing Cost to Access Large Language Models</title>
      <link>https://arxiv.org/abs/2410.11857</link>
      <description>arXiv:2410.11857v1 Announce Type: new 
Abstract: In this paper, we make a case for a proxy for large language models which has explicit support for cost-saving optimizations. We design LLMProxy, which supports three key optimizations: model selection, context management, and caching. These optimizations present tradeoffs in terms of cost, inference time, and response quality, which applications can navigate through our high level, bidirectional interface. As a case study, we implement a WhatsApp-based Q&amp;A service that uses LLMProxy to provide a rich set of features to the users. This service is deployed on a small scale (100+ users) leveraging the cloud; it has been operational for 15+ weeks and users have asked 1400+ questions so far. We report on the experiences of running this service as well as microbenchmark the specific benefits of the various cost-optimizations we present in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11857v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Martin, Abdullah Bin Faisal, Hiba Eltigani, Rukhshan Haroon, Swaminathan Lamelas, Fahad Dogar</dc:creator>
    </item>
    <item>
      <title>Towards using Reinforcement Learning for Scaling and Data Replication in Cloud Systems</title>
      <link>https://arxiv.org/abs/2410.11862</link>
      <description>arXiv:2410.11862v1 Announce Type: new 
Abstract: Given its intuitive nature, many Cloud providers opt for threshold-based data replication to enable automatic resource scaling. However, setting thresholds effectively needs human intervention to calibrate thresholds for each metric and requires a  deep knowledge of current workload trends, which can be challenging to achieve. Reinforcement learning is used in many areas related to the Cloud Computing, and it is a promising field to get automatic data replication strategies. In this work, we survey data replication strategies and data scaling based on reinforcement learning (RL).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11862v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Doctoral Conference on computer Science ADCCS'2024, Ecole Sup{\'e}rieure d'Informatique ESI, May 2024, Algier, Algeria</arxiv:journal_reference>
      <dc:creator>Riad Mokadem (IRIT-PYRAMIDE), Fahem Arar (IRIT-PYRAMIDE, ESI), Djamel Eddine Zegour</dc:creator>
    </item>
    <item>
      <title>A Framework for SLO, Carbon, and Wastewater-Aware Sustainable FaaS Cloud Platform Management</title>
      <link>https://arxiv.org/abs/2410.11875</link>
      <description>arXiv:2410.11875v1 Announce Type: new 
Abstract: Function-as-a-Service (FaaS) is a growing cloud computing paradigm that is expected to reduce the user cost of service over traditional serverful approaches. However, the environmental impact of FaaS has not received much attention. We investigate FaaS scheduling and scaling from a sustainability perspective in this work. We find that the service-level objectives (SLOs) of FaaS and carbon emissions conflict with each other. We also find that SLO-focused FaaS scheduling can exacerbate water use in a datacenter. We propose a novel sustainability-focused FaaS scheduling and scaling framework to co-optimize SLO performance, carbon emissions, and wastewater generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11875v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sirui Qi, Hayden Moore, Ninad Hogade, Dejan Milojicic, Cullen Bash, Sudeep Pasricha</dc:creator>
    </item>
    <item>
      <title>POSEIDON : Efficient Function Placement at the Edge using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.11879</link>
      <description>arXiv:2410.11879v1 Announce Type: new 
Abstract: Edge computing allows for reduced latency and operational costs compared to centralized cloud systems. In this context, serverless functions are emerging as a lightweight and effective paradigm for managing computational tasks on edge infrastructures. However, the placement of such functions in constrained edge nodes remains an open challenge. On one hand, it is key to minimize network delays and optimize resource consumption; on the other hand, decisions must be made in a timely manner due to the highly dynamic nature of edge environments.
  In this paper, we propose \approach, a solution based on Deep Reinforcement Learning for the efficient placement of functions at the edge. POSEIDON leverages Proximal Policy Optimization (PPO) to place functions across a distributed network of nodes under highly dynamic workloads. A comprehensive empirical evaluation demonstrates that POSEIDON significantly reduces execution time, network delay, and resource consumption compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11879v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Prakhar Jain, Prakhar Singhal, Divyansh Pandey, Giovanni Quatrocchi, Karthik Vaidhyanathan</dc:creator>
    </item>
    <item>
      <title>A Simulated Annealing Approach to Identical Parallel Machine Scheduling</title>
      <link>https://arxiv.org/abs/2410.11880</link>
      <description>arXiv:2410.11880v1 Announce Type: new 
Abstract: This paper studies the application of the simulated annealing metaheuristic on the identical parallel machine scheduling problem, a variant of the broader optimal job scheduling problem. In the identical parallel machine scheduling problem, $n$ jobs are to be assigned among $m$ machines. Furthermore, each job takes a certain amount of time that remains constant across machines. The goal of the paper is to schedule $n$ jobs on $m$ machines and minimize the maximum runtime of all machines. Both exact and heuristic methods have been applied to the problem, and the proposed algorithm falls in the heuristic category, making use of the simulated annealing metaheuristic. Compared to exact algorithms, simulated annealing was found to yield near-optimal solutions in comparable or less time for all problem cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11880v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxing Li, David Perkins</dc:creator>
    </item>
    <item>
      <title>Min-Max Gathering on Infinite Grid</title>
      <link>https://arxiv.org/abs/2410.11966</link>
      <description>arXiv:2410.11966v1 Announce Type: new 
Abstract: Gathering is a fundamental coordination problem in swarm robotics, where the objective is to bring robots together at a point not known to them at the beginning. While most research focuses on continuous domains, some studies also examine the discrete domain. This paper addresses the optimal gathering problem on an infinite grid, aiming to improve the energy efficiency by minimizing the maximum distance any robot must travel. The robots are autonomous, anonymous, homogeneous, identical, and oblivious. We identify all initial configurations where the optimal gathering problem is unsolvable. For the remaining configurations, we introduce a deterministic distributed algorithm that effectively gathers $n$ robots ($n\ge 9$). The algorithm ensures that the robots gathers at one of the designated min-max nodes in the grid. Additionally, we provide a comprehensive characterization of the subgraph formed by the min-max nodes in this infinite grid model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11966v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Pritam Goswami, Satakshi Ghosh</dc:creator>
    </item>
    <item>
      <title>From promise to practice: realizing high-performance decentralized training</title>
      <link>https://arxiv.org/abs/2410.11998</link>
      <description>arXiv:2410.11998v1 Announce Type: new 
Abstract: Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11998v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zesen Wang, Jiaojiao Zhang, Xuyang Wu, Mikael Johansson</dc:creator>
    </item>
    <item>
      <title>Cilium and VDM -- Towards Formal Analysis of Cilium Policies</title>
      <link>https://arxiv.org/abs/2410.12009</link>
      <description>arXiv:2410.12009v1 Announce Type: new 
Abstract: Industrial control systems are becoming more distributed and interconnected to allow for interaction with modern computing infrastructures. Furthermore, the amount of data generated by these systems is increasing due to integration of more sensors and the need to increase the reliability of the system based on predictive data models. One challenge in accommodating this data and interconnectivity increase is the change of the architecture of these systems from monolithic to component based, distributed systems. Questions such as how to deploy and operate such distributed system with many sub-components arise. One approach is the use of kubernetes to orchestrate the different components as containers. The critical nature of the industrial control systems however often requires strict component isolation and network segmentation to satisfy security requirements. Cilium is a popular network overlay for kubernetes that enables definition of network policies between different components running as kubernetes pods. The network policies are crucial for maintaining the secure operation of the system, however analysis of deployed policies is often lacking. In this paper, we explore the use of a formal analysis of Cilium network policies using VDM-SL. We provide examples of Cilium policies, an approach how they could be formalised using VDM-SL and analyse several scenarios to validate the policies against a model of simple real-life system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12009v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tomas Kulik, Jalil Boudjadar</dc:creator>
    </item>
    <item>
      <title>Accelerating Python Applications with Dask and ProxyStore</title>
      <link>https://arxiv.org/abs/2410.12092</link>
      <description>arXiv:2410.12092v1 Announce Type: new 
Abstract: Applications are increasingly written as dynamic workflows underpinned by an execution framework that manages asynchronous computations across distributed hardware. However, execution frameworks typically offer one-size-fits-all solutions for data flow management, which can restrict performance and scalability. ProxyStore, a middleware layer that optimizes data flow via an advanced pass-by-reference paradigm, has shown to be an effective mechanism for addressing these limitations. Here, we investigate integrating ProxyStore with Dask Distributed, one of the most popular libraries for distributed computing in Python, with the goal of supporting scalable and portable scientific workflows. Dask provides an easy-to-use and flexible framework, but is less optimized for scaling certain data-intensive workflows. We investigate these limitations and detail the technical contributions necessary to develop a robust solution for distributed applications and demonstrate improved performance on synthetic benchmarks and real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12092v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Gregory Pauloski, Klaudiusz Rydzy, Valerie Hayot-Sasson, Ian Foster, Kyle Chard</dc:creator>
    </item>
    <item>
      <title>Consider an Applications-First Approach for PDC</title>
      <link>https://arxiv.org/abs/2410.12116</link>
      <description>arXiv:2410.12116v1 Announce Type: new 
Abstract: I propose an applications-first approach for adjusting how parallel and distributed computing concepts are incorporated into curricula. By focusing on practical applications that leverage parallelism and distributed systems, this approach aims to make these complex topics more accessible and engaging for both CS and non-CS majors. An applications-first approach demonstrates the advantages of parallel and distributed computing in solving real-world problems while building practical experience and skills before delving into theoretical concepts. This could potentially broaden the appeal and retention of these concepts. I highlight some example application-centric efforts, and conclude with questions that could be investigated in the service of exploring applications-first approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12116v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Strout</dc:creator>
    </item>
    <item>
      <title>Juggernaut: Efficient Crypto-Agnostic Byzantine Agreement</title>
      <link>https://arxiv.org/abs/2410.12121</link>
      <description>arXiv:2410.12121v1 Announce Type: new 
Abstract: It is well known that a trusted setup allows one to solve the Byzantine agreement problem in the presence of $t&lt;n/2$ corruptions, bypassing the setup-free $t&lt;n/3$ barrier. Alas, the overwhelming majority of protocols in the literature have the caveat that their security crucially hinges on the security of the cryptography and setup, to the point where if the cryptography is broken, even a single corrupted party can violate the security of the protocol. Thus these protocols provide higher corruption resilience ($n/2$ instead of $n/3$) for the price of increased assumptions. Is this trade-off necessary?
  We further the study of crypto-agnostic Byzantine agreement among $n$ parties that answers this question in the negative. Specifically, let $t_s$ and $t_i$ denote two parameters such that (1) $2t_i + t_s &lt; n$, and (2) $t_i \leq t_s &lt; n/2$. Crypto-agnostic Byzantine agreement ensures agreement among honest parties if (1) the adversary is computationally bounded and corrupts up to $t_s$ parties, or (2) the adversary is computationally unbounded and corrupts up to $t_i$ parties, and is moreover given all secrets of all parties established during the setup. We propose a compiler that transforms any pair of resilience-optimal Byzantine agreement protocols in the authenticated and information-theoretic setting into one that is crypto-agnostic. Our compiler has several attractive qualities, including using only $O(\lambda n^2)$ bits over the two underlying Byzantine agreement protocols, and preserving round and communication complexity in the authenticated setting. In particular, our results improve the state-of-the-art in bit complexity by at least two factors of $n$ and provide either early stopping (deterministic) or expected constant round complexity (randomized). We therefore provide fallback security for authenticated Byzantine agreement for free for $t_i \leq n/4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12121v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Collins, Yuval Efron, Jovan Komatovic</dc:creator>
    </item>
    <item>
      <title>Proof of Team Sprint: A Collaborative Consensus Algorithm for Reducing Energy Consumption in Blockchain Systems</title>
      <link>https://arxiv.org/abs/2410.12135</link>
      <description>arXiv:2410.12135v1 Announce Type: new 
Abstract: This paper introduces Proof of Team Sprint (PoTS), a novel consensus algorithm designed to address the significant energy inefficiencies inherent in traditional Proof of Work (PoW) systems. PoTS shifts the consensus mechanism from an individual competition model to a collaborative team-based approach. Participants are organized into groups, with each group collaboratively working to solve cryptographic puzzles required to validate transactions and add new blocks to the blockchain. This collaborative approach significantly reduces the overall energy consumption of the network while maintaining high levels of security and decentralization. Our analysis shows that PoTS can reduce energy consumption by a factor of 1/N, where N is the number of participants in each group, compared to PoW. Furthermore, PoTS maintains a fair and equitable reward distribution among participants, ensuring continued engagement and network integrity. The paper also discusses the scalability, security implications, and potential challenges of adopting PoTS, positioning it as a promising alternative for sustainable blockchain technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12135v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Yonezawa</dc:creator>
    </item>
    <item>
      <title>Accelerating high-order continuum kinetic plasma simulations using multiple GPUs</title>
      <link>https://arxiv.org/abs/2410.12155</link>
      <description>arXiv:2410.12155v1 Announce Type: new 
Abstract: Kinetic plasma simulations solve the Vlasov-Poisson or Vlasov-Maxwell equations to evolve scalar-variable distribution functions in position-velocity phase space and vector-variable electromagnetic fields in physical space. The computational cost of evolving high-dimensional variables often limits the utility of continuum kinetic simulations and presents a challenge when it comes to accurately simulating real-world physical phenomena. To address this challenge, we developed techniques that accelerate and minimize the computational work required for a scalable Vlasov-Poisson solver. We present theoretical hardware compute and communication bounds required for solving a fourth-order finite-volume Vlasov-Poisson system. These bounds are then used to inform and evaluate the design of performance portable algorithms for a multiple graphics processing unit (GPU) accelerated version of the Vlasov-Poisson solver VCK-CPU. We demonstrate that the multi-GPU Vlasov solver implementation VCK-GPU simultaneously minimizes required inter-process data transfer while also being bounded by the machine network performance limits, leaving minimal room for theoretical performance improvements. This resulted in an overall strong scaling speedup per timestep of up to 40x in three-dimensional phase space (one position, two velocity coordinates) and 54x in four dimensional phase space (two position, two velocity coordinates) and a 341x increase in simulation throughput of the GPU accelerated code over the existing CPU code. The GPU code is also able to weak scale up to 256 compute nodes and 1024 GPUs. We then demonstrate how the improved compute performance can be used to explore configurations which were previously computationally infeasible such as resolving fine-scale distribution function filamentation and multi-species dynamics with realistic electron-proton mass ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12155v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Ho, Genia Vogman</dc:creator>
    </item>
    <item>
      <title>EdgeRL: Reinforcement Learning-driven Deep Learning Model Inference Optimization at Edge</title>
      <link>https://arxiv.org/abs/2410.12221</link>
      <description>arXiv:2410.12221v1 Announce Type: new 
Abstract: Balancing mutually diverging performance metrics, such as, processing latency, outcome accuracy, and end device energy consumption is a challenging undertaking for deep learning model inference in ad-hoc edge environments. In this paper, we propose EdgeRL framework that seeks to strike such balance by using an Advantage Actor-Critic (A2C) Reinforcement Learning (RL) approach that can choose optimal run-time DNN inference parameters and aligns the performance metrics based on the application requirements. Using real world deep learning model and a hardware testbed, we evaluate the benefits of EdgeRL framework in terms of end device energy savings, inference accuracy improvement, and end-to-end inference latency reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12221v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Motahare Mounesan, Xiaojie Zhang, Saptarshi Debroy</dc:creator>
    </item>
    <item>
      <title>Disentangling data distribution for Federated Learning</title>
      <link>https://arxiv.org/abs/2410.12530</link>
      <description>arXiv:2410.12530v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates collaborative training of a global model whose performance is boosted by private data owned by distributed clients, without compromising data privacy. Yet the wide applicability of FL is hindered by entanglement of data distributions across different clients. This paper demonstrates for the first time that by disentangling data distributions FL can in principle achieve efficiencies comparable to those of distributed systems, requiring only one round of communication. To this end, we propose a novel FedDistr algorithm, which employs stable diffusion models to decouple and recover data distributions. Empirical results on the CIFAR100 and DomainNet datasets show that FedDistr significantly enhances model utility and efficiency in both disentangled and near-disentangled scenarios while ensuring privacy, outperforming traditional federated learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12530v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyuan Zhao, Hanlin Gu, Lixin Fan, Qiang Yang, Yuxing Han</dc:creator>
    </item>
    <item>
      <title>FALCON: Pinpointing and Mitigating Stragglers for Large-Scale Hybrid-Parallel Training</title>
      <link>https://arxiv.org/abs/2410.12588</link>
      <description>arXiv:2410.12588v1 Announce Type: new 
Abstract: Fail-slows, or stragglers, are common but largely unheeded problems in large-scale hybrid-parallel training that spans thousands of GPU servers and runs for weeks to months. Yet, these problems are not well studied, nor can they be quickly detected and effectively mitigated. In this paper, we first present a characterization study on a shared production cluster with over 10,000 GPUs1. We find that fail-slows are caused by various CPU/GPU computation and cross-node networking issues, lasting from tens of seconds to nearly ten hours, and collectively delaying the average job completion time by 1.34%. The current practice is to manually detect these fail-slows and simply treat them as fail-stops using a checkpoint-and-restart failover approach, which are labor-intensive and time-consuming. In this paper, we propose FALCON, a framework that rapidly identifies fail-slowed GPUs and/or communication links, and effectively tackles them with a novel multi-level mitigation mechanism, all without human intervention. We have applied FALCON to detect human-labeled fail-slows in a production cluster with over 99% accuracy. Cluster deployment further demonstrates that FALCON effectively handles manually injected fail-slows, mitigating the training slowdown by 60.1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12588v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyuan Wu, Wei Wang, Yinghao Yu, Siran Yang, Wenchao Wu, Qinkai Duan, Guodong Yang, Jiamang Wang, Lin Qu, Liping Zhang</dc:creator>
    </item>
    <item>
      <title>Optimization and Application of Cloud-based Deep Learning Architecture for Multi-Source Data Prediction</title>
      <link>https://arxiv.org/abs/2410.12642</link>
      <description>arXiv:2410.12642v1 Announce Type: new 
Abstract: This study develops a cloud-based deep learning system for early prediction of diabetes, leveraging the distributed computing capabilities of the AWS cloud platform and deep learning technologies to achieve efficient and accurate risk assessment. The system utilizes EC2 p3.8xlarge GPU instances to accelerate model training, reducing training time by 93.2% while maintaining a prediction accuracy of 94.2%. With an automated data processing and model training pipeline built using Apache Airflow, the system can complete end-to-end updates within 18.7 hours. In clinical applications, the system demonstrates a prediction accuracy of 89.8%, sensitivity of 92.3%, and specificity of 95.1%. Early interventions based on predictions lead to a 37.5% reduction in diabetes incidence among the target population. The system's high performance and scalability provide strong support for large-scale diabetes prevention and management, showcasing significant public health value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12642v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zhang, Fa Wang, Xin Huang, Xintao Li, Sibei Liu, Hansong Zhang</dc:creator>
    </item>
    <item>
      <title>FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression</title>
      <link>https://arxiv.org/abs/2410.12707</link>
      <description>arXiv:2410.12707v1 Announce Type: new 
Abstract: To alleviate hardware scarcity in training large deep neural networks (DNNs), particularly large language models (LLMs), we present FusionLLM, a decentralized training system designed and implemented for training DNNs using geo-distributed GPUs across different computing clusters or individual devices. Decentralized training faces significant challenges regarding system design and efficiency, including: 1) the need for remote automatic differentiation (RAD), 2) support for flexible model definitions and heterogeneous software, 3) heterogeneous hardware leading to low resource utilization or the straggler problem, and 4) slow network communication. To address these challenges, in the system design, we represent the model as a directed acyclic graph of operators (OP-DAG). Each node in the DAG represents the operator in the DNNs, while the edge represents the data dependency between operators. Based on this design, 1) users are allowed to customize any DNN without caring low-level operator implementation; 2) we enable the task scheduling with the more fine-grained sub-tasks, offering more optimization space; 3) a DAG runtime executor can implement RAD withour requiring the consistent low-level ML framework versions.
  To enhance system efficiency, we implement a workload estimator and design an OP-Fence scheduler to cluster devices with similar bandwidths together and partition the DAG to increase throughput. Additionally, we propose an AdaTopK compressor to adaptively compress intermediate activations and gradients at the slowest communication links. To evaluate the convergence and efficiency of our system and algorithms, we train ResNet-101 and GPT-2 on three real-world testbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental results demonstrate that our system and method can achieve 1.45 - 9.39x speedup compared to baseline methods while ensuring convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12707v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, Xiaowen Chu</dc:creator>
    </item>
    <item>
      <title>Toward Optimal-Complexity Hash-Based Asynchronous MVBA with Optimal Resilience</title>
      <link>https://arxiv.org/abs/2410.12755</link>
      <description>arXiv:2410.12755v1 Announce Type: new 
Abstract: Multi-valued validated Byzantine agreement (MVBA), a fundamental primitive of distributed computing, enables n processes to agree on a valid L-bit value, despite t faulty processes behaving arbitrarily. Among hash-based protocols for the asynchronous setting with adaptive faults, the state-of-the-art HMVBA protocol has optimal O(1) time complexity and near-optimal O(n L + n^2 kappa log n) bit complexity, but tolerates only t &lt; n/5 faults. We present REDUCER, an MVBA protocol that matches HMVBA's time and bit complexity and improves resilience to t &lt; n/4. Like HMVBA, REDUCER relies solely on collision-resistant hash functions. Toward optimal one-third resilience, we also propose REDUCER++, an MVBA protocol with further improved t &lt; (1/3 - epsilon)n resilience, for any fixed epsilon &gt; 0, assuming hash functions modeled as random oracles. Time and bit complexity of REDUCER++ remain constant and quasi-quadratic, respectively, with constants depending on epsilon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12755v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jovan Komatovic, Joachim Neu, Tim Roughgarden</dc:creator>
    </item>
    <item>
      <title>Vaccinating Federated Learning for Robust Modulation Classification in Distributed Wireless Networks</title>
      <link>https://arxiv.org/abs/2410.12772</link>
      <description>arXiv:2410.12772v1 Announce Type: new 
Abstract: Automatic modulation classification (AMC) serves a vital role in ensuring efficient and reliable communication services within distributed wireless networks. Recent developments have seen a surge in interest in deep neural network (DNN)-based AMC models, with Federated Learning (FL) emerging as a promising framework. Despite these advancements, the presence of various noises within the signal exerts significant challenges while optimizing models to capture salient features. Furthermore, existing FL-based AMC models commonly rely on linear aggregation strategies, which face notable difficulties in integrating locally fine-tuned parameters within practical non-IID (Independent and Identically Distributed) environments, thereby hindering optimal learning convergence. To address these challenges, we propose FedVaccine, a novel FL model aimed at improving generalizability across signals with varying noise levels by deliberately introducing a balanced level of noise. This is accomplished through our proposed harmonic noise resilience approach, which identifies an optimal noise tolerance for DNN models, thereby regulating the training process and mitigating overfitting. Additionally, FedVaccine overcomes the limitations of existing FL-based AMC models' linear aggregation by employing a split-learning strategy using structural clustering topology and local queue data structure, enabling adaptive and cumulative updates to local models. Our experimental results, including IID and non-IID datasets as well as ablation studies, confirm FedVaccine's robust performance and superiority over existing FL-based AMC approaches across different noise levels. These findings highlight FedVaccine's potential to enhance the reliability and performance of AMC systems in practical wireless network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12772v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hunmin Lee, Hongju Seong, Wonbin Kim, Hyeokchan Kwon, Daehee Seo</dc:creator>
    </item>
    <item>
      <title>Age-of-Gradient Updates for Federated Learning over Random Access Channels</title>
      <link>https://arxiv.org/abs/2410.11986</link>
      <description>arXiv:2410.11986v1 Announce Type: cross 
Abstract: This paper studies the problem of federated training of a deep neural network (DNN) over a random access channel (RACH) such as in computer networks, wireless networks, and cellular systems. More precisely, a set of remote users participate in training a centralized DNN model using SGD under the coordination of a parameter server (PS). The local model updates are transmitted from the remote users to the PS over a RACH using a slotted ALOHA protocol. The PS collects the updates from the remote users, accumulates them, and sends central model updates to the users at regular time intervals. We refer to this setting as the RACH-FL setting. The RACH-FL setting crucially addresses the problem of jointly designing a (i) client selection and (ii) gradient compression strategy which addresses the communication constraints between the remote users and the PS when transmission occurs over a RACH. For the RACH-FL setting, we propose a policy, which we term the ''age-of-gradient'' (AoG) policy in which (i) gradient sparsification is performed using top-K sparsification, (ii) the error correction is performed using memory accumulation, and (iii) the slot transmission probability is obtained by comparing the current local memory magnitude minus the magnitude of the gradient update to a threshold. Intuitively, the AoG measure of ''freshness'' of the memory state is reminiscent of the concept of age-of-information (AoI) in the context of communication theory and provides a rather natural interpretation of this policy. Numerical simulations show the superior performance of the AoG policy as compared to other RACH-FL policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11986v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Heng Wu, Houman Asgari, Stefano Rini, Andrea Munari</dc:creator>
    </item>
    <item>
      <title>MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from {\mu}Watts to MWatts for Sustainable AI</title>
      <link>https://arxiv.org/abs/2410.12032</link>
      <description>arXiv:2410.12032v1 Announce Type: cross 
Abstract: Rapid adoption of machine learning (ML) technologies has led to a surge in power consumption across diverse systems, from tiny IoT devices to massive datacenter clusters. Benchmarking the energy efficiency of these systems is crucial for optimization, but presents novel challenges due to the variety of hardware platforms, workload characteristics, and system-level interactions. This paper introduces MLPerf Power, a comprehensive benchmarking methodology with capabilities to evaluate the energy efficiency of ML systems at power levels ranging from microwatts to megawatts. Developed by a consortium of industry professionals from more than 20 organizations, MLPerf Power establishes rules and best practices to ensure comparability across diverse architectures. We use representative workloads from the MLPerf benchmark suite to collect 1,841 reproducible measurements from 60 systems across the entire range of ML deployment scales. Our analysis reveals trade-offs between performance, complexity, and energy efficiency across this wide range of systems, providing actionable insights for designing optimized ML solutions from the smallest edge devices to the largest cloud infrastructures. This work emphasizes the importance of energy efficiency as a key metric in the evaluation and comparison of the ML system, laying the foundation for future research in this critical area. We discuss the implications for developing sustainable AI solutions and standardizing energy efficiency benchmarking for ML systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12032v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arya Tschand (Harvard University), Arun Tejusve Raghunath Rajan (Self / Meta), Sachin Idgunji (NVIDIA), Anirban Ghosh (NVIDIA), Jeremy Holleman (UNC Charlotte / Syntiant), Csaba Kiraly (Codex), Pawan Ambalkar (Dell), Ritika Borkar (NVIDIA), Ramesh Chukka (Intel), Trevor Cockrell (Dell), Oliver Curtis (SMC), Grigori Fursin (FlexAI / cTuning), Miro Hodak (AMD), Hiwot Kassa (Meta), Anton Lokhmotov (KRAI), Dejan Miskovic (NVIDIA), Yuechao Pan (Google), Manu Prasad Manmathan (Intel), Liz Raymond (Dell), Tom St. John (Decompute), Arjun Suresh (GATE Overflow), Rowan Taubitz (SMC), Sean Zhan (SMC), Scott Wasson (MLCommons), David Kanter (MLCommons), Vijay Janapa Reddi (Harvard University)</dc:creator>
    </item>
    <item>
      <title>EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference</title>
      <link>https://arxiv.org/abs/2410.12247</link>
      <description>arXiv:2410.12247v1 Announce Type: cross 
Abstract: Large Language Model (LLM) has revolutionized the field of artificial intelligence, with their capabilities expanding rapidly due to advances in deep learning and increased computational resources. The mixture-of-experts (MoE) model has emerged as a prominent architecture in the field of LLM, better balancing the model performance and computational efficiency. MoE architecture allows for effective scaling and efficient parallel processing, but the GEMM (General Matrix Multiply) of MoE and the large parameters introduce challenges in terms of computation efficiency and communication overhead, which becomes the throughput bottleneck during inference. Applying a single parallelism strategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal inference throughput, the straightforward combinations of existing different parallelisms on MoE can not obtain optimal inference throughput yet. This paper introduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond the existing inference parallelism schemes. Our approach focuses on optimizing the computation of MoE FFN (FeedForward Network) modules by dynamically selecting the best kernel implementation of GroupGemm and DenseGemm for different loads and adaptively overlapping these computations with \textit{all2all} communication, leading to a substantial increase in throughput. Our experimental results demonstrate an average 21% improvement in prefill throughput over existing parallel inference methods. Specifically, we validated our method on DeepSeekV2, a highly optimized model claimed to achieve a prefill throughput of 100K tokens per second. By applying EPS-MoE, we further accelerated it to at least 120K tokens per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12247v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang, Xunliang Cai</dc:creator>
    </item>
    <item>
      <title>TPFL: A Trustworthy Personalized Federated Learning Framework via Subjective Logic</title>
      <link>https://arxiv.org/abs/2410.12316</link>
      <description>arXiv:2410.12316v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy. Despite its widespread adoption, most FL approaches focusing solely on privacy protection fall short in scenarios where trustworthiness is crucial, necessitating advancements in secure training, dependable decision-making mechanisms, robustness on corruptions, and enhanced performance with Non-IID data. To bridge this gap, we introduce Trustworthy Personalized Federated Learning (TPFL) framework designed for classification tasks via subjective logic in this paper. Specifically, TPFL adopts a unique approach by employing subjective logic to construct federated models, providing probabilistic decisions coupled with an assessment of uncertainty rather than mere probability assignments. By incorporating a trainable heterogeneity prior to the local training phase, TPFL effectively mitigates the adverse effects of data heterogeneity. Model uncertainty and instance uncertainty are further utilized to ensure the safety and reliability of the training and inference stages. Through extensive experiments on widely recognized federated learning benchmarks, we demonstrate that TPFL not only achieves competitive performance compared with advanced methods but also exhibits resilience against prevalent malicious attacks, robustness on domain shifts, and reliability in high-stake scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12316v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinqian Chen, Jihua Zhu</dc:creator>
    </item>
    <item>
      <title>Federated Temporal Graph Clustering</title>
      <link>https://arxiv.org/abs/2410.12343</link>
      <description>arXiv:2410.12343v1 Announce Type: cross 
Abstract: Temporal graph clustering is a complex task that involves discovering meaningful structures in dynamic graphs where relationships and entities change over time. Existing methods typically require centralized data collection, which poses significant privacy and communication challenges. In this work, we introduce a novel Federated Temporal Graph Clustering (FTGC) framework that enables decentralized training of graph neural networks (GNNs) across multiple clients, ensuring data privacy throughout the process. Our approach incorporates a temporal aggregation mechanism to effectively capture the evolution of graph structures over time and a federated optimization strategy to collaboratively learn high-quality clustering representations. By preserving data privacy and reducing communication overhead, our framework achieves competitive performance on temporal graph datasets, making it a promising solution for privacy-sensitive, real-world applications involving dynamic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12343v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Zihao Zhou, Xianghong Xu, Qian Li</dc:creator>
    </item>
    <item>
      <title>SEMSO: A Secure and Efficient Multi-Data Source Blockchain Oracle</title>
      <link>https://arxiv.org/abs/2410.12540</link>
      <description>arXiv:2410.12540v1 Announce Type: cross 
Abstract: In recent years, blockchain oracle, as the key link between blockchain and real-world data interaction, has greatly expanded the application scope of blockchain. In particular, the emergence of the Multi-Data Source (MDS) oracle has greatly improved the reliability of the oracle in the case of untrustworthy data sources. However, the current MDS oracle scheme requires nodes to obtain data redundantly from multiple data sources to guarantee data reliability, which greatly increases the resource overhead and response time of the system. Therefore, in this paper, we propose a Secure and Efficient Multi-data Source Oracle framework (SEMSO), which nodes only need to access one data source to ensure the reliability of final data. First, we design a new off-chain data aggregation protocol TBLS, to guarantee data source diversity and reliability at low cost. Second, according to the rational man assumption, the data source selection task of nodes is modeled and solved based on the Bayesian game under incomplete information to maximize the node's revenue while improving the success rate of TBLS aggregation and system response speed. Security analysis verifies the reliability of the proposed scheme, and experiments show that under the same environmental assumptions, SEMSO takes into account data diversity while reducing the response time by 23.5\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12540v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youquan Xian, Xueying Zeng, Chunpei Li, Peng Wang, Dongcheng Li, Peng Liu, Xianxian Li</dc:creator>
    </item>
    <item>
      <title>HEnRY: A Multi-Agent System Framework for Multi-Domain Contexts</title>
      <link>https://arxiv.org/abs/2410.12720</link>
      <description>arXiv:2410.12720v1 Announce Type: cross 
Abstract: This project, named HEnRY, aims to introduce a Multi-Agent System (MAS) into Intesa Sanpaolo. The name HEnRY summarizes the project's core principles: the Hierarchical organization of agents in a layered structure for efficient resource management; Efficient optimization of resources and operations to enhance overall performance; Reactive ability of agents to quickly respond to environmental stimuli; and Yielding adaptability and flexibility of agents to handle unexpected situations. The discussion covers two distinct research paths: the first focuses on the system architecture, and the second on the collaboration between agents. This work is not limited to the specific structure of the Intesa Sanpaolo context; instead, it leverages existing research in MAS to introduce a new solution. Since Intesa Sanpaolo is organized according to a model that aligns with international corporate governance best practices, this approach could also be relevant to similar scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12720v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuele Lacavalla, Shuyi Yang, Riccardo Crupi, Joseph E. Gonzalez</dc:creator>
    </item>
    <item>
      <title>Guardian: Safe GPU Sharing in Multi-Tenant Environments</title>
      <link>https://arxiv.org/abs/2401.09290</link>
      <description>arXiv:2401.09290v2 Announce Type: replace 
Abstract: Modern GPU applications, such as machine learning (ML), can only partially utilize GPUs, leading to GPU underutilization in cloud environments. Sharing GPUs across multiple applications from different tenants can improve resource utilization and consequently cost, energy, and power efficiency. However, GPU sharing creates memory safety concerns because kernels must share a single GPU address space. Existing spatial-sharing mechanisms either lack fault isolation for memory accesses or require static partitioning, which leads to limited deployability or low utilization.
  In this paper, we present Guardian, a PTX-level bounds checking approach that provides memory isolation and supports dynamic GPU spatial-sharing. Guardian relies on three mechanisms: (1) It divides the common GPU address space into separate partitions for different applications. (2) It intercepts and checks all GPU related calls at the lowest level, fencing erroneous operations. (3) It instruments all GPU kernels at the PTX level -- available in closed GPU libraries -- fencing all kernel memory accesses outside application memory bounds. Guardian's approach is transparent to applications and supports real-life frameworks, such as Caffe and PyTorch, that issue billions of GPU kernels. Our evaluation shows that Guardian's overhead compared to native for such frameworks is between 4% - 12% and on average 9%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09290v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manos Pavlidakis, Giorgos Vasiliadis, Stelios Mavridis, Anargyros Argyros, Antony Chazapis, Angelos Bilas</dc:creator>
    </item>
    <item>
      <title>Smart Casual Verification of the Confidential Consortium Framework</title>
      <link>https://arxiv.org/abs/2406.17455</link>
      <description>arXiv:2406.17455v2 Announce Type: replace 
Abstract: The Confidential Consortium Framework (CCF) is an open-source platform for developing trustworthy and reliable cloud applications. CCF powers Microsoft's Azure Confidential Ledger service and as such it is vital to build confidence in the correctness of CCF's design and implementation. This paper reports our experiences applying smart casual verification to validate the correctness of CCF's novel distributed protocols, focusing on its unique distributed consensus protocol and its custom client consistency model. We use the term smart casual verification to describe our hybrid approach, which combines the rigor of formal specification and model checking with the pragmatism of automated testing, in our case binding the formal specification in TLA+ to the C++ implementation. While traditional formal methods approaches require substantial buy-in and are often one-off efforts by domain experts, we have integrated our smart casual verification approach into CCF's CI pipeline, allowing contributors to continuously validate CCF as it evolves. We describe the challenges we faced in applying smart casual verification to a complex existing codebase and how we overcame them to find six subtle bugs in the design and implementation before they could impact production</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17455v2</guid>
      <category>cs.DC</category>
      <category>cs.FL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heidi Howard, Markus A. Kuppe, Edward Ashton, Amaury Chamayou, Natacha Crooks</dc:creator>
    </item>
    <item>
      <title>SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization</title>
      <link>https://arxiv.org/abs/2410.10759</link>
      <description>arXiv:2410.10759v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been a disruptive innovation in recent years, and they play a crucial role in our daily lives due to their ability to understand and generate human-like text. Their capabilities include natural language understanding, information retrieval and search, translation, chatbots, virtual assistance, and many more. However, it is well known that LLMs are massive in terms of the number of parameters. Additionally, the self-attention mechanism in the underlying architecture of LLMs, Transformers, has quadratic complexity in terms of both computation and memory with respect to the input sequence length. For these reasons, LLM inference is resource-intensive, and thus, the throughput of LLM inference is limited, especially for the longer sequences. In this report, we design a collaborative inference architecture between a server and its clients to alleviate the throughput limit. In this design, we consider the available resources on both sides, i.e., the computation and communication costs. We develop a dynamic programming-based algorithm to optimally allocate computation between the server and the client device to increase the server throughput, while not violating the service level agreement (SLA). We show in the experiments that we are able to efficiently distribute the workload allowing for roughly 1/3 reduction in the server workload, while achieving 19 percent improvement over a greedy method. As a result, we are able to demonstrate that, in an environment with different types of LLM inference requests, the throughput of the server is improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10759v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akrit Mudvari, Yuang Jiang, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>Light-Weight Fault Tolerant Attention for Large Language Model Training</title>
      <link>https://arxiv.org/abs/2410.11720</link>
      <description>arXiv:2410.11720v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, the training of these models is computationally intensive and susceptible to faults, particularly in the attention mechanism, which is a critical component of transformer-based LLMs. In this paper, we investigate the impact of faults on LLM training, focusing on INF, NaN, and near-INF values in the computation results with systematic fault injection experiments. We observe the propagation patterns of these errors, which can trigger non-trainable states in the model and disrupt training, forcing the procedure to load from checkpoints. To mitigate the impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault Tolerance (ABFT) technique tailored for the attention mechanism in LLMs. ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training. Evaluations on four LLMs show that ATTNChecker on average incurs on average 7% overhead on training while detecting and correcting all extreme errors. Compared with the state-of-the-art checkpoint/restore approach, ATTNChecker reduces recovery overhead by up to 49x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11720v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, Jieyang Chen</dc:creator>
    </item>
    <item>
      <title>Modeling Short-Range Microwave Networks to Scale Superconducting Quantum Computation</title>
      <link>https://arxiv.org/abs/2201.08825</link>
      <description>arXiv:2201.08825v3 Announce Type: replace-cross 
Abstract: A core challenge for superconducting quantum computers is to scale up the number of qubits in each processor without increasing noise or cross-talk. Distributed quantum computing across small qubit arrays, known as chiplets, can address these challenges in a scalable manner. We propose a chiplet architecture over microwave links with potential to exceed monolithic performance on near-term hardware. Our methods of modeling and evaluating the chiplet architecture bridges the physical and network layers in these processors. We find evidence that distributing computation across chiplets may reduce the overall error rates associated with moving data across the device, despite higher error figures for transfers across links. Preliminary analyses suggest that latency is not substantially impacted, and that at least some applications and architectures may avoid bottlenecks around chiplet boundaries. In the long-term, short-range networks may underlie quantum computers just as local area networks underlie classical datacenters and supercomputers today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.08825v3</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas LaRacuente, Kaitlin N. Smith, Poolad Imany, Kevin L. Silverman, Frederic T. Chong</dc:creator>
    </item>
    <item>
      <title>Towards Robust Blockchain Price Oracle: A Study on Human-Centric Node Selection Strategy and Incentive Mechanism</title>
      <link>https://arxiv.org/abs/2309.04689</link>
      <description>arXiv:2309.04689v2 Announce Type: replace-cross 
Abstract: As a trusted middleware connecting the blockchain and the real world, the blockchain oracle can obtain trusted real-time price information for financial applications such as payment and settlement, and asset valuation on the blockchain. However, the current oracle schemes face the dilemma of security and service quality in the process of node selection, and the implicit interest relationship in financial applications leads to a significant conflict of interest between the task publisher and the executor, which reduces the participation enthusiasm of both parties and system security. Therefore, this paper proposes an anonymous node selection scheme that anonymously selects nodes with high reputations to participate in tasks to ensure the security and service quality of nodes. Then, this paper also details the interest requirements and behavioral motives of all parties in the payment settlement and asset valuation scenarios. Under the hypothesis of rational man, an incentive mechanism based on the Stackelberg game is proposed. It can achieve equilibrium under the pursuit of the revenue of task publishers and executors, thereby ensuring the revenue of all types of users and improving the enthusiasm for participation. Finally, we verify the security of the proposed scheme through security analysis. The experimental results show that the proposed scheme can reduce the variance of obtaining price data by about 55\% while ensuring security, and meeting the revenue of all parties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04689v2</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youquan Xian, Xueying Zeng, Hao Wu, Danping Yang, Peng Wang, Peng Liu</dc:creator>
    </item>
    <item>
      <title>cedar: Optimized and Unified Machine Learning Input Data Pipelines</title>
      <link>https://arxiv.org/abs/2401.08895</link>
      <description>arXiv:2401.08895v3 Announce Type: replace-cross 
Abstract: The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources - or worse - underutilize expensive accelerators.
  To address these demands, we present cedar, an optimized and unified programming framework for ML input data pipelines. cedar allows users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. cedar introduces an extensible optimizer that systematically applies a complex combination of optimizations (e.g., offloading, caching, prefetching, fusion, and reordering). It orchestrates processing across a customizable set of local and distributed compute resources in order to improve processing performance and efficiency, all without user input. Across eight pipelines, cedar improves performance by up to 1.87x to 10.65x compared to state-of-the-art input data systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08895v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Zhao, Emanuel Adamiak, Christos Kozyrakis</dc:creator>
    </item>
  </channel>
</rss>
