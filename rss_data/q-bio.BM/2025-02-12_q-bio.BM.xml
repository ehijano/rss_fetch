<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 02:48:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Review of Protein Language Models</title>
      <link>https://arxiv.org/abs/2502.06881</link>
      <description>arXiv:2502.06881v1 Announce Type: new 
Abstract: At the intersection of the rapidly growing biological data landscape and advancements in Natural Language Processing (NLP), protein language models (PLMs) have emerged as a transformative force in modern research. These models have achieved remarkable progress, highlighting the need for timely and comprehensive overviews. However, much of the existing literature focuses narrowly on specific domains, often missing a broader analysis of PLMs. This study provides a systematic review of PLMs from a macro perspective, covering key historical milestones and current mainstream trends. We focus on the models themselves and their evaluation metrics, exploring aspects such as model architectures, positional encoding, scaling laws, and datasets. In the evaluation section, we discuss benchmarks and downstream applications. To further support ongoing research, we introduce relevant mainstream tools. Lastly, we critically examine the key challenges and limitations in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06881v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Wang, Xudong Li, Han Zhang, Jinyi Wang, Dingkang Jiang, Zhidong Xue, Yan Wang</dc:creator>
    </item>
    <item>
      <title>QuantumDNA: A Python Package for Analyzing Quantum Charge Dynamics in DNA and Exploring Its Biological Relevance</title>
      <link>https://arxiv.org/abs/2502.06883</link>
      <description>arXiv:2502.06883v1 Announce Type: new 
Abstract: The study of DNA charge dynamics is a highly interdisciplinary field that bridges physics, chemistry, biology, and medicine, and plays a critical role in processes such as DNA damage detection, protein-DNA interactions, and DNA-based nanotechnology. However, despite significant advances in each of these areas, knowledge often remains inaccessible to other scientific communities, limiting the broader impact of advances across disciplines. To bridge this gap, we present QuantumDNA, an open-source Python package for simulating DNA charge transfer (CT) and excited states using quantum-physical methods. QuantumDNA combines an efficient Linear Combination of Atomic Orbitals (LCAO) approach with tight-binding (TB) models, incorporating open quantum systems techniques to account for environmental effects. This approach allows rapid yet accurate analysis of large DNA ensembles, enabling statistical studies of genetic and epigenetic phenomena. To ensure accessibility, the package features a graphical user interface (GUI), making it suitable for researchers across disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06883v1</guid>
      <category>q-bio.BM</category>
      <category>physics.bio-ph</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Herb, Marco Trenti, Marilena Mantela, Constantinos Simserides, Joachim Ankerhold, Mirko Rossini</dc:creator>
    </item>
    <item>
      <title>ScaffoldGPT: A Scaffold-based Large Language Model for Drug Improvement</title>
      <link>https://arxiv.org/abs/2502.06891</link>
      <description>arXiv:2502.06891v1 Announce Type: new 
Abstract: Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Large Language Model (LLM) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization LLM-based generator on molecule scaffold with enhanced performance. (3) A token-level decoding optimization strategy, TOP-N, that enabling controlled, reward-guided generation using pretrained/finetuned LLMs. Finally, by conducting a comprehensive evaluation on COVID and cancer benchmarks, we demonstrate that SCAFFOLDGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving the original functional scaffold and enhancing desired properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06891v1</guid>
      <category>q-bio.BM</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefeng Liu, Songhao Jiang, Rick Stevens</dc:creator>
    </item>
    <item>
      <title>Steering Protein Family Design through Profile Bayesian Flow</title>
      <link>https://arxiv.org/abs/2502.07671</link>
      <description>arXiv:2502.07671v1 Announce Type: new 
Abstract: Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends the discrete Bayesian Flow Network from an MSA profile perspective, which can be trained on single protein sequences by regarding it as a degenerate profile, thereby achieving efficient protein family design by avoiding large-scale MSA data construction and training. Empirical results show that ProfileBFN has a profound understanding of proteins. When generating diverse and novel family proteins, it can accurately capture the structural characteristics of the family. The enzyme produced by this method is more likely than the previous approach to have the corresponding function, offering better odds of generating diverse proteins with the desired functionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07671v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingjing Gong, Yu Pei, Siyu Long, Yuxuan Song, Zhe Zhang, Wenhao Huang, Ziyao Cao, Shuyi Zhang, Hao Zhou, Wei-Ying Ma</dc:creator>
    </item>
    <item>
      <title>Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure</title>
      <link>https://arxiv.org/abs/2502.06846</link>
      <description>arXiv:2502.06846v1 Announce Type: cross 
Abstract: Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q&amp;A systems. To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. Our model incorporates a modified ProteinMPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. To optimize training efficiency, we freeze the encoder and employ LoRA techniques for the decoder. We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. Furthermore, zero-shot prediction results highlight its strong generalization capabilities. This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06846v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhicong Wang, Zicheng Ma, Ziqiang Cao, Changlong Zhou, Jun Zhang, Yiqin Gao</dc:creator>
    </item>
    <item>
      <title>DrugImproverGPT: A Large Language Model for Drug Optimization with Fine-Tuning via Structured Policy Optimization</title>
      <link>https://arxiv.org/abs/2502.07237</link>
      <description>arXiv:2502.07237v1 Announce Type: cross 
Abstract: Finetuning a Large Language Model (LLM) is crucial for generating results towards specific objectives. This research delves into the realm of drug optimization and introduce a novel reinforcement learning algorithm to finetune a drug optimization LLM-based generative model, enhancing the original drug across target objectives, while retains the beneficial chemical properties of the original drug. This work is comprised of two primary components: (1) DrugImprover: A framework tailored for improving robustness and efficiency in drug optimization. It includes a LLM designed for drug optimization and a novel Structured Policy Optimization (SPO) algorithm, which is theoretically grounded. This algorithm offers a unique perspective for fine-tuning the LLM-based generative model by aligning the improvement of the generated molecule with the input molecule under desired objectives. (2) A dataset of 1 million compounds, each with OEDOCK docking scores on 5 human proteins associated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We conduct a comprehensive evaluation of SPO and demonstrate its effectiveness in improving the original drug across target properties. Our code and dataset will be publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07237v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>q-bio.BM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian Foster, Rick Stevens</dc:creator>
    </item>
    <item>
      <title>Higher-Order Message Passing for Glycan Representation Learning</title>
      <link>https://arxiv.org/abs/2409.13467</link>
      <description>arXiv:2409.13467v3 Announce Type: replace-cross 
Abstract: Glycans are the most complex biological sequence, with monosaccharides forming extended, non-linear sequences. As post-translational modifications, they modulate protein structure, function, and interactions. Due to their diversity and complexity, predictive models of glycan properties and functions are still insufficient.
  Graph Neural Networks (GNNs) are deep learning models designed to process and analyze graph-structured data. These architectures leverage the connectivity and relational information in graphs to learn effective representations of nodes, edges, and entire graphs. Iteratively aggregating information from neighboring nodes, GNNs capture complex patterns within graph data, making them particularly well-suited for tasks such as link prediction or graph classification across domains.
  This work presents a new model architecture based on combinatorial complexes and higher-order message passing to extract features from glycan structures into a latent space representation. The architecture is evaluated on an improved GlycanML benchmark suite, establishing a new state-of-the-art performance. We envision that these improvements will spur further advances in computational glycosciences and reveal the roles of glycans in biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13467v3</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Joeres, Daniel Bojar</dc:creator>
    </item>
  </channel>
</rss>
