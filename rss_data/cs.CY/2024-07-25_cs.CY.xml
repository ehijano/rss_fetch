<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Promoting Health via mHealth Applications Using a French Version of the Mobile App Rating Scale: Adaptation and Validation Study</title>
      <link>https://arxiv.org/abs/2407.17472</link>
      <description>arXiv:2407.17472v1 Announce Type: new 
Abstract: Background In the recent decades, the number of apps promoting health behaviors and health-related strategies and interventions has increased alongside the number of smartphone users. Nevertheless, the validity process for measuring and reporting app quality remains unsatisfactory for health professionals and end users and represents a public health concern. The Mobile Application Rating Scale (MARS) is a tool validated and widely used in the scientific literature to evaluate and compare mHealth app functionalities. However, MARS is not adapted to the French culture nor to the language. Objective This study aims to translate, adapt, and validate the equivalent French version of MARS (ie, MARS-F). Methods The original MARS was first translated to French by two independent bilingual scientists, and their common version was blind back-translated twice by two native English speakers, culminating in a final well-established MARS-F. Its comprehensibility was then evaluated by 6 individuals (3 researchers and 3 nonacademics), and the final MARS-F version was created. Two bilingual raters independently completed the evaluation of 63 apps using MARS and MARS-F. Interrater reliability was assessed using intraclass correlation coefficients. In addition, internal consistency and validity of both scales were assessed. Mokken scale analysis was used to investigate the scalability of both MARS and MARS-F. Results MARS-F had a good alignment with the original MARS, with properties comparable between the two scales. The correlation coefficients (r) between the corresponding dimensions of MARS and MARS-F ranged from 0.97 to 0.99. The internal consistencies of the MARS-F dimensions engagement ($\omega$=0.79), functionality ($\omega$=0.79), esthetics ($\omega$=0.78), and information quality ($\omega$=0.61) were acceptable and that for the overall MARS score ($\omega$=0.86) was good. Mokken scale analysis revealed a strong scalability for MARS (Loevinger H=0.37) and a good scalability for MARS-F (H=0.35). Conclusions MARS-F is a valid tool, and it would serve as a crucial aid for researchers, health care professionals, public health authorities, and interested third parties, to assess the quality of mHealth apps in French-speaking countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17472v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.2196/30480</arxiv:DOI>
      <arxiv:journal_reference>JMIR mHealth and uHealth, 2021, 9 (8), pp.e30480</arxiv:journal_reference>
      <dc:creator>Ina Saliasi (P2S), Prescilla Martinon (P2S), Emily Darlington (P2S), Colette Smentek (P2S), Delphine Tardivo (ADES, APHM, AMU ODONTO), Denis Bourgeois (P2S), Claude Dussart (P2S), Florence Carrouel (P2S), Laurie Fraticelli (P2S)</dc:creator>
    </item>
    <item>
      <title>Improving engagement, diversity, and retention in computer science with RadGrad: Results of a case study</title>
      <link>https://arxiv.org/abs/2407.17473</link>
      <description>arXiv:2407.17473v1 Announce Type: new 
Abstract: RadGrad is a curriculum initiative implemented via an application that combines features of social networks, degree planners, individual learning plans, and serious games. RadGrad redefines traditional meanings of "progress" and "success" in the undergraduate computer science degree program in an attempt to improve engagement, retention, and diversity. In this paper, we describe the RadGrad Project and report on an evaluation study designed to assess the impact of RadGrad on student engagement, diversity, and retention. We also present opportunities and challenges that result from the use of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17473v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip M. Johnson, Carleton Moore, Peter Leong, Seungoh Paek</dc:creator>
    </item>
    <item>
      <title>"My Kind of Woman": Analysing Gender Stereotypes in AI through The Averageness Theory and EU Law</title>
      <link>https://arxiv.org/abs/2407.17474</link>
      <description>arXiv:2407.17474v1 Announce Type: new 
Abstract: This study delves into gender classification systems, shedding light on the interaction between social stereotypes and algorithmic determinations. Drawing on the "averageness theory," which suggests a relationship between a face's attractiveness and the human ability to ascertain its gender, we explore the potential propagation of human bias into artificial intelligence (AI) systems. Utilising the AI model Stable Diffusion 2.1, we have created a dataset containing various connotations of attractiveness to test whether the correlation between attractiveness and accuracy in gender classification observed in human cognition persists within AI. Our findings indicate that akin to human dynamics, AI systems exhibit variations in gender classification accuracy based on attractiveness, mirroring social prejudices and stereotypes in their algorithmic decisions. This discovery underscores the critical need to consider the impacts of human perceptions on data collection and highlights the necessity for a multidisciplinary and intersectional approach to AI development and AI data training. By incorporating cognitive psychology and feminist legal theory, we examine how data used for AI training can foster gender diversity and fairness under the scope of the AI Act and GDPR, reaffirming how psychological and feminist legal theories can offer valuable insights for ensuring the protection of gender equality and non-discrimination in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17474v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miriam Doh, and Anastasia Karagianni</dc:creator>
    </item>
    <item>
      <title>An Approach to Detect Abnormal Submissions for CodeWorkout Dataset</title>
      <link>https://arxiv.org/abs/2407.17475</link>
      <description>arXiv:2407.17475v1 Announce Type: new 
Abstract: Students interactions while solving problems in learning environments (i.e. log data) are often used to support students learning. For example, researchers use log data to develop systems that can provide students with personalized problem recommendations based on their knowledge level. However, anomalies in the students log data, such as cheating to solve programming problems, could introduce a hidden bias in the log data. As a result, these systems may provide inaccurate problem recommendations, and therefore, defeat their purpose. Classical cheating detection methods, such as MOSS, can be used to detect code plagiarism. However, these methods cannot detect other abnormal events such as a student gaming a system with multiple attempts of similar solutions to a particular programming problem. This paper presents a preliminary study to analyze log data with anomalies. The goal of our work is to overcome the abnormal instances when modeling personalizable recommendations in programming learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17475v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alex Hicks, Yang Shi, Arun-Balajiee Lekshmi-Narayanan, Wei Yan, Samiha Marwan</dc:creator>
    </item>
    <item>
      <title>ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems</title>
      <link>https://arxiv.org/abs/2407.17476</link>
      <description>arXiv:2407.17476v1 Announce Type: new 
Abstract: Cognitive diagnosis models (CDMs) are designed to learn students' mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers' guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students' mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs' effectiveness in downstream tasks. CDMs comprise two core parts: learning students' mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models' prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17476v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>KDD 2024</arxiv:journal_reference>
      <dc:creator>Hong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, Aimin Zhou</dc:creator>
    </item>
    <item>
      <title>Toward Automated Detection of Biased Social Signals from the Content of Clinical Conversations</title>
      <link>https://arxiv.org/abs/2407.17477</link>
      <description>arXiv:2407.17477v1 Announce Type: new 
Abstract: Implicit bias can impede patient-provider interactions and lead to inequities in care. Raising awareness is key to reducing such bias, but its manifestations in the social dynamics of patient-provider communication are difficult to detect. In this study, we used automated speech recognition (ASR) and natural language processing (NLP) to identify social signals in patient-provider interactions. We built an automated pipeline to predict social signals from audio recordings of 782 primary care visits that achieved 90.1% average accuracy across codes, and exhibited fairness in its predictions for white and non-white patients. Applying this pipeline, we identified statistically significant differences in provider communication behavior toward white versus non-white patients. In particular, providers expressed more patient-centered behaviors towards white patients including more warmth, engagement, and attentiveness. Our study underscores the potential of automated tools in identifying subtle communication signals that may be linked with bias and impact healthcare quality and equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17477v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Chen, Manas Satish Bedmutha, Ray-Yuan Chung, Janice Sabin, Wanda Pratt, Brian R. Wood, Nadir Weibel, Andrea L. Hartzler, Trevor Cohen</dc:creator>
    </item>
    <item>
      <title>Human Oversight of Artificial Intelligence and Technical Standardisation</title>
      <link>https://arxiv.org/abs/2407.17481</link>
      <description>arXiv:2407.17481v1 Announce Type: new 
Abstract: The adoption of human oversight measures makes it possible to regulate, to varying degrees and in different ways, the decision-making process of Artificial Intelligence (AI) systems, for example by placing a human being in charge of supervising the system and, upstream, by developing the AI system to enable such supervision. Within the global governance of AI, the requirement for human oversight is embodied in several regulatory formats, within a diversity of normative sources. On the one hand, it reinforces the accountability of AI systems' users (for example, by requiring them to carry out certain checks) and, on the other hand, it better protects the individuals affected by the AI-based decision (for example, by allowing them to request a review of the decision). In the European context, the AI Act imposes obligations on providers of high-risk AI systems (and to some extent also on professional users of these systems, known as deployers), including the introduction of human oversight tools throughout the life cycle of AI systems, including by design (and their implementation by deployers). The EU legislator is therefore going much further than in the past in "spelling out" the legal requirement for human oversight. But it does not intend to provide for all implementation details; it calls on standardisation to technically flesh out this requirement (and more broadly all the requirements of section 2 of chapter III) on the basis of article 40 of the AI Act. In this multi-level regulatory context, the question of the place of humans in the AI decision-making process should be given particular attention. Indeed, depending on whether it is the law or the technical standard that sets the contours of human oversight, the "regulatory governance" of AI is not the same: its nature, content and scope are different. This analysis is at the heart of the contribution made (or to be made) by legal experts to the central reflection on the most appropriate regulatory governance -- in terms of both its institutional format and its substance -- to ensure the effectiveness of human oversight and AI trustworthiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17481v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marion Ho-Dac (UA, CDEP), Baptiste Martinez (UA, CDEP)</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning from Human Feedback: Whose Culture, Whose Values, Whose Perspectives?</title>
      <link>https://arxiv.org/abs/2407.17482</link>
      <description>arXiv:2407.17482v1 Announce Type: new 
Abstract: We argue for the epistemic and ethical advantages of pluralism in Reinforcement Learning from Human Feedback (RLHF) in the context of Large Language Models (LLM). Drawing on social epistemology and pluralist philosophy of science, we suggest ways in which RHLF can be made more responsive to human needs and how we can address challenges along the way. The paper concludes with an agenda for change, i.e. concrete, actionable steps to improve LLM development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17482v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Gonz\'alez Barman, Simon Lohse, Henk de Regt</dc:creator>
    </item>
    <item>
      <title>Tackling CS education in K-12: Implementing a Google CS4HS Grant Program in a Rural Underserved Area</title>
      <link>https://arxiv.org/abs/2407.17483</link>
      <description>arXiv:2407.17483v1 Announce Type: new 
Abstract: Providing computer science (CS) offerings in the K-12 education system is often limited by the lack of experienced teachers, especially in small or rural underserved school districts. By helping teachers in underserved areas develop CS curriculum and helping them become certified to teach CS courses, more young people in underserved areas are aware of IT-career opportunities, and prepared for CS education at the university level, which ultimately helps tackle the IT workforce deficit in the United States.
  This paper discusses a successful implementation of a Google CS4HS grant to a rural underserved area, as well as lessons learned through the implementation of the program. Key elements in the implementation included a face-to-face hands-on workshop, followed by a seven week graduate-level online summer course for the teachers to learn and develop curriculum that covers the CS concepts they will be teaching. The teachers were supported with an online community of practice for the year as they implemented the curriculum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17483v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Harms, S. K. (2017). Tackling CS education in K-12: Implementing a Google CS4HS Grant Program. La Crosse, WI: 2017 Midwest Instructional Computing Symposium Proceedings</arxiv:journal_reference>
      <dc:creator>Sherri Harms</dc:creator>
    </item>
    <item>
      <title>Explainable Natural Language Processing for Corporate Sustainability Analysis</title>
      <link>https://arxiv.org/abs/2407.17487</link>
      <description>arXiv:2407.17487v1 Announce Type: new 
Abstract: Sustainability commonly refers to entities, such as individuals, companies, and institutions, having a non-detrimental (or even positive) impact on the environment, society, and the economy. With sustainability becoming a synonym of acceptable and legitimate behaviour, it is being increasingly demanded and regulated. Several frameworks and standards have been proposed to measure the sustainability impact of corporations, including United Nations' sustainable development goals and the recently introduced global sustainability reporting framework, amongst others. However, the concept of corporate sustainability is complex due to the diverse and intricate nature of firm operations (i.e. geography, size, business activities, interlinks with other stakeholders). As a result, corporate sustainability assessments are plagued by subjectivity both within data that reflect corporate sustainability efforts (i.e. corporate sustainability disclosures) and the analysts evaluating them. This subjectivity can be distilled into distinct challenges, such as incompleteness, ambiguity, unreliability and sophistication on the data dimension, as well as limited resources and potential bias on the analyst dimension. Put together, subjectivity hinders effective cost attribution to entities non-compliant with prevailing sustainability expectations, potentially rendering sustainability efforts and its associated regulations futile. To this end, we argue that Explainable Natural Language Processing (XNLP) can significantly enhance corporate sustainability analysis. Specifically, linguistic understanding algorithms (lexical, semantic, syntactic), integrated with XAI capabilities (interpretability, explainability, faithfulness), can bridge gaps in analyst resources and mitigate subjectivity problems within data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17487v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keane Ong, Rui Mao, Ranjan Satapathy, Ricardo Shirota Filho, Erik Cambria, Johan Sulaeman, Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>Unveiling Legitimacy in the unexpected events context : An Inquiry into Information System Consultancy companies and international organizations through Topic Modeling Analysis</title>
      <link>https://arxiv.org/abs/2407.17509</link>
      <description>arXiv:2407.17509v1 Announce Type: new 
Abstract: In an increasingly dynamic and modern market, the recurrence of unexpected events necessitates proactive responses from information system (IS) stakeholders. Each IS actor strives to legitimize its actions and communicate its strategy. This study delves into the realm of IS legitimation, focusing on the communication of two key stakeholders: IS consultancy companies and international organizations, particularly in the context of unexpected events. To achieve this objective, we examined a diverse array of publications released by both actors. Employing a topic modeling methodology, we analyzed these documents to extract valuable insights regarding their methods of legitimation. Through this research, we aim to contribute to the legitimation discourse literature by offering an exploration of  two key IS stakeholders responding to the challenges posed by unexpected events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17509v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oussama Abidi (AMU ECO, CERGAM)</dc:creator>
    </item>
    <item>
      <title>Theorizing neuro-induced relationships between cognitive diversity, motivation, grit and academic performance in multidisciplinary engineering education context</title>
      <link>https://arxiv.org/abs/2407.17584</link>
      <description>arXiv:2407.17584v1 Announce Type: new 
Abstract: Nowadays, engineers need to tackle many unprecedented challenges that are often complex, and, most importantly, cannot be exhaustively compartmentalized into a single engineering discipline. In other words, most engineering problems need to be solved from a multidisciplinary approach. However, conventional engineering programs usually adopt pedagogical approaches specifically tailored to traditional, niched engineering disciplines, which become increasingly deviated from the industry needs as those programs are typically designed and taught by instructors with highly specialized engineering training and credentials. To reduce the gap, more multidisciplinary engineering programs emerge by systematically stretching across all engineering fibers, and challenge the sub-optimal traditional pedagogy crowded in engineering classrooms. To further advance future-oriented pedagogy, in this work, we hypothesized neuro-induced linkages between how cognitively different learners are and how the linkages would affect learners in the knowledge acquisition process. We situate the neuro-induced linkages in the context of multidisciplinary engineering education and propose possible pedagogical approaches to actualize the implications of this conceptual framework. Our study, based on the innovative concept of brain fingerprint, would serve as a pioneer model to theorize key components of learner-centered multidisciplinary engineering pedagogy which centers on the key question: how do we motivate engineering students of different backgrounds from a neuro-inspired perspective?</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17584v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duy Duong-Tran, Siqing Wei, Li Shen</dc:creator>
    </item>
    <item>
      <title>Quelle {\'e}thique pour quelle IA ?</title>
      <link>https://arxiv.org/abs/2407.17585</link>
      <description>arXiv:2407.17585v1 Announce Type: new 
Abstract: This study proposes an analysis of the different types of ethical approaches involved in the ethics of AI, and situates their interests and limits. First, the author introduces to the contemporary need for and meaning of ethics. He distinguishes it from other registers of normativities and underlines its inadequacy to formalization. He then presents a cartography of the landscape of ethical theories covered by moral philosophy, taking care to distinguish meta-ethics, normative ethics and applied ethics. In drawing up this overview, the author questions the relationship between ethics and artificial intelligence. The analysis focuses in particular on the main ethical currents that have imposed themselves in the ways of doing digital ethics and AI in our Western democracies. The author asks whether these practices of ethics, as they seem to crystallize today in a precise pattern, constitute a sufficient and sufficiently satisfactory response to our needs for ethics in AI. The study concludes with a reflection on the reasons why a human ethics of AI based on a pragmatic practice of contextual ethics remains necessary and irreducible to any formalization or automated treatment of the ethical questions that arise for humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17585v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Doat (ETHICS EA 7446)</dc:creator>
    </item>
    <item>
      <title>Development of Autonomous Artificial Intelligence Systems for Corporate Management</title>
      <link>https://arxiv.org/abs/2407.17588</link>
      <description>arXiv:2407.17588v1 Announce Type: new 
Abstract: The article discusses development of autonomous artificial intelligence systems for corporate management. The function of a corporate director is still one of the few that are legislated for execution by a "natural" rather than an "artificial" person. The main prerequisites for development of systems for full automation of management decisions made at the level of a board of directors are formed in the field of corporate law, machine learning, and compliance with the rules of non-discrimination, transparency, and accountability of decisions made and algorithms applied. The basic methodological approaches in terms of corporate law for the "autonomous director" have already been developed and do not get rejection among representatives of the legal sciences. However, there is an undeniable need for further extensive research in order to amend corporate law to effectively introduce "autonomous directors". In practice, there are two main options of management decisions automation at the level of top management and a board of directors: digital command centers or automation of separate functions. Artificial intelligence systems will be subject to the same strict requirements for non-discrimination, transparency, and accountability as "natural" directors. At a certain stage, autonomous systems can be an effective tool for countries, regions, and companies with a shortage of human capital, equalizing or providing additional chances for such countries and companies to compete on the global market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17588v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.18254/S207751800024942-5</arxiv:DOI>
      <arxiv:journal_reference>Artificial societies. 2023. V. 18. Issue 2</arxiv:journal_reference>
      <dc:creator>Anna Romanova</dc:creator>
    </item>
    <item>
      <title>Is computational creativity flourishing on the dead internet?</title>
      <link>https://arxiv.org/abs/2407.17590</link>
      <description>arXiv:2407.17590v1 Announce Type: new 
Abstract: The dead internet theory is a conspiracy theory that states that all interactions and posts on social media are no longer being made by real people, but rather by autonomous bots. While the theory is obviously not true, an increasing amount of posts on social media have been made by bots optimised to gain followers and drive engagement on social media platforms. This paper looks at the recent phenomenon of these bots, analysing their behaviour through the lens of computational creativity to investigate the question: is computational creativity flourishing on the dead internet?</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17590v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Terence Broad</dc:creator>
    </item>
    <item>
      <title>Unified Prediction Model for Employability in Indian Higher Education System</title>
      <link>https://arxiv.org/abs/2407.17591</link>
      <description>arXiv:2407.17591v1 Announce Type: new 
Abstract: Educational Data Mining has become extremely popular among researchers in last decade. Prior effort in this area was only directed towards prediction of academic performance of a student. Very less number of researches are directed towards predicting employability of a student i.e. prediction of students performance in campus placements at an early stage of enrollment. Furthermore, existing researches on students employability prediction are not universal in approach and is either based upon only one type of course or University/Institute. Henceforth, is not scalable from one context to another. With the necessity of unification, data of professional technical courses namely Bachelor in Engineering/Technology and Masters in Computer Applications students have been collected from 17 states of India. To deal with such a data, a unified predictive model has been developed and applied on 17 states datasets. The research done in this paper proves that model has universal application and can be applied to various states and institutes pan India with different cultural background and course structure. This paper also explores and proves statistically that there is no significant difference in Indian Education System with respect to states as far as prediction of employability of students is concerned. Model provides a generalized solution for student employability prediction in Indian Scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17591v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pooja Thakar, Anil Mehta,  Manisha</dc:creator>
    </item>
    <item>
      <title>Productive self/vulnerable body: self-tracking, overworking culture, and conflicted data practices</title>
      <link>https://arxiv.org/abs/2407.17618</link>
      <description>arXiv:2407.17618v1 Announce Type: new 
Abstract: Self-tracking, the collection, analysis, and interpretation of personal data, signifies an individualized way of health governance as people are demanded to build a responsible self by internalizing norms. However, the technological promises often bear conflicts with various social factors such as a strenuous schedule, a lack of motivation, stress, and anxieties, which fail to deliver health outcomes. To re-problematize the phenomenon, this paper situates self-tracking in an overworking culture in China and draws on semi structured and in depth interviews with overworking individuals to reveal the patterns in users interactions and interpretations with self-tracking data. It builds on the current literature of self-tracking and engages with theories from Science and Technology Studies, especially sociomaterial assemblages (Lupton 2016) and technological mediation (Verbeek 2005), to study self-tracking in a contextualized way which connects the micro (data reading, visualization, and affective elements in design) with the macro (work and workplaces, socioeconomic and political background) contexts of self-tracking. Drawing on investigation of the social context that users of self-tracking technologies internalize, reflect, or resist, the paper argues that the productivity and value oriented assumptions and workplace culture shape the imaginary of intensive (and sometimes impossible) self-care and health, an involution of competence embedded in the technological design and users affective experiences. Users respond by enacting different design elements and social contexts to frame two distinctive data practices of self-tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17618v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elise Li Zheng</dc:creator>
    </item>
    <item>
      <title>PICA: A Data-driven Synthesis of Peer Instruction and Continuous Assessment</title>
      <link>https://arxiv.org/abs/2407.17633</link>
      <description>arXiv:2407.17633v1 Announce Type: new 
Abstract: Peer Instruction (PI) and Continuous Assessment(CA) are two distinct educational techniques with extensive research demonstrating their effectiveness. The work herein combines PI and CA in a deliberate and novel manner to pair students together for a PI session in which they collaborate on a CA task. The data used to inform the pairing method is restricted to the most previous CA task students completed independently. The motivation for this data-driven collaborative learning is to improve student learning, communication, and engagement. Quantitative results from an investigation of the method show improved assessment scores on the PI CA tasks, although evidence of a positive effect on subsequent individual CA tasks was not statistically significant as anticipated. However, student perceptions were positive, engagement was high, and students interacted with a broader set of peers than is typical. These qualitative observations, together with extant research on the general benefits of improving student engagement and communication (e.g. improved sense of belonging, increased social capital, etc.), render the method worthy for further research into building and evaluating small student learning communities using student assessment data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17633v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Steve Geinitz</dc:creator>
    </item>
    <item>
      <title>Women's Participation in Computing: Evolving Research Methods</title>
      <link>https://arxiv.org/abs/2407.17677</link>
      <description>arXiv:2407.17677v1 Announce Type: new 
Abstract: A 2022 keynote for the ACM History Committee on "Why SIG History Matters: New Data on Gender Bias in ACM's Founding SIGs 1970-2000" presented new data describing women's participation as research-article authors in 13 early ACM Special Interest Groups, finding significant growth in women's participation across 1970-2000 and, additionally, remarkable differences in women's participation between the SIGs. That presentation built on several earlier publications that developed a research method for assessing the number of women computer scientists that [a] are chronologically prior to the availability of the Bureau of Labor Statistics (BLS) data on women in the IT workforce; and [b] permit focused investigation of varied sub-fields within computing. This present report expands on these earlier articles, and their evolving research method, connecting them to the ACM SIG Heritage presentation. It also outlines some of the choices and considerations made in developing and refining "mixed methods" research (using both quantitative and qualitative approaches) as well as extensions of the research being currently explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17677v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas J. Misa</dc:creator>
    </item>
    <item>
      <title>Instagram versus women of color: Why are women of color protesting Instagram's algorithmic changes?</title>
      <link>https://arxiv.org/abs/2407.17679</link>
      <description>arXiv:2407.17679v1 Announce Type: new 
Abstract: Instagram has been appropriated by communities for several contemporary social struggles, often translating into real world action. Likewise, women of color (WOC) have used it to protest, share information and support one another through its various affordances. However, Instagram is known to have frequent updates, and recently the updates have been more drastic. The newest update changed the recommendation algorithm such that it showed video-oriented content (reels) from unknown accounts over static media from a user's own network. Several marginalized communities, and especially WOC resisted this change and others that led to it. Due to the backlash, Instagram rolled back its changes. Drawing from past HCI work on digital platforms for marginalised communities, I propose a qualitative study informed by the open research strategy to understand why WOC are resisting these changes, and eventually provide implications for design that can help implement changes in a more inclusive manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17679v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ankolika De</dc:creator>
    </item>
    <item>
      <title>GermanPartiesQA: Benchmarking Commercial Large Language Models for Political Bias and Sycophancy</title>
      <link>https://arxiv.org/abs/2407.18008</link>
      <description>arXiv:2407.18008v1 Announce Type: new 
Abstract: LLMs are changing the way humans create and interact with content, potentially affecting citizens' political opinions and voting decisions. As LLMs increasingly shape our digital information ecosystems, auditing to evaluate biases, sycophancy, or steerability has emerged as an active field of research. In this paper, we evaluate and compare the alignment of six LLMs by OpenAI, Anthropic, and Cohere with German party positions and evaluate sycophancy based on a prompt experiment. We contribute to evaluating political bias and sycophancy in multi-party systems across major commercial LLMs. First, we develop the benchmark dataset GermanPartiesQA based on the Voting Advice Application Wahl-o-Mat covering 10 state and 1 national elections between 2021 and 2023. In our study, we find a left-green tendency across all examined LLMs. We then conduct our prompt experiment for which we use the benchmark and sociodemographic data of leading German parliamentarians to evaluate changes in LLMs responses. To differentiate between sycophancy and steerabilty, we use 'I am [politician X], ...' and 'You are [politician X], ...' prompts. Against our expectations, we do not observe notable differences between prompting 'I am' and 'You are'. While our findings underscore that LLM responses can be ideologically steered with political personas, they suggest that observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18008v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci</dc:creator>
    </item>
    <item>
      <title>Unraveling the Web of Disinformation: Exploring the Larger Context of State-Sponsored Influence Campaigns on Twitter</title>
      <link>https://arxiv.org/abs/2407.18098</link>
      <description>arXiv:2407.18098v1 Announce Type: new 
Abstract: Social media platforms offer unprecedented opportunities for connectivity and exchange of ideas; however, they also serve as fertile grounds for the dissemination of disinformation. Over the years, there has been a rise in state-sponsored campaigns aiming to spread disinformation and sway public opinion on sensitive topics through designated accounts, known as troll accounts. Past works on detecting accounts belonging to state-backed operations focus on a single campaign. While campaign-specific detection techniques are easier to build, there is no work done on developing systems that are campaign-agnostic and offer generalized detection of troll accounts unaffected by the biases of the specific campaign they belong to. In this paper, we identify several strategies adopted across different state actors and present a system that leverages them to detect accounts from previously unseen campaigns. We study 19 state-sponsored disinformation campaigns that took place on Twitter, originating from various countries. The strategies include sending automated messages through popular scheduling services, retweeting and sharing selective content and using fake versions of verified applications for pushing content. By translating these traits into a feature set, we build a machine learning-based classifier that can correctly identify up to 94% of accounts from unseen campaigns. Additionally, we run our system in the wild and find more accounts that could potentially belong to state-backed operations. We also present case studies to highlight the similarity between the accounts found by our system and those identified by Twitter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18098v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2024)</arxiv:journal_reference>
      <dc:creator>Mohammad Hammas Saeed, Shiza Ali, Pujan Paudel, Jeremy Blackburn, Gianluca Stringhini</dc:creator>
    </item>
    <item>
      <title>Tool-Assisted Learning of Computational Reductions</title>
      <link>https://arxiv.org/abs/2407.18215</link>
      <description>arXiv:2407.18215v1 Announce Type: new 
Abstract: Computational reductions are an important and powerful concept in computer science. However, they are difficult for many students to grasp. In this paper, we outline a concept for how the learning of reductions can be supported by educational support systems. We present an implementation of the concept within such a system, concrete web-based and interactive learning material for reductions, and report on our experiences using the material in a large introductory course on theoretical computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18215v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tristan Kneisel, Elias Radtke, Marko Schmellenkamp, Fabian Vehlken, Thomas Zeume</dc:creator>
    </item>
    <item>
      <title>Real-Time Automated donning and doffing detection of PPE based on Yolov4-tiny</title>
      <link>https://arxiv.org/abs/2407.17471</link>
      <description>arXiv:2407.17471v1 Announce Type: cross 
Abstract: Maintaining patient safety and the safety of healthcare workers (HCWs) in hospitals and clinics highly depends on following the proper protocol for donning and taking off personal protective equipment (PPE). HCWs can benefit from a feedback system during the putting on and removal process because the process is cognitively demanding and errors are common. Centers for Disease Control and Prevention (CDC) provided guidelines for correct PPE use which should be followed. A real time object detection along with a unique sequencing algorithms are used to identify and determine the donning and doffing process in real time. The purpose of this technical research is two-fold: The user gets real time alert to the step they missed in the sequence if they don't follow the proper procedure during donning or doffing. Secondly, the use of tiny machine learning (yolov4-tiny) in embedded system architecture makes it feasible and cost-effective to deploy in different healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17471v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anusha Verma, Ghazal Ghajari, K M Tawsik Jawad, Dr. Hugh P. Salehi, Dr. Fathi Amsaad</dc:creator>
    </item>
    <item>
      <title>LLM4PM: A case study on using Large Language Models for Process Modeling in Enterprise Organizations</title>
      <link>https://arxiv.org/abs/2407.17478</link>
      <description>arXiv:2407.17478v1 Announce Type: cross 
Abstract: We investigate the potential of using Large Language Models (LLM) to support process model creation in organizational contexts. Specifically, we carry out a case study wherein we develop and test an LLM-based chatbot, PRODIGY (PROcess moDellIng Guidance for You), in a multinational company, the Hilti Group. We are particularly interested in understanding how LLM can aid (human) modellers in creating process flow diagrams. To this purpose, we first conduct a preliminary user study (n=10) with professional process modellers from Hilti, inquiring for various pain-points they encounter in their daily routines. Then, we use their responses to design and implement PRODIGY. Finally, we evaluate PRODIGY by letting our user study's participants use PRODIGY, and then ask for their opinion on the pros and cons of PRODIGY. We coalesce our results in actionable takeaways. Through our research, we showcase the first practical application of LLM for process modelling in the real world, shedding light on how industries can leverage LLM to enhance their Business Process Management activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17478v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Clara Ziche, Giovanni Apruzzese</dc:creator>
    </item>
    <item>
      <title>A Survey of Accessible Explainable Artificial Intelligence Research</title>
      <link>https://arxiv.org/abs/2407.17484</link>
      <description>arXiv:2407.17484v1 Announce Type: cross 
Abstract: The increasing integration of Artificial Intelligence (AI) into everyday life makes it essential to explain AI-based decision-making in a way that is understandable to all users, including those with disabilities. Accessible explanations are crucial as accessibility in technology promotes digital inclusion and allows everyone, regardless of their physical, sensory, or cognitive abilities, to use these technologies effectively. This paper presents a systematic literature review of the research on the accessibility of Explainable Artificial Intelligence (XAI), specifically considering persons with sight loss. Our methodology includes searching several academic databases with search terms to capture intersections between XAI and accessibility. The results of this survey highlight the lack of research on Accessible XAI (AXAI) and stress the importance of including the disability community in XAI development to promote digital inclusion and accessibility and remove barriers. Most XAI techniques rely on visual explanations, such as heatmaps or graphs, which are not accessible to persons who are blind or have low vision. Therefore, it is necessary to develop explanation methods through non-visual modalities, such as auditory and tactile feedback, visual modalities accessible to persons with low vision, and personalized solutions that meet the needs of individuals, including those with multiple disabilities. We further emphasize the importance of integrating universal design principles into AI development practices to ensure that AI technologies are usable by everyone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17484v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chukwunonso Henry Nwokoye, Maria J. P. Peixoto, Akriti Pandey, Lauren Pardy, Mahadeo Sukhai, Peter R. Lewis</dc:creator>
    </item>
    <item>
      <title>Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning</title>
      <link>https://arxiv.org/abs/2407.17543</link>
      <description>arXiv:2407.17543v1 Announce Type: cross 
Abstract: The influence of bias in datasets on the fairness of model predictions is a topic of ongoing research in various fields. We evaluate the performance of skin lesion classification using ResNet-based CNNs, focusing on patient sex variations in training data and three different learning strategies. We present a linear programming method for generating datasets with varying patient sex and class labels, taking into account the correlations between these variables. We evaluated the model performance using three different learning strategies: a single-task model, a reinforcing multi-task model, and an adversarial learning scheme. Our observations include: 1) sex-specific training data yields better results, 2) single-task models exhibit sex bias, 3) the reinforcement approach does not remove sex bias, 4) the adversarial model eliminates sex bias in cases involving only female patients, and 5) datasets that include male patients enhance model performance for the male subgroup, even when female patients are the majority. To generalise these findings, in future research, we will examine more demographic attributes, like age, and other possibly confounding factors, such as skin colour and artefacts in the skin lesions. We make all data and models available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17543v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ralf Raumanns, Gerard Schouten, Josien P. W. Pluim, Veronika Cheplygina</dc:creator>
    </item>
    <item>
      <title>Big5PersonalityEssays: Introducing a Novel Synthetic Generated Dataset Consisting of Short State-of-Consciousness Essays Annotated Based on the Five Factor Model of Personality</title>
      <link>https://arxiv.org/abs/2407.17586</link>
      <description>arXiv:2407.17586v1 Announce Type: cross 
Abstract: Given the high advances of large language models (LLM) it is of vital importance to study their behaviors and apply their utility in all kinds of scientific fields. Psychology has been, in recent years, poorly approached using novel computational tools. One of the reasons is the high complexity of the data required for a proper analysis. Moreover, psychology, with a focus on psychometry, has few datasets available for analysis and artificial intelligence usage. Because of these facts, this study introduces a synthethic database of short essays labeled based on the five factor model (FFM) of personality traits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17586v1</guid>
      <category>cs.OH</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iustin Floroiu</dc:creator>
    </item>
    <item>
      <title>Towards Neural Network based Cognitive Models of Dynamic Decision-Making by Humans</title>
      <link>https://arxiv.org/abs/2407.17622</link>
      <description>arXiv:2407.17622v1 Announce Type: cross 
Abstract: Modelling human cognitive processes in dynamic decision-making tasks has been an endeavor in AI for a long time. Some initial works have attempted to utilize neural networks (and large language models) but often assume one common model for all humans and aim to emulate human behavior in aggregate. However, behavior of each human is distinct, heterogeneous and relies on specific past experiences in specific tasks. To that end, we build on a well known model of cognition, namely Instance Based Learning (IBL), that posits that decisions are made based on similar situations encountered in the past. We propose two new attention based neural network models to model human decision-making in dynamic settings. We experiment with two distinct datasets gathered from human subject experiment data, one focusing on detection of phishing email by humans and another where humans act as attackers in a cybersecurity setting and decide on an attack option. We conduct extensive experiments with our two neural network models, IBL, and GPT3.5, and demonstrate that one of our neural network models achieves the best performance in representing human decision-making. We find an interesting trend that all models predict a human's decision better if that human is better at the task. We also explore explanation of human decisions based on what our model considers important in prediction. Overall, our work yields promising results for further use of neural networks in cognitive modelling of human decision making. Our code is available at https://github.com/shshnkreddy/NCM-HDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17622v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changyu Chen, Shashank Reddy Chirra, Maria Jos\'e Ferreira, Cleotilde Gonzalez, Arunesh Sinha, Pradeep Varakantham</dc:creator>
    </item>
    <item>
      <title>Graph Neural Ordinary Differential Equations for Coarse-Grained Socioeconomic Dynamics</title>
      <link>https://arxiv.org/abs/2407.18108</link>
      <description>arXiv:2407.18108v1 Announce Type: cross 
Abstract: We present a data-driven machine-learning approach for modeling space-time socioeconomic dynamics. Through coarse-graining fine-scale observations, our modeling framework simplifies these complex systems to a set of tractable mechanistic relationships -- in the form of ordinary differential equations -- while preserving critical system behaviors. This approach allows for expedited 'what if' studies and sensitivity analyses, essential for informed policy-making. Our findings, from a case study of Baltimore, MD, indicate that this machine learning-augmented coarse-grained model serves as a powerful instrument for deciphering the complex interactions between social factors, geography, and exogenous stressors, offering a valuable asset for system forecasting and resilience planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18108v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Koch, Pranab Roy Chowdhury, Heng Wan, Parin Bhaduri, Jim Yoon, Vivek Srikrishnan, W. Brent Daniel</dc:creator>
    </item>
    <item>
      <title>Detecting and explaining (in)equivalence of context-free grammars</title>
      <link>https://arxiv.org/abs/2407.18220</link>
      <description>arXiv:2407.18220v1 Announce Type: cross 
Abstract: We propose a scalable framework for deciding, proving, and explaining (in)equivalence of context-free grammars. We present an implementation of the framework and evaluate it on large data sets collected within educational support systems. Even though the equivalence problem for context-free languages is undecidable in general, the framework is able to handle a large portion of these datasets. It introduces and combines techniques from several areas, such as an abstract grammar transformation language to identify equivalent grammars as well as sufficiently similar inequivalent grammars, theory-based comparison algorithms for a large class of context-free languages, and a graph-theory-inspired grammar canonization that allows to efficiently identify isomorphic grammars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18220v1</guid>
      <category>cs.FL</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko Schmellenkamp, Thomas Zeume, Sven Argo, Sandra Kiefer, Cedric Siems, Fynn Stebel</dc:creator>
    </item>
    <item>
      <title>An Interactive Decision-Support Dashboard for Optimal Hospital Capacity Management</title>
      <link>https://arxiv.org/abs/2403.15634</link>
      <description>arXiv:2403.15634v3 Announce Type: replace 
Abstract: Data-driven optimization models have the potential to significantly improve hospital capacity management, particularly during demand surges, when effective allocation of capacity is most critical and challenging. However, integrating models into existing processes in a way that provides value requires recognizing that hospital administrators are ultimately responsible for making capacity management decisions, and carefully building trustworthy and accessible tools for them. In this study, we develop an interactive, user-friendly, electronic dashboard for informing hospital capacity management decisions during surge periods. The dashboard integrates real-time hospital data, predictive analytics, and optimization models. It allows hospital administrators to interactively customize parameters, enabling them to explore a range of scenarios, and provides real-time updates on recommended optimal decisions. The dashboard was created through a participatory design process, involving hospital administrators in the development team to ensure practical utility, trustworthiness, transparency, explainability, and usability. We successfully deployed our dashboard within the Johns Hopkins Health System during the height of the COVID-19 pandemic, addressing the increased need for tools to inform hospital capacity management. It was used on a daily basis, with results regularly communicated to hospital leadership. This study demonstrates the practical application of a prospective, data-driven, interactive decision-support tool for hospital system capacity management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15634v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix Parker, Diego A. Mart\'inez, James Scheulen, Kimia Ghobadi</dc:creator>
    </item>
    <item>
      <title>A Non-Expert's Introduction to Data Ethics for Mathematicians</title>
      <link>https://arxiv.org/abs/2201.07794</link>
      <description>arXiv:2201.07794v5 Announce Type: replace-cross 
Abstract: I give a short introduction to data ethics. I begin with some background information and societal context for data ethics. I then discuss data ethics in mathematical-science education and indicate some available course material. I briefly highlight a few efforts -- at my home institution and elsewhere -- on data ethics, society, and social good. I then discuss open data in research, research replicability and some other ethical issues in research, and the tension between privacy and open data and code, and a few controversial studies and reactions to studies. I then discuss ethical principles, institutional review boards, and a few other considerations in the scientific use of human data. I then briefly survey a variety of research and lay articles that are relevant to data ethics and data privacy. I conclude with a brief summary and some closing remarks.
  My focal audience is mathematicians, but I hope that this chapter will also be useful to others. I am not an expert about data ethics, and this chapter provides only a starting point on this wide-ranging topic. I encourage you to examine the resources that I discuss and to reflect carefully on data ethics, its role in mathematics education, and the societal implications of data and data analysis. As data and technology continue to evolve, I hope that such careful reflection will continue throughout your life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.07794v5</guid>
      <category>math.HO</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>physics.soc-ph</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mason A. Porter</dc:creator>
    </item>
    <item>
      <title>Improving Stance Detection by Leveraging Measurement Knowledge from Social Sciences: A Case Study of Dutch Political Tweets and Traditional Gender Role Division</title>
      <link>https://arxiv.org/abs/2212.06543</link>
      <description>arXiv:2212.06543v2 Announce Type: replace-cross 
Abstract: Stance detection (SD) concerns automatically determining the viewpoint (i.e., in favour of, against, or neutral) of a text's author towards a target. SD has been applied to many research topics, among which the detection of stances behind political tweets is an important one. In this paper, we apply SD to a dataset of tweets from official party accounts in the Netherlands between 2017 and 2021, with a focus on stances towards traditional gender role division, a dividing issue between (some) Dutch political parties. To implement and improve SD of traditional gender role division, we propose to leverage an established survey instrument from social sciences, which has been validated for the purpose of measuring attitudes towards traditional gender role division. Based on our experiments, we show that using such a validated survey instrument helps to improve SD performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06543v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri</dc:creator>
    </item>
    <item>
      <title>PATCH! Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Proficiency in 8th Grade Mathematics</title>
      <link>https://arxiv.org/abs/2404.01799</link>
      <description>arXiv:2404.01799v2 Announce Type: replace-cross 
Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of LLMs. PATCH addresses the aforementioned limitations, presenting a new direction for LLM benchmark research. Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing benchmarking practices. Third, we release 4 high-quality datasets to support measuring and comparing LLM proficiency in grade school mathematics and science against human populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01799v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qixiang Fang, Daniel L. Oberski, Dong Nguyen</dc:creator>
    </item>
    <item>
      <title>Analyzing LLM Usage in an Advanced Computing Class in India</title>
      <link>https://arxiv.org/abs/2404.04603</link>
      <description>arXiv:2404.04603v2 Announce Type: replace-cross 
Abstract: This study examines the use of large language models (LLMs) by undergraduate and graduate students for programming assignments in advanced computing classes. Unlike existing research, which primarily focuses on introductory classes and lacks in-depth analysis of actual student-LLM interactions, our work fills this gap. We conducted a comprehensive analysis involving 411 students from a Distributed Systems class at an Indian university, where they completed three programming assignments and shared their experiences through Google Form surveys.
  Our findings reveal that students leveraged LLMs for a variety of tasks, including code generation, debugging, conceptual inquiries, and test case creation. They employed a spectrum of prompting strategies, ranging from basic contextual prompts to advanced techniques like chain-of-thought prompting and iterative refinement. While students generally viewed LLMs as beneficial for enhancing productivity and learning, we noted a concerning trend of over-reliance, with many students submitting entire assignment descriptions to obtain complete solutions. Given the increasing use of LLMs in the software industry, our study highlights the need to update undergraduate curricula to include training on effective prompting strategies and to raise awareness about the benefits and potential drawbacks of LLM usage in academic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04603v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anupam Garg, Aryaman Raina, Aryan Gupta, Jaskaran Singh, Manav Saini, Prachi Iiitd, Ronit Mehta, Rupin Oberoi, Sachin Sharma, Samyak Jain, Sarthak Tyagi, Utkarsh Arora, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>SoK: Bridging Trust into the Blockchain. A Systematic Review on On-Chain Identity</title>
      <link>https://arxiv.org/abs/2407.17276</link>
      <description>arXiv:2407.17276v2 Announce Type: replace-cross 
Abstract: The ongoing regulation of blockchain-based services and applications requires the identification of users who are issuing transactions on the blockchain. This systematic review explores the current status, identifies research gaps, and outlines future research directions for establishing trusted and privacy-compliant identities on the blockchain (on-chain identity). A systematic search term was applied across various scientific databases, collecting 2232 potentially relevant research papers. These papers were narrowed down in two methodologically executed steps to 98 and finally to 13 relevant sources. The relevant articles were then systematically analyzed based on a set of screening questions. The results of the selected studies have provided insightful findings on the mechanisms of on-chain identities. On-chain identities are established using zero-knowledge proofs, public key infrastructure/certificates, and web of trust approaches. The technologies and architectures used by the authors are also highlighted. Trust has emerged as a key research gap, manifesting in two ways: firstly, a gap in how to trust the digital identity representation of a physical human; secondly, a gap in how to trust identity providers that issue identity confirmations on-chain. Potential future research avenues are suggested to help fill the current gaps in establishing trust and on-chain identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17276v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Awid Vaziry, Kaustabh Barman, Patrick Herbke</dc:creator>
    </item>
  </channel>
</rss>
