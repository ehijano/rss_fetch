<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:33:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FlairGPT: Repurposing LLMs for Interior Designs</title>
      <link>https://arxiv.org/abs/2501.04648</link>
      <description>arXiv:2501.04648v1 Announce Type: new 
Abstract: Interior design involves the careful selection and arrangement of objects to create an aesthetically pleasing, functional, and harmonized space that aligns with the client's design brief. This task is particularly challenging, as a successful design must not only incorporate all the necessary objects in a cohesive style, but also ensure they are arranged in a way that maximizes accessibility, while adhering to a variety of affordability and usage considerations. Data-driven solutions have been proposed, but these are typically room- or domain-specific and lack explainability in their design design considerations used in producing the final layout. In this paper, we investigate if large language models (LLMs) can be directly utilized for interior design. While we find that LLMs are not yet capable of generating complete layouts, they can be effectively leveraged in a structured manner, inspired by the workflow of interior designers. By systematically probing LLMs, we can reliably generate a list of objects along with relevant constraints that guide their placement. We translate this information into a design layout graph, which is then solved using an off-the-shelf constrained optimization setup to generate the final layouts. We benchmark our algorithm in various design configurations against existing LLM-based methods and human designs, and evaluate the results using a variety of quantitative and qualitative metrics along with user studies. In summary, we demonstrate that LLMs, when used in a structured manner, can effectively generate diverse high-quality layouts, making them a viable solution for creating large-scale virtual scenes. Project webpage at https://flairgpt.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04648v1</guid>
      <category>cs.GR</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabrielle Littlefair, Niladri Shekhar Dutt, Niloy J. Mitra</dc:creator>
    </item>
    <item>
      <title>Adaptive Algebraic Reuse of Reordering in Cholesky Factorization with Dynamic Sparsity Pattern</title>
      <link>https://arxiv.org/abs/2501.04011</link>
      <description>arXiv:2501.04011v1 Announce Type: cross 
Abstract: Cholesky linear solvers are a critical bottleneck in challenging applications within computer graphics and scientific computing. These applications include but are not limited to elastodynamic barrier methods such as Incremental Potential Contact (IPC), and geometric operations such as remeshing and morphology. In these contexts, the sparsity patterns of the linear systems frequently change across successive calls to the Cholesky solver, necessitating repeated symbolic analyses that dominate the overall solver runtime.
  To address this bottleneck, we evaluate our method on over 150,000 linear systems generated from diverse nonlinear problems with dynamic sparsity changes in Incremental Potential Contact (IPC) and patch remeshing on a wide range of triangular meshes of various sizes. Our analysis using three leading sparse Cholesky libraries, Intel MKL Pardiso, SuiteSparse CHOLMOD, and Apple Accelerate, reveals that the primary performance constraint lies in the symbolic re-ordering phase of the solver. Recognizing this, we introduce Parth, an innovative re-ordering method designed to update ordering vectors only where local connectivity changes occur adaptively. Parth employs a novel hierarchical graph decomposition algorithm to break down the dual graph of the input matrix into fine-grained subgraphs, facilitating the selective reuse of fill-reducing orderings when sparsity patterns exhibit temporal coherence.
  Our extensive evaluation demonstrates that Parth achieves up to a 255x and 13x speedup in fill-reducing ordering for our IPC and remeshing benchmark and a 6.85x and 10.7x acceleration in symbolic analysis. These enhancements translate to up to 2.95x and 5.89x reduction in overall solver runtime. Additionally, Parth's integration requires only three lines of code, resulting in significant computational savings without the requirement of changes to the computational stack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04011v1</guid>
      <category>math.NA</category>
      <category>cs.GR</category>
      <category>cs.NA</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Behrooz Zarebavani, Danny M. Kaufman, David I. W. Levin, Maryam Mehri Dehnavi</dc:creator>
    </item>
    <item>
      <title>Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation</title>
      <link>https://arxiv.org/abs/2501.04144</link>
      <description>arXiv:2501.04144v1 Announce Type: cross 
Abstract: In this paper, we push the boundaries of fine-grained 3D generation into truly creative territory. Current methods either lack intricate details or simply mimic existing objects -- we enable both. By lifting 2D fine-grained understanding into 3D through multi-view diffusion and modeling part latents as continuous distributions, we unlock the ability to generate entirely new, yet plausible parts through interpolation and sampling. A self-supervised feature consistency loss further ensures stable generation of these unseen parts. The result is the first system capable of creating novel 3D objects with species-specific details that transcend existing examples. While we demonstrate our approach on birds, the underlying framework extends beyond things that can chirp! Code will be released at https://github.com/kamwoh/chirpy3d.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04144v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kam Woh Ng, Jing Yang, Jia Wei Sii, Jiankang Deng, Chee Seng Chan, Yi-Zhe Song, Tao Xiang, Xiatian Zhu</dc:creator>
    </item>
    <item>
      <title>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</title>
      <link>https://arxiv.org/abs/2501.04689</link>
      <description>arXiv:2501.04689v1 Announce Type: cross 
Abstract: We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions better by modeling distributions, but are computationally expensive and the generation is often misaligned with visible surfaces. In this paper, we present SPAR3D, a novel two-stage approach aiming to take the best of both directions. The first stage of SPAR3D generates sparse 3D point clouds using a lightweight point diffusion model, which has a fast sampling speed. The second stage uses both the sampled point cloud and the input image to create highly detailed meshes. Our two-stage design enables probabilistic modeling of the ill-posed single-image 3D task while maintaining high computational efficiency and great output fidelity. Using point clouds as an intermediate representation further allows for interactive user edits. Evaluated on diverse datasets, SPAR3D demonstrates superior performance over previous state-of-the-art methods, at an inference speed of 0.7 seconds. Project page with code and model: https://spar3d.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04689v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani</dc:creator>
    </item>
    <item>
      <title>Accelerating Transfer Function Update for Distance Map based Volume Rendering</title>
      <link>https://arxiv.org/abs/2407.21552</link>
      <description>arXiv:2407.21552v3 Announce Type: replace 
Abstract: Direct volume rendering using ray-casting is widely used in practice. By using GPUs and applying acceleration techniques as empty space skipping, high frame rates are possible on modern hardware. This enables performance-critical use-cases such as virtual reality volume rendering. The currently fastest known technique uses volumetric distance maps to skip empty sections of the volume during ray-casting but requires the distance map to be updated per transfer function change. In this paper, we demonstrate a technique for subdividing the volume intensity range into partitions and deriving what we call partitioned distance maps. These can be used to accelerate the distance map computation for a newly changed transfer function by a factor up to 30. This allows the currently fastest known empty space skipping approach to be used while maintaining high frame rates even when the transfer function is changed frequently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21552v3</guid>
      <category>cs.GR</category>
      <category>cs.PF</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VIS55277.2024.00042</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2024 IEEE Visualization and Visual Analytics (VIS), pp. 171-175</arxiv:journal_reference>
      <dc:creator>Michael Rauter, Lukas Zimmermann, Markus Zeilinger</dc:creator>
    </item>
    <item>
      <title>Click2Mask: Local Editing with Dynamic Mask Generation</title>
      <link>https://arxiv.org/abs/2409.08272</link>
      <description>arXiv:2409.08272v2 Announce Type: replace-cross 
Abstract: Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts. This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area. Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors. We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description). A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss. Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution. Our experiments demonstrate that Click2Mask not only minimizes user effort but also enables competitive or superior local image manipulations compared to SoTA methods, according to both human judgement and automatic metrics. Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08272v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Omer Regev, Omri Avrahami, Dani Lischinski</dc:creator>
    </item>
  </channel>
</rss>
