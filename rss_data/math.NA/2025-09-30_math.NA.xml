<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AW-EL-PINNs: A Multi-Task Learning Physics-Informed Neural Network for Euler-Lagrange Systems in Optimal Control Problems</title>
      <link>https://arxiv.org/abs/2509.25262</link>
      <description>arXiv:2509.25262v1 Announce Type: new 
Abstract: This paper presents adaptive weighted Euler-Lagrange theorem combined physics-informed neural networks (AW-EL-PINNs) for solving Euler-Lagrange systems in optimal control problems. The framework systematically converts optimal control frameworks into two-point boundary value problems (TPBVPs) while establishing a multi-task learning paradigm through innovative integration of the Euler-Lagrange theorem with deep learning architecture. An adaptive loss weighting mechanism dynamically balances loss function components during training, decreasing tedious manual tuning of weighting the loss functions compared to the conventional physics-informed neural networks (PINNs). Based on six numerical examples, it's clear that AW-EL-PINNs achieve enhanced solution accuracy compared to baseline methods while maintaining stability throughout the optimization process. These results highlight the framework's capability to improve precision and ensure stability in solving Euler-Lagrange systems in optimal control problems, offering potential strategies for problems under physical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25262v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuandong Li, Runtian Zeng</dc:creator>
    </item>
    <item>
      <title>The Asad Correctional Power Series Method: A Novel Approach to Solving Fractional Differential Equations</title>
      <link>https://arxiv.org/abs/2509.25354</link>
      <description>arXiv:2509.25354v1 Announce Type: new 
Abstract: This paper introduces the Asad Correctional Power Series Method (ACPS), a novel and groundbreaking approach designed to simplify and optimize the solution of fractional differential equations. The ACPS combines algebraic manipulation with iterative refinement to achieve greater accuracy and computational efficiency than mainstream methods. By incorporating principles from both fractional calculus and functional analysis, the method offers a flexible framework capable of addressing a wide range of fractional equations, from linear to highly nonlinear cases. Additionally, a representative counterexample is provided to indicate that the conformable fractional derivative does not fulfill the mathematical criteria for a valid definition of fractional differentiation. The Asad Correctional Power Series (ACPS) method is employed to construct an analytic solution of the fractional SIR model in the form of a rapidly convergent power series. Its performance is validated through comparisons with the classical fourth-order Runge Kutta method, where both numerical and graphical analyses corroborate the method's precision and efficiency. The application of ACPS to the fractional epidemic model highlights its ability to capture memory and hereditary effects, offering more realistic insights into disease transmission dynamics than integer-order models. These findings demonstrate that ACPS can serve as a useful tool for solving fractional differential equations arising in real world applications</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25354v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asad Freihet, Mohammed Alabedalhadi</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation</title>
      <link>https://arxiv.org/abs/2509.25753</link>
      <description>arXiv:2509.25753v1 Announce Type: new 
Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of semi-linear parabolic reaction-diffusion partial differential equations used to model tumor growth. Mathematical models of tumor growth are largely phenomenological in nature, capturing infiltration of the tumor into surrounding healthy tissue, proliferation of the existing tumor, and patient response to therapies, such as chemotherapy and radiotherapy. Considerable inter-patient variability, inherent heterogeneity of the disease, sparse and noisy data collection, and model inadequacy all contribute to significant uncertainty in the model parameters. It is crucial that these uncertainties can be efficiently propagated through the model to compute quantities of interest (QoIs), which in turn may be used to inform clinical decisions. We show that QMC methods can be successful in computing expectations of meaningful QoIs. Well-posedness results are developed for the model and used to show a theoretical error bound for the case of uniform random fields. The theoretical linear error rate, which is superior to that of standard Monte Carlo, is verified numerically. Encouraging computational results are also provided for lognormal random fields, prompting further theoretical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25753v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander D. Gilbert, Frances Y. Kuo, Dirk Nuyens, Graham Pash, Ian H. Sloan, Karen E. Willcox</dc:creator>
    </item>
    <item>
      <title>Numerical approximations to invariant measures of hybrid stochastic differential equations with superlinear coefficients via the backward Euler-Maruyama method</title>
      <link>https://arxiv.org/abs/2509.25799</link>
      <description>arXiv:2509.25799v1 Announce Type: new 
Abstract: For stochastic stochastic differential equations with Markovian switching, whose drift and diffusion coefficients are allowed to contain superlinear terms, the backward Euler-Maruyama (BEM) method is proposed to approximate the invariant measure. The existence and uniqueness of the invariant measure of the numerical solution generated by the BEM method is proved. Then the convergence of the numerical invariant measure to its underlying counterpart is shown. The results obtained in this work release the requirement of the global Lipschitz condition on the diffusion coefficient in [X. Li et al. SIAM J. Numer. Anal. 56(3)(2018), pp. 1435-1455]. Numerical simulations are provided to demonstrate those theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25799v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liu, Jie Xu</dc:creator>
    </item>
    <item>
      <title>A Reduced Basis Method for the Stochastic Landau-Lifshitz-Gilbert Equation</title>
      <link>https://arxiv.org/abs/2509.25909</link>
      <description>arXiv:2509.25909v1 Announce Type: new 
Abstract: In this work, we consider the construction of efficient surrogates for the stochastic version of the Landau-Lifshitz-Gilbert (LLG) equation using model order reduction techniques, in particular, the Reduced Basis (RB) method. The Stochastic LLG (SLLG) equation is a widely used phenomenological model for the time evolution of the magnetization field confined to a ferromagnetic body while taking into account the effect of random heat perturbations. This phenomenon is mathematically formulated as a nonlinear parabolic problem, where the stochastic component is represented as a parameter-dependent datum depending on a non-compact and high-dimensional parameter. In an $\textit{offline}$ phase, we use Proper Orthogonal Decomposition (POD) on high-fidelity samples of the unbounded parameter space. To that end, we use the so-called $\textit{tangent plane scheme}$. For the $\textit{online}$ phase of the RB method, we again employ the tangent plane scheme in the RB space. This is possible due to our particular construction that reduces both spaces of the magnetization and of its time derivative. Due to the saddle-point nature of this scheme, a stabilization that appropriately enriches the RB space is required. Numerical experiments show a clear advantage over earlier approaches using sparse grid interpolation. In a complementary approach, we test a sparse grid approximation of the reduced coefficients in a purely data-driven method, exhibiting the weaknesses of earlier sparse grid approaches, but benefiting from increased stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25909v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Scaglioni, Michael Feischl, Fernando Henr\'iquez</dc:creator>
    </item>
    <item>
      <title>Flexible fixed-point iteration and its applications for nonsymmetric algebraic Riccati equations</title>
      <link>https://arxiv.org/abs/2509.25942</link>
      <description>arXiv:2509.25942v1 Announce Type: new 
Abstract: In this paper, we reveal the intrinsic Toeplitz structure in the unique stabilizing solution for nonsymmetric algebraic Riccati equations by employing a shift-involved fixed-point iteration, and propose an RADI-type method for computing this solution for large-scale equations of this type with sparse and low-rank structure by incorporating flexible shifts into the fixed-point iteration. We present a shift-selection strategy, termed Leja shifts, based on rational approximation theory, which is incorporated into the RADI-type method. We further discuss important implementation aspects for the method, such as low-rank factorization of residuals, implicit update of large-scale sparse matrices, real arithmetics with complex shifts, and related equations of other type.
  Numerical experiments demonstrate the efficiency of both the proposed method and the introduced shift-selection strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25942v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen-Chen Guo, Xin Liang</dc:creator>
    </item>
    <item>
      <title>Embedding General Conservation Constraints in Discretizations of Hyperbolic Systems on Arbitrary Meshes: A Multidimensional Framework</title>
      <link>https://arxiv.org/abs/2509.25967</link>
      <description>arXiv:2509.25967v1 Announce Type: new 
Abstract: The purpose of this review is to discuss the notion of conservation in hyperbolic systems and how one can formulate it at the discrete level depending on the solution representation of the solution. A general theory is difficult. We discuss several possibilities: if the solution is represented by average in volumes; if the mesh is staggerred; if the solution is solely represented by point values and an example where all the previous options are mixed.
  We show how each configuration can provide, or not, enough flexibility. The discussion could be adapted to any hyperbolic system endowed with an entropy, but we focus on compressible fluid mechanics, in its Eulerian and Lagrangian formulations. The unifying element is that we systematically express the update of conserved variables as $u^{n+1}=u^n- \Delta t\; \delta u$, where the functional $\delta u$ depends on the value of $u$ in the stencil of the scheme. Then, one can naturally define a graph connecting the states defining $\delta u$. The notion of local conservation can be defined from this graph. We are aware of only two possible situations: either the graph is constructed from the faces of the mesh elements (or the dual mesh), or it is defined from the mesh itself. Two notions of local conservation then emerge: either we define a numerical flux, or we define a "residual" attached to elements and the degrees of freedom within the element. We show that this two notions are in a way equivalent, but the one with residual allows much more flexibility, especially if additional algebraic constraints must be satisfied. Examples of specific additional conservation constraints are provided to illustrate this. We also show that this notion of conservation gives a very clear framework for the design of scheme in the Lagrangian framework. We end by providing a number of ongoing research questions, and highlight some open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25967v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Abgrall, Pierre-Henri Maire, Mario Ricchiuto</dc:creator>
    </item>
    <item>
      <title>Trustworthy AI in numerics: On verification algorithms for neural network-based PDE solvers</title>
      <link>https://arxiv.org/abs/2509.26122</link>
      <description>arXiv:2509.26122v1 Announce Type: new 
Abstract: We present new algorithms for a posteriori verification of neural networks (NNs) approximating solutions to PDEs. These verification algorithms compute accurate estimates of $L^p$ norms of NNs and their derivatives. When combined with residual bounds for specific PDEs, the algorithms provide guarantees of $\eps$-accuracy (in a suitable norm) with respect to the true, but unknown, solution of the PDE -- for arbitrary $\eps &gt;0$. In particular, if the NN fails to meet the desired accuracy, our algorithms will detect that and reject it, whereas any NN that passes the verification algorithms is certified to be $\eps$-accurate. This framework enables trustworthy algorithms for NN-based PDE solvers, regardless of how the NN is initially computed. Such a posteriori verification is essential, since a priori error bounds in general cannot guarantee the accuracy of computed solutions, due to algorithmic undecidability of the optimization problems used to train NNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26122v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emil Haugen, Alexei Stepanenko, Anders C. Hansen</dc:creator>
    </item>
    <item>
      <title>Zeta expansion for long-range interactions under periodic boundary conditions with applications to micromagnetics</title>
      <link>https://arxiv.org/abs/2509.26274</link>
      <description>arXiv:2509.26274v1 Announce Type: new 
Abstract: We address the efficient computation of power-law-based interaction potentials of homogeneous $d$-dimensional bodies with an infinite $n$-dimensional array of copies, including their higher-order derivatives. This problem forms a serious challenge in micromagnetics with periodic boundary conditions and related fields. Nowadays, it is common practice to truncate the associated infinite lattice sum to a finite number of images, introducing uncontrolled errors. We show that, for general interacting geometries, the exact infinite sum for both dipolar interactions and generalized Riesz power-law potentials can be obtained by complementing a small direct sum by a correction term that involves efficiently computable derivatives of generalized zeta functions. We show that the resulting representation converges exponentially in the derivative order, reaching machine precision at a computational cost no greater than that of truncated summation schemes. In order to compute the generalized zeta functions efficiently, we provide a superexponentially convergent algorithm for their evaluation, as well as for all required special functions, such as incomplete Bessel functions. Magnetic fields can thus be evaluated to machine precision in arbitrary cuboidal domains periodically extended along one or two dimensions. We benchmark our method against known formulas for magnetic interactions and against direct summation for Riesz potentials with large exponents, consistently achieving full precision. In addition, we identify new corrections to the asymptotic limit of the demagnetization field and tabulate high-precision benchmark values that can be used as a reliable reference for micromagnetic solvers. The techniques developed are broadly applicable, with direct impact in other areas such as molecular dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26274v1</guid>
      <category>math.NA</category>
      <category>cond-mat.str-el</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas A. Buchheit, Jonathan K. Busse, Torsten Ke{\ss}ler, Filipp N. Rybakov</dc:creator>
    </item>
    <item>
      <title>A robust computational framework for the mixture-energy-consistent six-equation two-phase model with instantaneous mechanical relaxation terms</title>
      <link>https://arxiv.org/abs/2509.26284</link>
      <description>arXiv:2509.26284v1 Announce Type: new 
Abstract: We present a robust computational framework for the numerical solution of a hyperbolic 6-equation single-velocity two-phase model. The model's main interest is that, when combined with instantaneous mechanical relaxation, it recovers the solution of the 5-equation model of Kapila. Several numerical methods based on this strategy have been developed over the years. However, neither the 5- nor 6-equation model admits a complete set of jump conditions because they involve non-conservative products. Different discretizations of these terms in the 6-equation model exist. The precise impact of these discretizations on the numerical solutions of the 5-equation model, in particular for shocks, is still an open question to which this work provides new insights. We consider the phasic total energies as prognostic variables to naturally enforce discrete conservation of total energy and compare the accuracy and robustness of different discretizations for the hyperbolic operator. Namely, we discuss the construction of an HLLC approximate Riemann solver in relation to jump conditions. We then compare an HLLC wave-propagation scheme which includes the non-conservative terms, with Rusanov and HLLC solvers for the conservative part in combination with suitable approaches for the non-conservative terms. We show that some approaches for the discretization of non-conservative terms fit within the framework of path-conservative schemes for hyperbolic problems. We then analyze the use of various numerical strategies on several relevant test cases, showing both the impact of the theoretical shortcomings of the models as well as the importance of the choice of a robust framework for the global numerical strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26284v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Orlando (CMAP), Ward Haegeman (CMAP), Marica Pelanti (ENSTA Paris), Marc Massot (CMAP)</dc:creator>
    </item>
    <item>
      <title>Nearest matrix with multiple eigenvalues by Riemannian optimization</title>
      <link>https://arxiv.org/abs/2509.26344</link>
      <description>arXiv:2509.26344v1 Announce Type: new 
Abstract: Given a square complex matrix $A$, we tackle the problem of finding the nearest matrix with multiple eigenvalues or, equivalently when $A$ had distinct eigenvalues, the nearest defective matrix. To this goal, we extend the general framework described in [M. Gnazzo, V. Noferini, L. Nyman, F. Poloni, \emph{Riemann-Oracle: A general-purpose Riemannian optimizer to solve nearness problems in matrix theory}, Found. Comput. Math., To appear] and based on variable projection and Riemannian optimization, allowing the ambient manifold to simultaneously track left and right eigenvectors. Our method also allows us to impose arbitrary complex-linear constraints on either the perturbation or the perturbed matrix; this can be useful to study structured eigenvalue condition numbers. We present numerical experiments, comparing with preexisting algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26344v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vanni Noferini, Lauri Nyman, Federico Poloni</dc:creator>
    </item>
    <item>
      <title>HANN: Homotopy auxiliary neural network for solving nonlinear algebraic equations</title>
      <link>https://arxiv.org/abs/2509.26358</link>
      <description>arXiv:2509.26358v1 Announce Type: new 
Abstract: Solving nonlinear algebraic equations is a fundamental but challenging problem in scientific computations and also has many applications in system engineering. Though traditional iterative methods and modern optimization algorithms have exerted effective roles in addressing certain specific problems, there still exist certain weaknesses such as the initial value sensitivity, limited accuracy and slow convergence rate, particulary without flexible input for the neural network methods. In this paper, we propose a homotopy auxiliary neural network (HANN) for solving nonlinear algebraic equations which integrates the classical homotopy continuation method and popular physics-informed neural network. Consequently, the HANN-1 has strong learning ability and can rapidly give an acceptable solution for the problem which outperforms some known methods, while the HANN-2 can further improve its accuracy. Numerical results on the benchmark problems confirm that the HANN method can effectively solve the problems of determining the total number of solutions of a single equation, finding solutions of transcendental systems involving the absolute value function or trigonometric function, ill-conditioned and normal high-dimensional nonlinear systems and time-varying nonlinear problems, for which the Python's built-in Fsolve function exhibits significant limitations, even fails to work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26358v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ling-Zhe Zai, Lei-Lei Guo, Zhi-Yong Zhang</dc:creator>
    </item>
    <item>
      <title>Finite element discretizations of bending plates with prestrained microstructure</title>
      <link>https://arxiv.org/abs/2509.26438</link>
      <description>arXiv:2509.26438v1 Announce Type: new 
Abstract: We investigate a finite element discretization of an elastic bending-plate model with an effective prestrain. The model has been obtained via homogenization and dimension reduction by B\"onlein at al. (2023). Its energy functional is the $\Gamma$-limit of a three-dimensional nonlinear microstructured elasticity functional. In the derived effective model, the microstructure is incorporated as a local corrector problem, a system of linear elliptic partial differential equations posed on a three-dimensional representative volume element. The discretization uses Discrete Kirchhoff Triangle elements for the macroscopic bending-plate problem on a mesh of scale $H$, and first-order Lagrange elements for the microscopic corrector problem on an axis-aligned mesh of scale $h$. We show that the discretized model $\Gamma$-converges to the continuous one as $(h,H)\to 0$,provided that there exists a microstructure mesh such that the elasticity tensor is Lipschitz continuous on each mesh element. This extends earlier results by Rumpf et al. (2024) to prestrained composites. Our argument does not require any rate of convergence for the microscopic discretization error. As a corollary, we also obtain convergence when $h \to 0$ and $H \to 0$ consecutively, and we prove that these limit processes commute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26438v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus B\"ohnlein, Stefan Neukamm, Oliver Sander</dc:creator>
    </item>
    <item>
      <title>Computing Linear Combinations of $\varphi$-Function Actions for Exponential Integrators</title>
      <link>https://arxiv.org/abs/2509.26475</link>
      <description>arXiv:2509.26475v1 Announce Type: new 
Abstract: We propose a matrix-free algorithm for evaluating linear combinations of $\varphi$-function actions, $w_i := \sum_{j=0}^{p} \alpha_i^{\,j}\,\varphi_j(t_i A)v_j$ for $i=1\colon r$, arising in exponential integrators. The method combines the scaling and recovering method with a truncated Taylor series, choosing a spectral shift and a scaling parameter by minimizing a power-based objective of the shifted operator. Accuracy is user-controlled and ultimately limited by the working precision. The algorithm decouples the stage abscissae $t_i$ from the polynomial weights $\alpha_i^j$, and a block variant enables simultaneous evaluation of $\{w_i\}_{i=1}^r$. Across standard benchmarks, including stiff and highly nonnormal matrices, the algorithm attains near-machine accuracy (IEEE double precision in our tests) for small step sizes and maintains reliable accuracy for larger steps where several existing Krylov-based algorithms deteriorate, providing a favorable balance of reliability and computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26475v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awad H. Al-Mohy</dc:creator>
    </item>
    <item>
      <title>Structure-preserving numerical calculation of wave equation for a vector field</title>
      <link>https://arxiv.org/abs/2509.26504</link>
      <description>arXiv:2509.26504v1 Announce Type: new 
Abstract: For the Proca equation, which is a wave equation for a vector field, we derive the canonical formulation including constraints from the Stueckelberg action and propose discrete equations with a structure-preserving scheme for conserving the constraints at the discrete level. Numerical simulations are performed using these discrete equations and other discrete equations with a standard scheme. We show the results obtained using the structure-preserving scheme and provide more accurate and stable numerical solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26504v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>quant-ph</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Tsuchiya</dc:creator>
    </item>
    <item>
      <title>Second order interlaced polynomial lattice rules for integration over $\mathbb{R}^s$</title>
      <link>https://arxiv.org/abs/2509.26624</link>
      <description>arXiv:2509.26624v1 Announce Type: new 
Abstract: We study numerical integration of functions $f: \mathbb{R}^{s} \to \mathbb{R}$ with respect to a probability measure. By applying the corresponding inverse cumulative distribution function, the problem is transformed into integrating an induced function over the unit cube $(0,1)^{s}$. We introduce a new orthonormal system: \emph{order~2 localized Walsh functions}. These basis functions retain the approximation power of classical Walsh functions for twice-differentiable integrands while inheriting the spatial localization of Haar wavelets. Localization is crucial because the transformed integrand is typically unbounded at the boundary. We show that the worst-case quasi-Monte Carlo integration error decays like $\mathcal{O}(N^{-1/\lambda})$ for every $\lambda \in (1/2,1]$. As an application, we consider elliptic partial differential equations with a finite number of log-normal random coefficients and show that our error estimates remain valid for their stochastic Galerkin discretizations by applying a suitable importance sampling density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26624v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiangang Cui, Josef Dick, Friedrich Pillichshammer</dc:creator>
    </item>
    <item>
      <title>Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference</title>
      <link>https://arxiv.org/abs/2509.25269</link>
      <description>arXiv:2509.25269v1 Announce Type: cross 
Abstract: In this work, we present and investigate the novel blind inverse problem of position-blind ptychography, i.e., ptychographic phase retrieval without any knowledge of scan positions, which then must be recovered jointly with the image. The motivation for this problem comes from single-particle diffractive X-ray imaging, where particles in random orientations are illuminated and a set of diffraction patterns is collected. If one uses a highly focused X-ray beam, the measurements would also become sensitive to the beam positions relative to each particle and therefore ptychographic, but these positions are also unknown. We investigate the viability of image reconstruction in a simulated, simplified 2-D variant of this difficult problem, using variational inference with modern data-driven image priors in the form of score-based diffusion models. We find that, with the right illumination structure and a strong prior, one can achieve reliable and successful image reconstructions even under measurement noise, in all except the most difficult evaluated imaging scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25269v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.optics</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Welker, Lorenz Kuger, Tim Roith, Berthy Feng, Martin Burger, Timo Gerkmann, Henry Chapman</dc:creator>
    </item>
    <item>
      <title>Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains</title>
      <link>https://arxiv.org/abs/2509.25450</link>
      <description>arXiv:2509.25450v1 Announce Type: cross 
Abstract: This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25450v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Moritz von Tresckow, Ion Gabriel Ion, Dimitrios Loukrezis</dc:creator>
    </item>
    <item>
      <title>Cross-Model Verification of Wall-Bounded Flows using Finite-JAX</title>
      <link>https://arxiv.org/abs/2509.25569</link>
      <description>arXiv:2509.25569v1 Announce Type: cross 
Abstract: Accurate prediction of wall-bounded flows remains central to advancing both theoretical understanding and computational methods in fluid mechanics. In this study, we perform a numerical simulation of channel flow using a complementary approach: a high-performance, differentiable finite-difference solver developed in JAX (Finite-JAX) and an analytical solution derived from the Navier-Stokes Equations, also referred to as the Hagen-Poiseuille equation. The solver is applied to the incompressible Navier-Stokes equations, along with appropriate boundary conditions, to capture canonical flow features such as velocity profiles and pressure gradients. Cross-model verification is conducted by systematically comparing numerical results between Finite-JAX and the analytical solution, with a focus on velocity distributions. In addition, numerical results are benchmarked against analytical solutions for laminar regimes, allowing for the direct quantification of verification accuracy errors. Our findings demonstrate that cross-model verification not only strengthens confidence in simulation fidelity but also provides a pathway for integrating differentiable solvers with established computational fluid dynamics platforms, paving the way for future fluid flow research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25569v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arturo Rodriguez, Avinash Potluri, Aryan Singh, Vyom Kumar, Kate Reza, Francisco O. Aguirre Ortega, Vineeth Vijaya Kumar, Noah L. Estrada</dc:creator>
    </item>
    <item>
      <title>When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness</title>
      <link>https://arxiv.org/abs/2509.25630</link>
      <description>arXiv:2509.25630v1 Announce Type: cross 
Abstract: Efficient sampling from complex and high dimensional target distributions turns out to be a fundamental task in diverse disciplines such as scientific computing, statistics and machine learning. In this paper, we revisit the randomized Langevin Monte Carlo (RLMC) for sampling from high dimensional distributions without log-concavity. Under the gradient Lipschitz condition and the log-Sobolev inequality, we prove a uniform-in-time error bound in $\mathcal{W}_2$-distance of order $O(\sqrt{d}h)$ for the RLMC sampling algorithm, which matches the best one in the literature under the log-concavity condition. Moreover, when the gradient of the potential $U$ is non-globally Lipschitz with superlinear growth, modified RLMC algorithms are proposed and analyzed, with non-asymptotic error bounds established. To the best of our knowledge, the modified RLMC algorithms and their non-asymptotic error bounds are new in the non-globally Lipschitz setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25630v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojie Wang, Bin Yang</dc:creator>
    </item>
    <item>
      <title>Deep set based operator learning with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2509.25646</link>
      <description>arXiv:2509.25646v1 Announce Type: cross 
Abstract: Learning operators from data is central to scientific machine learning. While DeepONets are widely used for their ability to handle complex domains, they require fixed sensor numbers and locations, lack mechanisms for uncertainty quantification (UQ), and are thus limited in practical applicability. Recent permutationinvariant extensions, such as the Variable-Input Deep Operator Network (VIDON), relax these sensor constraints but still rely on sufficiently dense observations and cannot capture uncertainties arising from incomplete measurements or from operators with inherent randomness. To address these challenges, we propose UQ-SONet, a permutation-invariant operator learning framework with built-in UQ. Our model integrates a set transformer embedding to handle sparse and variable sensor locations, and employs a conditional variational autoencoder (cVAE) to approximate the conditional distribution of the solution operator. By minimizing the negative ELBO, UQ-SONet provides principled uncertainty estimation while maintaining predictive accuracy. Numerical experiments on deterministic and stochastic PDEs, including the Navier-Stokes equation, demonstrate the robustness and effectiveness of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25646v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Ma, Ling Guo, Hao Wu, Tao Zhou</dc:creator>
    </item>
    <item>
      <title>PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils</title>
      <link>https://arxiv.org/abs/2509.26186</link>
      <description>arXiv:2509.26186v1 Announce Type: cross 
Abstract: Neural operator models for solving partial differential equations (PDEs) often rely on global mixing mechanisms-such as spectral convolutions or attention-which tend to oversmooth sharp local dynamics and introduce high computational cost. We present FINO, a finite-difference-inspired neural architecture that enforces strict locality while retaining multiscale representational power. FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and evolves states via an explicit, learnable time-stepping scheme. A central Local Operator Block leverage a differential stencil layer, a gating mask, and a linear fuse step to construct adaptive derivative-like local features that propagate forward in time. Embedded in an encoder-decoder with a bottleneck, FINO captures fine-grained local structures while preserving interpretability. We establish (i) a composition error bound linking one-step approximation error to stable long-horizon rollouts under a Lipschitz condition, and (ii) a universal approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six benchmarks and a climate modelling task, FINO achieves up to 44\% lower error and up to around 2\times speedups over state-of-the-art operator-learning baselines, demonstrating that strict locality with learnable time-stepping yields an accurate and scalable foundation for neural PDE solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26186v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun-Wun Cheng, Bin Dong, Carola-Bibiane Sch\"onlieb, Angelica I Aviles-Rivero</dc:creator>
    </item>
    <item>
      <title>Error bounds for perspective cones of a class of nonnegative Legendre functions</title>
      <link>https://arxiv.org/abs/2509.26289</link>
      <description>arXiv:2509.26289v1 Announce Type: cross 
Abstract: Error bounds play a central role in the study of conic optimization problems, including the analysis of convergence rates for numerous algorithms. Curiously, those error bounds are often H\"olderian with exponent 1/2. In this paper, we try to explain the prevalence of the 1/2 exponent by investigating generic properties of error bounds for conic feasibility problems where the underlying cone is a perspective cone constructed from a nonnegative Legendre function on $\mathbb{R}$. Our analysis relies on the facial reduction technique and the computation of one-step facial residual functions (1-FRFs). Specifically, under appropriate assumptions on the Legendre function, we show that 1-FRFs can be taken to be H\"olderian of exponent 1/2 almost everywhere with respect to the two-dimensional Hausdorff measure. This enables us to further establish that having a uniform H\"olderian error bound with exponent 1/2 is a generic property for a class of feasibility problems involving these cones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26289v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.MG</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaozhou Wang, Bruno F. Louren\c{c}o, Ting Kei Pong</dc:creator>
    </item>
    <item>
      <title>Reservoir computing based predictive reduced order model for steel grade intermixing in an industrial continuous casting tundish</title>
      <link>https://arxiv.org/abs/2509.26293</link>
      <description>arXiv:2509.26293v1 Announce Type: cross 
Abstract: Continuous casting is a widely adopted process in the steel industry, where maintaining high steel quality is paramount. Efficient prediction of grade intermixing during ladle changeover operations is critical for maintaining steel quality and minimizing material losses in the continuous casting process. Among various factors influencing grade intermixing, operating parameters play a significant role, in addition to tundish geometry and flow control devices. In this study, three-dimensional, transient, two-phase turbulent flow simulations are conducted to investigate the ladle changeover operation. During this process, the molten steel level in the tundish typically varies over time, significantly affecting the grade intermixing phenomena. The influence of ladle change time on intermixing time has been presented. However, high-fidelity full-order simulations of such complex transient phenomena are computationally expensive and are impractical for real-time monitoring or design-space exploration in industrial-scale applications. To address this issue, a reduced order modelling approach based on proper orthogonal decomposition (POD) and reservoir computing (RC) is employed to efficiently predict intermixing time. The proposed reduced order model (ROM) demonstrates excellent predictive accuracy using limited training data while requiring significantly less computational resources and training time. The results demonstrate the potential of the proposed methodology as a fast, reliable tool for real-time process monitoring and optimization in industrial continuous casting operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26293v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harshith Gowrachari, Mattia Giuseppe Barra, Giovanni Stabile, Gianluca Bazzaro, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Robust fully discrete error bounds for the Kuznetsov equation in the inviscid limit</title>
      <link>https://arxiv.org/abs/2401.06492</link>
      <description>arXiv:2401.06492v2 Announce Type: replace 
Abstract: The Kuznetsov equation is a classical wave model of acoustics that incorporates quadratic gradient nonlinearities. When its strong damping vanishes, it undergoes a singular behavior change, switching from a parabolic-like to a hyperbolic quasilinear evolution. In this work, we establish for the first time the optimal error bounds for its finite element approximation as well as a semi-implicit fully discrete approximation that are robust with respect to the vanishing damping parameter. The core of the new arguments lies in devising energy estimates directly for the error equation where one can more easily exploit the polynomial structure of the nonlinearities and compensate inverse estimates with smallness conditions on the error. Numerical experiments are included to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06492v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin D\"orich, Vanja Nikoli\'c</dc:creator>
    </item>
    <item>
      <title>A unified error analysis for randomized low-rank approximation with application to data assimilation</title>
      <link>https://arxiv.org/abs/2405.04811</link>
      <description>arXiv:2405.04811v2 Announce Type: replace 
Abstract: Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems. Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role. Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods. However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms. We propose a unified framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices. Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability. Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms. The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter. Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04811v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Scotto Di Perrotolo, Youssef Diouane, Selime G\"urol, Xavier Vasseur</dc:creator>
    </item>
    <item>
      <title>Fast training of accurate physics-informed neural networks without gradient descent</title>
      <link>https://arxiv.org/abs/2405.20836</link>
      <description>arXiv:2405.20836v2 Announce Type: replace 
Abstract: Solving time-dependent Partial Differential Equations (PDEs) is one of the most critical problems in computational science. While Physics-Informed Neural Networks (PINNs) offer a promising framework for approximating PDE solutions, their accuracy and training speed are limited by two core barriers: gradient-descent-based iterative optimization over complex loss landscapes and non-causal treatment of time as an extra spatial dimension. We present Frozen-PINN, a novel PINN based on the principle of space-time separation that leverages random features instead of training with gradient descent, and incorporates temporal causality by construction. On eight PDE benchmarks, including challenges such as extreme advection speeds, shocks, and high dimensionality, Frozen-PINNs achieve superior training efficiency and accuracy over state-of-the-art PINNs, often by several orders of magnitude. Our work addresses longstanding training and accuracy bottlenecks of PINNs, delivering quickly trainable, highly accurate, and inherently causal PDE solvers, a combination that prior methods could not realize. Our approach challenges the reliance of PINNs on stochastic gradient-descent-based methods and specialized hardware, leading to a paradigm shift in PINN training and providing a challenging benchmark for the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20836v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Datar, Taniya Kapoor, Abhishek Chandra, Qing Sun, Erik Lien Bolager, Iryna Burak, Anna Veselovska, Massimo Fornasier, Felix Dietrich</dc:creator>
    </item>
    <item>
      <title>Parallel subspace correction methods for semicoercive and nearly semicoercive convex optimization with applications to nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2412.17318</link>
      <description>arXiv:2412.17318v4 Announce Type: replace 
Abstract: We present new convergence analyses for parallel subspace correction methods for unconstrained semicoercive and nearly semicoercive convex optimization problems, generalizing the theory of singular and nearly singular linear problems to a class of nonlinear problems. Our results demonstrate that the elegant theoretical framework developed for singular and nearly singular linear problems can be extended to unconstrained semicoercive and nearly semicoercive convex optimization problems. For semicoercive problems, we show that the convergence rate can be estimated in terms of a seminorm stable decomposition over the subspaces and the kernel of the problem, aligning with the theory for singular linear problems. For nearly semicoercive problems, we establish a parameter-independent convergence rate, assuming the kernel of the semicoercive part can be decomposed into a sum of local kernels, which aligns with the theory for nearly singular problems. To demonstrate the applicability of our results, we provide convergence analyses of two-level additive Schwarz methods for solving certain nonlinear partial differential equations with Neumann boundary conditions, within the proposed abstract framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17318v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Young-Ju Lee, Jongho Park</dc:creator>
    </item>
    <item>
      <title>Surrogate models for diffusion on graphs via sparse polynomials</title>
      <link>https://arxiv.org/abs/2502.06595</link>
      <description>arXiv:2502.06595v2 Announce Type: replace 
Abstract: Diffusion kernels over graphs have been widely utilized as effective tools in various applications due to their ability to accurately model the flow of information through nodes and edges. However, there is a notable gap in the literature regarding the development of surrogate models for diffusion processes on graphs. In this work, we fill this gap by proposing sparse polynomial-based surrogate models for parametric diffusion equations on graphs with community structure. In tandem, we provide convergence guarantees for both least squares and compressed sensing-based approximations by showing the holomorphic regularity of parametric solutions to these diffusion equations. Our theoretical findings are accompanied by a series of numerical experiments conducted on both synthetic and real-world graphs that demonstrate the applicability of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06595v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Alessio D'Inverno, Kylian Ajavon, Simone Brugiapaglia</dc:creator>
    </item>
    <item>
      <title>Exploiting Inexact Computations in Multilevel Monte Carlo and Other Sampling Methods</title>
      <link>https://arxiv.org/abs/2503.05533</link>
      <description>arXiv:2503.05533v3 Announce Type: replace 
Abstract: Multilevel sampling methods, such as multilevel and multifidelity Monte Carlo, multilevel stochastic collocation, or delayed acceptance Markov chain Monte Carlo, have become standard uncertainty quantification (UQ) tools for a wide class of forward and inverse problems. The underlying idea is to achieve faster convergence by leveraging a hierarchy of models, such as partial differential equation (PDE) or stochastic differential equation (SDE) discretisations with increasing accuracy. By optimally redistributing work among the levels, multilevel methods can achieve significant performance improvement compared to single level methods working with one high-fidelity model. Intuitively, approximate solutions on coarser levels can tolerate large computational error without affecting the overall accuracy. We show how this can be used in high-performance computing applications to obtain a significant performance gain.
  As a use case, we analyse the computational error in the standard multilevel Monte Carlo method and formulate an adaptive algorithm which determines a minimum required computational accuracy on each level of discretisation. We show two examples of how the inexactness can be converted into actual gains using an elliptic PDE with lognormal random coefficients. Using a low precision sparse direct solver combined with iterative refinement results in a simulated gain in memory references of up to $3.5\times$ compared to the reference double precision solver; while using a MINRES iterative solver, a practical speedup of up to $1.5\times$ in terms of FLOPs is achieved. These results provide a step in the direction of energy-aware scientific computing, with significant potential for energy savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05533v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josef Mart\'inek, Erin Carson, Robert Scheichl</dc:creator>
    </item>
    <item>
      <title>Approximation properties of neural ODEs</title>
      <link>https://arxiv.org/abs/2503.15696</link>
      <description>arXiv:2503.15696v2 Announce Type: replace 
Abstract: We study the approximation properties of shallow neural networks whose activation function is defined as the flow map of a neural ordinary differential equation (neural ODE) at the final time of the integration interval. We prove the universal approximation property (UAP) of such shallow neural networks in the space of continuous functions. Furthermore, we investigate the approximation properties of shallow neural networks whose parameters satisfy specific constraints. In particular, we constrain the Lipschitz constant of the neural ODE's flow map and the norms of the weights to increase the network's stability. We prove that the UAP holds if we consider either constraint independently. When both are enforced, there is a loss of expressiveness, and we derive approximation bounds that quantify how accurately such a constrained network can approximate a continuous function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15696v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arturo De Marinis, Davide Murari, Elena Celledoni, Nicola Guglielmi, Brynjulf Owren, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Random-sketching Techniques to Enhance the Numerical Stability of Block Orthogonalization Algorithms for s-step GMRES</title>
      <link>https://arxiv.org/abs/2503.16717</link>
      <description>arXiv:2503.16717v3 Announce Type: replace 
Abstract: We integrate random sketching techniques into block orthogonalization schemes needed for s-step GMRES. The resulting block orthogonalization schemes generate the basis vectors whose overall orthogonality error is bounded by machine precision as long as each of the corresponding block vectors are numerically full rank. We implement these randomized block orthogonalization schemes using standard distributed-memory linear algebra kernels for s-step GMRES available in the Trilinos software packages. Our performance results on the Perlmutter supercomputer (with four NVIDIA A100 GPUs per node) demonstrate that these randomized techniques can enhance the numerical stability of the orthogonalization and overall solver, without a significant increase in the execution time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16717v3</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ichitaro Yamazaki, Andrew J. Higgins, Erik G. Boman, Daniel B. Szyld</dc:creator>
    </item>
    <item>
      <title>A parametric finite element method for a degenerate multi-phase Stefan problem with triple junctions</title>
      <link>https://arxiv.org/abs/2505.13165</link>
      <description>arXiv:2505.13165v2 Announce Type: replace 
Abstract: In this study, we propose a parametric finite element method for a degenerate multi-phase Stefan problem with triple junctions. This model describes the energy-driven motion of a surface cluster whose distributional solution was studied by Garcke and Sturzenhecker. We approximate the weak formulation of this sharp interface model by an unfitted finite element method that uses parametric elements for the representation of the moving interfaces. We establish existence and uniqueness of the discrete solution and prove unconditional stability of the proposed scheme. Moreover, a modification of the original scheme leads to a structure-preserving variant, in that it conserves the discrete analogue of a quantity that is preserved by the classical solution. Some numerical results demonstrate the applicability of our introduced schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13165v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tokuhiro Eto, Harald Garcke, Robert N\"urnberg</dc:creator>
    </item>
    <item>
      <title>Real and finite field versions of Chebotarev's theorem</title>
      <link>https://arxiv.org/abs/2506.02947</link>
      <description>arXiv:2506.02947v2 Announce Type: replace 
Abstract: Chebotarev's theorem on roots of unity states that all minors of the Fourier matrix of prime size are non-vanishing. This result has been rediscovered several times and proved via different techniques. We follow the proof of Evans and Isaacs and generalize the original result to a real version and a version over finite fields. For the latter, we are able to remove an order condition between the characteristic of the field and the size of the matrix as well as decrease a sufficient lower bound on the characteristic by Zhang considerably. Direct applications include a specific real phase retrieval problem as well as a recent result for Riesz bases of exponentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02947v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarek Emmrich, Stefan Kunis</dc:creator>
    </item>
    <item>
      <title>Reconstruction of a potential parameter in subdiffusion via a Kohn--Vogelius type functional: Theory and computation</title>
      <link>https://arxiv.org/abs/2509.19260</link>
      <description>arXiv:2509.19260v2 Announce Type: replace 
Abstract: This work considers the reconstruction of a space-dependent potential from boundary observations in subdiffusion by a stable and robust recovery method. Specifically, we develop an algorithm to minimize the Kohn-Vogelius cost function, which measures the difference between the solutions of two excitations. The inverse potential problem is recast into an optimization problem, where the objective is to minimize a Kohn-Vogelius-type functional within a set of admissible potentials. We establish the well-posedness of this optimization problem by proving the existence and uniqueness of a minimizer and demonstrating its stability with respect to perturbations in the boundary data. Furthermore, we analyze the Fr\'echet differentiability of the Kohn-Vogelius functional and prove the Lipschitz continuity of its gradient. These theoretical results enable the development of a convergent conjugate gradient algorithm for numerical reconstruction. The effectiveness and robustness of the proposed method are confirmed through several numerical examples in both one and two dimensions, including cases with noisy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19260v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamza Kahlaoui, Mourad Hrizi, Abdessamad Oulmelk, Xiangcheng Zheng, Mahmoud A. Zaky, Ahmed Hendy</dc:creator>
    </item>
    <item>
      <title>Nonlinear quantum computation by amplified encodings</title>
      <link>https://arxiv.org/abs/2411.16435</link>
      <description>arXiv:2411.16435v2 Announce Type: replace-cross 
Abstract: This paper presents a novel framework for high-dimensional nonlinear quantum computation that exploits tensor products of amplified vector and matrix encodings to efficiently evaluate multivariate polynomials. The approach enables the solution of nonlinear equations by quantum implementations of the fixed-point iteration and Newton's method, with quantitative runtime bounds derived in terms of the error tolerance. These results show that a quantum advantage, characterized by a logarithmic scaling of complexity with the dimension of the problem, is preserved. While Newton's method attains near-optimal theoretical complexity, the fixed-point iteration may be better suited to near-term noisy hardware, as supported by our numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16435v2</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Deiml, Daniel Peterseim</dc:creator>
    </item>
    <item>
      <title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski Equality to Annealed Importance Sampling and beyond</title>
      <link>https://arxiv.org/abs/2502.04575</link>
      <description>arXiv:2502.04575v2 Announce Type: replace-cross 
Abstract: Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation, we propose a new algorithm based on reverse diffusion samplers, establish a framework for analyzing its complexity, and empirically demonstrate its efficiency in tackling multimodality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04575v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Guo, Molei Tao, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Monte Carlo on a single sample</title>
      <link>https://arxiv.org/abs/2509.17025</link>
      <description>arXiv:2509.17025v3 Announce Type: replace-cross 
Abstract: In this paper, we consider a Monte Carlo simulation method (MinMC) that approximates prices and risk measures for a range $\Gamma$ of model parameters at once. The simulation method that we study has recently gained popularity [HS20, FPP22, BDG24], and we provide a theoretical framework and convergence rates for it. In particular, we show that sample-based approximations to $\mathbb{E}_{\theta}[X]$, where $\theta$ denotes the model and $\mathbb{E}_{\theta}$ the expectation with respect to the distribution $P_\theta$ of the model $\theta$, can be obtained across all $\theta \in \Gamma$ by minimizing a map $V:H\rightarrow \mathbb{R}$ with $H$ a suitable function space. The minimization can be achieved easily by fitting a standard feedforward neural network with stochastic gradient descent. We show that MinMC, which uses only one sample for each model, significantly outperforms a traditional Monte Carlo method performed for multiple values of $\theta$, which are subsequently interpolated. Our case study suggests that MinMC might serve as a new benchmark for parameter-dependent Monte Carlo simulations, which appear not only in quantitative finance but also in many other areas of scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17025v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Detering, Nicole Hufnagel, Paul Kr\"uhner</dc:creator>
    </item>
  </channel>
</rss>
