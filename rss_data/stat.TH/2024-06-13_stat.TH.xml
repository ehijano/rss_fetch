<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 01:53:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Conditional least squares estimation for the AD(1,n) model</title>
      <link>https://arxiv.org/abs/2406.07653</link>
      <description>arXiv:2406.07653v1 Announce Type: new 
Abstract: This paper deals with the problem of global parameter estimation of AD(1, n) where n is a positive integer which is a subclass of affine diffusions introduced by Duffie, Filipovic, and Schachermayer. In general affine models are applied to the pricing of bond and stock options, which is illustrated for the Vasicek, Cox-Ingersoll-Ross and Heston models. Our main results are about the conditional least squares estimation of AD(1, n) drift parameters based on two types of observations : continuous time observations and discrete time observations with high frequency and infinite horizon. Then, for each case, we study the asymptotic properties according to ergodic and non-ergodic cases. This paper introduces as well some moment results relative to the AD(1, n) model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07653v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Houssem Dahbi, Hamdi Fathallah</dc:creator>
    </item>
    <item>
      <title>The maximum likelihood type estimator of SDEs with fractional Brownian motion under small noise asymptotics in the rough case</title>
      <link>https://arxiv.org/abs/2406.07804</link>
      <description>arXiv:2406.07804v1 Announce Type: new 
Abstract: We study the problem of parametric estimation for continuously observed stochastic differential equation driven by fractional Brownian motion. Under some assumptions on drift and diffusion coefficients, we construct maximum likelihood estimator and establish its the asymptotic normality and moment convergence of the drift parameter when a small dispersion coefficient vanishes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07804v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shohei Nakajima</dc:creator>
    </item>
    <item>
      <title>Bias-Corrected Joint Spectral Embedding for Multilayer Networks with Invariant Subspace: Entrywise Eigenvector Perturbation and Inference</title>
      <link>https://arxiv.org/abs/2406.07849</link>
      <description>arXiv:2406.07849v1 Announce Type: new 
Abstract: In this paper, we propose to estimate the invariant subspace across heterogeneous multiple networks using a novel bias-corrected joint spectral embedding algorithm. The proposed algorithm recursively calibrates the diagonal bias of the sum of squared network adjacency matrices by leveraging the closed-form bias formula and iteratively updates the subspace estimator using the most recent estimated bias. Correspondingly, we establish a complete recipe for the entrywise subspace estimation theory for the proposed algorithm, including a sharp entrywise subspace perturbation bound and the entrywise eigenvector central limit theorem. Leveraging these results, we settle two multiple network inference problems: the exact community detection in multilayer stochastic block models and the hypothesis testing of the equality of membership profiles in multilayer mixed membership models. Our proof relies on delicate leave-one-out and leave-two-out analyses that are specifically tailored to block-wise symmetric random matrices and a martingale argument that is of fundamental interest for the entrywise eigenvector central limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07849v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of linear multiplier for processes driven by a bifractional Brownian motion</title>
      <link>https://arxiv.org/abs/2406.07889</link>
      <description>arXiv:2406.07889v1 Announce Type: new 
Abstract: We study the problem of nonparametric estimation of the linear multiplier function $\theta(t)$ for processes satisfying stochastic differential equations of the type $$dX_t=\theta(t)X_tdt+\epsilon dW_t^{H,K}, X_0=x_0,0\leq t \leq T$$ where $\{W_t^{H,K}, t \geq 0\}$ is a bifractional Brownian motion with known parameters $H\in (0,1), K\in (0,1]$ and $HK\in (\frac{1}{2},1).$ We investigate the asymptotic behaviour of the estimator of the unknown function $\theta(t)$ as $\epsilon \rightarrow 0.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07889v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. L. S. Prakasa Rao</dc:creator>
    </item>
    <item>
      <title>Expected value and a Cayley-Menger formula for the generalized earth mover's distance</title>
      <link>https://arxiv.org/abs/2406.07972</link>
      <description>arXiv:2406.07972v1 Announce Type: new 
Abstract: The earth mover's distance (EMD), also known as the 1-Wasserstein metric, measures the minimum amount of work required to transform one probability distribution into another. The EMD can be naturally generalized to measure the "distance" between any number (say $d$) of distributions. In previous work (2021), we found a recursive formula for the expected value of the generalized EMD, assuming the uniform distribution on the standard $n$-simplex. This recursion, however, was computationally expensive, requiring $\binom{d+n}{d}$ iterations. The main result of the present paper is a nonrecursive formula for this expected value, expressed as the integral of a certain polynomial of degree at most $dn$. As a secondary result, we resolve an unanswered problem by giving a formula for the generalized EMD in terms of pairwise EMDs; this can be viewed as an analogue of the Cayley-Menger determinant formula that gives the hypervolume of a simplex in terms of its edge lengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07972v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Q. Erickson</dc:creator>
    </item>
    <item>
      <title>An Extension of Greenwood's Formula to Variances</title>
      <link>https://arxiv.org/abs/2406.07994</link>
      <description>arXiv:2406.07994v1 Announce Type: new 
Abstract: In this article, we introduce an estimator for the asymptotic variance of the Greenwood variance estimator, where the latter is crucial for assessing the accuracy of the Kaplan-Meier survival estimator. The result indicates that the asymptotic variance of the Greenwood variance estimator is considerably smaller than that of the Kaplan-Meier variance estimator. This finding emphasizes the robustness of the Greenwood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07994v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Rodenkirchen, A. Hoyer</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Learning and Planning in Separated Latent MDPs</title>
      <link>https://arxiv.org/abs/2406.07920</link>
      <description>arXiv:2406.07920v1 Announce Type: cross 
Abstract: We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp *statistical threshold* for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07920v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Constantinos Daskalakis, Noah Golowich, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>Mode-based estimation of the center of symmetry</title>
      <link>https://arxiv.org/abs/2406.08241</link>
      <description>arXiv:2406.08241v1 Announce Type: cross 
Abstract: In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08241v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e E. Chac\'on, Javier Fern\'andez Serrano</dc:creator>
    </item>
    <item>
      <title>Copy-composition for probabilistic graphical models</title>
      <link>https://arxiv.org/abs/2406.08286</link>
      <description>arXiv:2406.08286v1 Announce Type: cross 
Abstract: In probabilistic modelling, joint distributions are often of more interest than their marginals, but the standard composition of stochastic channels is defined by marginalization. Recently, the notion of 'copy-composition' was introduced in order to circumvent this problem and express the chain rule of the relative entropy fibrationally, but while that goal was achieved, copy-composition lacked a satisfactory origin story. Here, we supply such a story for two standard probabilistic tools: directed and undirected graphical models. We explain that (directed) Bayesian networks may be understood as "stochastic terms" of product type, in which context copy-composition amounts to a pull-push operation. Likewise, we show that (undirected) factor graphs compose by copy-composition. In each case, our construction yields a double fibration of decorated (co)spans. Along the way, we introduce a useful bifibration of measure kernels, to provide semantics for the notion of stochastic term, which allows us to generalize probabilistic modelling from product to dependent types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08286v1</guid>
      <category>math.CT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Toby St Clere Smithe</dc:creator>
    </item>
    <item>
      <title>Nystr\"om Kernel Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2406.08401</link>
      <description>arXiv:2406.08401v1 Announce Type: cross 
Abstract: Kernel methods underpin many of the most successful approaches in data science and statistics, and they allow representing probability measures as elements of a reproducing kernel Hilbert space without loss of information. Recently, the kernel Stein discrepancy (KSD), which combines Stein's method with kernel techniques, gained considerable attention. Through the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it is sufficient to know the target distribution up to a multiplicative constant. However, the typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity, which hinders their application in large-scale settings. In this work, we propose a Nystr\"om-based KSD acceleration -- with runtime $\mathcal O\!\left(mn+m^3\right)$ for $n$ samples and $m\ll n$ Nystr\"om points -- , show its $\sqrt{n}$-consistency under the null with a classical sub-Gaussian assumption, and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08401v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kalinke, Zoltan Szabo, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title>
      <link>https://arxiv.org/abs/2406.08466</link>
      <description>arXiv:2406.08466v1 Announce Type: cross 
Abstract: Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.
  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a&gt;1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08466v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee</dc:creator>
    </item>
    <item>
      <title>Optimal score estimation via empirical Bayes smoothing</title>
      <link>https://arxiv.org/abs/2402.07747</link>
      <description>arXiv:2402.07747v2 Announce Type: replace 
Abstract: We study the problem of estimating the score function of an unknown probability distribution $\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\tilde \Theta(n^{-\frac{2}{d+4}})$ for this estimation problem under the loss function $\|\hat s - s^*\|^2_{L^2(\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss extensions to estimating $\beta$-H\"older continuous scores with $\beta \leq 1$, as well as the implication of our theory on the sample complexity of score-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07747v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andre Wibisono, Yihong Wu, Kaylee Yingxi Yang</dc:creator>
    </item>
    <item>
      <title>Efficiency and Robustness of Rosenbaum's Regression (Un)-Adjusted Rank-based Estimator in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2111.15524</link>
      <description>arXiv:2111.15524v4 Announce Type: replace-cross 
Abstract: Mean-based estimators of the causal effect in a completely randomized experiment may behave poorly if the potential outcomes have a heavy-tail, or contain outliers. We study an alternative estimator by Rosenbaum that estimates the constant additive treatment effect by inverting a randomization test using ranks. By investigating the breakdown point and asymptotic relative efficiency of this rank-based estimator, we show that it is provably robust against outliers and heavy-tailed potential outcomes, and has asymptotic variance at most 1.16 times that of the difference-in-means estimator (and much smaller when the potential outcomes are not light-tailed). We further derive a consistent estimator of the asymptotic standard error for Rosenbaum's estimator which yields a readily computable confidence interval for the treatment effect. We also study a regression adjusted version of Rosenbaum's estimator to incorporate additional covariate information in randomization inference. We prove gain in efficiency by this regression adjustment method under a linear regression model. We illustrate through synthetic and real data that, unlike the mean-based estimators, these rank-based estimators (both unadjusted or regression adjusted) are efficient and robust against heavy-tailed distributions, contamination, and model misspecification. Finally, we initiate the study of Rosenbaum's estimator when the constant treatment effect assumption may be violated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.15524v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Nabarun Deb, Bikram Karmakar, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic convergence bounds for modified tamed unadjusted Langevin algorithm in non-convex setting</title>
      <link>https://arxiv.org/abs/2207.02600</link>
      <description>arXiv:2207.02600v2 Announce Type: replace-cross 
Abstract: We consider the problem of sampling from a high-dimensional target distribution $\pi_\beta$ on $\mathbb{R}^d$ with density proportional to $\theta\mapsto e^{-\beta U(\theta)}$ using explicit numerical schemes based on discretising the Langevin stochastic differential equation (SDE). In recent literature, taming has been proposed and studied as a method for ensuring stability of Langevin-based numerical schemes in the case of super-linearly growing drift coefficients for the Langevin SDE. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) was proposed in [Bro+19] to sample from such target distributions with the gradient of the potential $U$ being super-linearly growing. However, theoretical guarantees in Wasserstein distances for Langevin-based algorithms have traditionally been derived assuming strong convexity of the potential $U$. In this paper, we propose a novel taming factor and derive, under a setting with possibly non-convex potential $U$ and super-linearly growing gradient of $U$, non-asymptotic theoretical bounds in Wasserstein-1 and Wasserstein-2 distances between the law of our algorithm, which we name the modified Tamed Unadjusted Langevin Algorithm (mTULA), and the target distribution $\pi_\beta$. We obtain respective rates of convergence $\mathcal{O}(\lambda)$ and $\mathcal{O}(\lambda^{1/2})$ in Wasserstein-1 and Wasserstein-2 distances for the discretisation error of mTULA in step size $\lambda$. High-dimensional numerical simulations which support our theoretical findings are presented to showcase the applicability of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02600v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Matthew Ng Cheng En, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>A smoothed-Bayesian approach to frequency recovery from sketched data</title>
      <link>https://arxiv.org/abs/2309.15408</link>
      <description>arXiv:2309.15408v2 Announce Type: replace-cross 
Abstract: We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a {\em smoothed-Bayesian} method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on \emph{multi-view} learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15408v2</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Fitting an ellipsoid to random points: predictions using the replica method</title>
      <link>https://arxiv.org/abs/2310.01169</link>
      <description>arXiv:2310.01169v2 Announce Type: replace-cross 
Abstract: We consider the problem of fitting a centered ellipsoid to $n$ standard Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to \infty$ with $n/d^2 \to \alpha &gt; 0$. It has been conjectured that this problem is, with high probability, satisfiable (SAT; that is, there exists an ellipsoid passing through all $n$ points) for $\alpha &lt; 1/4$, and unsatisfiable (UNSAT) for $\alpha &gt; 1/4$. In this work we give a precise analytical argument, based on the non-rigorous replica method of statistical physics, that indeed predicts a SAT/UNSAT transition at $\alpha = 1/4$, as well as the shape of a typical fitting ellipsoid in the SAT phase (i.e., the lengths of its principal axes). Besides the replica method, our main tool is the dilute limit of extensive-rank "HCIZ integrals" of random matrix theory. We further study different explicit algorithmic constructions of the matrix characterizing the ellipsoid. In particular, we show that a procedure based on minimizing its nuclear norm yields a solution in the whole SAT phase. Finally, we characterize the SAT/UNSAT transition for ellipsoid fitting of a large class of rotationally-invariant random vectors. Our work suggests mathematically rigorous ways to analyze fitting ellipsoids to random vectors, which is the topic of a companion work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01169v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Maillard, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>An information-theoretic lower bound in time-uniform estimation</title>
      <link>https://arxiv.org/abs/2402.08794</link>
      <description>arXiv:2402.08794v2 Announce Type: replace-cross 
Abstract: We present an information-theoretic lower bound for the problem of parameter estimation with time-uniform coverage guarantees. Via a new a reduction to sequential testing, we obtain stronger lower bounds that capture the hardness of the time-uniform setting. In the case of location model estimation, logistic regression, and exponential family models, our $\Omega(\sqrt{n^{-1}\log \log n})$ lower bound is sharp to within constant factors in typical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08794v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John C. Duchi, Saminul Haque</dc:creator>
    </item>
    <item>
      <title>Extrapolation-Aware Nonparametric Statistical Inference</title>
      <link>https://arxiv.org/abs/2402.09758</link>
      <description>arXiv:2402.09758v2 Announce Type: replace-cross 
Abstract: We define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable. This type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account. While extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models. In this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions. The proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support. We illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification. We furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds. The procedure is empirically evaluated on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09758v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Pfister, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Shifted Interpolation for Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.00278</link>
      <description>arXiv:2403.00278v2 Announce Type: replace-cross 
Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and R\'enyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00278v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinho Bok, Weijie Su, Jason M. Altschuler</dc:creator>
    </item>
    <item>
      <title>A Dynamic Likelihood Approach to Filtering for Advection-Diffusion Dynamics</title>
      <link>https://arxiv.org/abs/2406.06837</link>
      <description>arXiv:2406.06837v2 Announce Type: replace-cross 
Abstract: A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering. The DLF was developed specifically for hyperbolic problems -waves-, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems. In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time. The estimate posterior thus inherits phase information. For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations. We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty. The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06837v2</guid>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johannes Krotz, Juan M. Restrepo, Jorge Ramirez</dc:creator>
    </item>
  </channel>
</rss>
