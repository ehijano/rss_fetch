<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 02:36:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distribution-Matching Posterior Inference for Incomplete Structural Models</title>
      <link>https://arxiv.org/abs/2601.01077</link>
      <description>arXiv:2601.01077v1 Announce Type: new 
Abstract: This paper introduces a Bayesian inference framework for incomplete structural models, termed distribution-matching posterior inference (DMPI). Extending the minimal econometric interpretation (MEI), DMPI constructs a divergence-based quasi-likelihood using the Jensen-Shannon divergence between theoretical and empirical population-moment distributions, based on a Dirichlet-multinomial structure with additive smoothing. The framework accommodates model misspecification and stochastic singularity. Posterior inference is implemented via a sequential Monte Carlo algorithm with Metropolis-Hastings mutation that jointly samples structural parameters and theoretical moment distributions. Monte Carlo experiments using misspecified New Keynesian (NK) models demonstrate that DMPI yields robust inference and improves distribution-matching coherence by probabilistically down-weighting moment distributions inconsistent with the structural model. An empirical application to U.S. data shows that a parsimonious stochastic singular NK model provides a better fit to business-cycle moments than an overparameterized full-rank counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01077v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Kano</dc:creator>
    </item>
    <item>
      <title>Optimizing Patient Placement in Normal Care Units: An Instrumental Causal Forest Approach Minimizing Mortality</title>
      <link>https://arxiv.org/abs/2601.01149</link>
      <description>arXiv:2601.01149v1 Announce Type: new 
Abstract: Normal care units (NCU) placement affects health outcomes. NCUs in a hospital have different specialisations. There are patients that can potentially stay in multiple different NCUs. On a given day the NCUs are on different utilisation levels, which also affects health outcomes. Our approach uses instrumental variable causal forests, with emergency admission as an instrument, to estimate how the effect of NCU placement varies across patients and utilisation levels. The results show a clear trade-off between specialisation and utilization. Based on these findings, we design a minimax regret placement policy, using frequentist, Balke-Pearl and Manski bounds, that lowers mortality without capacity expansion. The policy reallocates patients according to their individualized average treatment effects, showing that data-driven patient placement can improve outcomes by using existing resources more efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01149v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Cordier</dc:creator>
    </item>
    <item>
      <title>When and Why State-Dependent Local Projections Work</title>
      <link>https://arxiv.org/abs/2601.01622</link>
      <description>arXiv:2601.01622v1 Announce Type: new 
Abstract: This paper studies state-dependent local projections (LPs). First, I establish a general characterization of their estimand: under minimal assumptions, state-dependent LPs recover weighted averages of causal effects. This holds for essentially all specifications used in practice. Second, I show that state-dependent LPs and VARs target different estimands and propose a simple VAR-based estimator whose probability limit equals the LP estimand. Third, in instrumental variable (LP-IV) settings, state-dependent weighting can generate nonzero interaction terms, even when the effects are not state-dependent. Overall, this paper shows how to correctly interpret state-dependent LPs, clarifying their connection to VARs and highlighting a key source of LP-IV misinterpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01622v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Winkler</dc:creator>
    </item>
    <item>
      <title>Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects</title>
      <link>https://arxiv.org/abs/2601.01783</link>
      <description>arXiv:2601.01783v1 Announce Type: new 
Abstract: The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the "too-similar-to-fail" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01783v1</guid>
      <category>econ.EM</category>
      <category>q-fin.CP</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Jun Huang, Lutfu S Sua, Jaime Ortiz, Jinshyang Roan, Bahram Alidaee</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning Based Computationally Efficient Conditional Choice Simulation Estimation of Dynamic Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2601.02069</link>
      <description>arXiv:2601.02069v1 Announce Type: new 
Abstract: Dynamic discrete choice (DDC) models have found widespread application in marketing. However, estimating these becomes challenging in "big data" settings with high-dimensional state-action spaces. To address this challenge, this paper develops a Reinforcement Learning (RL)-based two-step ("computationally light") Conditional Choice Simulation (CCS) estimation approach that combines the scalability of machine learning with the transparency, explainability, and interpretability of structural models, which is particularly valuable for counterfactual policy analysis. The method is premised on three insights: (1) the CCS ("forward simulation") approach is a special case of RL algorithms, (2) starting from an initial state-action pair, CCS updates the corresponding value function only after each simulation path has terminated, whereas RL algorithms may update for all the state-action pairs visited along a simulated path, and (3) RL focuses on inferring an agent's optimal policy with known reward functions, whereas DDC models focus on estimating the reward functions presupposing optimal policies. The procedure's computational efficiency over CCS estimation is demonstrated using Monte Carlo simulations with a canonical machine replacement and a consumer food purchase model. Framing CCS estimation of DDC models as an RL problem increases their applicability and scalability to high-dimensional marketing problems while retaining both interpretability and tractability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02069v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Khwaja, Sonal Srivastava</dc:creator>
    </item>
    <item>
      <title>Fare-Free Bus Service and CO2 Reductions: Evidence from a Natural Experiment</title>
      <link>https://arxiv.org/abs/2601.02190</link>
      <description>arXiv:2601.02190v1 Announce Type: new 
Abstract: We devise a difference-in-difference study design to assess the impact of fare-free bus service in Alexandria, located in the Washington, DC metro area. Our surveys show modest to no effect, with at most 6% more residents in Alexandria increasing their bus usage compared to control locations. We find no effect on ground-level ozone or road crashes, suggesting little to no impact on road traffic.
  One-third of respondents in control locations indicated they would use buses more frequently if fare-free service were available in their areas. Based on the respondent-reported reductions in car miles, the program led to a reduction of 0.294 to 0.494 tons of CO2 per year, or 5% to 9% of the average annual emissions from a US car, at a cost of $70-$120 per ton of CO2. We predict a CO2 reduction of 0.454 tons per year, equivalent to 8% of the average US car's annual emissions if the fare-free bus covered all of the study areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02190v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna Alberini, Javier Bas, Cinzia Cirillo</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning of Continuous Treatment Effects with General Instrumental Variables</title>
      <link>https://arxiv.org/abs/2601.01471</link>
      <description>arXiv:2601.01471v1 Announce Type: cross 
Abstract: Estimating causal effects of continuous treatments is a common problem in practice, for example, in studying dose-response functions. Classical analyses typically assume that all confounders are fully observed, whereas in real-world applications, unmeasured confounding often persists. In this article, we propose a novel framework for local identification of dose-response functions using instrumental variables, thereby mitigating bias induced by unobserved confounders. We introduce the concept of a uniform regular weighting function and consider covering the treatment space with a finite collection of open sets. On each of these sets, such a weighting function exists, allowing us to identify the dose-response function locally within the corresponding region. For estimation, we develop an augmented inverse probability weighting score for continuous treatments under a debiased machine learning framework with instrumental variables. We further establish the asymptotic properties when the dose-response function is estimated via kernel regression or empirical risk minimization. Finally, we conduct both simulation and empirical studies to assess the finite-sample performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01471v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Chen, Peng Zhang, Yifan Cui</dc:creator>
    </item>
    <item>
      <title>Identification of Semiparametric Panel Multinomial Choice Models with Infinite-Dimensional Fixed Effects</title>
      <link>https://arxiv.org/abs/2009.00085</link>
      <description>arXiv:2009.00085v3 Announce Type: replace 
Abstract: This paper proposes a robust method for semiparametric identification and estimation in panel multinomial choice models, where we allow for infinite-dimensional fixed effects that enter into consumer utilities in an additively nonseparable way, thus incorporating rich forms of unobserved heterogeneity. Our identification strategy exploits multivariate monotonicity in parametric indexes, and uses the logical contraposition of an intertemporal inequality on choice probabilities to obtain identifying restrictions. We provide a consistent estimation procedure, and demonstrate the practical advantages of our method with Monte Carlo simulations and an empirical illustration on popcorn sales with the Nielsen data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.00085v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wayne Yuan Gao, Ming Li</dc:creator>
    </item>
    <item>
      <title>Modularity, Architectural Innovation, and New Venture Success</title>
      <link>https://arxiv.org/abs/2405.15042</link>
      <description>arXiv:2405.15042v2 Announce Type: replace 
Abstract: Startups face a classic dilemma in innovation strategy: should they pursue cumulative, low-risk improvements or disruptive, high-risk breakthroughs? The Henderson and Clark framework suggests that architectural innovation, which reconfigures existing economic modules in novel ways, tends to be disruptive and risky for established organizations, but the success of this strategy for entrepreneurs remains less well understood, largely based on methodological constraints. Building on a complex-economics perspective and advanced computational models, we distinguish architectural innovation from modular innovation, which incrementally updates economic modules, and modular invention, which forges new ones, within the entrepreneurship context. Then we examine how each strategy influences startup performance. We analyze 298,915 U.S. venture-funded start-ups from 1976-2020, embedding company descriptions within a dynamic semantic space constructed from business and patent discourse to measure innovation structure across the entire economy. Event history models reveal that architectural innovation leads to successful IPOs and high-value acquisitions, while both modular innovation and invention increase the risk of failure. By comparing the outcomes of architectural and modular innovation and invention, this paper reveals that what is typically seen as the riskiest form of innovation can, for startups, be the safest route to success. This reconceptualization inverts the trade-off between exploration-exploitation typically assumed in organizational learning with critical implications for entrepreneurial strategy and innovation policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15042v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likun Cao, Ziwen Chen, James Evans</dc:creator>
    </item>
    <item>
      <title>Inference After Ranking with Applications to Economic Mobility</title>
      <link>https://arxiv.org/abs/2410.19212</link>
      <description>arXiv:2410.19212v5 Announce Type: replace 
Abstract: This paper considers the problem of inference after ranking. In our setting, we are interested in any population whose rank according to some random quantity, such as an estimated treatment effect, a measure of value-added, or benefit (net of cost), falls in a pre-specified range of values. As such, this framework generalizes the inference on winners setting previously considered in Andrews et al. (2023), in which a winner is understood to be the single population whose rank according to some random quantity is highest. We show that this richer setting accommodates a broad variety of empirically-relevant applications. We develop a two-step method for inference, which we compare to existing methods or their natural generalizations to this setting. We first show the finite-sample validity of this method in a normal location model and then develop asymptotic counterparts to these results by proving uniform validity over a large class of distributions satisfying a weak uniform integrability condition. Importantly, our results permit degeneracy in the covariance matrix of the limiting distribution, which arises naturally in many applications. In an application to the literature on economic mobility, we find that it is difficult to distinguish between high and low-mobility census tracts when correcting for selection. Finally, we demonstrate the practical relevance of our theoretical results through an extensive set of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19212v5</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Petrou-Zeniou, Azeem M. Shaikh</dc:creator>
    </item>
    <item>
      <title>Robust Econometrics for Growth-at-Risk</title>
      <link>https://arxiv.org/abs/2508.00263</link>
      <description>arXiv:2508.00263v2 Announce Type: replace 
Abstract: The Growth-at-Risk (GaR) framework has garnered attention in recent econometric literature, yet current approaches implicitly assume a constant Pareto exponent. We introduce novel and robust econometrics to estimate the tails of GaR based on a rigorous theoretical framework and establish validity and effectiveness. Simulations demonstrate consistent outperformance relative to existing alternatives in terms of predictive accuracy. We perform a long-term GaR analysis that provides accurate and insightful predictions, effectively capturing financial anomalies better than current methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00263v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Adrian, Yuya Sasaki, Yulong Wang</dc:creator>
    </item>
    <item>
      <title>A further look at Modified ML estimation of the panel AR(1) model with fixed effects and arbitrary initial conditions</title>
      <link>https://arxiv.org/abs/2508.20753</link>
      <description>arXiv:2508.20753v3 Announce Type: replace 
Abstract: In this paper we consider two generalizations of Lancaster's (Review of Economic Studies, 2002) Modified Maximum Likelihood estimator (MMLE) for the panel AR(1) model with fixed effects, arbitrary initial conditions, and strictly exogenous covariates when the time dimension of the panel, T, is fixed. When the autoregressive parameter rho=1, the limiting modified profile log-likelihood function for this model has a stationary point of inflection, and rho is first-order underidentified but second-order identified. We show that, unlike the Random Effects and Transformed MLEs for this type of model, the generalized MMLEs are uniquely defined in finite samples w.p.1. for any value of |rho|=&lt;1. When rho=1, the rate of convergence of the MMLEs is N^{1/4}, where N is the cross-sectional dimension of the panel. We derive the limiting distributions of the MMLEs when rho=1. They are generally asymmetric. We also show that Quasi LM tests that are based on the modified profile log-likelihood function and use its expected rather than observed Hessian for hypotheses that include a restriction on rho, and confidence sets that are based on inverting these tests have correct asymptotic size in a uniform sense when |rho|=&lt;1. Finally, we investigate the finite sample properties of the MMLEs and the QLM test in a Monte Carlo study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20753v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Kruiniger</dc:creator>
    </item>
    <item>
      <title>Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis</title>
      <link>https://arxiv.org/abs/2511.05128</link>
      <description>arXiv:2511.05128v2 Announce Type: replace 
Abstract: We study whether access to standardized test scores improves the quality of teachers' secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05128v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Ichino, Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>LABOR-LLM: Language-Based Occupational Representations with Large Language Models</title>
      <link>https://arxiv.org/abs/2406.17972</link>
      <description>arXiv:2406.17972v4 Announce Type: replace-cross 
Abstract: This paper builds an empirical model that predicts a worker's next occupation as a function of the worker's occupational history. Because histories are sequences of occupations, the covariate space is high-dimensional, and further, the outcome (the next occupation) is a discrete choice that can take on many values. To estimate the parameters of the model, we leverage an approach from generative artificial intelligence. Estimation begins from a ``foundation model'' trained on non-representative data and then ``fine-tunes'' the estimation using data about careers from a representative survey. We convert tabular data from the survey into text files that resemble resumes and fine-tune the parameters of the foundation model, a large language model (LLM), using these text files with the objective of predicting the next token (word). The resulting fine-tuned LLM is used to calculate estimates of worker transition probabilities. Its predictive performance surpasses all prior models, both for the task of granularly predicting the next occupation as well as for specific tasks such as predicting whether the worker changes occupations or stays in the labor force. We quantify the value of fine-tuning and further show that by adding more career data from a different population, fine-tuning smaller LLMs (fewer parameters) surpasses the performance of fine-tuning larger models. When we omit the English language occupational title and replace it with a unique code, predictive performance declines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17972v4</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Herman Brunborg, Tianyu Du, Ayush Kanodia, Keyon Vafa</dc:creator>
    </item>
    <item>
      <title>Sharp Structure-Agnostic Lower Bounds for General Linear Functional Estimation</title>
      <link>https://arxiv.org/abs/2512.17341</link>
      <description>arXiv:2512.17341v2 Announce Type: replace-cross 
Abstract: We establish a general statistical optimality theory for estimation problems where the target parameter is a linear functional of an unknown nuisance component that must be estimated from data. This formulation covers many causal and predictive parameters and has applications to numerous disciplines. We adopt the structure-agnostic framework introduced by \citet{balakrishnan2023fundamental}, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we first prove the statistical optimality of the celebrated and widely used doubly robust estimators for the Average Treatment Effect (ATE), the most central parameter in causal inference. We then characterize the minimax optimal rate under the general formulation. Notably, we differentiate between two regimes in which double robustness can and cannot be achieved and in which first-order debiasing yields different error rates. Our result implies that first-order debiasing is simultaneously optimal in both regimes. We instantiate our theory by deriving optimal error rates that recover existing results and extend to various settings of interest, including the case when the nuisance is defined by generalized regressions and when covariate shift exists for training and test distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17341v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</title>
      <link>https://arxiv.org/abs/2512.17929</link>
      <description>arXiv:2512.17929v2 Announce Type: replace-cross 
Abstract: We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publicly accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Notably, despite its simplicity, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17929v2</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Wang, Kyle Feinstein, Sheryl Chen</dc:creator>
    </item>
  </channel>
</rss>
