<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Sep 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards</title>
      <link>https://arxiv.org/abs/2509.00140</link>
      <description>arXiv:2509.00140v1 Announce Type: new 
Abstract: Ontologies have supported knowledge representation and whitebox reasoning for decades; thus, the automated ontology generation (AOG) plays a crucial role in scaling their use. Software engineering standards (SES) consist of long, unstructured text (with high noise) and paragraphs with domain-specific terms. In this setting, relation triple extraction (RTE), together with term extraction, constitutes the first stage toward AOG. This work proposes an open-source large language model (LLM)-assisted approach to RTE for SES. Instead of solely relying on prompt-engineering-based methods, this study promotes the use of LLMs as an aid in constructing ontologies and explores an effective AOG workflow that includes document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment. Golden-standard benchmarks at three granularities are constructed and used to evaluate the ontology generated from the study. The results show that it is comparable and potentially superior to the OpenIE method of triple extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00140v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songhui Yue</dc:creator>
    </item>
    <item>
      <title>LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers</title>
      <link>https://arxiv.org/abs/2509.00256</link>
      <description>arXiv:2509.00256v1 Announce Type: new 
Abstract: Floating-point inconsistencies across compilers can undermine the reliability of numerical software. We present LLM4FP, the first framework that uses Large Language Models (LLMs) to generate floating-point programs specifically designed to trigger such inconsistencies. LLM4FP combines Grammar-Based Generation and Feedback-Based Mutation to produce diverse and valid programs. We evaluate LLM4FP across multiple compilers and optimization levels, measuring inconsistency rate, time cost, and program diversity. LLM4FP detects over twice as many inconsistencies compared to the state-of-the-art tool, Varity. Notably, most of the inconsistencies involve real-valued differences, rather than extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies across a wider range of optimization levels, and finds the most mismatches between host and device compilers. These results show that LLM-guided program generation improves the detection of numerical inconsistencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00256v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Wang, Cindy Rubio-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>JS-TOD: Detecting Order-Dependent Flaky Tests in Jest</title>
      <link>https://arxiv.org/abs/2509.00466</link>
      <description>arXiv:2509.00466v1 Announce Type: new 
Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that can extract, reorder, and rerun Jest tests to reveal possible order-dependent test flakiness. Test order dependency is one of the leading causes of test flakiness. Ideally, each test should operate in isolation and yield consistent results no matter the sequence in which tests are run. However, in practice, test outcomes can vary depending on their execution order. JS-TOD employed a systematic approach to randomising tests, test suites, and describe blocks. The tool is highly customisable, as one can set the number of orders and reruns required (the default setting is 10 reorder and 10 reruns for each test and test suite). Our evaluation using JS-TOD reveals two main causes of test order dependency flakiness: shared files and shared mocking state between tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00466v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Negar Hashemi, Amjed Tahir, Shawn Rasheed, August Shi, Rachel Blagojevic</dc:creator>
    </item>
    <item>
      <title>Bug Whispering: Towards Audio Bug Reporting</title>
      <link>https://arxiv.org/abs/2509.00785</link>
      <description>arXiv:2509.00785v1 Announce Type: new 
Abstract: Bug reporting is a key feature of mobile applications, as it enables developers to collect information about faults that escaped testing and thus affected end-users. This paper explores the idea of allowing end-users to immediately report the problems that they experience by recording and submitting audio messages. Audio recording is simple to implement and has the potential to increase the number of bug reports that development teams can gather, thus potentially improving the rate at which bugs are identified and fixed. However, audio bug reports exhibit specific characteristics that challenge existing techniques for reproducing bugs. This paper discusses these challenges based on a preliminary experiment, and motivates further research on the collection and analysis of audio-based bug reports</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00785v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Masserini, Daniela Micucci, Leonardo Mariani</dc:creator>
    </item>
    <item>
      <title>REConnect: Participatory RE that Matters</title>
      <link>https://arxiv.org/abs/2509.01006</link>
      <description>arXiv:2509.01006v1 Announce Type: new 
Abstract: Software increasingly shapes the infrastructures of daily life, making requirements engineering (RE) central to ensuring that systems align with human values and lived experiences. Yet, current popular practices such as CrowdRE and AI-assisted elicitation strategies risk detaching requirements work from the cultural, social, and political contexts that shape lived experiences, human values, and real user needs. In this paper, we introduce REConnect that re-centers RE on the human connection as central to the understanding of lived experiences where impact is sought. REConnect advocates for a human-centered participatory approach "that matters" to the communities and beneficiaries involved, ensuring alignment with their values and aspirations. Drawing on three case studies of societal impact: BloodSync in rural Nepal, Herluma supporting women at risk of homelessness in Canada, and BridgingRoots to revitalize Indigenous languages in the Canadian Arctic. REConnect argues that three key principles and enablers: building trusting relationships, co-designing with and alongside stakeholders, and empowering users as agents of change, can yield requirements that are culturally grounded, socially legitimate, and sustainable beyond system delivery. REConnect also proposes a set of actionable practices (REActions) that embed relationality and ongoing stakeholder engagement throughout requirements elicitation, analysis, and validation of solution development. Finally, we situate REConnect in the era of Generative AI. While AI can accelerate and scale certain RE tasks, its integration must be guided by participatory practices that not only preserve human agency but also empower humans' roles to become guardians of values and ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in iterative review cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01006v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniela Damian, Bachan Ghimire, Ze Shi Li</dc:creator>
    </item>
    <item>
      <title>Generative Goal Modeling</title>
      <link>https://arxiv.org/abs/2509.01048</link>
      <description>arXiv:2509.01048v1 Announce Type: new 
Abstract: In software engineering, requirements may be acquired from stakeholders through elicitation methods, such as interviews, observational studies, and focus groups. When supporting acquisition from interviews, business analysts must review transcripts to identify and document requirements. Goal modeling is a popular technique for representing early stakeholder requirements as it lends itself to various analyses, including refinement to map high-level goals into software operations, and conflict and obstacle analysis. In this paper, we describe an approach to use textual entailment to reliably extract goals from interview transcripts and to construct goal models. The approach has been evaluated on 15 interview transcripts across 29 application domains. The findings show that GPT-4o can reliably extract goals from interview transcripts, matching 62.0% of goals acquired by humans from the same transcripts, and that GPT-4o can trace goals to originating text in the transcript with 98.7% accuracy. In addition, when evaluated by human annotators, GPT-4o generates goal model refinement relationships among extracted goals with 72.2% accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01048v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ateeq Sharfuddin, Travis Breaux</dc:creator>
    </item>
    <item>
      <title>A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps</title>
      <link>https://arxiv.org/abs/2509.01068</link>
      <description>arXiv:2509.01068v1 Announce Type: new 
Abstract: [Background:] Research on automated requirements elicitation and analysis of mobile apps employed lots of techniques and tools proposed by RE researchers and practitioners. However, little is known about the characteristics of these techniques and tools as well as the RE tasks in requirements elicitation and analysis that got supported with the help of respective techniques and tools. [Aims:] The goal of this paper is to investigate the state-of-the-art of the techniques and tools used in automated requirements elicitation and analysis of mobile apps. [Method:] We carried out a systematic mapping study by following the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we found the most frequently used techniques - semi-automatic techniques, and the main characteristics of the tools - open-sourced and non-self-developed tools for requirements analysis and text pre-processing. Plus, the most three investigated RE tasks are requirements analysis, mining and classification. [Conclusions:] Our most important conclusions are: (1) there is a growth in the use of techniques and tools in automated requirements elicitation and analysis of mobile apps, (2) semi-automatic techniques are mainly used in the publications on this research topic, (3) requirements analysis, mining and classification are the top three RE tasks with the support of automatic techniques and tools, and (4) the most popular tools are open-sourced and non-self-developed, and they are mainly used in requirements analysis and text processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01068v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Wang, Haoning Wu, Peng Liang, Maya Daneva, Marten van Sinderen</dc:creator>
    </item>
    <item>
      <title>Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound</title>
      <link>https://arxiv.org/abs/2509.01149</link>
      <description>arXiv:2509.01149v1 Announce Type: new 
Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in Electronic Design Automation (EDA), translating Register-Transfer Level (RTL) designs into gate-level netlists. The correctness and reliability of FPGA logic synthesis tools are critically important, as unnoticed bugs in these tools may infect the final hardware implementations. However, recent approaches often rely heavily on random selection strategies, limiting the structural diversity of the generated HDL test cases and resulting in inadequate exploration of the tool's feature space. To address this limitation, we propose Lin-Hunter, a novel testing framework designed to systematically enhance the diversity of HDL test cases and the efficiency of FPGA logic synthesis tool validation. Specifically, Lin-Hunter introduces a principled set of metamorphic transformation rules to generate functionally equivalent yet structurally diverse HDL test case variants, effectively addressing the limited diversity of existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter integrates an adaptive strategy selection mechanism based on the Linear Upper Confidence Bound (LinUCB) method. This method leverages feedback from synthesis logs of previously executed test cases to dynamically prioritize transformation strategies that have empirically demonstrated a higher likelihood of triggering synthesis bugs. Comprehensive experiments conducted over a three-month period demonstrate the practical effectiveness of Lin-Hunter. Our method has discovered 18 unique bugs, including 10 previously unreported defects, which have been confirmed by official developers. Moreover, our method outperforms state-of-the-art testing methods in both test-case diversity and bug-discovery efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01149v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Zeng, Zhihao Xu, Hui Li, Siwen Wang, Qian Ma</dc:creator>
    </item>
    <item>
      <title>Policy-driven Software Bill of Materials on GitHub: An Empirical Study</title>
      <link>https://arxiv.org/abs/2509.01255</link>
      <description>arXiv:2509.01255v1 Announce Type: new 
Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list of all the software dependencies included in a software. SBOM emerged as way to assist securing the software supply chain. However, despite mandates from governments to use SBOM, research on this artifact is still in its early stages. Aims. We want to understand the current state of SBOM in open-source projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to achieve security goals, such as enhancing project transparency and ensuring compliance, rather than being used as fixtures for tools or artificially generated for benchmarking or academic research purposes. Method. We performed a mining software repository study to collect and carefully select SBOM files hosted on GitHub. We analyzed the information reported in policy-driven SBOMs and the vulnerabilities associated with the declared dependencies by means of descriptive statistics. Results. We show that only 0.56% of popular GitHub repositories contain policy-driven SBOM. The declared dependencies contain 2,202 unique vulnerabilities, while 22% of them do not report licensing information. Conclusion. Our findings provide insights for SBOM usage to support security assessment and licensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01255v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oleksii Novikov, Davide Fucci, Oleksandr Adamov, Daniel Mendez</dc:creator>
    </item>
    <item>
      <title>Metamorphic Testing of Multimodal Human Trajectory Prediction</title>
      <link>https://arxiv.org/abs/2509.01294</link>
      <description>arXiv:2509.01294v1 Announce Type: new 
Abstract: Context: Predicting human trajectories is crucial for the safety and reliability of autonomous systems, such as automated vehicles and mobile robots. However, rigorously testing the underlying multimodal Human Trajectory Prediction (HTP) models, which typically use multiple input sources (e.g., trajectory history and environment maps) and produce stochastic outputs (multiple possible future paths), presents significant challenges. The primary difficulty lies in the absence of a definitive test oracle, as numerous future trajectories might be plausible for any given scenario. Objectives: This research presents the application of Metamorphic Testing (MT) as a systematic methodology for testing multimodal HTP systems. We address the oracle problem through metamorphic relations (MRs) adapted for the complexities and stochastic nature of HTP. Methods: We present five MRs, targeting transformations of both historical trajectory data and semantic segmentation maps used as an environmental context. These MRs encompass: 1) label-preserving geometric transformations (mirroring, rotation, rescaling) applied to both trajectory and map inputs, where outputs are expected to transform correspondingly. 2) Map-altering transformations (changing semantic class labels, introducing obstacles) with predictable changes in trajectory distributions. We propose probabilistic violation criteria based on distance metrics between probability distributions, such as the Wasserstein or Hellinger distance. Conclusion: This study introduces tool, a MT framework for the oracle-less testing of multimodal, stochastic HTP systems. It allows for assessment of model robustness against input transformations and contextual changes without reliance on ground-truth trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01294v1</guid>
      <category>cs.SE</category>
      <category>cs.RO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Helge Spieker, Nadjib Lazaar, Arnaud Gotlieb, Nassim Belmecheri</dc:creator>
    </item>
    <item>
      <title>Aligning Requirement for Large Language Model's Code Generation</title>
      <link>https://arxiv.org/abs/2509.01313</link>
      <description>arXiv:2509.01313v1 Announce Type: new 
Abstract: Code generation refers to the automatic generation of source code based on a given programming specification, which has garnered significant attention particularly with the advancement of large language models (LLMs). However, due to the inherent complexity of real-world problems, the LLM-generated code often fails to fully align with the provided specification. While state-of-the-art agent-based techniques have been proposed to enhance LLM code generation, they overlook the critical issue of specification perception, resulting in persistent misalignment issues. Given that accurate perception of programming specifications serves as the foundation of the LLM-based code generation paradigm, ensuring specification alignment is particularly crucial. In this work, we draw on software requirements engineering to propose Specine, a novel specification alignment technique for LLM code generation. Its key idea is to identify misaligned input specifications, lift LLM-perceived specifications, and align them to enhance the code generation performance of LLMs. Our comprehensive experiments on four state-of-the-art LLMs across five challenging competitive benchmarks by comparing with ten state-of-the-art baselines, demonstrate the effectiveness of Specine. For example, Specine outperforms the most effective baseline, achieving an average improvement of 29.60\% across all subjects in terms of Pass@1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01313v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Tian, Junjie Chen</dc:creator>
    </item>
    <item>
      <title>Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing</title>
      <link>https://arxiv.org/abs/2509.01318</link>
      <description>arXiv:2509.01318v1 Announce Type: new 
Abstract: SystemC-based virtual prototypes have emerged as widely adopted tools to test software ahead of hardware availability, reducing the time-to-market and improving software reliability. Recently, fuzzing has become a popular method for automated software testing due to its ability to quickly identify corner-case errors. However, its application to embedded software is still limited. Simulator tools can help bridge this gap by providing a more powerful and controlled execution environment for testing. Existing solutions, however, often tightly couple fuzzers with built-in simulators that lack support for hardware peripherals and of- fer limited flexibility, restricting their ability to test embedded software. To address these limitations, we present a framework that allows the integration of American-Fuzzy-Lop-based fuzzers and SystemC-based simulators. The framework provides a harness to decouple the adopted fuzzer and simulator. In addition, it intercepts peripheral accesses and queries the fuzzer for values, effectively linking peripheral behavior to the fuzzer. This solution enables flexible interchangeability of peripher- als within the simulation environment and supports the interfacing of different SystemC-based virtual prototypes. The flexibility of the pro- posed solution is demonstrated by integrating the harness with different simulators and by testing various softwares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01318v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiara Ghinami, Jonas Winzer, Nils Bosbach, Lennart M. Reimann, Lukas J\"unger, Simon W\"orner, Rainer Leupers</dc:creator>
    </item>
    <item>
      <title>Towards Multi-Platform Mutation Testing of Task-based Chatbots</title>
      <link>https://arxiv.org/abs/2509.01389</link>
      <description>arXiv:2509.01389v1 Announce Type: new 
Abstract: Chatbots, also known as conversational agents, have become ubiquitous, offering services for a multitude of domains. Unlike general-purpose chatbots, task-based chatbots are software designed to prioritize the completion of tasks of the domain they handle (e.g., flight booking). Given the growing popularity of chatbots, testing techniques that can generate full conversations as test cases have emerged. Still, thoroughly testing all the possible conversational scenarios implemented by a task-based chatbot is challenging, resulting in incorrect behaviors that may remain unnoticed. To address this challenge, we proposed MUTABOT, a mutation testing approach for injecting faults in conversations and producing faulty chatbots that emulate defects that may affect the conversational aspects. In this paper, we present our extension of MUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments that show how mutation testing can be used to reveal weaknesses in test suites generated by the Botium state-of-the-art test generator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01389v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Clerissi, Elena Masserini, Daniela Micucci, Leonardo Mariani</dc:creator>
    </item>
    <item>
      <title>Non Technical Debt in Agile Software Development</title>
      <link>https://arxiv.org/abs/2509.01445</link>
      <description>arXiv:2509.01445v1 Announce Type: new 
Abstract: NonTechnical Debt (NTD) is a common challenge in agile software development, manifesting in four critical forms, Process Debt, Social Debt, People Debt, Organizational debt. NODLA project is a collaboration between Karlstad University and four leading Swedish industrial partners, reveals how various debt types disrupt large scale Agile Software Development (ASD) environments. Through extensive surveys, indepth interviews, and statistical analyses involving a diverse group of software professionals, we identified key drivers of NTD and their impacts. Our findings emphasize (1) Well structured, highly cohesive teams learn faster, adapt more effectively, and innovate consistently. (2) Psychological safety, fostered by proactive leadership, is essential for innovation, experimentation, and keeping employees. (3) Inefficient processes and unclear roles contribute significantly to drops in job satisfaction, productivity and team morale. (4) Social fragmentation, particularly in remote and hybrid settings, breeds rework, delays, and increased costs. (5) Neglected human resource needs, such as delayed hiring or insufficient training, limit an organization ability to meet growing demands. This white paper distils these insights into practical, evidence based strategies, such as refining team composition, clarifying roles, fostering psychological safety, streamlining workflows, and embracing failure as a learning tool. By implementing these strategies, organizations can reduce NTD, reclaim agility, and unlock their teams full potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01445v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.59217/xkcw2955</arxiv:DOI>
      <dc:creator>Muhammad Ovais Ahmad, Tomas Gustavsson</dc:creator>
    </item>
    <item>
      <title>Benchmarking and Studying the LLM-based Code Review</title>
      <link>https://arxiv.org/abs/2509.01494</link>
      <description>arXiv:2509.01494v1 Announce Type: new 
Abstract: Automated Code Review (ACR) is crucial for software quality, yet existing benchmarks often fail to reflect real-world complexities, hindering the evaluation of modern Large Language Models (LLMs). Current benchmarks frequently focus on fine-grained code units, lack complete project context, and use inadequate evaluation metrics. To address these limitations, we introduce SWRBench , a new benchmark comprising 1000 manually verified Pull Requests (PRs) from GitHub, offering PR-centric review with full project context. SWRBench employs an objective LLM-based evaluation method that aligns strongly with human judgment (~90 agreement) by verifying if issues from a structured ground truth are covered in generated reviews. Our systematic evaluation of mainstream ACR tools and LLMs on SWRBench reveals that current systems underperform, and ACR tools are more adept at detecting functional errors. Subsequently, we propose and validate a simple multi-review aggregation strategy that significantly boosts ACR performance, increasing F1 scores by up to 43.67%. Our contributions include the SWRBench benchmark, its objective evaluation method, a comprehensive study of current ACR capabilities, and an effective enhancement approach, offering valuable insights for advancing ACR research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01494v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengran Zeng, Ruikai Shi, Keke Han, Yixin Li, Kaicheng Sun, Yidong Wang, Zhuohao Yu, Rui Xie, Wei Ye, Shikun Zhang</dc:creator>
    </item>
    <item>
      <title>A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model</title>
      <link>https://arxiv.org/abs/2509.01527</link>
      <description>arXiv:2509.01527v1 Announce Type: new 
Abstract: Web applications are increasingly used in critical domains such as education, finance, and e-commerce. This highlights the need to ensure their failure-free performance. One effective method for evaluating failure-free performance is web form testing, where defining effective test scenarios is key to a complete and accurate evaluation. A core aspect of this process involves filling form fields with suitable values to create effective test cases. However, manually generating these values is time-consuming and prone to errors. To address this, various tools have been developed to assist testers. With the appearance of large language models (LLMs), a new generation of tools seeks to handle this task more intelligently. Although many LLM-based tools have been introduced, as these models typically rely on cloud infrastructure, their use in testing confidential web forms raises concerns about unintended data leakage and breaches of confidentiality. This paper introduces a privacy-preserving recommender that operates locally using a large language model. The tool assists testers in web form testing by suggesting effective field values. This tool analyzes the HTML structure of forms, detects input types, and extracts constraints based on each field's type and contextual content, guiding proper field filling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01527v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirreza Nayyeri, Abbas Rasoolzadegan</dc:creator>
    </item>
    <item>
      <title>WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing</title>
      <link>https://arxiv.org/abs/2509.01612</link>
      <description>arXiv:2509.01612v1 Announce Type: new 
Abstract: Fuzzing REST APIs is an important research problem, with practical applications and impact in industry. As such, a lot of research work has been carried out on this topic in the last few years. However, there are three major issues that hinder further progress: how to deal with API authentication; how to catalog and compare different fault types found by different fuzzers; and what to use as case study to facilitate fair comparisons among fuzzers. To address these important challenges, we present Web Fuzzing Commons (WFC) and Web Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema definitions to declaratively specify authentication info and catalog different types of faults that fuzzers can automatically detect. WFD is a collection of 36 open-source APIs with all necessary scaffolding to easily run experiments with fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of experiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web APIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster with other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest, RESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as well as providing guidelines with support of WFC/WFD to avoid them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01612v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omur Sahin, Man Zhang, Andrea Arcuri</dc:creator>
    </item>
    <item>
      <title>Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing</title>
      <link>https://arxiv.org/abs/2509.01616</link>
      <description>arXiv:2509.01616v1 Announce Type: new 
Abstract: Issue-reproducing tests fail on buggy code and pass once a patch is applied, thus increasing developers' confidence that the issue has been resolved and will not be re-introduced. However, past research has shown that developers often commit patches without such tests, making the automated generation of issue-reproducing tests an area of interest. We propose BLAST, a tool for automatically generating issue-reproducing tests from issue-patch pairs by combining LLMs and search-based software testing (SBST). For the LLM part, we complement the issue description and the patch by extracting relevant context through git history analysis, static analysis, and SBST-generated tests. For the SBST part, we adapt SBST for generating issue-reproducing tests; the issue description and the patch are fed into the SBST optimization through an intermediate LLM-generated seed, which we deserialize into SBST-compatible form. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%) of the issues from a curated Python benchmark, outperforming the state-of-the-art (23.5%). Additionally, to measure the real-world impact of BLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR) linked to an issue is opened, and if BLAST generates an issue-reproducing test, the bot proposes it as a comment in the PR. We deployed the bot in three open-source repositories for three months, gathering data from 32 PRs-issue pairs. BLAST generated an issue-reproducing test in 11 of these cases, which we proposed to the developers. By analyzing the developers' feedback, we discuss challenges and opportunities for researchers and tool builders. Data and material: https://doi.org/10.5281/zenodo.16949042</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01616v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Kitsios, Marco Castelluccio, Alberto Bacchelli</dc:creator>
    </item>
    <item>
      <title>Tether: A Personalized Support Assistant for Software Engineers with ADHD</title>
      <link>https://arxiv.org/abs/2509.01946</link>
      <description>arXiv:2509.01946v1 Announce Type: new 
Abstract: Equity, diversity, and inclusion in software engineering often overlook neurodiversity, particularly the experiences of developers with Attention Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that population in SE, few tools are designed to support their cognitive challenges (e.g., sustained attention, task initiation, self-regulation) within development workflows. We present Tether, an LLM-powered desktop application designed to support software engineers with ADHD by delivering adaptive, context-aware assistance. Drawing from engineering research methodology, Tether combines local activity monitoring, retrieval-augmented generation (RAG), and gamification to offer real-time focus support and personalized dialogue. The system integrates operating system level system tracking to prompt engagement and its chatbot leverages ADHD-specific resources to offer relevant responses. Preliminary validation through self-use revealed improved contextual accuracy following iterative prompt refinements and RAG enhancements. Tether differentiates itself from generic tools by being adaptable and aligned with software-specific workflows and ADHD-related challenges. While not yet evaluated by target users, this work lays the foundation for future neurodiversity-aware tools in SE and highlights the potential of LLMs as personalized support systems for underrepresented cognitive needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01946v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aarsh Shah, Cleyton Magalhaes, Kiev Gama, Ronnie de Souza Santos</dc:creator>
    </item>
    <item>
      <title>Automated Repair of C Programs Using Large Language Models</title>
      <link>https://arxiv.org/abs/2509.01947</link>
      <description>arXiv:2509.01947v1 Announce Type: new 
Abstract: This study explores the potential of Large Language Models (LLMs) in automating the repair of C programs. We present a framework that integrates spectrum-based fault localization (SBFL), runtime feedback, and Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike prior approaches, our method explicitly combines statistical program analysis with LLM reasoning. The iterative repair cycle leverages a structured Chain-of-Thought (CoT) prompting approach, where the model reasons over failing tests, suspicious code regions, and prior patch outcomes, before generating new candidate patches. The model iteratively changes the code, evaluates the results, and incorporates reasoning from previous attempts into subsequent modifications, reducing repeated errors and clarifying why some bugs remain unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where our approach achieves 44.93% repair accuracy, representing a 3.61% absolute improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT. This outcome highlights a practical pathway toward integrating statistical program analysis with generative AI in automated debugging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01947v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Farzandway, Fatemeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>ProbTest: Unit Testing for Probabilistic Programs (Extended Version)</title>
      <link>https://arxiv.org/abs/2509.02012</link>
      <description>arXiv:2509.02012v1 Announce Type: new 
Abstract: Testing probabilistic programs is non-trivial due to their stochastic nature. Given an input, the program may produce different outcomes depending on the underlying stochastic choices in the program. This means testing the expected outcomes of probabilistic programs requires repeated test executions unlike deterministic programs where a single execution may suffice for each test input. This raises the following question: how many times should we run a probabilistic program to effectively test it? This work proposes a novel black-box unit testing method, ProbTest, for testing the outcomes of probabilistic programs. Our method is founded on the theory surrounding a well-known combinatorial problem, the coupon collector's problem. Using this method, developers can write unit tests as usual without extra effort while the number of required test executions is determined automatically with statistical guarantees for the results. We implement ProbTest as a plug-in for PyTest, a well-known unit testing tool for python programs. Using this plug-in, developers can write unit tests similar to any other Python program and the necessary test executions are handled automatically. We evaluate the method on case studies from the Gymnasium reinforcement learning library and a randomized data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02012v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katrine Christensen, Mahsa Varshosaz, Ra\'ul Pardo</dc:creator>
    </item>
    <item>
      <title>Scalable Thread-Safety Analysis of Java Classes with CodeQL</title>
      <link>https://arxiv.org/abs/2509.02022</link>
      <description>arXiv:2509.02022v1 Announce Type: new 
Abstract: In object-oriented languages software developers rely on thread-safe classes to implement concurrent applications. However, determining whether a class is thread-safe is a challenging task. This paper presents a highly scalable method to analyze thread-safety in Java classes. We provide a definition of thread-safety for Java classes founded on the correctness principle of the Java memory model, data race freedom. We devise a set of properties for Java classes that are proven to ensure thread-safety. We encode these properties in the static analysis tool CodeQL to automatically analyze Java source code. We perform an evaluation on the top 1000 GitHub repositories. The evaluation comprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from 71 repositories. These repositories include highly popular software such as Apache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k starts), and gRPC (11.6k starts). Our queries detected thousands of thread-safety errors. The running time of our queries is below 2 minutes for repositories up to 200k lines of code, 20k methods, 6000 fields, and 1200 classes. We have submitted a selection of detected concurrency errors as PRs, and developers positively reacted to these PRs. We have submitted our CodeQL queries to the main CodeQL repository, and they are currently in the process of becoming available as part of GitHub actions. The results demonstrate the applicability and scalability of our method to analyze thread-safety in real-world code bases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02022v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bj{\o}rnar Haugstad J{\aa}tten, Simon Boye J{\o}rgensen, Rasmus Petersen, Ra\'ul Pardo</dc:creator>
    </item>
    <item>
      <title>Curiosity-Driven Testing for Sequential Decision-Making Process</title>
      <link>https://arxiv.org/abs/2509.02025</link>
      <description>arXiv:2509.02025v1 Announce Type: new 
Abstract: Sequential decision-making processes (SDPs) are fundamental for complex real-world challenges, such as autonomous driving, robotic control, and traffic management. While recent advances in Deep Learning (DL) have led to mature solutions for solving these complex problems, SDMs remain vulnerable to learning unsafe behaviors, posing significant risks in safety-critical applications. However, developing a testing framework for SDMs that can identify a diverse set of crash-triggering scenarios remains an open challenge. To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows a fuzzer to effectively explore novel and diverse scenarios, leading to improved detection of crashtriggering scenarios. Additionally, we introduce a multi-objective seed selection technique to balance the exploration of novel scenarios and the generation of crash-triggering scenarios, thereby optimizing the fuzzing process. We evaluate CureFuzz on various SDMs and experimental results demonstrate that CureFuzz outperforms the state-of-the-art method by a substantial margin in the total number of faults and distinct types of crash-triggering scenarios. We also demonstrate that the crash-triggering scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a valuable tool for testing SDMs and optimizing their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02025v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junda He, Zhou Yang, Jieke Shi, Chengran Yang, Kisub Kim, Bowen Xu, Xin Zhou, David Lo</dc:creator>
    </item>
    <item>
      <title>Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports</title>
      <link>https://arxiv.org/abs/2509.02150</link>
      <description>arXiv:2509.02150v1 Announce Type: new 
Abstract: With the rapid advancement of deep learning and related technologies, Autonomous Driving Systems (ADSs) have made significant progress and are gradually being widely applied in safety-critical fields. However, numerous accident reports show that ADSs still encounter challenges in complex scenarios. As a result, scenario-based testing has become essential for identifying defects and ensuring reliable performance. In particular, real-world accident reports offer valuable high-risk scenarios for more targeted ADS testing. Despite their potential, existing methods often rely on visual data, which demands large memory and manual annotation. Additionally, since existing methods do not adopt standardized scenario formats (e.g., OpenSCENARIO), the generated scenarios are often tied to specific platforms and ADS implementations, limiting their scalability and portability. To address these challenges, we propose Txt2Sce, a method for generating test scenarios in OpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM to convert textual accident reports into corresponding OpenSCENARIO scenario files. It then generates a derivation-based scenario file tree through scenario disassembly, scenario block mutation, and scenario assembly. By utilizing the derivation relationships between nodes in the scenario tree, Txt2Sce helps developers identify the scenario conditions that trigger unexpected behaviors of ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file trees, resulting in a total of 4,373 scenario files for testing the open-source ADS, Autoware. The experimental results show that Txt2Sce successfully converts textual reports into valid OpenSCENARIO files, enhances scenario diversity through mutation, and effectively detects unexpected behaviors of Autoware in terms of safety, smartness, and smoothness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02150v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pin Ji, Yang Feng, Zongtai Li, Xiangchi Zhou, Jia Liu, Jun Sun, Zhihong Zhao</dc:creator>
    </item>
    <item>
      <title>Formalizing Operational Design Domains with the Pkl Language</title>
      <link>https://arxiv.org/abs/2509.02221</link>
      <description>arXiv:2509.02221v1 Announce Type: new 
Abstract: The deployment of automated functions that can operate without direct human supervision has changed safety evaluation in domains seeking higher levels of automation. Unlike conventional systems that rely on human operators, these functions require new assessment frameworks to demonstrate that they do not introduce unacceptable risks under real-world conditions. To make a convincing safety claim, the developer must present a thorough justification argument, supported by evidence, that a function is free from unreasonable risk when operated in its intended context. The key concept relevant to the presented work is the intended context, often captured by an Operational Design Domain specification (ODD). ODD formalization is challenging due to the need to maintain flexibility in adopting diverse specification formats while preserving consistency and traceability and integrating seamlessly into the development, validation, and assessment. This paper presents a way to formalize an ODD in the Pkl language, addressing central challenges in specifying ODDs while improving usability through specialized configuration language features. The approach is illustrated with an automotive example but can be broadly applied to ensure rigorous assessments of operational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02221v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IV64158.2025.11097576</arxiv:DOI>
      <dc:creator>Martin Skoglund, Fredrik Warg, Anders Thors\'en, Sasikumar Punnekkat, Hans Hansson</dc:creator>
    </item>
    <item>
      <title>Methodology for Test Case Allocation based on a Formalized ODD</title>
      <link>https://arxiv.org/abs/2509.02311</link>
      <description>arXiv:2509.02311v1 Announce Type: new 
Abstract: The emergence of Connected, Cooperative, and Automated Mobility (CCAM) systems has significantly transformed the safety assessment landscape. Because they integrate automated vehicle functions beyond those managed by a human driver, new methods are required to evaluate their safety. Approaches that compile evidence from multiple test environments have been proposed for type-approval and similar evaluations, emphasizing scenario coverage within the systems Operational Design Domain (ODD). However, aligning diverse test environment requirements with distinct testing capabilities remains challenging. This paper presents a method for evaluating the suitability of test case allocation to various test environments by drawing on and extending an existing ODD formalization with key testing attributes. The resulting construct integrates ODD parameters and additional test attributes to capture a given test environments relevant capabilities. This approach supports automatic suitability evaluation and is demonstrated through a case study on an automated reversing truck function. The system's implementation fidelity is tied to ODD parameters, facilitating automated test case allocation based on each environments capacity for object-detection sensor assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02311v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-02018-5_5</arxiv:DOI>
      <dc:creator>Martin Skoglund, Fredrik Warg, Anders Thoren, Sasikumar Punnekkat, Hans Hansson</dc:creator>
    </item>
    <item>
      <title>ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2509.02330</link>
      <description>arXiv:2509.02330v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated impressive capabilities in code-related tasks, such as code generation and automated program repair. Despite their promising performance, most existing approaches for code repair suffer from high training costs or computationally expensive inference. Retrieval-augmented generation (RAG), with its efficient in-context learning paradigm, offers a more scalable alternative. However, conventional retrieval strategies, which are often based on holistic code-text embeddings, fail to capture the structural intricacies of code, resulting in suboptimal retrieval quality. To address the above limitations, we propose ReCode, a fine-grained retrieval-augmented in-context learning framework designed for accurate and efficient code repair. Specifically, ReCode introduces two key innovations: (1) an algorithm-aware retrieval strategy that narrows the search space using preliminary algorithm type predictions; and (2) a modular dual-encoder architecture that separately processes code and textual inputs, enabling fine-grained semantic matching between input and retrieved contexts. Furthermore, we propose RACodeBench, a new benchmark constructed from real-world user-submitted buggy code, which addresses the limitations of synthetic benchmarks and supports realistic evaluation. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost, highlighting its practical value for real-world code repair scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02330v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicong Zhao, Shisong Chen, Jiacheng Zhang, Zhixu Li</dc:creator>
    </item>
    <item>
      <title>afspm: A Framework for Manufacturer-Agnostic Automation in Scanning Probe Microscopy</title>
      <link>https://arxiv.org/abs/2509.00113</link>
      <description>arXiv:2509.00113v1 Announce Type: cross 
Abstract: Scanning probe microscopy (SPM) is a valuable technique by which one can investigate the physical characteristics of the surfaces of materials. However, its widespread use is hampered by the time-consuming nature of running an experiment and the significant domain knowledge required. Recent studies have shown the value of multiple forms of automation in improving this, but their use is limited due to the difficulty of integrating them with SPMs other than the one it was developed for. With this in mind, we propose an automation framework for SPMs aimed toward facilitating code sharing and reusability of developed components. Our framework defines generic control and data structure schemas which are passed among independent software processes (components), with the final SPM commands sent after passing through an SPM-specific translator. This approach permits multi-language support and allows for experimental components to be decoupled among multiple computers. Our mediation logic limits access to the SPM to a single component at a time, with a simple override mechanism in order to correct detected experiment problems. To validate our proposal, we integrated and tested it with two SPMs from separate manufacturers, and ran an experiment involving a thermal drift correction component.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00113v1</guid>
      <category>physics.ins-det</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas J. Sullivan, Julio J. Vald\'es, Kirk H. Bevan, Peter Grutter</dc:creator>
    </item>
    <item>
      <title>SHERPA: A Model-Driven Framework for Large Language Model Execution</title>
      <link>https://arxiv.org/abs/2509.00272</link>
      <description>arXiv:2509.00272v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have achieved widespread application across various fields. Despite their impressive capabilities, LLMs suffer from a lack of structured reasoning ability, particularly for complex tasks requiring domain-specific best practices, which are often unavailable in the training data. Although multi-step prompting methods incorporating human best practices, such as chain-of-thought and tree-of-thought, have gained popularity, they lack a general mechanism to control LLM behavior. In this paper, we propose SHERPA, a model-driven framework to improve the LLM performance on complex tasks by explicitly incorporating domain-specific best practices into hierarchical state machines. By structuring the LLM execution processes using state machines, SHERPA enables more fine-grained control over their behavior via rules or decisions driven by machine learning-based approaches, including LLMs. We show that SHERPA is applicable to a wide variety of tasks-specifically, code generation, class name generation, and question answering-replicating previously proposed approaches while further improving the performance. We demonstrate the effectiveness of SHERPA for the aforementioned tasks using various LLMs. Our systematic evaluation compares different state machine configurations against baseline approaches without state machines. Results show that integrating well-designed state machines significantly improves the quality of LLM outputs, and is particularly beneficial for complex tasks with well-established human best practices but lacking data used for training LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00272v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boqi Chen, Kua Chen, Jos\'e Antonio Hern\'andez L\'opez, Gunter Mussbacher, D\'aniel Varr\'o, Amir Feizpour</dc:creator>
    </item>
    <item>
      <title>Semantic Technologies in Practical Demand Response: An Informational Requirement-based Roadmap</title>
      <link>https://arxiv.org/abs/2509.01459</link>
      <description>arXiv:2509.01459v1 Announce Type: cross 
Abstract: The future grid will be highly complex and decentralized, requiring sophisticated coordination across numerous human and software agents that manage distributed resources such as Demand Response (DR). Realizing this vision demands significant advances in semantic interoperability, which enables scalable and cost-effective automation across heterogeneous systems. While semantic technologies have progressed in commercial building and DR domains, current ontologies have two critical limitations: they are often developed without a formal framework that reflects real-world DR requirements, and proposals for integrating general and application-specific ontologies remain mostly conceptual, lacking formalization or empirical validation.
  In this paper, we address these gaps by applying a formal ontology evaluation/development approach to define the informational requirements (IRs) necessary for semantic interoperability in the area of incentive-based DR for commercial buildings. We identify the IRs associated with each stage of the wholesale incentive-based DR process, focusing on the perspective of building owners. Using these IRs, we evaluate how well existing ontologies (Brick, DELTA, and EFOnt) support the operational needs of DR participation. Our findings reveal substantial misalignments between current ontologies and practical DR requirements. Based on our assessments, we propose a roadmap of necessary extensions and integrations for these ontologies. This work ultimately aims to enhance the interoperability of today's and future smart grid, thereby facilitating scalable integration of DR systems into the grid's complex operational framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01459v1</guid>
      <category>eess.SY</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ozan Baris Mulayim, Yuvraj Agarwal, Mario Berg\'es, Steve Schaefer, Mitali Shah, Derek Supple</dc:creator>
    </item>
    <item>
      <title>Traq: Estimating the Quantum Cost of Classical Programs</title>
      <link>https://arxiv.org/abs/2509.01508</link>
      <description>arXiv:2509.01508v1 Announce Type: cross 
Abstract: Predicting practical speedups offered by future quantum computers has become a major focus of the quantum computing community. Typically, these predictions are supported by lengthy manual analyses and numerical simulations and are carried out for one specific application at a time. In this paper, we present Traq, a principled approach towards estimating the quantum speedup of classical programs fully automatically and with provable guarantees. It consists of a classical language that includes high-level primitives amenable to quantum speedups, a cost analysis, and a compilation to low-level quantum programs. Our cost analysis upper bounds the complexity of the resulting quantum program in a fine-grained way: it captures non-asymptotic information and is sensitive to the input of the program (rather than providing worst-case costs). We also provide a proof-of-concept implementation and a case study inspired by AND-OR trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01508v1</guid>
      <category>quant-ph</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anurudh Peduri, Gilles Barthe, Michael Walter</dc:creator>
    </item>
    <item>
      <title>Disentangling the schema turn: Restoring the information base to conceptual modelling</title>
      <link>https://arxiv.org/abs/2509.01617</link>
      <description>arXiv:2509.01617v1 Announce Type: cross 
Abstract: If one looks at contemporary mainstream development practices for conceptual modelling in computer science, these so clearly focus on a conceptual schema completely separated from its information base that the conceptual schema is often just called the conceptual model. These schema-centric practices are crystallized in almost every database textbook. We call this strong, almost universal, bias towards conceptual schemas the schema turn. The focus of this paper is on disentangling this turn within (computer science) conceptual modeling. It aims to shed some light on how it emerged and so show that it is not fundamental. To show that modern technology enables the adoption of an inclusive schema-and-base conceptual modelling approach, which in turn enables more automated, and empirically motivated practices. And to show, more generally, the space of possible conceptual modelling practices is wider than currently assumed. It also uses the example of bCLEARer to show that the implementations in this wider space will probably need to rely on new pipeline-based conceptual modelling techniques. So, it is possible that the schema turn's complete exclusion of the information base could be merely a temporary evolutionary detour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01617v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Partridge, Andrew Mitchell, Sergio de Cesare, Oscar Xiberta Soto</dc:creator>
    </item>
    <item>
      <title>When Agents go Astray: Course-Correcting SWE Agents with PRMs</title>
      <link>https://arxiv.org/abs/2509.02360</link>
      <description>arXiv:2509.02360v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly deployed for complex, multi-step software engineering (SWE) tasks. However, their trajectories often contain costly inefficiencies, such as redundant exploration, looping, and failure to terminate once a solution is reached. Prior work has largely treated these errors in a post-hoc manner, diagnosing failures only after execution. In this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM) that intervenes during execution to detect and course-correct trajectory-level errors. Our PRM design leverages a taxonomy of common inefficiencies and delivers lightweight, interpretable feedback without modifying the underlying policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0% to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among feedback strategies, taxonomy-guided PRMs outperform unguided or explicit action-prescriptive variants, increasing success rate while reducing trajectory length. These benefits come at an acceptable added inference cost of as low as $0.2, making PRMs a practical and scalable mechanism for improving SWE agents' reliability and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02360v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Gandhi, Jason Tsay, Jatin Ganhotra, Kiran Kate, Yara Rizk</dc:creator>
    </item>
    <item>
      <title>Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs</title>
      <link>https://arxiv.org/abs/2509.02372</link>
      <description>arXiv:2509.02372v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become critical to modern software development, but their reliance on internet datasets for training introduces a significant security risk: the absorption and reproduction of malicious content. To evaluate this threat, this paper introduces a scalable, automated audit framework that synthesizes innocuous, developer-style prompts from known scam databases to query production LLMs and determine if they generate code containing harmful URLs. We conducted a large-scale evaluation across four production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and found a systemic vulnerability, with all tested models generating malicious code at a non-negligible rate. On average, 4.2\% of programs generated in our experiments contained malicious URLs. Crucially, this malicious code is often generated in response to benign prompts. We manually validate the prompts which cause all four LLMs to generate malicious code, and resulting in 177 innocuous prompts that trigger all models to produce harmful outputs. These results provide strong empirical evidence that the training data of production LLMs has been successfully poisoned at scale, underscoring the urgent need for more robust defense mechanisms and post-generation safety checks to mitigate the propagation of hidden security threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02372v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyang Chen, Tara Saba, Xun Deng, Xujie Si, Fan Long</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification</title>
      <link>https://arxiv.org/abs/2305.04228</link>
      <description>arXiv:2305.04228v4 Announce Type: replace 
Abstract: Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural networks (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order data correlations that already exist between nodes of the same field or called attribute in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose a heterogeneous directed hypergraph (HDHG) to represent AST and a heterogeneous directed hypergraph neural network (HDHGN) to process the graph for code classification. Our method improves code understanding and can represent high-order data correlations beyond paired interactions. We assess our heterogeneous directed hypergraph neural network (HDHGN) on public datasets of Python and Java programs. Our method outperforms previous AST-based and GNN-based methods, which demonstrates the capability of our model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04228v4</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.18293/SEKE2023-136</arxiv:DOI>
      <dc:creator>Guang Yang, Tiancheng Jin, Liang Dou</dc:creator>
    </item>
    <item>
      <title>Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing</title>
      <link>https://arxiv.org/abs/2312.10448</link>
      <description>arXiv:2312.10448v2 Announce Type: replace 
Abstract: Software crash bugs cause unexpected program behaviors or even abrupt termination, thus demanding immediate resolution. However, resolving crash bugs can be challenging due to their complex root causes, which can originate from issues in the source code or external factors like third-party library dependencies. Large language models (LLMs) have shown promise in software engineering tasks. However, existing research predominantly focuses on the capability of LLMs to localize and repair code-related crash bugs, leaving their effectiveness in resolving environment-related crash bugs in real-world software unexplored. To fill this gap, we conducted the first comprehensive study to assess the capability of LLMs in resolving real-world environment-related crash bugs. We first systematically compare LLMs' performance in resolving code-related and environment-related crash bugs with varying levels of crash contextual information. Our findings reveal that localization is the primary challenge for resolving code-related crashes, while repair poses a greater challenge for environment-related crashes. Furthermore, we investigate the impact of different prompt strategies on improving the resolution of environment-related crash bugs, incorporating different prompt templates and multi-round interactions. Building on this, we further explore an advanced active inquiry prompting strategy leveraging the self-planning capabilities of LLMs. Based on these explorations, we propose IntDiagSolver, an interactive methodology designed to enable precise crash bug resolution through ongoing engagement with LLMs. Extensive evaluations of IntDiagSolver across multiple LLMs (including GPT-3.5, GPT-4, Claude, CodeLlama, DeepSeek-R1, and Qwen-3-Coder) demonstrate consistent improvements in resolution accuracy, with substantial enhancements ranging from 9.1% to 43.3% in localization and 9.1% to 53.3% in repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10448v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueying Du, Mingwei Liu, Hanlin Wang, Juntao Li, Xin Peng, Yiling Lou</dc:creator>
    </item>
    <item>
      <title>On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance</title>
      <link>https://arxiv.org/abs/2403.17154</link>
      <description>arXiv:2403.17154v4 Announce Type: replace 
Abstract: Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers. This study aims to empirically assess the accuracy vs inference time trade-off of different black-box Edge AI deployment strategies, i.e., combinations of deployment operators and deployment tiers. In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers. Our findings suggest that Edge deployment using the hybrid Quantization + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss. However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a Quantization operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge) operators. In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment. For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies. For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17154v4</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaskirat Singh, Emad Fallahzadeh, Bram Adams, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Open Source Software Development Tool Installation: Challenges and Strategies For Novice Developers</title>
      <link>https://arxiv.org/abs/2404.14637</link>
      <description>arXiv:2404.14637v3 Announce Type: replace 
Abstract: As the world of technology advances, so do the tools that software developers use to create new programs. In recent years, software development tools have become more popular, allowing developers to work more efficiently and produce higher-quality software. Still, installing such tools can be challenging for novice developers at the early stage of their careers, as they may face challenges, such as compatibility issues (e.g., operating systems). Therefore, this work aims to investigate the challenges novice developers face in software development when installing software development tools. To investigate these, we conducted an analysis of 24 live software installation sessions to observe challenges and comprehend their actions, the strategies they apply, and the type of source of information they consult when encountering challenges. Our findings show that unclear documentation, such as installation instructions, and inadequate feedback during the installation process are common challenges faced by novice developers. Moreover, reformulating search queries and relying on non-official documentation were some of the strategies employed to overcome challenges. Based on our findings, we provide practical recommendations for tool vendors, tool users, and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14637v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Larissa Salerno, Christoph Treude, Patanamon Thongtatunam</dc:creator>
    </item>
    <item>
      <title>Towards Explainable Vulnerability Detection with Large Language Models</title>
      <link>https://arxiv.org/abs/2406.09701</link>
      <description>arXiv:2406.09701v4 Announce Type: replace 
Abstract: Software vulnerabilities pose significant risks to the security and integrity of software systems. Although prior studies have explored vulnerability detection using deep learning and pre-trained models, these approaches often fail to provide the detailed explanations necessary for developers to understand and remediate vulnerabilities effectively. The advent of large language models (LLMs) has introduced transformative potential due to their advanced generative capabilities and ability to comprehend complex contexts, offering new possibilities for addressing these challenges. In this paper, we propose LLMVulExp, an automated framework designed to specialize LLMs for the dual tasks of vulnerability detection and explanation. To address the challenges of acquiring high-quality annotated data and injecting domain-specific knowledge, LLMVulExp leverages prompt-based techniques for annotating vulnerability explanations and finetunes LLMs using instruction tuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect vulnerability types in code while generating detailed explanations, including the cause, location, and repair suggestions. Additionally, we employ a Chain-of-Thought (CoT) based key code extraction strategy to focus LLMs on analyzing vulnerability-prone code, further enhancing detection accuracy and explanatory depth. Our experimental results demonstrate that LLMVulExp achieves over a 90% F1 score on the SeVC dataset, effectively combining high detection accuracy with actionable and coherent explanations. This study highlights the feasibility of utilizing LLMs for real-world vulnerability detection and explanation tasks, providing critical insights into their adaptation and application in software security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09701v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiheng Mao, Zhenhao Li, Xing Hu, Kui Liu, Xin Xia, Jianling Sun</dc:creator>
    </item>
    <item>
      <title>On the correlation between Architectural Smells and Static Analysis Warnings</title>
      <link>https://arxiv.org/abs/2406.17354</link>
      <description>arXiv:2406.17354v2 Announce Type: replace 
Abstract: Background. Software quality assurance is essential during software development and maintenance. Static Analysis Tools (SAT) are widely used for assessing code quality. Architectural smells are becoming more daunting to address and evaluate among quality issues.
  Objective. We aim to understand the relationships between static analysis warnings (SAW) and architectural smells (AS) to guide developers/maintainers in focusing their efforts on SAWs more prone to co-occurring with AS.
  Method. We performed an empirical study on 103 Java projects totaling 72 million LOC belonging to projects from a vast set of domains, and 785 SAW detected by four SAT, Checkstyle, Findbugs, PMD, SonarQube, and 4 architectural smells detected by ARCAN tool. We analyzed how SAWs influence AS presence. Finally, we proposed an AS remediation effort prioritization based on SAW severity and SAW proneness to specific ASs.
  Results. Our study reveals a moderate correlation between SAWs and ASs. Different combinations of SATs and SAWs significantly affect AS occurrence, with certain SAWs more likely to co-occur with specific ASs. Conversely, 33.79% of SAWs act as "healthy carriers", not associated with any ASs.
  Conclusion. Practitioners can ignore about a third of SAWs and focus on those most likely to be associated with ASs. Prioritizing AS remediation based on SAW severity or SAW proneness to specific ASs results in effective rankings like those based on AS severity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17354v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Mikel Robredo, Francesca Arcelli Fontana, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>Model-guided Fuzzing of Distributed Systems</title>
      <link>https://arxiv.org/abs/2410.02307</link>
      <description>arXiv:2410.02307v3 Announce Type: replace 
Abstract: We present a coverage-guided testing algorithm for distributed systems implementations. Our main innovation is the use of an abstract formal model of the system that is used to define coverage. Such abstract models are frequently developed in early phases of protocol design and verification but are infrequently used at testing time. We show that guiding random test generation using model coverage can be effective in covering interesting points in the implementation state space. We have implemented a fuzzer for distributed system implementations and abstract models written in TLA+. Our algorithm shows better coverage over purely random exploration as well as random exploration guided by different notions of scheduler coverage and mutation. In particular, we show consistently higher coverage and detect bugs faster on implementations of distributed consensus protocols such as those in Etcd-raft and RedisRaft. Moreover, we discovered 13 previously unknown bugs in their implementations, four of which could only be detected by model-guided fuzzing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02307v3</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ege Berkay Gulcan, Burcu Kulahcioglu Ozkan, Rupak Majumdar, Srinidhi Nagendra</dc:creator>
    </item>
    <item>
      <title>Are Decoder-Only Large Language Models the Silver Bullet for Code Search?</title>
      <link>https://arxiv.org/abs/2410.22240</link>
      <description>arXiv:2410.22240v2 Announce Type: replace 
Abstract: Code search is essential for code reuse, allowing developers to efficiently locate relevant code snippets. The advent of powerful decoder-only Large Language Models (LLMs) has revolutionized many code intelligence tasks. However, their effectiveness for the retrieval-based task of code search, particularly compared to established encoder-based models, remains underexplored. This paper addresses this gap by presenting a large-scale systematic evaluation of eleven decoder-only LLMs, analyzing their performance across zero-shot and fine-tuned settings.
  Our results show that fine-tuned decoder-only models, particularly CodeGemma, significantly outperform encoder-only models like UniXcoder, achieving a 40.4% higher Mean Average Precision (MAP) on the CoSQA$^+$ benchmark. Our analysis further reveals two crucial nuances for practitioners: first, the relationship between model size and performance is non-monotonic, with mid-sized models often outperforming larger variants; second, the composition of the training data is critical, as a multilingual dataset enhances generalization while a small amount of data from a specific language can act as noise and interfere with model effectiveness. These findings offer a comprehensive guide to selecting and optimizing modern LLMs for code search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22240v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Chen, Mingwei Liu, Guangsheng Ou, Anji Li, Dekun Dai, Yanlin Wang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>More Rigorous Software Engineering Would Improve Reproducibility in Machine Learning Research</title>
      <link>https://arxiv.org/abs/2502.00902</link>
      <description>arXiv:2502.00902v2 Announce Type: replace 
Abstract: While experimental reproduction remains a pillar of the scientific method, we observe that the software best practices supporting the reproduction of machine learning ( ML ) research are often undervalued or overlooked, leading both to poor reproducibility and damage to trust in the ML community. We quantify these concerns by surveying the usage of software best practices in software repositories associated with publications at major ML conferences and journals such as NeurIPS, ICML, ICLR, TMLR, and MLOSS within the last decade. We report the results of this survey that identify areas where software best practices are lacking and areas with potential for growth in the ML community. Finally, we discuss the implications and present concrete recommendations on how we, as a community, can improve reproducibility in ML research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00902v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Moritz Wolter, Lokesh Veeramacheneni, Charles Tapley Hoyt</dc:creator>
    </item>
    <item>
      <title>Why Johnny Signs with Next-Generation Tools: A Usability Case Study of Sigstore</title>
      <link>https://arxiv.org/abs/2503.00271</link>
      <description>arXiv:2503.00271v5 Announce Type: replace 
Abstract: Software signing is the most robust method for ensuring the integrity and authenticity of components in a software supply chain. Traditional signing tools burdened practitioners with key management and signer identification, creating both usability challenges and security risks. A new class of next-generation signing tools has automated many of these concerns, but little is known about their usability and its effect on adoption and effectiveness in practice. A usability evaluation can clarify the extent to which next-generation designs succeed and highlight priorities for improvement.
  To fill this gap, we conducted a usability study of Sigstore, a pioneering and widely adopted exemplar of next-generation signing. Through interviews with 17 industry experts, we examined (1) the problems and advantages associated with practitioners' tooling choices, (2) how and why their signing-tool usage has evolved over time, and (3) the contexts that cause usability concerns. Our findings illuminate the usability factors of next-generation signing tools and yield recommendations for toolmakers, adopting organizations, and the research community. Notably, components of next-generation tooling exhibit different levels of maturity and readiness for adoption, and integration flexibility is a common pain point, but potentially mitigable through plugins and APIs. Our results will help next-generation signing toolmakers further strengthen software supply chain security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00271v5</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelechi G. Kalu, Sofia Okorafor, Tanmay Singla, Sophie Chen, Santiago Torres-Arias, James C. Davis</dc:creator>
    </item>
    <item>
      <title>Reducing Friction in Cloud Migration of Services</title>
      <link>https://arxiv.org/abs/2503.07169</link>
      <description>arXiv:2503.07169v2 Announce Type: replace 
Abstract: Public cloud services are integral to modern software development, offering scalability and flexibility to organizations. Based on customer requests, a large product development organization considered migrating the microservice-based product deployments of a large customer to a public cloud provider.
  We conducted an exploratory single-case study, utilizing quantitative and qualitative data analysis to understand how and why deployment costs would change when transitioning the product from a private to a public cloud environment while preserving the software architecture. We also isolated the major factors driving the changes in deployment costs.
  We found that switching to the customer-chosen public cloud provider would increase costs by up to 50\%, even when sharing some resources between deployments, and limiting the use of expensive cloud services such as security log analyzers. A large part of the cost was related to the sizing and license costs of the existing relational database, which was running on Virtual Machines in the cloud. We also found that existing system integrators, using the product via its API, were likely to use the product inefficiently, in many cases causing at least 10\% more load to the system than needed.
  From a deployment cost perspective, successful migration to a public cloud requires considering the entire system architecture, including services like relational databases, value-added cloud services, and enabled product features. Our study highlights the importance of leveraging end-to-end usage data to assess and manage these cost drivers effectively, especially in environments with elastic costs, such as public cloud deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07169v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anders Sundelin, Javier Gonzalez-Huerta, Krzysztof Wnuk</dc:creator>
    </item>
    <item>
      <title>An Explanation of Software Architecture Explanations</title>
      <link>https://arxiv.org/abs/2503.08628</link>
      <description>arXiv:2503.08628v2 Announce Type: replace 
Abstract: Software architecture knowledge transfer is essential for software development, but related documentation is often incomplete or ambiguous, making oral explanations a common means. Our broader aim is to explore how such explanations might be better supported and eventually automated; as a prerequisite, we first investigate how explanations are actually conducted in practice across five areas: explanation topics, explanation plans, supporting artefacts, typical questions, and expectations and challenges. We report on semi-structured interviews with 17 software professionals across diverse organisations and countries. Our findings include that explanations must balance problem- and technical-domain while considering the explainee's role, experience, and goals. Moreover, explainees frequently seek not only structure and behaviour, but also decision rationale. We propose the Explanation Window, a framework for focusing information by adjusting functionality scope and detail, and emphasise the importance of including system context. These findings provide an empirical basis for improving architecture explanations and guiding future work on tool support and automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08628v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satrio Adi Rukmono, Filip Zamfirov, Lina Ochoa, Floris Pex, Michel Chaudron</dc:creator>
    </item>
    <item>
      <title>Change Logging and Mining of Change Logs of Business Processes -- A Literature Review</title>
      <link>https://arxiv.org/abs/2504.14627</link>
      <description>arXiv:2504.14627v2 Announce Type: replace 
Abstract: Context: Change mining enables organizations to understand the changes that occurred in their business processes. This allows them to enhance their business processes and adapt to dynamic environments. Therefore, change mining is becoming a topic of interest for researchers, scholars, and practitioners.
  Objective: Motivated by the goal of establishing the state of the art in this area, this paper aims to investigate the literature in change logging and mining in process-aware information systems, provide an overview of the methods that are used in the existing publications, and identify gaps in the research on the topic of logging and mining process changes.
  Method: A literature review is conducted with the objective to identify and define methods to mine, store, and record changes in business processes. From 1136 publications, we selected 6 papers related to changes in business process and extended the list to 9 papers by including the relevant articles referenced by the papers that we selected originally.
  Results: In answer of our research questions, we have identified two classes of change mining methods, two ways of recording the changes into change logs, five formats for change log representation, and four objectives to be learned from changes.
  Conclusion: The literature review provides a summary of existing change mining and logging methods in process-aware information systems and identifies a number of research gaps in the area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14627v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arash Yadegari Ghahderijani, Hande Naz Turgay, Dimka Karastoyanova</dc:creator>
    </item>
    <item>
      <title>Tech-ASan: Two-stage check for Address Sanitizer</title>
      <link>https://arxiv.org/abs/2506.05022</link>
      <description>arXiv:2506.05022v4 Announce Type: replace 
Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety violations, including temporal and spatial errors hidden in C/C++ programs during execution. However, ASan incurs significant runtime overhead, which limits its efficiency in testing large software. The overhead mainly comes from sanitizer checks due to the frequent and expensive shadow memory access. Over the past decade, many methods have been developed to speed up ASan by eliminating and accelerating sanitizer checks, however, they either fail to adequately eliminate redundant checks or compromise detection capabilities. To address this issue, this paper presents Tech-ASan, a two-stage check based technique to accelerate ASan with safety assurance. First, we propose a novel two-stage check algorithm for ASan, which leverages magic value comparison to reduce most of the costly shadow memory accesses. Second, we design an efficient optimizer to eliminate redundant checks, which integrates a novel algorithm for removing checks in loops. Third, we implement Tech-ASan as a memory safety tool based on the LLVM compiler infrastructure. Our evaluation using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative cases than ASan and ASan-- when testing on the Juliet Test Suite under the same redzone setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05022v4</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3755881.3755918</arxiv:DOI>
      <dc:creator>Yixuan Cao, Yuhong Feng, Huafeng Li, Chongyi Huang, Fangcao Jian, Haoran Li, Xu Wang</dc:creator>
    </item>
    <item>
      <title>Waterfall Model Simulation: A Systematic Mapping Study</title>
      <link>https://arxiv.org/abs/2506.19653</link>
      <description>arXiv:2506.19653v2 Announce Type: replace 
Abstract: This paper systematically maps peer-reviewed research and graduate theses/dissertations that explicitly simulate the waterfall model. Following Petersen's mapping guidelines and Kitchenham's systematic literature review practices, major databases (ACM Digital Library, IEEE Xplore, Scopus, Springer, Google Scholar, and Web of Science) were searched for studies published between 2000-2024 using the title query ("simulation" OR "simulating") AND "waterfall". A PRISMA workflow guided the screening process, and approximately 9% of retrieved records met the inclusion criteria. A repeated extraction process captured methods, tools, venues, geography, publication years, comparative scope, and fidelity to Royce's original model; findings were synthesized thematically. Discrete-event simulation dominates (80%) compared to system dynamics (20%). Reported tools center on Simphony.NET (40%) and SimPy (20%), while 40% of studies omit tool details, limiting reproducibility. Research is distributed across Italy, Lebanon, India, Japan, and the United States; publication venues include 60% journals and 40% conferences. Sixty percent of studies are comparative, while 40% model only the waterfall approach. No study reproduces Royce's original model; all employ adaptations. The paper concludes by presenting a consolidated view of waterfall simulation research and recommending clearer model reporting, fuller tool disclosure, and wider adoption of open-source platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19653v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Saravanos (New York University)</dc:creator>
    </item>
    <item>
      <title>PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning</title>
      <link>https://arxiv.org/abs/2507.05995</link>
      <description>arXiv:2507.05995v4 Announce Type: replace 
Abstract: The high configurability of modern software systems has made configuration tuning a crucial step for assuring system performance, e.g., latency or throughput. However, given the expensive measurements, large configuration space, and rugged configuration landscape, existing tuners suffer ineffectiveness due to the difficult balance of budget utilization between exploring uncertain regions (for escaping from local optima) and exploiting guidance of known good configurations (for fast convergence). The root cause is that we lack knowledge of where the promising regions lay, which also causes challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by causally purified rules. PromiseTune is unique in the sense that we learn rules, which reflect certain regions in the configuration landscape, and purify them with causal inference. The remaining rules serve as approximated reflections of the promising regions, bounding the tuning to emphasize these places in the landscape. This, as we demonstrate, can effectively mitigate the impact of the exploration and exploitation trade-off. Those purified regions can then be paired with the measured configurations to provide spatial explainability at the landscape level. Comparing with 11 state-of-the-art tuners on 12 systems and varying budgets, we show that PromiseTune performs significantly better than the others with 42% superior rank to the overall second best while providing richer information to explain the hidden system characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05995v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3764552</arxiv:DOI>
      <dc:creator>Pengzhou Chen, Tao Chen</dc:creator>
    </item>
    <item>
      <title>MCeT: Behavioral Model Correctness Evaluation using Large Language Models</title>
      <link>https://arxiv.org/abs/2508.00630</link>
      <description>arXiv:2508.00630v2 Announce Type: replace 
Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00630v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng</dc:creator>
    </item>
    <item>
      <title>Tool-Assisted Conformance Checking to Reference Process Models</title>
      <link>https://arxiv.org/abs/2508.00738</link>
      <description>arXiv:2508.00738v3 Announce Type: replace 
Abstract: Reference models convey best practices and standards. The reference frameworks necessitate conformance checks to ensure adherence to established guidelines and principles, which is crucial for maintaining quality and consistency in various processes. This paper explores automated conformance checks for concrete process models against reference models using causal dependency analysis of tasks and events. Existing notions of conformance checking for process models focus on verifying process execution traces and lack the expressiveness and automation needed for semantic model comparison, leaving this question unresolved. We integrate our approach into a broader semantic framework for defining reference model conformance. We outline an algorithm for reference process model conformance checking, evaluate it through a case study, and discuss its strengths and limitations. Our research provides a tool-assisted solution enhancing accuracy and flexibility in process model conformance verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00738v3</guid>
      <category>cs.SE</category>
      <category>cs.FL</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Rumpe, Max Stachon, Sebastian St\"uber, Valdes Voufo</dc:creator>
    </item>
    <item>
      <title>Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures</title>
      <link>https://arxiv.org/abs/2508.00749</link>
      <description>arXiv:2508.00749v2 Announce Type: replace 
Abstract: In the context of model-driven development, ensuring the correctness and consistency of evolving models is paramount. This paper investigates the application of Dynamic Symbolic Execution (DSE) for semantic difference analysis of component-and-connector architectures, specifically utilizing MontiArc models. We have enhanced the existing MontiArc-to-Java generator to gather both symbolic and concrete execution data at runtime, encompassing transition conditions, visited states, and internal variables of automata. This data facilitates the identification of significant execution traces that provide critical insights into system behavior. We evaluate various execution strategies based on the criteria of runtime efficiency, minimality, and completeness, establishing a framework for assessing the applicability of DSE in semantic difference analysis. Our findings indicate that while DSE shows promise for analyzing component and connector architectures, scalability remains a primary limitation, suggesting further research is needed to enhance its practical utility in larger systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00749v2</guid>
      <category>cs.SE</category>
      <category>cs.FL</category>
      <category>cs.SC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Grahl, Bernhard Rumpe, Max Stachon, Sebastian St\"uber</dc:creator>
    </item>
    <item>
      <title>SATORI: Static Test Oracle Generation for REST APIs</title>
      <link>https://arxiv.org/abs/2508.16318</link>
      <description>arXiv:2508.16318v2 Announce Type: replace 
Abstract: REST API test case generation tools are evolving rapidly, with growing capabilities for the automated generation of complex tests. However, despite their strengths in test data generation, these tools are constrained by the types of test oracles they support, often limited to crashes, regressions, and noncompliance with API specifications or design standards. This paper introduces SATORI (Static API Test ORacle Inference), a black-box approach for generating test oracles for REST APIs by analyzing their OpenAPI Specification. SATORI uses large language models to infer the expected behavior of an API by analyzing the properties of the response fields of its operations, such as their name and descriptions. To foster its adoption, we extended the PostmanAssertify tool to automatically convert the test oracles reported by SATORI into executable assertions. Evaluation results on 17 operations from 12 industrial APIs show that SATORI can automatically generate up to hundreds of valid test oracles per operation. SATORI achieved an F1-score of 74.3%, outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which requires executing the API-when generating comparable oracle types. Moreover, our findings show that static and dynamic oracle inference methods are complementary: together, SATORI and AGORA+ found 90% of the oracles in our annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo) leading to documentation updates by the API maintainers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16318v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan C. Alonso, Alberto Martin-Lopez, Sergio Segura, Gabriele Bavota, Antonio Ruiz-Cort\'es</dc:creator>
    </item>
    <item>
      <title>Agentic AI for Software: thoughts from Software Engineering community</title>
      <link>https://arxiv.org/abs/2508.17343</link>
      <description>arXiv:2508.17343v3 Announce Type: replace 
Abstract: AI agents have recently shown significant promise in software engineering. Much public attention has been transfixed on the topic of code generation from Large Language Models (LLMs) via a prompt. However, software engineering is much more than programming, and AI agents go far beyond instructions given by a prompt.
  At the code level, common software tasks include code generation, testing, and program repair. Design level software tasks may include architecture exploration, requirements understanding, and requirements enforcement at the code level. Each of these software tasks involves micro-decisions which can be taken autonomously by an AI agent, aided by program analysis tools. This creates the vision of an AI software engineer, where the AI agent can be seen as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based software workflows will be to resolve the core difficulty in software engineering - the deciphering and clarification of developer intent. Specification inference, or deciphering the intent, thus lies at the heart of many software tasks, including software maintenance and program repair. A successful deployment of agentic technology into software engineering would involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes more automated. Higher automation also leads to higher volume of code being automatically generated, and then integrated into code-bases. Thus to deal with this explosion, an emerging direction is AI-based verification and validation (V &amp; V) of AI generated code. We posit that agentic software workflows in future will include such AIbased V&amp;V.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17343v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhik Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Efficient Detection of Toxic Prompts in Large Language Models</title>
      <link>https://arxiv.org/abs/2408.11727</link>
      <description>arXiv:2408.11727v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%, outperforming state-of-the-art methods. Additionally, ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11727v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu</dc:creator>
    </item>
    <item>
      <title>ORCAS: Obfuscation-Resilient Binary Code Similarity Analysis using Dominance Enhanced Semantic Graph</title>
      <link>https://arxiv.org/abs/2506.06161</link>
      <description>arXiv:2506.06161v2 Announce Type: replace-cross 
Abstract: Binary code similarity analysis (BCSA) serves as a foundational technique for binary analysis tasks such as vulnerability detection and malware identification. Existing graph based BCSA approaches capture more binary code semantics and demonstrate remarkable performance. However, when code obfuscation is applied, the unstable control flow structure degrades their performance. To address this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based on Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary code representation, capturing more binaries' implicit semantics without control flow structure, including inter-instruction relations (e.g., def-use), inter-basic block relations (i.e., dominance and post-dominance), and instruction-basic block relations. ORCAS takes binary functions from different obfuscation options, optimization levels, and instruction set architectures as input and scores their semantic similarity more robustly. Extensive experiments have been conducted on ORCAS against eight baseline approaches over the BinKit dataset. For example, ORCAS achieves an average 12.1% PR-AUC improvement when using combined three obfuscation options compared to the state-of-the-art approaches. In addition, an original obfuscated real-world vulnerability dataset has been constructed and released to facilitate a more comprehensive research on obfuscated binary code analysis. ORCAS outperforms the state-of-the-art approaches over this newly released real-world vulnerability dataset by up to a recall improvement of 43%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06161v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3746252.3761266</arxiv:DOI>
      <dc:creator>Yufeng Wang, Yuhong Feng, Yixuan Cao, Haoran Li, Haiyue Feng, Yifeng Wang</dc:creator>
    </item>
  </channel>
</rss>
