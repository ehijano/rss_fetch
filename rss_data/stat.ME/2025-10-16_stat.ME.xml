<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Inference for Single-factor Graphical Models</title>
      <link>https://arxiv.org/abs/2510.13981</link>
      <description>arXiv:2510.13981v1 Announce Type: new 
Abstract: We introduce efficient MCMC algorithms for Bayesian inference for single-factor models with correlated residuals where the residuals' distribution is a Gaussian graphical model. We call this family of models single-factor graphical models. We extend single-factor graphical models to datasets that also involve binary and ordinal categorical variables and to the modeling of multiple datasets that are spatially or temporally related. Our models are able to capture multivariate associations through latent factors across time and space, as well as residual conditional dependence structures at each spatial location or time point through Gaussian graphical models. We illustrate the application of single-factor graphical models in simulated and real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13981v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Marcano, Adrian Dobra</dc:creator>
    </item>
    <item>
      <title>Joint modeling and inference of multiple-subject high-dimensional sparse vector autoregressive models</title>
      <link>https://arxiv.org/abs/2510.14044</link>
      <description>arXiv:2510.14044v1 Announce Type: new 
Abstract: The multiple-subject vector autoregression (multi-VAR) model captures heterogeneous network Granger causality across subjects by decomposing individual sparse VAR transition matrices into commonly shared and subject-unique paths. The model has been applied to characterize hidden shared and unique paths among subjects and has demonstrated performance compared to methods commonly used in psychology and neuroscience. Despite this innovation, the model suffers from using a weighted median for identifying the common effects, leading to statistical inefficiency as the convergence rates of the common and unique paths are determined by the least sparse subject and the smallest sample size across all subjects. We propose a new identifiability condition for the multi-VAR model based on a communication-efficient data integration framework. We show that this approach achieves convergence rates tailored to each subject's sparsity level and sample size. Furthermore, we develop hypothesis tests to assess the nullity and homogeneity of individual paths, using Wald-type test statistics constructed from individual debiased estimators. A test for the significance of the common paths can also be derived through the framework. Simulation studies under various heterogeneity scenarios and a real data application demonstrate the performance of the proposed method compared to existing benchmark across standard evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14044v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation of the Complier General Causal Effect in Randomized Controlled Trials with One-Sided Noncompliance</title>
      <link>https://arxiv.org/abs/2510.14142</link>
      <description>arXiv:2510.14142v1 Announce Type: new 
Abstract: A randomized controlled trial (RCT) is widely regarded as the gold standard for assessing the causal effect of a treatment or intervention, assuming perfect implementation. In practice, however, randomization can be compromised for various reasons, such as one-sided noncompliance. In this paper, we address the issue of one-sided noncompliance and propose a general estimand, the complier general causal effect (CGCE), to characterize the causal effect among compliers. We further investigate the conditions under which efficient estimation of the CGCE can be achieved under minimal assumptions. Comprehensive simulation studies and a real data application are conducted to illustrate the proposed methods and to compare them with existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14142v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Tang, Yanyuan Ma, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for the uniformity of random geometric graph</title>
      <link>https://arxiv.org/abs/2510.14210</link>
      <description>arXiv:2510.14210v1 Announce Type: new 
Abstract: Random geometric graphs are widely used in modeling geometry and dependence structure in networks. In a random geometric graph, nodes are independently generated from some probability distribution $F$ over a metric space, and edges link nodes if their distance is less than some threshold. Most studies assume the distribution $F$ to be uniform. However, recent research shows that some real-world networks may be better modeled by nonuniform distribution $F$. Moreover, graphs with nonuniform $F$ have notably different properties from graphs with uniform $F$. A fundamental question is: given a network from a random geometric graph, is the distribution $F$ uniform or not? In this paper, we approach this question through hypothesis testing. This problem is particularly challenging due to the inherent dependencies among edges in random geometric graphs, a property not present in classic random graphs. We propose the first statistical test. Under the null hypothesis, the test statistic converges in distribution to the standard normal distribution. The asymptotic distribution is derived using the asymptotic theory of degenerate U-statistics with a kernel function dependent on the number of nodes. This technique is different from existing methods in network hypothesis testing problems. In addition, we present a method for efficiently calculating the test statistic directly from the adjacency matrix. We also analytically characterize the power of the proposed test. The simulation study shows that the proposed uniformity test has high power. Real data applications are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14210v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingao Yuan</dc:creator>
    </item>
    <item>
      <title>Marginal Causal Effect Estimation with Continuous Instrumental Variables</title>
      <link>https://arxiv.org/abs/2510.14368</link>
      <description>arXiv:2510.14368v1 Announce Type: new 
Abstract: Instrumental variables (IVs) are often continuous, arising in diverse fields such as economics, epidemiology, and the social sciences. Existing approaches for continuous IVs typically impose strong parametric models or assume homogeneous treatment effects, while fully nonparametric methods may perform poorly in moderate- to high-dimensional covariate settings. We propose a new framework for identifying the average treatment effect with continuous IVs via conditional weighted average derivative effects. Using a conditional Riesz representer, our framework unifies continuous and categorical IVs. In this framework, the average treatment effect is typically overidentified, leading to a semiparametric observed-data model with a nontrivial tangent space. Characterizing this tangent space involves a delicate construction of a second-order parametric submodel, which, to the best of our knowledge, has not been standard practice in this literature. For estimation, building on an influence function in the semiparametric model that is also locally efficient within a submodel, we develop a locally efficient, triply robust, bounded, and easy-to-implement estimator. We apply our methods to an observational clinical study from the Princess Margaret Cancer Centre to examine the so-called obesity paradox in oncology, assessing the causal effect of excess body weight on two-year mortality among patients with non-small cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14368v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mei Dong, Lin Liu, Dingke Tang, Geoffrey Liu, Wei Xu, Linbo Wang</dc:creator>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>https://arxiv.org/abs/2510.14415</link>
      <description>arXiv:2510.14415v1 Announce Type: new 
Abstract: This paper develops a sensitivity analysis framework that transfers the average total treatment effect (ATTE) from source data with a fully observed network to target data whose network is completely unknown. The ATTE represents the average social impact of a policy that assigns the treatment to every individual in the dataset. We postulate a covariate-shift type assumption that both source and target datasets share the same conditional mean outcome. However, because the target network is unobserved, this assumption alone is not sufficient to pin down the ATTE for the target data. To address this issue, we consider a sensitivity analysis based on the uncertainty of the target network's degree distribution, where the extent of uncertainty is measured by the Wasserstein distance from a given reference degree distribution. We then construct bounds on the target ATTE using a linear programming-based estimator. The limiting distribution of the bound estimator is derived via the functional delta method, and we develop a wild bootstrap approach to approximate the distribution. As an empirical illustration, we revisit the social network experiment on farmers' weather insurance adoption in China by Cai et al. (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14415v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tadao Hoshino</dc:creator>
    </item>
    <item>
      <title>ROC Analysis with Covariate Adjustment Using Neural Network Models: Evaluating the Role of Age in the Physical Activity-Mortality Association</title>
      <link>https://arxiv.org/abs/2510.14494</link>
      <description>arXiv:2510.14494v1 Announce Type: new 
Abstract: The receiver operating characteristic (ROC) curve and its summary measure, the Area Under the Curve (AUC), are well-established tools for evaluating the efficacy of biomarkers in biomedical studies. Compared to the traditional ROC curve, the covariate-adjusted ROC curve allows for individual evaluation of the biomarker. However, the use of machine learning models has rarely been explored in this context, despite their potential to develop more powerful and sophisticated approaches for biomarker evaluation. The goal of this paper is to propose a framework for neural network-based covariate-adjusted ROC modeling that allows flexible and nonlinear evaluation of the effectiveness of a biomarker to discriminate between two reference populations. The finite-sample performance of our method is investigated through extensive simulation tests under varying dependency structures between biomarkers, covariates, and referenced populations. The methodology is further illustrated in a clinically case study that assesses daily physical activity - measured as total activity time (TAC), a proxy for daily step count-as a biomarker to predict mortality at three, five and eight years. Analyzes stratified by sex and adjusted for age and BMI reveal distinct covariate effects on mortality outcomes. These results underscore the importance of covariate-adjusted modeling in biomarker evaluation and highlight TAC's potential as a functional capacity biomarker based on specific individual characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14494v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziad Akram Ali Hammouri, Yating Zhou, Rahul Ghosal, Juan C. Vidal, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Additive Density Regression</title>
      <link>https://arxiv.org/abs/2510.14502</link>
      <description>arXiv:2510.14502v1 Announce Type: new 
Abstract: We present a structured additive regression approach to model conditional densities given scalar covariates, where only samples of the conditional distributions are observed. This links our approach to distributional regression models for scalar data. The model is formulated in a Bayes Hilbert space -- preserving nonnegativity and integration to one under summation and scalar multiplication -- with respect to an arbitrary finite measure. This allows to consider, amongst others, continuous, discrete and mixed densities. Our theoretical results include asymptotic existence, uniqueness, consistency, and asymptotic normality of the penalized maximum likelihood estimator, as well as confidence regions and inference for the (effect) densities. For estimation, we propose to maximize the penalized log-likelihood corresponding to an appropriate multinomial, or equivalently, Poisson regression model, which we show to approximate the original penalized maximum likelihood problem. We apply our framework to a motivating gender economic data set from the German Socio-Economic Panel Study (SOEP), analyzing the distribution of the woman's share in a couple's total labor income given covariate effects for year, place of residence and age of the youngest child. As the income share is a continuous variable having discrete point masses at zero and one for single-earner couples, the corresponding densities are of mixed type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14502v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva-Maria Maier, Alexander Fottner, Sonja Greven, Almond St\"ocker</dc:creator>
    </item>
    <item>
      <title>Reframing cross-world independence for identifying path-specific effects</title>
      <link>https://arxiv.org/abs/2510.14559</link>
      <description>arXiv:2510.14559v1 Announce Type: new 
Abstract: Understanding causal mechanisms in complex systems requires evaluating path-specific effects (PSEs) in multi-mediator models. Identification of PSEs traditionally relies on the demanding cross-world independence assumption. To relax this, VanderWeele et al. (2014) proposed an interventional approach that redefines PSEs, while Stensrud et al. (2021) introduced dismissible component conditions for identifying separable effects under competing risks. In this study, we leverage SWIGs to dissect the causal foundations of these three semantics and make two key advances. First, we generalize separable effects beyond competing risks to the setting of multi-mediator models and derive the assumptions required for their identification. Second, we unify the three approaches by clarifying how they interpret counterfactual outcomes differently: as mediator-driven effects (classical), randomized contrasts (interventional), or component-specific actions (separable). We further demonstrate that violations of cross-world independence originate from mediators omitted from the model. By analogy to confounder control, we argue that just as exchangeability is achieved by conditioning on sufficient confounders, cross-world independence can be approximated by including sufficient mediators. This reframing turns an abstract assumption into a tangible modeling strategy, offering a more practical path forward for applied mediation analysis in complex causal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14559v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>En-Yu Lai, Jih-Chang Yu, Yen-Tsung Huang</dc:creator>
    </item>
    <item>
      <title>Accurate Bayesian inference for tail risk extrapolation in time series</title>
      <link>https://arxiv.org/abs/2510.14637</link>
      <description>arXiv:2510.14637v1 Announce Type: new 
Abstract: Accurately quantifying tail risks-rare but high-impact events such as financial crashes or extreme weather-is a central challenge in risk management, with serially dependent data. We develop a Bayesian framework based on the Generalized Pareto (GP) distribution for modeling threshold exceedances, providing posterior distributions for the GP parameters and tail quantiles in time series. Two cases are considered: extrapolation of tail quantiles for the stationary marginal distribution under beta-mixing dependence, and dynamic, past-conditional tail quantiles in heteroscedastic regression models. The proposal yields asymptotically honest credible regions, whose coverage probabilities converge to their nominal levels. We establish the asymptotic theory for the Bayesian procedure, deriving conditions on the prior distributions under which the posterior satisfies key asymptotic properties. To achieve this, we first develop a likelihood theory under serial dependence, providing local and global bounds for the empirical log-likelihood process of the misspecified GP model and deriving corresponding asymptotic properties of the Maximum Likelihood Estimator (MLE). Simulations demonstrate that our Bayesian credible regions outperform naive Bayesian and MLE-based confidence regions across several standard time series models, including ARMA, GARCH, and Markovian copula models. Two real-data applications-to U.S. interest rates and Swiss electricity demand-highlight the relevance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14637v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David L. Carl, Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Hierarchical shot-noise Cox process mixtures for clustering across groups</title>
      <link>https://arxiv.org/abs/2510.14681</link>
      <description>arXiv:2510.14681v1 Announce Type: new 
Abstract: Clustering observations across partially exchangeable groups of data is a routine task in Bayesian nonparametrics. Previously proposed models allow for clustering across groups by sharing atoms in the group-specific mixing measures. However, exact atom sharing can be overly rigid when groups differ subtly, introducing a trade-off between clustering and density estimates and fragmenting across-group clusters, particularly at larger sample sizes. We introduce the hierarchical shot-noise Cox process (HSNCP) mixture model, where group-specific atoms concentrate around shared centers through a kernel. This enables accurate density estimation within groups and flexible borrowing across groups, overcoming the density-clustering trade-off of previous approaches. Our construction, built on the shot-noise Cox process, remains analytically tractable: we derive closed-form prior moments and an inter-group correlation, obtain the marginal law and predictive distribution for latent parameters, as well as the posterior of the mixing measures given the latent parameters. We develop an efficient conditional MCMC algorithm for posterior inference. We assess the performance of the HSNCP model through simulations and an application to a large galaxy dataset, demonstrating balanced across-group clusters and improved density estimates compared with the hierarchical Dirichlet process, including under model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14681v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Carminati, Mario Beraha, Federico Camerlenghi, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Response to Discussions of "Causal and Counterfactual Views of Missing Data Models"</title>
      <link>https://arxiv.org/abs/2510.14694</link>
      <description>arXiv:2510.14694v1 Announce Type: new 
Abstract: We are grateful to the discussants, Levis and Kennedy [2025], Luo and Geng [2025], Wang and van der Laan [2025], and Yang and Kim [2025], for their thoughtful comments on our paper (Nabi et al., 2025). In this rejoinder, we summarize our main contributions and respond to each discussion in turn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14694v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, James M. Robins</dc:creator>
    </item>
    <item>
      <title>Loss functions arising from the index of agreement</title>
      <link>https://arxiv.org/abs/2510.14714</link>
      <description>arXiv:2510.14714v1 Announce Type: new 
Abstract: We examine the theoretical properties of the index of agreement loss function $L_W$, the negatively oriented counterpart of Willmott's index of agreement, a common metric in environmental sciences and engineering. We prove that $L_W$ is bounded within [0, 1], translation and scale invariant, and estimates the parameter $\Bbb{E}_{F}[\underline{y}] \pm \Bbb{V}_{F}^{1/2}[\underline{y}]$ when fitting a distribution. We propose $L_{\operatorname{NR}_2}$ as a theoretical improvement, which replaces the denominator of $L_W$ with the sum of Euclidean distances, better aligning with the underlying geometric intuition. This new loss function retains the appealing properties of $L_W$ but also admits closed-form solutions for linear model parameter estimation. We show that as the correlation between predictors and the dependent variable approaches 1, parameter estimates from squared error, $L_{\operatorname{NR}_2}$ and $L_W$ converge. This behavior is mirrored in hydrologic model calibration (a core task in water resources engineering), where performance becomes nearly identical across these loss functions. Finally, we suggest potential improvements for existing $L_p$-norm variants of the index of agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14714v1</guid>
      <category>stat.ME</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hristos Tyralis, Georgia Papacharalampous</dc:creator>
    </item>
    <item>
      <title>EM Approaches to Nonparametric Estimation for Mixture of Linear Regressions</title>
      <link>https://arxiv.org/abs/2510.14890</link>
      <description>arXiv:2510.14890v1 Announce Type: new 
Abstract: In a mixture of linear regression model, the regression coefficients are treated as random vectors that may follow either a continuous or discrete distribution. We propose two Expectation-Maximization (EM) algorithms to estimate this prior distribution. The first algorithm solves a kernelized version of the nonparametric maximum likelihood estimation (NPMLE). This method not only recovers continuous prior distributions but also accurately estimates the number of clusters when the prior is discrete. The second algorithm, designed to approximate the NPMLE, targets prior distributions with a density. It also performs well for discrete priors when combined with a post-processing step. We study the convergence properties of both algorithms and demonstrate their effectiveness through simulations and applications to real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14890v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Welbaum, Wanli Qiao</dc:creator>
    </item>
    <item>
      <title>A formative measurement validation methodology for survey questionnaires</title>
      <link>https://arxiv.org/abs/2510.14950</link>
      <description>arXiv:2510.14950v1 Announce Type: new 
Abstract: Model misspecification of formative indicators remains a widely documented issue across academic literature, yet scholars lack a clear consensus on pragmatic, prescriptive approaches to manage this gap. This ambiguity forces researchers to rely on psychometric frameworks primarily intended for reflective models, and thus risks misleading findings. This article introduces a Multi-Step Validation Methodology Framework specifically designed for formative constructs in survey-based research. The proposed framework is grounded in an exhaustive literature review and integrates essential pilot diagnostics through descriptive statistics and multicollinearity checks. The methodology provides researchers with the necessary theoretical and structural clarity to finally justify and adhere to appropriate validation techniques that accurately account for the causal nature of the constructs while ensuring high psychometric and statistical integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14950v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Dominique Dalipe Mu\~noz</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v1 Announce Type: cross 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Regression Model Selection Under General Conditions</title>
      <link>https://arxiv.org/abs/2510.14822</link>
      <description>arXiv:2510.14822v1 Announce Type: cross 
Abstract: Model selection criteria are one of the most important tools in statistics. Proofs showing a model selection criterion is asymptotically optimal are tailored to the type of model (linear regression, quantile regression, penalized regression, etc.), the estimation method (linear smoothers, maximum likelihood, generalized method of moments, etc.), the type of data (i.i.d., dependent, high dimensional, etc.), and the type of model selection criterion. Moreover, assumptions are often restrictive and unrealistic making it a slow and winding process for researchers to determine if a model selection criterion is selecting an optimal model. This paper provides general proofs showing asymptotic optimality for a wide range of model selection criteria under general conditions. This paper not only asymptotically justifies model selection criteria for most situations, but it also unifies and extends a range of previously disparate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14822v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amaze Lusompa</dc:creator>
    </item>
    <item>
      <title>Reconstruct Ising Model with Global Optimality via SLIDE</title>
      <link>https://arxiv.org/abs/2310.09257</link>
      <description>arXiv:2310.09257v3 Announce Type: replace 
Abstract: The reconstruction of interaction networks between random events is a critical problem arising from statistical physics and politics, sociology, biology, psychology, and beyond. The Ising model lays the foundation for this reconstruction process, but finding the underlying Ising model from the least amount of observed samples in a computationally efficient manner has been historically challenging for half a century. Using sparsity learning, we present an approach named SLIDE whose sample complexity is globally optimal. Furthermore, an algorithm is developed to give a statistically consistent solution of SLIDE in polynomial time with high probability. On extensive benchmarked cases, the SLIDE approach demonstrates dominant performance in reconstructing underlying Ising models, confirming its superior statistical properties. The application on the U.S. senators voting in the six congresses reveals that both the Republicans and Democrats noticeably assemble in each congress; interestingly, the assembling of Democrats is particularly pronounced in the latest congress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09257v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanyu Chen, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>$\ell_1$-Regularized Generalized Least Squares</title>
      <link>https://arxiv.org/abs/2405.10719</link>
      <description>arXiv:2405.10719v2 Announce Type: replace 
Abstract: We study an $\ell_{1}$-regularized generalized least-squares (GLS) estimator for high-dimensional regressions with autocorrelated errors. Specifically, we consider the case where errors are assumed to follow an autoregressive process, alongside a feasible variant of GLS that estimates the structure of this process in a data-driven manner. The estimation procedure consists of three steps: performing a LASSO regression, fitting an autoregressive model to the realized residuals, and then running a second-stage LASSO regression on the rotated (whitened) data. We examine the theoretical performance of the method in a sub-Gaussian random-design setting, in particular assessing the impact of the rotation on the design matrix and how this impacts the estimation error of the procedure. We show that our proposed estimators maintain smaller estimation error than an unadjusted LASSO regression when the errors are driven by an autoregressive process. A simulation study verifies the performance of the proposed method, demonstrating that the penalized (feasible) GLS-LASSO estimator performs on par with the LASSO in the case of white noise errors, whilst outperforming when the errors exhibit significant autocorrelation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10719v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaveh S. Nobari, Alex Gibberd</dc:creator>
    </item>
    <item>
      <title>Planning for gold: Hypothesis screening with split samples for valid powerful testing in matched observational studies</title>
      <link>https://arxiv.org/abs/2406.00866</link>
      <description>arXiv:2406.00866v2 Announce Type: replace 
Abstract: Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. One approach to mitigate this concern is to identify hypotheses likely to be more resilient to hidden biases by splitting the data into a planning sample for designing the study and an analysis sample for making inferences. We devise a powerful and flexible method for selecting hypotheses in the planning sample when an unknown number of outcomes are affected by the treatment, allowing researchers to gain the benefits of exploratory analysis and still conduct powerful inference under concerns of unmeasured confounding. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00866v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v4 Announce Type: replace 
Abstract: Adaptive treatment assignment algorithms, such as bandit algorithms, are increasingly used in digital health intervention clinical trials. Frequently, the data collected from these trials is used to conduct causal inference and related data analyses to decide how to refine the intervention, and whether to roll-out the intervention more broadly. This work studies inference for estimands that depend on the adaptive algorithm itself; a simple example is the mean reward under the adaptive algorithm. Specifically, we investigate the replicability of statistical analyses concerning such estimands when using data from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical estimators are guaranteed to be consistent and asymptotically normal. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health, where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Linking Potentially Misclassified Healthy Food Access to Diabetes Prevalence</title>
      <link>https://arxiv.org/abs/2505.01465</link>
      <description>arXiv:2505.01465v2 Announce Type: replace 
Abstract: Access to healthy food is key to maintaining a healthy lifestyle and can be quantified by the distance to the nearest grocery store. However, calculating this distance forces a trade-off between cost and correctness. Accurate route-based distances following passable roads are cost-prohibitive, while simple straight-line distances ignoring infrastructure and natural barriers are accessible yet error-prone. Categorizing low-access neighborhoods based on these straight-line distances induces misclassification and introduces bias into standard regression models estimating the relationship between disease prevalence and access. Yet, fully observing the more accurate, route-based food access measure is often impossible, which induces a missing data problem. We combat bias and address missingness with a new maximum likelihood estimator for Poisson regression with a binary, misclassified exposure (access to healthy food within some threshold), where the misclassification may depend on additional error-free covariates. In simulations, we show the consequence of ignoring the misclassification (bias) and how the proposed estimator corrects for bias while preserving more statistical efficiency than the complete case analysis (i.e., deleting observations with missing data). Finally, we apply our estimator to model the relationship between census tract diabetes prevalence and access to healthy food in northwestern North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01465v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley E. Mullan, P. D. Anh Nguyen, Sarah C. Lotspeich</dc:creator>
    </item>
    <item>
      <title>Bayesian Doubly Robust Causal Inference via Posterior Coupling</title>
      <link>https://arxiv.org/abs/2506.04868</link>
      <description>arXiv:2506.04868v2 Announce Type: replace 
Abstract: Bayesian doubly robust (DR) causal inference faces a fundamental dilemma: joint modeling of outcome and propensity score suffers from the feedback problem where outcome information contaminates propensity score estimation, while two-step analysis sacrifices valid posterior distributions for computational convenience. We resolve this dilemma through posterior coupling via entropic tilting. Our framework constructs independent posteriors for propensity score and outcome models, then couples them using entropic tilting to enforce the DR moment condition. This yields the first fully Bayesian DR estimator with an explicit posterior distribution. Theoretically, we establish three key properties: (i) when the outcome model is correctly specified, the tilted posterior coincides with the original; (ii) under propensity score model correctness, the posterior mean remains consistent despite outcome model misspecification; (iii) convergence rates improve for nonparametric outcome models. Simulations demonstrate superior bias reduction and efficiency compared to existing methods. We illustrate practical advantages of the proposed method through two applications: sensitivity analysis for unmeasured confounding in antihypertensive treatment effects on dementia, and high-dimensional confounder selection combining shrinkage priors with modified moment conditions for right heart catheterization mortality. We provide an R package implementing the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04868v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Constructing Confidence Intervals for Infinite-Dimensional Functional Parameters by Highly Adaptive Lasso</title>
      <link>https://arxiv.org/abs/2507.10511</link>
      <description>arXiv:2507.10511v3 Announce Type: replace 
Abstract: Estimating the conditional mean function is a central task in statistical learning. In this paper, we consider estimation and inference for a nonparametric class of real-valued cadlag functions with bounded sectional variation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der Laan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible empirical risk minimizer over linear combinations of tensor products of zero- or higher-order spline basis functions under an L1 norm constraint. Building on recent theoretical advances in asymptotic normality and uniform convergence rates for higher-order spline HAL estimators, this work focuses on constructing robust confidence intervals for HAL-based estimators of conditional means. First, we propose a targeted HAL with a debiasing step to remove the regularization bias of the targeted conditional mean and also consider a relaxed HAL estimator to reduce such bias within the working model. Second, we propose both global and local undersmoothing strategies to adaptively enlarge the working model and further reduce bias relative to variance. Third, we combine these estimation strategies with delta-method-based variance estimators to construct confidence intervals for the conditional mean. Through extensive simulation studies, we evaluate different combinations of our estimation procedures, model selection strategies, and confidence-interval constructions. The results show that our proposed approaches substantially reduce bias relative to variance and yield confidence intervals with coverage rates close to nominal levels across different scenarios. Finally, we demonstrate the general applicability of our framework by estimating conditional average treatment effect (CATE) functions, highlighting how HAL-based inference methods extend to other infinite-dimensional, non-pathwise-differentiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10511v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Junming Shi, Alan Hubbard, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>SLOPE and Designing Robust Studies for Generalization</title>
      <link>https://arxiv.org/abs/2510.01577</link>
      <description>arXiv:2510.01577v2 Announce Type: replace 
Abstract: A popular task in generalization is to learn about a new, target population based on data from an existing, source population. This task relies on conditional exchangeability, which asserts that differences between the source and target populations are fully captured by observable characteristics of the two populations. Unfortunately, this assumption is often untenable in practice due to unobservable differences between the source and target populations. Worse, the assumption cannot be verified with data, warranting the need for robust data collection processes and study designs that are inherently less sensitive to violation of the assumption. In this paper, we propose SLOPE (Sensitivity of LOcal Perturbations from Exchangeability), a simple, intuitive, and novel measure that quantifies the sensitivity to local violation of conditional exchangeability. SLOPE combines ideas from sensitivity analysis in causal inference and derivative-based measure of robustness from Hampel (1974). Among other properties, SLOPE can help investigators to choose (a) a robust source or target population or (b) a robust estimand. Also, we show an analytic relationship between SLOPE and influence functions, which investigators can use to derive SLOPE given an influence function. We conclude with a re-analysis of a multi-national randomized experiment and illustrate the role of SLOPE in informing robust study designs for generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01577v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinran Miao, Jiwei Zhao, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Learned harmonic mean estimation of the Bayesian evidence with normalizing flows</title>
      <link>https://arxiv.org/abs/2405.05969</link>
      <description>arXiv:2405.05969v3 Announce Type: replace-cross 
Abstract: We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05969v3</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alicja Polanska, Matthew A. Price, Davide Piras, Alessio Spurio Mancini, Jason D. McEwen</dc:creator>
    </item>
    <item>
      <title>Testing by Betting while Borrowing and Bargaining</title>
      <link>https://arxiv.org/abs/2407.11465</link>
      <description>arXiv:2407.11465v2 Announce Type: replace-cross 
Abstract: Testing by betting has been a cornerstone of the game-theoretic statistics literature. In this framework, a betting score (or more generally an e-process), as opposed to a traditional p-value, is used to quantify the evidence against a null hypothesis: the higher the betting score, the more money one has made betting against the null, and thus the larger the evidence that the null is false. A key ingredient assumed throughout past works is that one cannot bet more money than one currently has. In this paper, we ask what happens if the bettor is allowed to borrow money after going bankrupt, allowing further financial flexibility in this game of hypothesis testing. We propose various definitions of (adjusted) evidence relative to the wealth borrowed, indebted, and accumulated. We also ask what happens if the bettor can "bargain", in order to obtain odds bettor than specified by the null hypothesis. The adjustment of wealth in order to serve as evidence appeals to the characterization of arbitrage, interest rates, and num\'eraire-adjusted pricing in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11465v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Regularizing Extrapolation in Causal Inference</title>
      <link>https://arxiv.org/abs/2509.17180</link>
      <description>arXiv:2509.17180v2 Announce Type: replace-cross 
Abstract: Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17180v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, Avi Feller</dc:creator>
    </item>
  </channel>
</rss>
