<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Kernighan-Lin Search Algorithm</title>
      <link>https://arxiv.org/abs/2502.00316</link>
      <description>arXiv:2502.00316v1 Announce Type: new 
Abstract: The traveling salesman problem (TSP) and the graph partitioning problem (GPP) are two important combinatorial optimization problems with many applications. Due to the NP-hardness of these problems, heuristic algorithms are commonly used to find good, or hopefully near-optimal, solutions. Kernighan and Lin have proposed two of the most successful heuristic algorithms for these problems: The Lin-Kernighan (LK) algorithm for TSP and the Kernighan-Lin (KL) algorithm for GPP. Although these algorithms are problem specific to TSP and GPP, they share a problem-agnostic mechanism, called variable depth search, that has wide applicability for general search. This paper expresses this mechanism as part of a general search algorithm, called the Kernighan-Lin Search algorithm, to facilitate its use beyond the TSP and GPP problems. Experimental comparisons with other general search algorithms, namely, genetic algorithms, hill climbing, and simulated annealing, on function optimization test suites confirm that the new algorithm is very successful in solution quality and running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00316v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ali Dasdan</dc:creator>
    </item>
    <item>
      <title>Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation Problems</title>
      <link>https://arxiv.org/abs/2502.00841</link>
      <description>arXiv:2502.00841v1 Announce Type: new 
Abstract: We consider a learning-augmented framework for NP-hard permutation problems. The algorithm has access to predictions telling, given a pair $u,v$ of elements, whether $u$ is before $v$ or not in an optimal solution. Building on the work of Braverman and Mossel (SODA 2008), we show that for a class of optimization problems including scheduling, network design and other graph permutation problems, these predictions allow to solve them in polynomial time with high probability, provided that predictions are true with probability at least $1/2+\epsilon$. Moreover, this can be achieved with a parsimonious access to the predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00841v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evripidis Bampis, Bruno Escoffier, Dimitris Fotakis, Panagiotis Patsilinakos, Michalis Xefteris</dc:creator>
    </item>
    <item>
      <title>Minimum Riesz s-Energy Subset Selection in Ordered Point Sets via Dynamic Programming</title>
      <link>https://arxiv.org/abs/2502.01163</link>
      <description>arXiv:2502.01163v1 Announce Type: new 
Abstract: We present a dynamic programming algorithm for selecting a representative subset of size $k$ of points from a given set with $n$ points such that the Riesz s-energy is minimized. Whereas in general dimensions the problem is NP hard, in the one-dimensional case, the natural ordering of the data points allows for an efficient recursion. This approach is then extended to problems related to two-dimensional Pareto front representations arising in biobjective optimization problems. The proposed methods guarantee an optimal solution under the assumption of sorted (or non-dominated) input, and the overall time complexity is shown to be $O(n^2 k)$. Alongside the theory, we offer computational examples and an open-source Python implementation of the algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01163v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Emmerich</dc:creator>
    </item>
    <item>
      <title>IBB: Fast Burrows-Wheeler Transform Construction for Length-Diverse DNA Data</title>
      <link>https://arxiv.org/abs/2502.01327</link>
      <description>arXiv:2502.01327v1 Announce Type: new 
Abstract: The Burrows-Wheeler transform (BWT) is integral to the FM-index, which is used extensively in text compression, indexing, pattern search, and bioinformatic problems as de novo assembly and read alignment. Thus, efficient construction of the BWT in terms of time and memory usage is key to these applications. We present a novel external algorithm called Improved-Bucket Burrows-Wheeler transform (IBB) for constructing the BWT of DNA datasets with highly diverse sequence lengths. IBB uses a right-aligned approach to efficiently handle sequences of varying lengths, a tree-based data structure to manage relative insert positions and ranks, and fine buckets to reduce the necessary amount of input and output to external memory. Our experiments demonstrate that IBB is 10% to 40% faster than the best existing state-of-the-art BWT construction algorithms on most datasets while maintaining competitive memory consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01327v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enno Adler, Stefan B\"ottcher, Rita Hartel, Cederic Alexander Steininger</dc:creator>
    </item>
    <item>
      <title>Fair densest subgraph across multiple graphs</title>
      <link>https://arxiv.org/abs/2502.01381</link>
      <description>arXiv:2502.01381v1 Announce Type: new 
Abstract: Many real-world networks can be modeled as graphs. Finding dense subgraphs is a key problem in graph mining with applications in diverse domains. In this paper, we consider two variants of the densest subgraph problem where multiple graph snapshots are given and the goal is to find a fair densest subgraph without over-representing the density among the graph snapshots. More formally, given a set of graphs and input parameter $\alpha$, we find a dense subgraph maximizing the sum of densities across snapshots such that the difference between the maximum and minimum induced density is at most $\alpha$. We prove that this problem is NP-hard and present an integer programming based, exact algorithm and a practical polynomial-time heuristic. We also consider a minimization variant where given an input parameter $\sigma$, we find a dense subgraph which minimizes the difference between the maximum and minimum density while inducing a total density of at least $\sigma$ across the graph snapshots. We prove the NP-hardness of the problem and propose two algorithms: an exponential time algorithm based on integer programming and a greedy algorithm. We present an extensive experimental study that shows that our algorithms can find the ground truth in synthetic dataset and produce good results in real-world datasets. Finally, we present case studies that show the usefulness of our problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01381v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-70362-1_1</arxiv:DOI>
      <dc:creator>Chamalee Wickrama Arachchi, Nikolaj Tatti</dc:creator>
    </item>
    <item>
      <title>Fair Vertex Problems Parameterized by Cluster Vertex Deletion</title>
      <link>https://arxiv.org/abs/2502.01400</link>
      <description>arXiv:2502.01400v1 Announce Type: new 
Abstract: We study fair vertex problem metatheorems on graphs within the scope of structural parameterization in parameterized complexity. Unlike typical graph problems that seek the smallest set of vertices satisfying certain properties, the goal here is to find such a set that does not contain too many vertices in any neighborhood of any vertex. Formally, the task is to find a set $X\subseteq V(G)$ of fair cost $k$, i.e., such that for all $v\in V(G)$ $|X\cap N(v)|\le k$. Recently, Knop, Masa\v{r}\'ik, and Toufar [MFCS 2019] showed that all fair MSO$_1$ definable problems can be solved in FPT time parameterized by the twin cover of a graph. They asked whether such a statement would be achievable for a more general parameterization of cluster vertex deletion, i.e., the smallest number of vertices required to be removed from the graph such that what remains is a collection of cliques.
  In this paper, we prove that in full generality this is not possible by demonstrating a W[1]-hardness. On the other hand, we give a sufficient property under which a fair MSO$_1$ definable problem admits an FPT algorithm parameterized by the cluster vertex deletion number. Our algorithmic formulation is very general as it captures the fair variant of many natural vertex problems such as the Fair Feedback Vertex Set, the Fair Vertex Cover, the Fair Dominating Set, the Fair Odd Cycle Transversal, as well as a connected variant of thereof. Moreover, we solve the Fair $[\sigma,\rho]$-Domination problem for $\sigma$ finite, or $\sigma=\mathbb{N}$ and $\rho$ cofinite. Specifically, given finite or cofinite $\rho\subseteq \mathbb{N}$ and finite $\sigma$, or $\rho\subseteq \mathbb{N}$ cofinite and $\sigma=\mathbb{N}$, the task is to find set of vertices $X\subseteq V(G)$ of fair cost at most $k$ such that for all $v\in X$, $|N(v)\cap X|\in\sigma$ and for all $v\in V(G)\setminus X$, $|N(v)\cap X|\in\rho$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01400v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Masa\v{r}\'ik, J\k{e}drzej Olkowski, Anna Zych-Pawlewicz</dc:creator>
    </item>
    <item>
      <title>Node ranking in labeled networks</title>
      <link>https://arxiv.org/abs/2502.01408</link>
      <description>arXiv:2502.01408v1 Announce Type: new 
Abstract: The entities in directed networks arising from real-world interactions are often naturally organized under some hierarchical structure. Given a directed, weighted, graph with edges and node labels, we introduce ranking problem where the obtained hierarchy should be described using node labels. Such method has the advantage to not only rank the nodes but also provide an explanation for such ranking. To this end, we define a binary tree called label tree, where each leaf represents a rank and each non-leaf contains a single label, which is then used to partition, and consequently, rank the nodes in the input graph. We measure the quality of trees using agony score, a penalty score that penalizes the edges from higher ranks to lower ranks based on the severity of the violation. We show that the problem is NP-hard, and even inapproximable if we limit the size of the label tree. Therefore, we resort to heuristics, and design a divide-and-conquer algorithm which runs in $\bigO{(n + m) \log n + \ell R}$, where $R$ is the number of node-label pairs in the given graph, $\ell$ is the number of nodes in the resulting label tree, and $n$ and $m$ denote the number of nodes and edges respectively. We also report an experimental study that shows that our algorithm can be applied to large networks, that it can find ground truth in synthetic datasets, and can produce explainable hierarchies in real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01408v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611977653.ch47</arxiv:DOI>
      <dc:creator>Chamalee Wickrama Arachchi, Nikolaj Tatti</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Spectral Sparsification of Hypergraphs</title>
      <link>https://arxiv.org/abs/2502.01421</link>
      <description>arXiv:2502.01421v1 Announce Type: new 
Abstract: Spectral hypergraph sparsification, a natural generalization of the well-studied spectral sparsification notion on graphs, has been the subject of intensive research in recent years. In this work, we consider spectral hypergraph sparsification in the dynamic setting, where the goal is to maintain a spectral sparsifier of an undirected, weighted hypergraph subject to a sequence of hyperedge insertions and deletions. For any $0 &lt; \varepsilon \leq 1$, we give the first fully dynamic algorithm for maintaining an $ (1 \pm \varepsilon) $-spectral hypergraph sparsifier of size $ n r^3 \operatorname{poly}\left( \log n, \varepsilon ^{-1} \right) $ with amortized update time $ r^4 \operatorname{poly}\left( \log n, \varepsilon ^{-1} \right) $, where $n$ is the number of vertices of the underlying hypergraph and $r$ is an upper-bound on the rank of hyperedges. Our key contribution is to show that the spanner-based sparsification algorithm of Koutis and Xu (2016) admits a dynamic implementation in the hypergraph setting, thereby extending the dynamic spectral sparsification framework for ordinary graphs by Abraham et al. (2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01421v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gramoz Goranci, Ali Momeni</dc:creator>
    </item>
    <item>
      <title>Dense Subgraph Discovery Meets Strong Triadic Closure</title>
      <link>https://arxiv.org/abs/2502.01435</link>
      <description>arXiv:2502.01435v1 Announce Type: new 
Abstract: Finding dense subgraphs is a core problem with numerous graph mining applications such as community detection in social networks and anomaly detection. However, in many real-world networks connections are not equal. One way to label edges as either strong or weak is to use strong triadic closure~(STC). Here, if one node connects strongly with two other nodes, then those two nodes should be connected at least with a weak edge. STC-labelings are not unique and finding the maximum number of strong edges is NP-hard. In this paper, we apply STC to dense subgraph discovery. More formally, our score for a given subgraph is the ratio between the sum of the number of strong edges and weak edges, weighted by a user parameter $\lambda$, and the number of nodes of the subgraph. Our goal is to find a subgraph and an STC-labeling maximizing the score. We show that for $\lambda = 1$, our problem is equivalent to finding the densest subgraph, while for $\lambda = 0$, our problem is equivalent to finding the largest clique, making our problem NP-hard. We propose an exact algorithm based on integer linear programming and four practical polynomial-time heuristics. We present an extensive experimental study that shows that our algorithms can find the ground truth in synthetic datasets and run efficiently in real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01435v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.367169</arxiv:DOI>
      <dc:creator>Chamalee Wickrama Arachchi, Iiro Kumpulainen, Nikolaj Tatti</dc:creator>
    </item>
    <item>
      <title>Southwest Tree: A Low-Memory Data Structure for Partial Accumulations by Non-Commutative Invertible Operations</title>
      <link>https://arxiv.org/abs/2502.01603</link>
      <description>arXiv:2502.01603v1 Announce Type: new 
Abstract: The task of accumulating a portion of a list of values, whose values may be updated at any time, is widely used throughout various applications in computer science. While it is trivial to accomplish this task without any constraints, trivial solutions often sacrifice time complexity in either accumulating or updating the values, one being constant time and the other being linear. To even out the complexity, two well-known data structures have been used to accomplish this task, namely the Segment Tree and the Binary Indexed Tree, which are able to carry out both tasks in O(log_2 N) time for a list of N elements. However, the Segment Tree suffers from requiring auxiliary memory to contain additional values, while the Binary Indexed Tree is unable to handle non-commutative accumulation operations. Here, we present a data structure, called the Southwest Tree, that accomplishes these tasks for non-commutative, invertible accumulation operations in O(log_2 N) time and uses no additional memory to store the structure apart from the initial input array.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01603v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. C. Papadopoulos</dc:creator>
    </item>
    <item>
      <title>Satisfactory Budget Division</title>
      <link>https://arxiv.org/abs/2502.00484</link>
      <description>arXiv:2502.00484v1 Announce Type: cross 
Abstract: A divisible budget must be allocated to several projects, and agents are asked for their opinion on how much they would give to each project. We consider that an agent is satisfied by a division of the budget if, for at least a certain predefined number $\tau$ of projects, the part of the budget actually allocated to each project is at least as large as the amount the agent requested. The objective is to find a budget division that ``best satisfies'' the agents. In this context, different problems can be stated and we address the following ones. We study $(i)$ the largest proportion of agents that can be satisfied for any instance, $(ii)$ classes of instances admitting a budget division that satisfies all agents, $(iii)$ the complexity of deciding if, for a given instance, every agent can be satisfied, and finally $(iv)$ the question of finding, for a given instance, the smallest total budget to satisfy all agents. We provide answers to these complementary questions for several natural values of the parameter $\tau$, capturing scenarios where we seek to satisfy for each agent all; almost all; half; or at least one of her requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00484v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurent Gourv\`es, Michael Lampis, Nikolaos Melissinos, Aris Pagourtzis</dc:creator>
    </item>
    <item>
      <title>PAC Learning is just Bipartite Matching (Sort of)</title>
      <link>https://arxiv.org/abs/2502.00607</link>
      <description>arXiv:2502.00607v1 Announce Type: cross 
Abstract: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00607v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaddin Dughmi</dc:creator>
    </item>
    <item>
      <title>Optimal local certification on graphs of bounded pathwidth</title>
      <link>https://arxiv.org/abs/2502.00676</link>
      <description>arXiv:2502.00676v1 Announce Type: cross 
Abstract: We present proof labeling schemes for graphs with bounded pathwidth that can decide any graph property expressible in monadic second-order (MSO) logic using $O(\log n)$-bit vertex labels. Examples of such properties include planarity, Hamiltonicity, $k$-colorability, $H$-minor-freeness, admitting a perfect matching, and having a vertex cover of a given size.
  Our proof labeling schemes improve upon a recent result by Fraigniaud, Montealegre, Rapaport, and Todinca (Algorithmica 2024), which achieved the same result for graphs of bounded treewidth but required $O(\log^2 n)$-bit labels. Our improved label size $O(\log n)$ is optimal, as it is well-known that any proof labeling scheme that accepts paths and rejects cycles requires labels of size $\Omega(\log n)$.
  Our result implies that graphs with pathwidth at most $k$ can be certified using $O(\log n)$-bit labels for any fixed constant $k$. Applying the Excluding Forest Theorem of Robertson and Seymour, we deduce that the class of $F$-minor-free graphs can be certified with $O(\log n)$-bit labels for any fixed forest $F$, thereby providing an affirmative answer to an open question posed by Bousquet, Feuilloley, and Pierron (Journal of Parallel and Distributed Computing 2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00676v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Alden Baterisna, Yi-Jun Chang</dc:creator>
    </item>
    <item>
      <title>Learning-Based TSP-Solvers Tend to Be Overly Greedy</title>
      <link>https://arxiv.org/abs/2502.00767</link>
      <description>arXiv:2502.00767v1 Announce Type: cross 
Abstract: Deep learning has shown significant potential in solving combinatorial optimization problems such as the Euclidean traveling salesman problem (TSP). However, most training and test instances for existing TSP algorithms are generated randomly from specific distributions like uniform distribution. This has led to a lack of analysis and understanding of the performance of deep learning algorithms in out-of-distribution (OOD) generalization scenarios, which has a close relationship with the worst-case performance in the combinatorial optimization field. For data-driven algorithms, the statistical properties of randomly generated datasets are critical. This study constructs a statistical measure called nearest-neighbor density to verify the asymptotic properties of randomly generated datasets and reveal the greedy behavior of learning-based solvers, i.e., always choosing the nearest neighbor nodes to construct the solution path. Based on this statistical measure, we develop interpretable data augmentation methods that rely on distribution shifts or instance perturbations and validate that the performance of the learning-based solvers degenerates much on such augmented data. Moreover, fine-tuning learning-based solvers with augmented data further enhances their generalization abilities. In short, we decipher the limitations of learning-based TSP solvers tending to be overly greedy, which may have profound implications for AI-empowered combinatorial optimization solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00767v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiayang Li, Shihua Zhang</dc:creator>
    </item>
    <item>
      <title>Graph parameters that are coarsely equivalent to tree-length</title>
      <link>https://arxiv.org/abs/2502.00951</link>
      <description>arXiv:2502.00951v1 Announce Type: cross 
Abstract: Two graph parameters are said to be coarsely equivalent if they are within constant factors from each other for every graph $G$. Recently, several graph parameters were shown to be coarsely equivalent to tree-length. Recall that the length of a tree-decomposition ${\cal T}(G)$ of a graph $G$ is the largest diameter of a bag in ${\cal T}(G)$, and the tree-length of $G$ is the minimum of the length, over all tree-decompositions of $G$. We present simpler and sometimes with better bounds proofs for those known in literature results and further extend this list of graph parameters coarsely equivalent to tree-length. Among other new results, we show that the tree-length of a graph $G$ is small if and only if for every bramble ${\cal F}$ (or every Helly family of connected subgraphs ${\cal F}$, or every Helly family of paths ${\cal F}$) of $G$, there is a disk in $G$ with small radius that intercepts all members of ${\cal F}$. Furthermore, the tree-length of a graph $G$ is small if and only if $G$ can be embedded with a small additive distortion to an unweighted tree with the same vertex set as in $G$ (not involving any Steiner points). Additionally, we introduce a new natural "bridging`` property for cycles, which generalizes a known property of cycles in chordal graphs, and show that it also coarsely defines the tree-length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00951v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feodor F. Dragan</dc:creator>
    </item>
    <item>
      <title>Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known Optimality Gap</title>
      <link>https://arxiv.org/abs/2502.01067</link>
      <description>arXiv:2502.01067v1 Announce Type: cross 
Abstract: We investigate the sample-memory-pass trade-offs for pure exploration in multi-pass streaming multi-armed bandits (MABs) with the *a priori* knowledge of the optimality gap $\Delta_{[2]}$. Here, and throughout, the optimality gap $\Delta_{[i]}$ is defined as the mean reward gap between the best and the $i$-th best arms. A recent line of results by Jin, Huang, Tang, and Xiao [ICML'21] and Assadi and Wang [COLT'24] have shown that if there is no known $\Delta_{[2]}$, a pass complexity of $\Theta(\log(1/\Delta_{[2]}))$ (up to $\log\log(1/\Delta_{[2]})$ terms) is necessary and sufficient to obtain the *worst-case optimal* sample complexity of $O(n/\Delta^{2}_{[2]})$ with a single-arm memory. However, our understanding of multi-pass algorithms with known $\Delta_{[2]}$ is still limited. Here, the key open problem is how many passes are required to achieve the complexity, i.e., $O( \sum_{i=2}^{n}1/\Delta^2_{[i]})$ arm pulls, with a sublinear memory size.
  In this work, we show that the ``right answer'' for the question is $\Theta(\log{n})$ passes (up to $\log\log{n}$ terms). We first present a lower bound, showing that any algorithm that finds the best arm with slightly sublinear memory -- a memory of $o({n}/{\text{polylog}({n})})$ arms -- and $O(\sum_{i=2}^{n}{1}/{\Delta^{2}_{[i]}}\cdot \log{(n)})$ arm pulls has to make $\Omega(\frac{\log{n}}{\log\log{n}})$ passes over the stream. We then show a nearly-matching algorithm that assuming the knowledge of $\Delta_{[2]}$, finds the best arm with $O( \sum_{i=2}^{n}1/\Delta^2_{[i]} \cdot \log{n})$ arm pulls and a *single arm* memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01067v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolai Karpov, Chen Wang</dc:creator>
    </item>
    <item>
      <title>On Exact Learning of $d$-Monotone Functions</title>
      <link>https://arxiv.org/abs/2502.01265</link>
      <description>arXiv:2502.01265v1 Announce Type: cross 
Abstract: In this paper, we study the learnability of the Boolean class of $d$-monotone functions $f:{\cal X}\to\{0,1\}$ from membership and equivalence queries, where $({\cal X},\le)$ is a finite lattice. We show that the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function $F:\{0,1\}^d\to\{0,1\}$ and $g_1,\ldots,g_d:{\cal X}\to \{0,1\}$ are any monotone functions, is learnable in time $\sigma({\cal X})\cdot (size(f)/d+1)^{d}$ where $\sigma({\cal X})$ is the maximum sum of the number of immediate predecessors in a chain from the largest element to the smallest element in the lattice ${\cal X}$ and $size(f)=size(g_1)+\cdots+size(g_d)$, where $size(g_i)$ is the number of minimal elements in $g_i^{-1}(1)$.
  For the Boolean function $f:\{0,1\}^n\to\{0,1\}$, the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function and $g_1,\ldots,g_d$ are any monotone DNF, is learnable in time $O(n^2)\cdot (size(f)/d+1)^{d}$ where $size(f)=size(g_1)+\cdots+size(g_d)$.
  In particular, this class is learnable in polynomial time when $d$ is constant. Additionally, this class is learnable in polynomial time when $size(g_i)$ is constant for all $i$ and $d=O(\log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01265v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nader H. Bshouty</dc:creator>
    </item>
    <item>
      <title>Reductions in local certification</title>
      <link>https://arxiv.org/abs/2502.01551</link>
      <description>arXiv:2502.01551v1 Announce Type: cross 
Abstract: Local certification is a topic originating from distributed computing, where a prover tries to convince the vertices of a graph $G$ that $G$ satisfies some property $\mathcal{P}$. To convince the vertices, the prover gives a small piece of information, called certificate, to each vertex, and the vertices then decide whether the property $\mathcal{P}$ is satisfied by just looking at their certificate and the certificates of their neighbors. When studying a property $\mathcal{P}$ in the perspective of local certification, the aim is to find the optimal size of the certificates needed to certify $\mathcal{P}$, which can be viewed a measure of the local complexity of $\mathcal{P}$.
  A certification scheme is considered to be efficient if the size of the certificates is polylogarithmic in the number of vertices. While there have been a number of meta-theorems providing efficient certification schemes for general graph classes, the proofs of the lower bounds on the size of the certificates are usually very problem-dependent.
  In this work, we introduce a notion of hardness reduction in local certification, and show that we can transfer a lower bound on the certificates for a property $\mathcal{P}$ to a lower bound for another property $\mathcal{P}'$, via a (local) hardness reduction from $\mathcal{P}$ to $\mathcal{P}'$. We then give a number of applications in which we obtain polynomial lower bounds for many classical properties using such reductions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01551v1</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Esperet, S\'ebastien Zeitoun</dc:creator>
    </item>
    <item>
      <title>Expander Decomposition with Fewer Inter-Cluster Edges Using a Spectral Cut Player</title>
      <link>https://arxiv.org/abs/2205.10301</link>
      <description>arXiv:2205.10301v4 Announce Type: replace 
Abstract: A $(\phi,\epsilon)$-expander-decomposition of a graph $G$ (with $n$ vertices and $m$ edges) is a partition of $V$ into clusters $V_1,\ldots,V_k$ with conductance $\Phi(G[V_i]) \ge \phi$, such that there are at most $\epsilon m$ inter-cluster edges. Such a decomposition plays a crucial role in many graph algorithms. We give a randomized $\tilde{O}(m/\phi)$ time algorithm for computing a $(\phi, \phi\log^2 {n})$-expander decomposition. This improves upon the $(\phi, \phi\log^3 {n})$-expander decomposition also obtained in $\tilde{O}(m/\phi)$ time by [Saranurak and Wang, SODA 2019] (SW) and brings the number of inter-cluster edges within logarithmic factor of optimal.
  One crucial component of SW's algorithm is non-stop version of the cut-matching game of [Khandekar, Rao, Vazirani, JACM 2009] (KRV): The cut player does not stop when it gets from the matching player an unbalanced sparse cut, but continues to play on a trimmed part of the large side. The crux of our improvement is the design of a non-stop version of the cleverer cut player of [Orecchia, Schulman, Vazirani, Vishnoi, STOC 2008] (OSVV). The cut player of OSSV uses a more sophisticated random walk, a subtle potential function, and spectral arguments. Designing and analysing a non-stop version of this game was an explicit open question asked by SW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10301v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Agassy (Tel Aviv University), Dani Dorfman (Max Planck Institute for Informatics), Haim Kaplan (Tel Aviv University)</dc:creator>
    </item>
    <item>
      <title>Non-adaptive Bellman-Ford: Yen's improvement is optimal</title>
      <link>https://arxiv.org/abs/2402.10343</link>
      <description>arXiv:2402.10343v2 Announce Type: replace 
Abstract: The Bellman-Ford algorithm for single-source shortest paths repeatedly updates tentative distances in an operation called relaxing an edge. In several important applications a non-adaptive (oblivious) implementation is preferred, which means fixing the entire sequence of relaxations upfront, independently of the edge-weights. Such an implementation performs, in a dense graph on $n$ vertices, $(1 + o(1))n^3$ relaxations. An improvement by Yen from 1970 reduces the number of relaxations by a factor of two. We show that no further constant-factor improvements are possible, and every non-adaptive deterministic algorithm based on relaxations must perform $(\frac{1}{2} - o(1))n^3$ steps. This improves an earlier lower bound of Eppstein of $(\frac{1}{6} - o(1))n^3$. Given that a non-adaptive randomized variant of Bellman-Ford with at most $(\frac{1}{3} + o(1))n^3$ relaxations (with high probability) is known, our result implies a strict separation between deterministic and randomized strategies, answering an open question of Eppstein. On the complexity side, we show that deciding whether a given relaxation sequence is guaranteed to yield correct distances is NP-hard, even with the complete graph as input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10343v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialu Hu, L\'aszl\'o Kozma</dc:creator>
    </item>
    <item>
      <title>Bounded Edit Distance: Optimal Static and Dynamic Algorithms for Small Integer Weights</title>
      <link>https://arxiv.org/abs/2404.06401</link>
      <description>arXiv:2404.06401v3 Announce Type: replace 
Abstract: The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis. In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin [JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.
  The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit. A folklore approach supports updates in $\tilde O(k^2)$ time, where $\tilde O(\cdot)$ hides polylogarithmic factors. Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\tilde O(\min\{n,k^2\})$ raised an exciting open question of whether $\tilde O(k)$ is possible; we answer it affirmatively.
  Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved. The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance. Only recently, Das et al. [STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz [FOCS'23] presented an $\tilde O(n+\sqrt{nk^3})$-time solution and a matching conditional lower bound. In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\tilde O(n+Wk^2)$ time and maintained dynamically in $\tilde O(W^2k)$ time per update. Our static algorithm can also be implemented in $\tilde O(n+k^{2.5})$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06401v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Egor Gorbachev, Tomasz Kociumaka</dc:creator>
    </item>
    <item>
      <title>Simple and Optimal Sublinear Algorithms for Mean Estimation</title>
      <link>https://arxiv.org/abs/2406.05254</link>
      <description>arXiv:2406.05254v3 Announce Type: replace 
Abstract: We study the sublinear multivariate mean estimation problem in $d$-dimensional Euclidean space. Specifically, we aim to find the mean $\mu$ of a ground point set $A$, which minimizes the sum of squared Euclidean distances of the points in $A$ to $\mu$. We first show that a multiplicative $(1+\varepsilon)$ approximation to $\mu$ can be found with probability $1-\delta$ using $O(\varepsilon^{-1}\log \delta^{-1})$ many independent uniform random samples, and provide a matching lower bound. Furthermore, we give two sublinear time algorithms with optimal sample complexity for extracting a suitable approximate mean:
  1. A gradient descent approach running in time $O((\varepsilon^{-1}+\log\log \delta^{-1})\cdot \log \delta^{-1} \cdot d)$. It optimizes the geometric median objective while being significantly faster for our specific setting than all other known algorithms for this problem.
  2. An order statistics and clustering approach running in time $O\left((\varepsilon^{-1}+\log^{\gamma}\delta^{-1})\cdot \log \delta^{-1} \cdot d\right)$ for any constant $\gamma&gt;0$.
  Throughout our analysis, we also generalize the familiar median-of-means estimator to the multivariate case, showing that the geometric median-of-means estimator achieves an optimal sample complexity for estimating $\mu$, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05254v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Bertolotti, Matteo Russo, Chris Schwiegelshohn, Sudarshan Shyam</dc:creator>
    </item>
    <item>
      <title>Replication-proof Bandit Mechanism Design with Bayesian Agents</title>
      <link>https://arxiv.org/abs/2312.16896</link>
      <description>arXiv:2312.16896v2 Announce Type: replace-cross 
Abstract: We study the problem of designing replication-proof bandit mechanisms when agents strategically register or replicate their own arms to maximize their payoff. Specifically, we consider Bayesian agents who only know the distribution from which their own arms' mean rewards are sampled, unlike the original setting of by Shin et al. 2022. Interestingly, with Bayesian agents in stark contrast to the previous work, analyzing the replication-proofness of an algorithm becomes significantly complicated even in a single-agent setting. We provide sufficient and necessary conditions for an algorithm to be replication-proof in the single-agent setting, and present an algorithm that satisfies these properties. These results center around several analytical theorems that focus on \emph{comparing the expected regret of multiple bandit instances}, and therefore might be of independent interest since they have not been studied before to the best of our knowledge. We expand this result to the multi-agent setting, and provide a replication-proof algorithm for any problem instance. We finalize our result by proving its sublinear regret upper bound which matches that of Shin et al. 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16896v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suho Shin, Seyed A. Esmaeili, MohammadTaghi Hajiaghayi</dc:creator>
    </item>
    <item>
      <title>Positional Attention: Expressivity and Learnability of Algorithmic Computation</title>
      <link>https://arxiv.org/abs/2410.01686</link>
      <description>arXiv:2410.01686v2 Announce Type: replace-cross 
Abstract: There is a growing interest in the ability of neural networks to execute algorithmic tasks (e.g., arithmetic, summary statistics, and sorting). The goal of this work is to better understand the role of attention in Transformers for algorithmic execution. Its importance for algorithmic execution has been studied theoretically and empirically using parallel computational models. Notably, many parallel algorithms communicate between processors solely using positional information. Inspired by this observation, we investigate how Transformers can execute algorithms using positional attention, where attention weights depend exclusively on positional encodings. We prove that Transformers with positional attention (positional Transformers) maintain the same expressivity of parallel computational models, incurring a logarithmic depth cost relative to the input length. We analyze their in-distribution learnability and explore how parameter norms in positional attention affect sample complexity. Our results show that positional Transformers introduce a learning trade-off: while they exhibit better theoretical dependence on parameter norms, certain tasks may require more layers, which can, in turn, increase sample complexity. Finally, we empirically explore the out-of-distribution performance of positional Transformers and find that they perform well in tasks where their underlying algorithmic solution relies on positional information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01686v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veli\v{c}kovi\'c, Kimon Fountoulakis</dc:creator>
    </item>
  </channel>
</rss>
