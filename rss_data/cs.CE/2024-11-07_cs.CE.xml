<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 02:44:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FUsion-based ConstitutivE model (FuCe): Towards model-data augmentation in constitutive modelling</title>
      <link>https://arxiv.org/abs/2411.03318</link>
      <description>arXiv:2411.03318v1 Announce Type: new 
Abstract: Constitutive modeling is crucial for engineering design and simulations to accurately describe material behavior. However, traditional phenomenological models often struggle to capture the complexities of real materials under varying stress conditions due to their fixed forms and limited parameters. While recent advances in deep learning have addressed some limitations of classical models, purely data-driven methods tend to require large datasets, lack interpretability, and struggle to generalize beyond their training data. To tackle these issues, we introduce "Fusion-based Constitutive model (FuCe): Towards model-data augmentation in constitutive modelling". This approach combines established phenomenological models with an ICNN architecture, designed to train on the limited and noisy force-displacement data typically available in practical applications. The hybrid model inherently adheres to necessary constitutive conditions. During inference, Monte Carlo dropout is employed to generate Bayesian predictions, providing mean values and confidence intervals that quantify uncertainty. We demonstrate the model's effectiveness by learning two isotropic constitutive models and one anisotropic model with a single fiber direction, across six different stress states. The framework's applicability is also showcased in finite element simulations across three geometries of varying complexities. Our results highlight the framework's superior extrapolation capabilities, even when trained on limited and noisy data, delivering accurate and physically meaningful predictions across all numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03318v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator> Tushar, Sawan Kumar, Souvik Chakraborty</dc:creator>
    </item>
    <item>
      <title>Computational Tools for Real-time Analysis of High-throughput High-resolution TEM (HRTEM) Images of Conjugated Polymers</title>
      <link>https://arxiv.org/abs/2411.03474</link>
      <description>arXiv:2411.03474v1 Announce Type: new 
Abstract: Automated analysis of high-resolution transmission electron microscopy (HRTEM) images is increasingly essential for advancing research in organic electronics, where precise characterization of nanoscale crystal structures is crucial for optimizing material properties. This paper introduces an open-source computational framework designed for real-time analysis of HRTEM data, with a focus on characterizing complex microstructures in conjugated polymers, and illustrated using Poly[N-9$'$-heptadecanyl-2,7-carbazole-alt-5,5-(4$'$,7$'$-di-2-thienyl-2$'$,1$'$,3$'$-benzothiadiazole)] (PCDTBT), a key material in organic photovoltaics. The framework employs fast, automated image processing algorithms, enabling rapid extraction of structural features like \textit{d}-spacing, orientation, and shape metrics. Gaussian process optimization rapidly identifies the user-defined parameters in the approach, reducing the need for manual parameter tuning and thus enhancing reproducibility and usability. Additionally, the framework is compatible with high-performance computing (HPC) environments, allowing for efficient, large-scale data processing at near real-time speeds. A unique feature of the framework is a Wasserstein distance-based stopping criterion, which optimizes data collection by determining when further sampling no longer adds statistically significant information. This capability optimizes the amount of time the TEM facility is used while ensuring data adequacy for in-depth analysis. Open-source and tested on a substantial PCDTBT dataset, this tool offers a powerful, robust, and accessible solution for high-throughput material characterization in organic electronics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03474v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Gamdha, Ryan Fair, Adarsh Krishnamurthy, Enrique Gomez, Baskar Ganapathysubramanian</dc:creator>
    </item>
    <item>
      <title>Deriving Analytical Solutions Using Symbolic Matrix Structural Analysis: Part 1 -- Continuous Beams</title>
      <link>https://arxiv.org/abs/2411.03514</link>
      <description>arXiv:2411.03514v1 Announce Type: new 
Abstract: This study investigates the use of symbolic computation in Matrix Structural Analysis (MSA) for continuous beams, leveraging the MATLAB Symbolic Math Toolbox. By employing symbolic MSA, analytical expressions for displacements, support reactions, and internal forces are derived, offering deeper insights into structural behavior. This approach facilitates efficient and scalable sensitivity analysis, where partial derivatives of outputs concerning input parameters can be directly computed, enhancing design exploration. The development includes an open-source MATLAB program, hosted on GitHub, enabling symbolic analysis of continuous beams subjected to point and uniform loads. This approach is invaluable for both engineering practice and pedagogy, enriching the understanding of structural mechanics and aiding in education by illustrating clear parameter relationships. The program supports deriving influence lines and identifying maximum response values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03514v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vagelis Plevris, Afaq Ahmad</dc:creator>
    </item>
    <item>
      <title>Energy-based physics-informed neural network for frictionless contact problems under large deformation</title>
      <link>https://arxiv.org/abs/2411.03671</link>
      <description>arXiv:2411.03671v1 Announce Type: new 
Abstract: Numerical methods for contact mechanics are of great importance in engineering applications, enabling the prediction and analysis of complex surface interactions under various conditions. In this work, we propose an energy-based physics-informed neural network (PINNs) framework for solving frictionless contact problems under large deformation. Inspired by microscopic Lennard-Jones potential, a surface contact energy is used to describe the contact phenomena. To ensure the robustness of the proposed PINN framework, relaxation, gradual loading and output scaling techniques are introduced. In the numerical examples, the well-known Hertz contact benchmark problem is conducted, demonstrating the effectiveness and robustness of the proposed PINNs framework. Moreover, challenging contact problems with the consideration of geometrical and material nonlinearities are tested. It has been shown that the proposed PINNs framework provides a reliable and powerful tool for nonlinear contact mechanics. More importantly, the proposed PINNs framework exhibits competitive computational efficiency to the commercial FEM software when dealing with those complex contact problems. The codes used in this manuscript are available at https://github.com/JinshuaiBai/energy_PINN_Contact.(The code will be available after acceptance)</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03671v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinshuai Bai, Zhongya Lin, Yizheng Wang, Jiancong Wen, Yinghua Liu, Timon Rabczuk, YuanTong Gu, Xi-Qiao Feng</dc:creator>
    </item>
    <item>
      <title>Inexact block LU preconditioners for incompressible fluids with flow rate conditions</title>
      <link>https://arxiv.org/abs/2411.03929</link>
      <description>arXiv:2411.03929v1 Announce Type: new 
Abstract: When studying the dynamics of incompressible fluids in bounded domains the only available data often provide average flow rate conditions on portions of the domain's boundary. In engineering applications a common practice to complete these conditions is to prescribe a Dirichlet condition by assuming a-priori a spatial profile for the velocity field. However, this strongly influence the accuracy of the numerical solution. A more mathematically sound approach is to prescribe the flow rate conditions using Lagrange multipliers, resulting in an augmented weak formulation of the Navier-Stokes problem.
  In this paper we start from the SIMPLE preconditioner, introduced so far for the standard Navier-Stokes equations, and we derive two preconditioners for the monolithic solution of the augmented problem. This can be useful in complex applications where splitting the computation of the velocity/pressure and Lagrange multipliers numerical solutions can be very expensive. In particular, we investigate the numerical performance of the preconditioners in both idealized and real-life scenarios. Finally, we highlight the advantages of treating flow rate conditions with a Lagrange multipliers approach instead of prescribing a Dirichlet condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03929v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Crugnola, Christian Vergara</dc:creator>
    </item>
    <item>
      <title>Maximal Extractable Value in Decentralized Finance: Taxonomy, Detection, and Mitigation</title>
      <link>https://arxiv.org/abs/2411.03327</link>
      <description>arXiv:2411.03327v1 Announce Type: cross 
Abstract: Decentralized Finance (DeFi) leverages blockchain-enabled smart contracts to deliver automated and trustless financial services without the need for intermediaries. However, the public visibility of financial transactions on the blockchain can be exploited, as participants can reorder, insert, or remove transactions to extract value, often at the expense of others. This extracted value is known as the Maximal Extractable Value (MEV). MEV causes financial losses and consensus instability, disrupting the security, efficiency, and decentralization goals of the DeFi ecosystem. Therefore, it is crucial to analyze, detect, and mitigate MEV to safeguard DeFi. Our comprehensive survey offers a holistic view of the MEV landscape in the DeFi ecosystem. We present an in-depth understanding of MEV through a novel taxonomy of MEV transactions supported by real transaction examples. We perform a critical comparative analysis of various MEV detection approaches, evaluating their effectiveness in identifying different transaction types. Furthermore, we assess different categories of MEV mitigation strategies and discuss their limitations. We identify the challenges of current mitigation and detection approaches and discuss potential solutions. This survey provides valuable insights for researchers, developers, stakeholders, and policymakers, helping to curb and democratize MEV for a more secure and efficient DeFi ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03327v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huned Materwala, Shraddha M. Naik, Aya Taha, Tala Abdulrahman Abed, Davor Svetinovic</dc:creator>
    </item>
    <item>
      <title>Improving precision of A/B experiments using trigger intensity</title>
      <link>https://arxiv.org/abs/2411.03530</link>
      <description>arXiv:2411.03530v1 Announce Type: cross 
Abstract: In industry, online randomized controlled experiment (a.k.a A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. To improve the precision (or reduce standard error), we introduce the idea of trigger observations where the output of the treatment and the control model are different. We show that the evaluation with full information about trigger observations (full knowledge) improves the precision in comparison to a baseline method. However, detecting all such trigger observations is a costly affair, hence we propose a sampling based evaluation method (partial knowledge) to reduce the cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, evaluation with full knowledge reduces the standard error as much as 85%. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03530v1</guid>
      <category>econ.EM</category>
      <category>cs.CE</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanmoy Das, Dohyeon Lee, Arnab Sinha</dc:creator>
    </item>
    <item>
      <title>Shem: A Hardware-Aware Optimization Framework for Analog Computing Systems</title>
      <link>https://arxiv.org/abs/2411.03557</link>
      <description>arXiv:2411.03557v1 Announce Type: cross 
Abstract: As the demand for efficient data processing escalates, reconfigurable analog hardware which implements novel analog compute paradigms, is promising for energy-efficient computing at the sensing and actuation boundaries. These analog computing platforms embed information in physical properties and then use the physics of materials, devices, and circuits to perform computation. These hardware platforms are more sensitive to nonidealities, such as noise and fabrication variations, than their digital counterparts and accrue high resource costs when programmable elements are introduced. Identifying resource-efficient analog system designs that mitigate these nonidealities is done manually today.
  While design optimization frameworks have been enormously successful in other fields, such as photonics, they typically either target linear dynamical systems that have closed-form solutions or target a specific differential equation system and then derive the solution through hand analysis. In both cases, time-domain simulation is no longer needed to predict hardware behavior. In contrast, described analog hardware platforms have nonlinear time-evolving dynamics that vary substantially from design to design, lack closed-form solutions, and require the optimizer to consider time explicitly. We present Shem, an optimization framework for analog systems. Shem leverages differentiation methods recently popularized to train neural ODEs to enable the optimization of analog systems that exhibit nonlinear dynamics, noise and mismatch, and discrete behavior. We evaluate Shem on oscillator-based pattern recognizer, CNN edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03557v1</guid>
      <category>cs.ET</category>
      <category>cs.CE</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Neng Wang, Sara Achour</dc:creator>
    </item>
    <item>
      <title>Retentive Neural Quantum States: Efficient Ans\"atze for Ab Initio Quantum Chemistry</title>
      <link>https://arxiv.org/abs/2411.03900</link>
      <description>arXiv:2411.03900v1 Announce Type: cross 
Abstract: Neural-network quantum states (NQS) has emerged as a powerful application of quantum-inspired deep learning for variational Monte Carlo methods, offering a competitive alternative to existing techniques for identifying ground states of quantum problems. A significant advancement toward improving the practical scalability of NQS has been the incorporation of autoregressive models, most recently transformers, as variational ansatze. Transformers learn sequence information with greater expressiveness than recurrent models, but at the cost of increased time complexity with respect to sequence length. We explore the use of the retentive network (RetNet), a recurrent alternative to transformers, as an ansatz for solving electronic ground state problems in $\textit{ab initio}$ quantum chemistry. Unlike transformers, RetNets overcome this time complexity bottleneck by processing data in parallel during training, and recurrently during inference. We give a simple computational cost estimate of the RetNet and directly compare it with similar estimates for transformers, establishing a clear threshold ratio of problem-to-model size past which the RetNet's time complexity outperforms that of the transformer. Though this efficiency can comes at the expense of decreased expressiveness relative to the transformer, we overcome this gap through training strategies that leverage the autoregressive structure of the model -- namely, variational neural annealing. Our findings support the RetNet as a means of improving the time complexity of NQS without sacrificing accuracy. We provide further evidence that the ablative improvements of neural annealing extend beyond the RetNet architecture, suggesting it would serve as an effective general training strategy for autoregressive NQS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03900v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>quant-ph</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Knitter, Dan Zhao, James Stokes, Martin Ganahl, Stefan Leichenauer, Shravan Veerapaneni</dc:creator>
    </item>
    <item>
      <title>Interpretable and Efficient Data-driven Discovery and Control of Distributed Systems</title>
      <link>https://arxiv.org/abs/2411.04098</link>
      <description>arXiv:2411.04098v1 Announce Type: cross 
Abstract: Effectively controlling systems governed by Partial Differential Equations (PDEs) is crucial in several fields of Applied Sciences and Engineering. These systems usually yield significant challenges to conventional control schemes due to their nonlinear dynamics, partial observability, high-dimensionality once discretized, distributed nature, and the requirement for low-latency feedback control. Reinforcement Learning (RL), particularly Deep RL (DRL), has recently emerged as a promising control paradigm for such systems, demonstrating exceptional capabilities in managing high-dimensional, nonlinear dynamics. However, DRL faces challenges including sample inefficiency, robustness issues, and an overall lack of interpretability. To address these issues, we propose a data-efficient, interpretable, and scalable Dyna-style Model-Based RL framework for PDE control, combining the Sparse Identification of Nonlinear Dynamics with Control (SINDy-C) algorithm and an autoencoder (AE) framework for the sake of dimensionality reduction of PDE states and actions. This novel approach enables fast rollouts, reducing the need for extensive environment interactions, and provides an interpretable latent space representation of the PDE forward dynamics. We validate our method on two PDE problems describing fluid flows - namely, the 1D Burgers equation and 2D Navier-Stokes equations - comparing it against a model-free baseline, and carrying out an extensive analysis of the learned dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04098v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florian Wolf, Nicol\`o Botteghi, Urban Fasel, Andrea Manzoni</dc:creator>
    </item>
    <item>
      <title>Mastering truss structure optimization with tree search</title>
      <link>https://arxiv.org/abs/2406.06145</link>
      <description>arXiv:2406.06145v4 Announce Type: replace 
Abstract: This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures. Our approach accommodates intermediate construction stages characteristic of progressive construction settings. We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning. These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs. We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for highly performative solutions. Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality. While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06145v4</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Garayalde, Luca Rosafalco, Matteo Torzoni, Alberto Corigliano</dc:creator>
    </item>
    <item>
      <title>Tree-Cotree-Based Tearing and Interconnecting for 3D Magnetostatics: A Dual-Primal Approach</title>
      <link>https://arxiv.org/abs/2407.21707</link>
      <description>arXiv:2407.21707v2 Announce Type: replace 
Abstract: The simulation of electromagnetic devices with complex geometries and large-scale discrete systems benefits from advanced computational methods like IsoGeometric Analysis and Domain Decomposition. In this paper, we employ both concepts in an Isogeometric Tearing and Interconnecting method to enable the use of parallel computations for magnetostatic problems. We address the underlying non-uniqueness by using a graph-theoretic approach, the tree-cotree decomposition. The classical tree-cotree gauging is adapted to be feasible for parallelization, which requires that all local subsystems are uniquely solvable. Our contribution consists of an explicit algorithm for constructing compatible trees and combining it with a dual-primal approach to enable parallelization. The correctness of the proposed approach is proved and verified by numerical experiments, showing its accuracy, scalability and optimal convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21707v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Mally, Bernard Kapidani, Melina Merkel, Sebastian Sch\"ops, Rafael V\'azquez</dc:creator>
    </item>
    <item>
      <title>EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration</title>
      <link>https://arxiv.org/abs/2402.18595</link>
      <description>arXiv:2402.18595v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to represent the results with a wide bit representation. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks. Since the multiplication function is replaced by a simple logic representation, the critical paths in the resulting circuits become much shorter. Correspondingly, pipelining stages and intermediate registers used to store partial sums in the MAC array can be reduced, leading to a significantly smaller area as well as better power efficiency. The proposed design has been synthesized and verified by ResNet18- Cifar10, ResNet20-Cifar100, ResNet50-ImageNet, MobileNetV2-Cifar10, MobileNetV2-Cifar100, and EfficientNetB0-ImageNet. The experimental results confirmed the reduction of circuit area by up to 48.79% and the reduction of power consumption of executing DNNs by up to 64.41%, while the accuracy of the neural networks can still be well maintained. The open source code of this work can be found on GitHub with link https://github.com/Bo-Liu-TUM/EncodingNet/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18595v2</guid>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bo Liu, Grace Li Zhang, Xunzhao Yin, Ulf Schlichtmann, Bing Li</dc:creator>
    </item>
  </channel>
</rss>
