<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Feb 2026 02:54:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotically normal estimators in high-dimensional linear regression</title>
      <link>https://arxiv.org/abs/2602.07480</link>
      <description>arXiv:2602.07480v1 Announce Type: new 
Abstract: We establish asymptotic normality for estimators in high-dimensional linear regression by proving weak convergence in a separable Hilbert space, thereby enabling direct use of standard asymptotic tools, for example, the continuous mapping theorem. The approach allows the number of non-zero coefficients to grow, provided only a fixed number have moderate magnitude. As an application, we test linear hypotheses with a statistic whose null limit is a finite weighted sum of independent chi-squared variables, yielding plug-in critical values with asymptotically correct size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07480v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kou Fujimori, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>The Fisher score on the closed simplex</title>
      <link>https://arxiv.org/abs/2602.07665</link>
      <description>arXiv:2602.07665v1 Announce Type: new 
Abstract: We extend classical analytic tools for finite-state statistical models to allow zero probabilities. Using methods from algebraic statistics and information geometry, we develop a framework in which a smooth statistical model could hit the boundary of the simplex, for example, in contingency tables with non-structural zeros. The central object of our approach is the vector bundle whose fibres are the $p$-contrasts associated to each probability distribution $p$. In this framework, Fisher score and other key statistical concepts, such as entropy for one-dimensional statistical models, admit an algebraic representation also on the boundary of the simplex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07665v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Pistone, Fabio Rapallo, Eva Riccomagno</dc:creator>
    </item>
    <item>
      <title>The statistical threshold for planted matchings and spanning trees</title>
      <link>https://arxiv.org/abs/2602.07669</link>
      <description>arXiv:2602.07669v1 Announce Type: new 
Abstract: In this paper, we study the problem of detecting the presence of a planted perfect matching or spanning tree in an Erd\H{o}s--R\'enyi random graph. More precisely, we study the hypothesis testing problem where the statistician observes a graph on $n$ vertices. Under the null hypothesis, the graph is a realization of an Erd\H{o}s--R\'enyi random graph $G(n,q)$, while under the alternative hypothesis, the graph is the union of an Erd\H{o}s--R\'enyi random graph and a random perfect matching (or random spanning tree). In order to avoid trivial detection by counting edges, we adjust the alternative hypothesis so that the expected number of edges under both distributions coincides. We prove that in both problems, when $q\gg n^{-1/2}$, no test can perform better than random guessing, while for $q\ll n^{-1/2}$, there exist computationally efficient tests that guess correctly with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07669v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louigi Addario-Berry, Omer Angel, G\'abor Lugosi, Mikl\'os Z. R\'acz, Tselil Schramm</dc:creator>
    </item>
    <item>
      <title>Geometric ergodicity of Gibbs samplers for linear latent models with GIG variance mixtures</title>
      <link>https://arxiv.org/abs/2602.07944</link>
      <description>arXiv:2602.07944v1 Announce Type: new 
Abstract: We study geometric ergodicity of the Gibbs sampler for linear latent non-Gaussian models (LLnGMs), a class of hierarchical models in which conditional Gaussian structure is preserved through generalized inverse Gaussian (GIG) variance-mixture augmentation. Two complementary routes to geometric ergodicity are developed for the marginal chain on the mixing variables. First, we show that the associated Markov operator is trace-class, and hence admits a spectral gap, over a large portion of the GIG parameter space. Second, for the remaining boundary and heavy-tail regimes, we establish geometric ergodicity via drift and minorization, subject to an explicit null-smallness condition that quantifies how the drift interacts with the null space of the observation operator. Together, these results cover the full GIG parameter space, including the normal-inverse Gaussian, generalized asymmetric Laplace, and Student-$t$ special cases. The geometric ergodicity of this chain underpins the consistency of Gibbs-based stochastic-gradient estimators for maximum likelihood estimation, and we provide conditions that make the required integrability checks transparent. Numerical experiments illustrate the theoretical findings, contrasting mixing efficiency across parameter regimes and probing the role of the null-smallness constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07944v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsiddig Awadelkarim, David Bolin, Xiaotian Jin, Alexandre B. Simas, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>Fundamental Limits of Community Detection in Contextual Multi-Layer Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2602.08173</link>
      <description>arXiv:2602.08173v1 Announce Type: new 
Abstract: We consider the problem of community detection from the joint observation of a high-dimensional covariate matrix and $L$ sparse networks, all encoding noisy, partial information about the latent community labels of $n$ subjects. In the asymptotic regime where the networks have constant average degree and the number of features $p$ grows proportionally with $n$, we derive a sharp threshold under which detecting and estimating the subject labels is possible. Our results extend the work of \cite{MN23} to the constant-degree regime with noisy measurements, and also resolve a conjecture in \cite{YLS24+} when the number of networks is a constant.
  Our information-theoretic lower bound is obtained via a novel comparison inequality between Bernoulli and Gaussian moments, as well as a statistical variant of the ``recovery to chi-square divergence reduction'' argument inspired by \cite{DHSS25}. On the algorithmic side, we design efficient algorithms based on counting decorated cycles and decorated paths and prove that they achieve the sharp threshold for both detection and weak recovery. In particular, our results show that there is no statistical-computational gap in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08173v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Gong, Dong Huang, Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Asymptotically Minimax Robust Likelihood Ratio Test</title>
      <link>https://arxiv.org/abs/2602.08174</link>
      <description>arXiv:2602.08174v1 Announce Type: new 
Abstract: This paper develops a unified framework for asymptotically minimax robust hypothesis testing under distributional uncertainty, applicable to both Bayesian and Neyman--Pearson formulations (Type-I and Type-II). Uncertainty classes based on the KL-divergence, $\alpha$-divergence, and its symmetrized variant are considered. Using Sion's minimax theorem and Karush-Kuhn-Tucker conditions, the existence and uniqueness of the resulting robust tests are established. The least favorable distributions and corresponding robust likelihood ratio functions are derived in closed parametric forms, enabling computation via systems of nonlinear equations. It is proven that Dabak's approach does not yield an asymptotically minimax robust test. The proposed theory generalizes earlier work by offering a more systematic and comprehensive derivation of robust tests. Numerical simulations confirm the theoretical results and illustrate the behavior of the derived robust tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08174v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"okhan G\"ul</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Variable Selection with Lasso Statistics in the AMP Framework</title>
      <link>https://arxiv.org/abs/2602.08486</link>
      <description>arXiv:2602.08486v1 Announce Type: new 
Abstract: The Lasso is one of the most ubiquitous methods for variable selection in high-dimensional linear regression and has been studied extensively under different regimes. In a particular asymptotic setup entailing $n/p\to \text{constant}$, an i.i.d.~Gaussian $X$ matrix and linear sparsity, \citet{su2017false} analyzed the Lasso selection path and presented negative results, showing that maintaining small levels of the false discovery proportion comes at a substantial cost in power. Followup work by \citet{wang2020bridge} used the same framework to study the tradeoff between type I error and power for thresholded-Lasso selection, which ranks the variables based on the magnitude of the Lasso estimate instead of the order of appearance on the Lasso path, and demonstrated that significant improvements are possible if the regularization parameter is chosen appropriately. We take this line of research a step further, seeking an {\em optimal} selection procedure in the AMP framework among procedures that order the variables by some univariate function of the Lasso estimate at a fixed value $\lambda$ of the regularization term. Observing that the model for the Lasso estimates effectively reduces asymptotically to a version of the well-studied two-groups model, we propose an empirical Bayes variable selection procedure based on an estimate of the local false discovery rate. We extend existing results in the AMP framework to obtain exact predictions for the curve describing the asymptotic tradeoff between type I error and power of this procedure. Additionally, we prove that the optimal $\lambda$ is the minimizer of the asymptotic mean squared error, and accordingly propose to use the empirical Bayes procedure with $\lambda$ estimated by cross-validation. The theoretical predictions imply that the gains in power can be substantial, and we confirm this by numerical studies under different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08486v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lina Hidmi, Asaf Weinstein</dc:creator>
    </item>
    <item>
      <title>The Unseen Species Problem Revisited</title>
      <link>https://arxiv.org/abs/2602.08769</link>
      <description>arXiv:2602.08769v1 Announce Type: new 
Abstract: The unseen species problem is a classical problem in statistics. It asks us to, given n i.i.d. samples from an unknown discrete distribution over an unknown set, predict how many never before seen outcomes would be observed if m additional samples were collected. For small m we show the classical but poorly understood Good-Toulmin estimator to be minimax optimal to within a factor 2 and resolve the open problem of constructing principled prediction intervals for it. For intermediate m we propose a new estimator which achieves the minimax error for linear estimators up to an explicit multiplicative constant. Our estimator vastly outperforms the standard Smoothed Good-Toulmin estimator in the worst case and performs substantially better on several real data sets, namely those with many rare species. For large m we show that a previously mentioned estimator which did not have known rate guarantees actually achieves a marginally better rate than subsequent work. We find that this marginal rate improvement translates to meaningfully better performance in practice. We show in all three regimes that the same methods also achieve the same rate on incidence data, without further independence assumptions, provided that the sets are of bounded size. We establish, by means of bounded size biased couplings, concentration for some natural functionals of sequences of i.i.d. discrete-set-valued random variables which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08769v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Eriksson</dc:creator>
    </item>
    <item>
      <title>Distribution-free two-sample testing with blurred total variation distance</title>
      <link>https://arxiv.org/abs/2602.05862</link>
      <description>arXiv:2602.05862v1 Announce Type: cross 
Abstract: Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05862v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Hore, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Adaptive Experimental Design Using Shrinkage Estimators</title>
      <link>https://arxiv.org/abs/2602.07404</link>
      <description>arXiv:2602.07404v1 Announce Type: cross 
Abstract: In the setting of multi-armed trials, adaptive designs are a popular way to increase estimation efficiency, identify optimal treatments, or maximize rewards to individuals. Recent work has considered the case of estimating the effects of K active treatments, relative to a control arm, in a sequential trial. Several papers have proposed sequential versions of the classical Neyman allocation scheme to assign treatments as individuals arrive, typically with the goal of using Horvitz-Thompson-style estimators to obtain causal estimates at the end of the trial. However, this approach may be inefficient in that it fails to borrow information across the treatment arms.
  In this paper, we consider adaptivity when the final causal estimation is obtained using a Stein-like shrinkage estimator for heteroscedastic data. Such an estimator shares information across treatment effect estimates, providing provable reductions in expected squared error loss relative to estimating each causal effect in isolation. Moreover, we show that the expected loss of the shrinkage estimator takes the form of a Gaussian quadratic form, allowing it to be computed efficiently using numerical integration. This result paves the way for sequential adaptivity, allowing treatments to be assigned to minimize the shrinker loss. Through simulations, we demonstrate that this approach can yield meaningful reductions in estimation error. We also characterize how our adaptive algorithm assigns treatments differently than would a sequential Neyman allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07404v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan T. R. Rosenman, Kristen B. Hunter</dc:creator>
    </item>
    <item>
      <title>Inhomogeneous Priors for Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2602.07856</link>
      <description>arXiv:2602.07856v1 Announce Type: cross 
Abstract: Many inverse problems arising in engineering and applied sciences involve unknown quantities with pronounced spatial inhomogeneity, such as localized defects or spatially varying material properties, making reliable uncertainty quantification particularly challenging. While Bayesian inverse problem methodologies provide a principled framework for assessing reconstruction reliability, commonly used Gaussian priors, such as Whittle-Matern models, impose globally homogeneous assumptions that limit their ability to capture such structure in large-scale settings. We introduce a new class of inhomogeneous priors defined via convolution with white noise, yielding nonstationary Whittle-Matern-type random fields with a rigorous mathematical construction. These priors fit naturally within existing Bayesian well-posedness theory and enable efficient sampling by reducing prior realizations to the solution of a pseudo-differential equation, for which we develop numerical schemes with quantified approximation error. Numerical experiments in one-dimensional denoising and two-dimensional limited-angle X-ray tomography demonstrate significant improvements in reconstruction quality and uncertainty quantification, particularly in data-limited scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07856v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babak Maboudi Afkham, Tomas Soto, Mirza Karamehmedovic, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models</title>
      <link>https://arxiv.org/abs/2602.07997</link>
      <description>arXiv:2602.07997v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07997v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>TrungKhang Tran, TrungTin Nguyen, Md Abul Bashar, Nhat Ho, Richi Nayak, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Estimating the Shannon Entropy Using the Pitman--Yor Process</title>
      <link>https://arxiv.org/abs/2602.08347</link>
      <description>arXiv:2602.08347v1 Announce Type: cross 
Abstract: The Shannon entropy is a fundamental measure for quantifying diversity and model complexity in fields such as information theory, ecology, and genetics. However, many existing studies assume that the number of species is known, an assumption that is often unrealistic in practice. In recent years, efforts have been made to relax this restriction. Motivated by these developments, this study proposes an entropy estimation method based on the Pitman--Yor process, a representative approach in Bayesian nonparametrics. By approximating the true distribution as an infinite-dimensional process, the proposed method enables stable estimation even when the number of observed species is smaller than the true number of species. This approach provides a principled way to deal with the uncertainty in species diversity and enhances the reliability and robustness of entropy-based diversity assessment. In addition, we investigate the convergence property of the Shannon entropy for regularly varying distributions and use this result to establish the consistency of the proposed estimator. Finally, we demonstrate the effectiveness of the proposed method through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08347v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takato Hashino, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>Schr\"odinger bridge problem via empirical risk minimization</title>
      <link>https://arxiv.org/abs/2602.08374</link>
      <description>arXiv:2602.08374v1 Announce Type: cross 
Abstract: We study the Schr\"odinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schr\"odinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schr\"odinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08374v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Belomestny, Alexey Naumov, Nikita Puchkin, Denis Suchkov</dc:creator>
    </item>
    <item>
      <title>The Connection between Kriging and Large Neural Networks</title>
      <link>https://arxiv.org/abs/2602.08427</link>
      <description>arXiv:2602.08427v1 Announce Type: cross 
Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08427v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marius Marinescu</dc:creator>
    </item>
    <item>
      <title>Almost sure null bankruptcy of testing-by-betting strategies</title>
      <link>https://arxiv.org/abs/2602.08888</link>
      <description>arXiv:2602.08888v1 Announce Type: cross 
Abstract: The bounded mean betting procedure serves as a crucial interface between the domains of (1) sequential, anytime-valid statistical inference, and (2) online learning and portfolio selection algorithms. While recent work in both domains has established the exponential wealth growth of numerous betting strategies under any alternative distribution, the tightness of the inverted confidence sets, and the pathwise minimax regret bounds, little has been studied regarding the asymptotics of these strategies under the null hypothesis. Under the null, a strategy induces a wealth martingale converging to some random variable that can be zero (bankrupt) or non-zero (non-bankrupt, e.g. when it eventually stops betting). In this paper, we show the conceptually intuitive but technically nontrivial fact that these strategies (universal portfolio, Krichevsky-Trofimov, GRAPA, hedging, etc.) all go bankrupt with probability one, under any non-degenerate null distribution. Part of our analysis is based on the subtle almost sure divergence of various sums of $\sum O_p(n^{-1})$ type, a result of independent interest. We also demonstrate the necessity of null bankruptcy by showing that non-bankrupt strategies are all improvable in some sense. Our results significantly deepen our understanding of these betting strategies as they qualify their behavior on "almost all paths", whereas previous results are usually on "all paths" (e.g. regret bounds) or "most paths" (e.g. concentration inequalities and confidence sets).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08888v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Shubhada Agrawal, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Analysis of singular subspaces under random perturbations</title>
      <link>https://arxiv.org/abs/2403.09170</link>
      <description>arXiv:2403.09170v3 Announce Type: replace 
Abstract: We present a comprehensive analysis of singular vector and singular subspace perturbations in the signal-plus-noise matrix model with random Gaussian noise. Assuming a low-rank signal matrix, we extend the Davis-Kahan-Wedin theorem in a fully generalized manner, applicable to any unitarily invariant matrix norm, building on previous results by O'Rourke, Vu, and the author. Our analysis provides fine-grained insights, including $\ell_\infty$ bounds for singular vectors, $\ell_{2, \infty}$ bounds for singular subspaces, and results for linear and bilinear functions of singular vectors. Additionally, we derive $\ell_{2,\infty}$ bounds on perturbed singular vectors, taking into account the weighting by their corresponding singular values. Finally, we explore practical implications of these results in the Gaussian mixture model and the submatrix localization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09170v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Wang</dc:creator>
    </item>
    <item>
      <title>Precise Asymptotics for Linear Mixed Models with Crossed Random Effects</title>
      <link>https://arxiv.org/abs/2409.05066</link>
      <description>arXiv:2409.05066v3 Announce Type: replace 
Abstract: We obtain an asymptotic normality result that reveals the precise asymptotic behavior of the maximum likelihood estimators of parameters for a very general class of linear mixed models containing cross random effects. In achieving the result, we overcome theoretical difficulties that arise from random effects being crossed as opposed to the simpler nested random effects case. Our new theory is for a class of Gaussian response linear mixed models which includes crossed random slopes that partner arbitrary multivariate predictor effects and does not require the cell counts to be balanced. Statistical utilities include confidence interval construction, Wald hypothesis test and sample size calculations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05066v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiming Jiang, Matt P. Wand, Swarnadip Ghosh</dc:creator>
    </item>
    <item>
      <title>Consistency of Variational Inference for Nonlinear Inverse Problems of Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2409.18415</link>
      <description>arXiv:2409.18415v2 Announce Type: replace 
Abstract: We investigate the convergence rates of variational posterior distributions for statistical inverse problems involving nonlinear partial differential equations (PDEs). Departing from exact Bayesian inference, variational inference transforms the inference problem into an optimization problem by introducing variational sets. Based on a modified ``prior mass and testing'' framework, we propose general conditions for three categories of inverse problems: mildly ill-posed, severely ill-posed, and those with unknown model parameters. Concentrating on the widely utilized variational sets comprising the truncated Gaussian or the mean-field family, we demonstrate that for all three categories, the convergence rate can be decomposed into a true distribution term and a variational approximation term. Moreover, we illustrate that the true distribution term dominates the convergence rates, thereby substantiating the effectiveness of the variational inference method for inverse problems of PDEs. As specific examples, we examine a collection of non-linear inverse problems, including the Darcy flow problem, the inverse potential problem for a subdiffusion equation, and the inverse medium scattering problem. Besides, we show that our convergence rates are minimax optimal for these inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18415v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaokang Zu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Consistent model selection in a collection of stochastic block models</title>
      <link>https://arxiv.org/abs/2502.03848</link>
      <description>arXiv:2502.03848v3 Announce Type: replace 
Abstract: We introduce the penalized Krichevsky-Trofimov (KT) estimator as a convergent method for estimating the number of nodes clusters when observing multiple networks within both multi-layer and dynamic Stochastic Block Models. We establish the consistency of the KT estimator, showing that it converges to the correct number of clusters in both types of models when the number of nodes in the networks increases. Our estimator does not require a known upper bound on this number to be consistent. Furthermore, we show that these consistency results hold in both dense and sparse regimes, making the penalized KT estimator robust across various network configurations. We illustrate its performance on synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03848v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucie Arts (LPSM)</dc:creator>
    </item>
    <item>
      <title>Asymptotic expansions for spectral convergence of compact self-adjoint operators on general spectral subsets, with application to kernel Gram matrices</title>
      <link>https://arxiv.org/abs/2602.00999</link>
      <description>arXiv:2602.00999v2 Announce Type: replace 
Abstract: We study the spectral convergence of compact, self-adjoint operators on a separable Hilbert space under operator norm perturbations, and derive asymptotic expansions for their eigenvalues and eigenprojections. Our analysis focuses on eigenvalues indexed by a general subset, with minimal restrictions on their selection. The usefulness of the provided expansions is illustrated by an application to kernel Gram matrices, deriving concentration inequalities as well as weak convergence results, which, in contrast to existing literature, are primarily relying on assumptions on the kernel that are easy to check.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00999v2</guid>
      <category>math.ST</category>
      <category>math.SP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eunseong Bae, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Phase Transition of Spectral Fluctuations in Large Gram Matrices with a Variance Profile: A Unified Framework for Sparse CLTs</title>
      <link>https://arxiv.org/abs/2602.04302</link>
      <description>arXiv:2602.04302v2 Announce Type: replace 
Abstract: We study the asymptotic spectral behavior of high-dimensional random Gram matrices with sparsity and a variance profile, motivated by applications in wireless communications. Specifically, we consider the Gram matrices $\mathbf S_n=\mathbf Y_n\mathbf Y_n^*$, where the entries of $\mathbf Y_n$ are independent, centered, heteroscedastic, and sparse through Bernoulli masking. The sparsity level is parameterized as $s=q^2/n$, where $q$ ranges from polynomial order up to order $n^{1/2}$.
  We investigate two asymptotic regimes: a moderate-sparsity regime with fixed $s\in(0,1]$, and a high-sparsity regime where $s\to0$. In both regimes, we establish the convergence of the empirical spectral distribution of $\mathbf S_n$ to a deterministic limit, and further derive central limit theorems for linear spectral statistics using resolvent techniques and martingale difference arguments. Our analysis reveals a phase transition in the fluctuation behavior across the two regimes. In the high-sparsity regime, the asymptotic fluctuations are entirely governed by fourth-moment effects, with sparsity-scaled contributions being suppressed. Moreover, the leading deterministic term and the variance of the linear spectral statistic scale at different rates in $q$, causing the standard centering to fail and necessitating an explicit correction to recover a valid CLT. The results apply to both Gaussian and non-Gaussian entries and are illustrated through applications to hypothesis testing and outage probability analysis in large-scale MIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04302v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Guangming Pan, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Exact recovery for seeded graph matching</title>
      <link>https://arxiv.org/abs/2602.06832</link>
      <description>arXiv:2602.06832v2 Announce Type: replace 
Abstract: We study graph matching between two correlated networks in the almost fully seeded regime, where all but a vanishing fraction of vertex correspondences are revealed. Concretely, we consider the correlated stochastic block model and assume that $n^{1-\alpha}$ vertices remain unrevealed for some $\alpha \in (0,1)$, while the remaining $n - n^{1-\alpha}$ vertices are provided as seed correspondences. Our goal is to determine when the true permutation can be recovered efficiently as the proportion of unrevealed vertices vanishes.
  We prove that exact recovery of the remaining correspondences is achievable in polynomial time whenever $\lambda s^{2} &gt; 1 - \alpha$, where $\lambda = (a+b)/2$ is the SBM density parameter and $s$ denotes the edge retention parameter. This condition smoothly interpolates between the fully seeded setting and the classical unseeded threshold $\lambda s^{2} &gt; 1$ for matching in correlated Erd\H{o}s-R\'enyi graphs. Our analysis applies to both a simple neighborhood-overlap rule and a bistochastic relaxation followed by projection, establishing matching achievability in the almost fully seeded regime without requiring spectral methods or message passing.
  On the converse side, we show that below the same threshold, exact recovery is information-theoretically impossible with high probability. Thus, to our knowledge, we obtain the first tight statistical and computational characterization of graph matching when only a vanishing fraction of vertices remain unrevealed. Our results complement recent progress in semi-supervised community detection by demonstrating that revealing all but $n^{1-\alpha}$ correspondences similarly lowers the information threshold for graph matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06832v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Fraiman, Michael Nisenzon</dc:creator>
    </item>
    <item>
      <title>CoinPress: Practical Private Mean and Covariance Estimation</title>
      <link>https://arxiv.org/abs/2006.06618</link>
      <description>arXiv:2006.06618v3 Announce Type: replace-cross 
Abstract: We present simple differentially private estimators for the mean and covariance of multivariate sub-Gaussian data that are accurate at small sample sizes. We demonstrate the effectiveness of our algorithms both theoretically and empirically using synthetic and real-world datasets -- showing that their asymptotic error rates match the state-of-the-art theoretical bounds, and that they concretely outperform all previous methods. Specifically, previous estimators either have weak empirical accuracy at small sample sizes, perform poorly for multivariate data, or require the user to provide strong a priori estimates for the parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.06618v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman</dc:creator>
    </item>
    <item>
      <title>Kernel-based Optimally Weighted Conformal Time-Series Prediction</title>
      <link>https://arxiv.org/abs/2405.16828</link>
      <description>arXiv:2405.16828v3 Announce Type: replace-cross 
Abstract: In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real and synthetic time-series data against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16828v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the Thirteenth International Conference on Learning Representations (ICLR), 2025</arxiv:journal_reference>
      <dc:creator>Jonghyeok Lee, Chen Xu, Yao Xie</dc:creator>
    </item>
    <item>
      <title>On the Computational Efficiency of Bayesian Additive Regression Trees: An Asymptotic Analysis</title>
      <link>https://arxiv.org/abs/2406.19958</link>
      <description>arXiv:2406.19958v2 Announce Type: replace-cross 
Abstract: Bayesian Additive Regression Trees (BART) is a popular Bayesian non-parametric regression model that is commonly used in causal inference and beyond. Its strong predictive performance is supported by well-developed estimation theory, comprising guarantees that its posterior distribution concentrates around the true regression function at optimal rates under various data generative settings and for appropriate prior choices. However, the computational properties of the widely-used BART sampler proposed by Chipman et al. (2010) are yet to be well-understood. In this paper, we perform an asymptotic analysis of a slightly modified version of the default BART sampler when fitted to data-generating processes with discrete covariates. We show that the sampler's time to convergence, evaluated in terms of the hitting time of a high posterior density set, increases with the number of training samples, due to the multi-modal nature of the target posterior. On the other hand, we show that this trend can be dampened by simple changes, such as increasing the number of trees in the ensemble or raising the temperature of the sampler. These results provide a nuanced picture on the computational efficiency of the BART sampler in the presence of large amounts of training data while suggesting strategies to improve the sampler. We complement our theoretical analysis with a simulation study focusing on the default BART sampler. We observe that the increasing trend of convergence time against number training samples holds for the default BART sampler and is robust to changes in sampler initialization, number of burn-in iterations, feature selection prior, and discretization strategy. On the other hand, increasing the number of trees or raising the temperature sharply dampens this trend, as indicated by our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19958v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Shuo Tan, Omer Ronen, Theo Saarinen, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Cross-Validation with Antithetic Gaussian Randomization</title>
      <link>https://arxiv.org/abs/2412.14423</link>
      <description>arXiv:2412.14423v3 Announce Type: replace-cross 
Abstract: We introduce a new cross-validation method based on an equicorrelated Gaussian randomization scheme. Our method is well-suited for problems where sample splitting is infeasible, either because the data violate the assumption of independent and identically distributed samples, or because there are insufficient samples to form representative train-test data pairs. In such problems, our method provides a simple, principled, and computationally efficient approach to estimating prediction error, often outperforming standard cross-validation while requiring only a small number of repetitions.
  Drawing inspiration from recent splitting techniques like data fission and data thinning, our method constructs train-test data pairs using Gaussian randomization. Our main contribution is the introduction of an antithetic Gaussian randomization scheme, involving a carefully designed correlation structure among the randomization variables. We show theoretically that this antithetic construction can eliminate the bias of cross-validation for a broad class of smooth prediction functions, without inflating variance. Through simulations across a range of data types and loss functions, we demonstrate that our estimator outperforms existing methods for prediction error estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14423v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sifan Liu, Snigdha Panigrahi, Jake A. Soloff</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo with Gaussian Mixture Approximation for Infinite-Dimensional Statistical Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16028</link>
      <description>arXiv:2503.16028v4 Announce Type: replace-cross 
Abstract: By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm with Gaussian mixture approximation, together with an easy-to-use reduced version. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the reduced version has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16028v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Lu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Sparsified-Learning for High-Dimensional Heavy-Tailed Locally Stationary Time Series, Concentration and Oracle Inequalities</title>
      <link>https://arxiv.org/abs/2504.06477</link>
      <description>arXiv:2504.06477v2 Announce Type: replace-cross 
Abstract: Sparse learning is ubiquitous in many machine learning tasks. It aims to regularize the goodness-of-fit objective by adding a penalty term to encode structural constraints on the model parameters. In this paper, we develop a flexible sparse learning framework tailored to high-dimensional heavy-tailed locally stationary time series (LSTS). The data-generating mechanism incorporates a regression function that changes smoothly over time and is observed under noise belonging to the class of sub-Weibull and regularly varying distributions. We introduce a sparsity-inducing penalized estimation procedure that combines additive modeling with kernel smoothing and define an additive kernel-smoothing hypothesis class. In the presence of locally stationary dynamics, we assume exponentially decaying $\beta$-mixing coefficients to derive concentration inequalities for kernel-weighted sums of locally stationary processes with heavy-tailed noise. We further establish nonasymptotic prediction-error bounds, yielding both slow and fast convergence rates under different sparsity structures, including Lasso and total variation penalization with the least-squares loss. To support our theoretical results, we conduct numerical experiments on simulated LSTS with sub-Weibull and Pareto noise, highlighting how tail behavior affects prediction error across different covariate-dimensions as the sample size increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06477v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingjie Wang, Mokhtar Z. Alaya, Salim Bouzebda, Xinsheng Liu</dc:creator>
    </item>
    <item>
      <title>Liouville PDE-based sliced-Wasserstein flow</title>
      <link>https://arxiv.org/abs/2505.17204</link>
      <description>arXiv:2505.17204v2 Announce Type: replace-cross 
Abstract: The sliced Wasserstein flow (SWF), a nonparametric and implicit generative gradient flow, is transformed to a Liouville partial differential equation (PDE)-based formalism. First, the stochastic diffusive term from the Fokker-Planck equation-based Monte Carlo is reformulated to Liouville PDE-based transport without the diffusive term, and the involved density estimation is handled by normalizing flows of neural ODE. Next, the computation of the Wasserstein barycenter is approximated by the Liouville PDE-based SWF barycenter with the prescription of Kantorovich potentials for the induced gradient flow to generate its samples. These two efforts show outperforming convergence in training and testing Liouville PDE-based SWF and SWF barycenters with reduced variance. Applying the generative SWF barycenter for fair regression demonstrates competent profiles in the accuracy-fairness Pareto curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17204v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jayshawn Cooper, Pilhwa Lee</dc:creator>
    </item>
    <item>
      <title>Spectra of high-dimensional sparse random geometric graphs</title>
      <link>https://arxiv.org/abs/2507.06556</link>
      <description>arXiv:2507.06556v4 Announce Type: replace-cross 
Abstract: We analyze the spectral properties of the high-dimensional random geometric graph $G(n, d, p)$, formed by sampling $n$ i.i.d vectors $\{v_i\}_{i=1}^{n}$ uniformly on a $d$-dimensional unit sphere and connecting each pair $\{i,j\}$ whenever $\langle v_i, v_j \rangle \geq \tau$ so that $p=\mathbb P(\langle v_i,v_j\rangle \geq \tau)$. This model defines a nonlinear random matrix ensemble with dependent entries. We show that if $d =\omega( np\log^{2}(1/p))$ and $np\to\infty$, the limiting spectral distribution of the normalized adjacency matrix $\frac{A}{\sqrt{np(1-p)}}$ is the semicircle law. To our knowledge, this is the first such result for $G(n, d, p)$ in the sparse regime. In the constant sparsity case $p=\alpha/n$, we further show that if $d=\omega(\log^2(n))$ the limiting spectral distribution of $A$ in $G(n,\alpha/n)$ coincides with that of the Erd\H{o}s-R\'{e}nyi graph $G(n,\alpha/n)$.
  Our approach combines the classical moment method in random matrix theory with a novel recursive decomposition of closed-walk graphs, leveraging block-cut trees and ear decompositions, to control the moments of the empirical spectral distribution. A refined high trace analysis further yields a near-optimal bound on the second eigenvalue when $np=\Omega(\log^4 (n))$, removing technical conditions previously imposed in (Liu et al. 2023). As an application, we demonstrate that this improved eigenvalue bound sharpens the parameter requirements on $d$ and $p$ for spontaneous synchronization on random geometric graphs in (Abdalla et al. 2024) under the homogeneous Kuramoto model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06556v4</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cao, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Two-sample Testing with Block-wise Missingness in Multi-source Data</title>
      <link>https://arxiv.org/abs/2508.17411</link>
      <description>arXiv:2508.17411v2 Announce Type: replace-cross 
Abstract: Multi-source and multi-modal datasets are increasingly common in scientific research, yet they often exhibit block-wise missingness, where entire modalities are systematically absent in some sources or no single source contains all modalities. This structured missingness poses major challenges for two-sample hypothesis testing. Standard approaches, such as imputation or complete-case analysis, may introduce bias or suffer efficiency loss, especially under missingness-not-at-random mechanisms. To address this challenge, we propose the Block-Pattern Enhanced Test, a general framework for constructing two-sample testing statistics that explicitly accounts for block-wise missingness. We show that the framework yields valid tests under a new condition allowing for missing-not-at-random mechanism. Building on this general framework, we further propose the Block-wise Rank In Similarity graph Edge-count (BRISE) test, which accommodate heterogeneous modalities using rank-based similarity graphs. Theoretically, we establish that the null distribution of BRISE converges to a $\chi^2$ distribution, and that the test is consistent both in the standard asymptotic regime and in the high-dimensional low-sample-size setting under mild conditions. Simulation studies demonstrate that BRISE controls the type-I error rate and achieves strong power across a wide range of alternatives. Applications to two real-world datasets with block-wise missingness further illustrate the practical utility of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17411v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kejian Zhang, Muxuan Liang, Robert Maile, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Adaptive Off-Policy Inference for M-Estimators Under Model Misspecification</title>
      <link>https://arxiv.org/abs/2509.14218</link>
      <description>arXiv:2509.14218v2 Announce Type: replace-cross 
Abstract: When data are collected adaptively, such as in bandit algorithms, classical statistical approaches such as ordinary least squares and $M$-estimation will often fail to achieve asymptotic normality. Although recent lines of work have modified the classical approaches to ensure valid inference on adaptively collected data, most of these works assume that the model is correctly specified. The misspecified setting poses unique challenges because the parameter of interest itself may not be well-defined over a non-stationary distribution of rewards. We therefore tackle the problem of \emph{off-policy} inference in adaptive settings, where we uniquely define a projected solution over a stationary evaluation policy. Our method provides valid inference for $M$-estimators that use adaptively collected bandit data with a possibly misspecified working model. A key ingredient in our approach is the use of flexible approaches to stabilize the variance induced by adaptive data collection. A major novelty is that the procedure enables the construction of valid confidence sets even in settings where treatment policies are unstable and non-converging, such as when there is no unique optimal arm and standard bandit algorithms are used. Empirical results on semi-synthetic datasets constructed from the Osteoarthritis Initiative demonstrate that the method maintains type I error control, while existing methods for inference in adaptive settings do not cover in the misspecified case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14218v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Robin Dunn, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Identification and Debiased Learning of Causal Effects with General Instrumental Variables</title>
      <link>https://arxiv.org/abs/2510.20404</link>
      <description>arXiv:2510.20404v2 Announce Type: replace-cross 
Abstract: Instrumental variable methods are fundamental to causal inference when treatment assignment is confounded by unobserved variables. In this article, we develop a general nonparametric causal framework for identification and learning with multi-categorical or continuous instrumental variables. Specifically, the mean potential outcomes and the average treatment effect can be identified via a regular weighting function derived from the proposed framework. Leveraging semiparametric theory, we derive efficient influence functions and construct two consistent, asymptotically normal estimators via debiased machine learning. The first estimator uses a prespecified weighting function, while the second estimator selects the optimal weighting function adaptively. Extensions to longitudinal data, dynamic treatment regimes, and multiplicative instrumental variables are further developed. We demonstrate the proposed method by employing simulation studies and analyzing real data from the Job Training Partnership Act program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20404v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Chen, Peng Zhang, Yifan Cui</dc:creator>
    </item>
    <item>
      <title>Deep Ensembles for Epistemic Uncertainty: A Frequentist Perspective</title>
      <link>https://arxiv.org/abs/2510.22063</link>
      <description>arXiv:2510.22063v3 Announce Type: replace-cross 
Abstract: Decomposing prediction uncertainty into aleatoric (irreducible) and epistemic (reducible) components is critical for the reliable deployment of machine learning systems. While the mutual information between the response variable and model parameters is a principled measure for epistemic uncertainty, it requires access to the parameter posterior, which is computationally challenging to approximate. Consequently, practitioners often rely on probabilistic predictions from deep ensembles to quantify uncertainty, which have demonstrated strong empirical performance. However, a theoretical understanding of their success from a frequentist perspective remains limited. We address this gap by first considering a bootstrap-based estimator for epistemic uncertainty, which we prove is asymptotically correct. Next, we connect deep ensembles to the bootstrap estimator by decomposing it into data variability and training stochasticity; specifically, we show that deep ensembles capture the training stochasticity component. Through empirical studies, we show that this stochasticity component constitutes the majority of epistemic uncertainty, thereby explaining the effectiveness of deep ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22063v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anchit Jain, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond</title>
      <link>https://arxiv.org/abs/2512.04696</link>
      <description>arXiv:2512.04696v2 Announce Type: replace-cross 
Abstract: We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.
  Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04696v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuma Sawaya</dc:creator>
    </item>
    <item>
      <title>Extreme Score Distributions in Countable-Outcome Round-Robin Tournaments of Equally Strong Players</title>
      <link>https://arxiv.org/abs/2601.15950</link>
      <description>arXiv:2601.15950v2 Announce Type: replace-cross 
Abstract: We consider a general class of round-robin tournament models of equally strong players. In these models, each of the $n$ players competes against every other player exactly once. For each match between two players, the outcome is a value from a countable subset of the unit interval, and the scores of the two players in a match sum to one. The final score of each player is defined as the sum of the scores obtained in matches against all other players. We study the distribution of extreme scores, including the maximum, second maximum, and lower-order extremes. Since the exact distribution is computationally intractable even for small values of $n$, we derive asymptotic results as the number of players $n$ tends to infinity, including limiting distributions, and rates of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15950v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaakov Malinovsky</dc:creator>
    </item>
    <item>
      <title>Optimal Design under Interference, Homophily, and Robustness Trade-offs</title>
      <link>https://arxiv.org/abs/2601.17145</link>
      <description>arXiv:2601.17145v2 Announce Type: replace-cross 
Abstract: To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17145v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vydhourie Thiyageswaran, Alex Kokot, Jennifer Brennan, Marina Meila, Christina Lee Yu, Maryam Fazel</dc:creator>
    </item>
    <item>
      <title>Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions</title>
      <link>https://arxiv.org/abs/2602.01777</link>
      <description>arXiv:2602.01777v2 Announce Type: replace-cross 
Abstract: Stochastic gradient methods are central to large-scale learning, but they treat mini-batch gradients as unbiased estimators, which classical decision theory shows are inadmissible in high dimensions. We formulate gradient computation as a high-dimensional estimation problem and introduce a framework based on Stein-rule shrinkage. We construct a gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging statistics from adaptive optimizers. Under a Gaussian noise model, we show our estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal. We incorporate this into the Adam optimizer, yielding SR-Adam, a practical algorithm with negligible computational cost. Empirical evaluations on CIFAR10 and CIFAR100 across multiple levels of input noise show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled approach to improving stochastic gradient estimation in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01777v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Arashi, M. Amintoosi</dc:creator>
    </item>
  </channel>
</rss>
