<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 04:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalable Multivariate Fronthaul Quantization for Cell-Free Massive MIMO</title>
      <link>https://arxiv.org/abs/2409.06715</link>
      <description>arXiv:2409.06715v1 Announce Type: new 
Abstract: The conventional approach to the fronthaul design for cell-free massive MIMO system follows the compress-and-precode (CP) paradigm. Accordingly, encoded bits and precoding coefficients are shared by the distributed unit (DU) on the fronthaul links, and precoding takes place at the radio units (RUs). Previous theoretical work has shown that CP can be potentially improved by a significant margin by precode-and-compress (PC) methods, in which all baseband processing is carried out at the DU, which compresses the precoded signals for transmission on the fronthaul links. The theoretical performance gain of PC methods are particularly pronounced when the DU implements multivariate quantization (MQ), applying joint quantization across the signals for all the RUs. However, existing solutions for MQ are characterized by a computational complexity that grows exponentially with the sum-fronthaul capacity from the DU to all RUs. This work sets out to design scalable MQ strategies for PC-based cell-free massive MIMO systems. For the low-fronthaul capacity regime, we present alpha-parallel MQ (alpha-PMQ), whose complexity is exponential only in the fronthaul capacity towards an individual RU, while performing close to full MQ. alpha-PMQ tailors MQ to the topology of the network by allowing for parallel local quantization steps for RUs that do not interfere too much with each other. For the high-fronthaul capacity regime, we then introduce neural MQ, which replaces the exhaustive search in MQ with gradient-based updates for a neural-network-based decoder, attaining a complexity that grows linearly with the sum-fronthaul capacity. Numerical results demonstrate that the proposed scalable MQ strategies outperform CP for both the low and high-fronthaul capacity regimes at the cost of increased computational complexity at the DU (but not at the RUs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06715v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangwoo Park, Ahmet Hasim Gokceoglu, Li Wang, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Design of Threshold-Constrained Indirect Quantizers</title>
      <link>https://arxiv.org/abs/2409.06839</link>
      <description>arXiv:2409.06839v1 Announce Type: new 
Abstract: We address the problem of indirect quantization of a source subject to a mean-squared error distortion constraint. A well-known result of Wolf and Ziv is that the problem can be reduced to a standard (direct) quantization problem via a two-step approach: first apply the conditional expectation estimator, obtaining a ``new'' source, then solve for the optimal quantizer for the latter source. When quantization is implemented in hardware, however, invariably constraints on the allowable class of quantizers are imposed, typically limiting the class to \emph{time-invariant} scalar quantizers with contiguous quantization cells. In the present work, optimal indirect quantization subject to these constraints is considered. Necessary conditions an optimal quantizer within this class must satisfy are derived, in the form of generalized Lloyd-Max conditions, and an iterative algorithm for the design of such quantizers is proposed. Furthermore, for the case of a scalar observation, we derive a non-iterative algorithm for finding the optimal indirect quantizer based on dynamic programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06839v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Doubchak, Tal Philosof, Uri Erez, Amit Berman</dc:creator>
    </item>
    <item>
      <title>Refracting Reconfigurable Intelligent Surface Assisted URLLC for Millimeter Wave High-Speed Train Communication Coverage Enhancement</title>
      <link>https://arxiv.org/abs/2409.06946</link>
      <description>arXiv:2409.06946v1 Announce Type: new 
Abstract: High-speed train (HST) has garnered significant attention from both academia and industry due to the rapid development of railways worldwide. Millimeter wave (mmWave) communication, known for its large bandwidth is an effective way to address performance bottlenecks in cellular network based HST wireless communication systems. However, mmWave signals suffer from significant path loss when traversing carriage, posing substantial challenges to cellular networks. To address this issue, reconfigurable intelligent surfaces (RIS) have gained considerable interest for its ability to enhance cell coverage by reflecting signals toward receiver. Ensuring communication reliability, a core performance indicators of ultra-reliable and low-latency communications (URLLC) in fifth-generation systems, is crucial for providing steady and reliable data transmissions along railways, particularly for delivering safety and control messages and monitoring HST signaling information. In this paper, we investigate a refracting RIS-assisted multi-user multiple-input single-output URLLC system in mmWave HST communications. We propose a sum rate maximization problem, subject to base station beamforming constraint, as well as refracting RIS discrete phase shifts and reliability constraints. To solve this optimization problem, we design a joint optimization algorithm based on alternating optimization method. This involves decoupling the original optimization problem into active beamforming design and packet error probability optimization subproblem, and discrete phase shift design subproblems. These subproblems are addressed exploiting Lagrangian dual method and the local search method, respectively. Simulation results demonstrate the fast convergence of the proposed algorithm and highlight the benefits of refracting RIS adoption for sum rate improvement in mmWave HST networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06946v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changzhu Liu, Ruisi He, Yong Niu, Shiwen Mao, Bo Ai, Ruifeng Chen</dc:creator>
    </item>
    <item>
      <title>A High-Performance List Decoding Algorithm for Surface Codes with Erroneous Syndrome</title>
      <link>https://arxiv.org/abs/2409.06979</link>
      <description>arXiv:2409.06979v1 Announce Type: new 
Abstract: Quantum error-correcting codes (QECCs) are necessary for fault-tolerant quantum computation. Surface codes are a class of topological QECCs that have attracted significant attention due to their exceptional error-correcting capabilities and easy implementation. In the decoding process of surface codes, the syndromes are crucial for error correction, though they are not always correctly measured. Most of the existing decoding algorithms for surface codes are not equipped to handle erroneous syndrome information or need additional measurements to correct syndromes with errors, which implies a potential increase in inference complexity and decoding latency. In this paper, we propose a high-performance list decoding algorithm for surface codes with erroneous syndromes. More specifically, to cope with erroneous syndrome information, we incorporate syndrome soft information, allowing the syndrome to be listed as well. To enhance the efficiency of the list decoding algorithm, we use LCOSD, which can significantly reduce the average list size in classical error correction compared with the conventional ordered statistics decoding (OSD). Numerical results demonstrate that our proposed algorithm significantly improves the decoding performance of surface codes with erroneous syndromes compared to minimum-weight perfect matching (MWPM) and BP decoders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06979v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jifan Liang, Qianfan Wang, Lvzhou Li, Xiao Ma</dc:creator>
    </item>
    <item>
      <title>List-based Optimization of Proximal Decoding for LDPC Codes</title>
      <link>https://arxiv.org/abs/2409.07278</link>
      <description>arXiv:2409.07278v1 Announce Type: new 
Abstract: In this paper, the proximal decoding algorithm is considered within the context of additive white Gaussian noise (AWGN) channels. An analysis of the convergence behavior of the algorithm shows that proximal decoding inherently enters an oscillating behavior of the estimate after a certain number of iterations. Due to this oscillation, frame errors arising during decoding can often be attributed to only a few remaining wrongly decoded bit positions. In this letter, an improvement of the proximal decoding algorithm is proposed by establishing an additional step, in which these erroneous positions are attempted to be corrected. We suggest an empirical rule with which the components most likely needing correction can be determined. Using this insight and performing a subsequent ``ML-in-the-list'' decoding, a gain of up to 1 dB is achieved compared to conventional proximal decoding, depending on the decoder parameters and the code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07278v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCOMM.2024.3458422</arxiv:DOI>
      <dc:creator>Andreas Tsouchlos, Holger J\"akel, Laurent Schmalen</dc:creator>
    </item>
    <item>
      <title>Statistically Valid Information Bottleneck via Multiple Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2409.07325</link>
      <description>arXiv:2409.07325v1 Announce Type: new 
Abstract: The information bottleneck (IB) problem is a widely studied framework in machine learning for extracting compressed features that are informative for downstream tasks. However, current approaches to solving the IB problem rely on a heuristic tuning of hyperparameters, offering no guarantees that the learned features satisfy information-theoretic constraints. In this work, we introduce a statistically valid solution to this problem, referred to as IB via multiple hypothesis testing (IB-MHT), which ensures that the learned features meet the IB constraints with high probability, regardless of the size of the available dataset. The proposed methodology builds on Pareto testing and learn-then-test (LTT), and it wraps around existing IB solvers to provide statistical guarantees on the IB constraints. We demonstrate the performance of IB-MHT on classical and deterministic IB formulations, validating the effectiveness of IB-MHT in outperforming conventional methods in terms of statistical robustness and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07325v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amirmohammad Farzaneh, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Joint Energy and SINR Coverage Probability in UAV Corridor-assisted RF-powered IoT Networks</title>
      <link>https://arxiv.org/abs/2409.07333</link>
      <description>arXiv:2409.07333v1 Announce Type: new 
Abstract: This letter studies the joint energy and signal-to-interference-plus-noise (SINR)-based coverage probability in Unmanned Aerial Vehicle (UAV)-assisted radio frequency (RF)-powered Internet of Things (IoT) networks. The UAVs are spatially distributed in an aerial corridor that is modeled as a one-dimensional (1D) binomial point process (BPP). By accurately capturing the line-of-sight (LoS) probability of a UAV through large-scale fading i) an exact form expression for the energy coverage probability is derived, and ii) a tight approximation for the overall coverage performance is obtained. Among several key findings, numerical results reveal the optimal number of deployed UAV-BSs that maximizes the joint coverage probability, as well as the optimal length of the UAV corridors when designing such UAV-assisted IoT networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07333v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harris K. Armeniakos, Petros S. Bithas, Konstantinos Maliatsos, Athanasios G. Kanatas</dc:creator>
    </item>
    <item>
      <title>Decomposition of surprisal: Unified computational model of ERP components in language processing</title>
      <link>https://arxiv.org/abs/2409.06803</link>
      <description>arXiv:2409.06803v1 Announce Type: cross 
Abstract: The functional interpretation of language-related ERP components has been a central debate in psycholinguistics for decades. We advance an information-theoretic model of human language processing in the brain in which incoming linguistic input is processed at first shallowly and later with more depth, with these two kinds of information processing corresponding to distinct electroencephalographic signatures. Formally, we show that the information content (surprisal) of a word in context can be decomposed into two quantities: (A) heuristic surprise, which signals shallow processing difficulty for a word, and corresponds with the N400 signal; and (B) discrepancy signal, which reflects the discrepancy between shallow and deep interpretations, and corresponds to the P600 signal. Both of these quantities can be estimated straightforwardly using modern NLP models. We validate our theory by successfully simulating ERP patterns elicited by a variety of linguistic manipulations in previously-reported experimental data from six experiments, with successful novel qualitative and quantitative predictions. Our theory is compatible with traditional cognitive theories assuming a `good-enough' heuristic interpretation stage, but with a precise information-theoretic formulation. The model provides an information-theoretic model of ERP components grounded on cognitive processes, and brings us closer to a fully-specified neuro-computational model of language processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06803v1</guid>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxuan Li, Richard Futrell</dc:creator>
    </item>
    <item>
      <title>Five Key Enablers for Communication during and after Disasters</title>
      <link>https://arxiv.org/abs/2409.06822</link>
      <description>arXiv:2409.06822v1 Announce Type: cross 
Abstract: Civilian communication during disasters such as earthquakes, floods, and military conflicts is crucial for saving lives. Nevertheless, several challenges exist during these circumstances such as the destruction of cellular communication and electricity infrastructure, lack of line of sight (LoS), and difficulty of localization under the rubble. In this article, we discuss key enablers that can boost communication during disasters, namely, satellite and aerial platforms, redundancy, silencing, and sustainable networks aided with wireless energy transfer (WET). The article also highlights how these solutions can be implemented in order to solve the failure of communication during disasters. Finally, it sheds light on unresolved challenges, as well as future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06822v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Shehab, Mustafa Kishk, Maurilio Matracia, Mehdi Bennis, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free Massive MIMO Systems</title>
      <link>https://arxiv.org/abs/2208.12453</link>
      <description>arXiv:2208.12453v2 Announce Type: replace 
Abstract: Cell-free massive multiple-input-multiple-output is promising to meet the stringent quality-of-experience (QoE) requirements of railway wireless communications by coordinating many successional access points (APs) to serve the onboard users coherently. A key challenge is how to deliver the desired contents timely due to the radical changing propagation environment caused by the growing train speed. In this paper, we propose to proactively cache the likely-requesting contents at the upcoming APs which perform the coherent transmission to reduce end-to-end delay. A long-term QoE-maximization problem is formulated and two cache placement algorithms are proposed. One is based on heuristic convex optimization (HCO) and the other exploits deep reinforcement learning (DRL) with soft actor-critic (SAC). Compared to the conventional benchmark, numerical results show the advantage of our proposed algorithms on QoE and hit probability. With the advanced DRL model, SAC outperforms HCO on QoE by predicting the user requests accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12453v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhang, Shuaifei Chen, Jiayi Zhang</dc:creator>
    </item>
    <item>
      <title>The Principle of Uncertain Maximum Entropy</title>
      <link>https://arxiv.org/abs/2305.09868</link>
      <description>arXiv:2305.09868v4 Announce Type: replace 
Abstract: The principle of maximum entropy is a well-established technique for choosing a distribution that matches available information while minimizing bias. It finds broad use across scientific disciplines and in machine learning. However, the principle as defined by is susceptible to noise and error in observations. This forces real-world practitioners to use relaxed versions of the principle in an ad hoc way, negatively impacting interpretation. To address this situation, we present a new principle we call uncertain maximum entropy that generalizes the classic principle and provides interpretable solutions irrespective of the observational methods in use. We introduce a convex approximation and expectation-maximization based algorithm for finding solutions to our new principle. Finally, we contrast this new technique with two simpler generally applicable solutions theoretically and experimentally show our technique provides superior accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09868v4</guid>
      <category>cs.IT</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Bogert, Matthew Kothe</dc:creator>
    </item>
    <item>
      <title>Lossless Image Compression Using Multi-level Dictionaries: Binary Images</title>
      <link>https://arxiv.org/abs/2406.03087</link>
      <description>arXiv:2406.03087v3 Announce Type: replace 
Abstract: Lossless image compression is required in various applications to reduce storage or transmission costs of images, while requiring the reconstructed images to have zero information loss compared to the original. Existing lossless image compression methods either have simple design but poor compression performance, or complex design, better performance, but with no performance guarantees. In our endeavor to develop a lossless image compression method with low complexity and guaranteed performance, we argue that compressibility of a color image is essentially derived from the patterns in its spatial structure, intensity variations, and color variations. Thus, we divide the overall design of a lossless image compression scheme into three parts that exploit corresponding redundancies. We further argue that the binarized version of an image captures its fundamental spatial structure. In this first part of our work, we propose a scheme for lossless compression of binary images.
  The proposed scheme first learns dictionaries of $16\times16$, $8\times8$, $4\times4$, and $2\times 2$ square pixel patterns from various datasets of binary images. It then uses these dictionaries to encode binary images. These dictionaries have various interesting properties that are further exploited to construct an efficient and scalable scheme. Our preliminary results show that the proposed scheme consistently outperforms existing conventional and learning based lossless compression approaches, and provides, on average, as much as $1.5\times$ better performance than a common general purpose lossless compression scheme (WebP), more than $3\times$ better performance than a state of the art learning based scheme, and better performance than a specialized scheme for binary image compression (JBIG2).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03087v3</guid>
      <category>cs.IT</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samar Agnihotri, Renu Rameshan, Ritwik Ghosal</dc:creator>
    </item>
    <item>
      <title>Revisit the Arimoto-Blahut algorithm: New Analysis with Approximation</title>
      <link>https://arxiv.org/abs/2407.06013</link>
      <description>arXiv:2407.06013v3 Announce Type: replace 
Abstract: By the seminal paper of Claude Shannon \cite{Shannon48}, the computation of the capacity of a discrete memoryless channel has been considered as one of the most important and fundamental problems in Information Theory. Nearly 50 years ago, Arimoto and Blahut independently proposed identical algorithms to solve this problem in their seminal papers \cite{Arimoto1972AnAF, Blahut1972ComputationOC}. The Arimoto-Blahut algorithm was proven to converge to the capacity of the channel as $t \to \infty$ with the convergence rate upper bounded by $O\left(\log(m)/t\right)$, where $m$ is the size of the input distribution, and being inverse exponential when there is a unique solution in the interior of the input probability simplex \cite{Arimoto1972AnAF}. Recently it was proved, in \cite{Nakagawa2020AnalysisOT}, that the convergence rate is at worst inverse linear $O(1/t)$ in some specific cases.
  In this paper, we revisit this fundamental algorithm looking at the rate of convergence to the capacity and the time complexity, given $m,n$, where $n$ is size of the output of the channel, focusing on the approximation of the capacity. We prove that the rate of convergence to an $\varepsilon$-optimal solution, for any constant $\varepsilon &gt; 0$, is inverse exponential $O\left(\log(m)/c^t\right)$, for a constant $c &gt; 1$ and $O\left(\log \left(\log (m)/\varepsilon\right)\right)$ at most iterations, implying $O\left(m n\log \left(\log (m)/\varepsilon\right)\right)$ total complexity of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06013v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michail Fasoulakis, Konstantinos Varsos, Apostolos Traganitis</dc:creator>
    </item>
    <item>
      <title>From Concept to Reality: 5G Positioning with Open-Source Implementation of UL-TDoA in OpenAirInterface</title>
      <link>https://arxiv.org/abs/2409.05217</link>
      <description>arXiv:2409.05217v2 Announce Type: replace 
Abstract: This paper presents, for the first time, an open-source implementation of the 3GPP Uplink Time Difference of Arrival (UL-TDoA) positioning method using the OpenAirInterface (OAI) framework. UL-TDoA is a critical positioning technique in 5G networks, leveraging the time differences of signal arrival at multiple base stations to determine the precise location of User Equipment (UE). This implementation aims to democratize access to advanced positioning technology by integrating UL-TDoA capabilities into both the Radio Access Network (RAN) and Core Network (CN) components of OAI, providing a comprehensive and 3GPP-compliant solution. The development includes the incorporation of essential protocol procedures, message flows, and interfaces as defined by 3GPP standards. Validation is conducted using two distinct methods: an OAI-RF simulator-based setup for controlled testing and an O-RAN-based Localization Testbed at EURECOM in real-world conditions. The results demonstrate the viability of this open-source UL-TDoA implementation, enabling precise positioning in various environments. By making this implementation publicly available, the study paves the way for widespread research, development, and innovation in the field of 5G positioning technologies, fostering collaboration and accelerating the advancement of cellular network positioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05217v2</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adeel Malik, Mohsen Ahadi, Florian Kaltenberger, Klaus Warnke, Nguyen Tien Thinh, Nada Bouknana, Cedric Thienot, Godswill Onche, Sagar Arora</dc:creator>
    </item>
    <item>
      <title>Quantum Phase Estimation by Compressed Sensing</title>
      <link>https://arxiv.org/abs/2306.07008</link>
      <description>arXiv:2306.07008v4 Announce Type: replace-cross 
Abstract: As a signal recovery algorithm, compressed sensing is particularly useful when the data has low-complexity and samples are rare, which matches perfectly with the task of quantum phase estimation (QPE). In this work we present a new Heisenberg-limited QPE algorithm for early quantum computers based on compressed sensing. More specifically, given many copies of a proper initial state and queries to some unitary operators, our algorithm is able to recover the frequency with a total runtime $\mathcal{O}(\epsilon^{-1}\text{poly}\log(\epsilon^{-1}))$, where $\epsilon$ is the accuracy. Moreover, the maximal runtime satisfies $T_{\max}\epsilon \ll \pi$, which is comparable to the state of art algorithms, and our algorithm is also robust against certain amount of noise from sampling. We also consider the more general quantum eigenvalue estimation problem (QEEP) and show numerically that the off-grid compressed sensing can be a strong candidate for solving the QEEP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07008v4</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changhao Yi, Cunlu Zhou, Jun Takahashi</dc:creator>
    </item>
    <item>
      <title>Efficient entanglement purification based on noise guessing decoding</title>
      <link>https://arxiv.org/abs/2310.19914</link>
      <description>arXiv:2310.19914v4 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel bipartite entanglement purification protocol built upon hashing and upon the guessing random additive noise decoding (GRAND) approach recently devised for classical error correction codes. Our protocol offers substantial advantages over existing hashing protocols, requiring fewer qubits for purification, achieving higher fidelities, and delivering better yields with reduced computational costs. We provide numerical and semi-analytical results to corroborate our findings and provide a detailed comparison with the hashing protocol of Bennet et al. Although that pioneering work devised performance bounds, it did not offer an explicit construction for implementation. The present work fills that gap, offering both an explicit and more efficient purification method. We demonstrate that our protocol is capable of purifying states with noise on the order of 10% per Bell pair even with a small ensemble of 16 pairs. The work explores a measurement-based implementation of the protocol to address practical setups with noise. This work opens the path to practical and efficient entanglement purification using hashing-based methods with feasible computational costs. Compared to the original hashing protocol, the proposed method can achieve some desired fidelity with a number of initial resources up to one hundred times smaller. Therefore, the proposed method seems well-fit for future quantum networks with a limited number of resources and entails a relatively low computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19914v4</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Roque, Diogo Cruz, Francisco A. Monteiro, Bruno C. Coutinho</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Methods for a Tree-Structured Stick-Breaking Process Mixture of Gaussians by Application of the Bayes Codes for Context Tree Models</title>
      <link>https://arxiv.org/abs/2405.00385</link>
      <description>arXiv:2405.00385v2 Announce Type: replace-cross 
Abstract: The tree-structured stick-breaking process (TS-SBP) mixture model is a non-parametric Bayesian model that can represent tree-like hierarchical structures among the mixture components. For TS-SBP mixture models, only a Markov chain Monte Carlo (MCMC) method has been proposed and any variational Bayesian (VB) methods has not been proposed. In general, MCMC methods are computationally more expensive than VB methods. Therefore, we require a large computational cost to learn the TS-SBP mixture model. In this paper, we propose a learning algorithm with less computational cost for the TS-SBP mixture of Gaussians by using the VB method under an assumption of finite tree width and depth. When constructing such VB method, the main challenge is efficient calculation of a sum over all possible trees. To solve this challenge, we utilizes a subroutine in the Bayes coding algorithm for context tree models. We confirm the computational efficiency of our VB method through an experiments on a benchmark dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00385v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Nakahara</dc:creator>
    </item>
    <item>
      <title>Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</title>
      <link>https://arxiv.org/abs/2408.13276</link>
      <description>arXiv:2408.13276v2 Announce Type: replace-cross 
Abstract: For the problem of reconstructing a low-rank matrix from a few linear measurements, two classes of algorithms have been widely studied in the literature: convex approaches based on nuclear norm minimization, and non-convex approaches that use factorized gradient descent. Under certain statistical model assumptions, it is known that nuclear norm minimization recovers the ground truth as soon as the number of samples scales linearly with the number of degrees of freedom of the ground-truth. In contrast, while non-convex approaches are computationally less expensive, existing recovery guarantees assume that the number of samples scales at least quadratically with the rank $r$ of the ground-truth matrix. In this paper, we close this gap by showing that the non-convex approaches can be as efficient as nuclear norm minimization in terms of sample complexity. Namely, we consider the problem of reconstructing a positive semidefinite matrix from a few Gaussian measurements. We show that factorized gradient descent with spectral initialization converges to the ground truth with a linear rate as soon as the number of samples scales with $ \Omega (rd\kappa^2)$, where $d$ is the dimension, and $\kappa$ is the condition number of the ground truth matrix. This improves the previous rank-dependence in the sample complexity of non-convex matrix factorization from quadratic to linear. Our proof relies on a probabilistic decoupling argument, where we show that the gradient descent iterates are only weakly dependent on the individual entries of the measurement matrices. We expect that our proof technique is of independent interest for other non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13276v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik St\"oger, Yizhe Zhu</dc:creator>
    </item>
  </channel>
</rss>
