<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Groupoid-syntax of Type Theory is a Set</title>
      <link>https://arxiv.org/abs/2509.14988</link>
      <description>arXiv:2509.14988v1 Announce Type: new 
Abstract: Categories with families (CwFs) have been used to define the semantics of type theory in type theory. In the setting of Homotopy Type Theory (HoTT), one of the limitations of the traditional notion of CwFs is the requirement to set-truncate types, which excludes models based on univalent categories, such as the standard set model. To address this limitation, we introduce the concept of a Groupoid Category with Families (GCwF). This framework truncates types at the groupoid level and incorporates coherence equations, providing a natural extension of the CwF framework when starting from a 1-category.
  We demonstrate that the initial GCwF for a type theory with a base family of sets and Pi-types (groupoid-syntax) is set-truncated. Consequently, this allows us to utilize the conventional intrinsic syntax of type theory while enabling interpretations in semantically richer and more natural models. All constructions in this paper were formalised in Cubical Agda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14988v1</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thorsten Altenkirch, Ambrus Kaposi, Szumi Xie</dc:creator>
    </item>
    <item>
      <title>Theorem Provers: One Size Fits All?</title>
      <link>https://arxiv.org/abs/2509.15015</link>
      <description>arXiv:2509.15015v1 Announce Type: new 
Abstract: Theorem provers are important tools for people working in formal verification. There are a myriad of interactive systems available today, with varying features and approaches motivating their development. These design choices impact their usability, alongside the problem domain in which they are employed. We test-drive two such provers, Coq and Idris2, by proving the correctness of insertion sort, before providing a qualitative evaluation of their performance. We then compare their community and library support. This work helps users to make an informed choice of system, and highlight approaches in other systems that developers might find useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15015v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Oates, Hyeonggeun Yun, Nikhila Gurusinghe</dc:creator>
    </item>
    <item>
      <title>The mechanization of science illustrated by the Lean formalization of the multi-graded Proj construction</title>
      <link>https://arxiv.org/abs/2509.15116</link>
      <description>arXiv:2509.15116v1 Announce Type: new 
Abstract: We formalize the multi-graded Proj construction in Lean4, illustrating mechanized mathematics and formalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15116v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>math.AG</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Mayeux, Jujian Zhang</dc:creator>
    </item>
    <item>
      <title>An adequacy theorem between mixed powerdomains and probabilistic concurrency</title>
      <link>https://arxiv.org/abs/2409.15920</link>
      <description>arXiv:2409.15920v2 Announce Type: replace 
Abstract: We present an adequacy theorem for a concurrent extension of probabilistic GCL. The underlying denotational semantics is based on the so-called mixed powerdomains, which combine non-determinism with probabilistic behaviour. The theorem itself is formulated via M. Smyth's idea of treating observable properties as open sets of a topological space. The proof hinges on a 'topological generalisation' of K\"onig's lemma in the setting of probabilistic programming (a result that is proved in the paper as well).
  One application of the theorem is that it entails semi-decidability w.r.t. whether a concurrent program satisfies an observable property (written in a certain form). This is related to M. Escard\'o's conjecture about semi-decidability w.r.t. may and must probabilistic testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15920v2</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Neves</dc:creator>
    </item>
    <item>
      <title>Simple Classes of Automatic Structures</title>
      <link>https://arxiv.org/abs/2505.22821</link>
      <description>arXiv:2505.22821v2 Announce Type: replace 
Abstract: We study two subclasses of the class of automatic structures: automatic structures of polynomial growth and Presburger structures. We present algebraic characterisations of the groups and the equivalence structures in these two classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22821v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Achim Blumensath</dc:creator>
    </item>
    <item>
      <title>Rule-Based Error Detection and Correction to Operationalize Movement Trajectory Classification</title>
      <link>https://arxiv.org/abs/2308.14250</link>
      <description>arXiv:2308.14250v5 Announce Type: replace-cross 
Abstract: Classification of movement trajectories has many applications in transportation and is a key component for large-scale movement trajectory generation and anomaly detection which has key safety applications in the aftermath of a disaster or other external shock. However, the current state-of-the-art (SOTA) are based on supervised deep learning - which leads to challenges when the distribution of trajectories changes due to such a shock. We provide a neuro-symbolic rule-based framework to conduct error correction and detection of these models to integrate into our movement trajectory platform. We provide a suite of experiments on several recent SOTA models where we show highly accurate error detection, the ability to improve accuracy with a changing test distribution, and accuracy improvement for the base use case in addition to a suite of theoretical properties that informed algorithm development. Specifically, we show an F1 scores for predicting errors of up to 0.984, significant performance increase for out-of distribution accuracy (8.51% improvement over SOTA for zero-shot accuracy), and accuracy improvement over the SOTA model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14250v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bowen Xi, Kevin Scaria, Divyagna Bavikadi, Paulo Shakarian</dc:creator>
    </item>
    <item>
      <title>Trade-offs between classical and quantum space using spooky pebbling</title>
      <link>https://arxiv.org/abs/2401.10579</link>
      <description>arXiv:2401.10579v5 Announce Type: replace-cross 
Abstract: Pebble games are used to study space/time trade-offs. Recently, spooky pebble games were introduced to study classical space / quantum space / time trade-offs for simulation of classical circuits on quantum computers. In this paper, the spooky pebble game framework is applied for the first time to general circuits. Using this framework we prove an upper bound for quantum space in the spooky pebble game. We also prove that solving the spooky pebble game is PSPACE-complete. Moreover, we present a solver for the spooky pebble game based on satisfiability solvers combined with heuristic optimizers. This spooky pebble game solver was empirically evaluated by calculating optimal classical space / quantum space / time trade-offs. Within limited runtime, the solver could find a strategy reducing quantum space when classical space is taken into account, showing that the spooky pebble model is useful to reduce quantum space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10579v5</guid>
      <category>quant-ph</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arend-Jan Quist, Alfons Laarman</dc:creator>
    </item>
    <item>
      <title>Scalable Interconnect Learning in Boolean Networks</title>
      <link>https://arxiv.org/abs/2507.02585</link>
      <description>arXiv:2507.02585v2 Announce Type: replace-cross 
Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, allowing DBNs to scale to far wider layers than earlier learnable-interconnect designs while preserving their advantageous accuracy. To further reduce model size, we propose two complementary pruning stages: an SAT-based logic equivalence pass that removes redundant gates without affecting performance, and a similarity-based, data-driven pass that outperforms a magnitude-style greedy baseline and offers a superior compression-accuracy trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02585v2</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Fabian Kresse, Emily Yu, Christoph H. Lampert</dc:creator>
    </item>
    <item>
      <title>Neural Logic Networks for Interpretable Classification</title>
      <link>https://arxiv.org/abs/2508.08172</link>
      <description>arXiv:2508.08172v3 Announce Type: replace-cross 
Abstract: Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08172v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Perreault, Katsumi Inoue, Richard Labib, Alain Hertz</dc:creator>
    </item>
    <item>
      <title>Mini-Batch Robustness Verification of Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2508.15454</link>
      <description>arXiv:2508.15454v2 Announce Type: replace-cross 
Abstract: Neural network image classifiers are ubiquitous in many safety-critical applications. However, they are susceptible to adversarial attacks. To understand their robustness to attacks, many local robustness verifiers have been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers introduce a long analysis time or lose too much precision, making them less effective for a large set of inputs. In this work, we propose a new approach to local robustness: group local robustness verification. The key idea is to leverage the similarity of the network computations of certain $\epsilon$-balls to reduce the overall analysis time. We propose BaVerLy, a sound and complete verifier that boosts the local robustness verification of a set of $\epsilon$-balls by dynamically constructing and verifying mini-batches. BaVerLy adaptively identifies successful mini-batch sizes, accordingly constructs mini-batches of $\epsilon$-balls that have similar network computations, and verifies them jointly. If a mini-batch is verified, all its $\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected as not being robust, guiding the refinement. BaVerLy leverages the analysis results to expedite the analysis of that $\epsilon$-ball as well as the analysis of the mini-batch with the other $\epsilon$-balls. We evaluate BaVerLy on fully connected and convolutional networks for MNIST and CIFAR-10. Results show that BaVerLy scales the common one by one verification by 2.3x on average and up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6 hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15454v2</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763150</arxiv:DOI>
      <dc:creator>Saar Tzour-Shaday, Dana Drachsler-Cohen</dc:creator>
    </item>
  </channel>
</rss>
