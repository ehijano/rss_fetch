<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Jul 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Third-Party Assessment of Mobile Performance in the 5G Era</title>
      <link>https://arxiv.org/abs/2507.18834</link>
      <description>arXiv:2507.18834v1 Announce Type: new 
Abstract: The web experience using mobile devices is important since a significant portion of the Internet traffic is initiated from mobile devices. In the era of 5G, users expect a high-performance data network to stream media content and for other latency-sensitive applications. In this paper, we characterize mobile experience in terms of latency, throughput, and stability measured from a commercial, globally-distributed CDN. Unlike prior work, CDN data provides a relatively neutral, carrier-agnostic perspective, providing a clear view of multiple and international providers. Our analysis of mobile client traffic shows mobile users sometimes experience markedly low latency, even as low as 6 ms. However, only the top 5% users regularly experience less than 20 ms of minimum latency. While 100 Mb/s throughput is not rare, we show around 60% users observe less than 50 Mb/s throughput. We find the minimum mobile latency is generally stable at a specific location which can be an important characteristic for anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18834v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>ASM Rizvi, John Heidemann, David Plonka</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks</title>
      <link>https://arxiv.org/abs/2507.19050</link>
      <description>arXiv:2507.19050v1 Announce Type: new 
Abstract: In this paper, we propose a general digital twin edge computing network comprising multiple vehicles and a server. Each vehicle generates multiple computing tasks within a time slot, leading to queuing challenges when offloading tasks to the server. The study investigates task offloading strategies, queue stability, and resource allocation. Lyapunov optimization is employed to transform long-term constraints into tractable short-term decisions. To solve the resulting problem, an in-context learning approach based on large language model (LLM) is adopted, replacing the conventional multi-agent reinforcement learning (MARL) framework. Experimental results demonstrate that the LLM-based method achieves comparable or even superior performance to MARL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19050v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiong Wu, Yu Xie, Pingyi Fan, Dong Qin, Kezhi Wang, Nan Cheng, Khaled B. Letaief</dc:creator>
    </item>
    <item>
      <title>iPLAN: Redefining Indoor Wireless Network Planning Through Large Language Models</title>
      <link>https://arxiv.org/abs/2507.19096</link>
      <description>arXiv:2507.19096v1 Announce Type: new 
Abstract: Efficient indoor wireless network (IWN) planning is crucial for providing high-quality 5G in-building services. However, traditional meta-heuristic and artificial intelligence-based planning methods face significant challenges due to the intricate interplay between indoor environments (IEs) and IWN demands. In this article, we present an indoor wireless network Planning with large LANguage models (iPLAN) framework, which integrates multi-modal IE representations into large language model (LLM)-powered optimizers to improve IWN planning. First, we instate the role of LLMs as optimizers, outlining embedding techniques for IEs, and introducing two core applications of iPLAN: (i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE for new wireless-friendly buildings. For the former, we embed essential information into LLM optimizers by leveraging indoor descriptions, domain-specific knowledge, and performance-driven perception. For the latter, we conceptualize a multi-agent strategy, where intelligent agents collaboratively address key planning sub-tasks in a step-by-step manner while ensuring optimal trade-offs between the agents. The simulation results demonstrate that iPLAN achieves superior performance in IWN planning tasks and optimizes building wireless performance through the joint design of IEs and IWNs, exemplifying a paradigm shift in IWN planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19096v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Hou, Stefanos Bakirtzis, Kehai Qiu, Sichong Liao, Hui Song, Haonan Hu, Kezhi Wang, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR</title>
      <link>https://arxiv.org/abs/2507.19124</link>
      <description>arXiv:2507.19124v1 Announce Type: new 
Abstract: Wireless support of virtual reality (VR) has challenges when a network has multiple users, particularly for 3D VR gaming, digital AI avatars, and remote team collaboration. This work addresses these challenges through investigation of the low-rank channels that inevitably occur when there are more active users than there are degrees of spatial freedom, effectively often the number of antennas. The presented approach uses optimal nonlinear transceivers, equivalently generalized decision-feedback or successive cancellation for uplink and superposition or dirty-paper precoders for downlink. Additionally, a powerful optimization approach for the users' energy allocation and decoding order appears to provide large improvements over existing methods, effectively nearing theoretical optima. As the latter optimization methods pose real-time challenges, approximations using deep reinforcement learning (DRL) are used to approximate best performance with much lower (5x at least) complexity. Experimental results show significantly larger sum rates and very large power savings to attain the data rates found necessary to support VR. Experimental results show the proposed algorithm outperforms current industry standards like orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as well as the highly researched methods in multi-carrier NOMA (MC-NOMA), enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power level. For the same data rate, it achieves power savings of 75%, 45%, and 40%, making it ideal for VR applications. Additionally, a near-optimal deep reinforcement learning (DRL)-based resource allocation framework for real-time use by being 5x faster and reaching 83% of the global optimum is introduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19124v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Ahmed Mohsin, Sagnik Bhattacharya, Abhiram Gorle, Muhammad Ali Jamshed, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV</title>
      <link>https://arxiv.org/abs/2507.19234</link>
      <description>arXiv:2507.19234v1 Announce Type: new 
Abstract: Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19234v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Leilei Ding, Wei Wu, Qilin Fan, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access Point Coordination</title>
      <link>https://arxiv.org/abs/2507.19377</link>
      <description>arXiv:2507.19377v1 Announce Type: new 
Abstract: Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn, with a potential impact on future Wi-Fi networks. MAPC enables joint scheduling decisions across multiple access points (APs) to improve throughput, latency, and reliability in dense Wi-Fi deployments. However, implementing efficient scheduling policies under diverse traffic and interference conditions in overlapping basic service sets (OBSSs) remains a complex task. This paper presents a method to minimize the network-wide worst-case latency by formulating MAPC scheduling as a sequential decision-making problem and proposing a deep reinforcement learning (DRL) mechanism to minimize worst-case delays in OBSS deployments. Specifically, we train a DRL agent using proximal policy optimization (PPO) within an 802.11bn-compatible Gymnasium environment. This environment provides observations of queue states, delay metrics, and channel conditions, enabling the agent to schedule multiple AP-station pairs to transmit simultaneously by leveraging spatial reuse (SR) groups. Simulations demonstrate that our proposed solution outperforms state-of-the-art heuristic strategies across a wide range of network loads and traffic patterns. The trained machine learning (ML) models consistently achieve lower 99th-percentile delays, showing up to a 30% improvement over the best baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19377v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Nunez, Francesc Wilhelmi, Maksymilian Wojnar, Katarzyna Kosek-Szott, Szymon Szott, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems</title>
      <link>https://arxiv.org/abs/2507.18889</link>
      <description>arXiv:2507.18889v1 Announce Type: cross 
Abstract: Increasingly large AI workloads are calling for hyper-scale infrastructure; however, traditional interconnection network architecture is neither scalable nor cost-effective enough. Tree-based topologies such as the \textit{Rail-optimized} network are extremely expensive, while direct topologies such as \textit{Torus} have insufficient bisection bandwidth and flexibility. In this paper, we propose \textit{RailX}, a reconfigurable network architecture based on intra-node direct connectivity and inter-node circuit switching. Nodes and optical switches are physically 2D-organized, achieving better scalability than existing centralized circuit switching networks. We propose a novel interconnection method based on \textit{Hamiltonian Decomposition} theory to organize separate rail-based rings into \textit{all-to-all} topology, simultaneously optimizing ring-collective and all-to-all communication. More than $100$K chips with hyper bandwidth can be interconnected with a flat switching layer, and the diameter is only $2\sim4$ inter-node hops. The network cost per injection/All-Reduce bandwidth of \textit{RailX} is less than $10\%$ of the Fat-Tree, and the cost per bisection/All-to-All bandwidth is less than $50\%$ of the Fat-Tree. Specifically, only $\sim$\$$1.3$B is required to interconnect 200K chips with 1.8TB bandwidth. \textit{RailX} can also be used in the ML-as-a-service (MLaaS) scenario, where single or multiple training workloads with various shapes, scales, and parallelism strategies can be flexibly mapped, and failures can be worked around.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18889v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiao Feng, Tiancheng Chen, Yuchen Wei, Siyuan Shen, Shiju Wang, Wei Li, Kaisheng Ma, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>A Formalization of the Correctness of the Floodsub Protocol</title>
      <link>https://arxiv.org/abs/2507.19013</link>
      <description>arXiv:2507.19013v1 Announce Type: cross 
Abstract: Floodsub is a simple, robust and popular peer-to-peer publish/subscribe (pubsub) protocol, where nodes can arbitrarily leave or join the network, subscribe to or unsubscribe from topics  and forward newly received messages to all of their neighbors, except the sender or the originating peer. To show the correctness of Floodsub, we propose its specification: Broadcastsub, in which implementation details like network connections and neighbor subscriptions are elided. To show that Floodsub does really implement Broadcastsub, one would have to show that the two systems have related infinite computations. We prove this by reasoning locally about states and their successors using Well-Founded Simulation (WFS). In this paper, we focus on the mechanization of a proof which shows that Floodsub is a simulation refinement of Broadcastsub using WFS. To the best of our knowledge, ours is the first mechanized refinement-based verification of a real world pubsub protocol. </description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19013v1</guid>
      <category>cs.LO</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.423.9</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 423, 2025, pp. 84-103</arxiv:journal_reference>
      <dc:creator>Ankit Kumar (Northeastern University), Panagiotis Manolios (Northeastern University)</dc:creator>
    </item>
    <item>
      <title>Virtual local area network over HTTP for launching an insider attack</title>
      <link>https://arxiv.org/abs/2507.19055</link>
      <description>arXiv:2507.19055v1 Announce Type: cross 
Abstract: Computers and computer networks have become integral to virtually every aspect of modern life, with the Internet playing an indispensable role. Organizations, businesses, and individuals now store vast amounts of proprietary, confidential, and personal data digitally. As such, ensuring the security of this data from unauthorized access is critical. Common security measures, such as firewalls, intrusion detection systems (IDS), intrusion prevention systems (IPS), and antivirus software, are constantly evolving to safeguard computer systems and networks. However, these tools primarily focus on defending against external threats, leaving systems vulnerable to insider attacks. Security solutions designed to mitigate risks originating from within the organization are relatively limited and often ineffective. This paper demonstrates how a Local Area Network (LAN) can be covertly exposed to the Internet via an insider attack. Specifically, it illustrates how an external machine can gain access to a LAN by exploiting an unused secondary IP address of the attacked LAN, effectively bypassing existing security mechanisms by also exploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust external protections, such as firewalls and IDS, this form of insider attack reveals significant vulnerabilities in the way internal threats are addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19055v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuksel Arslan</dc:creator>
    </item>
    <item>
      <title>High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins</title>
      <link>https://arxiv.org/abs/2507.19173</link>
      <description>arXiv:2507.19173v1 Announce Type: cross 
Abstract: The design of accurate Digital Twins (DTs) of electromagnetic environments strictly depends on the fidelity of the underlying environmental modeling. Evaluating the differences among diverse levels of modeling accuracy is key to determine the relevance of the model features towards both efficient and accurate DT simulations. In this paper, we propose two metrics, the Hausdorff ray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently compare the temporal, angular and power features between two ray tracing simulations performed on 3D scenarios featured by environmental changes. To evaluate the introduced metrics, we considered a high-fidelity digital twin model of an area of Milan, Italy and we enriched it with two different types of environmental changes: (i) the inclusion of parked vehicles meshes, and (ii) the segmentation of the buildings facade faces to separate the windows mesh components from the rest of the building. We performed grid-based and vehicular ray tracing simulations at 28 GHz carrier frequency on the obtained scenarios integrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular traffic simulator. Both the HRT and CRT metrics highlighted the areas of the scenarios where the simulated radio propagation features differ owing to the introduced mesh integrations, while the vehicular ray tracing simulations allowed to uncover the distance patterns arising along realistic vehicular trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19173v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Cazzella, Francesco Linsalata, Damiano Badini, Matteo Matteucci, Maurizio Magarini, Umberto Spagnolini</dc:creator>
    </item>
    <item>
      <title>Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators</title>
      <link>https://arxiv.org/abs/2507.19349</link>
      <description>arXiv:2507.19349v1 Announce Type: cross 
Abstract: In emerging communication systems such as sixth generation (6G) wireless networks, efficient resource management and service delivery rely on accurate knowledge of spatially-varying quantities like signal-to-interference-noise ratio (SINR) maps, which are costly to acquire at high resolution. This work explores the reconstruction of such spatial signals from sparse measurements using Group Equivariant Non-Expansive Operators (GENEOs), offering a low-complexity alternative to traditional neural networks. The concept of GENEO, which originated in topological data analysis (TDA), is a mathematical tool used in machine learning to represent agents modelled as functional operators acting on data while incorporating application-specific invariances. Leveraging these invariances reduces the number of parameters with respect to traditional neural networks and mitigates data scarcity by enforcing known algebraic and geometric constraints that reflect symmetries in the agents' actions. In this paper, we introduce a novel GENEO-based approach for SINR map reconstruction in urban wireless communication networks using extremely sparse sampling. We demonstrate that this mathematical framework achieves competitive performance compared to established methods. Our evaluation, conducted using both statistical and TDA metrics, highlights the advantages of our approach in accurately reconstructing spatial signals under severe data limitations on the number of samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19349v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lorenzo Mario Amorosa, Francesco Conti, Nicola Quercioli, Flavio Zabini, Tayebeh Lotfi Mahyari, Yiqun Ge, Patrizio Frosini</dc:creator>
    </item>
    <item>
      <title>Towards Constraint-aware Learning for Resource Allocation in NFV Networks</title>
      <link>https://arxiv.org/abs/2410.22999</link>
      <description>arXiv:2410.22999v2 Announce Type: replace 
Abstract: Virtual Network Embedding (VNE) is a fundamental resource allocation challenge that is associated with hard and multifaceted constraints in network function virtualization (NFV). Existing works for VNE struggle to handle such complex constraints, leading to compromised system performance and stability. In this paper, we propose a \textbf{CON}straint-\textbf{A}ware \textbf{L}earning framework, named \textbf{CONAL}, for efficient constraint handling in VNE. Concretely, we formulate the VNE problem as a constrained Markov decision process with violation tolerance, enabling precise assessments of both solution quality and constraint violations. To achieve the persistent zero violation to guarantee solutions' feasibility, we propose a reachability-guided optimization with an adaptive reachability budget method. This method also stabilizes policy optimization by appropriately handling scenarios with no feasible solutions. Furthermore, we propose a constraint-aware graph representation method to efficiently learn cross-graph relations and constrained path connectivity in VNE. Finally, extensive experimental results demonstrate the superiority of our proposed method over state-of-the-art baselines. Our code is available at \href{https://github.com/GeminiLight/conal-vne}{https://github.com/GeminiLight/conal-vne}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22999v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianfu Wang, Long Yang, Chao Wang, Chuan Qin, Liwei Deng, Wei Wu, Junyang Wang, Li Shen, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>HARNode: A Time-Synchronised, Open-Source, Multi-Device, Wearable System for Ad Hoc Field Studies</title>
      <link>https://arxiv.org/abs/2506.03219</link>
      <description>arXiv:2506.03219v2 Announce Type: replace 
Abstract: Human activity recognition (HAR) research often lacks accessible, comprehensive field data. Commercial systems are rarely open source, hard to expand, and limited by issues like node synchronisation, data throughput, unclear sensor placement, complexity, and high cost. As a result, researchers typically use only a few intuitively placed sensors and conduct limited field trials. HARNode overcomes these challenges with a fully open-source hardware and software platform. Each node includes an ESP32-S3 module (AtomS3), a 9-axis IMU (Bosch BMX160), pressure and temperature sensors (Bosch BMP388), a display, and an I2C port. Data is streamed via Wi-Fi, with NTP-based time synchronisation achieving roughly 1 ms accuracy. The system runs for up to 8 hours and is built using off-the-shelf parts, a simple online PCB service, and a compact 3D-printed housing with Velcro straps, enabling flexible and scalable body placement while requiring little hardware knowledge. In a study with ten subjects wearing eleven HARNodes each, setup took under five minutes per person. A random forest classifier distinguished walking from stair-climbing transitions, showing the benefits of sensor-overprovisioning: Seven nodes achieved approx. 98% accuracy, matching the performance of all eleven. These findings confirm HARNode's value as a fast-deploying, scalable tool for field-based HAR research and optimised sensor placement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03219v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Lepold, Tobias R\"oddiger, Michael Beigl</dc:creator>
    </item>
    <item>
      <title>RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents</title>
      <link>https://arxiv.org/abs/2507.13140</link>
      <description>arXiv:2507.13140v2 Announce Type: replace 
Abstract: Sixth generation (6G) networks demand tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality of service (QoS) and resource efficiency requirements. Existing solutions struggle to bridge the gap between high level user intents and the low level, parameterized configurations required for optimal performance. To address this challenge, we propose RIDAS, a multi agent framework composed of representation driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open interface with tunable control parameters (rank and quantization bits, enabling explicit trade) offs between distortion and transmission rate. The IDA employs a two stage planning scheme (bandwidth pre allocation and reallocation) driven by a large language model (LLM) to map user intents and system state into optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\% more users than WirelessAgent under equivalent QoS constraints. These results validate ability of RIDAS to capture user intent and allocate resources more efficiently in AI RAN environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13140v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuiyuan Ding, Caili Guo, Yang Yang, Jianzhang Guo</dc:creator>
    </item>
  </channel>
</rss>
