<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 02:08:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on Commercial DRAM-PIMs</title>
      <link>https://arxiv.org/abs/2410.15621</link>
      <description>arXiv:2410.15621v1 Announce Type: new 
Abstract: Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic similarity search in large datasets, has become a fundamental component of critical applications such as information retrieval and retrieval-augmented generation (RAG). However, ANNS is a well-known I/O-intensive algorithm with a low compute-to-I/O ratio, often requiring massive storage due to the large volume of high-dimensional data. This leads to I/O bottlenecks on CPUs and memory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM) architecture, which offers high bandwidth, large-capacity memory, and the ability to perform efficient computation in or near the data, presents a promising solution for ANNS. In this work, we investigate the use of commercial DRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS engine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM exhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup tables (LUTs) to replace more multiplications with I/O operations. We then systematically tune ANNS to search optimized configurations with lower computational load, aligning the compute-to-I/O ratio of ANNS with that of DRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS algorithm, we further explore implementation optimizations to fully utilize the two thousand parallel processing units with private local memory in DRAM-PIMs. To address the load imbalance caused by ANNS requests distributed across different clusters of large datasets, we propose a load-balancing strategy that combines static data layout optimization with dynamic runtime request scheduling. Experimental results on representative datasets show that DRIM-ANN achieves an average performance speedup of 2.92x compared to a 32-thread CPU counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15621v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingkai Chen, Tianhua Han, Cheng Liu, Shengwen Liang, Kuai Yu, Lei Dai, Ziming Yuan, Ying Wang, Lei Zhang, Huawei Li, Xiaowei Li</dc:creator>
    </item>
    <item>
      <title>Industry 4.0 Connectors -- A Performance Experiment with Modbus/TCP</title>
      <link>https://arxiv.org/abs/2410.15813</link>
      <description>arXiv:2410.15813v1 Announce Type: new 
Abstract: For Industry 4.0 applications, communication protocols and data formats even for legacy devices are fundamental. In this paper, we focus on the Modbus/TCP protocol, which is, e.g., used in energy metering. Allowing Industry 4.0 applications to include data from such protocols without need for programming would increase flexibility and, in turn, improve development efficiency. As one particular approach, we discuss the automated generation of Modbus/TCP connectors for our Open Source oktoflow platform and compare the performance of handcrafted as well as generated connectors in different settings, including industrial energy metering devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15813v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Nikolajew, Holger Eichelberger</dc:creator>
    </item>
    <item>
      <title>ADS Performance Revisited</title>
      <link>https://arxiv.org/abs/2410.15853</link>
      <description>arXiv:2410.15853v1 Announce Type: new 
Abstract: Real-time measurements are important for in-depth control of manufacturing processes, which, for modern AI methods, need integration with high-level languages. In our last SSP paper we investigated the performance of a Python and a Java-JNA based approach to integrate the Beckhoff ADS protocol for real-time edge communication into an Industry 4.0 platform. There, we have shown that while Java outperforms Python, both solutions do not meet the desired goal of 1-20kHz depending on the task. However, we are are still lacking an explanation for this result as well as an analysis of alternatives. For the first topic, we show in this paper that 1) exchanging Java-JNA with Java-JNI in this setting does not further improve the performance 2) a C++ program realizing the same behavior in a more direct integration does not perform better and 3) profiling shows that the majority of the execution is spend in ADS. For the second topic, we show that alternative uses of the ADS library allow for better performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15853v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexander Weber, Holger Eichelberger, Jobst Hildebrand</dc:creator>
    </item>
    <item>
      <title>Towards Safer Heuristics With XPlain</title>
      <link>https://arxiv.org/abs/2410.15086</link>
      <description>arXiv:2410.15086v1 Announce Type: cross 
Abstract: Many problems that cloud operators solve are computationally expensive, and operators often use heuristic algorithms (that are faster and scale better than optimal) to solve them more efficiently. Heuristic analyzers enable operators to find when and by how much their heuristics underperform. However, these tools do not provide enough detail for operators to mitigate the heuristic's impact in practice: they only discover a single input instance that causes the heuristic to underperform (and not the full set), and they do not explain why.
  We propose XPlain, a tool that extends these analyzers and helps operators understand when and why their heuristics underperform. We present promising initial results that show such an extension is viable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15086v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pantea Karimi, Solal Pirelli, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Beibin Li, Pooria Namyar, Behnaz Arzani</dc:creator>
    </item>
    <item>
      <title>The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for Effective Carbon-Aware Scheduling</title>
      <link>https://arxiv.org/abs/2410.15087</link>
      <description>arXiv:2410.15087v1 Announce Type: cross 
Abstract: The rapid increase in computing demand and its corresponding energy consumption have focused attention on computing's impact on the climate and sustainability. Prior work proposes metrics that quantify computing's carbon footprint across several lifecycle phases, including its supply chain, operation, and end-of-life. Industry uses these metrics to optimize the carbon footprint of manufacturing hardware and running computing applications. Unfortunately, prior work on optimizing datacenters' carbon footprint often succumbs to the \emph{sunk cost fallacy} by considering embodied carbon emissions (a sunk cost) when making operational decisions (i.e., job scheduling and placement), which leads to operational decisions that do not always reduce the total carbon footprint.
  In this paper, we evaluate carbon-aware job scheduling and placement on a given set of servers for a number of carbon accounting metrics. Our analysis reveals state-of-the-art carbon accounting metrics that include embodied carbon emissions when making operational decisions can actually increase the total carbon footprint of executing a set of jobs. We study the factors that affect the added carbon cost of such suboptimal decision-making. We then use a real-world case study from a datacenter to demonstrate how the sunk carbon fallacy manifests itself in practice. Finally, we discuss the implications of our findings in better guiding effective carbon-aware scheduling in on-premise and cloud datacenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15087v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698038.3698542</arxiv:DOI>
      <dc:creator>Noman Bashir, Varun Gohil, Anagha Belavadi, Mohammad Shahrad, David Irwin, Elsa Olivetti, Christina Delimitrou</dc:creator>
    </item>
    <item>
      <title>EPIC: Efficient Position-Independent Context Caching for Serving Large Language Models</title>
      <link>https://arxiv.org/abs/2410.15332</link>
      <description>arXiv:2410.15332v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are critical for a wide range of applications, but serving them efficiently becomes increasingly challenging as inputs become more complex. Context caching improves serving performance by exploiting inter-request dependency and reusing key-value (KV) cache across requests, thus improving time-to-first-token (TTFT). However, existing prefix-based context caching requires exact token prefix matches, limiting cache reuse in few-shot learning, multi-document QA, or retrieval-augmented generation, where prefixes may vary. In this paper, we present EPIC, an LLM serving system that introduces position-independent context caching (PIC), enabling modular KV cache reuse regardless of token chunk position (or prefix). EPIC features two key designs: AttnLink, which leverages static attention sparsity to minimize recomputation for accuracy recovery, and KVSplit, a customizable chunking method that preserves semantic coherence. Our experiments demonstrate that Epic delivers up to 8x improvements in TTFT and 7x throughput over existing systems, with negligible or no accuracy loss. By addressing the limitations of traditional caching approaches, Epic enables more scalable and efficient LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15332v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie</dc:creator>
    </item>
    <item>
      <title>Real-time Event Joining in Practice With Kafka and Flink</title>
      <link>https://arxiv.org/abs/2410.15533</link>
      <description>arXiv:2410.15533v1 Announce Type: cross 
Abstract: Historically, machine learning training pipelines have predominantly relied on batch training models, retraining models every few hours. However, industrial practitioners have proved that real-time training can lead to a more adaptive and personalized user experience. The transition from batch to real-time is full of tradeoffs to get the benefits of accuracy and freshness while keeping the costs low and having a predictable, maintainable system.
  Our work characterizes migrating to a streaming pipeline for a machine learning model using Apache Kafka and Flink. We demonstrate how to transition from Google Pub/Sub to Kafka to handle incoming real-time events and leverage Flink for streaming joins using RocksDB and checkpointing. We also address challenges such as managing causal dependencies between events, balancing event time versus processing time, and ensuring exactly-once versus at-least-once delivery guarantees, among other issues. Furthermore, we showcase how we improved scalability by using topic partitioning in Kafka, reduced event throughput by \textbf{85\%} through the use of Avro schema and compression, decreased costs by \textbf{40\%}, and implemented a separate pipeline to ensure data correctness. Our findings provide valuable insights into the tradeoffs and complexities of real-time systems, enabling better-informed decisions tailored to specific requirements for building effective streaming systems that enhance user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15533v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srijan Saket, Vivek Chandela, Md. Danish Kalim</dc:creator>
    </item>
    <item>
      <title>Final Report for CHESS: Cloud, High-Performance Computing, and Edge for Science and Security</title>
      <link>https://arxiv.org/abs/2410.16093</link>
      <description>arXiv:2410.16093v1 Announce Type: cross 
Abstract: Automating the theory-experiment cycle requires effective distributed workflows that utilize a computing continuum spanning lab instruments, edge sensors, computing resources at multiple facilities, data sets distributed across multiple information sources, and potentially cloud. Unfortunately, the obvious methods for constructing continuum platforms, orchestrating workflow tasks, and curating datasets over time fail to achieve scientific requirements for performance, energy, security, and reliability. Furthermore, achieving the best use of continuum resources depends upon the efficient composition and execution of workflow tasks, i.e., combinations of numerical solvers, data analytics, and machine learning. Pacific Northwest National Laboratory's LDRD "Cloud, High-Performance Computing (HPC), and Edge for Science and Security" (CHESS) has developed a set of interrelated capabilities for enabling distributed scientific workflows and curating datasets. This report describes the results and successes of CHESS from the perspective of open science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16093v1</guid>
      <category>cs.DC</category>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Tallent, Jan Strube, Luanzheng Guo, Hyungro Lee, Jesun Firoz, Sayan Ghosh, Bo Fang, Oceane Bel, Steven Spurgeon, Sarah Akers, Christina Doty, Erol Cromwell</dc:creator>
    </item>
    <item>
      <title>Asymptotic Optimality of the Speed-Aware Join-the-Shortest-Queue in the Halfin-Whitt Regime for Heterogeneous Systems</title>
      <link>https://arxiv.org/abs/2312.10497</link>
      <description>arXiv:2312.10497v3 Announce Type: replace-cross 
Abstract: The Join-the-Shortest-Queue (JSQ) load balancing scheme is known to minimise the average response time of jobs in homogeneous systems with identical servers. However, for {\em heterogeneous} systems with servers having different processing speeds, finding an optimal load balancing scheme remains an open problem for finite system sizes. Recently, for systems with heterogeneous servers, a variant of the JSQ scheme, called the {\em Speed-Aware-Join-the-Shortest-Queue (SA-JSQ)} scheme, has been shown to achieve asymptotic optimality in the fluid-scaling regime where the number of servers $n$ tends to infinity but the normalised the arrival rate of jobs remains constant. {In this paper, we show that the SA-JSQ scheme is also asymptotically optimal for heterogeneous systems in the {\em Halfin-Whitt} traffic regime where the normalised arrival rate scales as $1-O(1/\sqrt{n})$.} Our analysis begins by establishing that an appropriately scaled and centered version of the Markov process describing system dynamics weakly converges to a two-dimensional reflected {\em Ornstein-Uhlenbeck (OU) process}. We then show using {\em Stein's method} that the stationary distribution of the underlying Markov process converges to that of the OU process as the system size increases by establishing the validity of interchange of limits. {Finally, through coupling with a suitably constructed system, we show that SA-JSQ asymptotically minimises the diffusion-scaled total number of jobs and the diffusion-scaled number of waiting jobs in the steady-state in the Halfin-Whitt regime among all policies which dispatch jobs based on queue lengths and server speeds.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10497v3</guid>
      <category>math.PR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sanidhay Bhambay, Burak B\"uke, Arpan Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>QOPS: A Compiler Framework for Quantum Circuit Simulation Acceleration with Profile Guided Optimizations</title>
      <link>https://arxiv.org/abs/2410.09326</link>
      <description>arXiv:2410.09326v2 Announce Type: replace-cross 
Abstract: Quantum circuit simulation is important in the evolution of quantum software and hardware. Novel algorithms can be developed and evaluated by performing quantum circuit simulations on classical computers before physical quantum computers are available. Unfortunately, compared with a physical quantum computer, a prolonged simulation time hampers the rapid development of quantum algorithms. Inspired by the feedback-directed optimization scheme used by classical compilers to improve the generated code, this work proposes a quantum compiler framework QOPS to enable profile-guided optimization (PGO) for quantum circuit simulation acceleration. The QOPS compiler instruments a quantum simulator to collect performance data during the circuit simulation and it then generates the optimized version of the quantum circuit based on the collected data. Experimental results show the PGO can effectively shorten the simulation time on our tested benchmark programs. Especially, the simulator-specific PGO (virtual swap) can be applied to the benchmarks to accelerate the simulation speed by a factor of 1.19. As for the hardware-independent PGO, compared with the brute force mechanism (turning on all available compilation flags), which achieves 21% performance improvement against the non-optimized version, the PGO can achieve 16% speedup with a factor of 63 less compilation time than the brute force approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09326v2</guid>
      <category>quant-ph</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu-Tsung Wu, Po-Hsuan Huang, Kai-Chieh Chang, Chia-Heng Tu, Shih-Hao Hung</dc:creator>
    </item>
  </channel>
</rss>
