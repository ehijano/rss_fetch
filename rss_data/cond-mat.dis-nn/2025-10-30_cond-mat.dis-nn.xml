<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Grokking in the Ising Model</title>
      <link>https://arxiv.org/abs/2510.25966</link>
      <description>arXiv:2510.25966v1 Announce Type: new 
Abstract: Delayed generalization, termed grokking, in a machine learning calculation occurs when the training accuracy approaches its maximum value long before the test accuracy. This paper examines grokking in the context of a neural network trained to classify 2D Ising model configurations.. We find, partially with the aid of novel PCA-based network layer analysis techniques, that the grokking behavior can be qualitatively interpreted as a phase transition in the neural network in which the fully connected network transforms into a relatively sparse subnetwork. This in turn reduces the confusion associated with a multiplicity of paths. The network can then identify the common features of the input classes and hence generalize to the recognition of previously unseen patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25966v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karolina Hutchison, David Yevick</dc:creator>
    </item>
    <item>
      <title>Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability</title>
      <link>https://arxiv.org/abs/2510.26792</link>
      <description>arXiv:2510.26792v1 Announce Type: cross 
Abstract: We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26792v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CR</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Tao, Maissam Barkeshli</dc:creator>
    </item>
    <item>
      <title>Is Grokking a Computational Glass Relaxation?</title>
      <link>https://arxiv.org/abs/2505.11411</link>
      <description>arXiv:2505.11411v2 Announce Type: replace-cross 
Abstract: Understanding neural network's (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches a near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11411v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaotian Zhang, Yue Shang, Entao Yang, Ge Zhang</dc:creator>
    </item>
    <item>
      <title>Low dimensional dynamics of a sparse balanced synaptic network of quadratic integrate-and-fire neurons</title>
      <link>https://arxiv.org/abs/2508.06253</link>
      <description>arXiv:2508.06253v2 Announce Type: replace-cross 
Abstract: Kinetics of a balanced network of neurons with a sparse grid of synaptic links is well representable by the stochastic dynamics of a generic neuron subject to an effective shot noise. The rate of delta-pulses of the noise is determined self-consistently from the probability density of the neuron states. Importantly, the most sophisticated (but robust) collective regimes of the network do not allow for the diffusion approximation, which is routinely adopted for a shot noise in mathematical neuroscience. These regimes can be expected to be biologically relevant. For the kinetics equations of the complete mean field theory of a homogeneous inhibitory network of quadratic integrate-and-fire neurons, we introduce circular cumulants of the genuine phase variable and derive a rigorous two cumulant reduction for both time-independent conditions and modulation of the excitatory current. The low dimensional model is examined with numerical simulations and found to be accurate for time-independent states and dynamic response to a periodic modulation deep into the parameter domain where the diffusion approximation is not applicable. The accuracy of a low dimensional model indicates and explains a low embedding dimensionality of the macroscopic collective dynamics of the network. The reduced model can be instrumental for theoretical studies of inhibitory-excitatory balanced neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06253v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria V. Ageeva, Denis S. Goldobin</dc:creator>
    </item>
    <item>
      <title>Renormalization of Interacting Random Graph Models</title>
      <link>https://arxiv.org/abs/2510.07186</link>
      <description>arXiv:2510.07186v2 Announce Type: replace-cross 
Abstract: Random graphs offer a useful mathematical representation of a variety of real world complex networks. Exponential random graphs, for example, are particularly suited towards generating random graphs constrained to have specified statistical moments. In this investigation, we elaborate on a generalization of the former where link probabilities are conditioned on the appearance of other links, corresponding to the introduction of interactions in an effective generalized statistical mechanical formalism. When restricted to the simplest non-trivial case of pairwise interactions, one can derive a closed form renormalization group transformation for maximum coordination number two on the corresponding line graph. Higher coordination numbers do not admit exact closed form renormalization group transformations, a feature that paraphrases the usual absence of exact transformations in two or more dimensional lattice systems. We introduce disorder and study the induced renormalization group flow on its probability assignments, highlighting its formal equivalence to time reversed anisotropic drift-diffusion on the statistical manifold associated with the effective Hamiltonian. We discuss the implications of our findings, stressing the long wavelength irrelevance of certain classes of pair-wise conditioning on random graphs, and conclude with possible applications. These include modeling the scaling behavior of preferential effects on social networks, opinion dynamics, and reinforcement effects on neural networks, as well as how our findings offer a systematic framework to deal with data limitations in inference and reconstruction problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07186v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>hep-th</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessio Catanzaro, Diego Garlaschelli, Subodh P. Patil</dc:creator>
    </item>
  </channel>
</rss>
