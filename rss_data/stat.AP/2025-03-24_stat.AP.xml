<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Mar 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ensemble Survival Analysis for Preclinical Cognitive Decline Prediction in Alzheimer's Disease Using Longitudinal Biomarkers</title>
      <link>https://arxiv.org/abs/2503.16645</link>
      <description>arXiv:2503.16645v1 Announce Type: new 
Abstract: Predicting the risk of clinical progression from cognitively normal (CN) status to mild cognitive impairment (MCI) or Alzheimer's disease (AD) is critical for early intervention in Alzheimer's disease (AD). Traditional survival models often fail to capture complex longitudinal biomarker patterns associated with disease progression. We propose an ensemble survival analysis framework integrating multiple survival models to improve early prediction of clinical progression in initially cognitively normal individuals. We analyzed longitudinal biomarker data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort, including 721 participants, limiting analysis to up to three visits (baseline, 6-month follow-up, 12-month follow-up). Of these, 142 (19.7%) experienced clinical progression to MCI or AD. Our approach combined penalized Cox regression (LASSO, Elastic Net) with advanced survival models (Random Survival Forest, DeepSurv, XGBoost). Model predictions were aggregated using ensemble averaging and Bayesian Model Averaging (BMA). Predictive performance was assessed using Harrell's concordance index (C-index) and time-dependent area under the curve (AUC). The ensemble model achieved a peak C-index of 0.907 and an integrated time-dependent AUC of 0.904, outperforming baseline-only models (C-index 0.608). One follow-up visit after baseline significantly improved prediction accuracy (48.1% C-index, 48.2% AUC gains), while adding a second follow-up provided only marginal gains (2.1% C-index, 2.7% AUC). Our ensemble survival framework effectively integrates diverse survival models and aggregation techniques to enhance early prediction of preclinical AD progression. These findings highlight the importance of leveraging longitudinal biomarker data, particularly one follow-up visit, for accurate risk stratification and personalized intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16645v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Samhita Pal, Michael Lutz, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Spatial-temporal models for forest inventory data</title>
      <link>https://arxiv.org/abs/2503.16691</link>
      <description>arXiv:2503.16691v1 Announce Type: new 
Abstract: The USDA Forest Inventory and Analysis (FIA) program conducts a national forest inventory for the United States through a network of permanent field plots. FIA produces estimates of area averages/totals for plot-measured forest variables through design-based inference, assuming a fixed population and a probability sample of field plot locations. The fixed-population assumption and characteristics of the FIA sampling scheme make it difficult to estimate change in forest variables over time using design-based inference. We propose spatial-temporal models based on Gaussian processes as a flexible tool for forest inventory data, capable of inferring forest variables and change thereof over arbitrary spatial and temporal domains. It is shown to be beneficial for the covariance function governing the latent Gaussian process to account for variation at multiple scales, separating spatially local variation from ecosystem-scale variation. We demonstrate a model for forest biomass density, inferring 20 years of biomass change within two US National Forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16691v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul B. May, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Addressing complex structures of measurement error arising in the exposure assessment in occupational epidemiology using a Bayesian hierarchical approach</title>
      <link>https://arxiv.org/abs/2503.17161</link>
      <description>arXiv:2503.17161v1 Announce Type: new 
Abstract: Exposure assessment in occupational epidemiology may involve multiple unknown quantities that are measured or reconstructed simultaneously for groups of workers and over several years. Additionally, exposures may be collected using different assessment strategies, depending on the period of exposure. As a consequence, researchers who are analyzing occupational cohort studies are commonly faced with challenging structures of exposure measurement error, involving complex dependence structures and multiple measurement error models, depending on the period of exposure. However, previous work has often made many simplifying assumptions concerning these errors. In this work, we propose a Bayesian hierarchical approach to account for a broad range of error structures arising in occupational epidemiology. The considered error structures may involve several unknown quantities that can be subject to mixtures of Berkson and classical measurement error. It is possible to account for different error structures, depending on the exposure period and the location of a worker. Moreover, errors can present complex dependence structures over time and between workers. We illustrate the proposed hierarchical approach on a subgroup of the German cohort of uranium miners to account for potential exposure uncertainties in the association between radon exposure and lung cancer mortality. The performance of the proposed approach and its sensitivity to model misspecification are evaluated in a simulation study. The results show that biases in estimates arising from very complex measurement errors can be corrected through the proposed Bayesian hierarchical approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17161v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Rehms, Nicole Ellenbach, Veronika Deffner, Sabine Hoffmann</dc:creator>
    </item>
    <item>
      <title>PieGlyph: An R package for creating axis invariant pie-glyphs for 2d plots</title>
      <link>https://arxiv.org/abs/2503.16478</link>
      <description>arXiv:2503.16478v1 Announce Type: cross 
Abstract: Effective visualisation of multidimensional data is crucial for generating insights. Glyph-based visualisations, which encode data dimensions onto multiple visual channels such as colour, shape, and size, provide an effective means of representing complex datasets. Pie-chart glyphs (pie-glyphs) are one such approach, where multiple data attributes are mapped to slices within a pie chart. This paper introduces the PieGlyph R package, which enables users to overlay any 2D plot with axis-invariant pie-glyphs, offering a compact and intuitive representation of multidimensional data. Unlike existing R packages such as scatterpie or ggforce, PieGlyph generates pie-glyphs independently of the plot axes by employing a nested coordinate system, ensuring they remain circular regardless of changes to the underlying coordinate system. This enhances interpretability, particularly in when visualising spatial data, as users can select the most appropriate map projection without distorting the glyphs' shape. Pie-glyphs are also particularly well-suited for visualising compositional data, where there is a natural sum-to-one constraint on the data attributes. PieGlyph is developed under the Grammar of Graphics paradigm using the ggplot2 framework and supports the generation of interactive pie-glyphs through the ggiraph package. Designed to integrate seamlessly with all features and extensions offered by ggplot2 and ggiraph, PieGlyph provides users with full flexibility in customising every aspect of the visualisation. This paper outlines the conceptual framework of PieGlyph, compares it with existing alternatives, and demonstrates its applications through example visualisations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16478v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rishabh Vishwakarma, Caroline Brophy, Catherine Hurley</dc:creator>
    </item>
    <item>
      <title>Procrustes Wasserstein Metric: A Modified Benamou-Brenier Approach with Applications to Latent Gaussian Distributions</title>
      <link>https://arxiv.org/abs/2503.16580</link>
      <description>arXiv:2503.16580v1 Announce Type: cross 
Abstract: We introduce a modified Benamou-Brenier type approach leading to a Wasserstein type distance that allows global invariance, specifically, isometries, and we show that the problem can be summarized to orthogonal transformations. This distance is defined by penalizing the action with a costless movement of the particle that does not change the direction and speed of its trajectory. We show that for Gaussian distribution resume to measuring the Euclidean distance between their ordered vector of eigenvalues and we show a direct application in recovering Latent Gaussian distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16580v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevine Meugang Toukam</dc:creator>
    </item>
    <item>
      <title>Modeling and forecasting subnational age distribution of death counts</title>
      <link>https://arxiv.org/abs/2503.16744</link>
      <description>arXiv:2503.16744v1 Announce Type: cross 
Abstract: This paper presents several forecasting methods to model and forecast subnational age distribution of death counts. The age distribution of death counts has many similarities to probability density functions, which are nonnegative and have a constrained integral, and thus live in a constrained nonlinear space. To address the nonlinear nature of objects, we implement a cumulative distribution function transformation that has an additional monotonicity. Using the Japanese subnational life-table death counts obtained from the Japanese Mortality Database (2025), we evaluate the forecast accuracy of the transformation and forecasting methods. The improved forecast accuracy of life-table death counts implemented here will be of great interest to demographers in estimating regional age-specific survival probabilities and life expectancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16744v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Cristian F. Jim\'enez-Var\'on</dc:creator>
    </item>
    <item>
      <title>A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations</title>
      <link>https://arxiv.org/abs/2503.17117</link>
      <description>arXiv:2503.17117v1 Announce Type: cross 
Abstract: The search for exoplanets is an active field in astronomy, with direct imaging as one of the most challenging methods due to faint exoplanet signals buried within stronger residual starlight. Successful detection requires advanced image processing to separate the exoplanet signal from this nuisance component. This paper presents a novel statistical model that captures nuisance fluctuations using a multi-scale approach, leveraging problem symmetries and a joint spectral channel representation grounded in physical principles. Our model integrates into an interpretable, end-to-end learnable framework for simultaneous exoplanet detection and flux estimation. The proposed algorithm is evaluated against the state of the art using datasets from the SPHERE instrument operating at the Very Large Telescope (VLT). It significantly improves the precision-recall trade-off, notably on challenging datasets that are otherwise unusable by astronomers. The proposed approach is computationally efficient, robust to varying data quality, and well suited for large-scale observational surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17117v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.EP</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Bodrito, Olivier Flasseur, Julien Mairal, Jean Ponce, Maud Langlois, Anne-Marie Lagrange</dc:creator>
    </item>
    <item>
      <title>ML-Based Bidding Price Prediction for Pay-As-Bid Ancillary Services Markets: A Use Case in the German Control Reserve Market</title>
      <link>https://arxiv.org/abs/2503.17214</link>
      <description>arXiv:2503.17214v1 Announce Type: cross 
Abstract: The increasing integration of renewable energy sources has led to greater volatility and unpredictability in electricity generation, posing challenges to grid stability. Ancillary service markets, such as the German control reserve market, allow industrial consumers and producers to offer flexibility in their power consumption or generation, contributing to grid stability while earning additional income. However, many participants use simple bidding strategies that may not maximize their revenues. This paper presents a methodology for forecasting bidding prices in pay-as-bid ancillary service markets, focusing on the German control reserve market. We evaluate various machine learning models, including Support Vector Regression, Decision Trees, and k-Nearest Neighbors, and compare their performance against benchmark models. To address the asymmetry in the revenue function of pay-as-bid markets, we introduce an offset adjustment technique that enhances the practical applicability of the forecasting models. Our analysis demonstrates that the proposed approach improves potential revenues by 27.43 % to 37.31 % compared to baseline models. When analyzing the relationship between the model forecasting errors and the revenue, a negative correlation is measured for three markets; according to the results, a reduction of 1 EUR/MW model price forecasting error (MAE) statistically leads to a yearly revenue increase between 483 EUR/MW and 3,631 EUR/MW. The proposed methodology enables industrial participants to optimize their bidding strategies, leading to increased earnings and contributing to the efficiency and stability of the electrical grid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17214v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Bezold, Lukas Baur, Alexander Sauer</dc:creator>
    </item>
    <item>
      <title>Leveraging statistical models to improve pre-season forecasting and in-season management of a recreational fishery</title>
      <link>https://arxiv.org/abs/2503.17293</link>
      <description>arXiv:2503.17293v1 Announce Type: cross 
Abstract: Effective management of recreational fisheries requires accurate forecasting of future harvests and real-time monitoring of ongoing harvests. Traditional methods that rely on historical catch data to predict short-term harvests can be unreliable, particularly if changes in management regulations alter angler behavior. In contrast, statistical modeling approaches can provide faster, more flexible, and potentially more accurate predictions, enhancing management outcomes. In this study, we developed and tested models to improve predictions of Gulf of Mexico gag harvests for both pre-season planning and in-season monitoring. Our best-fitting model outperformed traditional methods (i.e., estimates derived from historical average harvest) for both cumulative pre-season projections and in-season monitoring. Notably, our modeling framework appeared to be more accurate in more recent, shorter seasons due to its ability to account for effort compression. A key advantage of our framework is its ability to explicitly quantify the probability of exceeding harvest quotas for any given season duration. This feature enables managers to evaluate trade-offs between season duration and conservation goals. This is especially critical for vulnerable, highly targeted stocks. Our findings also underscore the value of statistical models to complement and advance traditional fisheries management approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17293v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Challen Hyman, Chloe Ramsay, Tiffanie A. Cross, Beverly Sauls, Thomas K. Frazer</dc:creator>
    </item>
    <item>
      <title>A barycenter-based approach for the multi-model ensembling of subseasonal forecasts</title>
      <link>https://arxiv.org/abs/2310.17933</link>
      <description>arXiv:2310.17933v2 Announce Type: replace 
Abstract: Ensemble forecasts and their combination are examined from the perspective of probability spaces. Manipulating ensemble forecasts as discrete probability distributions, multi-model ensemble (MME) forecasts are reformulated as barycenters of these distributions. We consider two barycenters, each defined with respect to a different distance metric: the L2 barycenter, which correspond to the traditional pooling method, and the Wasserstein barycenter, which better preserves certain geometric properties of the input ensemble distributions.
  As a proof of concept, we apply the L2 and Wasserstein barycenters to the combination of four models from the Subseasonal to Seasonal (S2S) prediction project database. Their performance is evaluated for the prediction of weekly 2m temperature, 10m wind speed, and 500hPa geopotential height over European winters. By construction, both barycenter-based MMEs have the same ensemble mean, but differ in their representation of the forecast uncertainty. Notably, the L2 barycenter has a larger ensemble spread, making it more prone to under-confidence. While both methods perform similarly on average in terms of the Continuous Ranked Probability Score (CRPS), the Wasserstein barycenter performs better more frequently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17933v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camille Le Coz, Alexis Tantet, R\'emi Flamary, Riwal Plougonven</dc:creator>
    </item>
    <item>
      <title>Elementary methods provide more replicable results in microbial differential abundance analysis</title>
      <link>https://arxiv.org/abs/2404.02691</link>
      <description>arXiv:2404.02691v3 Announce Type: replace 
Abstract: Differential abundance analysis is a key component of microbiome studies. Although dozens of methods exist there is currently no consensus on the preferred methods. While the correctness of results in differential abundance analysis is an ambiguous concept and cannot be fully evaluated without setting the ground truth and employing simulated data, we argue that a well-performing method should be effective in producing highly reproducible results.
  We compared the performance of 14 differential abundance analysis methods by employing datasets from 53 taxonomic profiling studies based on 16S rRNA gene or shotgun metagenomic sequencing. For each method, we examined how the results replicated between random partitions of each dataset and between datasets from separate studies. While certain methods showed good consistency, some widely used methods were observed to produce a substantial number of conflicting findings. Overall, when considering consistency together with sensitivity, the best performance was attained by analyzing relative abundances with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression/t-test. Moreover, a comparable performance was obtained by analyzing presence/absence of taxa with logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02691v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/bib/bbaf130</arxiv:DOI>
      <dc:creator>Juho Pelto, Kari Auranen, Janne Kujala, Leo Lahti</dc:creator>
    </item>
    <item>
      <title>Breaking the Balance: Asymmetric Negative Voting in the 2020 Presidential Election</title>
      <link>https://arxiv.org/abs/2502.16081</link>
      <description>arXiv:2502.16081v2 Announce Type: replace 
Abstract: While voters from opposing parties have traditionally exhibited symmetric levels of hostility toward out-party candidates, our analysis of the 2016 and 2020 Nationscape data reveals a notable departure from this pattern. In 2016, negative voting was relatively balanced, with similar levels of hostility directed at Hillary Clinton and Donald Trump. However, by 2020, asymmetric negative voting had emerged. As an incumbent seeking re-election amid a rapidly declining economy, the COVID-19 pandemic, and widespread uncertainty, Trump faced heightened negative perceptions fueled by dissatisfaction with his handling of the economy, race relations, the pandemic, and his leadership style. These factors galvanized younger, educated Democrats and Independents to vote against him in unprecedented numbers. In contrast, Republicans expressed less animosity toward Biden in 2020 than they had toward Clinton in 2016. This shift disrupted the balance in the typical pattern of symmetric negative voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16081v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng</dc:creator>
    </item>
    <item>
      <title>Using Mathlink Cubes to Introduce Data Wrangling with Examples in R</title>
      <link>https://arxiv.org/abs/2402.07029</link>
      <description>arXiv:2402.07029v2 Announce Type: replace-cross 
Abstract: This paper explores an innovative approach to teaching data wrangling skills to students through hands-on activities before transitioning to coding. Data wrangling, a critical aspect of data analysis, involves cleaning, transforming, and restructuring data. We introduce the use of a physical tool, mathlink cubes, to facilitate a tangible understanding of data sets. This approach helps students grasp the concepts of data wrangling before implementing them in coding languages such as R. We detail a classroom activity that includes hands-on tasks paralleling common data wrangling processes such as filtering, selecting, and mutating, followed by their coding equivalents using R's `dplyr` package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07029v2</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Affective Polarization Amongst Swedish Politicians</title>
      <link>https://arxiv.org/abs/2503.16193</link>
      <description>arXiv:2503.16193v2 Announce Type: replace-cross 
Abstract: This study investigates affective polarization among Swedish politicians on Twitter from 2021 to 2023, including the September 2022 parliamentary election. Analyzing over 25,000 tweets and employing large language models (LLMs) for sentiment and political classification, we distinguish between positive partisanship (support of allies) and negative partisanship (criticism of opponents).
  Our findings are contingent on the definition of the in-group. When political in-groups are defined at the ideological bloc level, negative and positive partisanship occur at similar rates. However, when the in-group is defined at the party level, negative partisanship becomes significantly more dominant and is 1.51 times more likely (1.45, 1.58). This effect is even stronger among extreme politicians, who engage in negativity more than their moderate counterparts. Negative partisanship also proves to be a strategic choice for online visibility, attracting 3.18 more likes and 1.69 more retweets on average.
  By adapting methods developed for two-party systems and leveraging LLMs for Swedish-language analysis, we provide novel insights into how multiparty politics shapes polarizing discourse. Our results underscore both the strategic appeal of negativity in digital spaces and the growing potential of LLMs for large-scale, non-English political research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16193v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois t'Serstevens, Roberto Cerina, Gustav Peper</dc:creator>
    </item>
  </channel>
</rss>
