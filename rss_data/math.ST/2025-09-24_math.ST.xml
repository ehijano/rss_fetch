<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:45:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving Cram\'er-Rao Bound With Multivariate Parameters: An Extrinsic Geometry Perspective</title>
      <link>https://arxiv.org/abs/2509.18978</link>
      <description>arXiv:2509.18978v1 Announce Type: new 
Abstract: We derive a vector generalization of the square root embedding-based curvature-corrected Cram\'er--Rao bound (CRB) previously considered by the same author in \cite{srk} with scalar parameters. A \emph{directional} curvature correction is established first, and sufficient conditions for a conservative matrix-level CRB refinement are formulated using a simple semidefinite program. The directional correction theorem is rigorously illustrated with a Gaussian example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18978v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunder Ram Krishnan</dc:creator>
    </item>
    <item>
      <title>Markov Combinations of Discrete Statistical Models</title>
      <link>https://arxiv.org/abs/2509.18983</link>
      <description>arXiv:2509.18983v1 Announce Type: new 
Abstract: Markov combination is an operation that takes two statistical models and produces a third whose marginal distributions include those of the original models. Building upon and extending existing work in the Gaussian case, we develop Markov combinations for categorical variables and their statistical models. We present several variants of this operation, both algorithmically and from a sampling perspective, and discuss relevant examples and theoretical properties. We describe Markov combinations for special models such as regular exponential families, discrete copulas, and staged trees. Finally, we offer results about model invariance and the maximum likelihood estimation of Markov combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18983v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Orlando Marigliano, Eva Riccomagno</dc:creator>
    </item>
    <item>
      <title>Optimization-centric cutting feedback for semiparametric models</title>
      <link>https://arxiv.org/abs/2509.18708</link>
      <description>arXiv:2509.18708v1 Announce Type: cross 
Abstract: Modern statistics deals with complex models from which the joint model used for inference is built by coupling submodels, called modules. We consider modular inference where the modules may depend on parametric and nonparametric components. In such cases, a joint Bayesian inference is highly susceptible to misspecification across any module, and inappropriate priors for nonparametric components may deliver subpar inferences for parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. The proposed generalized cut posteriors are defined through a variational optimization problem for generalized posteriors where regularization is based on R\'{e}nyi divergence, rather than Kullback-Leibler divergence (KLD), and variational computational methods are developed. We show empirically that using R\'{e}nyi divergence to define the cut posterior delivers more robust inferences than KLD. We derive novel posterior concentration results that accommodate the R\'{e}nyi divergence and allow for semiparametric components, greatly extending existing results for cut posteriors that were derived for parametric models and KLD. We demonstrate these new methods in a benchmark toy example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18708v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda S. L. Tan, David J. Nott, David T. Frazier</dc:creator>
    </item>
    <item>
      <title>Optimal estimation for regression discontinuity design with binary outcomes</title>
      <link>https://arxiv.org/abs/2509.18857</link>
      <description>arXiv:2509.18857v1 Announce Type: cross 
Abstract: We develop a finite-sample optimal estimator for regression discontinuity designs when the outcomes are bounded, including binary outcomes as the leading case. Our finite-sample optimal estimator achieves the exact minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function of a bounded outcome lies in a Lipschitz class. Although the original minimax problem involves an iterating (n+1)-dimensional non-convex optimization problem where n is the sample size, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of our estimator is that the Lipschitz constant is the only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than conventional large-sample techniques which may be unreliable when the effective sample size is small. We apply our method to an empirical multi-cutoff design where the sample size for each cutoff is small. In the application, our method yields informative confidence intervals, in contrast to the leading large-sample approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18857v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Masayuki Sawada, Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting</title>
      <link>https://arxiv.org/abs/2509.18887</link>
      <description>arXiv:2509.18887v1 Announce Type: cross 
Abstract: The System Price (SP) of the Nordic electricity market serves as a key reference for financial hedge contracts such as Electricity Price Area Differentials (EPADs) and other risk management instruments. Therefore, the identification of drivers and the accurate forecasting of SP are essential for market participants to design effective hedging strategies. This paper develops a systematic framework that combines interpretable drivers analysis with robust forecasting methods. It proposes an interpretable feature engineering algorithm to identify the main drivers of the Nordic SP based on a novel combination of K-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal Autoregressive Integrated Moving Average (SARIMA) model. Then, it applies principal component analysis (PCA) to the identified data matrix, which is adapted to the downstream task of price forecasting to mitigate the issue of imperfect multicollinearity in the data. Finally, we propose a multi-forecast selection-shrinkage algorithm for Nordic SP forecasting, which selects a subset of complementary forecast models based on their bias-variance tradeoff at the ensemble level and then computes the optimal weights for the retained forecast models to minimize the error variance of the combined forecast. Using historical data from the Nordic electricity market, we demonstrate that the proposed approach outperforms individual input models uniformly, robustly, and significantly, while maintaining a comparable computational cost. Notably, our systematic framework produces superior results using simple input models, outperforming the state-of-the-art Temporal Fusion Transformer (TFT). Furthermore, we show that our approach also exceeds the performance of several well-established practical forecast combination methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18887v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yousef Adeli Sadabad, Mohammad Reza Hesamzadeh, Gyorgy Dan, Matin Bagherpour, Darryl R. Biggar</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning</title>
      <link>https://arxiv.org/abs/2509.19098</link>
      <description>arXiv:2509.19098v1 Announce Type: cross 
Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning setting: before any pulls, the learner is given N'_k i.i.d. samples from each source distribution nu'_k, and the true target distributions nu_k lie within a known distance bound d_k(nu_k, nu'_k) &lt;= L_k. In this framework, we first derive a problem-dependent asymptotic lower bound on cumulative regret that extends the classical Lai-Robbins result to incorporate the transfer parameters (d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that matches this new bound in the Gaussian case. Finally, we validate our approach via simulations, showing that KL-UCB-Transfer significantly outperforms the no-prior baseline when source and target distributions are sufficiently close.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19098v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrien Prevost, Timothee Mathieu, Odalric-Ambrym Maillard</dc:creator>
    </item>
    <item>
      <title>Sharp Large Deviations and Gibbs Conditioning for Threshold Models in Portfolio Credit Risk</title>
      <link>https://arxiv.org/abs/2509.19151</link>
      <description>arXiv:2509.19151v1 Announce Type: cross 
Abstract: We obtain sharp large deviation estimates for exceedance probabilities in dependent triangular array threshold models with a diverging number of latent factors. The prefactors quantify how latent-factor dependence and tail geometry enter at leading order, yielding three regimes: Gaussian or exponential-power tails produce polylogarithmic refinements of the Bahadur-Rao $n^{-1/2}$ law; regularly varying tails yield index-driven polynomial scaling; and bounded-support (endpoint) cases lead to an $n^{-3/2}$ prefactor. We derive these results through Laplace-Olver asymptotics for exponential integrals and conditional Bahadur-Rao estimates for the triangular arrays. Using these estimates, we establish a Gibbs conditioning principle in total variation: conditioned on a large exceedance event, the default indicators become asymptotically i.i.d., and the loss-given-default distribution is exponentially tilted (with the boundary case handled by an endpoint analysis). As illustrations, we obtain second-order approximations for Value-at-Risk and Expected Shortfall, clarifying when portfolios operate in the genuine large-deviation regime. The results provide a transferable set of techniques-localization, curvature, and tilt identification-for sharp rare-event analysis in dependent threshold systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19151v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengnan Deng, Anand N. Vidyashankar, Jeffrey F. Collamore</dc:creator>
    </item>
    <item>
      <title>On the Performance of THz Wireless Systems over $\alpha$-$\mathcal{F}$ Channels with Beam Misalignment and Mobility</title>
      <link>https://arxiv.org/abs/2509.19235</link>
      <description>arXiv:2509.19235v1 Announce Type: cross 
Abstract: This paper investigates the performance of terahertz~(THz) wireless systems over the $\alpha$-$\mathcal{F}$ fading channels with beam misalignment and mobility. New expressions are derived for the probability density, cumulative distribution, and moment generating functions, as well as higher-order moments of the instantaneous signal-to-noise ratio. Building upon the aforementioned expressions, we extract novel formulas for the outage probability, symbol error probability, and average channel capacity. Asymptotic metrics are also deduced, which provide useful insights. Monte Carlo simulations results are presented to support the derived analytical framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19235v1</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wamberto J. L. Queiroz, Hugerles S. Silva, Higo T. P. Silva, Alexandros-Apostolos A. Boulogeorgos</dc:creator>
    </item>
    <item>
      <title>Linear Regression under Missing or Corrupted Coordinates</title>
      <link>https://arxiv.org/abs/2509.19242</link>
      <description>arXiv:2509.19242v1 Announce Type: cross 
Abstract: We study multivariate linear regression under Gaussian covariates in two settings, where data may be erased or corrupted by an adversary under a coordinate-wise budget. In the incomplete data setting, an adversary may inspect the dataset and delete entries in up to an $\eta$-fraction of samples per coordinate; a strong form of the Missing Not At Random model. In the corrupted data setting, the adversary instead replaces values arbitrarily, and the corruption locations are unknown to the learner. Despite substantial work on missing data, linear regression under such adversarial missingness remains poorly understood, even information-theoretically. Unlike the clean setting, where estimation error vanishes with more samples, here the optimal error remains a positive function of the problem parameters. Our main contribution is to characterize this error up to constant factors across essentially the entire parameter range. Specifically, we establish novel information-theoretic lower bounds on the achievable error that match the error of (computationally efficient) algorithms. A key implication is that, perhaps surprisingly, the optimal error in the missing data setting matches that in the corruption setting-so knowing the corruption locations offers no general advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19242v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Jelena Diakonikolas, Daniel M. Kane, Jasper C. H. Lee, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>On the minimum strength of (unobserved) covariates to overturn an insignificant result</title>
      <link>https://arxiv.org/abs/2408.13901</link>
      <description>arXiv:2408.13901v2 Announce Type: replace 
Abstract: We study conditions under which the addition of variables to a regression equation can turn a previously statistically insignificant result into a significant one. Specifically, we characterize the minimum strength of association required for these variables--both with the dependent and independent variables, or with the dependent variable alone--to elevate the observed t-statistic above a specified significance threshold. Interestingly, we show that it is considerably difficult to overturn a statistically insignificant result solely by reducing the standard error. Instead, included variables must also alter the point estimate to achieve such reversals in practice. Our results can be used to conduct sensitivity analyses against unobserved variables and to bound the maximum t-value one can obtain given different subsets of observed covariates, and may also offer algebraic explanations for patterns of reversals seen in empirical research, such as those documented by Lenz and Sahn (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13901v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danielle Tsao, Ronan Perry, Carlos Cinelli</dc:creator>
    </item>
    <item>
      <title>Consistent model selection in a collection of stochastic block models</title>
      <link>https://arxiv.org/abs/2502.03848</link>
      <description>arXiv:2502.03848v2 Announce Type: replace 
Abstract: We introduce the penalized Krichevsky-Trofimov (KT) estimator as a convergent method for estimating the number of nodes clusters when observing multiple networks within both multi-layer and dynamic Stochastic Block Models. We establish the consistency of the KT estimator, showing that it converges to the correct number of clusters in both types of models when the number of nodes in the networks increases. Our estimator does not require a known upper bound on this number to be consistent. Furthermore, we show that these consistency results hold in both dense and sparse regimes, making the penalized KT estimator robust across various network configurations. We illustrate its performance on synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03848v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucie Arts (LPSM)</dc:creator>
    </item>
    <item>
      <title>On admissibility in post-hoc hypothesis testing</title>
      <link>https://arxiv.org/abs/2508.00770</link>
      <description>arXiv:2508.00770v2 Announce Type: replace 
Abstract: The validity of classical hypothesis testing requires the significance level $\alpha$ be fixed before any statistical analysis takes place. This is a stringent requirement. For instance, it prohibits updating $\alpha$ during (or after) an experiment due to changing concern about the cost of false positives, or to reflect unexpectedly strong evidence against the null. Perhaps most disturbingly, witnessing a p-value $p\ll\alpha$ vs $p= \alpha- \epsilon$ for tiny $\epsilon &gt; 0$ has no (statistical) relevance for any downstream decision-making. Following recent work of Gr\"unwald (2024), we develop a theory of post-hoc hypothesis testing, enabling $\alpha$ to be chosen after seeing and analyzing the data. To study "good" post-hoc tests we introduce $\Gamma$-admissibility, where $\Gamma$ is a set of adversaries which map the data to a significance level. We classify the set of $\Gamma$-admissible rules for various sets $\Gamma$, showing they must be based on e-values, and recover the Neyman-Pearson lemma when $\Gamma$ is the constant map. We also give a Rao-Blackwellization result, proving that the expected utility of an e-value can be improved (for any concave utility) by conditioning on a sufficient statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00770v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Tyron Lardy, Aaditya Ramdas, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Cointegration with Occasionally Binding Constraints</title>
      <link>https://arxiv.org/abs/2211.09604</link>
      <description>arXiv:2211.09604v4 Announce Type: replace-cross 
Abstract: In the literature on nonlinear cointegration, a long-standing open problem relates to how a (nonlinear) vector autoregression, which provides a unified description of the short- and long-run dynamics of a vector of time series, can generate 'nonlinear cointegration' in the profound sense of those series sharing common nonlinear stochastic trends. We consider this problem in the setting of the censored and kinked structural VAR (CKSVAR), which provides a flexible yet tractable framework within which to model time series that are subject to threshold-type nonlinearities, such as those arising due to occasionally binding constraints, of which the zero lower bound (ZLB) on short-term nominal interest rates provides a leading example. We provide a complete characterisation of how common linear and nonlinear stochastic trends may be generated in this model, via unit roots and appropriate generalisations of the usual rank conditions, providing the first extension to date of the Granger-Johansen representation theorem to a nonlinearly cointegrated setting, and thereby giving the first successful treatment of the open problem. The limiting common trend processes include regulated, censored and kinked Brownian motions, none of which have previously appeared in the literature on cointegrated VARs. Our results and running examples illustrate that the CKSVAR is capable of supporting a far richer variety of long-run behaviour than is a linear VAR, in ways that may be particularly useful for the identification of structural parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09604v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James A. Duffy, Sophocles Mavroeidis, Sam Wycherley</dc:creator>
    </item>
  </channel>
</rss>
