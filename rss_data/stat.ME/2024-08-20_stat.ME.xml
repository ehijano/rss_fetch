<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive Uncertainty Quantification for Generative AI</title>
      <link>https://arxiv.org/abs/2408.08990</link>
      <description>arXiv:2408.08990v1 Announce Type: new 
Abstract: This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08990v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Sean O'Hagan, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Approximations to worst-case data dropping: unmasking failure modes</title>
      <link>https://arxiv.org/abs/2408.09008</link>
      <description>arXiv:2408.09008v1 Announce Type: new 
Abstract: A data analyst might worry about generalization if dropping a very small fraction of data points from a study could change its substantive conclusions. Finding the worst-case data subset to drop poses a combinatorial optimization problem. To overcome this intractability, recent works propose using additive approximations, which treat the contribution of a collection of data points as the sum of their individual contributions, and greedy approximations, which iteratively select the point with the highest impact to drop and re-run the data analysis without that point [Broderick et al., 2020, Kuschnig et al., 2021]. We identify that, even in a setting as simple as OLS linear regression, many of these approximations can break down in realistic data arrangements. Several of our examples reflect masking, where one outlier may hide or conceal the effect of another outlier. Based on the failures we identify, we provide recommendations for users and suggest directions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09008v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Y. Huang, David R. Burt, Tin D. Nguyen, Yunyi Shen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Autoregressive models for panel data causal inference with application to state-level opioid policies</title>
      <link>https://arxiv.org/abs/2408.09012</link>
      <description>arXiv:2408.09012v1 Announce Type: new 
Abstract: Motivated by the study of state opioid policies, we propose a novel approach that uses autoregressive models for causal effect estimation in settings with panel data and staggered treatment adoption. Specifically, we seek to estimate of the impact of key opioid-related policies by quantifying the effects of must access prescription drug monitoring programs (PDMPs), naloxone access laws (NALs), and medical marijuana laws on opioid prescribing. Existing methods, such as differences-in-differences and synthetic controls, are challenging to apply in these types of dynamic policy landscapes where multiple policies are implemented over time and sample sizes are small. Autoregressive models are an alternative strategy that have been used to estimate policy effects in similar settings, but until this paper have lacked formal justification. We outline a set of assumptions that tie these models to causal effects, and we study biases of estimates based on this approach when key causal assumptions are violated. In a set of simulation studies that mirror the structure of our application, we also show that our proposed estimators frequently outperform existing estimators. In short, we justify the use of autoregressive models to provide robust evidence on the effectiveness of four state policies in combating the opioid crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09012v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Antonelli, Max Rubinstein, Denis Agniel, Rosanna Smart, Elizabeth Stuart, Matthew Cefalu, Terry Schell, Joshua Eagan, Elizabeth Stone, Max Griswold, Mark Sorbero, Beth Ann Griffin</dc:creator>
    </item>
    <item>
      <title>Dynamic linear regression models for forecasting time series with semi long memory errors</title>
      <link>https://arxiv.org/abs/2408.09096</link>
      <description>arXiv:2408.09096v1 Announce Type: new 
Abstract: Dynamic linear regression models forecast the values of a time series based on a linear combination of a set of exogenous time series while incorporating a time series process for the error term. This error process is often assumed to follow an autoregressive integrated moving average (ARIMA) model, or seasonal variants thereof, which are unable to capture a long-range dependency structure of the error process. We propose a novel dynamic linear regression model that incorporates the long-range dependency feature of the errors. We demonstrate that the proposed error process may (i) have a significant impact on the posterior uncertainty of the estimated regression parameters and (ii) improve the model's forecasting ability. We develop a Markov chain Monte Carlo method to fit general dynamic linear regression models based on a frequency domain approach that enables fast, asymptotically exact Bayesian inference for large datasets. We demonstrate that our approximate algorithm is faster than the traditional time domain approaches, such as the Kalman filter and the multivariate Gaussian likelihood, while retaining a high accuracy when approximating the posterior distribution. We illustrate the method in simulated examples and two energy forecasting applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09096v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Goodwin, Matias Quiroz, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Learning Robust Treatment Rules for Censored Data</title>
      <link>https://arxiv.org/abs/2408.09155</link>
      <description>arXiv:2408.09155v1 Announce Type: new 
Abstract: There is a fast-growing literature on estimating optimal treatment rules directly by maximizing the expected outcome. In biomedical studies and operations applications, censored survival outcome is frequently observed, in which case the restricted mean survival time and survival probability are of great interest. In this paper, we propose two robust criteria for learning optimal treatment rules with censored survival outcomes; the former one targets at an optimal treatment rule maximizing the restricted mean survival time, where the restriction is specified by a given quantile such as median; the latter one targets at an optimal treatment rule maximizing buffered survival probabilities, where the predetermined threshold is adjusted to account the restricted mean survival time. We provide theoretical justifications for the proposed optimal treatment rules and develop a sampling-based difference-of-convex algorithm for learning them. In simulation studies, our estimators show improved performance compared to existing methods. We also demonstrate the proposed method using AIDS clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09155v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Junyi Liu, Tao Shen, Zhengling Qi, Xi Chen</dc:creator>
    </item>
    <item>
      <title>An exhaustive selection of sufficient adjustment sets for causal inference</title>
      <link>https://arxiv.org/abs/2408.09415</link>
      <description>arXiv:2408.09415v1 Announce Type: new 
Abstract: A subvector of predictor that satisfies the ignorability assumption, whose index set is called a sufficient adjustment set, is crucial for conducting reliable causal inference based on observational data. In this paper, we propose a general family of methods to detect all such sets for the first time in the literature, with no parametric assumptions on the outcome models and with flexible parametric and semiparametric assumptions on the predictor within the treatment groups; the latter induces desired sample-level accuracy. We show that the collection of sufficient adjustment sets can uniquely facilitate multiple types of studies in causal inference, including sharpening the estimation of average causal effect and recovering fundamental connections between the outcome and the treatment hidden in the dependence structure of the predictor. These findings are illustrated by simulation studies and a real data example at the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09415v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Luo, Fei Qin, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Grade of membership analysis for multi-layer categorical data</title>
      <link>https://arxiv.org/abs/2408.09418</link>
      <description>arXiv:2408.09418v1 Announce Type: new 
Abstract: Consider a group of individuals (subjects) participating in the same psychological tests with numerous questions (items) at different times. The observed responses can be recorded in multiple response matrices over time, named multi-layer categorical data. Assuming that each subject has a common mixed membership shared across all layers, enabling it to be affiliated with multiple latent classes with varying weights, the objective of the grade of membership (GoM) analysis is to estimate these mixed memberships from the data. When the test is conducted only once, the data becomes traditional single-layer categorical data. The GoM model is a popular choice for describing single-layer categorical data with a latent mixed membership structure. However, GoM cannot handle multi-layer categorical data. In this work, we propose a new model, multi-layer GoM, which extends GoM to multi-layer categorical data. To estimate the common mixed memberships, we propose a new approach, GoM-DSoG, based on a debiased sum of Gram matrices. We establish GoM-DSoG's per-subject convergence rate under the multi-layer GoM model. Our theoretical results suggest that fewer no-responses, more subjects, more items, and more layers are beneficial for GoM analysis. We also propose an approach to select the number of latent classes. Extensive experimental studies verify the theoretical findings and show GoM-DSoG's superiority over its competitors, as well as the accuracy of our method in determining the number of latent classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09418v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters</title>
      <link>https://arxiv.org/abs/2408.09598</link>
      <description>arXiv:2408.09598v1 Announce Type: new 
Abstract: Double (debiased) machine learning (DML) has seen widespread use in recent years for learning causal/structural parameters, in part due to its flexibility and adaptability to high-dimensional nuisance functions as well as its ability to avoid bias from regularization or overfitting. However, the classic double-debiased framework is only valid asymptotically for a predetermined sample size, thus lacking the flexibility of collecting more data if sharper inference is needed, or stopping data collection early if useful inferences can be made earlier than expected. This can be of particular concern in large scale experimental studies with huge financial costs or human lives at stake, as well as in observational studies where the length of confidence of intervals do not shrink to zero even with increasing sample size due to partial identifiability of a structural parameter. In this paper, we present time-uniform counterparts to the asymptotic DML results, enabling valid inference and confidence intervals for structural parameters to be constructed at any arbitrary (possibly data-dependent) stopping time. We provide conditions which are only slightly stronger than the standard DML conditions, but offer the stronger guarantee for anytime-valid inference. This facilitates the transformation of any existing DML method to provide anytime-valid guarantees with minimal modifications, making it highly adaptable and easy to use. We illustrate our procedure using two instances: a) local average treatment effect in online experiments with non-compliance, and b) partial identification of average treatment effect in observational studies with potential unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09598v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Patrick Bl\"obaum, Shiva Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Experimental Design For Causal Inference Through An Optimization Lens</title>
      <link>https://arxiv.org/abs/2408.09607</link>
      <description>arXiv:2408.09607v1 Announce Type: new 
Abstract: The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09607v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Regression with Imputed Binary Covariates with Application to Emotion Recognition</title>
      <link>https://arxiv.org/abs/2408.09619</link>
      <description>arXiv:2408.09619v1 Announce Type: new 
Abstract: In the flourishing live streaming industry, accurate recognition of streamers' emotions has become a critical research focus, with profound implications for audience engagement and content optimization. However, precise emotion coding typically requires manual annotation by trained experts, making it extremely expensive and time-consuming to obtain complete observational data for large-scale studies. Motivated by this challenge in streamer emotion recognition, we develop here a novel imputation method together with a principled statistical inference procedure for analyzing partially observed binary data. Specifically, we assume for each observation an auxiliary feature vector, which is sufficiently cheap to be fully collected for the whole sample. We next assume a small pilot sample with both the target binary covariates (i.e., the emotion status) and the auxiliary features fully observed, of which the size could be considerably smaller than that of the whole sample. Thereafter, a regression model can be constructed for the target binary covariates and the auxiliary features. This enables us to impute the missing binary features using the fully observed auxiliary features for the entire sample. We establish the associated asymptotic theory for principled statistical inference and present extensive simulation experiments, demonstrating the effectiveness and theoretical soundness of our proposed method. Furthermore, we validate our approach using a comprehensive dataset on emotion recognition in live streaming, demonstrating that our imputation method yields smaller standard errors and is more statistically efficient than using pilot data only. Our findings have significant implications for enhancing user experience and optimizing engagement on streaming platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09619v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqian Lin, Danyang Huang, Ziyu Xiong, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Penalized Likelihood Approach for the Four-parameter Kappa Distribution</title>
      <link>https://arxiv.org/abs/2408.09631</link>
      <description>arXiv:2408.09631v1 Announce Type: new 
Abstract: The four-parameter kappa distribution (K4D) is a generalized form of some commonly used distributions such as generalized logistic, generalized Pareto, generalized Gumbel, and generalized extreme value (GEV) distributions. Owing to its flexibility, the K4D is widely applied in modeling in several fields such as hydrology and climatic change. For the estimation of the four parameters, the maximum likelihood approach and the method of L-moments are usually employed. The L-moment estimator (LME) method works well for some parameter spaces, with up to a moderate sample size, but it is sometimes not feasible in terms of computing the appropriate estimates. Meanwhile, the maximum likelihood estimator (MLE) is optimal for large samples and applicable to a very wide range of situations, including non-stationary data. However, using the MLE of K4D with small sample sizes shows substantially poor performance in terms of a large variance of the estimator. We therefore propose a maximum penalized likelihood estimation (MPLE) of K4D by adjusting the existing penalty functions that restrict the parameter space. Eighteen combinations of penalties for two shape parameters are considered and compared. The MPLE retains modeling flexibility and large sample optimality while also improving on small sample properties. The properties of the proposed estimator are verified through a Monte Carlo simulation, and an application case is demonstrated taking Thailand's annual maximum temperature data. Based on this study, we suggest using combinations of penalty functions in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09631v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/02664763.2021.1871592</arxiv:DOI>
      <arxiv:journal_reference>Jour Appl Statist 49 (2022) 1559-1573</arxiv:journal_reference>
      <dc:creator>Nipada Papukdee, Jeong-Soo Park, Piyapatr Busababodhin</dc:creator>
    </item>
    <item>
      <title>Branch and Bound to Assess Stability of Regression Coefficients in Uncertain Models</title>
      <link>https://arxiv.org/abs/2408.09634</link>
      <description>arXiv:2408.09634v1 Announce Type: new 
Abstract: It can be difficult to interpret a coefficient of an uncertain model. A slope coefficient of a regression model may change as covariates are added or removed from the model. In the context of high-dimensional data, there are too many model extensions to check. However, as we show here, it is possible to efficiently search, with a branch and bound algorithm, for maximum and minimum values of that adjusted slope coefficient over a discrete space of regularized regression models. Here we introduce our algorithm, along with supporting mathematical results, an example application, and a link to our computer code, to help researchers summarize high-dimensional data and assess the stability of regression coefficients in uncertain models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09634v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Knaeble, R. Mitchell Hughes, George Rudolph, Mark A. Abramson, Daniel Razo</dc:creator>
    </item>
    <item>
      <title>Ensemble Prediction via Covariate-dependent Stacking</title>
      <link>https://arxiv.org/abs/2408.09755</link>
      <description>arXiv:2408.09755v1 Announce Type: new 
Abstract: This paper presents a novel approach to ensemble prediction called "Covariate-dependent Stacking" (CDST). Unlike traditional stacking methods, CDST allows model weights to vary flexibly as a function of covariates, thereby enhancing predictive performance in complex scenarios. We formulate the covariate-dependent weights through combinations of basis functions, estimate them by optimizing cross-validation, and develop an Expectation-Maximization algorithm, ensuring computational efficiency. To analyze the theoretical properties, we establish an oracle inequality regarding the expected loss to be minimized for estimating model weights. Through comprehensive simulation studies and an application to large-scale land price prediction, we demonstrate that CDST consistently outperforms conventional model averaging methods, particularly on datasets where some models fail to capture the underlying complexity. Our findings suggest that CDST is especially valuable for, but not limited to, spatio-temporal prediction problems, offering a powerful tool for researchers and practitioners in various fields of data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09755v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Regional and spatial dependence of poverty factors in Thailand, and its use into Bayesian hierarchical regression analysis</title>
      <link>https://arxiv.org/abs/2408.09760</link>
      <description>arXiv:2408.09760v1 Announce Type: new 
Abstract: Poverty is a serious issue that harms humanity progression. The simplest solution is to use one-shirt-size policy to alleviate it. Nevertheless, each region has its unique issues, which require a unique solution to solve them. In the aspect of spatial analysis, neighbor regions can provide useful information to analyze issues of a given region. In this work, we proposed inferred boundaries of regions of Thailand that can explain better the poverty dynamics, instead of the usual government administrative regions. The proposed regions maximize a trade-off between poverty-related features and geographical coherence. We use a spatial analysis together with Moran's cluster algorithms and Bayesian hierarchical regression models, with the potential of assist the implementation of the right policy to alleviate the poverty phenomenon. We found that all variables considered show a positive spatial autocorrelation. The results of analysis illustrate that 1) Northern, Northeastern Thailand, and in less extend Northcentral Thailand are the regions that require more attention in the aspect of poverty issues, 2) Northcentral, Northeastern, Northern and Southern Thailand present dramatically low levels of education, income and amount of savings contrasted with large cities such as Bangkok-Pattaya and Central Thailand, and 3) Bangkok-Pattaya is the only region whose average years of education is above 12 years, which corresponds (approx.) with a complete senior high school.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09760v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irving G\'omez-M\'endez, Chainarong Amornbunchornvej</dc:creator>
    </item>
    <item>
      <title>Shift-Dispersion Decompositions of Wasserstein and Cram\'er Distances</title>
      <link>https://arxiv.org/abs/2408.09770</link>
      <description>arXiv:2408.09770v1 Announce Type: new 
Abstract: Divergence functions are measures of distance or dissimilarity between probability distributions that serve various purposes in statistics and applications. We propose decompositions of Wasserstein and Cram\'er distances$-$which compare two distributions by integrating over their differences in distribution or quantile functions$-$into directed shift and dispersion components. These components are obtained by dividing the differences between the quantile functions into contributions arising from shift and dispersion, respectively. Our decompositions add information on how the distributions differ in a condensed form and consequently enhance the interpretability of the underlying divergences. We show that our decompositions satisfy a number of natural properties and are unique in doing so in location-scale families. The decompositions allow to derive sensitivities of the divergence measures to changes in location and dispersion, and they give rise to weak stochastic order relations that are linked to the usual stochastic and the dispersive order. Our theoretical developments are illustrated in two applications, where we focus on forecast evaluation of temperature extremes and on the design of probabilistic surveys in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09770v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Resin, Daniel Wolffram, Johannes Bracher, Timo Dimitriadis</dc:creator>
    </item>
    <item>
      <title>Weak instruments in multivariable Mendelian randomization: methods and practice</title>
      <link>https://arxiv.org/abs/2408.09868</link>
      <description>arXiv:2408.09868v1 Announce Type: new 
Abstract: The method of multivariable Mendelian randomization uses genetic variants to instrument multiple exposures, to estimate the effect that a given exposure has on an outcome conditional on all other exposures included in a linear model. Unfortunately, the inclusion of every additional exposure makes a weak instruments problem more likely, because we require conditionally strong genetic predictors of each exposure. This issue is well appreciated in practice, with different versions of F-statistics routinely reported as measures of instument strength. Less transparently, however, these F-statistics are sometimes used to guide instrument selection, and even to decide whether to report empirical results. Rather than discarding findings with low F-statistics, weak instrument-robust methods can provide valid inference under weak instruments. For multivariable Mendelian randomization with two-sample summary data, we encourage use of the inference strategy of Andrews (2018) that reports both robust and non-robust confidence sets, along with a statistic that measures how reliable the non-robust confidence set is in terms of coverage. We also propose a novel adjusted-Kleibergen statistic that corrects for overdispersion heterogeneity in genetic associations with the outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09868v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashish Patel, James Lane, Stephen Burgess</dc:creator>
    </item>
    <item>
      <title>Improving Genomic Prediction using High-dimensional Secondary Phenotypes: the Genetic Latent Factor Approach</title>
      <link>https://arxiv.org/abs/2408.09876</link>
      <description>arXiv:2408.09876v1 Announce Type: new 
Abstract: Decreasing costs and new technologies have led to an increase in the amount of data available to plant breeding programs. High-throughput phenotyping (HTP) platforms routinely generate high-dimensional datasets of secondary features that may be used to improve genomic prediction accuracy. However, integration of this data comes with challenges such as multicollinearity, parameter estimation in $p &gt; n$ settings, and the computational complexity of many standard approaches. Several methods have emerged to analyze such data, but interpretation of model parameters often remains challenging.
  We propose genetic factor best linear unbiased prediction (gfBLUP), a seven-step prediction pipeline that reduces the dimensionality of the original secondary HTP data using generative factor analysis. In short, gfBLUP uses redundancy filtered and regularized genetic and residual correlation matrices to fit a maximum likelihood factor model and estimate genetic latent factor scores. These latent factors are subsequently used in multi-trait genomic prediction. Our approach performs on par or better than alternatives in extensive simulations and a real-world application, while producing easily interpretable and biologically relevant parameters. We discuss several possible extensions and highlight gfBLUP as the basis for a flexible and modular multi-trait genomic prediction framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09876v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Killian A. C. Melsen, Jonathan F. Kunst, Jos\'e Crossa, Margaret R. Krause, Fred A. van Eeuwijk, Willem Kruijer, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Non-Plug-In Estimators Could Outperform Plug-In Estimators: a Cautionary Note and a Diagnosis</title>
      <link>https://arxiv.org/abs/2408.10091</link>
      <description>arXiv:2408.10091v1 Announce Type: new 
Abstract: Objectives: Highly flexible nonparametric estimators have gained popularity in causal inference and epidemiology. Popular examples of such estimators include targeted maximum likelihood estimators (TMLE) and double machine learning (DML). TMLE is often argued or suggested to be better than DML estimators and several other estimators in small to moderate samples -- even if they share the same large-sample properties -- because TMLE is a plug-in estimator and respects the known bounds on the parameter, while other estimators might fall outside the known bounds and yield absurd estimates. However, this argument is not a rigorously proven result and may fail in certain cases. Methods: In a carefully chosen simulation setting, I compare the performance of several versions of TMLE and DML estimators of the average treatment effect among treated in small to moderate samples. Results: In this simulation setting, DML estimators outperforms some versions of TMLE in small samples. TMLE fluctuations are unstable, and hence empirically checking the magnitude of the TMLE fluctuation might alert cases where TMLE might perform poorly. Conclusions: As a plug-in estimator, TMLE is not guaranteed to outperform non-plug-in counterparts such as DML estimators in small samples. Checking the fluctuation magnitude might be a useful diagnosis for TMLE. More rigorous theoretical justification is needed to understand and compare the finite-sample performance of these highly flexible estimators in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10091v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu</dc:creator>
    </item>
    <item>
      <title>Insights of the Intersection of Phase-Type Distributions and Positive Systems</title>
      <link>https://arxiv.org/abs/2408.10142</link>
      <description>arXiv:2408.10142v1 Announce Type: new 
Abstract: In this paper, we consider the relationship between phase-type distributions and positive systems through practical examples. Phase-type distributions, commonly used in modelling dynamic systems, represent the temporal evolution of a set of variables based on their phase. On the other hand, positive systems, prevalent in a wide range of disciplines, are those where the involved variables maintain non-negative values over time. Through some examples, we demonstrate how phase-type distributions can be useful in describing and analyzing positive systems, providing a perspective on their dynamic behavior. Our main objective is to establish clear connections between these seemingly different concepts, highlighting their relevance and utility in various fields of study. The findings presented here contribute to a better understanding of the interaction between phase-type distribution theory and positive system theory, opening new opportunities for future research in this exciting interdisciplinary field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10142v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Luz Judith Rodr\'iguez Esparza, Fernando Baltazar Larios</dc:creator>
    </item>
    <item>
      <title>A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data</title>
      <link>https://arxiv.org/abs/2408.10149</link>
      <description>arXiv:2408.10149v1 Announce Type: new 
Abstract: Randomized clinical trials (RCTs) often involve multiple longitudinal primary outcomes to comprehensively assess treatment efficacy. The Longitudinal Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based method, effectively controls Type I error and enhances statistical power by leveraging the temporal structure of the data without relying on distributional assumptions. However, the LRST is limited to two-arm comparisons. To address the need for comparing multiple doses against a control group in many RCTs, we extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a flexible and powerful approach for evaluating treatment efficacy across multiple arms and outcomes, with a strong capability for detecting the most effective dose in multi-arm trials. Extensive simulations demonstrate that this method maintains excellent Type I error control while providing greater power compared to the two-arm LRST with multiplicity adjustments. Application to the Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical utility and robustness, confirming its efficacy in complex clinical trial analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>[Invited Discussion] Randomization Tests to Address Disruptions in Clinical Trials: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions</title>
      <link>https://arxiv.org/abs/2408.09060</link>
      <description>arXiv:2408.09060v1 Announce Type: cross 
Abstract: Disruptions in clinical trials may be due to external events like pandemics, warfare, and natural disasters. Resulting complications may lead to unforeseen intercurrent events (events that occur after treatment initiation and affect the interpretation of the clinical question of interest or the existence of the measurements associated with it). In Uschner et al. (2023), several example clinical trial disruptions are described: treatment effect drift, population shift, change of care, change of data collection, and change of availability of study medication. A complex randomized controlled trial (RCT) setting with (planned or unplanned) intercurrent events is then described, and randomization tests are presented as a means for non-parametric inference that is robust to violations of assumption typically made in clinical trials. While estimation methods like Targeted Learning (TL) are valid in such settings, we do not see where the authors make the case that one should be going for a randomization test in such disrupted RCTs. In this discussion, we comment on the appropriateness of TL and the accompanying TL Roadmap in the context of disrupted clinical trials. We highlight a few key articles related to the broad applicability of TL for RCTs and real-world data (RWD) analyses with intercurrent events. We begin by introducing TL and motivating its utility in Section 2, and then in Section 3 we provide a brief overview of the TL Roadmap. In Section 4 we recite the example clinical trial disruptions presented in Uschner et al. (2023), discussing considerations and solutions based on the principles of TL. We request in an authors' rejoinder a clear theoretical demonstration with specific examples in this setting that a randomization test is the only valid inferential method relative to one based on following the TL Roadmap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09060v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael V. Phillips, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Deep Limit Model-free Prediction in Regression</title>
      <link>https://arxiv.org/abs/2408.09532</link>
      <description>arXiv:2408.09532v1 Announce Type: cross 
Abstract: In this paper, we provide a novel Model-free approach based on Deep Neural Network (DNN) to accomplish point prediction and prediction interval under a general regression setting. Usually, people rely on parametric or non-parametric models to bridge dependent and independent variables (Y and X). However, this classical method relies heavily on the correct model specification. Even for the non-parametric approach, some additive form is often assumed. A newly proposed Model-free prediction principle sheds light on a prediction procedure without any model assumption. Previous work regarding this principle has shown better performance than other standard alternatives. Recently, DNN, one of the machine learning methods, has received increasing attention due to its great performance in practice. Guided by the Model-free prediction idea, we attempt to apply a fully connected forward DNN to map X and some appropriate reference random variable Z to Y. The targeted DNN is trained by minimizing a specially designed loss function so that the randomness of Y conditional on X is outsourced to Z through the trained DNN. Our method is more stable and accurate compared to other DNN-based counterparts, especially for optimal point predictions. With a specific prediction procedure, our prediction interval can capture the estimation variability so that it can render a better coverage rate for finite sample cases. The superior performance of our method is verified by simulation and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09532v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejin Wu, Dimitris N. Politis</dc:creator>
    </item>
    <item>
      <title>Sample-Optimal Large-Scale Optimal Subset Selection</title>
      <link>https://arxiv.org/abs/2408.09537</link>
      <description>arXiv:2408.09537v1 Announce Type: cross 
Abstract: Ranking and selection (R&amp;S) conventionally aims to select the unique best alternative with the largest mean performance from a finite set of alternatives. However, for better supporting decision making, it may be more informative to deliver a small menu of alternatives whose mean performances are among the top $m$. Such problem, called optimal subset selection (OSS), is generally more challenging to address than the conventional R&amp;S. This challenge becomes even more significant when the number of alternatives is considerably large. Thus, the focus of this paper is on addressing the large-scale OSS problem. To achieve this goal, we design a top-$m$ greedy selection mechanism that keeps sampling the current top $m$ alternatives with top $m$ running sample means and propose the explore-first top-$m$ greedy (EFG-$m$) procedure. Through an extended boundary-crossing framework, we prove that the EFG-$m$ procedure is both sample optimal and consistent in terms of the probability of good selection, confirming its effectiveness in solving large-scale OSS problem. Surprisingly, we also demonstrate that the EFG-$m$ procedure enables to achieve an indifference-based ranking within the selected subset of alternatives at no extra cost. This is highly beneficial as it delivers deeper insights to decision-makers, enabling more informed decision-makings. Lastly, numerical experiments validate our results and demonstrate the efficiency of our procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09537v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaile Li, Weiwei Fan, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>A Likelihood-Free Approach to Goal-Oriented Bayesian Optimal Experimental Design</title>
      <link>https://arxiv.org/abs/2408.09582</link>
      <description>arXiv:2408.09582v1 Announce Type: cross 
Abstract: Conventional Bayesian optimal experimental design seeks to maximize the expected information gain (EIG) on model parameters. However, the end goal of the experiment often is not to learn the model parameters, but to predict downstream quantities of interest (QoIs) that depend on the learned parameters. And designs that offer high EIG for parameters may not translate to high EIG for QoIs. Goal-oriented optimal experimental design (GO-OED) thus directly targets to maximize the EIG of QoIs.
  We introduce LF-GO-OED (likelihood-free goal-oriented optimal experimental design), a computational method for conducting GO-OED with nonlinear observation and prediction models. LF-GO-OED is specifically designed to accommodate implicit models, where the likelihood is intractable. In particular, it builds a density ratio estimator from samples generated from approximate Bayesian computation (ABC), thereby sidestepping the need for likelihood evaluations or density estimations. The overall method is validated on benchmark problems with existing methods, and demonstrated on scientific applications of epidemiology and neural science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09582v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atlanta Chakraborty, Xun Huan, Tommie Catanach</dc:creator>
    </item>
    <item>
      <title>Robust spectral clustering with rank statistics</title>
      <link>https://arxiv.org/abs/2408.10136</link>
      <description>arXiv:2408.10136v1 Announce Type: cross 
Abstract: This paper analyzes the statistical performance of a robust spectral clustering method for latent structure recovery in noisy data matrices. We consider eigenvector-based clustering applied to a matrix of nonparametric rank statistics that is derived entrywise from the raw, original data matrix. This approach is robust in the sense that, unlike traditional spectral clustering procedures, it can provably recover population-level latent block structure even when the observed data matrix includes heavy-tailed entries and has a heterogeneous variance profile.
  Our main theoretical contributions are threefold and hold under flexible data generating conditions. First, we establish that robust spectral clustering with rank statistics can consistently recover latent block structure, viewed as communities of nodes in a graph, in the sense that unobserved community memberships for all but a vanishing fraction of nodes are correctly recovered with high probability when the data matrix is large. Second, we refine the former result and further establish that, under certain conditions, the community membership of any individual, specified node of interest can be asymptotically exactly recovered with probability tending to one in the large-data limit. Third, we establish asymptotic normality results associated with the truncated eigenstructure of matrices whose entries are rank statistics, made possible by synthesizing contemporary entrywise matrix perturbation analysis with the classical nonparametric theory of so-called simple linear rank statistics. Collectively, these results demonstrate the statistical utility of rank-based data transformations when paired with spectral techniques for dimensionality reduction. Additionally, for a dataset of human connectomes, our approach yields parsimonious dimensionality reduction and improved recovery of ground-truth neuroanatomical cluster structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10136v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Cape, Xianshi Yu, Jonquil Z. Liao</dc:creator>
    </item>
    <item>
      <title>Estimating large causal polytrees from small samples</title>
      <link>https://arxiv.org/abs/2209.07028</link>
      <description>arXiv:2209.07028v4 Announce Type: replace 
Abstract: We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07028v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Mathukumalli Vidyasagar</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation via Partial Derivatives</title>
      <link>https://arxiv.org/abs/2209.07672</link>
      <description>arXiv:2209.07672v2 Announce Type: replace 
Abstract: Traditional nonparametric estimation methods often lead to a slow convergence rate in large dimensions and require unrealistically enormous sizes of datasets for reliable conclusions. We develop an approach based on partial derivatives, either observed or estimated, to effectively estimate the function at near-parametric convergence rates. The novel approach and computational algorithm could lead to methods useful to practitioners in many areas of science and engineering. Our theoretical results reveal a behavior universal to this class of nonparametric estimation problems. We explore a general setting involving tensor product spaces and build upon the smoothing spline analysis of variance (SS-ANOVA) framework. For $d$-dimensional models under full interaction, the optimal rates with gradient information on $p$ covariates are identical to those for the $(d-p)$-interaction models without gradients and, therefore, the models are immune to the "curse of interaction." For additive models, the optimal rates using gradient information are root-$n$, thus achieving the "parametric rate." We demonstrate aspects of the theoretical results through synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07672v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowu Dai</dc:creator>
    </item>
    <item>
      <title>Unbiased Test Error Estimation in the Poisson Means Problem via Coupled Bootstrap Techniques</title>
      <link>https://arxiv.org/abs/2212.01943</link>
      <description>arXiv:2212.01943v3 Announce Type: replace 
Abstract: We propose a coupled bootstrap (CB) method for the test error of an arbitrary algorithm that estimates the mean in a Poisson sequence, often called the Poisson means problem. The idea behind our method is to generate two carefully-designed data vectors from the original data vector, by using synthetic binomial noise. One such vector acts as the training sample and the second acts as the test sample. To stabilize the test error estimate, we average this over multiple bootstrap B of the synthetic noise. A key property of the CB estimator is that it is unbiased for the test error in a Poisson problem where the original mean has been shrunken by a small factor, driven by the success probability $p$ in the binomial noise. Further, in the limit as $B \to \infty$ and $p \to 0$, we show that the CB estimator recovers a known unbiased estimator for test error based on Hudson's lemma, under no assumptions on the given algorithm for estimating the mean (in particular, no smoothness assumptions). Our methodology applies to two central loss functions that can be used to define test error: Poisson deviance and squared loss. Via a bias-variance decomposition, for each loss function, we analyze the effects of the binomial success probability and the number of bootstrap samples and on the accuracy of the estimator. We also investigate our method empirically across a variety of settings, using simulated as well as real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01943v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia L. Oliveira, Jing Lei, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Off-policy Evaluation in Doubly Inhomogeneous Environments</title>
      <link>https://arxiv.org/abs/2306.08719</link>
      <description>arXiv:2306.08719v4 Announce Type: replace 
Abstract: This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set from the Medical Information Mart for Intensive Care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08719v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Bian, Chengchun Shi, Zhengling Qi, Lan Wang</dc:creator>
    </item>
    <item>
      <title>Empirical sandwich variance estimator for iterated conditional expectation g-computation</title>
      <link>https://arxiv.org/abs/2306.10976</link>
      <description>arXiv:2306.10976v3 Announce Type: replace 
Abstract: Iterated conditional expectation (ICE) g-computation is an estimation approach for addressing time-varying confounding for both longitudinal and time-to-event data. Unlike other g-computation implementations, ICE avoids the need to specify models for each time-varying covariate. For variance estimation, previous work has suggested the bootstrap. However, bootstrapping can be computationally intense. Here, we present ICE g-computation as a set of stacked estimating equations. Therefore, the variance for the ICE g-computation estimator can be consistently estimated using the empirical sandwich variance estimator. Performance of the variance estimator was evaluated empirically with a simulation study. The proposed approach is also demonstrated with an illustrative example on the effect of cigarette smoking on the prevalence of hypertension. In the simulation study, the empirical sandwich variance estimator appropriately estimated the variance. When comparing runtimes between the sandwich variance estimator and the bootstrap for the applied example, the sandwich estimator was substantially faster, even when bootstraps were run in parallel. The empirical sandwich variance estimator is a viable option for variance estimation with ICE g-computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10976v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Rachael K Ross, Bonnie E Shook-Sa, Stephen R Cole, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Winner's Curse Free Robust Mendelian Randomization with Summary Data</title>
      <link>https://arxiv.org/abs/2309.04957</link>
      <description>arXiv:2309.04957v2 Announce Type: replace 
Abstract: In the past decade, the increased availability of genome-wide association studies summary data has popularized Mendelian Randomization (MR) for conducting causal inference. MR analyses, incorporating genetic variants as instrumental variables, are known for their robustness against reverse causation bias and unmeasured confounders. Nevertheless, classical MR analyses utilizing summary data may still produce biased causal effect estimates due to the winner's curse and pleiotropic issues. To address these two issues and establish valid causal conclusions, we propose a unified robust Mendelian Randomization framework with summary data, which systematically removes the winner's curse and screens out invalid genetic instruments with pleiotropic effects. Different from existing robust MR literature, our framework delivers valid statistical inference on the causal effect neither requiring the genetic pleiotropy effects to follow any parametric distribution nor relying on perfect instrument screening property. Under appropriate conditions, we show that our proposed estimator converges to a normal distribution and its variance can be well estimated. We demonstrate the performance of our proposed estimator through Monte Carlo simulations and two case studies. The codes implementing the procedures are available at https://github.com/ChongWuLab/CARE/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04957v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongming Xie, Wanheng Zhang, Jingshen Wang, Chong Wu</dc:creator>
    </item>
    <item>
      <title>A p-value for Process Tracing and other N=1 Studies</title>
      <link>https://arxiv.org/abs/2310.13826</link>
      <description>arXiv:2310.13826v2 Announce Type: replace 
Abstract: We introduce a method for calculating \(p\)-values when testing causal theories about a single case, for instance when conducting process tracing. As in Fisher's (1935) original design, our \(p\)-value indicates how frequently one would find the same or more favorable evidence while entertaining a rival theory (the null) for the sake of argument. We use an urn model to represent the null distribution and calibrate it to privilege false negative errors and reduce false positive errors. We also present an approach to sensitivity analysis and for representing the evidentiary weight of different observations. Our test suits any type of evidence, such as data from interviews and archives, observed in any combination. We apply our hypothesis test in two studies: a process tracing classic about the cause of the cholera outburst in Soho (Snow 1855) and a recent process tracing based explanation of the cause of a welfare policy shift in Uruguay (Rossel, Antia, and Manzi 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13826v2</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matias Lopez, Jake Bowers</dc:creator>
    </item>
    <item>
      <title>Evaluating Binary Outcome Classifiers Estimated from Survey Data</title>
      <link>https://arxiv.org/abs/2311.00596</link>
      <description>arXiv:2311.00596v4 Announce Type: replace 
Abstract: Surveys are commonly used to facilitate research in epidemiology, health, and the social and behavioral sciences. Often, these surveys are not simple random samples, and respondents are given weights reflecting their probability of selection into the survey. It is well known that analysts can use these survey weights to produce unbiased estimates of population quantities like totals. In this article, we show that survey weights also can be beneficial for evaluating the quality of predictive models when splitting data into training and test sets. In particular, we characterize model assessment statistics, such as sensitivity and specificity, as finite population quantities, and compute survey-weighted estimates of these quantities with sample test data comprising a random subset of the original data.Using simulations with data from the National Survey on Drug Use and Health and the National Comorbidity Survey, we show that unweighted metrics estimated with sample test data can misrepresent population performance, but weighted metrics appropriately adjust for the complex sampling design. We also show that this conclusion holds for models trained using upsampling for mitigating class imbalance. The results suggest that weighted metrics should be used when evaluating performance on sample test data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00596v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1097/EDE.0000000000001776</arxiv:DOI>
      <arxiv:journal_reference>Epidemiology (2024)</arxiv:journal_reference>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>On Doubly Robust Estimation with Nonignorable Missing Data Using Instrumental Variables</title>
      <link>https://arxiv.org/abs/2311.08691</link>
      <description>arXiv:2311.08691v4 Announce Type: replace 
Abstract: Suppose we are interested in the mean of an outcome that is subject to nonignorable nonresponse. This paper develops new semiparametric estimation methods with instrumental variables which affect nonresponse, but not the outcome. The proposed estimators remain consistent and asymptotically normal even under partial model misspecifications for two variation independent nuisance components. We evaluate the performance of the proposed estimators via a simulation study, and apply them in adjusting for missing data induced by HIV testing refusal in the evaluation of HIV seroprevalence in Mochudi, Botswana, using interviewer experience as an instrumental variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08691v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baoluo Sun, Wang Miao, Deshanee S. Wickramarachchi</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Transfer Learning for Building High-Dimensional Generalized Linear Models with Disparate Datasets</title>
      <link>https://arxiv.org/abs/2312.12786</link>
      <description>arXiv:2312.12786v2 Announce Type: replace 
Abstract: Development of comprehensive prediction models are often of great interest in many disciplines of science, but datasets with information on all desired features often have small sample sizes. We describe a transfer learning approach for building high-dimensional generalized linear models using data from a main study with detailed information on all predictors and an external, potentially much larger, study that has ascertained a more limited set of predictors. We propose using the external dataset to build a reduced model and then "transfer" the information on underlying parameters for the analysis of the main study through a set of calibration equations which can account for the study-specific effects of design variables. We then propose a penalized generalized method of moment framework for inference and a one-step estimation method that could be implemented using standard glmnet package. We develop asymptotic theory and conduct extensive simulation studies to investigate both predictive performance and post-selection inference properties of the proposed method. Finally, we illustrate an application of the proposed method for the development of risk models for five common diseases using the UK Biobank study, combining information on low-dimensional risk factors and high throughout proteomic biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12786v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruzhang Zhao, Prosenjit Kundu, Arkajyoti Saha, Nilanjan Chatterjee</dc:creator>
    </item>
    <item>
      <title>Reducing the dimensionality and granularity in hierarchical categorical variables</title>
      <link>https://arxiv.org/abs/2403.03613</link>
      <description>arXiv:2403.03613v2 Announce Type: replace 
Abstract: Hierarchical categorical variables often exhibit many levels (high granularity) and many classes within each level (high dimensionality). This may cause overfitting and estimation issues when including such covariates in a predictive model. In current literature, a hierarchical covariate is often incorporated via nested random effects. However, this does not facilitate the assumption of classes having the same effect on the response variable. In this paper, we propose a methodology to obtain a reduced representation of a hierarchical categorical variable. We show how entity embedding can be applied in a hierarchical setting. Subsequently, we propose a top-down clustering algorithm which leverages the information encoded in the embeddings to reduce both the within-level dimensionality as well as the overall granularity of the hierarchical categorical variable. In simulation experiments, we show that our methodology can effectively approximate the true underlying structure of a hierarchical covariate in terms of the effect on a response variable, and find that incorporating the reduced hierarchy improves the balance between model fit and complexity. We apply our methodology on a real dataset and find that the reduced hierarchy is an improvement over the original hierarchical structure and reduced structures proposed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03613v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Paul Wilsens, Katrien Antonio, Gerda Claeskens</dc:creator>
    </item>
    <item>
      <title>Novel Fuzzy Centrality Measures in Vague Social Networks</title>
      <link>https://arxiv.org/abs/2407.02401</link>
      <description>arXiv:2407.02401v2 Announce Type: replace 
Abstract: Social network analysis (SNA) helps us understand the relationships and interactions between individuals, groups, organizations, or other social entities. In the literature, ties are generally considered binary or weighted based on their strength. Nonetheless, when the actors are individuals, these relationships are often imprecise, and identifying them with simple scalars leads to information loss. Indeed, social relationships are often vague in real life, and although previous research has proposed the use of fuzzy networks, these are typically characterized by crisp ties. The use of weighted links does not align with the original philosophy of fuzzy logic, which instead aims to preserve the vagueness inherent in human language and real life. For this reason, this paper proposes a generalization of the so-called Fuzzy Social Network Analysis (FSNA) to the context of imprecise relationships among actors. Dealing with imprecise ties and introducing fuzziness in the definition of relationships requires an extension of social network analysis, defining ties as fuzzy numbers instead of crisp values and extending classical centrality indices to fuzzy centrality indexes. The article presents the theory and application of real data collected through a fascinating mouse-tracking technique to study the fuzzy relationships in a collaboration network among the members of a university department.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02401v2</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annamaria Porreca, Fabrizio Maturo, Viviana Ventre</dc:creator>
    </item>
    <item>
      <title>Estimating the FDR of variable selection</title>
      <link>https://arxiv.org/abs/2408.07231</link>
      <description>arXiv:2408.07231v2 Announce Type: replace 
Abstract: We introduce a generic estimator for the false discovery rate of any model selection procedure, in common statistical modeling settings including the Gaussian linear model, Gaussian graphical model, and model-X setting. We prove that our method has a conservative (non-negative) bias in finite samples under standard statistical assumptions, and provide a bootstrap method for assessing its standard error. For methods like the Lasso, forward-stepwise regression, and the graphical Lasso, our estimator serves as a valuable companion to cross-validation, illuminating the tradeoff between prediction error and variable selection accuracy as a function of the model complexity parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07231v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiang Luo, William Fithian, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Ridge detection for nonstationary multicomponent signals with time-varying wave-shape functions and its applications</title>
      <link>https://arxiv.org/abs/2309.06673</link>
      <description>arXiv:2309.06673v2 Announce Type: replace-cross 
Abstract: We introduce a novel ridge detection algorithm for time-frequency (TF) analysis, particularly tailored for intricate nonstationary time series encompassing multiple non-sinusoidal oscillatory components. The algorithm is rooted in the distinctive geometric patterns that emerge in the TF domain due to such non-sinusoidal oscillations. We term this method \textit{shape-adaptive mode decomposition-based multiple harmonic ridge detection} (\textsf{SAMD-MHRD}). A swift implementation is available when supplementary information is at hand. We demonstrate the practical utility of \textsf{SAMD-MHRD} through its application to a real-world challenge. We employ it to devise a cutting-edge walking activity detection algorithm, leveraging accelerometer signals from an inertial measurement unit across diverse body locations of a moving subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06673v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan-Wei Su, Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>Accurate, scalable, and efficient Bayesian optimal experimental design with derivative-informed neural operators</title>
      <link>https://arxiv.org/abs/2312.14810</link>
      <description>arXiv:2312.14810v3 Announce Type: replace-cross 
Abstract: We consider optimal experimental design (OED) problems in selecting the most informative observation sensors to estimate model parameters in a Bayesian framework. Such problems are computationally prohibitive when the parameter-to-observable (PtO) map is expensive to evaluate, the parameters are high-dimensional, and the optimization for sensor selection is combinatorial and high-dimensional. To address these challenges, we develop an accurate, scalable, and efficient computational framework based on derivative-informed neural operators (DINO). We propose to use derivative-informed dimension reduction to reduce the parameter dimensions, based on which we train DINO with derivative information as an accurate and efficient surrogate for the PtO map and its derivative. Moreover, we derive DINO-enabled efficient formulations in computing the maximum a posteriori (MAP) point, the eigenvalues of approximate posterior covariance, and three commonly used optimality criteria for the OED problems. Furthermore, we provide detailed error analysis for the approximations of the MAP point, the eigenvalues, and the optimality criteria. We also propose a modified swapping greedy algorithm for the sensor selection optimization and demonstrate that the proposed computational framework is scalable to preserve the accuracy for increasing parameter dimensions and achieves high computational efficiency, with an over 1000$\times$ speedup accounting for both offline construction and online evaluation costs, compared to high-fidelity Bayesian OED solutions for a three-dimensional nonlinear convection-diffusion-reaction example with tens of thousands of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14810v3</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Go, Peng Chen</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v3 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. As a practical implementation, we introduce PREGen, which combines our genericization method with an existing mitigation technique. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images. PREGen dramatically improves the performance of the existing method, reducing the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, although generative models have been found to generate copyrighted characters even when the names of characters are not directly mentioned in the prompt, PREGen almost completely suppresses the generation of copyrighted characters for such prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
  </channel>
</rss>
