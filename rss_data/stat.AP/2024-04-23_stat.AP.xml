<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 04:01:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Variational Bayesian surrogate modelling with application to robust design optimisation</title>
      <link>https://arxiv.org/abs/2404.14857</link>
      <description>arXiv:2404.14857v1 Announce Type: new 
Abstract: Surrogate models provide a quick-to-evaluate approximation to complex computational models and are essential for multi-query problems like design optimisation. The inputs of current computational models are usually high-dimensional and uncertain. We consider Bayesian inference for constructing statistical surrogates with input uncertainties and intrinsic dimensionality reduction. The surrogates are trained by fitting to data from prevalent deterministic computational models. The assumed prior probability density of the surrogate is a Gaussian process. We determine the respective posterior probability density and parameters of the posited statistical model using variational Bayes. The non-Gaussian posterior is approximated by a simpler trial density with free variational parameters and the discrepancy between them is measured using the Kullback-Leibler (KL) divergence. We employ the stochastic gradient method to compute the variational parameters and other statistical model parameters by minimising the KL divergence. We demonstrate the accuracy and versatility of the proposed reduced dimension variational Gaussian process (RDVGP) surrogate on illustrative and robust structural optimisation problems with cost functions depending on a weighted sum of the mean and standard deviation of model outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14857v1</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas A. Archbold, Ieva Kazlauskaite, Fehmi Cirak</dc:creator>
    </item>
    <item>
      <title>Bayesian Approaches to Collaborative Data Analysis with Strict Privacy Restrictions</title>
      <link>https://arxiv.org/abs/2404.14895</link>
      <description>arXiv:2404.14895v1 Announce Type: new 
Abstract: Collaborative data analysis between countries is crucial for enabling fast responses to increasingly multi-country disease outbreaks. Often, data early in outbreaks are of sensitive nature and subject to strict privacy restrictions. Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as a novel approach solving issues around data privacy and confidentiality. In the present study, we propose two approaches to federated analysis, based on simple Bayesian statistics and exploit this simplicity to make them feasible for rapid collaboration without the risks of data leaks and data reidentification, as they require neither data sharing nor direct communication between devices. The first approach uses summaries from parameters' posteriors previously obtained at a different location to update truncated normal distributions approximating priors of a new model. The second approach uses the entire previously sampled posterior, approximating via a multivariate normal distribution. We test these models on simulated and on real outbreak data to estimate the incubation period of infectious diseases. Results indicate that both approaches can recover incubation period parameters accurately, but they differ in terms of inferential capacity. The posterior summary approach shows higher stability and precision, but it cannot capture posterior correlations, meaning it is inferentially limited. The whole posterior approach can capture correlations, but it shows less stability, and its applicability is limited to fewer prior distributions. We discuss results in terms of the advantages of their simplicity and privacy-preserving properties, and in terms of their limited generalisability to more complex analytical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14895v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Busch-Moreno, Moritz U. G. Kraemer</dc:creator>
    </item>
    <item>
      <title>tsbootstrap: Enhancing Time Series Analysis with Advanced Bootstrapping Techniques</title>
      <link>https://arxiv.org/abs/2404.15227</link>
      <description>arXiv:2404.15227v1 Announce Type: new 
Abstract: In time series analysis, traditional bootstrapping methods often fall short due to their assumption of data independence, a condition rarely met in time-dependent data. This paper introduces tsbootstrap, a python package designed specifically to address this challenge. It offers a comprehensive suite of bootstrapping techniques, including Block, Residual, and advanced methods like Markov and Sieve Bootstraps, each tailored to respect the temporal dependencies in time series data. This framework not only enhances the accuracy of uncertainty estimation in time series analysis but also integrates seamlessly with the existing python data science ecosystem, making it an invaluable asset for researchers and practitioners in various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15227v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sankalp Gilda, Benedikt Heidrich, Franz Kiraly</dc:creator>
    </item>
    <item>
      <title>Simulation-Free Determination of Microstructure Representative Volume Element Size via Fisher Scores</title>
      <link>https://arxiv.org/abs/2404.15207</link>
      <description>arXiv:2404.15207v1 Announce Type: cross 
Abstract: A representative volume element (RVE) is a reasonably small unit of microstructure that can be simulated to obtain the same effective properties as the entire microstructure sample. Finite element (FE) simulation of RVEs, as opposed to much larger samples, saves computational expense, especially in multiscale modeling. Therefore, it is desirable to have a framework that determines RVE size prior to FE simulations. Existing methods select the RVE size based on when the FE-simulated properties of samples of increasing size converge with insignificant statistical variations, with the drawback that many samples must be simulated. We propose a simulation-free alternative that determines RVE size based only on a micrograph. The approach utilizes a machine learning model trained to implicitly characterize the stochastic nature of the input micrograph. The underlying rationale is to view RVE size as the smallest moving window size for which the stochastic nature of the microstructure within the window is stationary as the window moves across a large micrograph. For this purpose, we adapt a recently developed Fisher score-based framework for microstructure nonstationarity monitoring. Because the resulting RVE size is based solely on the micrograph and does not involve any FE simulation of specific properties, it constitutes an RVE for any property of interest that solely depends on the microstructure characteristics. Through numerical experiments of simple and complex microstructures, we validate our approach and show that our selected RVE sizes are consistent with when the chosen FE-simulated properties converge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15207v1</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0195232</arxiv:DOI>
      <arxiv:journal_reference>APL Mach. Learn. 2(2): 026101 (2024)</arxiv:journal_reference>
      <dc:creator>Wei Liu, Satyajit Mojumder, Wing Kam Liu, Wei Chen, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>A Gaussian process based approach for validation of multi-variable measurement systems: application to SAR measurement systems</title>
      <link>https://arxiv.org/abs/2211.12907</link>
      <description>arXiv:2211.12907v3 Announce Type: replace 
Abstract: Resource-efficient and robust validation of systems designed to measure a multi-dimensional parameter space is an unsolved problem as it would require millions of test permutations for comprehensive validation coverage. In the paper, an efficient and comprehensive validation approach based on a Gaussian Process (GP) model of the test system has been developed that can operate system-agnostically, avoids calibration to a fixed set of known validation benchmarks, and supports large configuration spaces. The approach consists of three steps that can be performed independently by different parties: 1) GP model creation, 2) model confirmation, and 3) targeted search for critical cases. It has been applied to two systems that measure specific absorption rate (SAR) for compliance testing of wireless devices and apply different SAR measurement methods: a probe-scanning system (per IEC/IEEE 62209-1528), and a static sensor-array system (per IEC 62209-3). The results demonstrate that the approach is practical, feasible, suitable for proving effective equivalence, and can be applied to any measurement method and implementation. The presented method is sufficiently general to be of value not only for SAR system validation, but also in a wide variety of applications that require critical, independent, and efficient validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12907v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. Bujard, E. Neufeld, M. Douglas, J. Wiart, N. Kuster</dc:creator>
    </item>
    <item>
      <title>Masked Transformer for Electrocardiogram Classification</title>
      <link>https://arxiv.org/abs/2309.07136</link>
      <description>arXiv:2309.07136v3 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformer for ECG data has not been fully realized, despite their widespread success in computer vision and natural language processing. In this work, we present Masked Transformer for ECG classification (MTECG), a simple yet effective method which significantly outperforms recent state-of-the-art algorithms in ECG classification. Our approach adapts the image-based masked autoencoders to self-supervised representation learning from ECG time series. We utilize a lightweight Transformer for the encoder and a 1-layer Transformer for the decoder. The ECG signal is split into a sequence of non-overlapping segments along the time dimension, and learnable positional embeddings are added to preserve the sequential information. We construct the Fuwai dataset comprising 220,251 ECG recordings with a broad range of diagnoses, annotated by medical experts, to explore the potential of Transformer. A strong pre-training and fine-tuning recipe is proposed from the empirical study. The experiments demonstrate that the proposed method increases the macro F1 scores by 3.4%-27.5% on the Fuwai dataset, 9.9%-32.0% on the PTB-XL dataset, and 9.4%-39.1% on a multicenter dataset, compared to the alternative methods. We hope that this study could direct future research on the application of Transformer to more ECG tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07136v3</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Xiaolin Diao, Yanni Huo, Yang Liu, Xiaohan Fan, Wei Zhao</dc:creator>
    </item>
    <item>
      <title>Approaches to biological species delimitation based on genetic and spatial dissimilarity</title>
      <link>https://arxiv.org/abs/2401.12126</link>
      <description>arXiv:2401.12126v3 Announce Type: replace-cross 
Abstract: The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered as well as new approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations showed that partial Mantel tests and mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12126v3</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele d'Angella, Christian Hennig</dc:creator>
    </item>
  </channel>
</rss>
