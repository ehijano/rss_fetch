<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 02:29:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dynamic Bayesian Item Response Model with Decomposition (D-BIRD): Modeling Cohort and Individual Learning Over Time</title>
      <link>https://arxiv.org/abs/2506.21723</link>
      <description>arXiv:2506.21723v1 Announce Type: new 
Abstract: We present D-BIRD, a Bayesian dynamic item response model for estimating student ability from sparse, longitudinal assessments. By decomposing ability into a cohort trend and individual trajectory, D-BIRD supports interpretable modeling of learning over time. We evaluate parameter recovery in simulation and demonstrate the model using real-world personalized learning data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21723v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hansol Lee, Jason B. Cho, David S. Matteson, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>Putting Skill as Nearly Indistinguishable from Noise: An Empirical Bayes Analysis of PGA Tour Performance</title>
      <link>https://arxiv.org/abs/2506.21822</link>
      <description>arXiv:2506.21822v1 Announce Type: new 
Abstract: We revisit a foundational question in golf analytics: how important are the core components of performance--driving, approach play, and putting--in explaining success on the PGA Tour? Building on Mark Broadie's strokes gained analyses, we use an empirical Bayes approach to estimate latent golfer skill and assess statistical significance using a multiple testing procedure that controls the false discovery rate. While tee-to-green skill shows clear and substantial differences across players, putting skill is both less variable and far less reliably estimable. Indeed, putting performance appears nearly indistinguishable from noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21822v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences</title>
      <link>https://arxiv.org/abs/2506.21921</link>
      <description>arXiv:2506.21921v1 Announce Type: new 
Abstract: Anomaly detection is the task of identifying rarely occurring (i.e. anormal or anomalous) samples that differ from almost all other samples in a dataset. As the patterns of anormal samples are usually not known a priori, this task is highly challenging. Consequently, anomaly detection lies between semi- and unsupervised learning. The detection of anomalies in sound data, often called 'ASD' (Anomalous Sound Detection), is a sub-field that deals with the identification of new and yet unknown effects in acoustic recordings. It is of great importance for various applications in Industry 4.0. Here, vibrational or acoustic data are typically obtained from standard sensor signals used for predictive maintenance. Examples cover machine condition monitoring or quality assurance to track the state of components or products. However, the use of intelligent algorithms remains a controversial topic. Management generally aims for cost-reduction and automation, while quality and maintenance experts emphasize the need for human expertise and comprehensible solutions. In this work, we present an anomaly detection approach specifically designed for spectrograms. The approach is based on statistical evaluations and is theoretically motivated. In addition, it features intrinsic explainability, making it particularly suitable for applications in industrial settings. Thus, this algorithm is of relevance for applications in which black-box algorithms are unwanted or unsuitable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21921v1</guid>
      <category>stat.AP</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicolas Thewes, Philipp Steinhauer, Patrick Trampert, Markus Pauly, Georg Schneider</dc:creator>
    </item>
    <item>
      <title>Identifying High-Risk Areas for Traffic Collisions in Montgomery, Maryland Using KDE and Spatial Autocorrelation Analysis</title>
      <link>https://arxiv.org/abs/2506.21930</link>
      <description>arXiv:2506.21930v1 Announce Type: new 
Abstract: Despite a global decline in motor vehicle crash fatalities due to improved research and road safety policies, road traffic injuries remain a significant public health concern. The World Health Organization 2023 report highlights that road traffic injuries are the leading cause of death among individuals aged 5-29, with over half of fatalities involving pedestrians, cyclists, and motorcyclists. This study addresses this critical issue by identifying high-risk areas in Montgomery County, Maryland, contributing to the global goal of halving road traffic deaths and injuries by 2030. Using Kernel Density Estimation (KDE) and spatial autocorrelation analysis, we estimate collision densities and identify hotspots for targeted interventions. Our findings reveal significant spatial clustering of traffic collisions, with distinct patterns in densely populated urban areas and rural regions, offering valuable insights for policymakers to enhance road safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21930v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav Liashkov</dc:creator>
    </item>
    <item>
      <title>Non-Parametric Time Between Events and Amplitude Methods for Monitoring Drought Characteristics</title>
      <link>https://arxiv.org/abs/2506.21970</link>
      <description>arXiv:2506.21970v1 Announce Type: new 
Abstract: Drought is a significant natural phenomenon with profound environmental, economic, and societal impacts. Effective monitoring of drought characteristics -- such as intensity, magnitude, and duration -- is crucial for resilience and mitigation strategies. This study proposes the use of non-parametric Time Between Events and Amplitude (TBEA) control charts for detecting changes in drought characteristics, specifically applying them to the Standardized Precipitation and Evapotranspiration Index. Aware of being non-exhaustive, we considered two non-parametric change-point control charts based on the Mann-Whitney and Kolmogorov-Smirnov statistics, respectively. We studied the in-control statistical performances of the change-point control charts in the time between events and amplitude framework through a simulation study. Furthermore, we assessed the coherence of the results obtained with a distribution-free upper sided Exponentially Weighted Moving Average control chart specifically designed for monitoring TBEA data. The findings suggest that the proposed methods may serve as valuable tools for climate resilience planning and water resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21970v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Scagliarini</dc:creator>
    </item>
    <item>
      <title>Linking climate and dengue in the Philippines using a two-stage Bayesian spatio-temporal model</title>
      <link>https://arxiv.org/abs/2506.22334</link>
      <description>arXiv:2506.22334v1 Announce Type: new 
Abstract: Dengue is an infectious disease which poses significant socioeconomic and disease burden in many tropical and subtropical regions of the world. This work aims to provide additional insight into the association between dengue and climate in the Philippines. We employ a two-stage modelling framework: the first stage fits climate models, while the second stage fits a health model that uses the climate predictions from the first stage as inputs. We postulate a Bayesian spatio-temporal model and use the integrated nested Laplace approximation (INLA) approach for inference. To account for the uncertainty in the climate models, we perform posterior sampling and then perform Bayesian model averaging to compute the final posterior estimates of second-stage model parameters. The results indicate that temperature is positively associated with dengue, although extremely hot conditions tend to have a negative effect. Moreover, the relationship between rainfall and dengue varies in space. In areas with uniform amounts of rainfall all year round, rainfall is negatively associated with dengue. In contrast, in regions with pronounced dry and wet season, rainfall shows a positive association with dengue. Finally, there remains unexplained structured variation in space and time after accounting for the impact of climate variables and other covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22334v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jun Villejo, Sara Martino, Janine Illian</dc:creator>
    </item>
    <item>
      <title>Personnel-adjustment for home run park effects in Major League Baseball</title>
      <link>https://arxiv.org/abs/2506.22350</link>
      <description>arXiv:2506.22350v1 Announce Type: new 
Abstract: In Major League Baseball, every ballpark is different, with different dimensions and climates. These differences make some ballparks more conducive to hitting home runs than others. Several factors conspire to make estimation of these differences challenging. Home runs are relatively rare, occurring in roughly 3\% of plate appearances. The quality of personnel and the frequency of batter-pitcher handedness combinations that appear in the thirty ballparks vary considerably. Because of asymmetries, effects due to ballpark can depend strongly on hitter handedness. We consider generalized linear mixed effects models based on the Poisson distribution for home runs. We use as our observational unit the combination of game and handedness-matchup. Our model allows for four theoretical mean home run frequency functions for each ballpark. We control for variation in personnel across games by constructing ``elsewhere'' measures of batter ability to hit home runs and pitcher tendency to give them up, using data from parks other than the one in which the response is observed. We analyze 13 seasons of data and find that the estimated home run frequencies adjusted to average personnel are substantially different from observed home run frequencies, leading to considerably different ballpark rankings than often appear in the media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22350v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason A. Osborne, Richard A. Levine</dc:creator>
    </item>
    <item>
      <title>Federated Item Response Theory Models</title>
      <link>https://arxiv.org/abs/2506.21744</link>
      <description>arXiv:2506.21744v1 Announce Type: cross 
Abstract: Item Response Theory (IRT) models have been widely used to estimate respondents' latent abilities and calibrate items' difficulty. Traditional IRT estimation requires all individual raw response data to be centralized in one place, thus potentially causing privacy issues. Federated learning is an emerging field in computer science and machine learning with added features of privacy protection and distributed computing. To integrate the advances from federated learning with modern psychometrics, we propose a novel framework, Federated Item Response Theory (IRT), to enable estimating traditional IRT models with additional privacy, allowing estimation in a distributed manner without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy similar to standard IRT estimation using popular R packages, while offering critical advantages: privacy protection and reduced communication costs. We also validate FedIRT's utility through a real-world exam dataset, demonstrating its effectiveness in realistic educational contexts. This new framework extends IRT's applicability to distributed settings, such as multi-school assessments, without sacrificing accuracy or security. To support practical adoption, we provide an open-ource R package, FedIRT, implementing the framework for the two-parameter logistic (2PL) and partial credit models (PCM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21744v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biying Zhou, Nanyu Luo, Feng Ji</dc:creator>
    </item>
    <item>
      <title>Simulated Intervention on Cross-Sectional Nested Data: Development of a Multilevel NIRA Approach</title>
      <link>https://arxiv.org/abs/2506.21991</link>
      <description>arXiv:2506.21991v1 Announce Type: cross 
Abstract: With the rise of the network perspective, researchers have made numerous important discoveries over the past decade by constructing psychological networks. Unfortunately, most of these networks are based on cross-sectional data, which can only reveal associations between variables but not their directional or causal relationships. Recently, the development of the nodeIdentifyR algorithm (NIRA) technique has provided a promising method for simulating causal processes based on cross-sectional network structures. However, this algorithm is not capable of handling cross-sectional nested data, which greatly limits its applicability. In response to this limitation, the present study proposes a multilevel extension of the NIRA algorithm, referred to as multilevel NIRA. We provide a detailed explanation of the algorithm's core principles and modeling procedures. Finally, we discuss the potential applications and practical implications of this approach, as well as its limitations and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21991v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Wu, Fei Wang</dc:creator>
    </item>
    <item>
      <title>Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings</title>
      <link>https://arxiv.org/abs/2506.22228</link>
      <description>arXiv:2506.22228v1 Announce Type: cross 
Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed investigations of cell-state transitions. Many biological processes unfold along continuous trajectories, yet it remains challenging to extract smooth, low-dimensional representations from inherently noisy, high-dimensional single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP, are widely used to embed high-dimensional single-cell data into low dimensions. But they often introduce undesirable distortions, resulting in misleading interpretations. Existing evaluation methods for NE algorithms primarily focus on separating discrete cell types rather than capturing continuous cell-state transitions, while dynamic modeling approaches rely on strong assumptions about cellular processes and specialized data. To address these challenges, we build on the Predictability-Computability-Stability (PCS) framework for reliable and reproducible data-driven discoveries. First, we systematically evaluate popular NE algorithms through empirical analysis, simulation, and theory, and reveal their key shortcomings, such as artifacts and instability. We then introduce NESS, a principled and interpretable machine learning approach to improve NE representations by leveraging algorithmic stability and to enable robust inference of smooth biological structures. NESS offers useful concepts, quantitative stability metrics, and efficient computational workflows to uncover developmental trajectories and cell-state transitions in single-cell data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent stem cell differentiation, organoid development, and multiple tissue-specific lineage trajectories. Across these diverse contexts, NESS consistently yields useful biological insights, such as identification of transitional and stable cell states and quantification of transcriptional dynamics during development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22228v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rong Ma, Xi Li, Jingyuan Hu, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Forecasting the future development in quality and value of professional football players</title>
      <link>https://arxiv.org/abs/2502.07528</link>
      <description>arXiv:2502.07528v2 Announce Type: replace 
Abstract: Transfers in professional football (soccer) are risky investments because of the large transfer fees and high risks involved. Although data-driven models can be used to improve transfer decisions, existing models focus on describing players' historical progress, leaving their future performance unknown. Moreover, recent developments have called for the use of explainable models combined with uncertainty quantification of predictions. This paper assesses explainable machine learning models based on predictive accuracy and uncertainty quantification methods for the prediction of the future development in quality and transfer value of professional football players. The predictive accuracy is studied by training the models to predict the quality and value of players one year ahead. This is carried out by training them on two data sets containing data-driven indicators describing the player quality and player value in historical settings. In general, the random forest model is found to be the most suitable model because it provides accurate predictions as well as an uncertainty quantification method that naturally arises from the bagging procedure of the random forest model. Additionally, this research shows that the development of player performance contains nonlinear patterns and interactions between variables, and that time series information can provide useful information for the modeling of player performance metrics. The resulting models can help football clubs make more informed, data-driven transfer decisions by forecasting player quality and transfer value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07528v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Koen W. van Arem, Floris Goes-Smit, Jakob S\"ohl</dc:creator>
    </item>
    <item>
      <title>Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth</title>
      <link>https://arxiv.org/abs/2502.20758</link>
      <description>arXiv:2502.20758v2 Announce Type: replace 
Abstract: We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single "correct" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20758v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Seyed Pouyan Mousavi Davoudi, Amin Gholami Davodi, Alireza Amiri-Margavi, Mahdi Jafari</dc:creator>
    </item>
    <item>
      <title>Experimentation for Homogenous Policy Change</title>
      <link>https://arxiv.org/abs/2101.12318</link>
      <description>arXiv:2101.12318v2 Announce Type: replace-cross 
Abstract: When the Stable Unit Treatment Value Assumption is violated and there is interference among units, there is not a uniquely defined Average Treatment Effect, and alternative estimands may be of interest. Among these are average unit-level differences in outcomes under different homogeneous treatment policies. We refer to such targets as Global Average Treatment Effects. We consider approaches to experimental design with multiple treatment conditions under partial interference and, given the estimand of interest, we show that difference-in-means estimators may perform better than correctly specified regression models in finite samples on root mean squared error for such targets. With errors correlated at the cluster level, we demonstrate that two-stage randomization procedures with intra-cluster correlation of treatment strictly between zero and one may dominate one-stage randomization designs on the same metric. Simulations illustrate performance of this approach; we consider an application to online experiments at Facebook.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.12318v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Molly Offer-Westort, Drew Dimmery</dc:creator>
    </item>
    <item>
      <title>A geometric approach in non-parametric Changepoint detection in circular data</title>
      <link>https://arxiv.org/abs/2403.00508</link>
      <description>arXiv:2403.00508v2 Announce Type: replace-cross 
Abstract: In many temporally ordered data sets, it is observed that the parameters of the underlying distribution change abruptly at unknown times. The detection of such changepoints is important for many applications. While this problem has been studied substantially in the linear data setup, not much work has been done for angular data. In this article, we utilize the intrinsic geometry of a torus to propose new non-parametric tests. First, we propose new tests for the existence of changepoint(s) in the concentration, and second, a test to detect mean direction and/or concentration. The limiting distributions of the test statistics are derived, and their powers are obtained using extensive simulation. It is seen that the tests have better power than the corresponding existing tests. The proposed methods have been implemented on three real-life data sets, revealing interesting insights. In particular, our method, when used to detect simultaneous changes in mean direction and concentration for hourly wind direction measurements of the cyclonic storm "Amphan," identified changepoints that could be associated with important meteorological events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00508v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v2 Announce Type: replace-cross 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. Design-based direct estimators are generally preferable because of their consistency, asymptotic normality, and reliance on fewer assumptions. However, when data are sparse at the desired area level, as is often the case when measuring rare events, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage towards the local average. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose two unit-level Bayesian models for small area estimation of rare event prevalence which use design-based direct estimates at a higher area level to increase consistency in aggregation. This model framework is designed to accommodate sparse data obtained from two-stage stratified cluster sampling, which is particularly relevant to applications in low and middle income countries. After introducing the model framework and its implementation, we conduct a simulation study to evaluate its properties and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 Demographic Health Surveys data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Risk Estimate under a Time-Varying Autoregressive Model for Data-Driven Reproduction Number Estimation</title>
      <link>https://arxiv.org/abs/2409.14937</link>
      <description>arXiv:2409.14937v3 Announce Type: replace-cross 
Abstract: COVID-19 pandemic has brought to the fore epidemiological models which, though describing a wealth of behaviors, have previously received little attention in the signal processing literature. In this work, a generalized time-varying autoregressive model is considered, encompassing, but not reducing to, a state-of-the-art model of viral epidemics propagation. The time-varying parameter of this model is estimated via the minimization of a penalized likelihood estimator. A major challenge is that the estimation accuracy strongly depends on hyperparameters fine-tuning. Without available ground truth, hyperparameters are selected by minimizing specifically designed data-driven oracles, used as proxy for the estimation error. Focusing on the time-varying autoregressive Poisson model, the Stein's Unbiased Risk Estimate formalism is generalized to construct asymptotically unbiased risk estimators based on the derivation of an original autoregressive counterpart of Stein's lemma. The accuracy of these oracles and of the resulting estimates are assessed through intensive Monte Carlo simulations on synthetic data. Then, elaborating on recent epidemiological models, a novel weekly scaled Poisson model is proposed, better accounting for intrinsic variability of the contamination while being robust to reporting errors. Finally, the overall data-driven procedure is particularized to the estimation of COVID-19 reproduction number demonstrating its ability to yield very consistent estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14937v3</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Pascal, Samuel Vaiter</dc:creator>
    </item>
    <item>
      <title>Adapting Probabilistic Risk Assessment for AI</title>
      <link>https://arxiv.org/abs/2504.18536</link>
      <description>arXiv:2504.18536v2 Announce Type: replace-cross 
Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which AI systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity bands, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. It introduces three methodological advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for lifecycle decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18536v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah</dc:creator>
    </item>
  </channel>
</rss>
