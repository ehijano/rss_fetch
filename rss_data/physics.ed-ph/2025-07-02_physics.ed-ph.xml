<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jul 2025 01:39:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Interviews to Equations: A Multi-Phase System Dynamics Model of Engineering Student Engagement</title>
      <link>https://arxiv.org/abs/2507.00382</link>
      <description>arXiv:2507.00382v1 Announce Type: new 
Abstract: This study presents a systematic approach for converting qualitative data into quantitative parameters within a system dynamics (SD) framework, focusing on modeling engineering student engagement. Although SD typically relies on numerical inputs, important "soft" factors such as motivation, confidence, and a sense of belonging have often been neglected due to the challenge of measurement. Semi-structured interviews were conducted with mechanical engineering students in a Learning Studio environment, capturing stories about hands-on coursework, peer support, and personal growth. Using inductive thematic analysis, frequent mentions of relevant factors were coded and converted into weighted parameters for a Vensim model. The resulting structure includes interconnected submodels illustrating how community cohesion influences motivation, which then affects learning outcomes and career goals. Simulation results show exponential growth in motivation, confidence, and sense of belonging over a sixteen-week period, alongside declines in negative factors like dissatisfaction. Introducing a logistic limit on belonging confirmed that, after social needs plateau, other positive aspects continue to improve. A scenario with a one-week delay in feedback loops reduced the rate of change but maintained the model's overall behavior. These findings align with educational theory, suggesting that community-driven interventions can enhance student engagement. This approach highlights the importance of capturing intangible, evolving student experiences in SD models. While additional validation with larger samples is necessary, the framework shows how incorporating qualitative insights into simulations can yield more actionable findings for educators and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00382v1</guid>
      <category>physics.ed-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed A. Alrizqi</dc:creator>
    </item>
    <item>
      <title>Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment</title>
      <link>https://arxiv.org/abs/2505.09438</link>
      <description>arXiv:2505.09438v2 Announce Type: replace 
Abstract: Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09438v2</guid>
      <category>physics.ed-ph</category>
      <category>cs.AI</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/6fmx-bsnl</arxiv:DOI>
      <dc:creator>Paul Tschisgale, Holger Maus, Fabian Kieser, Ben Kroehs, Stefan Petersen, Peter Wulff</dc:creator>
    </item>
  </channel>
</rss>
