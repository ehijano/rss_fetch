<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 02:32:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A computational loudness model for electrical stimulation with cochlear implants</title>
      <link>https://arxiv.org/abs/2501.17640</link>
      <description>arXiv:2501.17640v1 Announce Type: new 
Abstract: Cochlear implants (CIs) are devices that restore the sense of hearing in people with severe sensorineural hearing loss. An electrode array inserted in the cochlea bypasses the natural transducer mechanism that transforms mechanical sound waves into neural activity by artificially stimulating the auditory nerve fibers with electrical pulses. The perception of sounds is possible because the brain extracts features from this neural activity, and loudness is among the most fundamental perceptual features.
  A computational model that uses a three-dimensional (3D) representation of the peripheral auditory system of CI users was developed to predict categorical loudness from the simulated peripheral neural activity. In contrast, current state-of-the-art computational loudness models predict loudness from the electrical pulses with minimal parametrization of the electrode-nerve interface. In the proposed model, the spikes produced in a population of auditory nerve fibers were grouped by cochlear places, a physiological representation of the auditory filters in psychoacoustics, to be transformed into loudness contribution. Then, a loudness index was obtained with a spatiotemporal integration over this loudness contribution. This index served to define the simulated threshold of hearing (THL) and most comfortable loudness (MCL) levels resembling the growth function in CI users.
  The performance of real CI users in loudness summation experiments was also used to validate the computational model. These experiments studied the effect of stimulation rate, electrode separation and amplitude modulation. The proposed model provides a new set of perceptual features that can be used in computational frameworks for CIs and narrows the gap between simulations and the human peripheral neural activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17640v1</guid>
      <category>q-bio.NC</category>
      <category>eess.AS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Franklin Alvarez, Yixuan Zhang, Daniel Kipping, Waldo Nogueira</dc:creator>
    </item>
    <item>
      <title>Analysis of Neural Activation in Time-dependent Membrane Capacitance Models</title>
      <link>https://arxiv.org/abs/2501.17803</link>
      <description>arXiv:2501.17803v1 Announce Type: new 
Abstract: Most models of neurons incorporate a capacitor to account for the marked capacitive behavior exhibited by the cell membrane. However, such capacitance is widely considered constant, thereby neglecting the possible effects of time-dependent membrane capacitance on neural excitability. This study presents a modified formulation of a neuron model with time-dependent membrane capacitance and shows that action potentials can be elicited for certain capacitance dynamics. Our main results can be summarized as: (a) it is necessary to have significant and abrupt variations in the capacitance to generate action potentials; (b) certain simple and explicitly constructed capacitance profiles with strong variations do generate action potentials; (c) forcing abrupt changes in the capacitance too frequently may result in no action potentials. These findings can have great implications for the design of ultrasound-based or other neuromodulation strategies acting through transiently altering the membrane capacitance of neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17803v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\'ias Courdurier, Leonel E. Medina, Esteban Paduro</dc:creator>
    </item>
    <item>
      <title>Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?</title>
      <link>https://arxiv.org/abs/2501.17207</link>
      <description>arXiv:2501.17207v1 Announce Type: cross 
Abstract: Functional brain connectome is crucial for deciphering the neural mechanisms underlying cognitive functions and neurological disorders. Graph deep learning models have recently gained tremendous popularity in this field. However, their actual effectiveness in modeling the brain connectome remains unclear. In this study, we re-examine graph deep learning models based on four large-scale neuroimaging studies encompassing diverse cognitive and clinical outcomes. Surprisingly, we find that the message aggregation mechanism, a hallmark of graph deep learning models, does not help with predictive performance as typically assumed, but rather consistently degrades it. To address this issue, we propose a hybrid model combining a linear model with a graph attention network through dual pathways, achieving robust predictions and enhanced interpretability by revealing both localized and global neural connectivity patterns. Our findings urge caution in adopting complex deep learning models for functional brain connectome analysis, emphasizing the need for rigorous experimental designs to establish tangible performance gains and perhaps more importantly, to pursue improvements in model interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17207v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang</dc:creator>
    </item>
    <item>
      <title>Mechanisms for bump state localization in two-dimensional networks of leaky Integrate-and-Fire neurons</title>
      <link>https://arxiv.org/abs/2410.14256</link>
      <description>arXiv:2410.14256v2 Announce Type: replace 
Abstract: Networks of nonlocally coupled leaky Integrate-and-Fire neurons exhibit a variety of complex collective behaviors, such as partial synchronization, frequency or amplitude chimeras, solitary states and bump states. In particular, the bump states consist of one or many regions of asynchronous elements within a sea of subthreshold (quiescent) elements. The asynchronous domains travel in the network in a direction predetermined by the initial conditions. In the present study we investigate the occurrence of bump states in networks of leaky Integrate-and-Fire neurons in two-dimensions using nonlocal toroidal connectivity and we explore possible mechanisms for stabilizing the moving asynchronous domains. Our findings indicate that I) incorporating a refractory period can effectively anchor the position of these domains in the network, and II) the switching off of some randomly preselected nodes (i.e., making them permanently idle/inactive) can likewise contribute to stabilizing the positions of the asynchronous domains. In particular, in case II for large values of the coupling strength and a large percentage of idle elements, all nodes acquire different fixed (frozen) values in the quiescent region and oscillations cease throughout the network due to self-organization. For the special case of stationary bump states, we propose an analytical approach to predict their properties. This approach is based on the self-consistency argument and is valid for infinitely large networks. Case I is of particular biomedical interest in view of the importance of refractoriness for biological neurons, while case II can be biomedically relevant when designing therapeutic methods for stabilizing moving signals in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14256v2</guid>
      <category>q-bio.NC</category>
      <category>nlin.PS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Provata, J. Hizanidis, K. Anesiadis, O. E. Omel'chenko</dc:creator>
    </item>
    <item>
      <title>Attention when you need</title>
      <link>https://arxiv.org/abs/2501.07440</link>
      <description>arXiv:2501.07440v2 Announce Type: replace 
Abstract: Being attentive to task-relevant features can improve task performance, but paying attention comes with its own metabolic cost. Therefore, strategic allocation of attention is crucial in performing the task efficiently. This work aims to understand this strategy. Recently, de Gee et al. conducted experiments involving mice performing an auditory sustained attention-value task. This task required the mice to exert attention to identify whether a high-order acoustic feature was present amid the noise. By varying the trial duration and reward magnitude, the task allows us to investigate how an agent should strategically deploy their attention to maximize their benefits and minimize their costs. In our work, we develop a reinforcement learning-based normative model of the mice to understand how it balances attention cost against its benefits. The model is such that at each moment the mice can choose between two levels of attention and decide when to take costly actions that could obtain rewards. Our model suggests that efficient use of attentional resources involves alternating blocks of high attention with blocks of low attention. In the extreme case where the agent disregards sensory input during low attention states, we see that high attention is used rhythmically. Our model provides evidence about how one should deploy attention as a function of task utility, signal statistics, and how attention affects sensory evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07440v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lokesh Boominathan, Yizhou Chen, Matthew McGinley, Xaq Pitkow</dc:creator>
    </item>
    <item>
      <title>Federated Learning in Distributed Medical Databases: Meta-Analysis of Large-Scale Subcortical Brain Data</title>
      <link>https://arxiv.org/abs/1810.08553</link>
      <description>arXiv:1810.08553v4 Announce Type: replace-cross 
Abstract: At this moment, databanks worldwide contain brain images of previously unimaginable numbers. Combined with developments in data science, these massive data provide the potential to better understand the genetic underpinnings of brain diseases. However, different datasets, which are stored at different institutions, cannot always be shared directly due to privacy and legal concerns, thus limiting the full exploitation of big data in the study of brain disorders. Here we propose a federated learning framework for securely accessing and meta-analyzing any biomedical data without sharing individual information. We illustrate our framework by investigating brain structural relationships across diseases and clinical cohorts. The framework is first tested on synthetic data and then applied to multi-centric, multi-database studies including ADNI, PPMI, MIRIAD and UK Biobank, showing the potential of the approach for further applications in distributed analysis of multi-centric cohorts</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.08553v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Silva, Boris Gutman, Eduardo Romero, Paul M Thompson, Andre Altmann, Marco Lorenzi</dc:creator>
    </item>
    <item>
      <title>Exploring Biologically Inspired Mechanisms of Adversarial Robustness</title>
      <link>https://arxiv.org/abs/2405.00679</link>
      <description>arXiv:2405.00679v2 Announce Type: replace-cross 
Abstract: Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Understanding the biological mechanisms of robustness can pave the way towards building trust worthy and safe systems. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00679v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantin Holzhausen, Mia Merlid, H{\aa}kon Olav Torvik, Anders Malthe-S{\o}renssen, Mikkel Elle Lepper{\o}d</dc:creator>
    </item>
  </channel>
</rss>
