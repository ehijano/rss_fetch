<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 05:02:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Minimum Sliced Distance Estimation in a Class of Nonregular Econometric Models</title>
      <link>https://arxiv.org/abs/2412.05621</link>
      <description>arXiv:2412.05621v1 Announce Type: new 
Abstract: This paper proposes minimum sliced distance estimation in structural econometric models with possibly parameter-dependent supports. In contrast to likelihood-based estimation, we show that under mild regularity conditions, the minimum sliced distance estimator is asymptotically normally distributed leading to simple inference regardless of the presence/absence of parameter dependent supports. We illustrate the performance of our estimator on an auction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05621v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanqin Fan, Hyeonseok Park</dc:creator>
    </item>
    <item>
      <title>Property of Inverse Covariance Matrix-based Financial Adjacency Matrix for Detecting Local Groups</title>
      <link>https://arxiv.org/abs/2412.05664</link>
      <description>arXiv:2412.05664v1 Announce Type: new 
Abstract: In financial applications, we often observe both global and local factors that are modeled by a multi-level factor model. When detecting unknown local group memberships under such a model, employing a covariance matrix as an adjacency matrix for local group memberships is inadequate due to the predominant effect of global factors. Thus, to detect a local group structure more effectively, this study introduces an inverse covariance matrix-based financial adjacency matrix (IFAM) that utilizes negative values of the inverse covariance matrix. We show that IFAM ensures that the edge density between different groups vanishes, while that within the same group remains non-vanishing. This reduces falsely detected connections and helps identify local group membership accurately. To estimate IFAM under the multi-level factor model, we introduce a factor-adjusted GLASSO estimator to address the prevalent global factor effect in the inverse covariance matrix. An empirical study using returns from international stocks across 20 financial markets demonstrates that incorporating IFAM effectively detects latent local groups, which helps improve the minimum variance portfolio allocation performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05664v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minseog Oh, Donggyu Kim</dc:creator>
    </item>
    <item>
      <title>Convolution Mode Regression</title>
      <link>https://arxiv.org/abs/2412.05736</link>
      <description>arXiv:2412.05736v1 Announce Type: new 
Abstract: For highly skewed or fat-tailed distributions, mean or median-based methods often fail to capture the central tendencies in the data. Despite being a viable alternative, estimating the conditional mode given certain covariates (or mode regression) presents significant challenges. Nonparametric approaches suffer from the "curse of dimensionality", while semiparametric strategies often lead to non-convex optimization problems. In order to avoid these issues, we propose a novel mode regression estimator that relies on an intermediate step of inverting the conditional quantile density. In contrast to existing approaches, we employ a convolution-type smoothed variant of the quantile regression. Our estimator converges uniformly over the design points of the covariates and, unlike previous quantile-based mode regressions, is uniform with respect to the smoothing bandwidth. Additionally, the Convolution Mode Regression is dimension-free, carries no issues regarding optimization and preliminary simulations suggest the estimator is normally distributed in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05736v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo Schirmer Finn, Eduardo Horta</dc:creator>
    </item>
    <item>
      <title>Bundle Choice Model with Endogenous Regressors: An Application to Soda Tax</title>
      <link>https://arxiv.org/abs/2412.05794</link>
      <description>arXiv:2412.05794v1 Announce Type: new 
Abstract: This paper proposes a Bayesian factor-augmented bundle choice model to estimate joint consumption as well as the substitutability and complementarity of multiple goods in the presence of endogenous regressors. The model extends the two primary treatments of endogeneity in existing bundle choice models: (1) endogenous market-level prices and (2) time-invariant unobserved individual heterogeneity. A Bayesian sparse factor approach is employed to capture high-dimensional error correlations that induce taste correlation and endogeneity. Time-varying factor loadings allow for more general individual-level and time-varying heterogeneity and endogeneity, while the sparsity induced by the shrinkage prior on loadings balances flexibility with parsimony. Applied to a soda tax in the context of complementarities, the new approach captures broader effects of the tax that were previously overlooked. Results suggest that a soda tax could yield additional health benefits by marginally decreasing the consumption of salty snacks along with sugary drinks, extending the health benefits beyond the reduction in sugar consumption alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05794v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Sun</dc:creator>
    </item>
    <item>
      <title>Estimating Spillover Effects in the Presence of Isolated Nodes</title>
      <link>https://arxiv.org/abs/2412.05919</link>
      <description>arXiv:2412.05919v1 Announce Type: new 
Abstract: In estimating spillover effects under network interference, practitioners often use linear regression with either the number or fraction of treated neighbors as regressors. An often overlooked fact is that the latter is undefined for units without neighbors (``isolated nodes"). The common practice is to impute this fraction as zero for isolated nodes. This paper shows that such practice introduces bias through theoretical derivations and simulations. Causal interpretations of the commonly used spillover regression coefficients are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05919v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bora Kim</dc:creator>
    </item>
    <item>
      <title>Density forecast transformations</title>
      <link>https://arxiv.org/abs/2412.06092</link>
      <description>arXiv:2412.06092v1 Announce Type: new 
Abstract: The popular choice of using a $direct$ forecasting scheme implies that the individual predictions do not contain information on cross-horizon dependence. However, this dependence is needed if the forecaster has to construct, based on $direct$ density forecasts, predictive objects that are functions of several horizons ($e.g.$ when constructing annual-average growth rates from quarter-on-quarter growth rates). To address this issue we propose to use copulas to combine the individual $h$-step-ahead predictive distributions into a joint predictive distribution. Our method is particularly appealing to practitioners for whom changing the $direct$ forecasting specification is too costly. In a Monte Carlo study, we demonstrate that our approach leads to a better approximation of the true density than an approach that ignores the potential dependence. We show the superior performance of our method in several empirical examples, where we construct (i) quarterly forecasts using month-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly year-on-year $direct$ forecasts, and (iii) annual-average forecasts using quarter-on-quarter $direct$ forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06092v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Mogliani, Florens Odendahl</dc:creator>
    </item>
    <item>
      <title>Probabilistic Targeted Factor Analysis</title>
      <link>https://arxiv.org/abs/2412.06688</link>
      <description>arXiv:2412.06688v1 Announce Type: new 
Abstract: We develop a probabilistic variant of Partial Least Squares (PLS) we call Probabilistic Targeted Factor Analysis (PTFA), which can be used to extract common factors in predictors that are useful to predict a set of predetermined target variables. Along with the technique, we provide an efficient expectation-maximization (EM) algorithm to learn the parameters and forecast the targets of interest. We develop a number of extensions to missing-at-random data, stochastic volatility, and mixed-frequency data for real-time forecasting. In a simulation exercise, we show that PTFA outperforms PLS at recovering the common underlying factors affecting both features and target variables delivering better in-sample fit, and providing valid forecasts under contamination such as measurement error or outliers. Finally, we provide two applications in Economics and Finance where PTFA performs competitively compared with PLS and Principal Component Analysis (PCA) at out-of-sample forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06688v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Miguel C. Herculano, Santiago Montoya-Bland\'on</dc:creator>
    </item>
    <item>
      <title>Deriving Priorities From Inconsistent PCM using the Network Algorithms</title>
      <link>https://arxiv.org/abs/1510.04315</link>
      <description>arXiv:1510.04315v1 Announce Type: cross 
Abstract: In several multiobjective decision problems Pairwise Comparison Matrices (PCM) are applied to evaluate the decision variants. The problem that arises very often is the inconsistency of a given PCM. In such a situation it is important to approximate the PCM with a consistent one. The most common way is to minimize the Euclidean distance between the matrices. In the paper we consider the problem of minimizing the maximum distance. After applying the logarithmic transformation we are able to formulate the obtained subproblem as a Shortest Path Problem and solve it more efficiently. We analyze and completely characterize the form of the set of optimal solutions and provide an algorithm that results in a unique, Pareto-efficient solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:1510.04315v1</guid>
      <category>math.OC</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10479-018-2888-x</arxiv:DOI>
      <dc:creator>Marcin Anholcer, Janos F\"ul\"op</dc:creator>
    </item>
    <item>
      <title>Optimizing Returns from Experimentation Programs</title>
      <link>https://arxiv.org/abs/2412.05508</link>
      <description>arXiv:2412.05508v1 Announce Type: cross 
Abstract: Experimentation in online digital platforms is used to inform decision making. Specifically, the goal of many experiments is to optimize a metric of interest. Null hypothesis statistical testing can be ill-suited to this task, as it is indifferent to the magnitude of effect sizes and opportunity costs. Given access to a pool of related past experiments, we discuss how experimentation practice should change when the goal is optimization. We survey the literature on empirical Bayes analyses of A/B test portfolios, and single out the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which treats experimentation as a constrained optimization problem. We show that the framework can be solved with dynamic programming and implemented by appropriately tuning $p$-value thresholds. Furthermore, we develop several extensions of the A/B Testing Problem and discuss the implications of these results on experimentation programs in industry. For example, under no-cost assumptions, firms should be testing many more ideas, reducing test allocation sizes, and relaxing $p$-value thresholds away from $p = 0.05$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05508v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Sudijono, Simon Ejdemyr, Apoorva Lal, Martin Tingley</dc:creator>
    </item>
    <item>
      <title>Policy Choice in Time Series by Empirical Welfare Maximization</title>
      <link>https://arxiv.org/abs/2205.03970</link>
      <description>arXiv:2205.03970v4 Announce Type: replace 
Abstract: This paper develops a novel method for policy choice in a dynamic setting where the available data is a multi-variate time series. Building on the statistical treatment choice framework, we propose Time-series Empirical Welfare Maximization (T-EWM) methods to estimate an optimal policy rule by maximizing an empirical welfare criterion constructed using nonparametric potential outcome time series. We characterize conditions under which T-EWM consistently learns a policy choice that is optimal in terms of conditional welfare given the time-series history. We derive a nonasymptotic upper bound for conditional welfare regret. To illustrate the implementation and uses of T-EWM, we perform simulation studies and apply the method to estimate optimal restriction rules against Covid-19.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.03970v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toru Kitagawa, Weining Wang, Mengshan Xu</dc:creator>
    </item>
    <item>
      <title>Statistical Tests for Replacing Human Decision Makers with Algorithms</title>
      <link>https://arxiv.org/abs/2306.11689</link>
      <description>arXiv:2306.11689v2 Announce Type: replace 
Abstract: This paper proposes a statistical framework of using artificial intelligence to improve human decision making. The performance of each human decision maker is benchmarked against that of machine predictions. We replace the diagnoses made by a subset of the decision makers with the recommendation from the machine learning algorithm. We apply both a heuristic frequentist approach and a Bayesian posterior loss function approach to abnormal birth detection using a nationwide dataset of doctor diagnoses from prepregnancy checkups of reproductive age couples and pregnancy outcomes. We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11689v2</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong, Ke Tang, Jingyuan Wang</dc:creator>
    </item>
    <item>
      <title>One-step smoothing splines instrumental regression</title>
      <link>https://arxiv.org/abs/2307.14867</link>
      <description>arXiv:2307.14867v4 Announce Type: replace 
Abstract: We extend nonparametric regression smoothing splines to a context where there is endogeneity and instrumental variables are available. Unlike popular existing estimators, the resulting estimator is one-step and relies on a unique regularization parameter. We derive rates of the convergence for the estimator and its first derivative, which are uniform in the support of the endogenous variable. We also address the issue of imposing monotonicity in estimation and extend the approach to a partly linear model. Simulations confirm the good performances of our estimator compared to two-step procedures. Our method yields economically sensible results when used to estimate Engel curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14867v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jad Beyhum, Elia Lapenta, Pascal Lavergne</dc:creator>
    </item>
    <item>
      <title>Estimating Effects of Long-Term Treatments</title>
      <link>https://arxiv.org/abs/2308.08152</link>
      <description>arXiv:2308.08152v2 Announce Type: replace 
Abstract: Estimating the effects of long-term treatments through A/B testing is challenging. Treatments, such as updates to product functionalities, user interface designs, and recommendation algorithms, are intended to persist within the system for a long duration of time after their initial launches. However, due to the constraints of conducting long-term experiments, practitioners often rely on short-term experimental results to make product launch decisions. It remains open how to accurately estimate the effects of long-term treatments using short-term experimental data. To address this question, we introduce a longitudinal surrogate framework that decomposes the long-term effects into functions based on user attributes, short-term metrics, and treatment assignments. We outline identification assumptions, estimation strategies, inferential techniques, and validation methods under this framework. Empirically, we demonstrate that our approach outperforms existing solutions by using data from two real-world experiments, each involving more than a million users on WeChat, one of the world's largest social networking platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08152v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Huang (Jingjing), Chen Wang (Jingjing), Yuan Yuan (Jingjing), Jinglong Zhao (Jingjing),  Brocco (Jingjing),  Zhang</dc:creator>
    </item>
    <item>
      <title>kendallknight: An R Package for Efficient Implementation of Kendall's Correlation Coefficient Computation</title>
      <link>https://arxiv.org/abs/2408.09618</link>
      <description>arXiv:2408.09618v5 Announce Type: replace-cross 
Abstract: The kendallknight package introduces an efficient implementation of Kendall's correlation coefficient computation, significantly improving the processing time for large datasets without sacrificing accuracy. The kendallknight package, following Knight (1966) and posterior literature, reduces the computational complexity resulting in drastic reductions in computation time, transforming operations that would take minutes or hours into milliseconds or minutes, while maintaining precision and correctly handling edge cases and errors. The package is particularly advantageous in econometric and statistical contexts where rapid and accurate calculation of Kendall's correlation coefficient is desirable. Benchmarks demonstrate substantial performance gains over the base R implementation, especially for large datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09618v5</guid>
      <category>stat.CO</category>
      <category>cs.DS</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda</dc:creator>
    </item>
  </channel>
</rss>
