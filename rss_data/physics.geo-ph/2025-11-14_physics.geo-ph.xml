<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.geo-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.geo-ph</link>
    <description>physics.geo-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.geo-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Lithological Controls on the Permeability of Geologic Faults: Surrogate Modeling and Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2511.09674</link>
      <description>arXiv:2511.09674v1 Announce Type: new 
Abstract: Fault zones exhibit complex and heterogeneous permeability structures influenced by stratigraphic, compositional, and structural factors, making them critical yet uncertain components in subsurface flow modeling. In this study, we investigate how lithological controls influence fault permeability using the PREDICT framework: a probabilistic workflow that couples stochastic fault geometry generation, physically constrained material placement, and flow-based upscaling. The flow-based upscaling step, however, is a very computationally expensive component of the workflow and presents a major bottleneck that makes global sensitivity analysis (GSA) intractable, as it requires millions of model evaluations. To overcome this challenge, we develop a neural network surrogate to emulate the flow-based upscaling step. This surrogate model dramatically reduces the computational cost while maintaining high accuracy, thereby making GSA feasible. The surrogate-model-enabled GSA reveals new insights into the effects of lithological controls on fault permeability. In addition to identifying dominant parameters and negligible ones, the analysis uncovers significant nonlinear interactions between parameters that cannot be captured by traditional local sensitivity methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09674v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Lu, Llu{\i}s Salo-Salgado, Ruben Juanes</dc:creator>
    </item>
    <item>
      <title>Pervasive Label Errors in Seismological Machine Learning Datasets</title>
      <link>https://arxiv.org/abs/2511.09805</link>
      <description>arXiv:2511.09805v1 Announce Type: new 
Abstract: The recent boom in artificial intelligence and machine learning has been powered by large datasets with accurate labels, combined with algorithmic advances and efficient computing. The quality of data can be a major factor in determining model performance. Here, we detail observations of commonly occurring errors in popular seismological machine learning datasets. We used an ensemble of available deep learning models PhaseNet and EQTransformer to evaluate the dataset labels and found four types of errors ranked from most prevalent to least prevalent: (1) unlabeled earthquakes; (2) noise samples that contain earthquakes; (3) inaccurately labeled arrival times, and (4) absent earthquake signals. We checked a total of 8.6 million examples from the following datasets: Iquique, ETHZ, PNW, TXED, STEAD, INSTANCE, AQ2009, and CEED. The average error rate across all datasets is 3.9 %, ranging from nearly zero to 8 % for individual datasets. These faulty data and labels are likely to degrade model training and performance. By flagging these errors, we aim to increase the quality of the data used to train machine learning models, especially for the measurement of arrival times, and thereby to improve the reliability of the models. We present a companion list of examples that contain problems, aiming to integrate them into training routines so that only the reliable data is used for training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09805v1</guid>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Leonardo Aguilar Suarez, Gregory Beroza</dc:creator>
    </item>
    <item>
      <title>2.5D Transformer: An Efficient 3D Seismic Interpolation Method without Full 3D Training</title>
      <link>https://arxiv.org/abs/2511.10033</link>
      <description>arXiv:2511.10033v1 Announce Type: new 
Abstract: Transformer has emerged as a powerful deep-learning technique for two-dimensional (2D) seismic data interpolation, owing to its global modeling ability. However, its core operation introduces heavy computational burden due to the quadratic complexity, hindering its further application to higher-dimensional data. To achieve Transformer-based three-dimensional (3D) seismic interpolation, we propose a 2.5-dimensional Transformer network (T-2.5D) that adopts a cross-dimensional transfer learning (TL) strategy, so as to adapt the 2D Transformer encoders to 3D seismic data. The proposed T-2.5D is mainly composed of 2D Transformer encoders and 3D seismic dimension adapters (SDAs). Each 3D SDA is placed before a Transformer encoder to learn spatial correlation information across seismic lines. The proposed cross-dimensional TL strategy comprises two stages: 2D pre-training and 3D fine-tuning. In the first stage, we optimize the 2D Transformer encoders using a large amount of 2D data patches. In the second stage, we freeze the 2D Transformer encoders and fine-tune the 3D SDAs using limited 3D data volumes. Extensive experiments on multiple datasets are conducted to assess the effectiveness and efficiency of T-2.5D. Experimental results demonstrate that the proposed method achieves comparable performance to that of full 3D Transformer at a significantly low cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10033v1</guid>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changxin Wei, Xintong Dong, Xinyang Wang</dc:creator>
    </item>
    <item>
      <title>Fundamentals of interior modelling and challenges in the interpretation of observed rocky exoplanets</title>
      <link>https://arxiv.org/abs/2511.10269</link>
      <description>arXiv:2511.10269v1 Announce Type: cross 
Abstract: Most our knowledge about rocky exoplanets is based on their measure of mass and radius. These two parameters are routinely measured and are used to categorise different populations of observed exoplanets. They are also tightly linked to the planet's properties, in particular those of the interior. As such they offer the unique opportunity to interpret the observations and potentially infer the planet's chemistry and structure. Required for the interpretation are models of planetary interiors, calculated a priori, constrained using other available data, and based on the physiochemical properties of mineralogical phases. This article offers an overview of the current knowledge about exoplanet interiors, the fundamental aspects and tools for interior modelling and how to improve the contraints on the models, along with a discussion on the sources of uncertainty. The origin and fate of volatiles, and their role in planetary evolution is discussed. The chemistry and structure of planetary interiors have a pivotal role in the thermal evolution of planets and the development of large scale properties that might become observables with future space missions and ground-based surveys. As such, having reliable and well constrained interior models is of the utmost importance for the advancement of the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10269v1</guid>
      <category>astro-ph.EP</category>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Baumeister, Francesca Miozzi, Claire Marie Guimond, Marie-Luise Steinmeyer, Caroline Dorn, Shun-Ichiro Karato, Emeline Bolmont, Alexandre Revol, Alexander Thamm, Lena Noack</dc:creator>
    </item>
    <item>
      <title>Learning parameter-dependent shear viscosity from data, with application to sea and land ice</title>
      <link>https://arxiv.org/abs/2511.10452</link>
      <description>arXiv:2511.10452v1 Announce Type: cross 
Abstract: Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10452v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.flu-dyn</category>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo G. de Diego, Georg Stadler</dc:creator>
    </item>
    <item>
      <title>DRUM: Diffusion-based runoff model for probabilistic flood forecasting</title>
      <link>https://arxiv.org/abs/2412.11942</link>
      <description>arXiv:2412.11942v2 Announce Type: replace 
Abstract: Extreme floods pose escalating risks in a changing climate, yet forecasting remains challenging due to peak flow underestimation and high uncertainty. We introduce DRUM, a diffusion-based probabilistic deep learning approach that advances extreme flood forecasting across representative basins in the contiguous United States. DRUM outperforms state-of-the-art benchmarks, enhancing nowcasting skill for the top 0.1% of flows in 72.3% of studied basins. Under operational scenarios, DRUM extends reliable lead times by nearly a full day for 20- and 50-year floods. When evaluated with measured precipitation, an ideal condition, recall improves by 0.3-0.4 and the early warning window extends by 2.3 days for 50-year floods. The enhancement potential varies regionally, with precipitation-driven flood zones in the eastern and northwestern U.S. benefiting most, gaining 3-7 days in lead time. These findings highlight the transformative potential of diffusion models as a cutting-edge generative AI technique for advancing hydrology and broader Earth system sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11942v2</guid>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1029/2025GL115705</arxiv:DOI>
      <dc:creator>Zhigang Ou, Congyi Nai, Baoxiang Pan, Yi Zheng, Chaopeng Shen, Peishi Jiang, Xingcai Liu, Qiuhong Tang, Wenqing Li, Ming Pan</dc:creator>
    </item>
  </channel>
</rss>
