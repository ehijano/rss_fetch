<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 02:52:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Footstep recognition as people identification: A Systematic literature review</title>
      <link>https://arxiv.org/abs/2411.16688</link>
      <description>arXiv:2411.16688v1 Announce Type: new 
Abstract: Footstep recognition is a relatively new biometric which aims to discriminate people using walking characteristics. There are several feature and technology have been adopted in various research. This study will attempt to show a comparative technology and feature which is offered each previous related works. We performed a broad manually search to find SLRs published in the time period 1st January 2006 to 30th November 2018. Our broad search found 12 SLRs articles refer to 3 similar technology and 5 cluster feature. In over time, the number of published footstep recognition has increased, especially in conference publications. The differences in footsteps can be known from the power spectral density of sounds and vibrations generated by footsteps. Every footstep of the human has a certain density of frequency, either from density of sounds or vibrations generated. To improve accurately of the result, this paper suggests furthermore research to combining several measuring sensor and data processing method</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16688v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arif Rachmat</dc:creator>
    </item>
    <item>
      <title>Using virtual reality to enhance mobility, safety, and equity for persons with vision loss in urban environments</title>
      <link>https://arxiv.org/abs/2411.16916</link>
      <description>arXiv:2411.16916v1 Announce Type: new 
Abstract: This study explores the use of virtual reality (VR) as an innovative tool to enhance awareness, acceptance, and understanding of accessibility for persons with vision loss (VL). Through a VR-based workshop developed in collaboration with New York City's Department Of Transportation, participants experienced immersive simulations of VL and and related immersive mobility challenges. The methodology included pre- and post-intervention questionnaires, assessing changes in participants' knowledge, confidence, and perception. Participants included urban planners, designers, and architects. Results showed a significant increase in awareness of VL-related challenges that affect design guidelines, as well as improved confidence in addressing such challenges. Participants also expressed strong support for VR as a pedagogical tool, noting its potential for reshaping professional practices, improving capacity building, and enhancing inclusive design. The study demonstrates the effectiveness of VR as an experiential learning platform, fostering empathy and a long-term commitment to integrating VL considerations into urban design. These findings highlight the transformative potential of VR in advancing equity and accessibility in urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16916v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>F. S. Ricci, C. K. Ukegbu, A. Krassner, S. Hazarika, J. White, M. Porfiri, J. R. Rizzo</dc:creator>
    </item>
    <item>
      <title>Trinity: Synchronizing Verbal, Nonverbal, and Visual Channels to Support Academic Oral Presentation Delivery</title>
      <link>https://arxiv.org/abs/2411.17015</link>
      <description>arXiv:2411.17015v1 Announce Type: new 
Abstract: Academic Oral Presentation (AOP) allows English-As-Foreign-Language (EFL) students to express ideas, engage in academic discourse, and present research findings. However, while previous efforts focus on training efficiency or speech assistance, EFL students often face the challenge of seamlessly integrating verbal, nonverbal, and visual elements into their presentations to avoid coming across as monotonous and unappealing. Based on a need-finding survey, a design study, and an expert interview, we introduce Trinity, a hybrid mobile-centric delivery support system that provides guidance for multichannel delivery on-the-fly. On the desktop side, Trinity facilitates script refinement and offers customizable delivery support based on large language models (LLMs). Based on the desktop configuration, Trinity App enables a remote mobile visual control, multi-level speech pace modulation, and integrated delivery prompts for synchronized delivery. A controlled between-subject user study suggests that Trinity effectively supports AOP delivery and is perceived as significantly more helpful than baselines, without excessive cognitive load.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17015v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Wu, Shengxin Li, Shizhen Zhang, Xingbo Wang, Quan Li</dc:creator>
    </item>
    <item>
      <title>Exploring the Impact of Anthropomorphism in Role-Playing AI Chatbots on Media Dependency: A Case Study of Xuanhe AI</title>
      <link>https://arxiv.org/abs/2411.17157</link>
      <description>arXiv:2411.17157v1 Announce Type: new 
Abstract: Powered by large language models, the conversational capabilities of AI have seen significant improvements. In this context, a series of role-playing AI chatbots have emerged, exhibiting a strong tendency toward anthropomorphism, such as conversing like humans, possessing personalities, and fulfilling social and companionship functions. Informed by media dependency theory in communication studies, this work hypothesizes that a higher level of anthropomorphism of the role-playing chatbots will increase users' media dependency (i.e., people will depend on media that meets their needs and goals). Specifically, we conducted a user study on a Chinese role-playing chatbot platform, Xuanhe AI, selecting four representative chatbots as research targets. We invited 149 users to interact with these chatbots over a period. A questionnaire survey revealed a significant positive correlation between the degree of anthropomorphism in role-playing chatbots and users' media dependency, with user satisfaction mediating this relationship. Next, based on the quantitative results, we conducted semi-structured interviews with ten users to further understand the factors that deterred them from depending on anthropomorphic chatbots. In conclusion, this work has provided empirical insights for the design of role-playing AI chatbots and deepened the understanding of how users engage with conversational AI over a longer period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17157v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiufang Yu, Xingyu Lan</dc:creator>
    </item>
    <item>
      <title>The Role of Urban Designers in the Era of AIGC: An Experimental Study Based on Public Participation</title>
      <link>https://arxiv.org/abs/2411.17194</link>
      <description>arXiv:2411.17194v1 Announce Type: new 
Abstract: This study explores the application of Artificial Intelligence Generated Content (AIGC) technology in urban planning and design, with a particular focus on its impact on placemaking and public participation. By utilizing natural language pro-cessing and image generation models such as Stable Diffusion, AIGC enables efficient transformation from textual descriptions to visual representations, advancing the visualization of urban spatial experiences. The research examines the evolving role of designers in participatory planning processes, specifically how AIGC facilitates their transition from traditional creators to collaborators and facilitators, and the implications of this shift on the effectiveness of public engagement. Through experimental evaluation, the study assesses the de-sign quality of urban pocket gardens generated under varying levels of designer involvement, analyzing the influence of de-signers on the aesthetic quality and contextual relevance of AIGC outputs. The findings reveal that designers significantly improve the quality of AIGC-generated designs by providing guidance and structural frameworks, highlighting the substantial potential of human-AI collaboration in urban design. This research offers valuable insights into future collaborative approaches between planners and AIGC technologies, aiming to integrate technological advancements with professional practice to foster sustainable urban development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17194v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Mo, Keyi Liu, Qi Tian, Dengyun Li, Liyan Xu, Junyan Ye</dc:creator>
    </item>
    <item>
      <title>Metaverse Innovation Canvas: A Tool for Extended Reality Product/Service Development</title>
      <link>https://arxiv.org/abs/2411.17541</link>
      <description>arXiv:2411.17541v1 Announce Type: new 
Abstract: This study investigated the factors contributing to the failure of augmented reality (AR) and virtual reality (VR) startups in the emerging metaverse landscape. Through an in-depth analysis of 29 failed AR/VR startups from 2016 to 2022, key pitfalls were identified, such as a lack of scalability, poor usability, unclear value propositions, and the failure to address specific user problems. Grounded in these findings, we developed the Metaverse Innovation Canvas (MIC) a tailored business ideation framework for XR products and services. The canvas guides founders to define user problems, articulate unique XR value propositions, evaluate usability factors such as the motion-based interaction load, consider social/virtual economy opportunities, and plan for long term scalability. Unlike generalized models, specialized blocks prompt the consideration of critical XR factors from the outset. The canvas was evaluated through expert testing with startup consultants on five failed venture cases. The results highlighted the tool's effectiveness in surfacing overlooked usability issues and technology constraints upfront, enhancing the viability of future metaverse startups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17541v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amir Reza Asadi, Mohamad Saraee, Azadeh Mohammadi</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust Dynamics</title>
      <link>https://arxiv.org/abs/2411.17589</link>
      <description>arXiv:2411.17589v1 Announce Type: new 
Abstract: Introduction: The use of chatbots is becoming increasingly important across various aspects of daily life. However, the privacy concerns associated with these communications have not yet been thoroughly addressed. The aim of this study was to investigate user awareness of privacy risks in chatbot interactions, the privacy-preserving behaviours users practice, and how these behaviours relate to their awareness of privacy threats, even when no immediate threat is perceived. Methods: We developed a novel "privacy-safe" setup to analyse user behaviour under the guarantees of anonymization and non-sharing. We employed a mixed-methods approach, starting with the quantification of broader trends by coding responses, followed by conducting a qualitative content analysis to gain deeper insights. Results: Overall, there was a substantial lack of understanding among users about how chatbot providers handle data (27% of the participants) and the basics of privacy risks (76% of the participants). Older users, in particular, expressed fears that chatbot providers might sell their data. Moreover, even users with privacy knowledge do not consistently exhibit privacy-preserving behaviours when assured of transparent data processing by chatbots. Notably, under-protective behaviours were observed among more expert users. Discussion: These findings highlight the need for a strategic approach to enhance user education on privacy concepts to ensure informed decision when interacting with chatbot technology. This includes the development of tools to help users monitor and control the information they share with chatbots</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17589v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Ive, Vishal Yadav, Mariia Ignashina, Matthew Rand, Paulina Bondaronek</dc:creator>
    </item>
    <item>
      <title>DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</title>
      <link>https://arxiv.org/abs/2411.16729</link>
      <description>arXiv:2411.16729v1 Announce Type: cross 
Abstract: Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage, approximately 2.4 times, and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16729v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <category>eess.AS</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Zhang, Siyuan Zhao, Naye Ji, Zhaohan Wang, Jingmei Wu, Fuxing Gao, Zhenqing Ye, Leyao Yan, Lanxin Dai, Weidong Geng, Xin Lyu, Bozuo Zhao, Dingguo Yu, Hui Du, Bin Hu</dc:creator>
    </item>
    <item>
      <title>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</title>
      <link>https://arxiv.org/abs/2411.17465</link>
      <description>arXiv:2411.17465v1 Announce Type: cross 
Abstract: Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17465v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou</dc:creator>
    </item>
    <item>
      <title>Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence</title>
      <link>https://arxiv.org/abs/2404.00029</link>
      <description>arXiv:2404.00029v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has the potential to significantly enhance human performance across various domains. Ideally, collaboration between humans and AI should result in complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the principle and the application of complementarity. Therefore, we develop a general concept of complementarity and formalize its theoretical potential as well as the actual realized effect in decision-making situations. Moreover, we identify information and capability asymmetry as the two key sources of complementarity. Finally, we illustrate the impact of each source on complementarity potential and effect in two empirical studies. Our work provides researchers with a comprehensive theoretical foundation of human-AI complementarity in decision-making and demonstrates that leveraging these sources constitutes a viable pathway towards designing effective human-AI collaboration, i.e., the realization of CTP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00029v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Patrick Hemmer, Max Schemmer, Niklas K\"uhl, Michael V\"ossing, Gerhard Satzger</dc:creator>
    </item>
    <item>
      <title>ActSonic: Recognizing Everyday Activities from Inaudible Acoustic Wave Around the Body</title>
      <link>https://arxiv.org/abs/2404.13924</link>
      <description>arXiv:2404.13924v3 Announce Type: replace 
Abstract: We present ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses that can recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing) from inaudible acoustic waves around the body. It requires only a pair of miniature speakers and microphones mounted on each hinge of the eyeglasses to emit ultrasonic waves, creating an acoustic aura around the body. The acoustic signals are reflected based on the position and motion of various body parts, captured by the microphones, and analyzed by a customized self-supervised deep learning framework to infer the performed activities on a remote device such as a mobile phone or cloud server. ActSonic was evaluated in user studies with 19 participants across 19 households to track its efficacy in everyday activity recognition. Without requiring any training data from new users (leave-one-participant-out evaluation), ActSonic detected 27 activities, achieving an average F1-score of 86.6% in fully unconstrained scenarios and 93.4% in prompted settings at participants' homes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13924v3</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3699752</arxiv:DOI>
      <dc:creator>Saif Mahmud, Vineet Parikh, Qikang Liang, Ke Li, Ruidong Zhang, Ashwin Ajit, Vipin Gunda, Devansh Agarwal, Fran\c{c}ois Guimbreti\`ere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Designing the virtual CAT: A digital tool for algorithmic thinking assessment in compulsory education</title>
      <link>https://arxiv.org/abs/2408.01263</link>
      <description>arXiv:2408.01263v3 Announce Type: replace 
Abstract: Algorithmic thinking (AT) is a critical skill in today's digital society, and it is indispensable not only in computer science-related fields but also in everyday problem-solving. As a foundational component of digital education and literacy, fostering AT skills is increasingly relevant for all students and should become a standard part of compulsory education. However, successfully integrating AT into formal education requires effective teaching strategies and robust and scalable assessment procedures. In this paper, we present the design and development process of the virtual Cross Array Task (CAT), a digital adaptation of an unplugged assessment activity aimed at evaluating algorithmic skills in Swiss compulsory education. The development process followed iterative design cycles, incorporating expert evaluations to refine the tool's usability, accessibility and functionality. A participatory design study played a dual role in shaping the platform. First, it gathered valuable insights from end users, including students and teachers, to ensure the tool's relevance and practicality in classroom settings. Second, it facilitated the collection and preliminary analysis of data related to students' AT skills, providing an initial evaluation of the tool's assessment capabilities across various developmental stages. This was achieved through a pilot study involving a diverse group of students aged 4 to 12, spanning preschool to lower secondary school levels. The resulting instrument features multilingual support and includes both gesture-based and visual block-based programming interfaces, making it accessible to a broad range of learners. Findings from the pilot study demonstrate the platform's usability and accessibility, as well as its suitability for assessing AT skills, with preliminary results showing its ability to cater to diverse age groups and educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01263v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Adorni, Alberto Piatti</dc:creator>
    </item>
    <item>
      <title>Noninvasive Extraction of Maternal and Fetal Electrocardiograms Using Progressive Periodic Source Peel-off</title>
      <link>https://arxiv.org/abs/2406.01281</link>
      <description>arXiv:2406.01281v4 Announce Type: replace-cross 
Abstract: Abdominal electrocardiogram (AECG) gives a safe and non-invasive way to monitor fetal well-being during pregnancy using surface electrodes. However, it is challenging to extract weak fetal ECG (fECG) from the AECG recordings with larger maternal ECG (mECG) and external noises. In this study, we introduce a novel progressive periodic source peel-off (PPSP) method for extracting periodic ECG sources from multi-channel AECG recordings, including three main modules: 1) A periodic constrained FastICA (PCFICA) module with ECG physiology-informed constraints for extracting precise ECG spike trains, 2) A singular value decomposition module for estimating ECG waveforms, and 3) A peel-off strategy that facilitates to discern weak fECG source by eliminating previously separated sources or noises. The performance of the PPSP method was examined on two public databases, synthetic data and our clinical data. For extracting fECG spike trains, our PPSP method achieved an F1-scores of 99.59% on public data, 99.50% on synthetic data at the highest noise level. It further yielded the lowest RMSE of fetal heart rate of 6.20% on clinical data. It significantly outperformed other state-of-the-art methods on any set of data (p &lt; 0.05). This study demonstrated effectiveness of the PPSP method for extracting and separating mECG and weak fECG signals, with high precision especially at high noise levels. Our study promotes noninvasive measurement and intelligent monitoring of both fetal and maternal heart activities towards advanced healthcare in perinatal medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01281v4</guid>
      <category>physics.med-ph</category>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Li, Xuanyu Luo, Haowen Zhao, Jiawen Cui, Yangfan She, Dongfang Li, Lai Jiang, Xu Zhang</dc:creator>
    </item>
    <item>
      <title>Self-Centering 3-DoF Feet Controller for Hands-Free Locomotion Control in Telepresence and Virtual Reality</title>
      <link>https://arxiv.org/abs/2408.02319</link>
      <description>arXiv:2408.02319v3 Announce Type: replace-cross 
Abstract: We present a novel seated feet controller for handling 3-DoF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering feet controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02319v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Memmesheimer, Christian Lenz, Max Schwarz, Michael Schreiber, Sven Behnke</dc:creator>
    </item>
    <item>
      <title>FORS-EMG: A Novel sEMG Dataset for Hand Gesture Recognition Across Multiple Forearm Orientations</title>
      <link>https://arxiv.org/abs/2409.07484</link>
      <description>arXiv:2409.07484v2 Announce Type: replace-cross 
Abstract: Surface electromyography (sEMG) signals hold significant potential for gesture recognition and robust prosthetic hand development. However, sEMG signals are affected by various physiological and dynamic factors, including forearm orientation, electrode displacement, and limb position. Most existing sEMG datasets lack these dynamic considerations. This study introduces a novel multichannel sEMG dataset to evaluate commonly used hand gestures across three distinct forearm orientations. The dataset was collected from nineteen able-bodied subjects performing twelve hand gestures in three forearm orientations--supination, rest, and pronation. Eight MFI EMG electrodes were strategically placed at the elbow and mid-forearm to record high-quality EMG signals. Signal quality was validated through Signal-to-Noise Ratio (SNR) and Signal-to-Motion artifact ratio (SMR) metrics. Hand gesture classification performance across forearm orientations was evaluated using machine learning classifiers, including LDA, SVM, and KNN, alongside five feature extraction methods: TDD, TSD, FTDD, AR-RMS, and SNTDF. Furthermore, deep learning models such as 1D CNN, RNN, LSTM, and hybrid architectures were employed for a comprehensive analysis. Notably, the LDA classifier achieved the highest F1 score of 88.58\% with the SNTDF feature set when trained on hand gesture data of resting and tested across gesture data of all orientations. The promising results from extensive analyses underscore the proposed dataset's potential as a benchmark for advancing gesture recognition technologies, clinical sEMG research, and human-computer interaction applications. The dataset is publicly available in MATLAB format. Dataset: \url{https://www.kaggle.com/datasets/ummerummanchaity/fors-emg-a-novel-semg-dataset}</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07484v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Umme Rumman, Arifa Ferdousi, Bipin Saha, Md. Sazzad Hossain, Md. Johirul Islam, Shamim Ahmad, Mamun Bin Ibne Reaz, Md. Rezaul Islam</dc:creator>
    </item>
    <item>
      <title>Neural refractive index field: Unlocking the Potential of Background-oriented Schlieren Tomography in Volumetric Flow Visualization</title>
      <link>https://arxiv.org/abs/2409.14722</link>
      <description>arXiv:2409.14722v2 Announce Type: replace-cross 
Abstract: Background-oriented Schlieren tomography (BOST) is a prevalent method for visualizing intricate turbulent flows, valued for its ease of implementation and capacity to capture three-dimensional distributions of a multitude of flow parameters. However, the voxel-based meshing scheme leads to significant challenges, such as inadequate spatial resolution, substantial discretization errors, poor noise immunity, and excessive computational costs. This work presents an innovative reconstruction approach termed neural refractive index field (NeRIF) which implicitly represents the flow field with a neural network, which is trained with tailored strategies. Both numerical simulations and experimental demonstrations on turbulent Bunsen flames suggest that our approach can significantly improve the reconstruction accuracy and spatial resolution while concurrently reducing computational expenses. Although showcased in the context of background-oriented schlieren tomography here, the key idea embedded in the NeRIF can be readily adapted to various other tomographic modalities including tomographic absorption spectroscopy and tomographic particle imaging velocimetry, broadening its potential impact across different domains of flow visualization and analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14722v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>physics.optics</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanzhe He, Yutao Zheng, Shijie Xu, Chang Liu, Di Peng, Yingzheng Liu, Weiwei Cai</dc:creator>
    </item>
    <item>
      <title>A Computational Method for Measuring "Open Codes" in Qualitative Analysis</title>
      <link>https://arxiv.org/abs/2411.12142</link>
      <description>arXiv:2411.12142v2 Announce Type: replace-cross 
Abstract: Qualitative analysis is critical to understanding human datasets in many social science disciplines. Open coding is an inductive qualitative process that identifies and interprets "open codes" from datasets. Yet, meeting methodological expectations (such as "as exhaustive as possible") can be challenging. While many machine learning (ML)/generative AI (GAI) studies have attempted to support open coding, few have systematically measured or evaluated GAI outcomes, increasing potential bias risks. Building on Grounded Theory and Thematic Analysis theories, we present a computational method to measure and identify potential biases from "open codes" systematically. Instead of operationalizing human expert results as the "ground truth," our method is built upon a team-based approach between human and machine coders. We experiment with two HCI datasets to establish this method's reliability by 1) comparing it with human analysis, and 2) analyzing its output stability. We present evidence-based suggestions and example workflows for ML/GAI to support open coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12142v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Chen, Alexandros Lotsos, Lexie Zhao, Caiyi Wang, Jessica Hullman, Bruce Sherin, Uri Wilensky, Michael Horn</dc:creator>
    </item>
    <item>
      <title>Is Attention All You Need For Actigraphy? Foundation Models of Wearable Accelerometer Data for Mental Health Research</title>
      <link>https://arxiv.org/abs/2411.15240</link>
      <description>arXiv:2411.15240v2 Announce Type: replace-cross 
Abstract: Wearable accelerometry (actigraphy) has provided valuable data for clinical insights since the 1970s and is increasingly important as wearable devices continue to become widespread. The effectiveness of actigraphy in research and clinical contexts is heavily dependent on the modeling architecture utilized. To address this, we developed the Pretrained Actigraphy Transformer (PAT)--the first pretrained and fully attention-based model designed specifically to handle actigraphy. PAT was pretrained on actigraphy from 29,307 participants in NHANES, enabling it to deliver state-of-the-art performance when fine-tuned across various actigraphy prediction tasks in the mental health domain, even in data-limited scenarios. For example, when trained to predict benzodiazepine usage using actigraphy from only 500 labeled participants, PAT achieved an 8.8 percentage-point AUC improvement over the best baseline. With fewer than 2 million parameters and built-in model explainability, PAT is robust yet easy to deploy in health research settings.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15240v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C Jacobson</dc:creator>
    </item>
  </channel>
</rss>
