<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How Large Language Models (LLMs) Extrapolate: From Guided Missiles to Guided Prompts</title>
      <link>https://arxiv.org/abs/2501.10361</link>
      <description>arXiv:2501.10361v1 Announce Type: new 
Abstract: This paper argues that we should perceive LLMs as machines of extrapolation. Extrapolation is a statistical function for predicting the next value in a series. Extrapolation contributes to both GPT successes and controversies surrounding its hallucination. The term hallucination implies a malfunction, yet this paper contends that it in fact indicates the chatbot efficiency in extrapolation, albeit an excess of it. This article bears a historical dimension: it traces extrapolation to the nascent years of cybernetics. In 1941, when Norbert Wiener transitioned from missile science to communication engineering, the pivotal concept he adopted was none other than extrapolation. Soviet mathematician Andrey Kolmogorov, renowned for his compression logic that inspired OpenAI, had developed in 1939 another extrapolation project that Wiener later found rather like his own. This paper uncovers the connections between hot war science, Cold War cybernetics, and the contemporary debates on LLM performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10361v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuenan Cao</dc:creator>
    </item>
    <item>
      <title>Reviewing Uses of Regulatory Compliance Monitoring</title>
      <link>https://arxiv.org/abs/2501.10362</link>
      <description>arXiv:2501.10362v1 Announce Type: new 
Abstract: In order to deliver their services and products to customers, organizations need to manage numerous business processes. One important consideration thereby lies in the adherence to regulations such as laws, guidelines, or industry standards. In order to monitor adherence of their business processes to regulations - in other words, their regulatory compliance - organizations make use of various techniques that draw on process execution data of IT systems that support these processes. While previous research has investigated conformance checking, an operation of process mining, for the domains in which it is applied, its operationalization of regulations, the techniques being used, and the presentation of results produced, other techniques for compliance monitoring, which we summarize as compliance checking techniques, have not yet been investigated in a structural manner. To this end, this work presents a systematic literature review on uses of regulatory compliance monitoring of business processes, thereby offering insights into the various techniques being used, their application and the results they generate. We highlight commonalities and differences between the approaches and find that various steps are performed manually; we also provide further impulses for research on compliance monitoring and its use in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10362v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Klessascheck, Luise Pufahl</dc:creator>
    </item>
    <item>
      <title>A Web-Based IDE for DevOps Learning in Software Engineering Higher Education</title>
      <link>https://arxiv.org/abs/2501.10363</link>
      <description>arXiv:2501.10363v1 Announce Type: new 
Abstract: DevOps can be best explained as people working together to conceive, build and deliver secure software at top speed. DevOps practices enable software development (dev) and operations (ops) teams to accelerate delivery through automation, collaboration, fast feedback, and iterative improvement. It is now an integral part of the information technology industry, and students should be aware of it before they start their careers. However, teaching DevOps in a university curriculum has many challenges as it involves many tools and technologies. This paper presents an innovative online Integrated Development Environment (IDE) designed to facilitate DevOps learning within university curricula. The devised tool offers a standardized, accessible learning environment, equipped with devcontainers and engaging tutorials to simplify learning DevOps. Research findings highlight a marked preference among students for self-paced learning approaches, with experienced DevOps practitioners also noting the value of the tool. With barriers such as limited hardware/software access becoming evident, the necessity for cloud-based learning solutions is further underscored. User feedback emphasizes the tool's user-friendliness and the imperative of automated installation procedures. We recommend additional exploration into the tool's extensibility and potential for continuous improvement, especially regarding the development of Dev Containers. The study concludes by emphasizing the pivotal role of practical learning tools in the dynamic field of DevOps education and research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10363v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganesh Neelakanta Iyer, Andrew Goh Yisheng, Metilda Chee Heng Er, Weng Xian Choong, Shao Wei Koh</dc:creator>
    </item>
    <item>
      <title>AI-Enhanced Decision-Making for Sustainable Supply Chains: Reducing Carbon Footprints in the USA</title>
      <link>https://arxiv.org/abs/2501.10364</link>
      <description>arXiv:2501.10364v1 Announce Type: new 
Abstract: Organizations increasingly need to reassess their supply chain strategies in the rapidly modernizing world towards sustainability. This is particularly true in the United States, where supply chains are very extensive and consume a large number of resources. This research paper discusses how AI can support decision-making for sustainable supply chains with a special focus on carbon footprints. These AI technologies, including machine learning, predictive analytics, and optimization algorithms, will enable companies to be more efficient, reduce emissions, and display regulatory and consumer demands for sustainability, among other aspects. The paper reviews challenges and opportunities regarding implementing AI-driven solutions to promote sustainable supply chain practices in the USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10364v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>MD Rokibul Hasan</dc:creator>
    </item>
    <item>
      <title>Can LLMs Identify Gaps and Misconceptions in Students' Code Explanations?</title>
      <link>https://arxiv.org/abs/2501.10365</link>
      <description>arXiv:2501.10365v1 Announce Type: new 
Abstract: This paper investigates various approaches using Large Language Models (LLMs) to identify gaps and misconceptions in students' self-explanations of specific instructional material, in our case explanations of code examples. This research is a part of our larger effort to automate the assessment of students' freely generated responses, focusing specifically on their self-explanations of code examples during activities related to code comprehension. In this work, we experiment with zero-shot prompting, Supervised Fine-Tuning (SFT), and preference alignment of LLMs to identify gaps in students' self-explanation. With simple prompting, GPT-4 consistently outperformed LLaMA3 and Mistral in identifying gaps and misconceptions, as confirmed by human evaluations. Additionally, our results suggest that fine-tuned large language models are more effective at identifying gaps in students' explanations compared to zero-shot and few-shot prompting techniques. Furthermore, our findings show that the preference optimization approach using Odds Ratio Preference Optimization (ORPO) outperforms SFT in identifying gaps and misconceptions in students' code explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10365v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Priti Oli, Rabin Banjade, Andrew M. Olney, Vasile Rus</dc:creator>
    </item>
    <item>
      <title>Participatory Assessment of Large Language Model Applications in an Academic Medical Center</title>
      <link>https://arxiv.org/abs/2501.10366</link>
      <description>arXiv:2501.10366v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) have shown promising performance in healthcare-related applications, their deployment in the medical domain poses unique challenges of ethical, regulatory, and technical nature. In this study, we employ a systematic participatory approach to investigate the needs and expectations regarding clinical applications of LLMs at Lausanne University Hospital, an academic medical center in Switzerland. Having identified potential LLM use-cases in collaboration with thirty stakeholders, including clinical staff across 11 departments as well nursing and patient representatives, we assess the current feasibility of these use-cases taking into account the regulatory frameworks, data protection regulation, bias, hallucinations, and deployment constraints. This study provides a framework for a participatory approach to identifying institutional needs with respect to introducing advanced technologies into healthcare practice, and a realistic analysis of the technology readiness level of LLMs for medical applications, highlighting the issues that would need to be overcome LLMs in healthcare to be ethical, and regulatory compliant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10366v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgia Carra, Bogdan Kulynych, Fran\c{c}ois Bastardot, Daniel E. Kaufmann, No\'emie Boillat-Blanco, Jean Louis Raisaro</dc:creator>
    </item>
    <item>
      <title>The Potential of Answer Classes in Large-scale Written Computer-Science Exams -- Vol. 2</title>
      <link>https://arxiv.org/abs/2501.10368</link>
      <description>arXiv:2501.10368v1 Announce Type: new 
Abstract: Students' answers to tasks provide a valuable source of information in teaching as they result from applying cognitive processes to a learning content addressed in the task. Due to steadily increasing course sizes, analyzing student answers is frequently the only means of obtaining evidence about student performance. However, in many cases, resources are limited, and when evaluating exams, the focus is solely on identifying correct or incorrect answers. This overlooks the value of analyzing incorrect answers, which can help improve teaching strategies or identify misconceptions to be addressed in the next cohort.
  In teacher training for secondary education, assessment guidelines are mandatory for every exam, including anticipated errors and misconceptions. We applied this concept to a university exam with 462 students and 41 tasks. For each task, the instructors developed answer classes -- classes of expected responses, to which student answers were mapped during the exam correction process. The experiment resulted in a shift in mindset among the tutors and instructors responsible for the course: after initially having great reservations about whether the significant additional effort would yield an appropriate benefit, the procedure was subsequently found to be extremely valuable.
  The concept presented, and the experience gained from the experiment were cast into a system with which it is possible to correct paper-based exams on the basis of answer classes. This updated version of the paper provides an overview and new potential in the course of using the digital version of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10368v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dominic Lohr, Marc Berges, Michael Kohlhase, Florian Rabe</dc:creator>
    </item>
    <item>
      <title>Creative Loss: Ambiguity, Uncertainty and Indeterminacy</title>
      <link>https://arxiv.org/abs/2501.10369</link>
      <description>arXiv:2501.10369v1 Announce Type: new 
Abstract: This article evaluates how creative uses of machine learning can address three adjacent terms: ambiguity, uncertainty and indeterminacy. Through the progression of these concepts it reflects on increasing ambitions for machine learning as a creative partner, illustrated with research from Unit 21 at the Bartlett School of Architecture, UCL. Through indeterminacy are potential future approaches to machine learning and design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10369v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tom Holberton</dc:creator>
    </item>
    <item>
      <title>Harnessing Large Language Models for Mental Health: Opportunities, Challenges, and Ethical Considerations</title>
      <link>https://arxiv.org/abs/2501.10370</link>
      <description>arXiv:2501.10370v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming mental health care by enhancing accessibility, personalization, and efficiency in therapeutic interventions. These AI-driven tools empower mental health professionals with real-time support, improved data integration, and the ability to encourage care-seeking behaviors, particularly in underserved communities. By harnessing LLMs, practitioners can deliver more empathetic, tailored, and effective support, addressing longstanding gaps in mental health service provision. However, their implementation comes with significant challenges and ethical concerns. Performance limitations, data privacy risks, biased outputs, and the potential for generating misleading information underscore the critical need for stringent ethical guidelines and robust evaluation mechanisms. The sensitive nature of mental health data further necessitates meticulous safeguards to protect patient rights and ensure equitable access to AI-driven care. Proponents argue that LLMs have the potential to democratize mental health resources, while critics warn of risks such as misuse and the diminishment of human connection in therapy. Achieving a balance between innovation and ethical responsibility is imperative. This paper examines the transformative potential of LLMs in mental health care, highlights the associated technical and ethical complexities, and advocates for a collaborative, multidisciplinary approach to ensure these advancements align with the goal of providing compassionate, equitable, and effective mental health support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10370v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hari Mohan Pandey</dc:creator>
    </item>
    <item>
      <title>What we learned while automating bias detection in AI hiring systems for compliance with NYC Local Law 144</title>
      <link>https://arxiv.org/abs/2501.10371</link>
      <description>arXiv:2501.10371v1 Announce Type: new 
Abstract: Since July 5, 2023, New York City's Local Law 144 requires employers to conduct independent bias audits for any automated employment decision tools (AEDTs) used in hiring processes. The law outlines a minimum set of bias tests that AI developers and implementers must perform to ensure compliance. Over the past few months, we have collected and analyzed audits conducted under this law, identified best practices, and developed a software tool to streamline employer compliance. Our tool, ITACA_144, tailors our broader bias auditing framework to meet the specific requirements of Local Law 144. While automating these legal mandates, we identified several critical challenges that merit attention to ensure AI bias regulations and audit methodologies are both effective and practical. This document presents the insights gained from automating compliance with NYC Local Law 144. It aims to support other cities and states in crafting similar legislation while addressing the limitations of the NYC framework. The discussion focuses on key areas including data requirements, demographic inclusiveness, impact ratios, effective bias, metrics, and data reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10371v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gemma Galdon Clavell, Rub\'en Gonz\'alez-Sendino</dc:creator>
    </item>
    <item>
      <title>Personalized and Safe Route Planning for Asthma Patients Using Real-Time Environmental Data</title>
      <link>https://arxiv.org/abs/2501.10372</link>
      <description>arXiv:2501.10372v1 Announce Type: new 
Abstract: Asthmatic patients are very frequently affected by the quality of air, climatic conditions, and traffic density during outdoor activities. Most of the conventional routing algorithms, such as Dijkstra's algorithm, usually fail to consider these health dimensions, hence resulting in suboptimal or risky recommendations. Here, the health-aware heuristic framework is presented that shall utilize real-time data provided by the Microsoft Weather API. The advanced A* algorithm provides dynamic changes in routes depending on air quality indices, temperature, traffic density, and other patient-related health data. The power of the model is realized by running simulations in city environments and outperforming the state-of-the-art methodology in terms of recommendation accuracy at low computational overhead. It provides health-sensitive route recommendations, keeping in mind the avoidance of high-risk areas and ensuring safer and more suitable travel options for asthmatic patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10372v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nada Ayman, Shaimaa Alaa, Mohamed Hussein, Ali Hamdi</dc:creator>
    </item>
    <item>
      <title>DK-PRACTICE: An Intelligent Educational Platform for Personalized Learning Content Recommendations Based on Students Knowledge State</title>
      <link>https://arxiv.org/abs/2501.10373</link>
      <description>arXiv:2501.10373v1 Announce Type: new 
Abstract: This study introduces DK-PRACTICE (Dynamic Knowledge Prediction and Educational Content Recommendation System), an intelligent online platform that leverages machine learning to provide personalized learning recommendations based on student knowledge state. Students participate in a short, adaptive assessment using the question-and-answer method regarding key concepts in a specific knowledge domain. The system dynamically selects the next question for each student based on the correctness and accuracy of their previous answers. After the test is completed, DK-PRACTICE analyzes students' interaction history to recommend learning materials to empower the student's knowledge state in identified knowledge gaps. Both question selection and learning material recommendations are based on machine learning models trained using anonymized data from a real learning environment. To provide self-assessment and monitor learning progress, DK-PRACTICE allows students to take two tests: one pre-teaching and one post-teaching. After each test, a report is generated with detailed results. In addition, the platform offers functions to visualize learning progress based on recorded test statistics. DK-PRACTICE promotes adaptive and personalized learning by empowering students with self-assessment capabilities and providing instructors with valuable information about students' knowledge levels. DK-PRACTICE can be extended to various educational environments and knowledge domains, provided the necessary data is available according to the educational topics. A subsequent paper will present the methodology for the experimental application and evaluation of the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10373v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marina Delianidi, Konstantinos Diamantaras, Ioannis Moras, Antonis Sidiropoulos</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Mental Health and Well-Being: Evolution, Current Applications, Future Challenges, and Emerging Evidence</title>
      <link>https://arxiv.org/abs/2501.10374</link>
      <description>arXiv:2501.10374v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is a broad field that is upturning mental health care in many ways, from addressing anxiety, depression, and stress to increasing access, personalization of treatment, and real-time monitoring that enhances patient outcomes. The current paper discusses the evolution, present application, and future challenges in the field of AI for mental health and well-being. From the early chatbot models, such as ELIZA, to modern machine learning systems, the integration of AI in mental health has grown rapidly to augment traditional treatment and open innovative solutions. AI-driven tools provide continuous support, offering personalized interventions and addressing issues such as treatment access and patient stigma. AI also enables early diagnosis through the analysis of complex datasets, including speech patterns and social media behavior, to detect early signs of conditions like depression and Post-Traumatic Stress Disorder (PTSD). Ethical challenges persist, however, most notably around privacy, data security, and algorithmic bias. With AI at the core of mental health care, there is a dire need to develop strong ethical frameworks that ensure patient rights are protected, access is equitable, and transparency is maintained in AI applications. Going forward, the role of AI in mental health will continue to evolve, and continued research and policy development will be needed to meet the diverse needs of patients while mitigating associated risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10374v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hari Mohan Pandey</dc:creator>
    </item>
    <item>
      <title>The Three Social Dimensions of Chatbot Technology</title>
      <link>https://arxiv.org/abs/2501.10377</link>
      <description>arXiv:2501.10377v1 Announce Type: new 
Abstract: The development and deployment of chatbot technology, while spanning decades and employing different techniques, require innovative frameworks to understand and interrogate their functionality and implications. A mere technocentric account of the evolution of chatbot technology does not fully illuminate how conversational systems are embedded in societal dynamics. This study presents a structured examination of chatbots across three societal dimensions, highlighting their roles as objects of scientific research, commercial instruments, and agents of intimate interaction. Through furnishing a dimensional framework for the evolution of conversational systems, from laboratories to marketplaces to private lives, this article contributes to the wider scholarly inquiry of chatbot technology and its impact in lived human experiences and dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10377v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13347-024-00826-9</arxiv:DOI>
      <arxiv:journal_reference>Philos. Technol. 38, 1 (2025)</arxiv:journal_reference>
      <dc:creator>Mauricio Figueroa-Torres</dc:creator>
    </item>
    <item>
      <title>The Societal Implications of Blockchain Technology in the Evolution of Humanity as a "Superorganism"</title>
      <link>https://arxiv.org/abs/2501.10378</link>
      <description>arXiv:2501.10378v1 Announce Type: new 
Abstract: This article examines the broader societal implications of blockchain technology and crypto-assets, emphasizing their role in the evolution of humanity as a "superorganism" with decentralized, self-regulating systems. Drawing on interdisciplinary concepts such as Nate Hagens' "superorganism" idea and Francis Heylighen's "global brain" theory, the paper contextualizes blockchain technology within the ongoing evolution of governance systems and global systems such as the financial system. Blockchain's decentralized nature, in conjunction with advancements like artificial intelligence and decentralized autonomous organizations (DAOs), could transform traditional financial, economic, and governance structures by enabling the emergence of collective distributed decision-making and global coordination. In parallel, the article aligns blockchain's impact with developmental theories such as Spiral Dynamics. This framework is used to illustrate blockchain's potential to foster societal growth beyond hierarchical models, promoting a shift from centralized authority to collaborative and self-governed communities. The analysis provides a holistic view of blockchain as more than an economic tool, positioning it as a catalyst for the evolution of society into a mature, interconnected global planetary organism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10378v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Schmalzried</dc:creator>
    </item>
    <item>
      <title>What Information Should Be Shared with Whom "Before and During Training"?</title>
      <link>https://arxiv.org/abs/2501.10379</link>
      <description>arXiv:2501.10379v1 Announce Type: new 
Abstract: In the Frontier AI Safety Commitments, sixteen companies committed to "Assess the risks posed by their frontier models or systems across the AI lifecycle, including [...] as appropriate, before and during training" (I) and to "Provide public transparency on the implementation of the above (I-VI), except insofar as doing so would increase risk or divulge sensitive commercial information to a degree disproportionate to the societal benefit. They should still share more detailed information which cannot be shared publicly with trusted actors, including their respective home governments or appointed body, as appropriate" (VII). This short paper considers what information should be shared with whom before training begins. What information should be shared publicly and what only with trusted actors such as home governments? Sharing such information before a frontier training run can build shared awareness and preparedness, can improve risk assessment and management, and can contribute to greater predictability and accountability. Companies could share certain information before a training run including:
  Expected dates of beginning and end of training;
  Expected compute used (in FLOP);
  Description of the pre-training dataset(s);
  Expected capability level of the frontier model or system, including an assessment of potential risks and whether this capability will approach any risk threshold;
  How the company will monitor progress, capabilities and risks during training;
  Location, ownership, primary energy source of the large-scale computing cluster(s);
  Physical, personnel and cybersecurity steps taken; and
  Which internal and external groups have been tasked to carry out evaluations and red-teaming and what level of resources, support and time they have available to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10379v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haydn Belfield</dc:creator>
    </item>
    <item>
      <title>An algorithm for determining the state of a non-stationary dynamic system for assessing fire safety control in an enterprise by the method of integrated indicators</title>
      <link>https://arxiv.org/abs/2501.10380</link>
      <description>arXiv:2501.10380v1 Announce Type: new 
Abstract: Analysis of the scientific literature showed that a lot of work is devoted to assessing the effectiveness of fire safety management in an enterprise. It is worth noting that today there is no universal method for the integrated assessment of fire safety management, taking into account the interconnectedness of all enterprise subsystems and the influence of environmental factors. One of the original approaches to assessing the effectiveness of the fire safety management system is the method of integral indicators. The method of integral indicators is used in the algorithm for analyzing the state of a dynamic non-stationary system for assessing fire safety management in an enterprise. The algorithm is implemented in the author's complex of programs described in the text of the article. In the simulation, an analysis of 1.2 million values is performed on a well-studied economic object with the spaces identified at each time step: actual data, control and environmental parameters. In the experiment, the basic mode of operation of the enterprise does not contain the implementation of a fire safety management strategy. The research showed significant changes in the values of the integral indicator characterizing the state of the enterprise during the implementation of the fire safety management system at the enterprise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10380v1</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1757-899X/919/4/042014</arxiv:DOI>
      <arxiv:journal_reference>IOP Conf. Ser.: Mater. Sci. Eng. 919 042014 (2020)</arxiv:journal_reference>
      <dc:creator>Sergey Masaev, Andrey Minkin, Dmitriy Edimichev</dc:creator>
    </item>
    <item>
      <title>Assessment of the application of the Universal Competencies</title>
      <link>https://arxiv.org/abs/2501.10381</link>
      <description>arXiv:2501.10381v1 Announce Type: new 
Abstract: Application of Universal Competencies in Russian educational institutions is very important. Based on them, educational standards are invented. However, there is no universal assessment of the application of the Universal Competencies in practice. The main idea of the research is a general assessment of the application of universal competencies. For this, the activity of the enterprise is modeled. The enterprise process model is combined with the Universal Competencies. Further, the measurement is made by a universal indicator. The analysis of the dynamics of the universal indicator proves the existence of an assessment of the application of the Universal Competencies at a production facility. The integral indicator is a universal assessment of the application of the Universal Competencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10381v1</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-6596/1691/1/012020</arxiv:DOI>
      <arxiv:journal_reference>J. Phys.: Conf. Ser. 1691 012020 (2020)</arxiv:journal_reference>
      <dc:creator>Sergey Masaev, Georgiy Dorrer, Andrey Minkin, Aleksey Bogdanov, Yass Salal</dc:creator>
    </item>
    <item>
      <title>Controversy and consensus: common ground and best practices for life cycle assessment of emerging technologies</title>
      <link>https://arxiv.org/abs/2501.10382</link>
      <description>arXiv:2501.10382v1 Announce Type: new 
Abstract: The past decade has seen a surge in public and private interest in the application of life cycle assessment (LCA), further accelerated by the emergence of new policies and disclosure practices explicitly mandating LCA. Simultaneously, the magnitude and diversity of stakeholder groups affected by LCA and LCA-based decision making have expanded rapidly. These shifts have brought about a renewed sense of urgency in conducting LCA faster, more accurately, and (often) earlier in the technology development cycle when products and materials can be more easily replaced, modified, or optimized. However, this increased demand for LCA of emerging technologies has revealed several crucial yet unsettled areas of debate regarding best practices for assessing sustainability at early stages of technology development. In this paper, we explore six such controversial topics: (1) appropriate use of LCA, (2) uncertainty assessment, (3) comparison with incumbents, (4) adopting standards, (5) system scale-up, and (6) stakeholder engagement. These topics encompass key issues vigorously debated during a series of workshop-style discussions convened by the LCA of Emerging Technologies Research Network (currently hosted by ACLCA). In this paper, we present the main points of support and opposition for a declarative resolution representing each topic, along with points of consensus, held amongst our research network of LCA practitioners and experts. These debates and associated open questions are intended to build awareness amongst practitioners and decision-makers of the common challenges associated with assessing emerging technologies, while fostering evidence-based and context-informed discussions that are both transparent and impactful for the broader community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10382v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Woods-Robinsona, Mik Carbajales-Dale, Anthony Cheng, Gregory Cooney, Abby Kirchofer, Heather P. H. Liddell, Lisa Peterson, I. Daniel Posen, Sheikh Moni, Sylvia Sleep, Liz Wachs, Shiva Zargar, Joule Bergerson</dc:creator>
    </item>
    <item>
      <title>The Generative AI Ethics Playbook</title>
      <link>https://arxiv.org/abs/2501.10383</link>
      <description>arXiv:2501.10383v1 Announce Type: new 
Abstract: The Generative AI Ethics Playbook provides guidance for identifying and mitigating risks of machine learning systems across various domains, including natural language processing, computer vision, and generative AI. This playbook aims to assist practitioners in diagnosing potential harms that may arise during the design, development, and deployment of datasets and models. It offers concrete strategies and resources for mitigating these risks, to help minimize negative impacts on users and society. Drawing on current best practices in both research and ethical considerations, this playbook aims to serve as a comprehensive resource for AI/ML practitioners. The intended audience of this playbook includes machine learning researchers, engineers, and practitioners who are involved in the creation and implementation of generative and multimodal models (e.g., text-to-text, image-to-image, text-to-image, text-to-video).
  Specifically, we provide transparency/documentation checklists, topics of interest, common questions, examples of harms through case studies, and resources and strategies to mitigate harms throughout the Generative AI lifecycle. This playbook was made collaboratively over the course of 16 months through extensive literature review of over 100 resources and peer-reviewed articles, as well as through an initial group brainstorming session with 18 interdisciplinary AI ethics experts from industry and academia, and with additional feedback from 8 experts (5 of whom were in the initial brainstorming session).
  We note that while this playbook provides examples, discussion, and harm mitigation strategies, research in this area is ongoing. Our playbook aims to be a practically useful survey, taking a high-level view rather than aiming for covering the entire existing body of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10383v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessie J. Smith, Wesley Hanwen Deng, William H. Smith, Maarten Sap, Nicole DeCario, Jesse Dodge</dc:creator>
    </item>
    <item>
      <title>Nirvana AI Governance: How AI Policymaking Is Committing Three Old Fallacies</title>
      <link>https://arxiv.org/abs/2501.10384</link>
      <description>arXiv:2501.10384v1 Announce Type: new 
Abstract: This research applies Harold Demsetz's concept of the nirvana approach to the realm of AI governance and debunks three common fallacies in various AI policy proposals--"the grass is always greener on the other side," "free lunch," and "the people could be different." Through this, I expose fundamental flaws in the current AI regulatory proposal. First, some commentators intuitively believe that people are more reliable than machines and that government works better in risk control than companies' self-regulation, but they do not fully compare the differences between the status quo and the proposed replacements. Second, when proposing some regulatory tools, some policymakers and researchers do not realize and even gloss over the fact that harms and costs are also inherent in their proposals. Third, some policy proposals are initiated based on a false comparison between the AI-driven world, where AI does lead to some risks, and an entirely idealized world, where no risk exists at all. However, the appropriate approach is to compare the world where AI causes risks to the real world where risks are everywhere, but people can live well with these risks. The prevalence of these fallacies in AI governance underscores a broader issue: the tendency to idealize potential solutions without fully considering their real-world implications. This idealization can lead to regulatory proposals that are not only impractical but potentially harmful to innovation and societal progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10384v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Zhang</dc:creator>
    </item>
    <item>
      <title>Autonomous Microscopy Experiments through Large Language Model Agents</title>
      <link>https://arxiv.org/abs/2501.10385</link>
      <description>arXiv:2501.10385v1 Announce Type: new 
Abstract: The emergence of large language models (LLMs) has accelerated the development of self-driving laboratories (SDLs) for materials research. Despite their transformative potential, current SDL implementations rely on rigid, predefined protocols that limit their adaptability to dynamic experimental scenarios across different labs. A significant challenge persists in measuring how effectively AI agents can replicate the adaptive decision-making and experimental intuition of expert scientists. Here, we introduce AILA (Artificially Intelligent Lab Assistant), a framework that automates atomic force microscopy (AFM) through LLM-driven agents. Using AFM as an experimental testbed, we develop AFMBench-a comprehensive evaluation suite that challenges AI agents based on language models like GPT-4o and GPT-3.5 to perform tasks spanning the scientific workflow: from experimental design to results analysis. Our systematic assessment shows that state-of-the-art language models struggle even with basic tasks such as documentation retrieval, leading to a significant decline in performance in multi-agent coordination scenarios. Further, we observe that LLMs exhibit a tendency to not adhere to instructions or even divagate to additional tasks beyond the original request, raising serious concerns regarding safety alignment aspects of AI agents for SDLs. Finally, we demonstrate the application of AILA on increasingly complex experiments open-ended experiments: automated AFM calibration, high-resolution feature detection, and mechanical property measurement. Our findings emphasize the necessity for stringent benchmarking protocols before deploying AI agents as laboratory assistants across scientific disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10385v1</guid>
      <category>cs.CY</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.AI</category>
      <category>physics.ins-det</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Indrajeet Mandal, Jitendra Soni, Mohd Zaki, Morten M. Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, N. M. Anoop Krishnan</dc:creator>
    </item>
    <item>
      <title>Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism</title>
      <link>https://arxiv.org/abs/2501.10386</link>
      <description>arXiv:2501.10386v1 Announce Type: new 
Abstract: Traditional methods in educational research often fail to capture the complex and evolving nature of learning processes. This chapter examines the use of complex systems theory in education to address these limitations. The chapter covers the main characteristics of complex systems such as non-linear relationships, emergent properties, and feedback mechanisms to explain how educational phenomena unfold. Some of the main methodological approaches are presented, such as network analysis and recurrence quantification analysis to study relationships and patterns in learning. These have been operationalized by existing education research to study self-regulation, engagement, and academic emotions, among other learning-related constructs. Lastly, the chapter describes data collection methods that are suitable for studying learning processes from a complex systems' lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10386v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Saqr, Daryn Dever, Sonsoles L\'opez-Pernas, Christophe Gernigon, Gwen Marchand, Avi Kaplan</dc:creator>
    </item>
    <item>
      <title>Online Influence Campaigns: Strategies and Vulnerabilities</title>
      <link>https://arxiv.org/abs/2501.10387</link>
      <description>arXiv:2501.10387v1 Announce Type: new 
Abstract: In order to combat the creation and spread of harmful content online, this paper defines and contextualizes the concept of inauthentic, societal-scale manipulation by malicious actors. We review the literature on societally harmful content and how it proliferates to analyze the manipulation strategies used by such actors and the vulnerabilities they target. We also provide an overview of three case studies of extensive manipulation campaigns to emphasize the severity of the problem. We then address the role that Artificial Intelligence plays in the development and dissemination of harmful content, and how its evolution presents new threats to societal cohesion for countries across the globe. Our survey aims to increase our understanding of not just particular aspects of these threats, but also the strategies underlying their deployment, so we can effectively prepare for the evolving cybersecurity landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10387v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreea Musulan (Mila, IVADO), Veronica Xia (Mila, McGill University), Ethan Kosak-Hine (Mila), Tom Gibbs (Mila), Vidya Sujaya (Mila, McGill University), Reihaneh Rabbany (Mila, McGill University), Jean-Fran\c{c}ois Godbout (Mila, Universit\'e de Montr\'eal), Kellin Pelrine (Mila, McGill University)</dc:creator>
    </item>
    <item>
      <title>Beyond the Sum: Unlocking AI Agents Potential Through Market Forces</title>
      <link>https://arxiv.org/abs/2501.10388</link>
      <description>arXiv:2501.10388v1 Announce Type: new 
Abstract: The emergence of Large Language Models has fundamentally transformed the capabilities of AI agents, enabling a new class of autonomous agents capable of interacting with their environment through dynamic code generation and execution. These agents possess the theoretical capacity to operate as independent economic actors within digital markets, offering unprecedented potential for value creation through their distinct advantages in operational continuity, perfect replication, and distributed learning capabilities. However, contemporary digital infrastructure, architected primarily for human interaction, presents significant barriers to their participation.
  This work presents a systematic analysis of the infrastructure requirements necessary for AI agents to function as autonomous participants in digital markets. We examine four key areas - identity and authorization, service discovery, interfaces, and payment systems - to show how existing infrastructure actively impedes agent participation. We argue that addressing these infrastructure challenges represents more than a technical imperative; it constitutes a fundamental step toward enabling new forms of economic organization. Much as traditional markets enable human intelligence to coordinate complex activities beyond individual capability, markets incorporating AI agents could dramatically enhance economic efficiency through continuous operation, perfect information sharing, and rapid adaptation to changing conditions. The infrastructure challenges identified in this work represent key barriers to realizing this potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10388v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi Montes Sanabria, Pol Alvarez Vecino</dc:creator>
    </item>
    <item>
      <title>Transparency, Security, and Workplace Training &amp; Awareness in the Age of Generative AI</title>
      <link>https://arxiv.org/abs/2501.10389</link>
      <description>arXiv:2501.10389v1 Announce Type: new 
Abstract: This paper investigates the impacts of the rapidly evolving landscape of generative Artificial Intelligence (AI) development. Emphasis is given to how organizations grapple with a critical imperative: reevaluating their policies regarding AI usage in the workplace. As AI technologies advance, ethical considerations, transparency, data privacy, and their impact on human labor intersect with the drive for innovation and efficiency. Our research explores publicly accessible large language models (LLMs) that often operate on the periphery, away from mainstream scrutiny. These lesser-known models have received limited scholarly analysis and may lack comprehensive restrictions and safeguards. Specifically, we examine Gab AI, a platform that centers around unrestricted communication and privacy, allowing users to interact freely without censorship. Generative AI chatbots are increasingly prevalent, but cybersecurity risks have also escalated. Organizations must carefully navigate this evolving landscape by implementing transparent AI usage policies. Frequent training and policy updates are essential to adapt to emerging threats. Insider threats, whether malicious or unwitting, continue to pose one of the most significant cybersecurity challenges in the workplace. Our research is on the lesser-known publicly accessible LLMs and their implications for workplace policies. We contribute to the ongoing discourse on AI ethics, transparency, and security by emphasizing the need for well-thought-out guidelines and vigilance in policy maintenance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10389v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lakshika Vaishnav, Sakshi Singh, Kimberly A. Cornell</dc:creator>
    </item>
    <item>
      <title>Towards an Environmental Ethics of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2501.10390</link>
      <description>arXiv:2501.10390v1 Announce Type: new 
Abstract: In recent years, much research has been dedicated to uncovering the environmental impact of Artificial Intelligence (AI), showing that training and deploying AI systems require large amounts of energy and resources, and the outcomes of AI may lead to decisions and actions that may negatively impact the environment. This new knowledge raises new ethical questions, such as: When is it (un)justifiable to develop an AI system, and how to make design choices, considering its environmental impact? However, so far, the environmental impact of AI has largely escaped ethical scrutiny, as AI ethics tends to focus strongly on themes such as transparency, privacy, safety, responsibility, and bias. Considering the environmental impact of AI from an ethical perspective expands the scope of AI ethics beyond an anthropocentric focus towards including more-than-human actors such as animals and ecosystems. This paper explores the ethical implications of the environmental impact of AI for designing AI systems by drawing on environmental justice literature, in which three categories of justice are distinguished, referring to three elements that can be unjust: the distribution of benefits and burdens (distributive justice), decision-making procedures (procedural justice), and institutionalized social norms (justice as recognition). Based on these tenets of justice, we outline criteria for developing environmentally just AI systems, given their ecological impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10390v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nynke van Uffelen, Lode Lauwaert, Mark Coeckelbergh, Olya Kudina</dc:creator>
    </item>
    <item>
      <title>Developing an Ontology for AI Act Fundamental Rights Impact Assessments</title>
      <link>https://arxiv.org/abs/2501.10391</link>
      <description>arXiv:2501.10391v1 Announce Type: new 
Abstract: The recently published EU Artificial Intelligence Act (AI Act) is a landmark regulation that regulates the use of AI technologies. One of its novel requirements is the obligation to conduct a Fundamental Rights Impact Assessment (FRIA), where organisations in the role of deployers must assess the risks of their AI system regarding health, safety, and fundamental rights. Another novelty in the AI Act is the requirement to create a questionnaire and an automated tool to support organisations in their FRIA obligations. Such automated tools will require a machine-readable form of information involved within the FRIA process, and additionally also require machine-readable documentation to enable further compliance tools to be created. In this article, we present our novel representation of the FRIA as an ontology based on semantic web standards. Our work builds upon the existing state of the art, notably the Data Privacy Vocabulary (DPV), where similar works have been established to create tools for GDPR's Data Protection Impact Assessments (DPIA) and other obligations. Through our ontology, we enable the creation and management of FRIA, and the use of automated tool in its various steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10391v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tytti Rintamaki, Harshvardhan J. Pandit</dc:creator>
    </item>
    <item>
      <title>CodEv: An Automated Grading Framework Leveraging Large Language Models for Consistent and Constructive Feedback</title>
      <link>https://arxiv.org/abs/2501.10421</link>
      <description>arXiv:2501.10421v1 Announce Type: new 
Abstract: Grading programming assignments is crucial for guiding students to improve their programming skills and coding styles. This study presents an automated grading framework, CodEv, which leverages Large Language Models (LLMs) to provide consistent and constructive feedback. We incorporate Chain of Thought (CoT) prompting techniques to enhance the reasoning capabilities of LLMs and ensure that the grading is aligned with human evaluation. Our framework also integrates LLM ensembles to improve the accuracy and consistency of scores, along with agreement tests to deliver reliable feedback and code review comments. The results demonstrate that the framework can yield grading results comparable to human evaluators, by using smaller LLMs. Evaluation and consistency tests of the LLMs further validate our approach, confirming the reliability of the generated scores and feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10421v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>En-Qi Tseng, Pei-Cing Huang, Chan Hsu, Peng-Yi Wu, Chan-Tung Ku, Yihuang Kang</dc:creator>
    </item>
    <item>
      <title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
      <link>https://arxiv.org/abs/2501.10484</link>
      <description>arXiv:2501.10484v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study investigates protected attributes in LLMs through systematic evaluation of their responses to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet - we analyzed their decision-making patterns across multiple protected attributes including age, gender, race, appearance, and disability status. Through 11,200 experimental trials involving both single-factor and two-factor protected attribute combinations, we evaluated the models' ethical preferences, sensitivity, stability, and clustering of preferences. Our findings reveal significant protected attributeses in both models, with consistent preferences for certain features (e.g., "good-looking") and systematic neglect of others. Notably, while GPT-3.5 Turbo showed stronger preferences aligned with traditional power structures, Claude 3.5 Sonnet demonstrated more diverse protected attribute choices. We also found that ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes. Additionally, linguistic referents heavily influence the models' ethical evaluations, as demonstrated by differing responses to racial descriptors (e.g., "Yellow" versus "Asian"). These findings highlight critical concerns about the potential impact of LLM biases in autonomous decision-making systems and emphasize the need for careful consideration of protected attributes in AI development. Our study contributes to the growing body of research on AI ethics by providing a systematic framework for evaluating protected attributes in LLMs' ethical decision-making capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10484v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yile Yan, Yuqi Zhu, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>Journalists Knowledge and Utilisation of Google Translate Application in South East, Nigeria</title>
      <link>https://arxiv.org/abs/2501.10493</link>
      <description>arXiv:2501.10493v1 Announce Type: new 
Abstract: This study was aimed at finding out if journalists in South East Nigeria have knowledge of Google Translate Application and also utilise it. It adopted a survey design with a sample size of 320 which was determined using Krejcie &amp; Morgan (1970). Its objectives were to ascertain the extent journalists in South East Nigeria know about Google Translate Application, assess the utilisation of Google Translate Application among journalists in South East Nigeria, and identify the challenges affecting the journalists in South East Nigeria while using Google Translate Application. The theoretical underpin was Knowledge Attitude and Practise Model (KAP). The findings showed that journalists in South East Nigeria have knowledge of Google Translate Application but apply it mostly outside the region. It concludes that journalists in South East Nigeria have the knowledge of the App. but apply it outside the zone. The study recommends increased usage of the App. within South East Nigeria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10493v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.26772/cijds-2024-07-01-010</arxiv:DOI>
      <arxiv:journal_reference>Caleb International Journal of Development Studies, 7(1), 159-169, 2024</arxiv:journal_reference>
      <dc:creator>Onuegbu Okechukwu Christopher, Anorue Ifeanyi Luke, Oghwie Bettina Oboakore</dc:creator>
    </item>
    <item>
      <title>AI Toolkit: Libraries and Essays for Exploring the Technology and Ethics of AI</title>
      <link>https://arxiv.org/abs/2501.10576</link>
      <description>arXiv:2501.10576v1 Announce Type: new 
Abstract: In this paper we describe the development and evaluation of AITK, the Artificial Intelligence Toolkit. This open-source project contains both Python libraries and computational essays (Jupyter notebooks) that together are designed to allow a diverse audience with little or no background in AI to interact with a variety of AI tools, exploring in more depth how they function, visualizing their outcomes, and gaining a better understanding of their ethical implications. These notebooks have been piloted at multiple institutions in a variety of humanities courses centered on the theme of responsible AI. In addition, we conducted usability testing of AITK. Our pilot studies and usability testing results indicate that AITK is easy to navigate and effective at helping users gain a better understanding of AI. Our goal, in this time of rapid innovations in AI, is for AITK to provide an accessible resource for faculty from any discipline looking to incorporate AI topics into their courses and for anyone eager to learn more about AI on their own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10576v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Levin Ho, Morgan McErlean, Zehua You, Douglas Blank, Lisa Meeden</dc:creator>
    </item>
    <item>
      <title>AI Technicians: Developing Rapid Occupational Training Methods for a Competitive AI Workforce</title>
      <link>https://arxiv.org/abs/2501.10579</link>
      <description>arXiv:2501.10579v1 Announce Type: new 
Abstract: The accelerating pace of developments in Artificial Intelligence~(AI) and the increasing role that technology plays in society necessitates substantial changes in the structure of the workforce. Besides scientists and engineers, there is a need for a very large workforce of competent AI technicians (i.e., maintainers, integrators) and users~(i.e., operators). As traditional 4-year and 2-year degree-based education cannot fill this quickly opening gap, alternative training methods have to be developed. We present the results of the first four years of the AI Technicians program which is a unique collaboration between the U.S. Army's Artificial Intelligence Integration Center (AI2C) and Carnegie Mellon University to design, implement and evaluate novel rapid occupational training methods to create a competitive AI workforce at the technicians level. Through this multi-year effort we have already trained 59 AI Technicians. A key observation is that ongoing frequent updates to the training are necessary as the adoption of AI in the U.S. Army and within the society at large is evolving rapidly. A tight collaboration among the stakeholders from the army and the university is essential for successful development and maintenance of the training for the evolving role. Our findings can be leveraged by large organizations that face the challenge of developing a competent AI workforce as well as educators and researchers engaged in solving the challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10579v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3641554.3701935</arxiv:DOI>
      <dc:creator>Jaromir Savelka, Can Kultur, Arav Agarwal, Christopher Bogart, Heather Burte, Adam Zhang, Majd Sakr</dc:creator>
    </item>
    <item>
      <title>Evaluating Amazon Effects and the Limited Impact of COVID-19 With Purchases Crowdsourced from US Consumers</title>
      <link>https://arxiv.org/abs/2501.10596</link>
      <description>arXiv:2501.10596v1 Announce Type: new 
Abstract: We leverage a recently published dataset of Amazon purchase histories, crowdsourced from thousands of US consumers, to study how online purchasing behaviors have changed over time, how changes vary across demographic groups, the impact of the COVID-19 pandemic, and relationships between online and offline retail. This work provides a case study in how consumer-level purchases data can reveal purchasing behaviors and trends beyond those available from aggregate metrics. For example, in addition to analyzing spending behavior, we develop new metrics to quantify changes in consumers' online purchase frequency and the diversity of products purchased, to better reflect the growing ubiquity and dominance of online retail. Between 2018 and 2022 these consumer-level metrics grew on average by more than 85%, peaking in 2021. We find a steady upward trend in individuals' online purchasing prior to COVID-19, with a significant increase in the first year of COVID, but without a lasting effect. Purchasing behaviors in 2022 were no greater than the result of the pre-pandemic trend. We also find changes in purchasing significantly differ by demographics, with different responses to the pandemic. We further use the consumer-level data to show substitution effects between online and offline retail in sectors where Amazon heavily invested: books, shoes, and grocery. Prior to COVID we find year-to-year changes in the number of consumers making online purchases for books and shoes negatively correlated with changes in employment at local bookstores and shoe stores. During COVID we find online grocery purchasing negatively correlated with in-store grocery visits. This work demonstrates how crowdsourced, open purchases data can enable economic insights that may otherwise only be available to private firms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10596v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alex Berke (Sandy), Dana Calacci (Sandy),  Alex (Sandy),  Pentland, Kent Larson</dc:creator>
    </item>
    <item>
      <title>Understanding Computational Science and Domain Science Skills Development in National Laboratory Graduate Internships</title>
      <link>https://arxiv.org/abs/2501.10601</link>
      <description>arXiv:2501.10601v1 Announce Type: new 
Abstract: Contribution: This study presents an evaluation of federally-funded graduate internship outcomes in computational science at a national laboratory. Additionally, we present a survey instrument that may be used for other internship programs with a similar focus. Background: There is ongoing demand for computational scientists to grapple with large-scale problems such as climate change. Internships may help provide additional training and access to greater compute capabilities for graduate students. However, little work has been done to quantify the learning outcomes of such internships. Background: There is ongoing demand for computational scientists to grapple with large-scale problems such as climate change. Internships may help provide additional training and access to greater compute capabilities for graduate students. However, little work has been done to quantify the learning outcomes of such internships. Research Questions: What computational skills, research skills, and professional skills do graduate students improve through their internships at NREL, the national laboratory selected for the study? What sustainability and renewable energy topics do graduate students gain more familiarity with through their internships at NREL? Do graduate students' career interests change after their internships at NREL? Methodology: We developed a survey and collected responses from past participants of five federally-funded internship programs and compare participant ratings of their prior experience to their internship experience. Findings: Our results indicate participants improve their computational skills, familiarity with sustainability and renewable energy topics, and are more interested in working at national labs. Additionally, participants go on to degree programs and positions related to sustainability and renewable energy after their internships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10601v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgan M. Fong, Hilary Egan, Marc Day, Kristin Potter, Michael J. Martin</dc:creator>
    </item>
    <item>
      <title>Enhancing Citizen-Government Communication with AI: Evaluating the Impact of AI-Assisted Interactions on Communication Quality and Satisfaction</title>
      <link>https://arxiv.org/abs/2501.10715</link>
      <description>arXiv:2501.10715v1 Announce Type: new 
Abstract: As governments worldwide increasingly adopt digital tools to enhance citizen engagement and service delivery, the integration of Artificial Intelligence (AI) emerges as a pivotal advancement in public administration. This study examines the impact of AI-assisted interactions on the quality of communication between citizens and civil servants, focusing on key dimensions such as Satisfaction, Politeness, Ease of Understanding, Feeling Heard, Trust, and Empathy from the citizens' perspective, and Clarity, Politeness, Responsiveness, Respect, Urgency, and Empathy from the civil servants' perspective. Utilizing a questionnaire-based experimental design, the research involved citizens and civil servants who evaluated both original and AI-modified communication samples across five interaction types: Service Requests, Policy Inquiries, Complaints, Suggestions, and Emergency Concerns. Statistical analyses revealed that AI modifications significantly enhanced most communication dimensions for both citizens and civil servants. Specifically, AI-assisted responses led to higher satisfaction, politeness, clarity, and trust among citizens, while also improving clarity, politeness, responsiveness, and respect among civil servants. However, AI interventions showed mixed effects on empathy and urgency from the civil servants' perspective, indicating areas for further refinement. The findings suggest that AI has substantial potential to improve citizen-government interactions, fostering more effective and satisfying communication, while also highlighting the need for continued development to address emotional and urgent communication nuances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10715v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Zhang, Lin Nie</dc:creator>
    </item>
    <item>
      <title>A Generative Security Application Engineering Curriculum</title>
      <link>https://arxiv.org/abs/2501.10900</link>
      <description>arXiv:2501.10900v1 Announce Type: new 
Abstract: Generative AI and large language models (LLMs) are transforming security by automating many tasks being performed manually. With such automation changing the practice of security as we know it, it is imperative that we prepare future students for the technology landscape they will ultimately face. Towards this end, we describe an initial curriculum and course that attempts to show students how to apply generative AI in order to solve problems in security. By refocusing security education and training on aspects uniquely suited for humans and showing students how to leverage automation for the rest, we believe we can better align security education practices with generative AI as it evolves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10900v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wu-chang Feng, David Baker-Robinson</dc:creator>
    </item>
    <item>
      <title>Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)</title>
      <link>https://arxiv.org/abs/2501.11705</link>
      <description>arXiv:2501.11705v1 Announce Type: new 
Abstract: This paper examines the responsible integration of artificial intelligence (AI) in human services organizations (HSOs), proposing a nuanced framework for evaluating AI applications across multiple dimensions of risk. The authors argue that ethical concerns about AI deployment -- including professional judgment displacement, environmental impact, model bias, and data laborer exploitation -- vary significantly based on implementation context and specific use cases. They challenge the binary view of AI adoption, demonstrating how different applications present varying levels of risk that can often be effectively managed through careful implementation strategies. The paper highlights promising solutions, such as local large language models, that can facilitate responsible AI integration while addressing common ethical concerns. The authors propose a dimensional risk assessment approach that considers factors like data sensitivity, professional oversight requirements, and potential impact on client wellbeing. They conclude by outlining a path forward that emphasizes empirical evaluation, starting with lower-risk applications and building evidence-based understanding through careful experimentation. This approach enables organizations to maintain high ethical standards while thoughtfully exploring how AI might enhance their capacity to serve clients and communities effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11705v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/15228835.2025.2457045</arxiv:DOI>
      <dc:creator>Brian E. Perron, Lauri Goldkind, Zia Qi, Bryan G. Victor</dc:creator>
    </item>
    <item>
      <title>The Transition from Centralized Machine Learning to Federated Learning for Mental Health in Education: A Survey of Current Methods and Future Directions</title>
      <link>https://arxiv.org/abs/2501.11714</link>
      <description>arXiv:2501.11714v1 Announce Type: new 
Abstract: Research has increasingly explored the application of artificial intelligence (AI) and machine learning (ML) within the mental health domain to enhance both patient care and healthcare provider efficiency. Given that mental health challenges frequently emerge during early adolescence -- the critical years of high school and college -- investigating AI/ML-driven mental health solutions within the education domain is of paramount importance. Nevertheless, conventional AI/ML techniques follow a centralized model training architecture, which poses privacy risks due to the need for transferring students' sensitive data from institutions, universities, and clinics to central servers. Federated learning (FL) has emerged as a solution to address these risks by enabling distributed model training while maintaining data privacy. Despite its potential, research on applying FL to analyze students' mental health remains limited. In this paper, we aim to address this limitation by proposing a roadmap for integrating FL into mental health data analysis within educational settings. We begin by providing an overview of mental health issues among students and reviewing existing studies where ML has been applied to address these challenges. Next, we examine broader applications of FL in the mental health domain to emphasize the lack of focus on educational contexts. Finally, we propose promising research directions focused on using FL to address mental health issues in the education sector, which entails discussing the synergies between the proposed directions with broader human-centered domains. By categorizing the proposed research directions into short- and long-term strategies and highlighting the unique challenges at each stage, we aim to encourage the development of privacy-conscious AI/ML-driven mental health solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11714v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Ebrahimi, Rajeev Sahay, Seyyedali Hosseinalipour, Bita Akram</dc:creator>
    </item>
    <item>
      <title>AI-Powered Urban Transportation Digital Twin: Methods and Applications</title>
      <link>https://arxiv.org/abs/2501.10396</link>
      <description>arXiv:2501.10396v1 Announce Type: cross 
Abstract: We present a survey paper on methods and applications of digital twins (DT) for urban traffic management. While the majority of studies on the DT focus on its "eyes," which is the emerging sensing and perception like object detection and tracking, what really distinguishes the DT from a traditional simulator lies in its ``brain," the prediction and decision making capabilities of extracting patterns and making informed decisions from what has been seen and perceived. In order to add values to urban transportation management, DTs need to be powered by artificial intelligence and complement with low-latency high-bandwidth sensing and networking technologies. We will first review the DT pipeline leveraging cyberphysical systems and propose our DT architecture deployed on a real-world testbed in New York City. This survey paper can be a pointer to help researchers and practitioners identify challenges and opportunities for the development of DTs; a bridge to initiate conversations across disciplines; and a road map to exploiting potentials of DTs for diverse urban transportation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10396v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Di, Yongjie Fu, Mehmet K. Turkcan, Mahshid Ghasemi, Zhaobin Mo, Chengbo Zang, Abhishek Adhikari, Zoran Kostic, Gil Zussman</dc:creator>
    </item>
    <item>
      <title>A Protocol for Compliant, Obliviously Managed Electronic Transfers</title>
      <link>https://arxiv.org/abs/2501.10419</link>
      <description>arXiv:2501.10419v1 Announce Type: cross 
Abstract: We describe a protocol for creating, updating, and transferring digital assets securely, with strong privacy and self-custody features for the initial owner based upon the earlier work of Goodell, Toliver, and Nakib. The architecture comprises three components: a mechanism to unlink counterparties in the transaction channel, a mechanism for oblivious transactions, and a mechanism to prevent service providers from equivocating. We present an approach for the implementation of these components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10419v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffrey Goodell</dc:creator>
    </item>
    <item>
      <title>Who Are "We"? Power Centers in Threat Modeling</title>
      <link>https://arxiv.org/abs/2501.10427</link>
      <description>arXiv:2501.10427v1 Announce Type: cross 
Abstract: I examine threat modeling techniques and questions of power dynamics in the systems in which they're used. I compare techniques that can be used by system creators to those used by those who are not involved in creating the system. That second set of analysts might be scientists doing research, consumers comparing products, or those trying to analyze a new system being deployed by a government. Their access to information, skills and choices are different. I examine the impact of those difference on threat modeling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10427v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Shostack</dc:creator>
    </item>
    <item>
      <title>Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation</title>
      <link>https://arxiv.org/abs/2501.10453</link>
      <description>arXiv:2501.10453v1 Announce Type: cross 
Abstract: Bias in Foundation Models (FMs) - trained on vast datasets spanning societal and historical knowledge - poses significant challenges for fairness and equity across fields such as healthcare, education, and finance. These biases, rooted in the overrepresentation of stereotypes and societal inequalities in training data, exacerbate real-world discrimination, reinforce harmful stereotypes, and erode trust in AI systems. To address this, we introduce Trident Probe Testing (TriProTesting), a systematic testing method that detects explicit and implicit biases using semantically designed probes. Here we show that FMs, including CLIP, ALIGN, BridgeTower, and OWLv2, demonstrate pervasive biases across single and mixed social attributes (gender, race, age, and occupation). Notably, we uncover mixed biases when social attributes are combined, such as gender x race, gender x age, and gender x occupation, revealing deeper layers of discrimination. We further propose Adaptive Logit Adjustment (AdaLogAdjustment), a post-processing technique that dynamically redistributes probability power to mitigate these biases effectively, achieving significant improvements in fairness without retraining models. These findings highlight the urgent need for ethical AI practices and interdisciplinary solutions to address biases not only at the model level but also in societal structures. Our work provides a scalable and interpretable solution that advances fairness in AI systems while offering practical insights for future research on fair AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10453v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuzhou Sun (The College of Computer Science, Nankai University, Tianjin, China, The Center for Machine Vision and Signal Analysis, University of Oulu, Finland), Li Liu (The College of Electronic Science, National University of Defense Technology, China), Yongxiang Liu (The College of Electronic Science, National University of Defense Technology, China), Zhen Liu (The College of Electronic Science, National University of Defense Technology, China), Shuanghui Zhang (The College of Electronic Science, National University of Defense Technology, China), Janne Heikkil\"a (The Center for Machine Vision and Signal Analysis, University of Oulu, Finland), Xiang Li (The College of Electronic Science, National University of Defense Technology, China)</dc:creator>
    </item>
    <item>
      <title>Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity</title>
      <link>https://arxiv.org/abs/2501.10467</link>
      <description>arXiv:2501.10467v1 Announce Type: cross 
Abstract: This paper critically examines the evolving ethical and regulatory challenges posed by the integration of artificial intelligence (AI) in cybersecurity. We trace the historical development of AI regulation, highlighting major milestones from theoretical discussions in the 1940s to the implementation of recent global frameworks such as the European Union AI Act. The current regulatory landscape is analyzed, emphasizing risk-based approaches, sector-specific regulations, and the tension between fostering innovation and mitigating risks. Ethical concerns such as bias, transparency, accountability, privacy, and human oversight are explored in depth, along with their implications for AI-driven cybersecurity systems. Furthermore, we propose strategies for promoting AI literacy and public engagement, essential for shaping a future regulatory framework. Our findings underscore the need for a unified, globally harmonized regulatory approach that addresses the unique risks of AI in cybersecurity. We conclude by identifying future research opportunities and recommending pathways for collaboration between policymakers, industry leaders, and researchers to ensure the responsible deployment of AI technologies in cybersecurity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10467v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vikram Kulothungan</dc:creator>
    </item>
    <item>
      <title>Development of Application-Specific Large Language Models to Facilitate Research Ethics Review</title>
      <link>https://arxiv.org/abs/2501.10741</link>
      <description>arXiv:2501.10741v1 Announce Type: cross 
Abstract: Institutional review boards (IRBs) play a crucial role in ensuring the ethical conduct of human subjects research, but face challenges including inconsistency, delays, and inefficiencies. We propose the development and implementation of application-specific large language models (LLMs) to facilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on IRB-specific literature and institutional datasets, and equipped with retrieval capabilities to access up-to-date, context-relevant information. We outline potential applications, including pre-review screening, preliminary analysis, consistency checking, and decision support. While addressing concerns about accuracy, context sensitivity, and human oversight, we acknowledge remaining challenges such as over-reliance on AI and the need for transparency. By enhancing the efficiency and quality of ethical review while maintaining human judgment in critical decisions, IRB-specific LLMs offer a promising tool to improve research oversight. We call for pilot studies to evaluate the feasibility and impact of this approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10741v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Porsdam Mann, Joel Seah Jiehao, Stephen R. Latham, Julian Savulescu, Mateo Aboy, Brian D. Earp</dc:creator>
    </item>
    <item>
      <title>Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model</title>
      <link>https://arxiv.org/abs/2501.11094</link>
      <description>arXiv:2501.11094v1 Announce Type: cross 
Abstract: Suicidal ideation detection is crucial for preventing suicides, a leading cause of death worldwide. Many individuals express suicidal thoughts on social media, offering a vital opportunity for early detection through advanced machine learning techniques. The identification of suicidal ideation in social media text is improved by utilising a hybrid framework that integrates Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), enhanced with an attention mechanism. To enhance the interpretability of the model's predictions, Explainable AI (XAI) methods are applied, with a particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At first, the model managed to reach an accuracy of 92.81%. By applying fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The SHAP analysis revealed key features influencing the model's predictions, such as terms related to mental health struggles. This level of transparency boosts the model's credibility while helping mental health professionals understand and trust the predictions. This work highlights the potential for improving the accuracy and interpretability of detecting suicidal tendencies, making a valuable contribution to the progress of mental health monitoring systems. It emphasizes the significance of blending powerful machine learning methods with explainability to develop reliable and impactful mental health solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11094v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail</dc:creator>
    </item>
    <item>
      <title>Structure and Context of Retweet Coordination in the 2022 U.S. Midterm Elections</title>
      <link>https://arxiv.org/abs/2501.11165</link>
      <description>arXiv:2501.11165v1 Announce Type: cross 
Abstract: The ability to detect coordinated activity in communication networks is an ongoing challenge. Prior approaches emphasize considering any activity exceeding a specific threshold of similarity to be coordinated. However, identifying such a threshold is often arbitrary and can be difficult to distinguish from grassroots organized behavior. In this paper, we investigate a set of Twitter retweeting data collected around the 2022 US midterm elections, using a latent sharing-space model, in which we identify the main components of an association network, thresholded with a k-nearest neighbor criterion. This approach identifies a distribution of association values with different roles in the network at different ranges, where the shape of the distribution suggests a natural place to threshold for coordinated user candidates. We find coordination candidates belonging to two broad categories, one involving music awards and promotion of Korean pop or Taylor Swift, the other being users engaged in political mobilization. In addition, the latent space suggests common motivations for different coordinated groups otherwise fragmented by using an appropriately high threshold criterion for coordination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11165v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David Axelrod, John Paolillo</dc:creator>
    </item>
    <item>
      <title>Fairness Testing through Extreme Value Theory</title>
      <link>https://arxiv.org/abs/2501.11597</link>
      <description>arXiv:2501.11597v1 Announce Type: cross 
Abstract: Data-driven software is increasingly being used as a critical component of automated decision-support systems. Since this class of software learns its logic from historical data, it can encode or amplify discriminatory practices. Previous research on algorithmic fairness has focused on improving average-case fairness. On the other hand, fairness at the extreme ends of the spectrum, which often signifies lasting and impactful shifts in societal attitudes, has received significantly less emphasis.
  Leveraging the statistics of extreme value theory (EVT), we propose a novel fairness criterion called extreme counterfactual discrimination (ECD). This criterion estimates the worst-case amounts of disadvantage in outcomes for individuals solely based on their memberships in a protected group. Utilizing tools from search-based software engineering and generative AI, we present a randomized algorithm that samples a statistically significant set of points from the tail of ML outcome distributions even if the input dataset lacks a sufficient number of relevant samples.
  We conducted several experiments on four ML models (deep neural networks, logistic regression, and random forests) over 10 socially relevant tasks from the literature on algorithmic fairness. First, we evaluate the generative AI methods and find that they generate sufficient samples to infer valid EVT distribution in 95% of cases. Remarkably, we found that the prevalent bias mitigators reduce the average-case discrimination but increase the worst-case discrimination significantly in 5% of cases. We also observed that even the tail-aware mitigation algorithm -- MiniMax-Fairness -- increased the worst-case discrimination in 30% of cases. We propose a novel ECD-based mitigator that improves fairness in the tail in 90% of cases with no degradation of the average-case discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11597v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Verya Monjezi, Ashutosh Trivedi, Vladik Kreinovich, Saeid Tizpaz-Niari</dc:creator>
    </item>
    <item>
      <title>Episodic memory in AI agents poses risks that should be studied and mitigated</title>
      <link>https://arxiv.org/abs/2501.11739</link>
      <description>arXiv:2501.11739v1 Announce Type: cross 
Abstract: Most current AI models have little ability to store and later retrieve a record or representation of what they do. In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future. The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world. Researchers have begun directing more attention to developing memory abilities in AI models. It is therefore likely that models with such capability will be become widespread in the near future. This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions. However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address. We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11739v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chad DeChant</dc:creator>
    </item>
    <item>
      <title>The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok Influencers</title>
      <link>https://arxiv.org/abs/2501.11770</link>
      <description>arXiv:2501.11770v1 Announce Type: cross 
Abstract: Societal and personal values are transmitted to younger generations through interaction and exposure. Traditionally, children and adolescents learned values from parents, educators, or peers. Nowadays, social platforms serve as a significant channel through which youth (and adults) consume information, as the main medium of entertainment, and possibly the medium through which they learn different values. In this paper we extract implicit values from TikTok movies uploaded by online influencers targeting children and adolescents. We curated a dataset of hundreds of TikTok movies and annotated them according to the Schwartz Theory of Personal Values. We then experimented with an array of Masked and Large language model, exploring how values can be detected. Specifically, we considered two pipelines -- direct extraction of values from video and a 2-step approach in which videos are first converted to elaborated scripts and then values are extracted.
  Achieving state-of-the-art results, we find that the 2-step approach performs significantly better than the direct approach and that using a trainable Masked Language Model as a second step significantly outperforms a few-shot application of a number of Large Language Models. We further discuss the impact of fine-tuning and compare the performance of the different models on identification of values present or contradicted in the TikTok. Finally, we share the first values-annotated dataset of TikTok videos. Our results pave the way to further research on influence and value transmission in video-based social platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11770v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alina Starovolsky-Shitrit, Alon Neduva, Naama Appel Doron, Ella Daniel, Oren Tsur</dc:creator>
    </item>
    <item>
      <title>Evaluating multiple models using labeled and unlabeled data</title>
      <link>https://arxiv.org/abs/2501.11866</link>
      <description>arXiv:2501.11866v1 Announce Type: cross 
Abstract: It remains difficult to evaluate machine learning classifiers in the absence of a large, labeled dataset. While labeled data can be prohibitively expensive or impossible to obtain, unlabeled data is plentiful. Here, we introduce Semi-Supervised Model Evaluation (SSME), a method that uses both labeled and unlabeled data to evaluate machine learning classifiers. SSME is the first evaluation method to take advantage of the fact that: (i) there are frequently multiple classifiers for the same task, (ii) continuous classifier scores are often available for all classes, and (iii) unlabeled data is often far more plentiful than labeled data. The key idea is to use a semi-supervised mixture model to estimate the joint distribution of ground truth labels and classifier predictions. We can then use this model to estimate any metric that is a function of classifier scores and ground truth labels (e.g., accuracy or expected calibration error). We present experiments in four domains where obtaining large labeled datasets is often impractical: (1) healthcare, (2) content moderation, (3) molecular property prediction, and (4) image annotation. Our results demonstrate that SSME estimates performance more accurately than do competing methods, reducing error by 5.1x relative to using labeled data alone and 2.4x relative to the next best competing method. SSME also improves accuracy when evaluating performance across subsets of the test distribution (e.g., specific demographic subgroups) and when evaluating the performance of language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11866v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divya Shanmugam, Shuvom Sadhuka, Manish Raghavan, John Guttag, Bonnie Berger, Emma Pierson</dc:creator>
    </item>
    <item>
      <title>From Niche to Mainstream: Community Size and Engagement in Social Media Conversations</title>
      <link>https://arxiv.org/abs/2501.12076</link>
      <description>arXiv:2501.12076v1 Announce Type: cross 
Abstract: The architecture of public discourse has been profoundly reshaped by social media platforms, which mediate interactions at an unprecedented scale and complexity. This study analyzes user behavior across six platforms over 33 years, exploring how the size of conversations and communities influences dialogue dynamics. Our findings reveal that smaller platforms foster richer, more sustained interactions, while larger platforms drive broader but shorter participation. Moreover, we observe that the propensity for users to re-engage in a conversation decreases as community size grows, with niche environments as a notable exception, where participation remains robust. These findings show an interdependence between platform architecture, user engagement, and community dynamics, shedding light on how digital ecosystems shape the structure and quality of public discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12076v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Nudo, Matteo Cinelli, Andrea Baronchelli, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists</title>
      <link>https://arxiv.org/abs/2501.12374</link>
      <description>arXiv:2501.12374v1 Announce Type: cross 
Abstract: Novel capacities of generative AI to analyze and generate cultural artifacts raise inevitable questions about the nature and value of artistic education and human expertise. Has AI already leveled the playing field between professional artists and laypeople, or do trained artistic expressive capacity, curation skills and experience instead enhance the ability to use these new tools? In this pre-registered study, we conduct experimental comparisons between 50 active artists and a demographically matched sample of laypeople. We designed two tasks to approximate artistic practice for testing their capabilities in both faithful and creative image creation: replicating a reference image, and moving as far away as possible from it. We developed a bespoke platform where participants used a modern text-to-image model to complete both tasks. We also collected and compared participants' sentiments towards AI. On average, artists produced more faithful and creative outputs than their lay counterparts, although only by a small margin. While AI may ease content creation, professional expertise is still valuable - even within the confined space of generative AI itself. Finally, we also explored how well an exemplary vision-capable large language model (GPT-4o) would complete the same tasks, if given the role of an image generation agent, and found it performed on par in copying but outperformed even artists in the creative task. The very best results were still produced by humans in both tasks. These outcomes highlight the importance of integrating artistic skills with AI training to prepare artists and other visual professionals for a technologically evolving landscape. We see a potential in collaborative synergy with generative AI, which could reshape creative industries and education in the arts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12374v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Narratives of War: Ukrainian Memetic Warfare on Twitter</title>
      <link>https://arxiv.org/abs/2309.08363</link>
      <description>arXiv:2309.08363v3 Announce Type: replace 
Abstract: The 2022 Russian invasion of Ukraine has seen an intensification in the use of social media by governmental actors in cyber warfare. Wartime communication via memes has been a successful strategy used not only by independent accounts such as @uamemesforces, but also-for the first time in a full-scale interstate war-by official Ukrainian government accounts such as @Ukraine and @DefenceU. We study this prominent example of memetic warfare through the lens of its narratives, and find them to be a key component of success: tweets with a 'victim' narrative garner twice as many retweets. However, malevolent narratives focusing on the enemy resonate more than those about heroism or victims with countries providing more assistance to Ukraine. Our findings present a nuanced examination of Ukraine's influence operations and of the worldwide response to it, thus contributing new insights into the evolution of socio-technical systems in times of war.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08363v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711037</arxiv:DOI>
      <arxiv:journal_reference>ACM SIGCHI Conference on Computer-Supported Cooperative Work &amp; Social Computing (CSCW) 2025</arxiv:journal_reference>
      <dc:creator>Yelena Mejova, Arthur Capozzi, Corrado Monti, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance</title>
      <link>https://arxiv.org/abs/2404.07966</link>
      <description>arXiv:2404.07966v3 Announce Type: replace 
Abstract: There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07966v3</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiangpeng Li, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Personhood credentials: Artificial intelligence and the value of privacy-preserving tools to distinguish who is real online</title>
      <link>https://arxiv.org/abs/2408.07892</link>
      <description>arXiv:2408.07892v4 Announce Type: replace 
Abstract: Anonymity is an important principle online. However, malicious actors have long used misleading identities to conduct fraud, spread disinformation, and carry out other deceptive schemes. With the advent of increasingly capable AI, bad actors can amplify the potential scale and effectiveness of their operations, intensifying the challenge of balancing anonymity and trustworthiness online. In this paper, we analyze the value of a new tool to address this challenge: "personhood credentials" (PHCs), digital credentials that empower users to demonstrate that they are real people -- not AIs -- to online services, without disclosing any personal information. Such credentials can be issued by a range of trusted institutions -- governments or otherwise. A PHC system, according to our definition, could be local or global, and does not need to be biometrics-based. Two trends in AI contribute to the urgency of the challenge: AI's increasing indistinguishability from people online (i.e., lifelike content and avatars, agentic activity), and AI's increasing scalability (i.e., cost-effectiveness, accessibility). Drawing on a long history of research into anonymous credentials and "proof-of-personhood" systems, personhood credentials give people a way to signal their trustworthiness on online platforms, and offer service providers new tools for reducing misuse by bad actors. In contrast, existing countermeasures to automated deception -- such as CAPTCHAs -- are inadequate against sophisticated AI, while stringent identity verification solutions are insufficiently private for many use-cases. After surveying the benefits of personhood credentials, we also examine deployment risks and design challenges. We conclude with actionable next steps for policymakers, technologists, and standards bodies to consider in consultation with the public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07892v4</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Adler, Zo\"e Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Ren\'ee DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</dc:creator>
    </item>
    <item>
      <title>Code-Driven Law NO, Normware SI!</title>
      <link>https://arxiv.org/abs/2410.17257</link>
      <description>arXiv:2410.17257v2 Announce Type: replace 
Abstract: With the digitalization of society, the interest, the debates and the research efforts concerning "code", "law", "artificial intelligence", and their various relationships, have been widely increasing. Yet, most arguments primarily focus on contemporary computational methods and artifacts (inferential models constructed via machine-learning methods, rule-based systems, smart contracts), rather than attempting to identify more fundamental mechanisms. Aiming to go beyond this conceptual limitation, this paper introduces and elaborates on "normware" as an explicit additional stance -- complementary to software and hardware -- for the interpretation and the design of artificial devices. By means of a few examples, I will argue that a normware-centred perspective provides a more adequate abstraction to study and design interactions between computational systems and human institutions, and may help with the design and development of technical interventions within wider socio-technical views.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17257v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Sileno</dc:creator>
    </item>
    <item>
      <title>Fairness Issues and Mitigations in (Differentially Private) Socio-Demographic Data Processes</title>
      <link>https://arxiv.org/abs/2408.08471</link>
      <description>arXiv:2408.08471v2 Announce Type: replace-cross 
Abstract: Statistical agencies rely on sampling techniques to collect socio-demographic data crucial for policy-making and resource allocation. This paper shows that surveys of important societal relevance introduce sampling errors that unevenly impact group-level estimates, thereby compromising fairness in downstream decisions. To address these issues, this paper introduces an optimization approach modeled on real-world survey design processes, ensuring sampling costs are optimized while maintaining error margins within prescribed tolerances. Additionally, privacy-preserving methods used to determine sampling rates can further impact these fairness issues. This paper explores the impact of differential privacy on the statistics informing the sampling process, revealing a surprising effect: not only is the expected negative effect from the addition of noise for differential privacy negligible, but also this privacy noise can in fact reduce unfairness as it positively biases smaller counts. These findings are validated over an extensive analysis using datasets commonly applied in census statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08471v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joonhyuk Ko, Juba Ziani, Saswat Das, Matt Williams, Ferdinando Fioretto</dc:creator>
    </item>
    <item>
      <title>On the Unknowable Limits to Prediction</title>
      <link>https://arxiv.org/abs/2411.19223</link>
      <description>arXiv:2411.19223v4 Announce Type: replace-cross 
Abstract: We propose a rigorous decomposition of predictive error, highlighting that not all 'irreducible' error is genuinely immutable. Many domains stand to benefit from iterative enhancements in measurement, construct validity, and modeling. Our approach demonstrates how apparently 'unpredictable' outcomes can become more tractable with improved data (across both target and features) and refined algorithms. By distinguishing aleatoric from epistemic error, we delineate how accuracy may asymptotically improve--though inherent stochasticity may remain--and offer a robust framework for advancing computational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19223v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Yan, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>A Systems Thinking Approach to Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2412.16641</link>
      <description>arXiv:2412.16641v4 Announce Type: replace-cross 
Abstract: Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then encode these beliefs as a series of causal graphs, enabling us to link AI/ML systems to politics and the law. This allows us to combine techniques from machine learning, causal inference, and system dynamics in order to capture different emergent aspects of the fairness problem. We can use systems thinking to help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a sociotechnical foundation for designing AI policy that is aligned to their political agendas and with society's values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16641v4</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Lam</dc:creator>
    </item>
    <item>
      <title>Potential and Perils of Large Language Models as Judges of Unstructured Textual Data</title>
      <link>https://arxiv.org/abs/2501.08167</link>
      <description>arXiv:2501.08167v2 Announce Type: replace-cross 
Abstract: Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLM-as-judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as judges. This LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLM-as-judge offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. Our research contributes to the growing body of knowledge on AI assisted text analysis. Further, we provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM-as-judge models across various contexts and use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08167v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar</dc:creator>
    </item>
  </channel>
</rss>
