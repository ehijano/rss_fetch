<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Apr 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hub Star Modeling 2.0 for Medallion Architecture</title>
      <link>https://arxiv.org/abs/2504.08788</link>
      <description>arXiv:2504.08788v1 Announce Type: new 
Abstract: Data warehousing enables performant access to high-quality data integrated from dynamic data sources. The medallion architecture, a standard for data warehousing, addresses these goals by organizing data into bronze, silver and gold layers, representing raw, integrated, and fit-to-purpose data, respectively. In terms of data modeling, bronze layer retains the structure of source data with additional metadata. The gold layer follows established modeling approaches such as star schema, snowflake, and flattened tables. The silver layer, acting as a canonical form, requires a flexible and scalable model to support continuous changes and incremental development. This paper introduces an enhanced Hub Star modeling approach tailored for the medallion architecture, simplifying silver-layer data modeling by generalizing hub and star concepts. This approach has been demonstrated using Databricks and the retail-org sample dataset, with all modeling and transformation scripts available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08788v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahram Salami</dc:creator>
    </item>
    <item>
      <title>Circuits and Formulas for Datalog over Semirings</title>
      <link>https://arxiv.org/abs/2504.08914</link>
      <description>arXiv:2504.08914v1 Announce Type: new 
Abstract: In this paper, we study circuits and formulas for provenance polynomials of Datalog programs. We ask the following question: given an absorptive semiring and a fact of a Datalog program, what is the optimal depth and size of a circuit/formula that computes its provenance polynomial? We focus on absorptive semirings as these guarantee the existence of a polynomial-size circuit. Our main result is a dichotomy for several classes of Datalog programs on whether they admit a formula of polynomial size or not. We achieve this result by showing that for these Datalog programs the optimal circuit depth is either $\Theta(\log m)$ or $\Theta(\log^2 m)$, where $m$ is the input size. We also show that for Datalog programs with the polynomial fringe property, we can always construct low-depth circuits of size $O(\log^2 m)$. Finally, we give characterizations of when Datalog programs are bounded over more general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08914v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austen Z. Fan, Paraschos Koutris, Sudeepa Roy</dc:creator>
    </item>
    <item>
      <title>Where Does Academic Database Research Go From Here?</title>
      <link>https://arxiv.org/abs/2504.08948</link>
      <description>arXiv:2504.08948v1 Announce Type: new 
Abstract: Panel proposal for an open forum to discuss and debate the future of database research in the context of industry, other research communities, and AI. Includes positions from panelists as well as a sample of the data management community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08948v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene Wu, Raul Castro Fernandez</dc:creator>
    </item>
    <item>
      <title>A Formalism and Library for Database Visualization</title>
      <link>https://arxiv.org/abs/2504.08979</link>
      <description>arXiv:2504.08979v1 Announce Type: new 
Abstract: Existing data visualization formalisms are restricted to single-table inputs, which makes existing visualization grammars like Vega-lite or ggplot2 tedious to use, have overly complex APIs, and unsound when visualization multi-table data. This paper presents the first visualization formalism to support databases as input -- in other words, *database visualization*. A visualization specification is defined as a mapping from database constraints (e.g., schemas, types, foreign keys) to visual representations of those constraints, and we state that a visualization is *faithful* if it visually preserves the underlying database constraints. This formalism explains how visualization designs are the result of implicit data modeling decisions. We further develop a javascript library called dvl and use a series of case studies to show its expressiveness over specialized visualization systems and existing grammar-based languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08979v1</guid>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene Wu, Xiang Yu Tuang, Antonio Li, Vareesh Bainwala</dc:creator>
    </item>
    <item>
      <title>Substitutability-Based Graph Node Pricing</title>
      <link>https://arxiv.org/abs/2504.09065</link>
      <description>arXiv:2504.09065v1 Announce Type: new 
Abstract: In the era o fdat commodification,the pricing o fgraph data presents unique challenges that differ significantly from traditional data markets. This paper addresses the critical issue of node pricing within graph structures, an area that has been largely overlooked in existing literature. We introduce a novel pricing mechanism based on the concept of substitutability, inspired by economic principles, to better reflect the ntrinsic value of nodes in a graph. Unlike previous studies that assumed known prices for nodes or subgraphs, our approach emphasizes the structural significance of nodes by employing a dominator tree, utilizing the Lengauer-Tarjan algorithm to extract dominance relationships. This innovative framework allows us to derive a more realistic pricing strategy that accounts for the unique connectivity and roles of nodes within their respective networks. Our comparative experiments demonstrate that the proposed method significantly outperforms existing pricing strategies, yielding high-quality solutions across various datasets. This research aims to contribute to the existing literature by addressing an important gap and providing insights that may assist in the more effective valuation of graph data, potentially supporting improved decision-making in data-driven environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09065v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiju Wang, Yuanyuan Gao, Zhengkui Wang, Xiao Yue</dc:creator>
    </item>
    <item>
      <title>Pneuma: Leveraging LLMs for Tabular Data Representation and Retrieval in an End-to-End System</title>
      <link>https://arxiv.org/abs/2504.09207</link>
      <description>arXiv:2504.09207v1 Announce Type: new 
Abstract: Finding relevant tables among databases, lakes, and repositories is the first step in extracting value from data. Such a task remains difficult because assessing whether a table is relevant to a problem does not always depend only on its content but also on the context, which is usually tribal knowledge known to the individual or team. While tools like data catalogs and academic data discovery systems target this problem, they rely on keyword search or more complex interfaces, limiting non-technical users' ability to find relevant data. The advent of large language models (LLMs) offers a unique opportunity for users to ask questions directly in natural language, making dataset discovery more intuitive, accessible, and efficient.
  In this paper, we introduce Pneuma, a retrieval-augmented generation (RAG) system designed to efficiently and effectively discover tabular data. Pneuma leverages large language models (LLMs) for both table representation and table retrieval. For table representation, Pneuma preserves schema and row-level information to ensure comprehensive data understanding. For table retrieval, Pneuma augments LLMs with traditional information retrieval techniques, such as full-text and vector search, harnessing the strengths of both to improve retrieval performance. To evaluate Pneuma, we generate comprehensive benchmarks that simulate table discovery workload on six real-world datasets including enterprise data, scientific databases, warehousing data, and open data. Our results demonstrate that Pneuma outperforms widely used table search systems (such as full-text search and state-of-the-art RAG systems) in accuracy and resource efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09207v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3725337</arxiv:DOI>
      <dc:creator>Muhammad Imam Luthfi Balaka, David Alexander, Qiming Wang, Yue Gong, Adila Krisnadhi, Raul Castro Fernandez</dc:creator>
    </item>
    <item>
      <title>Enhancing Productivity in Database Management Through AI: A Three-Phase Approach for Database</title>
      <link>https://arxiv.org/abs/2504.09288</link>
      <description>arXiv:2504.09288v1 Announce Type: new 
Abstract: This paper presents a novel AI-powered framework designed to streamline database management and query optimization for PostgreSQL systems. Structured in three phases: Natural Language to SQL Translation, Query Execution and Analysis, and Insight Generation, the approach empowers users with intuitive, intelligent interaction with databases. By leveraging advanced natural language processing and language models, the system enables non-technical users to extract meaningful insights from complex datasets, reducing the dependency on specialized DBAs. The framework also introduces iterative refinement of queries, automatic report generation, and support for temporal data forecasting. Experimental results demonstrate improved productivity, reduced query latency, and enhanced accuracy, validating the system's effectiveness across diverse business use cases. The solution was developed and evaluated at ABV-IIITM Gwalior, under the guidance of Prof. Dr. Arun Kuma</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09288v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kushagra Parashar, Ajay Dev, Aditya Kumar, Darpan Khatri</dc:creator>
    </item>
    <item>
      <title>Dupin: A Parallel Framework for Densest Subgraph Discovery in Fraud Detection on Massive Graphs (Technical Report)</title>
      <link>https://arxiv.org/abs/2504.09311</link>
      <description>arXiv:2504.09311v1 Announce Type: new 
Abstract: Detecting fraudulent activities in financial and e-commerce transaction networks is crucial. One effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. To address these challenges, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency. Dupin provides userfriendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations demonstrate that Dupin consistently outperforms several existing DSD methods, achieving performance improvements of up to 100 times compared to traditional approaches. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions from 45% to 94.5% and reduces density error from 30% to below 5%, as supported by our experimental results. These findings highlight the effectiveness of Dupin in real-world applications, ensuring both speed and accuracy in fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09311v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxin Jiang, Siyuan Yao, Yuchen Li, Qiange Wang, Bingsheng He, Min Chen</dc:creator>
    </item>
    <item>
      <title>A Categorical Unification for Multi-Model Data: Part II Categorical Algebra and Calculus</title>
      <link>https://arxiv.org/abs/2504.09515</link>
      <description>arXiv:2504.09515v1 Announce Type: new 
Abstract: Multi-model databases are designed to store, manage, and query data in various models, such as relational, hierarchical, and graph data, simultaneously. In this paper, we provide a theoretical basis for querying categorical databases. We propose two formal query languages: categorical calculus and categorical algebra, by extending relational calculus and relational algebra respectively. We demonstrate the equivalence between these two languages of queries. We propose a series of transformation rules of categorical algebra to facilitate query optimization. Finally, we analyze the expressive power and computation complexity for the proposed query languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09515v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaheng Lu</dc:creator>
    </item>
    <item>
      <title>Using Process Calculus for Optimizing Data and Computation Sharing in Complex Stateful Parallel Computations</title>
      <link>https://arxiv.org/abs/2504.09788</link>
      <description>arXiv:2504.09788v1 Announce Type: new 
Abstract: We propose novel techniques that exploit data and computation sharing to improve the performance of complex stateful parallel computations, like agent-based simulations. Parallel computations are translated into behavioral equations, a novel formalism layered on top of the foundational process calculus $\pi$-calculus. Behavioral equations blend code and data, allowing a system to easily compose and transform parallel programs into specialized programs. We show how optimizations like merging programs, synthesizing efficient message data structures, eliminating local messaging, rewriting communication instructions into local computations, and {aggregation pushdown} can be expressed as transformations of behavioral equations. We have also built a system called OptiFusion that implements behavioral equations and the aforementioned optimizations. Our experiments showed that OptiFusion is over 10$\times$ faster than state-of-the-art stateful systems benchmarked via complex stateful workloads. Generating specialized instructions that are impractical to write by hand allows OptiFusion to outperform even the hand-optimized implementations by up to 2$\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09788v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3725421</arxiv:DOI>
      <dc:creator>Zilu Tian, Dan Olteanu, Christoph Koch</dc:creator>
    </item>
    <item>
      <title>Rel: A Programming Language for Relational Data</title>
      <link>https://arxiv.org/abs/2504.10323</link>
      <description>arXiv:2504.10323v1 Announce Type: new 
Abstract: From the moment of their inception, languages for relational data have been described as sublanguages embedded in a host programming language. Rel is a new relational language whose key design goal is to go beyond this paradigm with features that allow for programming in the large, making it possible to fully describe end to end application semantics. With the new approach we can model the semantics of entire enterprise applications relationally, which helps significantly reduce architecture complexity and avoid the well-known impedance mismatch problem. This paradigm shift is enabled by 50 years of database research, making it possible to revisit the sublanguage/host language paradigm, starting from the fundamental principles. We present the main features of Rel: those that give it the power to express traditional query language operations and those that are designed to grow the language and allow programming in the large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10323v1</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3722212.3724450</arxiv:DOI>
      <dc:creator>Molham Aref, Paolo Guagliardo, George Kastrinis, Leonid Libkin, Victor Marsault, Wim Martens, Mary McGrath, Filip Murlak, Nathaniel Nystrom, Liat Peterfreund, Allison Rogers, Cristina Sirangelo, Domagoj Vrgoc, David Zhao, Abdul Zreika</dc:creator>
    </item>
    <item>
      <title>Streaming Democratized: Ease Across the Latency Spectrum with Delayed View Semantics and Snowflake Dynamic Tables</title>
      <link>https://arxiv.org/abs/2504.10438</link>
      <description>arXiv:2504.10438v1 Announce Type: new 
Abstract: Streaming data pipelines remain challenging and expensive to build and maintain, despite significant advancements in stronger consistency, event time semantics, and SQL support over the last decade. Persistent obstacles continue to hinder usability, such as the need for manual incrementalization, semantic discrepancies across SQL implementations, and the lack of enterprise-grade operational features. While the rise of incremental view maintenance (IVM) as a way to integrate streaming with databases has been a huge step forward, transaction isolation in the presence of IVM remains underspecified, leaving the maintenance of application-level invariants as a painful exercise for the user. Meanwhile, most streaming systems optimize for latencies of 100 ms to 3 sec, whereas many practical use cases are well-served by latencies ranging from seconds to tens of minutes.
  We present delayed view semantics (DVS), a conceptual foundation that bridges the semantic gap between streaming and databases, and introduce Dynamic Tables, Snowflake's declarative streaming transformation primitive designed to democratize analytical stream processing. DVS formalizes the intuition that stream processing is primarily a technique to eagerly compute derived results asynchronously, while also addressing the need to reason about the resulting system end to end. Dynamic Tables then offer two key advantages: ease of use through DVS, enterprise-grade features, and simplicity; as well as scalable cost efficiency via IVM with an architecture designed for diverse latency requirements.
  We first develop extensions to transaction isolation that permit the preservation of invariants in streaming applications. We then detail the implementation challenges of Dynamic Tables and our experience operating it at scale. Finally, we share insights into user adoption and discuss our vision for the future of stream processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10438v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3722212.3724455</arxiv:DOI>
      <dc:creator>Daniel Sotolongo, Daniel Mills, Tyler Akidau, Anirudh Santhiar, Attila-P\'eter T\'oth, Ilaria Battiston, Ankur Sharma, Botong Huang, Boyuan Zhang, Dzmitry Pauliukevich, Enrico Sartorello, Igor Belianski, Ivan Kalev, Lawrence Benson, Leon Papke, Ling Geng, Matt Uhlar, Nikhil Shah, Niklas Semmler, Olivia Zhou, Saras Nowak, Sasha Lionheart, Till Merker, Vlad Lifliand, Wendy Grus, Yi Huang, Yiwen Zhu</dc:creator>
    </item>
    <item>
      <title>Domain Specific Question to SQL Conversion with Embedded Data Balancing Technique</title>
      <link>https://arxiv.org/abs/2504.08753</link>
      <description>arXiv:2504.08753v1 Announce Type: cross 
Abstract: The rise of deep learning in natural language processing has fostered the creation of text to structured query language models composed of an encoder and a decoder. Researchers have experimented with various intermediate processing like schema linking, table type aware, value extract. To generate accurate SQL results for the user question. However error analysis performed on the failed cases on these systems shows, 29 percentage of the errors would be because the system was unable to understand the values expressed by the user in their question. This challenge affects the generation of accurate SQL queries, especially when dealing with domain-specific terms and specific value conditions, where traditional methods struggle to maintain consistency and precision. To overcome these obstacles, proposed two intermediations like implementing data balancing technique and over sampling domain-specific queries which would refine the model architecture to enhance value recognition and fine tuning the model for domain-specific questions. This proposed solution achieved 10.98 percentage improvement in accuracy of the model performance compared to the state of the art model tested on WikiSQL dataset. to convert the user question accurately to SQL queries. Applying oversampling technique on the domain-specific questions shown a significant improvement as compared with traditional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08753v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator> Jyothi, T. Satyanarayana Murthy</dc:creator>
    </item>
    <item>
      <title>Datum-wise Transformer for Synthetic Tabular Data Detection in the Wild</title>
      <link>https://arxiv.org/abs/2504.08829</link>
      <description>arXiv:2504.08829v1 Announce Type: cross 
Abstract: The growing power of generative models raises major concerns about the authenticity of published content. To address this problem, several synthetic content detection methods have been proposed for uniformly structured media such as image or text. However, little work has been done on the detection of synthetic tabular data, despite its importance in industry and government. This form of data is complex to handle due to the diversity of its structures: the number and types of the columns may vary wildly from one table to another. We tackle the tough problem of detecting synthetic tabular data ''in the wild'', i.e. when the model is deployed on table structures it has never seen before. We introduce a novel datum-wise transformer architecture and show that it outperforms existing models. Furthermore, we investigate the application of domain adaptation techniques to enhance the effectiveness of our model, thereby providing a more robust data-forgery detection solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08829v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.NE</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G. Charbel N. Kindji (IRISA, MALT), Elisa Fromont (MALT, IRISA), Lina Maria Rojas-Barahona, Tanguy Urvoy</dc:creator>
    </item>
    <item>
      <title>Federated Prototype Graph Learning</title>
      <link>https://arxiv.org/abs/2504.09493</link>
      <description>arXiv:2504.09493v1 Announce Type: cross 
Abstract: In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\% in accuracy while reducing communication costs by 168x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09493v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.SI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhengyu Wu, Xunkai Li, Yinlin Zhu, Rong-Hua Li, Guoren Wang, Chenghu Zhou</dc:creator>
    </item>
    <item>
      <title>Towards Unbiased Federated Graph Learning: Label and Topology Perspectives</title>
      <link>https://arxiv.org/abs/2504.09963</link>
      <description>arXiv:2504.09963v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09963v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.SI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhengyu Wu, Boyang Pang, Xunkai Li, Yinlin Zhu, Daohan Su, Bowen Fan, Rong-Hua Li, Guoren Wang, Chenghu Zhou</dc:creator>
    </item>
    <item>
      <title>Turn-taking annotation for quantitative and qualitative analyses of conversation</title>
      <link>https://arxiv.org/abs/2504.09980</link>
      <description>arXiv:2504.09980v1 Announce Type: cross 
Abstract: This paper has two goals. First, we present the turn-taking annotation layers created for 95 minutes of conversational speech of the Graz Corpus of Read and Spontaneous Speech (GRASS), available to the scientific community. Second, we describe the annotation system and the annotation process in more detail, so other researchers may use it for their own conversational data. The annotation system was developed with an interdisciplinary application in mind. It should be based on sequential criteria according to Conversation Analysis, suitable for subsequent phonetic analysis, thus time-aligned annotations were made Praat, and it should be suitable for automatic classification, which required the continuous annotation of speech and a label inventory that is not too large and results in a high inter-rater agreement. Turn-taking was annotated on two layers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP; similar to transition relevance places). We provide a detailed description of the annotation process and of segmentation and labelling criteria. A detailed analysis of inter-rater agreement and common confusions shows that agreement for IPU annotation is near-perfect, that agreement for PCOMP annotations is substantial, and that disagreements often are either partial or can be explained by a different analysis of a sequence which also has merit. The annotation system can be applied to a variety of conversational data for linguistic studies and technological applications, and we hope that the annotations, as well as the annotation system will contribute to a stronger cross-fertilization between these disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09980v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anneliese Kelterer, Barbara Schuppler</dc:creator>
    </item>
    <item>
      <title>AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference</title>
      <link>https://arxiv.org/abs/2504.10326</link>
      <description>arXiv:2504.10326v1 Announce Type: cross 
Abstract: AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10326v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yangshen Deng, Zhengxin You, Long Xiang, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang</dc:creator>
    </item>
    <item>
      <title>Revisiting the Expressiveness Landscape of Data Graph Queries</title>
      <link>https://arxiv.org/abs/2406.17871</link>
      <description>arXiv:2406.17871v2 Announce Type: replace 
Abstract: The study of graph queries in database theory has spanned more than three decades, resulting in a multitude of proposals for graph query languages. These languages differ in the mechanisms. We can identify three main families of languages, with the canonical representatives being: (1) regular path queries, (2) walk logic, and (3) first-order logic with transitive closure operators. This paper provides a complete picture of the expressive power of these languages in the context of data graphs. Specifically, we consider a graph data model that supports querying over both data and topology. For example, "Does there exist a path between two different persons in a social network with the same last name?". We also show that an extension of (1), augmented with transitive closure operators, can unify the expressivity of (1)--(3) without increasing the query evaluation complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17871v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Benedikt, Anthony Widjaja Lin, Di-De Yen</dc:creator>
    </item>
    <item>
      <title>Table Integration in Data Lakes Unleashed: Pairwise Integrability Judgment, Integrable Set Discovery, and Multi-Tuple Conflict Resolution</title>
      <link>https://arxiv.org/abs/2412.00324</link>
      <description>arXiv:2412.00324v2 Announce Type: replace 
Abstract: Table integration aims to create a comprehensive table by consolidating tuples containing relevant information. In this work, we investigate the challenge of integrating multiple tables from a data lake, focusing on three core tasks: 1) pairwise integrability judgment, which determines whether a tuple pair is integrable, accounting for any occurrences of semantic equivalence or typographical errors; 2) integrable set discovery, which identifies all integrable sets in a table based on pairwise integrability judgments established in the first task; 3) multi-tuple conflict resolution, which resolves conflicts between multiple tuples during integration. To this end, we train a binary classifier to address the task of pairwise integrability judgment. Given the scarcity of labeled data in data lakes, we propose a self-supervised adversarial contrastive learning algorithm to perform classification, which incorporates data augmentation methods and adversarial examples to autonomously generate new training data. Upon the output of pairwise integrability judgment, each integrable set can be considered as a community, a densely connected sub-graph where nodes and edges correspond to tuples in the table and their pairwise integrability respectively, we proceed to investigate various community detection algorithms to address the integrable set discovery objective. Moving forward to tackle multi-tuple conflict resolution, we introduce an innovative in-context learning methodology. This approach capitalizes on the knowledge embedded within large language models (LLMs) to effectively resolve conflicts that arise when integrating multiple tuples. Notably, our method minimizes the need for annotated data, making it particularly suited for scenarios where labeled datasets are scarce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00324v2</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daomin Ji, Hui Luo, Zhifeng Bao, Shane Culpepper</dc:creator>
    </item>
    <item>
      <title>Finding Near-Optimal Maximum Set of Disjoint $k$-Cliques in Real-World Social Networks</title>
      <link>https://arxiv.org/abs/2503.20299</link>
      <description>arXiv:2503.20299v2 Announce Type: replace-cross 
Abstract: A $k$-clique is a dense graph, consisting of $k$ fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint $k$-cliques in a given large real-world graph with a user-defined fixed number $k$, which can contribute to a good performance of teaming collaborative events in online games. However, this problem is NP-hard when $k \geq 3$, making it difficult to solve. To address that, we propose an efficient lightweight method that avoids significant overheads and achieves a $k$-approximation to the optimal, which is equipped with several optimization techniques, including the ordering method, degree estimation in the clique graph, and a lightweight implementation. Besides, to handle dynamic graphs that are widely seen in real-world social networks, we devise an efficient indexing method with careful swapping operations, leading to the efficient maintenance of a near-optimal result with frequent updates in the graph. In various experiments on several large graphs, our proposed approaches significantly outperform the competitors by up to 2 orders of magnitude in running time and 13.3\% in the number of computed disjoint $k$-cliques, which demonstrates the superiority of the proposed approaches in terms of efficiency and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20299v2</guid>
      <category>cs.SI</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqing Lin, Xin Chen, Haoxuan Xie, Sibo Wang, Siqiang Luo</dc:creator>
    </item>
  </channel>
</rss>
