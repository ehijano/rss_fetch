<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Open Workflow Model for Improving Educational Video Design: Tools, Data, and Insights</title>
      <link>https://arxiv.org/abs/2512.16254</link>
      <description>arXiv:2512.16254v1 Announce Type: new 
Abstract: Educational videos are widely used across various instructional models in higher education to support flexible and self-paced learning. However, student engagement with these videos varies significantly depending on how they are designed. While several studies have identified potential influencing factors, there remains a lack of scalable tools and open datasets to support large-scale, data-driven improvements in video design. This study aims to advance data-driven approaches to educational video design. Its core contributions include: (1) a workflow model for analysing educational videos; (2) an open-source implementation for extracting video metadata and features; (3) an accessible, community-driven database of video attributes; (4) a case study applying the approach to two engineering courses; and (5) an initial machine learning-based analysis to explore the relative influence of various video characteristics on student engagement. This work lays the groundwork for a shared, evidence-based approach to educational video design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16254v1</guid>
      <category>stat.AP</category>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohamed Tolba, Olivia Kendall, Daniel Tudball Smith, Alexander Gregg, Tony Vo, Scott Wordley</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure</title>
      <link>https://arxiv.org/abs/2512.16463</link>
      <description>arXiv:2512.16463v1 Announce Type: new 
Abstract: Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model (JM) using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients. We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score (IBS) and Integrated Calibration Index (ICI). The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable. These findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16463v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Farina, Francois Mercier, Christian Wohlfart, Serge Masson, Silvia Metelli</dc:creator>
    </item>
    <item>
      <title>Opening the House: Datasets for Mixed Doubles Curling</title>
      <link>https://arxiv.org/abs/2512.16574</link>
      <description>arXiv:2512.16574v1 Announce Type: new 
Abstract: We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16574v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robyn Ritchie, Alexandre Leblanc, Thomas Loughin</dc:creator>
    </item>
    <item>
      <title>Quantile-based causal inference for spatio-temporal processes: Assessing the impacts of wildfires on US air quality</title>
      <link>https://arxiv.org/abs/2512.16603</link>
      <description>arXiv:2512.16603v1 Announce Type: new 
Abstract: Wildfires pose an increasingly severe threat to air quality, yet quantifying their causal impact remains challenging due to unmeasured meteorological and geographic confounders. Moreover, wildfire impacts on air quality may exhibit heterogeneous effects across pollution levels, which conventional mean-based causal methods fail to capture. To address these challenges, we develop a Quantile-based Latent Spatial Confounder Model (QLSCM) that substitutes conditional expectations with conditional quantiles, enabling causal analysis across the entire outcome distribution. We establish the causal interpretation of QLSCM theoretically, prove the identifiability of causal effects, and demonstrate estimator consistency under mild conditions. Simulations confirm the bias correction capability and the advantage of quantile-based inference over mean-based approaches. Applying our method to contiguous US wildfire and air quality data, we uncover important heterogeneous effects: fire radiative power exerts significant positive causal effects on aerosol optical depth at high quantiles in Western states like California and Oregon, while insignificant at lower quantiles. This indicates that wildfire impacts on air quality primarily manifest during extreme pollution events. Regional analyses reveal that Western and Northwestern regions experience the strongest causal effects during such extremes. These findings provide critical insights for environmental policy by identifying where and when mitigation efforts would be most effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16603v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zipei Geng, Jordan Richards, Raphael Huser, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Hidden Order in Trades Predicts the Size of Price Moves</title>
      <link>https://arxiv.org/abs/2512.15720</link>
      <description>arXiv:2512.15720v1 Announce Type: cross 
Abstract: Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p &lt; 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15720v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mainak Singha</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation for Scaled Inhomogeneous Phase-Type Distributions from Discrete Observations</title>
      <link>https://arxiv.org/abs/2512.16061</link>
      <description>arXiv:2512.16061v1 Announce Type: cross 
Abstract: Inhomogeneous phase-type (IPH) distributions extend classical phase-type models by allowing transition intensities to vary over time, offering greater flexibility for modeling heavy-tailed or time-dependent absorption phenomena. We focus on the subclass of IPH distributions with time-scaled sub-intensity matrices of the form ${\Lambda}(t) = h_{\beta}(t){\Lambda}$, which admits a time transformation to a homogeneous Markov jump process. For this class, we develop a statistical inference framework for discretely observed trajectories that combines Markov-bridge reconstruction with a stochastic EM algorithm and a gradient-based up- date. The resulting method yields joint maximum-likelihood estimates of both the baseline sub-intensity matrix ${\Lambda}$ and the time-scaling parameter $\beta$. Through simulation studies for the matrix-Gompertz and matrix-Weibull families, and a real-data application to coronary allograft vasculopathy progression, we demonstrate that the proposed approach provides an accurate and computationally tractable tool for fitting time-scaled IPH models to irregular multi-state data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16061v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Baltazar-Larios, Alejandra Quintos</dc:creator>
    </item>
    <item>
      <title>Hazard-based distributional regression via ordinary differential equations</title>
      <link>https://arxiv.org/abs/2512.16336</link>
      <description>arXiv:2512.16336v1 Announce Type: cross 
Abstract: The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16336v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection</title>
      <link>https://arxiv.org/abs/2512.16411</link>
      <description>arXiv:2512.16411v1 Announce Type: cross 
Abstract: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Louis Perot</dc:creator>
    </item>
    <item>
      <title>Short-term CO2 emissions forecasting: insight from the Italian electricity market</title>
      <link>https://arxiv.org/abs/2507.12992</link>
      <description>arXiv:2507.12992v2 Announce Type: replace 
Abstract: This study investigates the short-term forecasting of carbon emissions from electricity generation in the Italian power market. Using hourly data from 2021 to 2023, several statistical models and forecast combination methods are evaluated and compared at the national and zonal levels. Four main model classes are considered: (i) linear parametric models, such as seasonal autoregressive integrated moving average and its exogenous variable extension; (ii) functional parametric models, including seasonal functional autoregressive models, with and without exogenous variables; (iii) (semi) non-parametric and possibly non-linear models, notably the generalised additive model (GAM) and TBATS (trigonometric seasonality, Box-Cox transformation, ARMA errors, trend, and seasonality); and (iv) a semi-functional approach based on the K-nearest neighbours. Forecast combinations include simple averaging, the optimal Bates and Granger weighting scheme, and a selection-based strategy that chooses the best model for each hour. The results show that the GAM produces the most accurate forecasts during the daytime hours, while the functional parametric models perform best in the early morning. Among the combination methods, the simple average and the selection-based approaches consistently outperform all individual models. The findings underscore the value of hybrid forecasting frameworks in improving the accuracy and reliability of short-term carbon emissions predictions in power systems. In addition, they highlight the importance of considering zonal specificities when implementing flexible energy demand strategies, as the timing of low-carbon emissions varies between market zones throughout the day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12992v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Francesco Lisi</dc:creator>
    </item>
    <item>
      <title>Clustered Network Connectedness: A New Measurement Framework with Application to Global Equity Markets</title>
      <link>https://arxiv.org/abs/2502.15458</link>
      <description>arXiv:2502.15458v2 Announce Type: replace-cross 
Abstract: Network connections, both across and within markets, are central in countless economic contexts. In recent decades, a large literature has developed and applied flexible methods for measuring network connectedness and its evolution, based on variance decompositions from vector autoregressions (VARs), as in Diebold and Yilmaz (2014). Those VARs are, however, typically identified using full orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran and Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special and extreme cases of a more general framework that we develop in this paper. In particular, we allow network nodes to be connected in ``clusters", such as asset classes, industries, regions, etc., where shocks are orthogonal across clusters (Sims style orthogonalized identification) but correlated within clusters (Koop-Pesaran-Potter-Shin style generalized identification), so that the ordering of network nodes is relevant across clusters but irrelevant within clusters. After developing the clustered connectedness framework, we apply it in a detailed empirical exploration of sixteen country equity markets spanning three global regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15458v2</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bastien Buchwalter, Francis X. Diebold, Kamil Yilmaz</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Learning for Discrete Choice</title>
      <link>https://arxiv.org/abs/2505.18077</link>
      <description>arXiv:2505.18077v2 Announce Type: replace-cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18077v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel F. Villarraga, Ricardo A. Daziano</dc:creator>
    </item>
    <item>
      <title>The Role of Congeniality in Multiple Imputation for Doubly Robust Causal Estimation</title>
      <link>https://arxiv.org/abs/2510.11633</link>
      <description>arXiv:2510.11633v2 Announce Type: replace-cross 
Abstract: This paper provides clear and practical guidance on the specification of imputation models when multiple imputation is used in conjunction with doubly robust estimation methods for causal inference. Through theoretical arguments and targeted simulations, we demonstrate that if a confounder has missing data, the corresponding imputation model must include all variables appearing in either the propensity score model or the outcome model, in addition to both the exposure and the outcome, and that these variables must enter the imputation model in the same functional form as in the final analysis. Violating these conditions can lead to biased treatment effect estimates, even when both components of the doubly robust estimator are correctly specified. We present a mathematical framework for doubly robust estimation combined with multiple imputation, establish the theoretical requirements for proper imputation in this setting, and demonstrate the consequences of misspecification through simulation. Based on these findings, we offer concrete recommendations to ensure valid inference when using multiple imputation with doubly robust methods in applied causal analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11633v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan</dc:creator>
    </item>
  </channel>
</rss>
