<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IT</link>
    <description>cs.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Optimality of Coded Distributed Computing for Ring Networks</title>
      <link>https://arxiv.org/abs/2507.00091</link>
      <description>arXiv:2507.00091v1 Announce Type: new 
Abstract: We consider a coded distributed computing problem in a ring-based communication network, where $N$ computing nodes are arranged in a ring topology and each node can only communicate with its neighbors within a constant distance $d$. To mitigate the communication bottleneck in exchanging intermediate values, we propose new coded distributed computing schemes for the ring-based network that exploit both ring topology and redundant computation (i.e., each map function is computed by $r$ nodes). Two typical cases are considered: all-gather where each node requires all intermediate values mapped from all input files, and all-to-all where each node requires a distinct set of intermediate values from other nodes. For the all-gather case, we propose a new coded scheme based on successive reverse carpooling where nodes transmit every encoded packet containing two messages traveling in opposite directions along the same path. Theoretical converse proof shows that our scheme achieves the optimal tradeoff between communication load, computation load $r$, and broadcast distance $d$ when $N\gg d$. For the all-to-all case, instead of simply repeating our all-gather scheme, we delicately deliver intermediate values based on their proximity to intended nodes to reduce unnecessary transmissions. We derive an information-theoretic lower bound on the optimal communication load and show that our scheme is asymptotically optimal under the cyclic placement when $N\gg r$. The optimality results indicate that in ring-based networks, the redundant computation $r$ only leads to an additive gain in reducing communication load while the broadcast distance $d$ contributes to a multiplicative gain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00091v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenhao Huang, Minquan Cheng, Kai Wan, Qifu Tyler Sun, Youlong Wu</dc:creator>
    </item>
    <item>
      <title>Wireless AI Evolution: From Statistical Learners to Electromagnetic-Guided Foundation Models</title>
      <link>https://arxiv.org/abs/2507.00366</link>
      <description>arXiv:2507.00366v1 Announce Type: new 
Abstract: While initial applications of artificial intelligence (AI) in wireless communications over the past decade have demonstrated considerable potential using specialized models for targeted communication tasks, the revolutionary demands of sixth-generation (6G) networks for holographic communications, ubiquitous sensing, and native intelligence are propelling a necessary evolution towards AI-native wireless networks. The arrival of large AI models paves the way for the next phase of Wireless AI, driven by wireless foundation models (WFMs). In particular, pre-training on universal electromagnetic (EM) principles equips WFMs with the essential adaptability for a multitude of demanding 6G applications. However, existing large AI models face critical limitations, including pre-training strategies disconnected from EM-compliant constraints leading to physically inconsistent predictions, a lack of embedded understanding of wave propagation physics, and the inaccessibility of massive labeled datasets for comprehensive EM-aware training. To address these challenges, this article presents an electromagnetic information theory-guided self-supervised pre-training (EIT-SPT) framework designed to systematically inject EM physics into WFMs. The EIT-SPT framework aims to infuse WFMs with intrinsic EM knowledge, thereby enhancing their physical consistency, generalization capabilities across varied EM landscapes, and overall data efficiency. Building upon the proposed EIT-SPT framework, this article first elaborates on diverse potential applications in 6G scenarios of WFMs, then validates the efficacy of the proposed framework through illustrative case studies, and finally summarizes critical open research challenges and future directions for WFMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00366v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Xiao, Ji Wang, Kunrui Cao, Xingwang Li, Zhao Chen, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Accuracy and Security-Guaranteed Participant Selection and Beamforming Design for RIS-Assisted Federated Learning</title>
      <link>https://arxiv.org/abs/2507.00388</link>
      <description>arXiv:2507.00388v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as an effective approach for training neural network models without requiring the sharing of participants' raw data, thereby addressing data privacy concerns. In this paper, we propose a reconfigurable intelligent surface (RIS)-assisted FL framework in the presence of eavesdropping, where partial edge devices are selected to participate in the FL training process. In contrast, the remaining devices serve as cooperative jammers by transmitting jamming signals to disrupt eavesdropping. We aim to minimize the training latency in each FL round by jointly optimizing participant selection, bandwidth allocation, and RIS beamforming design, subject to the convergence accuracy of FL and the secure uploading requirements. To solve the resulting mixed-integer nonlinear programming problem, we propose a twin delayed deep deterministic policy gradient (TD3) algorithm. Simulation results demonstrate that the proposed scheme reduces the FL training latency by approximately 27$\%$ compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00388v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengru Wu, Yu Gao, Weidang Lu, Huimei Han, Lei Sun, Wanli Ni</dc:creator>
    </item>
    <item>
      <title>Construction of LDPC convolutional codes with large girth from Latin squares</title>
      <link>https://arxiv.org/abs/2507.00591</link>
      <description>arXiv:2507.00591v1 Announce Type: new 
Abstract: Due to their capacity approaching performance low-density parity-check (LDPC) codes gained a lot of attention in the last years. The parity-check matrix of the codes can be associated with a bipartite graph, called Tanner graph. To decrease the probability of decoding failure it is desirable to have LDPC codes with large girth of the associated Tanner graph. Moreover, to store such codes efficiently, it is desirable to have compact constructions for them. In this paper, we present constructions of LDPC convolutional codes with girth up to $12$ using a special class of Latin squares and several lifting steps, which enables a compact representation of these codes. With these techniques, we can provide constructions for well-performing and efficiently storable time-varying and time-invariant LDPC convolutional codes as well as for LDPC block codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00591v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elisa Junghans, Julia Lieb</dc:creator>
    </item>
    <item>
      <title>On the rank weight hierarchy of $M$-codes</title>
      <link>https://arxiv.org/abs/2507.00609</link>
      <description>arXiv:2507.00609v1 Announce Type: new 
Abstract: We study the rank weight hierarchy of linear codes which are stable under a linear endomorphism defined over the base field, in particular when the endomorphism is cyclic. In this last case, we give a necessary and sufficient condition for such a code to have first rank weight equal to $1$ in terms of its generator polynomial, as well as an explicit formula for its last rank weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00609v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>G. Berhuy, J. Molina</dc:creator>
    </item>
    <item>
      <title>Decentralized Pliable Index Coding For Federated Learning In Intelligent Transportation Systems</title>
      <link>https://arxiv.org/abs/2507.00643</link>
      <description>arXiv:2507.00643v1 Announce Type: new 
Abstract: Federated Learning is a promising option for data privacy and security in ITS, because it allows edge devices, Road Side Units (RSUs), and Central Server (CS) to jointly train the machine learning model. Since RSU collects data from the vehicles passing through its range, the local data of each RSU will have a non-IID distribution, which adversely affects the convergence speed and accuracy of FL training. Generating synthetic data locally at individual nodes, followed by data shuffling among the nodes, is a promising approach to address the Non-IID data problem. In this work, we propose pliable index coding (PIC) solutions for efficient data shuffling among the nodes in an FL system. In PIC($S$) problems, a client is satisfied if it can retrieve any $S$ new messages not originally present in its side-information. We particularly consider decentralized pliable index coding problems (DPIC) where the clients communicate among themselves without a central server to model the data shuffling in FL. A class of DPIC, known as Consecutive Decentralized Pliable Index Coding (CDPIC($S$,$K$)), where each client has $K$ consecutive messages as side-information, is considered. For CDPIC($S$,$K$) problems, pliable index code designs are provided for any value of $K$ and $S$, and optimality proofs for some of the cases are established. Further, these CDPIC solutions are applied for data shuffling in FL, to transform the local data distribution towards IID progressively with each transmission, thereby enhancing the performance of FL. The improvement in the accuracy and convergence of the most popular FL technique, FedAvg, and a promising federated submodel technique, CELL (Communication Efficient Lottery Learning), are analysed by providing different degrees of data shuffling using the proposed CDPIC schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00643v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sadina Kadakkottiri, Narisetty Harish, Nujoom Sageer Karat, Deepthi Paramel Pattathil, Balaji Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>The Rate-Distortion Function for Sampled Cyclostationary Gaussian Processes with Memory and with Bounded Processing Delay: Extended Version with Proofs</title>
      <link>https://arxiv.org/abs/2507.00656</link>
      <description>arXiv:2507.00656v1 Announce Type: new 
Abstract: We study the rate-distortion function (RDF) for the lossy compression of discrete-time (DT) wide-sense almost cyclostationary (WSACS) Gaussian processes with memory, arising from sampling continuous-time (CT) wide-sense cyclostationary (WSCS) Gaussian source processes. The importance of this problem arises as such CT processes represent communications signals, and sampling must be applied to facilitate the DT processing associated with their compression. Moreover, the physical characteristics of oscillators imply that the sampling interval is incommensurate with the period of the autocorrelation function (AF) of the physical process, giving rise to the DT WSACS model considered. In addition, to reduce the loss, the sampling interval is generally shorter than the correlation length, and thus, the DT process is correlated as well. The difficulty in the RDF characterization follows from the information-instability of WSACS processes, which renders the traditional information-theoretic tools inapplicable. In this work we utilize the information-spectrum framework to characterize the RDF when a finite and bounded delay is allowed between processing of subsequent source sequences. This scenario extends our previous works which studied settings without processing delays or without memory. Numerical evaluations reveal the impact of scenario parameters on the RDF with asynchronous sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00656v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zikun Tan, Ron Dabora, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>On Hierarchical Coded Caching with Offline Users</title>
      <link>https://arxiv.org/abs/2507.00727</link>
      <description>arXiv:2507.00727v1 Announce Type: new 
Abstract: This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00727v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rashid Ummer N. T., B. Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>MichelangeRoll: Sculpting Rational Distributions Exactly and Efficiently</title>
      <link>https://arxiv.org/abs/2507.00915</link>
      <description>arXiv:2507.00915v1 Announce Type: new 
Abstract: Simulating an arbitrary discrete distribution $D \in [0, 1]^n$ using fair coin tosses incurs trade-offs between entropy complexity and space and time complexity. Shannon's theory suggests that $H(D)$ tosses are necessary and sufficient, but does not guarantee exact distribution. Knuth and Yao showed that a decision tree consumes fewer than $H(D) + 2$ tosses for one exact sample. Drapper and Saad's recent work addresses the space and time aspect, showing that $H(D) + 2$ tosses, $O(n \log(n) \log(m))$ memory, and $O(H(D))$ operations are all it costs, where $m$ is the common denominator of the probability masses in $D$ and $n$ is the number of possible outcomes.
  In this paper, MichelangeRoll recycles leftover entropy to break the "$+2$" barrier. With $O((n + 1/\varepsilon) \log(m/\varepsilon))$ memory, the entropy cost of generating a ongoing sequence of $D$ is reduced to $H(D) + \varepsilon$ per sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00915v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jui-Hsiang Shao, Hsin-Po Wang</dc:creator>
    </item>
    <item>
      <title>Optimal Feedback Schemes for Dirty Paper Channels With State Estimation at the Receiver</title>
      <link>https://arxiv.org/abs/2507.00942</link>
      <description>arXiv:2507.00942v1 Announce Type: new 
Abstract: In the literature, it has been shown that feedback does not increase the optimal rate-distortion region of the dirty paper channel with state estimation at the receiver (SE-R). On the other hand, it is well-known that feedback helps to construct low-complexity coding schemes in Gaussian channels, such as the elegant Schalkwijk-Kailath (SK) feedback scheme. This motivates us to explore capacity-achieving SK-type schemes in dirty paper channels with SE-R and feedback. In this paper, we first propose a capacity-achieving feedback scheme for the dirty paper channel with SE-R (DPC-SE-R), which combines the superposition coding and the classical SK-type scheme. Then, we extend this scheme to the dirty paper multiple-access channel with SE-R and feedback, and also show the extended scheme is capacity-achieving. Finally, we discuss how to extend our scheme to a noisy state observation case of the DPC-SE-R. However, the capacity-achieving SK-type scheme for such a case remains unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00942v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengfeng Xia, Han Deng, Haonan Zhang, Fan Cheng, Bin Dai, Liuguo Yin</dc:creator>
    </item>
    <item>
      <title>AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise</title>
      <link>https://arxiv.org/abs/2507.00145</link>
      <description>arXiv:2507.00145v1 Announce Type: cross 
Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform entropy directly from physical noise, eliminating the need for bulky quantum devices or expensive laboratory-grade RF receivers. Instead, it relies on a low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number generators (RNGs), our dynamic inner-outer network couples adaptive natural sources and reseeding, yielding truly unpredictable and autonomous sequences. Generated numbers pass the NIST SP 800-22 battery better than a CPU-based method. It also passes nineteen bespoke statistical tests for both bit- and integer-level analysis. All results satisfy cryptographic standards, while forward and backward prediction experiments reveal no exploitable biases. The model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG broadens the reach of high-integrity random number generators across secure systems, cryptographic protocols, embedded and edge devices, stochastic simulations, and server applications that need randomness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00145v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Yi\u{g}it</dc:creator>
    </item>
    <item>
      <title>Fully Parallelized BP Decoding for Quantum LDPC Codes Can Outperform BP-OSD</title>
      <link>https://arxiv.org/abs/2507.00254</link>
      <description>arXiv:2507.00254v1 Announce Type: cross 
Abstract: In this work, we propose a lightweight decoder based solely on belief-propagation (BP), augmented with a speculative post-processing strategy inspired by classical Chase decoding. Our method identifies unreliable bits via BP oscillation statistics, generates a set of modified test patterns, and decodes them in parallel using low-iteration BP. We demonstrate that our approach can achieve logical error rates comparable to or even better than BP-OSD, but has lower latency over its parallelization for a variety of bivariate bicycle codes, which significantly reduces decoding complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00254v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Wang, Ang Li, Frank Mueller</dc:creator>
    </item>
    <item>
      <title>Best Agent Identification for General Game Playing</title>
      <link>https://arxiv.org/abs/2507.00451</link>
      <description>arXiv:2507.00451v1 Announce Type: cross 
Abstract: We present an efficient and generalised procedure to accurately identify the best performing algorithm for each sub-task in a multi-problem domain. Our approach treats this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific task and each arm corresponds to a specific algorithm or agent. We propose an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction. We evaluate the performance of Optimistic-WS on two of the most popular general game domains, the General Video Game AI (GVGAI) framework and the Ludii general game playing system, with the goal of identifying the highest performing agent for each game within a limited number of trials. Compared to previous best arm identification algorithms for multi-armed bandits, our results demonstrate a substantial performance improvement in terms of average simple regret. This novel approach can be used to significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks, as well as other multi-task domains with high algorithm runtimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00451v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Stephenson, Alex Newcombe, Eric Piette, Dennis Soemers</dc:creator>
    </item>
    <item>
      <title>Linear rank-metric intersecting codes</title>
      <link>https://arxiv.org/abs/2507.00569</link>
      <description>arXiv:2507.00569v1 Announce Type: cross 
Abstract: In this paper we introduce and investigate rank-metric intersecting codes, a new class of linear codes in the rank-metric context, inspired by the well-studied notion of intersecting codes in the Hamming metric. A rank-metric code is said to be intersecting if any two nonzero codewords have supports intersecting non trivially. We explore this class from both a coding-theoretic and geometric perspective, highlighting its relationship with minimal codes, MRD codes, and Hamming-metric intersecting codes. We derive structural properties, sufficient conditions based on minimum distance, and geometric characterizations in terms of 2-spannable $q$-systems. We establish upper and lower bounds on code parameters and show some constructions, which leave a range of unexplored parameters. Finally, we connect rank-intersecting codes to other combinatorial structures such as $(2,1)$-separating systems and frameproof codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00569v1</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Bartoli, Martino Borello, Giuseppe Marino, Martin Scotti</dc:creator>
    </item>
    <item>
      <title>On the Cost of Consecutive Estimation Error: Significance-Aware Non-linear Aging</title>
      <link>https://arxiv.org/abs/2410.03637</link>
      <description>arXiv:2410.03637v2 Announce Type: replace 
Abstract: This paper considers the semantics-aware remote state estimation of an asymmetric Markov chain with prioritized states. Due to resource constraints, the sensor needs to trade between estimation quality and communication cost. The aim is to exploit the significance of information through the history of system realizations to determine the optimal timing of transmission, thereby reducing the amount of uninformative data transmitted in the network. To this end, we introduce a new metric, the significance-aware Age of Consecutive Error (AoCE), that captures two semantic attributes: the significance of estimation error and the cost of consecutive error. Different costs and non-linear age functions are assigned to different estimation errors to account for their relative importance to system performance. We identify the optimal transmission problem as a countably infinite state Markov decision process (MDP) with unbounded costs. We first give sufficient conditions on the age functions, source pattern, and channel reliability so that an optimal policy exists to have bounded average costs. We show that the optimal policy exhibits a switching structure. That is, the sensor triggers a transmission only when the system has been trapped in an error for a certain number of consecutive time slots. We also provide sufficient conditions under which the switching policy degenerates into a simple threshold policy, i.e., featuring identical thresholds for all estimation errors. Furthermore, we exploit the structural properties and develop a structured policy iteration (SPI) algorithm that considerably reduces computation overhead. Numerical results show that the optimal policy outperforms the classic rule-, distortion- and age-based policies. An important takeaway is that the more semantic attributes we utilize, the fewer transmissions are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03637v2</guid>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiping Luo, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>Optimally Decoding Two-Dimensional Reed-Solomon Codes Against Deletion Errors</title>
      <link>https://arxiv.org/abs/2412.20771</link>
      <description>arXiv:2412.20771v2 Announce Type: replace 
Abstract: Constructing Reed-Solomon (RS) codes that can correct insertion and deletion (ins-del) errors has been the focus of several recent studies. However, efficient decoding algorithms for such codes have received less attention and remain a significant open problem. In this work, we take a first step toward addressing this problem by designing a decoding algorithm for the case of $2$-dimensional RS codes that can correct deletions up to the half-Singleton bound and is optimal in terms of field operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20771v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubhransh Singhvi</dc:creator>
    </item>
    <item>
      <title>Source-Channel Separation Theorems for Distortion Perception Coding</title>
      <link>https://arxiv.org/abs/2501.17706</link>
      <description>arXiv:2501.17706v2 Announce Type: replace 
Abstract: It is well known that separation between lossy source coding and channel coding is asymptotically optimal under classical additive distortion measures. Recently, coding under a new class of quality considerations, often referred to as perception or realism, has attracted significant attention due to its close connection to neural generative models and semantic communications. In this work, we revisit source-channel separation under the consideration of distortion-perception. We show that when the perception quality is measured on the block level, i.e., in the strong-sense, the optimality of separation still holds when common randomness is shared between the encoder and the decoder; however, separation is no longer optimal when such common randomness is not available. In contrast, when the perception quality is the average per-symbol measure, i.e., in the weak-sense, the optimality of separation holds regardless of the availability of common randomness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17706v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Tian, Jun Chen, Krishna Narayanan</dc:creator>
    </item>
    <item>
      <title>SKALD: Scalable K-Anonymisation for Large Datasets</title>
      <link>https://arxiv.org/abs/2505.03529</link>
      <description>arXiv:2505.03529v2 Announce Type: replace 
Abstract: Data privacy and anonymisation are critical concerns in today's data-driven society, particularly when handling personal and sensitive user data. Regulatory frameworks worldwide recommend privacy-preserving protocols such as k-anonymisation to de-identify releases of tabular data. Available hardware resources provide an upper bound on the maximum size of dataset that can be processed at a time. Large datasets with sizes exceeding this upper bound must be broken up into smaller data chunks for processing. In these cases, standard k-anonymisation tools such as ARX can only operate on a per-chunk basis. This paper proposes SKALD, a novel algorithm for performing k-anonymisation on large datasets with limited RAM. Our SKALD algorithm offers multi-fold performance improvement over standard k-anonymisation methods by extracting and combining sufficient statistics from each chunk during processing to ensure successful k-anonymisation while providing better utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03529v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kailash Reddy, Novoneel Chakraborty, Amogh Dharmavaram, Anshoo Tandon</dc:creator>
    </item>
    <item>
      <title>Limitations to Computing Quadratic Functions on Reed-Solomon Encoded Data</title>
      <link>https://arxiv.org/abs/2505.08000</link>
      <description>arXiv:2505.08000v2 Announce Type: replace 
Abstract: We study the problem of low-bandwidth non-linear computation on Reed-Solomon encoded data. Given an $[n,k]$ Reed-Solomon encoding of a message vector $\vec{f} \in \mathbb{F}_q^k$, and a polynomial $g \in \mathbb{F}_q[X_1, X_2, \ldots, X_k]$, a user wishing to evaluate $g(\vec{f})$ is given local query access to each codeword symbol. The query response is allowed to be the output of an arbitrary function evaluated locally on the codeword symbol, and the user's aim is to minimize the total information downloaded in order to compute $g(\vec{f})$.
  We show that when $k=2$ and $q = p^e$ for prime $p &gt; 2$, then any scheme that evaluates the quadratic monomial $g(X_1, X_2) := X_1 X_2$ must download at least $2 \log_2(q-1) - 3$ bits of information; compare this with the na\"{\i}ve scheme of Reed-Solomon interpolation which recovers $\vec{f}$ in its entirety, which downloads $2 \log_2(q) $ bits. Our result shows that dimension-2 Reed-Solomon codes do not admit any meaningful low-bandwidth scheme for the evaluation of quadratic functions over the encoded data. This contrasts sharply with prior work for low-bandwidth evaluation of \emph{linear} functions $g(\vec{f})$ over Reed-Solomon encoded data, for which it is possible to substantially improve upon the na\"{\i}ve bound of $k \log_2(q) $.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08000v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keller Blackwell, Mary Wootters</dc:creator>
    </item>
    <item>
      <title>Ansatz-free Hamiltonian learning with Heisenberg-limited scaling</title>
      <link>https://arxiv.org/abs/2502.11900</link>
      <description>arXiv:2502.11900v2 Announce Type: replace-cross 
Abstract: Learning the unknown interactions that govern a quantum system is crucial for quantum information processing, device benchmarking, and quantum sensing. The problem, known as Hamiltonian learning, is well understood under the assumption that interactions are local, but this assumption may not hold for arbitrary Hamiltonians. Previous methods all require high-order inverse polynomial dependency with precision, unable to surpass the standard quantum limit and reach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited Hamiltonian learning is possible without prior assumptions about the interaction structures, a challenge we term \emph{ansatz-free Hamiltonian learning}, remains an open question. In this work, we present a quantum algorithm to learn arbitrary sparse Hamiltonians without any structure constraints using only black-box queries of the system's real-time evolution and minimal digital controls to attain Heisenberg-limited scaling in estimation error. Our method is also resilient to state-preparation-and-measurement errors, enhancing its practical feasibility. We numerically demonstrate our ansatz-free protocol for learning physical Hamiltonians and validating analog quantum simulations, benchmarking our performance against the state-of-the-art Heisenberg-limited learning approach. Moreover, we establish a fundamental trade-off between total evolution time and quantum control on learning arbitrary interactions, revealing the intrinsic interplay between controllability and total evolution time complexity for any learning algorithm. These results pave the way for further exploration into Heisenberg-limited Hamiltonian learning in complex quantum systems under minimal assumptions, potentially enabling new benchmarking and verification protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11900v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong-Ye Hu, Muzhou Ma, Weiyuan Gong, Qi Ye, Yu Tong, Steven T. Flammia, Susanne F. Yelin</dc:creator>
    </item>
    <item>
      <title>From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</title>
      <link>https://arxiv.org/abs/2505.17117</link>
      <description>arXiv:2505.17117v3 Announce Type: replace-cross 
Abstract: Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17117v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv</dc:creator>
    </item>
    <item>
      <title>Revisiting mean estimation over $\ell_p$ balls: Is the MLE optimal?</title>
      <link>https://arxiv.org/abs/2506.10354</link>
      <description>arXiv:2506.10354v2 Announce Type: replace-cross 
Abstract: We revisit the problem of mean estimation in the Gaussian sequence model with $\ell_p$ constraints for $p \in [0, \infty]$. We demonstrate two phenomena for the behavior of the maximum likelihood estimator (MLE), which depend on the noise level, the radius of the (quasi)norm constraint, the dimension, and the norm index $p$. First, if $p$ lies between $0$ and $1 + \Theta(\tfrac{1}{\log d})$, inclusive, or if it is greater than or equal to $2$, the MLE is minimax rate-optimal for all noise levels and all constraint radii. On the other hand, for the remaining norm indices -- namely, if $p$ lies between $1 + \Theta(\tfrac{1}{\log d})$ and $2$ -- here is a more striking behavior: the MLE is minimax rate-suboptimal, despite its nonlinearity in the observations, for essentially all noise levels and constraint radii for which nonlinear estimates are necessary for minimax-optimal estimation. Our results imply that when given $n$ independent and identically distributed Gaussian samples, the MLE can be suboptimal by a polynomial factor in the sample size. Our lower bounds are constructive: whenever the MLE is rate-suboptimal, we provide explicit instances on which the MLE provably incurs suboptimal risk. Finally, in the non-convex case -- namely when $p &lt; 1$ -- we develop sharp local Gaussian width bounds, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10354v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan, Reese Pathak, Annie Ulichney</dc:creator>
    </item>
  </channel>
</rss>
