<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2025 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ordering Results between Two Extreme Order Statistics with Heterogeneous Linear Failure Rate Distributed Components</title>
      <link>https://arxiv.org/abs/2506.05773</link>
      <description>arXiv:2506.05773v1 Announce Type: new 
Abstract: Stochastic comparisons of series and parallel systems are important in many areas of engineering, operations research and reliability analysis. These comparisons allow for the evaluation of the performance and reliability of systems under different conditions, and can inform decisions related to system design, probabilities of failure, maintenance and operation. In this paper, we investigate the stochastic comparisons of the series and parallel systems under the assumption that the component lifetimes have independent heterogeneous linear failure rate distributions. The comparisons are established based on the various stochastic orders including magnitude, transform and variability orders. Several numerical examples and counterexamples are constructed to illustrate the theoretical outcomes of this paper. Finally, we summarized our findings with a real-world application and possible future scopes of the present study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05773v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>CM Revathi, Rajesh Moharana, Raju Bhakta</dc:creator>
    </item>
    <item>
      <title>On Rank Correlation Coefficients</title>
      <link>https://arxiv.org/abs/2506.06056</link>
      <description>arXiv:2506.06056v1 Announce Type: new 
Abstract: In the present paper, we propose a new rank correlation coefficient $r_n$, which is a sample analogue of the theoretical correlation coefficient $r$, which, in turn, was proposed in the recent work of Stepanov (2025b). We discuss the properties of $r_n$ and compare $r_n$ with known rank Spearman $\rho_{S,n}$, Kendall $\tau_n$ and sample Pearson $\rho_n$ correlation coefficients. Simulation experiments show that when the relationship between $X$ and $Y$ is not close to linear, $r_n$ performs better than other correlation coefficients. We also find analytically the values of $Var(\tau_n)$ and $Var(r_n)$. This allows to estimate theoretically the asymptotic performance of $\tau_n$ and $r_n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06056v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexei Stepanov</dc:creator>
    </item>
    <item>
      <title>An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds</title>
      <link>https://arxiv.org/abs/2506.06259</link>
      <description>arXiv:2506.06259v1 Announce Type: new 
Abstract: Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds -- another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry.
  In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds -- such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024) -- but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06259v1</guid>
      <category>math.ST</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Chen, Theodor Misiakiewicz, Ilias Zadik, Peiyuan Zhang</dc:creator>
    </item>
    <item>
      <title>On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2506.05945</link>
      <description>arXiv:2506.05945v1 Announce Type: cross 
Abstract: This paper focuses on the estimation of distributional treatment effects in randomized experiments that use covariate-adaptive randomization (CAR). These include designs such as Efron's biased-coin design and stratified block randomization, where participants are first grouped into strata based on baseline covariates and assigned treatments within each stratum to ensure balance across groups. In practice, datasets often contain additional covariates beyond the strata indicators. We propose a flexible distribution regression framework that leverages off-the-shelf machine learning methods to incorporate these additional covariates, enhancing the precision of distributional treatment effect estimates. We establish the asymptotic distribution of the proposed estimator and introduce a valid inference procedure. Furthermore, we derive the semiparametric efficiency bound for distributional treatment effects under CAR and demonstrate that our regression-adjusted estimator attains this bound. Simulation studies and empirical analyses of microcredit programs highlight the practical advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05945v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the International Conference on Machine Learning, 2025</arxiv:journal_reference>
      <dc:creator>Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui</dc:creator>
    </item>
    <item>
      <title>The envelope of a complex Gaussian random variable</title>
      <link>https://arxiv.org/abs/2305.03038</link>
      <description>arXiv:2305.03038v3 Announce Type: replace 
Abstract: The envelope of an elliptical Gaussian complex vector, or equivalently, the amplitude or norm of a bivariate normal random vector has application in many weather and signal processing contexts. We explicitly characterize its distribution in the general case through its probability density, cumulative distribution and moment generating function. Moments and limiting distributions are also derived. These derivations are exploited to also characterize the special cases where the bivariate Gaussian mean vector and covariance matrix have a simpler structure, providing new additional insights in many cases. Simulations illustrate the benefits of using our formulae over Monte Carlo methods. We also use our derivations to get a better initial characterization of the distribution of the observed values in structural Magnetic Resonance Imaging datasets, and of wind speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03038v3</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sattwik Ghosal, Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>Hoeffding-type decomposition for $U$-statistics on bipartite networks</title>
      <link>https://arxiv.org/abs/2308.14518</link>
      <description>arXiv:2308.14518v4 Announce Type: replace 
Abstract: We consider a broad class of random bipartite networks, the distribution of which is invariant under permutation within each type of nodes. We are interested in $U$-statistics defined on the adjacency matrix of such a network, for which we define a new type of Hoeffding decomposition based on the Aldous-Hoover-Kallenberg representation of row-column exchangeable matrices. This decomposition enables us to characterize non-degenerate $U$-statistics -- which are then asymptotically normal -- and provides us with a natural and easy-to-implement estimator of their asymptotic variance. \\ We illustrate the use of this general approach on some typical random graph models and use it to estimate or test some quantities characterizing the topology of the associated network. We also assess the accuracy and the power of the proposed estimates or tests, via a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14518v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>T\^am Le Minh, Sophie Donnet, Fran\c{c}ois Massol, St\'ephane Robin</dc:creator>
    </item>
    <item>
      <title>Density estimation using the perceptron</title>
      <link>https://arxiv.org/abs/2312.17701</link>
      <description>arXiv:2312.17701v4 Announce Type: replace 
Abstract: We propose a new density estimation algorithm. Given $n$ i.i.d. observations from a distribution belonging to a class of densities on $\mathbb{R}^d$, our estimator outputs any density in the class whose "perceptron discrepancy" with the empirical distribution is at most $O(\sqrt{d / n})$. The perceptron discrepancy is defined as the largest difference in mass two distribution place on any halfspace. It is shown that this estimator achieves the expected total variation distance to the truth that is almost minimax optimal over the class of densities with bounded Sobolev norm and Gaussian mixtures. This suggests that the regularity of the prior distribution could be an explanation for the efficiency of the ubiquitous step in machine learning that replaces optimization over large function spaces with simpler parametric classes (such as discriminators of GANs). We also show that replacing the perceptron discrepancy with the generalized energy distance of Sz\'ekely and Rizzo (2013) further improves total variation loss. The generalized energy distance between empirical distributions is easily computable and differentiable, which makes it especially useful for fitting generative models. To the best of our knowledge, it is the first "simple" distance with such properties that yields minimax optimal statistical guarantees.
  In addition, we shed light on the ubiquitous method of representing discrete data in domain $[k]$ via embedding vectors on a unit ball in $\mathbb{R}^d$. We show that taking $d \asymp \log (k)$ allows one to use simple linear probing to evaluate and estimate total variation distance, as well as recovering minimax optimal sample complexity for the class of discrete distributions on $[k]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17701v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik R\'obert Gerber, Tianze Jiang, Yury Polyanskiy, Rui Sun</dc:creator>
    </item>
    <item>
      <title>Finite sample expansions and risk bounds in high-dimensional SLS models</title>
      <link>https://arxiv.org/abs/2404.14227</link>
      <description>arXiv:2404.14227v3 Announce Type: replace 
Abstract: This note extends the results of classical parametric statistics like Fisher and Wilks theorem to modern setups with a high or infinite parameter dimension, limited sample size, and possible model misspecification. We consider a special class of stochastically linear smooth (SLS) models satisfying three major conditions: the stochastic component of the log-likelihood is linear in the model parameter and the expected log-likelihood is a smooth and concave function. For the penalized maximum likelihood estimators (pMLE), we establish three types of results: (1) concentration in a small vicinity of the ``truth''; (2) Fisher and Wilks expansions; (3) risk bounds. In all results, the remainder is given explicitly and can be evaluated in terms of the effective sample size and effective parameter dimension which allows us to identify the so-called \emph{critical parameter dimension}. The results are also dimension and coordinate-free. The obtained finite sample expansions are of special interest because they can be used not only for obtaining the risk bounds but also for inference, studying the asymptotic distribution, analysis of resampling procedures, etc. The main tool for all these expansions is the so-called ``basic lemma'' about linearly perturbed optimization. Despite their generality, all the presented bounds are nearly sharp and the classical asymptotic results can be obtained as simple corollaries. Our results indicate that the use of advanced fourth-order expansions allows to relax the critical dimension condition $ \mathbb{p}^{3} \ll n $ from Spokoiny (2023a) to $ \mathbb{p}^{3/2} \ll n $. Examples for classical models like logistic regression, log-density and precision matrix estimation illustrate the applicability of general results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14227v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Estimation and goodness-of-fit testing for non-negative random variables with explicit Laplace transform</title>
      <link>https://arxiv.org/abs/2405.15041</link>
      <description>arXiv:2405.15041v2 Announce Type: replace 
Abstract: Many flexible families of positive random variables exhibit non-closed forms of the density and distribution functions and this feature is considered unappealing for modelling purposes. However, such families are often characterized by a simple expression of the corresponding Laplace transform. Relying on the Laplace transform, we propose to carry out parameter estimation and goodness-of-fit testing for a general class of non-standard laws. We suggest a novel data-driven inferential technique, providing parameter estimators and goodness-of-fit tests, whose large-sample properties are derived. The implementation of the method is specifically considered for the positive stable and Tweedie distributions. A Monte Carlo study shows good finite-sample performance of the proposed technique for such laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15041v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio Barabesi, Antonio Di Noia, Marzia Marcheselli, Caterina Pisani, Luca Pratelli</dc:creator>
    </item>
    <item>
      <title>Likelihood distortion and Bayesian local robustness</title>
      <link>https://arxiv.org/abs/2405.15141</link>
      <description>arXiv:2405.15141v3 Announce Type: replace 
Abstract: Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15141v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Marchenko-Pastur laws for Daniell smoothed periodograms</title>
      <link>https://arxiv.org/abs/2408.14618</link>
      <description>arXiv:2408.14618v5 Announce Type: replace 
Abstract: Given a sample $X_0,...,X_{n-1}$ from a $d$-dimensional stationary time series $(X_t)_{t \in \mathbb{Z}}$, the most commonly used estimator for the spectral density matrix $F(\theta)$ at a given frequency $\theta \in [0,2\pi)$ is the Daniell smoothed periodogram $$S(\theta) = \frac{1}{2m+1} \sum\limits_{j=-m}^m I\Big( \theta + \frac{2\pi j}{n} \Big) \ ,$$ which is an average over $2m+1$ many periodograms at slightly perturbed frequencies. We prove that the Marchenko-Pastur law holds for the eigenvalues of $S(\theta)$ uniformly in $\theta \in [0,2\pi)$, when $d$ and $m$ grow with $n$ such that $\frac{d}{m} \rightarrow c&gt;0$ and $d\asymp n^{\alpha}$ for some $\alpha \in (0,1)$. This demonstrates that high-dimensional effects can cause $S(\theta)$ to become inconsistent, even when the dimension $d$ is much smaller than the sample size $n$.
  Notably, we do not assume independence of the $d$ components of the time series. The Marchenko-Pastur law thus holds for Daniell smoothed periodograms, even when it does not necessarily hold for sample auto-covariance matrices of the same processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14618v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>Two statistical problems for multivariate mixture distributions</title>
      <link>https://arxiv.org/abs/2503.12147</link>
      <description>arXiv:2503.12147v2 Announce Type: replace 
Abstract: After presenting a short review of random-projection techniques, we address two important statistical problems: that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections, and that of measuring the agreement between two different random partitions. The results are based on an earlier work of the authors, where it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12147v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Fraiman, Leonardo Moreno, Thomas Ransford</dc:creator>
    </item>
    <item>
      <title>Multiscale Asymptotic Normality in Quantile Regression: Hilbert Matrices and Polynomial Designs</title>
      <link>https://arxiv.org/abs/2503.15041</link>
      <description>arXiv:2503.15041v3 Announce Type: replace 
Abstract: This paper investigates the asymptotic properties of quantile regression estimators in linear models, with a particular focus on polynomial regressors and robustness to heavy-tailed noise. Under independent and identically distributed (i.i.d.) errors with continuous density around the quantile of interest, we establish a general Central Limit Theorem (CLT) for the quantile regression estimator under normalization using $\Delta_n^{-1}$, yielding asymptotic normality with variance $\tau(1-\tau)/f^2(0) \cdot D_0^{-1}$. In the specific case of polynomial regressors, we show that the design structure induces a Hilbert matrix in the asymptotic covariance, and we derive explicit scaling rates for each coefficient. This generalizes Pollard's and Koenker's earlier results on LAD regression to arbitrary quantile levels $\tau \in (0, 1)$. We also examine the convergence behavior of the estimators and propose a relaxation of the standard CLT-based confidence intervals, motivated by a theoretical inclusion principle. This relaxation replaces the usual $T^{j+1/2}$ scaling with $T^\alpha$, for $\alpha &lt; j + 1/2$, to improve finite-sample coverage. Through extensive simulations under Laplace, Gaussian, and Cauchy noise, we validate this approach and highlight the improved robustness and empirical accuracy of relaxed confidence intervals. This study provides both a unifying theoretical framework and practical inference tools for quantile regression under structured regressors and heavy-tailed disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15041v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sa\"id Maanan (LPP), Azzouz Dermoune (LPP), Ahmed El Ghini</dc:creator>
    </item>
    <item>
      <title>Statistical microlocal analysis in two-dimensional X-ray CT</title>
      <link>https://arxiv.org/abs/2506.05113</link>
      <description>arXiv:2506.05113v2 Announce Type: replace 
Abstract: In many imaging applications it is important to assess how well the edges of the original object, $f$, are resolved in an image, $f^\text{rec}$, reconstructed from the measured data, $g$. In this paper we consider the case of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a function describing the object being scanned, and $g=Rf + \eta$ be the Radon transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with step size $\sim\epsilon$. Conventional microlocal analysis provides conditions for edge detectability based on the scanner geometry in the case of continuous, noiseless data (when $\eta = 0$), but does not account for noise and finite sampling step size. We develop a novel technique called Statistical Microlocal Analysis (SMA), which uses a statistical hypothesis testing framework to determine if an image edge (singularity) of $f$ is detectable from $f^\text{rec}$, and we quantify edge detectability using the statistical power of the test. Our approach is based on the theory we developed in previous work, which provides a characterization of $f^\text{rec}$ in local $O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical test for the presence and direction of an edge microlocally given the magnitude of $\eta$ and data sampling step size. Using the properties of the null distribution of the test, we quantify the uncertainty of the edge magnitude and direction. We validate our theory using simulations, which show strong agreement between our predictions and experimental observations. Our work is not only of practical value, but of theoretical value as well. SMA is a natural extension of classical microlocal analysis theory which accounts for practical measurement imperfections, such as noise and finite step size, at the highest possible resolution compatible with the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05113v2</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Abhishek, Alexander Katsevich, James W. Webber</dc:creator>
    </item>
    <item>
      <title>Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</title>
      <link>https://arxiv.org/abs/2408.13276</link>
      <description>arXiv:2408.13276v3 Announce Type: replace-cross 
Abstract: For the problem of reconstructing a low-rank matrix from a few linear measurements, two classes of algorithms have been widely studied in the literature: convex approaches based on nuclear norm minimization, and non-convex approaches that use factorized gradient descent. Under certain statistical model assumptions, it is known that nuclear norm minimization recovers the ground truth as soon as the number of samples scales linearly with the number of degrees of freedom of the ground truth. In contrast, while non-convex approaches are computationally less expensive, existing recovery guarantees assume that the number of samples scales at least quadratically with the rank $r$ of the ground-truth matrix. In this paper, we close this gap by showing that the non-convex approaches can be as efficient as nuclear norm minimization in terms of sample complexity. Namely, we consider the problem of reconstructing a positive semidefinite matrix from a few Gaussian measurements. We show that factorized gradient descent with spectral initialization converges to the ground truth with a linear rate as soon as the number of samples scales with $ \Omega (rd\kappa^2)$, where $d$ is the dimension, and $\kappa$ is the condition number of the ground truth matrix. This improves the previous rank-dependence in the sample complexity of non-convex matrix factorization from quadratic to linear. Our proof relies on a probabilistic decoupling argument, where we show that the gradient descent iterates are only weakly dependent on the individual entries of the measurement matrices. We expect that our proof technique is of independent interest for other non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13276v3</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik St\"oger, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Improving Numerical Stability of Normalized Mutual Information Estimator on High Dimensions</title>
      <link>https://arxiv.org/abs/2410.07642</link>
      <description>arXiv:2410.07642v2 Announce Type: replace-cross 
Abstract: Mutual information provides a powerful, general-purpose metric for quantifying the amount of shared information between variables. Estimating normalized mutual information using a k-Nearest Neighbor (k-NN) based approach involves the calculation of the scaling-invariant k-NN radius. Calculation of the radius suffers from numerical overflow when the joint dimensionality of the data becomes high, typically in the range of several hundred dimensions. To address this issue, we propose a logarithmic transformation technique that improves the numerical stability of the radius calculation in high-dimensional spaces. By applying the proposed transformation during the calculation of the radius, numerical overflow is avoided, and precision is maintained. Proposed transformation is validated through both theoretical analysis and empirical evaluation, demonstrating its ability to stabilize the calculation without compromising precision, increasing bias, or adding significant computational overhead, while also helping to maintain estimator variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07642v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko Tuononen, Ville Hautam\"aki</dc:creator>
    </item>
    <item>
      <title>Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups</title>
      <link>https://arxiv.org/abs/2410.14477</link>
      <description>arXiv:2410.14477v2 Announce Type: replace-cross 
Abstract: Markov processes serve as a universal model for many real-world random processes. This paper presents a data-driven approach for learning these models through the spectral decomposition of the infinitesimal generator (IG) of the Markov semigroup. The unbounded nature of IGs complicates traditional methods such as vector-valued regression and Hilbert-Schmidt operator analysis. Existing techniques, including physics-informed kernel regression, are computationally expensive and limited in scope, with no recovery guarantees for transfer operator methods when the time-lag is small. We propose a novel method that leverages the IG's resolvent, characterized by the Laplace transform of transfer operators. This approach is robust to time-lag variations, ensuring accurate eigenvalue learning even for small time-lags. Our statistical analysis applies to a broader class of Markov processes than current methods while reducing computational complexity from quadratic to linear in the state dimension. Finally, we illustrate the behaviour of our method in two experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14477v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir R. Kostic, Karim Lounici, H\'el\`ene Halconruy, Timoth\'ee Devergne, Pietro Novelli, Massimiliano Pontil</dc:creator>
    </item>
    <item>
      <title>A Proof of The Changepoint Detection Threshold Conjecture in Preferential Attachment Models</title>
      <link>https://arxiv.org/abs/2502.00514</link>
      <description>arXiv:2502.00514v3 Announce Type: replace-cross 
Abstract: We investigate the problem of detecting and estimating a changepoint in the attachment function of a network evolving according to a preferential attachment model on $n$ vertices, using only a single final snapshot of the network. Bet et al.~\cite{bet2023detecting} show that a simple test based on thresholding the number of vertices with minimum degrees can detect the changepoint when the change occurs at time $n-\Omega(\sqrt{n})$. They further make the striking conjecture that detection becomes impossible for any test if the change occurs at time $n-o(\sqrt{n}).$ Kaddouri et al.~\cite{kaddouri2024impossibility} make a step forward by proving the detection is impossible if the change occurs at time $n-o(n^{1/3}).$ In this paper, we resolve the conjecture affirmatively, proving that detection is indeed impossible if the change occurs at time $n-o(\sqrt{n}).$ Furthermore, we establish that estimating the changepoint with an error smaller than $o(\sqrt{n})$ is also impossible, thereby confirming that the estimator proposed in Bhamidi et al.~\cite{bhamidi2018change} is order-optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00514v3</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Du, Shuyang Gong, Jiaming Xu</dc:creator>
    </item>
    <item>
      <title>Estimating stationary mass, frequency by frequency</title>
      <link>https://arxiv.org/abs/2503.12808</link>
      <description>arXiv:2503.12808v3 Announce Type: replace-cross 
Abstract: Suppose we observe a trajectory of length $n$ from an exponentially $\alpha$-mixing stochastic process over a finite but potentially large state space. We consider the problem of estimating the probability mass placed by the stationary distribution of any such process on elements that occur with a certain frequency in the observed sequence. We estimate this vector of probabilities in total variation distance, showing universal consistency in $n$ and recovering known results for i.i.d. sequences as special cases. Our proposed methodology -- implementable in linear time -- carefully combines the plug-in (or empirical) estimator with a recently-proposed modification of the Good--Turing estimator called WingIt, which was originally developed for Markovian sequences. En route to controlling the error of our estimator, we develop new performance bounds on WingIt and the plug-in estimator for exponentially $\alpha$-mixing stochastic processes. Importantly, the extensively used method of Poissonization can no longer be applied in our non i.i.d. setting, and so we develop complementary tools -- including concentration inequalities for a natural self-normalized statistic of mixing sequences -- that may prove independently useful in the design and analysis of estimators for related problems. Simulation studies corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12808v3</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milind Nakul, Vidya Muthukumar, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Kernel Density Machines</title>
      <link>https://arxiv.org/abs/2504.21419</link>
      <description>arXiv:2504.21419v2 Announce Type: replace-cross 
Abstract: We introduce kernel density machines (KDM), a nonparametric estimator of a Radon--Nikodym derivative, based on reproducing kernel Hilbert spaces. KDM applies to general probability measures on countably generated measurable spaces under minimal assumptions. For computational efficiency, we incorporate a low-rank approximation with precisely controlled error that grants scalability to large-sample settings. We provide rigorous theoretical guarantees, including asymptotic consistency, a functional central limit theorem, and finite-sample error bounds, establishing a strong foundation for practical use. Empirical results based on simulated and real data demonstrate the efficacy and precision of KDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21419v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
  </channel>
</rss>
