<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Feb 2025 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multimarginal Schr\"{o}dinger Barycenter</title>
      <link>https://arxiv.org/abs/2502.02726</link>
      <description>arXiv:2502.02726v1 Announce Type: new 
Abstract: The Wasserstein barycenter plays a fundamental role in averaging measure-valued data under the framework of optimal transport. However, there are tremendous challenges in computing and estimating the Wasserstein barycenter for high-dimensional distributions. In this paper, we introduce the multimarginal Schr\"{o}dinger barycenter (MSB) based on the entropy regularized multimarginal optimal transport problem that admits general-purpose fast algorithms for computation. By recognizing a proper dual geometry, we derive non-asymptotic rates of convergence for estimating several key MSB quantities from point clouds randomly sampled from the input marginal distributions. Specifically, we show that our obtained sample complexity is statistically optimal for estimating the cost functional, Schr\"{o}dinger coupling and barycenter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02726v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengtao Li, Xiaohui Chen</dc:creator>
    </item>
    <item>
      <title>Early Stopping in Contextual Bandits and Inferences</title>
      <link>https://arxiv.org/abs/2502.02793</link>
      <description>arXiv:2502.02793v1 Announce Type: new 
Abstract: Bandit algorithms sequentially accumulate data using adaptive sampling policies, offering flexibility for real-world applications. However, excessive sampling can be costly, motivating the devolopment of early stopping methods and reliable post-experiment conditional inferences. This paper studies early stopping methods in linear contextual bandits, including both pre-determined and online stopping rules, to minimize in-experiment regrets while accounting for sampling costs. We propose stopping rules based on the Opportunity Cost and Threshold Method, utilizing the variances of unbiased or consistent online estimators to quantify the upper regret bounds of learned optimal policy. The study focuses on batched settings for stability, selecting a weighed combination of batched estimators as the online estimator and deriving its asymptotic distribution. Online statistical inferences are performed based on the selected estimator, conditional on the realized stopping time. Our proposed method provides a systematic approach to minimize in-experiment regret and conduct robust post-experiment inferences, facilitating decision-making in future applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02793v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Cui (University of Michigan)</dc:creator>
    </item>
    <item>
      <title>Kronecker sum covariance models for spatio-temporal data</title>
      <link>https://arxiv.org/abs/2502.02848</link>
      <description>arXiv:2502.02848v1 Announce Type: new 
Abstract: In this paper, we study the subgaussian matrix variate model, where we observe the matrix variate data $X$ which consists of a signal matrix $X_0$ and a noise matrix $W$. More specifically, we study a subgaussian model using the Kronecker sum covariance as in Rudelson and Zhou (2017). Let $Z_1, Z_2$ be independent copies of a subgaussian random matrix $Z =(Z_{ij})$, where $Z_{ij}, \forall i, j$ are independent mean 0, unit variance, subgaussian random variables with bounded $\psi_2$ norm. We use $X \sim \mathcal{M}_{n,m}(0, A \oplus B)$ to denote the subgaussian random matrix $X_{n \times m}$ which is generated using: $$ X = Z_1 A^{1/2} + B^{1/2} Z_2. $$ In this covariance model, the first component $A \otimes I_n$ describes the covariance of the signal $X_0 = Z_1 A^{1/2}$, which is an ${n \times m}$ random design matrix with independent subgaussian row vectors, and the other component $I_m \otimes B$ describes the covariance for the noise matrix $W =B^{1/2} Z_2$, which contains independent subgaussian column vectors $w^1, \ldots, w^m$, independent of $X_0$. This leads to a non-separable class of models for the observation $X$, which we denote by $X \sim \mathcal{M}_{n,m}(0, A \oplus B)$ throughout this paper. Our method on inverse covariance estimation corresponds to the proposal in Yuan (2010) and Loh and Wainwright (2012), only now dropping the i.i.d. or Gaussian assumptions. We present the statistical rates of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02848v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuheng Zhou, Seyoung Park, Kerby Shedden</dc:creator>
    </item>
    <item>
      <title>Matching Criterion for Identifiability in Sparse Factor Analysis</title>
      <link>https://arxiv.org/abs/2502.02986</link>
      <description>arXiv:2502.02986v1 Announce Type: new 
Abstract: Factor analysis models explain dependence among observed variables by a smaller number of unobserved factors. A main challenge in confirmatory factor analysis is determining whether the factor loading matrix is identifiable from the observed covariance matrix. The factor loading matrix captures the linear effects of the factors and, if unrestricted, can only be identified up to an orthogonal transformation of the factors. However, in many applications the factor loadings exhibit an interesting sparsity pattern that may lead to identifiability up to column signs. We study this phenomenon by connecting sparse factor models to bipartite graphs and providing sufficient graphical conditions for identifiability of the factor loading matrix up to column signs. In contrast to previous work, our main contribution, the matching criterion, exploits sparsity by operating locally on the graph structure, thereby improving existing conditions. Our criterion is efficiently decidable in time that is polynomial in the size of the graph, when restricting the search steps to sets of bounded size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02986v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Sturma, Miriam Kranzlmueller, Irem Portakal, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Ordinal Patterns Based Change Points Detection</title>
      <link>https://arxiv.org/abs/2502.03099</link>
      <description>arXiv:2502.03099v1 Announce Type: new 
Abstract: The ordinal patterns of a fixed number of consecutive values in a time series is the spatial ordering of these values. Counting how often a specific ordinal pattern occurs in a time series provides important insights into the properties of the time series. In this work, we prove the asymptotic normality of the relative frequency of ordinal patterns for time series with linear increments. Moreover, we apply ordinal patterns to detect changes in the distribution of a time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03099v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Betken, Giorgio Micali, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Robust Label Shift Quantification</title>
      <link>https://arxiv.org/abs/2502.03174</link>
      <description>arXiv:2502.03174v1 Announce Type: new 
Abstract: In this paper, we investigate the label shift quantification problem. We propose robust estimators of the label distribution which turn out to coincide with the Maximum Likelihood Estimator. We analyze the theoretical aspects and derive deviation bounds for the proposed method, providing optimal guarantees in the well-specified case, along with notable robustness properties against outliers and contamination. Our results provide theoretical validation for empirical observations on the robustness of Maximum Likelihood Label Shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03174v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Lecestre</dc:creator>
    </item>
    <item>
      <title>rSRD: An R package for the Sum of Ranking Differences statistical procedure</title>
      <link>https://arxiv.org/abs/2502.03208</link>
      <description>arXiv:2502.03208v1 Announce Type: new 
Abstract: Sum of Ranking Differences (SRD) is a relatively novel, non-para-metric statistical procedure that has become increasingly popular recently. SRD compares solutions via a reference by applying a rank transformation on the input and calculating the distance from the reference in $L_1$ norm. Although the computation of the test statistics is simple, validating the results is cumbersome -- at least by hand. There are two validation steps involved. Comparison of Ranks with Random Numbers, which is a permutation-test, and cross-validation combined with statistical testing. Both options impose computational difficulties albeit different ones. The rSRD package was devised to simplify the validation process by reducing both validation steps into single function calls. In addition, the package provides various useful tools including data preprocessing and plotting. The package makes SRD accessible to a wide audience as there are currently no other software options with such a comprehensive toolkit. This paper aims to serve as a guide for practitioners by offering a detailed presentation of the features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03208v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bal\'azs R. Sziklai, Attila Gere, K\'aroly H\'eberger, Jochen Staudacher</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Bias Detection with Subsampled Point-to-Subspace Distances</title>
      <link>https://arxiv.org/abs/2502.02623</link>
      <description>arXiv:2502.02623v1 Announce Type: cross 
Abstract: Sample complexity of bias estimation is a lower bound on the runtime of any bias detection method. Many regulatory frameworks require the bias to be tested for all subgroups, whose number grows exponentially with the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of bias detection for a single subgroup. At the same time, the reference data may be based on surveys, and thus come with non-trivial uncertainty.
  Here, we reformulate bias detection as a point-to-subspace problem on the space of measures and show that, for supremum norm, it can be subsampled efficiently. In particular, our probabilistically approximately correct (PAC) results are corroborated by tests on well-known instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02623v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>German Martinez Matilla, Jakub Marecek</dc:creator>
    </item>
    <item>
      <title>Sufficient dimension reduction for regression with spatially correlated errors: application to prediction</title>
      <link>https://arxiv.org/abs/2502.02781</link>
      <description>arXiv:2502.02781v1 Announce Type: cross 
Abstract: In this paper, we address the problem of predicting a response variable in the context of both, spatially correlated and high-dimensional data. To reduce the dimensionality of the predictor variables, we apply the sufficient dimension reduction (SDR) paradigm, which reduces the predictor space while retaining relevant information about the response. To achieve this, we impose two different spatial models on the inverse regression: the separable spatial covariance model (SSCM) and the spatial autoregressive error model (SEM). For these models, we derive maximum likelihood estimators for the reduction and use them to predict the response via nonparametric rules for forward regression. Through simulations and real data applications, we demonstrate the effectiveness of our approach for spatial data prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02781v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liliana Forzani, Rodrigo Garc\'ia Arancibia, Antonella Gieco, Pamela Llop, Anne Yao</dc:creator>
    </item>
    <item>
      <title>Variations on the Expectation Due to Changes in the Probability Measure</title>
      <link>https://arxiv.org/abs/2502.02887</link>
      <description>arXiv:2502.02887v1 Announce Type: cross 
Abstract: Closed-form expressions are presented for the variation of the expectation of a given function due to changes in the probability measure used for the expectation. They unveil interesting connections with Gibbs probability measures, the mutual information, and the lautum information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02887v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samir M. Perlaza, Gaetan Bisson</dc:creator>
    </item>
    <item>
      <title>Data denoising with self consistency, variance maximization, and the Kantorovich dominance</title>
      <link>https://arxiv.org/abs/2502.02925</link>
      <description>arXiv:2502.02925v1 Announce Type: cross 
Abstract: We introduce a new framework for data denoising, partially inspired by martingale optimal transport. For a given noisy distribution (the data), our approach involves finding the closest distribution to it among all distributions which 1) have a particular prescribed structure (expressed by requiring they lie in a particular domain), and 2) are self-consistent with the data. We show that this amounts to maximizing the variance among measures in the domain which are dominated in convex order by the data. For particular choices of the domain, this problem and a relaxed version of it, in which the self-consistency condition is removed, are intimately related to various classical approaches to denoising. We prove that our general problem has certain desirable features: solutions exist under mild assumptions, have certain robustness properties, and, for very simple domains, coincide with solutions to the relaxed problem.
  We also introduce a novel relationship between distributions, termed Kantorovich dominance, which retains certain aspects of the convex order while being a weaker, more robust, and easier-to-verify condition. Building on this, we propose and analyze a new denoising problem by substituting the convex order in the previously described framework with Kantorovich dominance. We demonstrate that this revised problem shares some characteristics with the full convex order problem but offers enhanced stability, greater computational efficiency, and, in specific domains, more meaningful solutions. Finally, we present simple numerical examples illustrating solutions for both the full convex order problem and the Kantorovich dominance problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02925v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Zoen-Git Hiew, Tongseok Lim, Brendan Pass, Marcelo Cruz de Souza</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of Unit-Weibull distribution based on dual generalized order statistics with application to the Cotton Production Data</title>
      <link>https://arxiv.org/abs/2502.02927</link>
      <description>arXiv:2502.02927v1 Announce Type: cross 
Abstract: The Unit Weibull distribution with parameters $\alpha$ and $\beta$ is considered to study in the context of dual generalized order statistics. For the analysis purpose, Bayes estimators based on symmetric and asymmetric loss functions are obtained. The methods which are utilized for Bayesian estimation are approximation and simulation tools such as Lindley, Tierney-Kadane and Markov chain Monte Carlo methods. The authors have considered squared error loss function as symmetric and LINEX and general entropy loss function as asymmetric loss functions. After presenting the mathematical results, a simulation study is conducted to exhibit the performances of various derived estimators. As this study is considered for the dual generalized order statistics that is unification of models based distinct ordered random variable such as order statistics, record values, etc. This provides flexibility in our results and in continuation of this, the cotton production data of USA is analyzed for both submodels of ordered random variables: order statistics and record values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02927v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qazi J. Azhad, Abdul Nasir Khan, Bhagwati Devi, Jahangir Sabbir Khan, Ayush Tripathi</dc:creator>
    </item>
    <item>
      <title>Parametric Scaling Law of Tuning Bias in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.03023</link>
      <description>arXiv:2502.03023v1 Announce Type: cross 
Abstract: Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03023v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei</dc:creator>
    </item>
    <item>
      <title>CARROT: A Cost Aware Rate Optimal Router</title>
      <link>https://arxiv.org/abs/2502.03261</link>
      <description>arXiv:2502.03261v1 Announce Type: cross 
Abstract: With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. Following this line of work, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based on any desired trade-off between performance and cost. Given a query, CARROT selects a model based on estimates of models' cost and performance. Its simplicity lends CARROT computational efficiency, while our theoretical analysis demonstrates minimax rate-optimality in its routing performance. Alongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03261v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, M\'irian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity</dc:creator>
    </item>
    <item>
      <title>Support estimation in high-dimensional heteroscedastic mean regression</title>
      <link>https://arxiv.org/abs/2011.01591</link>
      <description>arXiv:2011.01591v2 Announce Type: replace 
Abstract: A current strand of research in high-dimensional statistics deals with robustifying the available methodology with respect to deviations from the pervasive light-tail assumptions. In this paper we consider a linear mean regression model with random design and potentially heteroscedastic, heavy-tailed errors, and investigate support estimation in this framework. We use a strictly convex, smooth variant of the Huber loss function with tuning parameter depending on the parameters of the problem, as well as the adaptive LASSO penalty for computational efficiency. For the resulting estimator we show sign-consistency and optimal rates of convergence in the $\ell_\infty$ norm as in the homoscedastic, light-tailed setting. In our analysis, we have to deal with the issue that the support of the target parameter in the linear mean regression model and its robustified version may differ substantially even for small values of the tuning parameter of the Huber loss function. Simulations illustrate the favorable numerical performance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.01591v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Hermann, Hajo Holzmann</dc:creator>
    </item>
    <item>
      <title>On the Minimum Attainable Risk in Permutation Invariant Problems</title>
      <link>https://arxiv.org/abs/2110.06250</link>
      <description>arXiv:2110.06250v2 Announce Type: replace 
Abstract: We consider a broad class of permutation invariant statistical problems by extending the standard decision theoretic definition to allow also selective inference tasks, where the target is specified only after seeing the data. For any such problem we show that, among all permutation invariant procedures, the minimizer of the risk at $\boldsymbol{\theta}$ is precisely the rule that minimizes the Bayes risk under a (postulated) discrete prior assigning equal probability to every permutation of $\boldsymbol{\theta}$. This gives an explicit characterization of the greatest lower bound on the risk of every sensible procedure in a wide range of problems. Furthermore, in a permutation invariant problem of estimating the parameter of a selected population under squared loss, we prove that this lower bound coincides asymptotically with a simpler lower bound, attained by the Bayes solution that replaces the aforementioned uniform prior on all permutations of $\boldsymbol{\theta}$ by the i.i.d. prior with the same marginals. This has important algorithmic implications because it suggests that our greatest lower bound is asymptotically attainable uniformly in $\boldsymbol{\theta}$ by an empirical Bayes procedure. Altogether, the above extends theory that has been established in the existing literature only for the very special case of compound decision problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.06250v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asaf Weinstein</dc:creator>
    </item>
    <item>
      <title>Observable asymptotics of regularized Cox regression models with standard Gaussian designs: a statistical mechanics approach</title>
      <link>https://arxiv.org/abs/2405.13690</link>
      <description>arXiv:2405.13690v2 Announce Type: replace 
Abstract: We study the asymptotic behaviour of the Regularized Maximum Partial Likelihood Estimator (RMPLE) in the proportional limit, considering an arbitrary convex regularizer and assuming that the covariates $\mathbf{X}_i\in\mathbb{R}^{p}$ follow a multivariate Gaussian law with covariance $\mathbf{I}_p/p$ for each $i=1, \dots, n$. In order to efficiently compute the estimator under investigation, we propose a modified Approximate Message Passing (AMP) algorithm, that we name COX-AMP, and compare its performance with the Coordinate-wise Descent (CD) algorithm, which is taken as reference. By means of the Replica method, we derive a set of six Replica Symmetric (RS) equations that we show to correctly describe the average behaviour of the estimators when the sample size and the number of covariates is large and commensurate. These equations cannot be solved in practice, as the data generating process (that we are trying to estimate) is not known. However, the update equations of COX-AMP suggest the construction of a local field that can in turn be used to accurately estimate all the RS order parameters of the theory \emph{solely from the data}, \emph{without} actually solving the RS equations. We emphasize that this approach can be applied when the estimator is computed via any method and is not restricted to COX-AMP. Once the RS order parameters are estimated, we have access to the amount of signal and noise in the RMPLE, but also its generalization error, directly from the data. Although we focus on the Partial Likelihood objective, we envisage broader application of the methodology proposed here, for instance to GLMs with nuisance parameters, which include some non-proportional hazards models, e.g. Accelerated Failure Time models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13690v2</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Massa, Anthony Coolen</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination</title>
      <link>https://arxiv.org/abs/2501.11280</link>
      <description>arXiv:2501.11280v2 Announce Type: replace 
Abstract: This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, there are many unexplained aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with a limited number of parameters. It is shown that the estimators diverge under a certain condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can produce ARD mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11280v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsukasa Yoshida, Kazuho Watanabe</dc:creator>
    </item>
    <item>
      <title>Robust Mean Estimation With Auxiliary Samples</title>
      <link>https://arxiv.org/abs/2501.18095</link>
      <description>arXiv:2501.18095v2 Announce Type: replace 
Abstract: In data-driven learning and inference tasks, the high cost of acquiring samples from the target distribution often limits performance. A common strategy to mitigate this challenge is to augment the limited target samples with data from a more accessible "auxiliary" distribution. This paper establishes fundamental limits of this approach by analyzing the improvement in the mean square error (MSE) when estimating the mean of the target distribution. Using the Wasserstein-2 metric to quantify the distance between distributions, we derive expressions for the worst-case MSE when samples are drawn (with labels) from both a target distribution and an auxiliary distribution within a specified Wasserstein-2 distance from the target distribution. We explicitly characterize the achievable MSE and the optimal estimator in terms of the problem dimension, the number of samples from the target and auxiliary distributions, the Wasserstein-2 distance, and the covariance of the target distribution. We note that utilizing samples from the auxiliary distribution effectively improves the MSE when the squared radius of the Wasserstein-2 uncertainty ball is small compared to the variance of the true distribution and the number of samples from the true distribution is limited. Numerical simulations in the Gaussian location model illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18095v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barron Han, Danil Akhtiamov, Reza Ghane, Babak Hassibi</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Dimension-Reduced Clustering for High-Dimensional Nonspherical Mixtures</title>
      <link>https://arxiv.org/abs/2502.02580</link>
      <description>arXiv:2502.02580v2 Announce Type: replace 
Abstract: In mixture models, nonspherical (anisotropic) noise within each cluster is widely present in real-world data. We study both the minimax rate and optimal statistical procedure for clustering under high-dimensional nonspherical mixture models. In high-dimensional settings, we first establish the information-theoretic limits for clustering under Gaussian mixtures. The minimax lower bound unveils an intriguing informational dimension-reduction phenomenon: there exists a substantial gap between the minimax rate and the oracle clustering risk, with the former determined solely by the projected centers and projected covariance matrices in a low-dimensional space. Motivated by the lower bound, we propose a novel computationally efficient clustering method: Covariance Projected Spectral Clustering (COPO). Its key step is to project the high-dimensional data onto the low-dimensional space spanned by the cluster centers and then use the projected covariance matrices in this space to enhance clustering. We establish tight algorithmic upper bounds for COPO, both for Gaussian noise with flexible covariance and general noise with local dependence. Our theory indicates the minimax-optimality of COPO in the Gaussian case and highlights its adaptivity to a broad spectrum of dependent noise. Extensive simulation studies under various noise structures and real data analysis demonstrate our method's superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02580v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>How to Tell When a Result Will Replicate: Significance and Replication in Distributional Null Hypothesis Tests</title>
      <link>https://arxiv.org/abs/2211.02609</link>
      <description>arXiv:2211.02609v3 Announce Type: replace-cross 
Abstract: There is a well-known problem in Null Hypothesis Significance Testing: many statistically significant results fail to replicate in subsequent experiments. We show that this problem arises because standard `point-form null' significance tests consider only within-experiment but ignore between-experiment variation, and so systematically underestimate the degree of random variation in results. We give an extension to standard significance testing that addresses this problem by analysing both within- and between-experiment variation. This `distributional null' approach does not underestimate experimental variability and so is not overconfident in identifying significance; because this approach addresses between-experiment variation, it gives mathematically coherent estimates for the probability of replication of significant results. Using a large-scale replication dataset (the first `Many Labs' project), we show that many experimental results that appear statistically significant in standard tests are in fact consistent with random variation when both within- and between-experiment variation are taken into account in this approach. Further, grouping experiments in this dataset into `predictor-target' pairs we show that the predicted replication probabilities for target experiments produced in this approach (given predictor experiment results and the sample sizes of the two experiments) are strongly correlated with observed replication rates. Distributional null hypothesis testing thus gives researchers a statistical tool for identifying statistically significant and reliably replicable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02609v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fintan Costello, Paul Watts</dc:creator>
    </item>
    <item>
      <title>Latent Gaussian and H\"usler--Reiss Graphical Models with Golazo Penalty</title>
      <link>https://arxiv.org/abs/2408.12482</link>
      <description>arXiv:2408.12482v2 Announce Type: replace-cross 
Abstract: The existence of latent variables in practical problems is common, for example when some variables are difficult or expensive to measure, or simply unknown. When latent variables are unaccounted for, structure learning for Gaussian graphical models can be blurred by additional correlation between the observed variables that is incurred by the latent variables. A standard approach for this problem is a latent version of the graphical lasso that splits the inverse covariance matrix into a sparse and a low-rank part that are penalized separately. This approach has recently been extended successfully to H\"usler--Reiss graphical models, which can be considered as an analogue of Gaussian graphical models in extreme value statistics. In this paper we propose a generalization of structure learning for Gaussian and H\"usler--Reiss graphical models via the flexible Golazo penalty. This allows us to introduce latent versions of for example the adaptive lasso, positive dependence constraints or predetermined sparsity patterns, and combinations of those. We develop algorithms for both latent graphical models with the Golazo penalty and demonstrate them on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12482v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ignacio Echave-Sustaeta Rodr\'iguez, Frank R\"ottger</dc:creator>
    </item>
    <item>
      <title>Randomness, exchangeability, and conformal prediction</title>
      <link>https://arxiv.org/abs/2501.11689</link>
      <description>arXiv:2501.11689v2 Announce Type: replace-cross 
Abstract: This paper continues development of the functional theory of randomness, a modification of the algorithmic theory of randomness getting rid of unspecified additive constants. It introduces new kinds of confidence predictors, including randomness predictors (the most general confidence predictors based on the assumption of IID observations) and exchangeability predictors (the most general confidence predictors based on the assumption of exchangeable observations). The main result implies that both are close to conformal predictors and quantifies the difference between randomness prediction and conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11689v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
  </channel>
</rss>
