<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>COSMOS: A Data-Driven Probabilistic Time Series simulator for Chemical Plumes across Spatial Scales</title>
      <link>https://arxiv.org/abs/2505.22436</link>
      <description>arXiv:2505.22436v1 Announce Type: new 
Abstract: The development of robust odor navigation strategies for automated environmental monitoring applications requires realistic simulations of odor time series for agents moving across large spatial scales. Traditional approaches that rely on computational fluid dynamics (CFD) methods can capture the spatiotemporal dynamics of odor plumes, but are impractical for large-scale simulations due to their computational expense. On the other hand, puff-based simulations, although computationally tractable for large scales and capable of capturing the stochastic nature of plumes, fail to reproduce naturalistic odor statistics. Here, we present COSMOS (Configurable Odor Simulation Model over Scalable Spaces), a data-driven probabilistic framework that synthesizes realistic odor time series from spatial and temporal features of real datasets. COSMOS generates similar distributions of key statistical features such as whiff frequency, duration, and concentration as observed in real data, while dramatically reducing computational overhead. By reproducing critical statistical properties across a variety of flow regimes and scales, COSMOS enables the development and evaluation of agent-based navigation strategies with naturalistic odor experiences. To demonstrate its utility, we compare odor-tracking agents exposed to CFD-generated plumes versus COSMOS simulations, showing that both their odor experiences and resulting behaviors are quite similar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22436v1</guid>
      <category>stat.AP</category>
      <category>cs.RO</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arunava Nag, Floris van Breugel</dc:creator>
    </item>
    <item>
      <title>pared: Model selection using multi-objective optimization</title>
      <link>https://arxiv.org/abs/2505.21730</link>
      <description>arXiv:2505.21730v1 Announce Type: cross 
Abstract: Motivation: Model selection is a ubiquitous challenge in statistics. For penalized models, model selection typically entails tuning hyperparameters to maximize a measure of fit or minimize out-of-sample prediction error. However, these criteria fail to reflect other desirable characteristics, such as model sparsity, interpretability, or smoothness. Results: We present the R package pared to enable the use of multi-objective optimization for model selection. Our approach entails the use of Gaussian process-based optimization to efficiently identify solutions that represent desirable trade-offs. Our implementation includes popular models with multiple objectives including the elastic net, fused lasso, fused graphical lasso, and group graphical lasso. Our R package generates interactive graphics that allow the user to identify hyperparameter values that result in fitted models which lie on the Pareto frontier. Availability: We provide the R package pared and vignettes illustrating its application to both simulated and real data at https://github.com/priyamdas2/pared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21730v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das, Sarah Robinson, Christine B. Peterson</dc:creator>
    </item>
    <item>
      <title>Adaptive Block-Based Change-Point Detection for Sparse Spatially Clustered Data with Applications in Remote Sensing Imaging</title>
      <link>https://arxiv.org/abs/2505.21814</link>
      <description>arXiv:2505.21814v1 Announce Type: cross 
Abstract: We present a non-parametric change-point detection approach to detect potentially sparse changes in a time series of high-dimensional observations or non-Euclidean data objects. We target a change in distribution that occurs in a small, unknown subset of dimensions, where these dimensions may be correlated. Our work is motivated by a remote sensing application, where changes occur in small, spatially clustered regions over time. An adaptive block-based change-point detection framework is proposed that accounts for spatial dependencies across dimensions and leverages these dependencies to boost detection power and improve estimation accuracy. Through simulation studies, we demonstrate that our approach has superior performance in detecting sparse changes in datasets with spatial or local group structures. An application of the proposed method to detect activity, such as new construction, in remote sensing imagery of the Natanz Nuclear facility in Iran is presented to demonstrate the method's efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21814v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan Moore, Lynna Chu, Zhengyuan Zhu</dc:creator>
    </item>
    <item>
      <title>Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations</title>
      <link>https://arxiv.org/abs/2505.21824</link>
      <description>arXiv:2505.21824v1 Announce Type: cross 
Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus (T2DM), is rapidly increasing, posing significant health and economic challenges. T2DM not only disrupts blood glucose regulation but also damages vital organs such as the heart, kidneys, eyes, nerves, and blood vessels, leading to substantial morbidity and mortality. In the US alone, the economic burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of individuals at risk is critical to mitigating these impacts. While machine learning approaches for T2DM prediction are increasingly adopted, many rely on supervised learning, which is often limited by the lack of confirmed negative cases. To address this limitation, we propose a novel unsupervised framework that integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify individuals at risk of developing T2DM. Our method identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate the T2DM risk in undiagnosed individuals. By leveraging data-driven insights from comorbidity and medication usage, our approach provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, ultimately improving patient outcomes and potentially reducing the future health and economic burden of T2DM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21824v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Praveen Kumar, Vincent T. Metzger, Scott A. Malec</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v1 Announce Type: cross 
Abstract: How should we analyze randomized experiments in which the main outcome is measured in multiple ways and each measure contains some degree of error? Since Costner (1971) and Bagozzi (1977), methodological discussions of experiments with latent outcomes have reviewed the modeling assumptions that are invoked when the quantity of interest is the average treatment effect (ATE) of a randomized intervention on a latent outcome that is measured with error. Many authors have proposed methods to estimate this ATE when multiple measures of an outcome are available. Despite this extensive literature, social scientists rarely use these modeling approaches when analyzing experimental data, perhaps because the surge of interest in experiments coincides with increased skepticism about the modeling assumptions that these methods invoke. The present paper takes a fresh look at the use of latent variable models to analyze experiments. Like the skeptics, we seek to minimize reliance on ad hoc assumptions that are not rooted in the experimental design and measurement strategy. At the same time, we think that some of the misgivings that are frequently expressed about latent variable models can be addressed by modifying the research design in ways that make the underlying assumptions defensible or testable. We describe modeling approaches that enable researchers to identify and estimate key parameters of interest, suggest ways that experimental designs can be augmented so as to make the modeling requirements more credible, and discuss empirical tests of key modeling assumptions. Simulations and an empirical application illustrate the gains in terms of precision and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Beyond data: leveraging non-empirical information and expert knowledge in Bayesian model calibration</title>
      <link>https://arxiv.org/abs/2505.21934</link>
      <description>arXiv:2505.21934v1 Announce Type: cross 
Abstract: Mathematical models connect theory with the real world through data, enabling us to interpret, understand, and predict complex phenomena. However, scientific knowledge often extends beyond what can be empirically measured, offering valuable insights into complex and uncertain systems. Here, we introduce a statistical framework for calibrating mathematical models using non-empirical information. Through examples in ecology, biology, and medicine, we demonstrate how expert knowledge, scientific theory, and qualitative observations can meaningfully constrain models. In each case, these non-empirical insights guide models toward more realistic dynamics and more informed predictions than empirical data alone could achieve. Now, our understanding of the systems represented by mathematical models is not limited by the data that can be obtained; they instead sit at the edge of scientific understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21934v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah A. Vollert, Christopher Drovandi, Cailan Jeynes-Smith, Luz V. Pascal, Matthew P. Adams</dc:creator>
    </item>
    <item>
      <title>Post-processing of wind gusts from COSMO-REA6 with a spatial Bayesian hierarchical extreme value model</title>
      <link>https://arxiv.org/abs/2505.22182</link>
      <description>arXiv:2505.22182v1 Announce Type: cross 
Abstract: The aim of this study is to provide a probabilistic gust analysis for the region of Germany that is calibrated with station observations and with an interpolation to unobserved locations. To this end, we develop a spatial Bayesian hierarchical model (BHM) for the post-processing of surface maximum wind gusts from the COSMO-REA6 reanalysis. Our approach uses a non-stationary extreme value distribution for the gust observations, with parameters that vary according to a linear model using COSMO-REA6 predictor variables. To capture spatial patterns in surface wind gust behavior, the regression coefficients are modeled as 2-dimensional Gaussian random fields with a constant mean and an isotropic covariance function that depends on the distance between locations. In addition, we include an elevation offset in the distance metric for the covariance function to account for the topography. This allows us to include data from mountaintop stations in the training process. The training of the BHM is carried out with an independent data set from which the data at the station to be predicted are excluded. We evaluate the spatial prediction performance at the withheld station using Brier score and quantile score, including their decomposition, and compare the performance of our BHM to climatological forecasts and a non-hierarchical, spatially constant baseline model. This is done for 109 weather stations in Germany. Compared to the spatially constant baseline model, the spatial BHM significantly improves the estimation of local gust parameters. It shows up to 5 % higher skill for prediction quantiles and provides a particularly improved skill for extreme wind gusts. In addition, the BHM improves the prediction of threshold levels at most of the stations. Although a spatially constant approach already provides high skill, our BHM further improves predictions and improves spatial consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22182v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Ertz, Petra Friederichs</dc:creator>
    </item>
    <item>
      <title>Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays</title>
      <link>https://arxiv.org/abs/2505.22496</link>
      <description>arXiv:2505.22496v1 Announce Type: cross 
Abstract: This paper presents a novel approach to catheter and line position detection in chest X-rays, combining multi-task learning with risk-sensitive conformal prediction to address critical clinical requirements. Our model simultaneously performs classification, segmentation, and landmark detection, leveraging the synergistic relationship between these tasks to improve overall performance. We further enhance clinical reliability through risk-sensitive conformal prediction, which provides statistically guaranteed prediction sets with higher reliability for clinically critical findings. Experimental results demonstrate excellent performance with 90.68\% overall empirical coverage and 99.29\% coverage for critical conditions, while maintaining remarkable precision in prediction sets. Most importantly, our risk-sensitive approach achieves zero high-risk mispredictions (cases where the system dangerously declares problematic tubes as confidently normal), making the system particularly suitable for clinical deployment. This work offers both accurate predictions and reliably quantified uncertainty -- essential features for life-critical medical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22496v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Long Hui</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2505.22551</link>
      <description>arXiv:2505.22551v1 Announce Type: cross 
Abstract: Limited DXA access hinders osteoporosis screening. This proof-of-concept study proposes using widely available knee X-rays for opportunistic Bone Mineral Density (BMD) estimation via deep learning, emphasizing robust uncertainty quantification essential for clinical use. An EfficientNet model was trained on the OAI dataset to predict BMD from bilateral knee radiographs. Two Test-Time Augmentation (TTA) methods were compared: traditional averaging and a multi-sample approach. Crucially, Split Conformal Prediction was implemented to provide statistically rigorous, patient-specific prediction intervals with guaranteed coverage. Results showed a Pearson correlation of 0.68 (traditional TTA). While traditional TTA yielded better point predictions, the multi-sample approach produced slightly tighter confidence intervals (90%, 95%, 99%) while maintaining coverage. The framework appropriately expressed higher uncertainty for challenging cases. Although anatomical mismatch between knee X-rays and standard DXA limits immediate clinical use, this method establishes a foundation for trustworthy AI-assisted BMD screening using routine radiographs, potentially improving early osteoporosis detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22551v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Long Hui, Wai Lok Yeung</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Modelling of Age-Specific Counts in Many Demographic Subpopulations</title>
      <link>https://arxiv.org/abs/2401.08247</link>
      <description>arXiv:2401.08247v3 Announce Type: replace 
Abstract: Analysing age-specific mortality, fertility, and migration patterns is a crucial task in demography with significant policy relevance. In practice, such analysis is challenging when studying a large number of subpopulations, due to small observation counts within groups and increasing demographic heterogeneity between groups. This article proposes a Bayesian model for the joint analysis of age-specific counts in many, potentially small, demographic subpopulations. The model utilizes smooth latent factors to capture common age-specific patterns across subpopulations and facilitates additional information sharing through a hierarchical prior. It provides smoothed estimates of the latent age pattern in each subpopulation, allows testing for heterogeneity, and can be used to assess the impact of covariates on the demographic process. An in-depth case study of age-specific immigration flows to Austria, disaggregated by sex and 155 countries of origin, is discussed. Comparative analysis demonstrates that the model outperforms commonly used benchmark frameworks in both in-sample imputation and out-of-sample predictive exercises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08247v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens</dc:creator>
    </item>
    <item>
      <title>Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes</title>
      <link>https://arxiv.org/abs/2401.12195</link>
      <description>arXiv:2401.12195v5 Announce Type: replace 
Abstract: When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail rather than the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals that different facets of heat extremes are influenced by individual circulation features, such as the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. This enriches our process understanding from a data-driven perspective. Our approach can extrapolate beyond the range of the data to make risk-related probabilistic statements. It applies more generally to other weather extremes and offers an alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12195v5</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1098/rspa.2024.0763</arxiv:DOI>
      <arxiv:journal_reference>Proc. R. Soc. A. 481 (2025)</arxiv:journal_reference>
      <dc:creator>Jonathan Koh, Daniel Steinfeld, Olivia Martius</dc:creator>
    </item>
    <item>
      <title>Absolute Risk Prediction for Cannabis Use Disorder in Adolescence and Early Adulthood Using Bayesian Machine Learning</title>
      <link>https://arxiv.org/abs/2501.09156</link>
      <description>arXiv:2501.09156v2 Announce Type: replace 
Abstract: Introduction: Substance use disorders (SUDs) have emerged as a pressing public health concern in the United States, with adolescent substance use often leading to SUDs in adulthood. Effective strategies are needed to stem this progression. To help fulfill this need, we developed a novel absolute risk prediction model for cannabis use disorder (CUD) for adolescents or young adults who use cannabis. Methods: We trained a Bayesian machine learning model that provides a personalized CUD absolute risk for adolescents or young adults who use cannabis with data from the National Longitudinal Study of Adolescent to Adult Health. Model performance was assessed using 5-fold cross-validation (CV) with area under the curve (AUC) and ratio of the expected to observed number of cases (E/O). Independent validation of the final model was conducted using two datasets. Results: The proposed model has five risk factors: biological sex, delinquency, and scores on personality traits of conscientiousness, neuroticism, and openness. For predicting CUD risk within five years of first cannabis use, AUC values for the training dataset and two validation datasets were 0.68, 0.64, and 0.75, respectively, and E/O values were 0.95, 0.98, and 1, respectively. This indicates good discrimination and calibration performance of the model. Discussion and Conclusion: The proposed model can aid clinicians in assessing the risk of developing CUD among adolescents and young adults who use cannabis, enabling clinically appropriate interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09156v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingfang Wang, Joseph M. Boden, Swati Biswas, Pankaj K. Choudhary</dc:creator>
    </item>
    <item>
      <title>Stochastic comparison of series and parallel systems lifetime in Archimedean copula under random shock</title>
      <link>https://arxiv.org/abs/2406.05834</link>
      <description>arXiv:2406.05834v2 Announce Type: replace-cross 
Abstract: In this paper, we studied the stochastic ordering behavior of series as well as parallel systems' lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions on the lifetime of individual components where the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to get the results. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05834v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarikul Islam, Nitin Gupta</dc:creator>
    </item>
  </channel>
</rss>
