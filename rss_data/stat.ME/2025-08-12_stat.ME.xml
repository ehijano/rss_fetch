<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Consistent Bayesian Spatial Domain Partitioning Using Predictive Spanning Tree Methods</title>
      <link>https://arxiv.org/abs/2508.08324</link>
      <description>arXiv:2508.08324v1 Announce Type: new 
Abstract: Bayesian model-based spatial clustering methods are widely used for their flexibility in estimating latent clusters with an unknown number of clusters while accounting for spatial proximity. Many existing methods are designed for clustering finite spatial units, limiting their ability to make predictions, or may impose restrictive geometric constraints on the shapes of subregions. Furthermore, the posterior clustering consistency theory of spatial clustering models remains largely unexplored in the literature. In this study, we propose a Spatial Domain Random Partition Model (Spat-RPM) and demonstrate its application for spatially clustered regression, which extends spanning tree-based Bayesian spatial clustering by partitioning the spatial domain into disjoint blocks and using spanning tree cuts to induce contiguous domain partitions. Under an infill-domain asymptotic framework, we introduce a new distance metric to study the posterior concentration of domain partitions. We show that Spat-RPM achieves a consistent estimation of domain partitions, including the number of clusters, and derive posterior concentration rates for partition, parameter, and prediction. We also establish conditions on the hyperparameters of priors and the number of blocks, offering important practical guidance for hyperparameter selection. Finally, we examine the asymptotic properties of our model through simulation studies and apply it to Atlantic Ocean data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08324v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Huang, Huiyan Sang</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes for Data Integration</title>
      <link>https://arxiv.org/abs/2508.08336</link>
      <description>arXiv:2508.08336v1 Announce Type: new 
Abstract: We discuss the use of empirical Bayes for data integration, in the sense of transfer learning. Our main interest is in settings where one wishes to learn structure (e.g. feature selection) and one only has access to incomplete data from previous studies, such as summaries, estimates or lists of relevant features. We discuss differences between full Bayes and empirical Bayes, and develop a computational framework for the latter. We discuss how empirical Bayes attains consistent variable selection under weaker conditions (sparsity and betamin assumptions) than full Bayes and other standard criteria do, and how it attains faster convergence rates. Our high-dimensional regression examples show that fully Bayesian inference enjoys excellent properties, and that data integration with empirical Bayes can offer moderate yet meaningful improvements in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08336v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell</dc:creator>
    </item>
    <item>
      <title>Multiple Regression Analysis of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2508.08412</link>
      <description>arXiv:2508.08412v1 Announce Type: new 
Abstract: Whereas confidence intervals are used to assess uncertainty due to unmeasured individuals, confounding intervals can be used to assess uncertainty due to unmeasured attributes. Previously, we have introduced a methodology for computing confounding intervals in a simple regression setting in a paper titled ``Regression Analysis of Unmeasured Confounding." Here we extend that methodology for more general application in the context of multiple regression. Our multiple regression analysis of unmeasured confounding utilizes subject matter knowledge about coefficients of determination to bound omitted variables bias, while taking into account measured covariate data. Our generalized methodology can be used to partially identify causal effects. The methodology is demonstrated with example applications, to show how coefficients of determination, being complementary to randomness, can support sensitivity analysis for causal inference from observational data. The methodology is best applied when natural sources of randomness are present and identifiable within the data generating process. Our main contribution is an algorithm that supports our methodology. The purpose of this article is to describe our algorithm in detail. In the paper we provide a link to our GitHub page for readers who would like to access and utilize our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08412v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Knaeble, R Mitchell Hughes</dc:creator>
    </item>
    <item>
      <title>Doubly robust pointwise confidence intervals for a monotonic continuous treatment effect curve</title>
      <link>https://arxiv.org/abs/2508.08415</link>
      <description>arXiv:2508.08415v1 Announce Type: new 
Abstract: We study nonparametric inference for the causal dose-response (or treatment effect) curve when the treatment variable is continuous rather than binary or discrete. We do this by developing doubly robust confidence intervals for the continuous treatment effect curve (at a fixed point) under the assumption that it is monotonic, based on inverting a likelihood ratio-type test. Monotonicity of the treatment effect curve is often a very natural assumption, and this assumption removes the need to choose a smoothing or tuning parameter for the nonparametrically estimated curve. The likelihood ratio procedure is effective because it allows us to avoid estimating the curve's unknown bias, which is challenging to do. The test statistic is ``doubly robust'' in that a remainder term is the product of errors for the two so-called nuisance functions that naturally arise (the outcome regression and generalized propensity score functions), which allows one nuisance to be estimated poorly if the other is estimated well. Furthermore, we propose a version of our test or confidence interval that is adaptive to a range of the unknown curve's flatness level. We present versions with and without cross fitting. We illustrate the new methods via simulations and a study of a dataset relating the effect of nurse staffing hours on hospital performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08415v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles R. Doss</dc:creator>
    </item>
    <item>
      <title>Single and multi-objective optimal designs for group testing experiments</title>
      <link>https://arxiv.org/abs/2508.08445</link>
      <description>arXiv:2508.08445v1 Announce Type: new 
Abstract: Group testing, or pooled sample testing, is an active research area with increasingly diverse applications across disciplines. This paper considers design issues for group testing problems when a statistical model, an optimality criterion and a cost function are given. We use the software CVX to find designs that best estimate all or some of the model parameters ($D$ -, $D_s$ -, $A$-optimality) when there is one or more objectives in the study. A novel feature is that we include maximin types of optimal designs, like $E$-optimal designs, which do not have a differentiable criterion and have not been used in group testing problems before, or, as part of a criterion in a multi-objective design problem. When the sample size is large, we search for optimal approximate designs; otherwise, we find optimal exact designs and compare their robustness properties under a variation of criteria, statistical models, and cost functions. We also provide free user-friendly CVX sample codes to facilitate implementation of our proposed designs and amend them to find other types of optimal designs, such as, robust $E$-optimal designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08445v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Kuang Yeh, Weng Kee Wong, Julie Zhou</dc:creator>
    </item>
    <item>
      <title>Causal Geodesy: Counterfactual Estimation Along the Path Between Correlation and Causation</title>
      <link>https://arxiv.org/abs/2508.08499</link>
      <description>arXiv:2508.08499v1 Announce Type: new 
Abstract: We introduce causal geodesy, a framework for studying the landscape of stochastic interventions that lie between the two extremes of performing no intervention, and performing a sharp intervention that sets an exposure equal to a specific value. We define this framework by constructing paths of distributions that smoothly interpolate between the treatment density and a point mass at the target intervention. Thus, each path starts at a purely observational (or correlational) quantity and moves into a counterfactual world. Of particular interest are paths that correspond to geodesics in some metric, i.e. the shortest path. We then consider the interpretation and estimation of the corresponding causal effects as we move along the path from correlation toward causation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08499v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Kernel Two-Sample Testing via Directional Components Analysis</title>
      <link>https://arxiv.org/abs/2508.08564</link>
      <description>arXiv:2508.08564v1 Announce Type: new 
Abstract: We propose a novel kernel-based two-sample test that leverages the spectral decomposition of the maximum mean discrepancy (MMD) statistic to identify and utilize well-estimated directional components in reproducing kernel Hilbert space (RKHS). Our approach is motivated by the observation that the estimation quality of these components varies significantly, with leading eigen-directions being more reliably estimated in finite samples. By focusing on these directions and aggregating information across multiple kernels, the proposed test achieves higher power and improved robustness, especially in high-dimensional and unbalanced sample settings. We further develop a computationally efficient multiplier bootstrap procedure for approximating critical values, which is theoretically justified and significantly faster than permutation-based alternatives. Extensive simulations and empirical studies on microarray datasets demonstrate that our method maintains the nominal Type I error rate and delivers superior power compared to other existing MMD-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08564v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Cui, Yuhao Li, Xiaojun Song</dc:creator>
    </item>
    <item>
      <title>Analytics of Adaptive Online Testing in Practice Over a Decade</title>
      <link>https://arxiv.org/abs/2508.08643</link>
      <description>arXiv:2508.08643v1 Announce Type: new 
Abstract: Adaptive online testing efficiently assesses examinee proficiency by dynamically adjusting the difficulty of test items based on their performance. To achieve this, items are selected so that their difficulty closely matches the test taker's estimated ability at each stage of the test. This alignment implies that the probability of a correct answer tends toward 0.5. However, in practical settings, this probability may not converge to 0.5 unless the test comprises a sufficiently large number of items. This could give the impression that the adaptive mechanism is not functioning properly. Nevertheless, even when the number of items is small, such as 5 or 7, the adaptive nature of the system can still be observed by examining the relationship between item difficulty and the mean estimated ability of examinees for the corresponding item.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08643v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hideo Hirose</dc:creator>
    </item>
    <item>
      <title>Identifiability of linear stochastic state-space models with application to ecology</title>
      <link>https://arxiv.org/abs/2508.08714</link>
      <description>arXiv:2508.08714v1 Announce Type: new 
Abstract: State-space models are dynamical systems defined by a latent and an observed process. In ecology, stochastic state-space models in discrete time are most often used to describe the imperfectly observed dynamics of population sizes or animal movement. However, several studies have observed identifiability issues when state-space models are fitted to simulated or real data, and it is not currently clear whether those are due to data limitations or more fundamental model non-identifiability. To investigate such theoretical identifiability, a suitable exhaustive summary is required, defined as a vector of parameter combinations which fully determines the model. Previous work on exhaustive summaries has used expectations of the stochastic process, so that noise parameters are unaccounted for. In this paper, we build an exhaustive summary using the spectral density of the observed process, which fully accounts for all mean and variance parameters. This diagnostic is applied to contrasted ecological models and we show that they are generally theoretically identifiable, unless some model compartements are unobserved. This suggest that issues encountered while fitting models are mostly due to practical identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08714v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederic Barraquand, Julien Gibaud</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis to Unobserved Confounding with Copula-based Normalizing Flows</title>
      <link>https://arxiv.org/abs/2508.08752</link>
      <description>arXiv:2508.08752v1 Announce Type: new 
Abstract: We propose a novel method for sensitivity analysis to unobserved confounding in causal inference. The method builds on a copula-based causal graphical normalizing flow that we term $\rho$-GNF, where $\rho \in [-1,+1]$ is the sensitivity parameter. The parameter represents the non-causal association between exposure and outcome due to unobserved confounding, which is modeled as a Gaussian copula. In other words, the $\rho$-GNF enables scholars to estimate the average causal effect (ACE) as a function of $\rho$, accounting for various confounding strengths. The output of the $\rho$-GNF is what we term the $\rho_{curve}$, which provides the bounds for the ACE given an interval of assumed $\rho$ values. The $\rho_{curve}$ also enables scholars to identify the confounding strength required to nullify the ACE. We also propose a Bayesian version of our sensitivity analysis method. Assuming a prior over the sensitivity parameter $\rho$ enables us to derive the posterior distribution over the ACE, which enables us to derive credible intervals. Finally, leveraging on experiments from simulated and real-world data, we show the benefits of our sensitivity analysis method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08752v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourabh Balgi, Marc Braun, Jose M. Pe\~na, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Random-effects meta-analysis via generalized linear mixed models: A Bartlett-corrected approach for few studies</title>
      <link>https://arxiv.org/abs/2508.08758</link>
      <description>arXiv:2508.08758v1 Announce Type: new 
Abstract: Random-effects meta-analysis is widely used for synthesizing results across studies, but the implicit assumption of normality in study-specific aggregate data is often violated. Such violations can lead to biased estimates and misleading conclusions, especially in meta-analyses with small studies or rare events. A prominent example occurs with the log-odds ratio, which exhibits bias that depends on the within-study sample sizes. We first show that conventional methods assuming normality fail to eliminate such biases, even as the number of studies increases. To overcome this limitation, we introduce a generalized linear mixed-effects model that uses only aggregate data, accommodating a wide range of outcome types, including binomial, Poisson, gamma, and other members of the exponential family, without requiring individual participant data. To enable valid interval estimation when the number of studies is small, we further propose a simple Bartlett correction for the test statistics. The proposed method yields consistent point estimators without relying on the normality assumption and achieves accurate interval coverage across diverse outcome types. It is particularly applicable in clinical and epidemiological research where only summary data are available, making it a practical alternative to conventional approaches. Simulation studies and applications to three published meta-analyses with binary, Poisson, and gamma outcomes demonstrate that the method provides reliable inference and maintains nominal coverage, thereby supporting sound decision-making and guideline development when only aggregate data are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08758v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Tomoyuki Sugimoto</dc:creator>
    </item>
    <item>
      <title>Debiased machine learning for combining probability and non-probability survey data</title>
      <link>https://arxiv.org/abs/2508.08948</link>
      <description>arXiv:2508.08948v1 Announce Type: new 
Abstract: We consider the problem of estimating the finite population mean $\bar{Y}$ of an outcome variable $Y$ using data from a nonprobability sample and auxiliary information from a probability sample. Existing double robust (DR) estimators of this mean $\bar{Y}$ require the estimation of two nuisance functions: the conditional probability of selection into the nonprobability sample given covariates $X$ that are observed in both samples, and the conditional expectation of $Y$ given $X$. These nuisance functions can be estimated using parametric models, but the resulting estimator of $\bar{Y}$ will typically be biased if both parametric models are misspecified. It would therefore be advantageous to be able to use more flexible data-adaptive / machine-learning estimators of the nuisance functions. Here, we develop a general framework for the valid use of DR estimators of $\bar{Y}$ when the design of the probability sample uses sampling without replacement at the first stage and data-adaptive / machine-learning estimators are used for the nuisance functions. We prove that several DR estimators of $\bar{Y}$, including targeted maximum likelihood estimators, are asymptotically normally distributed when the estimators of the nuisance functions converge faster than the $n^{1/4}$ rate and cross-fitting is used. We present a simulation study that demonstrates good performance of these DR estimators compared to the corresponding DR estimators that rely on at least one correctly specified parametric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08948v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun Seaman</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v1 Announce Type: new 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Chang, Arkaprava Roy, John Kairalla</dc:creator>
    </item>
    <item>
      <title>Bias correction for Chatterjee's graph-based correlation coefficient</title>
      <link>https://arxiv.org/abs/2508.09040</link>
      <description>arXiv:2508.09040v1 Announce Type: new 
Abstract: Azadkia and Chatterjee (2021) recently introduced a simple nearest neighbor (NN) graph-based correlation coefficient that consistently detects both independence and functional dependence. Specifically, it approximates a measure of dependence that equals 0 if and only if the variables are independent, and 1 if and only if they are functionally dependent. However, this NN estimator includes a bias term that may vanish at a rate slower than root-$n$, preventing root-$n$ consistency in general. In this article, we propose a bias correction approach that overcomes this limitation, yielding an NN-based estimator that is both root-$n$ consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09040v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mona Azadkia, Leihao Chen, Fang Han</dc:creator>
    </item>
    <item>
      <title>Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs</title>
      <link>https://arxiv.org/abs/2508.09135</link>
      <description>arXiv:2508.09135v1 Announce Type: new 
Abstract: Adaptive experimental designs have gained popularity in clinical trials and online experiments. Unlike traditional, fixed experimental designs, adaptive designs can dynamically adjust treatment randomization probabilities and other design features in response to data accumulated sequentially during the experiment. These adaptations are useful to achieve diverse objectives, including reducing uncertainty in the estimation of causal estimands or increasing participants' chances of receiving better treatments during the experiment. At the end of the experiment, it is often desirable to answer causal questions from the observed data. However, the adaptive nature of such experiments and the resulting dependence among observations pose significant challenges to providing valid statistical inference and efficient estimation of causal estimands. Building upon the Targeted Maximum Likelihood Estimator (TMLE) framework tailored for adaptive designs (van der Laan, 2008), we introduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a variety of causal estimands from adaptive experiment data. We establish asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed positivity and design stabilization assumptions for adaptive experiments. Motivated by efficiency results, we further propose a novel adaptive design aimed at minimizing the variance of estimators based on data generated under that design. Using the average treatment effect as a representative example, simulation studies show that ADL-TMLE demonstrates superior variance-reduction performance across different types of adaptive experiments, and that the proposed adaptive design attains lower variance than the standard efficiency-oriented adaptive design. Finally, we generalize this estimation and design framework to broader settings with longitudinal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09135v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Differentiable Cyclic Causal Discovery Under Unmeasured Confounders</title>
      <link>https://arxiv.org/abs/2508.08450</link>
      <description>arXiv:2508.08450v1 Announce Type: cross 
Abstract: Understanding causal relationships between variables is fundamental across scientific disciplines. Most causal discovery algorithms rely on two key assumptions: (i) all variables are observed, and (ii) the underlying causal graph is acyclic. While these assumptions simplify theoretical analysis, they are often violated in real-world systems, such as biological networks. Existing methods that account for confounders either assume linearity or struggle with scalability. To address these limitations, we propose DCCD-CONF, a novel framework for differentiable learning of nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data. Through experiments on synthetic data and real-world gene perturbation datasets, we show that DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification. Additionally, we also provide consistency guarantees for our framework, reinforcing its theoretical soundness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08450v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muralikrishnna G. Sethuraman, Faramarz Fekri</dc:creator>
    </item>
    <item>
      <title>Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption</title>
      <link>https://arxiv.org/abs/2508.08883</link>
      <description>arXiv:2508.08883v1 Announce Type: cross 
Abstract: Causal machine learning has the potential to revolutionize decision-making by combining the predictive power of machine learning algorithms with the theory of causal inference. However, these methods remain underutilized by the broader machine learning community, in part because current empirical evaluations do not permit assessment of their reliability and robustness, undermining their practical utility. Specifically, one of the principal criticisms made by the community is the extensive use of synthetic experiments. We argue, on the contrary, that synthetic experiments are essential and necessary to precisely assess and understand the capabilities of causal machine learning methods. To substantiate our position, we critically review the current evaluation practices, spotlight their shortcomings, and propose a set of principles for conducting rigorous empirical analyses with synthetic data. Adopting the proposed principles will enable comprehensive evaluations that build trust in causal machine learning methods, driving their broader adoption and impactful real-world use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08883v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Audrey Poinsot, Panayiotis Panayiotou, Alessandro Leite, Nicolas Chesneau, \"Ozg\"ur \c{S}im\c{s}ek, Marc Schoenauer</dc:creator>
    </item>
    <item>
      <title>Network Dynamics in the Colombian Armed Conflict: Armed Structures and Municipalities</title>
      <link>https://arxiv.org/abs/2508.09051</link>
      <description>arXiv:2508.09051v1 Announce Type: cross 
Abstract: In this article, we explore how the escalating victimization of civilians during civil wars is mirrored in the fragmented distribution of territorial control, focusing on the Colombian armed conflict. Through an exhaustive characterization of the topology of bipartite and projected networks of both armed groups and municipalities, we are able to describe changes in territorial configurations across different periods between 1980 and 2007. Our findings show that during periods dominated by a small set of actors, networks adopt a centralized node-periphery structure, while during times of widespread conflict, areas of influence of armed groups overlap in complex ways. Also, by employing stochastic block models for count data, we identify cohesive municipal communities during 2000-2001, shaped by both geographic proximity and affinities between armed structures, as well as the existence of internally dispersed groups with a high likelihood of interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09051v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Perdomo-Londo\~no, Juan Sosa, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2301.07513</link>
      <description>arXiv:2301.07513v3 Announce Type: replace 
Abstract: Random graphs have been widely used in statistics, for example in network analysis and graphical models. In some applications, the data may contain an inherent hierarchical ordering among its vertices, which prevents directed edges between pairs of vertices that do not respect this order. For example, in bibliometrics, older papers cannot cite newer ones. In such situations, the resulting graph forms a Directed Acyclic Graph. In this article, we extend the Stochastic Block Model (SBM) to account for the presence of such ordering in the data, ignoring which can lead to biased estimates of the number of blocks. The proposed approach includes in the model likelihood a topological ordering, which is treated as an unknown parameter and endowed with a prior distribution. We describe how to formalise the model and perform posterior inference for a Bayesian nonparametric version of the SBM in which both the hierarchical ordering and the number of latent blocks are learnt from the data. Finally, an illustration with real-world datasets from bibliometrics is presented. Additional supplementary materials are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07513v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clement Lee, Marco Battiston</dc:creator>
    </item>
    <item>
      <title>Generalized Estimating Equations for Hearing Loss Data with Specified Correlation Structures</title>
      <link>https://arxiv.org/abs/2306.16104</link>
      <description>arXiv:2306.16104v2 Announce Type: replace 
Abstract: Due to the nature of pure-tone audiometry test, hearing loss data often has a complicated correlation structure. Generalized estimating equation (GEE) is commonly used to investigate the association between exposures and hearing loss, because it is robust to misspecification of the correlation matrix. However, this robustness typically entails a moderate loss of estimation efficiency in finite samples. This paper proposes to model the correlation coefficients and use second-order generalized estimating equations to estimate the correlation parameters. In simulation studies, we assessed the finite sample performance of our proposed method and compared it with other methods, such as GEE with independent, exchangeable and unstructured correlation structures. Our method achieves an efficiency gain which is larger for the coefficients of the covariates corresponding to the within-cluster variation (e.g., ear-level covariates) than the coefficients of cluster-level covariates. The efficiency gain is also more pronounced when the within-cluster correlations are moderate to strong, or when comparing to GEE with an unstructured correlation structure. As a real-world example, we applied the proposed method to data from the Audiology Assessment Arm of the Conservation of Hearing Study, and studied the association between a dietary adherence score and hearing loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16104v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoran Wei, Hanbing Zhu, Sharon Curhan, Gary Curhan, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Estimating and evaluating counterfactual prediction models</title>
      <link>https://arxiv.org/abs/2308.13026</link>
      <description>arXiv:2308.13026v4 Announce Type: replace 
Abstract: Counterfactual prediction methods are required when a model will be deployed in a setting where treatment policies differ from the setting where the model was developed, or when a model provides predictions under hypothetical interventions to support decision-making. However, estimating and evaluating counterfactual prediction models is challenging because, unlike traditional (factual) prediction, one does not observe the potential outcomes for all individuals under all treatment strategies of interest. Here, we discuss how to estimate a counterfactual prediction model, how to assess the model's performance, and how to perform model and tuning parameter selection. We provide identification and estimation results for counterfactual prediction models and for multiple measures of counterfactual model performance, including loss-based measures, the area under the receiver operating characteristic curve, and the calibration curve. Importantly, our results allow valid estimates of model performance under counterfactual intervention even if the candidate prediction model is misspecified, permitting a wider array of use cases. We illustrate these methods using simulation and apply them to the task of developing a statin-naive risk prediction model for cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13026v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Issa J. Dahabreh, Jon A. Steingrimsson</dc:creator>
    </item>
    <item>
      <title>Utilizing Multiple Testing for Grouping in Singular Spectrum Analysis</title>
      <link>https://arxiv.org/abs/2401.01665</link>
      <description>arXiv:2401.01665v2 Announce Type: replace 
Abstract: A key step in separating signal from noise in time series by means of singular spectrum analysis (SSA) is grouping. We present a multiple testing method for the grouping step in SSA. As separability criterion, we utilize the weighted correlation between the signal and the noise component of the (reconstructed) time series, and we test whether this weighted correlation is equal to zero. This test has to be performed for several possible groupings, resulting in a multiple test problem. The null distributions of the corresponding test statistics are approximated by a wild bootstrap procedure. The performance of our proposed method is assessed in a simulation study, and we illustrate its practical application with an analysis of real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01665v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maryam Movahedifar, Friederike Preusse, Anna Vesely, Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>Nonparametric Causal Survival Analysis with Clustered Interference</title>
      <link>https://arxiv.org/abs/2409.13190</link>
      <description>arXiv:2409.13190v2 Announce Type: replace 
Abstract: Inferring treatment effects on a survival time outcome based on data from an observational study is challenging due to the presence of censoring and possible confounding. An additional challenge occurs when a unit's treatment affects the outcome of other units, i.e., there is interference. In some settings, units may be grouped into clusters such that it is reasonable to assume interference only occurs within clusters, i.e., there is clustered interference. In this paper, methods are developed which can accommodate confounding, censored outcomes, and clustered interference. The approach avoids parametric assumptions and permits inference about counterfactual scenarios corresponding to any stochastic policy which modifies the propensity score distribution, and thus may have application across diverse settings. The proposed nonparametric sample splitting estimators allow for flexible data-adaptive estimation of nuisance functions and are consistent and asymptotically normal with parametric convergence rates. Simulation studies demonstrate the finite sample performance of the proposed estimators, and the methods are applied to a cholera vaccine study in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13190v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chanhwa Lee, Donglin Zeng, Michael Emch, John D. Clemens, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Risk Estimate under a Time-Varying Autoregressive Model for Data-Driven Reproduction Number Estimation</title>
      <link>https://arxiv.org/abs/2409.14937</link>
      <description>arXiv:2409.14937v4 Announce Type: replace 
Abstract: COVID-19 pandemic has brought to the fore epidemiological models which, though describing a wealth of behaviors, have previously received little attention in signal processing literature. In this work, a generalized time-varying autoregressive model is considered, encompassing, but not reducing to, a state-of-the-art model of viral epidemics propagation. The time-varying parameter of this model is estimated via the minimization of a penalized likelihood estimator. A major challenge is that the estimation accuracy strongly depends on hyperparameters fine-tuning. Without available ground truth, hyperparameters are selected by minimizing specifically designed data-driven oracles, used as proxy for the estimation error. Focusing on the time-varying autoregressive Poisson model, Stein's Unbiased Risk Estimate formalism is generalized to construct asymptotically unbiased risk estimators based on the derivation of an original autoregressive counterpart of Stein's lemma. The accuracy of these oracles and of the resulting estimates are assessed through intensive Monte Carlo simulations on synthetic data. Then, elaborating on recent epidemiological models, a novel weekly scaled Poisson model is proposed, better accounting for intrinsic variability of the contaminations while being robust to reporting errors. Finally, the data-driven procedure is particularized to the estimation of COVID-19 reproduction number from weekly infection counts demonstrating its ability to tackle real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14937v4</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Pascal, Samuel Vaiter</dc:creator>
    </item>
    <item>
      <title>Sparse Multivariate Linear Regression with Strongly Associated Response Variables</title>
      <link>https://arxiv.org/abs/2410.10025</link>
      <description>arXiv:2410.10025v3 Announce Type: replace 
Abstract: We propose new methods for multivariate linear regression when the regression coefficient matrix is sparse and the error covariance matrix is dense. We assume that the error covariance matrix has equicorrelation across the response variables. Two procedures are proposed: one is based on constant marginal response variance (compound symmetry), and the other is based on general varying marginal response variance. Two approximate procedures are also developed for high dimensions. We propose an approximation to the Gaussian validation likelihood for tuning parameter selection. Extensive numerical experiments illustrate when our procedures outperform relevant competitors as well as their robustness to a moderate degree of model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10025v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daeyoung Ham, Bradley S. Price, Adam J. Rothman</dc:creator>
    </item>
    <item>
      <title>The Nudge Average Treatment Effect</title>
      <link>https://arxiv.org/abs/2410.23590</link>
      <description>arXiv:2410.23590v3 Announce Type: replace 
Abstract: The instrumental variable method is a prominent approach to recover under certain conditions, valid inference about a treatment causal effect even when unmeasured confounding might be present. In a groundbreaking paper, Imbens and Angrist (1994) established that a valid instrument nonparametrically identifies the average causal effect among compliers, also known as the local average treatment effect under a certain monotonicity assumption which rules out the existence of so-called defiers. An often-cited attractive property of monotonicity is that it facilitates a causal interpretation of the instrumental variable estimand without restricting the degree of heterogeneity of the treatment causal effect. In this paper, we introduce an alternative equally straightforward and interpretable condition for identification, which accommodates both the presence of defiers and heterogenous treatment effects. Mainly, we show that under our new conditions, the instrumental variable estimand recovers the average causal effect for the subgroup of units for whom the treatment is manipulable by the instrument, a subgroup which may consist of both defiers and compliers, therefore recovering an effect estimand we aptly call the Nudge Average Treatment Effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23590v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric J Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding</title>
      <link>https://arxiv.org/abs/2504.20360</link>
      <description>arXiv:2504.20360v4 Announce Type: replace 
Abstract: The test-negative design (TND) is frequently used to evaluate vaccine effectiveness in real-world settings. In a TND study, individuals with similar symptoms who seek care are tested for the disease of interest, and vaccine effectiveness is estimated by comparing the vaccination history of test-positive cases and test-negative controls. The design has previously been justified on the grounds that it reduces confounding due to unmeasured health-seeking behavior, although this has not been formally described using potential outcomes. However, it is also widely acknowledged that, by conditioning participation on receipt of a test, the TND risks inducing selection bias. In this paper, we propose a formal justification for the TND based on the assumption of odds ratio equi-confounding, where unmeasured confounders influence test-positive and test-negative individuals equivalently on the odds ratio scale, with health-seeking behavior being just one plausible example. We also show that these results hold under outcome-dependent sampling design of the TND. We discuss the implications of the equi-confounding assumption for TND design and provide alternative estimators for the marginal risk ratio among the vaccinated under equi-confounding, including estimators based on outcome modeling and inverse probability weighting as well as a semiparametric estimator that is doubly-robust. When the equi-confounding assumption does not hold, we suggest a straightforward sensitivity analysis that parameterizes the magnitude of the deviation on the odds ratio scale. We conduct a simulation study to evaluate the empirical performance of our proposed estimators under a wide range of scenarios. Finally, we also discuss how test-negative outcomes may be used more broadly to de-bias estimates from cohort studies where testing is symptom-triggered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20360v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Kendrick Qijun Li, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>High-dimensional Longitudinal Inference via a De-sparsified Dantzig-Selector</title>
      <link>https://arxiv.org/abs/2508.07498</link>
      <description>arXiv:2508.07498v2 Announce Type: replace 
Abstract: In this paper, we consider statistical inference with generalized linear models in high dimensions under a longitudinal clustered data framework. Specifically, we propose a de-sparsified version of an initial Dantzig-type regularized estimator in regression settings and provide theoretical justification for both linear and generalized linear models. We present extensive numerical simulations demonstrating the effectiveness of our method for continuous and binary data. For continuous outcomes under linear models, we show that our estimator asymptotically attains an appropriate efficiency bound when the correlation structure is correctly specified. We conclude with an application of our method to a well-established genetics dataset, with bacterial riboflavin production as the outcome of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huey</dc:creator>
    </item>
    <item>
      <title>Selecting the number of components in PCA via random signflips</title>
      <link>https://arxiv.org/abs/2012.02985</link>
      <description>arXiv:2012.02985v4 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a foundational tool in modern data analysis, and a crucial step in PCA is selecting the number of components to keep. However, classical selection methods (e.g., scree plots, parallel analysis, etc.) lack statistical guarantees in the increasingly common setting of large-dimensional data with heterogeneous noise, i.e., where each entry may have a different noise variance. Moreover, it turns out that these methods, which are highly effective for homogeneous noise, can fail dramatically for data with heterogeneous noise. This paper proposes a new method called signflip parallel analysis (FlipPA) for the setting of approximately symmetric noise: it compares the data singular values to those of "empirical null" matrices generated by flipping the sign of each entry randomly with probability one-half. We develop a rigorous theory for FlipPA, showing that it has nonasymptotic type I error control and that it consistently selects the correct rank for signals rising above the noise floor in the large-dimensional limit (even when the noise is heterogeneous). We also rigorously explain why classical permutation-based parallel analysis degrades under heterogeneous noise. Finally, we illustrate that FlipPA compares favorably to state-of-the art methods via numerical simulations and an illustration on data coming from astronomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.02985v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hong, Yue Sheng, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Tracing the impacts of Mount Pinatubo eruption on regional climate using spatially-varying changepoint detection</title>
      <link>https://arxiv.org/abs/2409.08908</link>
      <description>arXiv:2409.08908v2 Announce Type: replace-cross 
Abstract: Significant events, such as volcanic eruptions, can have global and long-lasting impacts on climate. These global impacts, however, are not uniform across space and time. Understanding how the Mt. Pinatubo eruption affects global and regional climate is of great interest for predicting the impact on climate due to similar events as well as understanding the possible effect of the stratospheric aerosol injections proposed to combat climate change. While many studies illustrated the impact of the Pinatubo eruption on a global scale, studies at a fine regional scale are scarce. We propose a Bayesian spatially-varying changepoint detection and estimation method to trace the impact of Mt. Pinatubo eruption on regional climate. Our approach takes into account the diffusing nature and spatial correlation of the climate changes attributed to the volcanic eruption. We illustrate our method and demonstrate its advantages over an existing changepoint detection method through simulations. Finally, we apply our method to monthly stratospheric aerosol optical depth and surface temperature data from 1985 to 1995 to detect and estimate changepoints following the 1991 Mt. Pinatubo eruption. Our results quantitatively characterize the spatial pattern of the eruption's impact on regional climate, complementing the previous studies on the global impact of the Pinatubo eruption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08908v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1968</arxiv:DOI>
      <dc:creator>Samantha Shi-Jun, Lyndsay Shand, Bo Li</dc:creator>
    </item>
  </channel>
</rss>
