<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Apr 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure</title>
      <link>https://arxiv.org/abs/2404.01347</link>
      <description>arXiv:2404.01347v1 Announce Type: new 
Abstract: In this uncertain world, data uncertainty is inherent in many applications and its importance is growing drastically due to the rapid development of modern technologies. Nowadays, researchers have paid more attention to mine patterns in uncertain databases. A few recent works attempt to mine frequent uncertain sequential patterns. Despite their success, they are incompetent to reduce the number of false-positive pattern generation in their mining process and maintain the patterns efficiently. In this paper, we propose multiple theoretically tightened pruning upper bounds that remarkably reduce the mining space. A novel hierarchical structure is introduced to maintain the patterns in a space-efficient way. Afterward, we develop a versatile framework for mining uncertain sequential patterns that can effectively handle weight constraints as well. Besides, with the advent of incremental uncertain databases, existing works are not scalable. There exist several incremental sequential pattern mining algorithms, but they are limited to mine in precise databases. Therefore, we propose a new technique to adapt our framework to mine patterns when the database is incremental. Finally, we conduct extensive experiments on several real-life datasets and show the efficacy of our framework in different applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01347v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kashob Kumar Roy, Md Hasibul Haque Moon, Md Mahmudur Rahman, Chowdhury Farhan Ahmed, Carson K. Leung</dc:creator>
    </item>
    <item>
      <title>FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets</title>
      <link>https://arxiv.org/abs/2404.01585</link>
      <description>arXiv:2404.01585v1 Announce Type: new 
Abstract: Frequent Subgraph Mining (FSM) is the process of identifying common subgraph patterns that surpass a predefined frequency threshold. While FSM is widely applicable in fields like bioinformatics, chemical analysis, and social network anomaly detection, its execution remains time-consuming and complex. This complexity stems from the need to recognize high-frequency subgraphs and ascertain if they exceed the set threshold. Current approaches to identifying these patterns often rely on edge or vertex extension methods. However, these strategies can introduce redundancies and cause increased latency. To address these challenges, this paper introduces a novel approach for identifying potential k-vertex patterns by combining two frequently observed (k - 1)-vertex patterns. This method optimizes the breadth-]first search, which allows for quicker search termination based on vertices count and support value. Another challenge in FSM is the validation of the presumed pattern against a specific threshold. Existing metrics, such as Maximum Independent Set (MIS) and Minimum Node Image (MNI), either demand significant computational time or risk overestimating pattern counts. Our innovative approach aligns with the MIS and identifies independent subgraphs. Through the "Maximal Independent Set" metric, this paper offers an efficient solution that minimizes latency and provides users with control over pattern overlap. Through extensive experimentation, our proposed method achieves an average of 10.58x speedup when compared to GraMi and an average 3x speedup when compared to T-FSM</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01585v1</guid>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akshit Sharma, Sam Reinher, Dinesh Mehta, Bo Wu</dc:creator>
    </item>
    <item>
      <title>Practical Persistent Multi-Word Compare-and-Swap Algorithms for Many-Core CPUs</title>
      <link>https://arxiv.org/abs/2404.01710</link>
      <description>arXiv:2404.01710v1 Announce Type: new 
Abstract: In the last decade, academic and industrial researchers have focused on persistent memory because of the development of the first practical product, Intel Optane. One of the main challenges of persistent memory programming is to guarantee consistent durability over separate memory addresses, and Wang et al. proposed a persistent multi-word compare-and-swap (PMwCAS) algorithm to solve this problem. However, their algorithm contains redundant compare-and-swap (CAS) and cache flush instructions and does not achieve sufficient performance on many-core CPUs. This paper proposes a new algorithm to improve performance on many-core CPUs by removing useless CAS/flush instructions from PMwCAS operations. We also exclude dirty flags, which help ensure consistent durability in the original algorithm, from our algorithm using PMwCAS descriptors as write-ahead logs. Experimental results show that the proposed method is up to ten times faster than the original algorithm and suggests several productive uses of PMwCAS operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01710v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kento Sugiura, Manabu Nishimura, Yoshiharu Ishikawa</dc:creator>
    </item>
    <item>
      <title>A drug classification pipeline for Medicaid claims using RxNorm</title>
      <link>https://arxiv.org/abs/2404.01514</link>
      <description>arXiv:2404.01514v1 Announce Type: cross 
Abstract: Objective: Freely preprocess drug codes recorded in electronic health records and insurance claims to drug classes that may then be used in biomedical research.
  Materials and Methods: We developed a drug classification pipeline for linking National Drug Codes to the World Health Organization Anatomical Therapeutic Chemical classification. To implement our solution, we created an R package interface to the National Library of Medicine's RxNorm API.
  Results: Using the classification pipeline, 59.4% of all unique NDC were linked to an ATC, resulting in 95.5% of all claims being successfully linked to a drug classification. We identified 12,004 unique NDC codes that were classified as being an opioid or non-opioid prescription for treating pain.
  Discussion: Our proposed pipeline performed similarly well to other NDC classification routines using commercial databases. A check of a small, random sample of non-active NDC found the pipeline to be accurate for classifying these codes.
  Conclusion: The RxNorm NDC classification pipeline is a practical and reliable tool for categorizing drugs in large-scale administrative claims data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01514v1</guid>
      <category>q-bio.QM</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Williams, Kara E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Optimizing Distributed Protocols with Query Rewrites [Technical Report]</title>
      <link>https://arxiv.org/abs/2404.01593</link>
      <description>arXiv:2404.01593v1 Announce Type: cross 
Abstract: Distributed protocols such as 2PC and Paxos lie at the core of many systems in the cloud, but standard implementations do not scale. New scalable distributed protocols are developed through careful analysis and rewrites, but this process is ad hoc and error-prone. This paper presents an approach for scaling any distributed protocol by applying rule-driven rewrites, borrowing from query optimization. Distributed protocol rewrites entail a new burden: reasoning about spatiotemporal correctness. We leverage order-insensitivity and data dependency analysis to systematically identify correct coordination-free scaling opportunities. We apply this analysis to create preconditions and mechanisms for coordination-free decoupling and partitioning, two fundamental vertical and horizontal scaling techniques. Manual rule-driven applications of decoupling and partitioning improve the throughput of 2PC by $5\times$ and Paxos by $3\times$, and match state-of-the-art throughput in recent work. These results point the way toward automated optimizers for distributed protocols based on correct-by-construction rewrite rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01593v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chu, Rithvik Panchapakesan, Shadaj Laddad, Lucky Katahanas, Chris Liu, Kaushik Shivakumar, Natacha Crooks, Joseph M. Hellerstein, Heidi Howard</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies</title>
      <link>https://arxiv.org/abs/2403.06749</link>
      <description>arXiv:2403.06749v2 Announce Type: replace 
Abstract: Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06749v2</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Berti, Humam Kourani, Hannes Hafke, Chiao-Yun Li, Daniel Schuster</dc:creator>
    </item>
    <item>
      <title>A proof-of-concept online metadata catalogue service of Earth observation datasets for human health research in exposomics</title>
      <link>https://arxiv.org/abs/2311.08770</link>
      <description>arXiv:2311.08770v3 Announce Type: replace-cross 
Abstract: This article describes research carried out during 2023 under an International Society for Photogrammetry and Remote Sensing (ISPRS)-funded project to develop and disseminate a metadata catalogue of Earth observation data sources/products and types that are relevant to human health research in exposomics, as a free service to interested researchers worldwide. The proof-of-concept catalogue was informed by input from existing research literature on the subject (desk research), as well as online communications with, and relevant research publications collected from, a small panel (n = 5) of select experts from the academia in three countries (China, UK and USA). It has 90 metadata records of relevant Earth observation datasets (n = 40) and associated health-focused research publications (n = 50). The project's online portal offers a searchable version of the catalogue featuring a number of search modes and filtering options. It is hoped future, more comprehensive versions of this service will enable more researchers and studies to discover and use remote sensing data about population-level exposures to disease determinants (exposomic determinants of disease) in combination with other relevant data to reveal fresh insights that could improve our understanding of relevant diseases, and hence contribute to the development of better-optimized prevention and management plans to tackle them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08770v3</guid>
      <category>cs.DL</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Keumseok Koh, Maged N. Kamel Boulos, Gang Zheng, Hongsheng Zhang, Muralikrishna V. Iyyanki, Bosco Bwambale, Ashraf Dewan</dc:creator>
    </item>
  </channel>
</rss>
