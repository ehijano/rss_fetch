<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2024 05:03:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent</title>
      <link>https://arxiv.org/abs/2412.18174</link>
      <description>arXiv:2412.18174v1 Announce Type: new 
Abstract: Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18174v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow</dc:creator>
    </item>
    <item>
      <title>JANUS: A Stablecoin 3.0 Blueprint for Navigating the Stablecoin Trilemma Through Dual-Token Design, Multi-Collateralization, Soft Peg, and AI-Driven Stabilization</title>
      <link>https://arxiv.org/abs/2412.18182</link>
      <description>arXiv:2412.18182v1 Announce Type: new 
Abstract: This paper introduces JANUS, a Stablecoin 3.0 protocol designed to address the stablecoin trilemma--simultaneously improving decentralization (D), capital efficiency (E), and safety-stability (S). Building upon insights from previous stablecoin generations, JANUS leverages a dual-token system (Alpha and Omega), integrates crypto-assets and real-world assets (RWAs), employs a soft-peg mechanism, and utilizes AI-driven stabilization.
  We provide a comprehensive theoretical framework, including formal definitions of D, E, and S, along with equilibrium existence proofs and analogies drawn from international trade and open-economy macroeconomics. By introducing a second token backed by external yield, JANUS breaks from ponzinomic dynamics and creates a more robust foundation. Multi-collateralization and a soft peg enable controlled price oscillations, while AI-driven parameter adjustments maintain equilibrium.
  Through these innovations, JANUS aims to approach the center of the stablecoin trilemma, offering a globally resilient, inflation-adjusted, and decentralized stablecoin ecosystem bridging DeFi and TradFi. The main body presents a high-level overview of the trilemma and JANUS's key features, while the Appendix provides more formal mathematical treatments, including rigorous metrics for decentralization, capital efficiency, and stability, as well as the optimization challenges inherent in the trilemma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18182v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stylianos Kampakis</dc:creator>
    </item>
    <item>
      <title>Learning Generalized Residual Exchange-Correlation-Uncertain Functional for Density Functional Theory</title>
      <link>https://arxiv.org/abs/2412.18350</link>
      <description>arXiv:2412.18350v1 Announce Type: new 
Abstract: Density Functional Theory (DFT) stands as a widely used and efficient approach for addressing the many-electron Schr\"odinger equation across various domains such as physics, chemistry, and biology. However, a core challenge that persists over the long term pertains to refining the exchange-correlation (XC) approximation. This approximation significantly influences the triumphs and shortcomings observed in DFT applications. Nonetheless, a prevalent issue among XC approximations is the presence of systematic errors, stemming from deviations from the mathematical properties of the exact XC functional. For example, although both B3LYP and DM21 (DeepMind 21) exhibit improvements over previous benchmarks, there is still potential for further refinement. In this paper, we propose a strategy for enhancing XC approximations by estimating the neural uncertainty of the XC functional, named Residual XC-Uncertain Functional. Specifically, our approach involves training a neural network to predict both the mean and variance of the XC functional, treating it as a Gaussian distribution. To ensure stability in each sampling point, we construct the mean by combining traditional XC approximations with our neural predictions, mitigating the risk of divergence or vanishing values. It is crucial to highlight that our methodology excels particularly in cases where systematic errors are pronounced. Empirical outcomes from three benchmark tests substantiate the superiority of our approach over existing state-of-the-art methods. Our approach not only surpasses related techniques but also significantly outperforms both the popular B3LYP and the recent DM21 methods, achieving average RMSE improvements of 62\% and 37\%, respectively, across the three benchmarks: W4-17, G21EA, and G21IP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18350v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sizhuo Jin, Shuo Chen, Jianjun Qian, Ying Tai, Jun Li</dc:creator>
    </item>
    <item>
      <title>Normalized field product approach: A parameter-free density evaluation method for close-to-binary solutions in topology optimization with embedded length scale</title>
      <link>https://arxiv.org/abs/2412.18441</link>
      <description>arXiv:2412.18441v1 Announce Type: new 
Abstract: This paper provides a normalized field product approach for topology optimization to achieve close-to-binary optimal designs. The method employs a parameter-free density measure that implicitly enforces a minimum length scale on the solid phase, allowing for smooth and transition-free topologies. The density evaluation does not rely on weight functions; however, the related density functions must have values between 0 and 1. The method combines the SIMP scheme and the introduced density function for material stiffness interpolation. The success and efficacy of the approach are demonstrated for designing both two- and three-dimensional designs, encompassing stiff structures and compliant mechanisms. The structure's compliance is minimized for the former, while the latter involves optimizing a multi-criteria objective. Numerical examples consider different volume fractions, length scales, and density functions. A volume-preserving smoothing and resolution scheme is implemented to achieve serrated-free boundaries. The proposed method is also seamlessly extended with advanced elements for solving 3D problems. The optimized designs obtained are close to binary without any user intervention while satisfying the desired feature size on the solid phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18441v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikhil Singh, Prabhat Kumar, Anupam Saxena</dc:creator>
    </item>
    <item>
      <title>Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2412.17908</link>
      <description>arXiv:2412.17908v1 Announce Type: cross 
Abstract: With the rapid development of generative artificial intelligence, particularly large language models, a number of sub-fields of deep learning have made significant progress and are now very useful in everyday applications. For example, well-known financial institutions simulate a wide range of scenarios for various models created by their research teams using reinforcement learning, both before production and after regular operations. In this work, we propose a backdoor attack that focuses solely on data poisoning. This particular backdoor attack is classified as an attack without prior consideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to examine the potential effects of large language models that use reinforcement learning systems for text production or speech recognition, finance, physics, or the ecosystem of contemporary artificial intelligence models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17908v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Orson Mengara</dc:creator>
    </item>
    <item>
      <title>Time-Probability Dependent Knowledge Extraction in IoT-enabled Smart Building</title>
      <link>https://arxiv.org/abs/2412.18042</link>
      <description>arXiv:2412.18042v1 Announce Type: cross 
Abstract: Smart buildings incorporate various emerging Internet of Things (IoT) applications for comprehensive management of energy efficiency, human comfort, automation, and security. However, the development of a knowledge extraction framework is fundamental. Currently, there is a lack of a unified and practical framework for modeling heterogeneous sensor data within buildings. In this paper, we propose a practical inference framework for extracting status-to-event knowledge within smart building. Our proposal includes IoT-based API integration, ontology model design, and time probability dependent knowledge extraction methods. The Building Topology Ontology (BOT) was leveraged to construct spatial relations among sensors and spaces within the building. We utilized Apache Jena Fuseki's SPARQL server for storing and querying the RDF triple data. Two types of knowledge could be extracted: timestamp-based probability for abnormal event detection and time interval-based probability for conjunction of multiple events. We conducted experiments (over a 78-day period) in a real smart building environment. The data of light and elevator states has been collected for evaluation. The evaluation revealed several inferred events, such as room occupancy, elevator trajectory tracking, and the conjunction of both events. The numerical values of detected event counts and probability demonstrate the potential for automatic control in the smart building.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18042v1</guid>
      <category>cs.IR</category>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hangli Ge, Hirotsugu Seike, Noboru Koshizuka</dc:creator>
    </item>
    <item>
      <title>Efficient Aircraft Design Optimization Using Multi-Fidelity Models and Multi-fidelity Physics Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2412.18564</link>
      <description>arXiv:2412.18564v1 Announce Type: cross 
Abstract: Aircraft design optimization traditionally relies on computationally expensive simulation techniques such as Finite Element Method (FEM) and Finite Volume Method (FVM), which, while accurate, can significantly slow down the design iteration process. The challenge lies in reducing the computational complexity while maintaining high accuracy for quick evaluations of multiple design alternatives. This research explores advanced methods, including surrogate models, reduced-order models (ROM), and multi-fidelity machine learning techniques, to achieve more efficient aircraft design evaluations. Specifically, the study investigates the application of Multi-fidelity Physics-Informed Neural Networks (MPINN) and autoencoders for manifold alignment, alongside the potential of Generative Adversarial Networks (GANs) for refining design geometries. Through a proof-of-concept task, the research demonstrates the ability to predict high-fidelity results from low-fidelity simulations, offering a path toward faster and more cost effective aircraft design iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18564v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Apurba Sarker</dc:creator>
    </item>
    <item>
      <title>GRATEV2.0: Computational Tools for Real-time Analysis of High-throughput High-resolution TEM (HRTEM) Images of Conjugated Polymers</title>
      <link>https://arxiv.org/abs/2411.03474</link>
      <description>arXiv:2411.03474v3 Announce Type: replace 
Abstract: Automated analysis of high-resolution transmission electron microscopy (HRTEM) images is increasingly essential for advancing research in organic electronics, where precise characterization of nanoscale crystal structures is crucial for optimizing material properties. This paper introduces an open-source computational framework called GRATEV2.0 (GRaph-based Analysis of TEM), designed for real-time analysis of HRTEM data, with a focus on characterizing complex microstructures in conjugated polymers, illustrated using Poly[N-9'-heptadecanyl-2,7-carbazole-alt-5,5-(4',7'-di-2-thienyl-2',1',3'-benzothiadiazole)] (PCDTBT), a key material in organic photovoltaics. GRATEV2.0 employs fast, automated image processing algorithms, enabling rapid extraction of structural features like d-spacing, orientation, and crystal shape metrics. Gaussian process optimization rapidly identifies the user-defined parameters in the approach, reducing the need for manual parameter tuning and thus enhancing reproducibility and usability. Additionally, GRATEV2.0 is compatible with high-performance computing (HPC) environments, allowing for efficient, large-scale data processing at near real-time speeds. A unique feature of GRATEV2.0 is a Wasserstein distance-based stopping criterion, which optimizes data collection by determining when further sampling no longer adds statistically significant information. This capability optimizes the amount of time the TEM facility is used while ensuring data adequacy for in-depth analysis. Open-source and tested on a substantial PCDTBT dataset, this tool offers a powerful, robust, and accessible solution for high-throughput material characterization in organic electronics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03474v3</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Gamdha, Ryan Fair, Adarsh Krishnamurthy, Enrique Gomez, Baskar Ganapathysubramanian</dc:creator>
    </item>
    <item>
      <title>A Layered Swarm Optimization Method for Fitting Battery Thermal Runaway Models to Accelerating Rate Calorimetry Data</title>
      <link>https://arxiv.org/abs/2412.16367</link>
      <description>arXiv:2412.16367v2 Announce Type: replace 
Abstract: Thermal runaway in lithium ion batteries is a critical safety concern for the battery industry due to its potential to cause uncontrolled temperature rises and subsequent fires that can engulf the battery pack and its surroundings. Modeling and simulation offer cost effective tools for designing strategies to mitigate thermal runaway. Accurately simulating the chemical kinetics of thermal runaway,commonly represented by systems of Arrhenius based Ordinary Differential Equations (ODEs), requires fitting kinetic parameters to experimental calorimetry data, such as Accelerating Rate Calorimetry (ARC) measurements. However, existing fitting methods often rely on empirical assumptions and simplifications that compromise generality or require manual tuning during the fitting process. Particle Swarm Optimization (PSO) offers a promising approach for directly fitting kinetic parameters to experimental data. Yet, for systems involving large search spaces, such as those created by multiple Arrhenius ODEs, the computational cost of fitting can become prohibitive. This work introduces a divide and conquer approach based on PSO to fit N equation Arrhenius ODE models to ARC data. The proposed method achieves accurate parameter fitting while maintaining low computational costs. The resulting fit is analyzed using two distinct ARC datasets, highlighting the methods flexibility. The resulting models are further validated through simulations of 3D ARC and oven tests, showing excellent agreement with experimental data and alignment with expected trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16367v2</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saakaar Bhatnagar, Andrew Comerford, Zelu Xu, Simone Reitano, Luigi Scrimieri, Luca Giuliano, Araz Banaeizadeh</dc:creator>
    </item>
    <item>
      <title>Portability of Fortran's `do concurrent' on GPUs</title>
      <link>https://arxiv.org/abs/2408.07843</link>
      <description>arXiv:2408.07843v2 Announce Type: replace-cross 
Abstract: There is a continuing interest in using standard language constructs for accelerated computing in order to avoid (sometimes vendor-specific) external APIs. For Fortran codes, the {\tt do concurrent} (DC) loop has been successfully demonstrated on the NVIDIA platform. However, support for DC on other platforms has taken longer to implement. Recently, Intel has added DC GPU offload support to its compiler, as has HPE for AMD GPUs. In this paper, we explore the current portability of using DC across GPU vendors using the in-production solar surface flux evolution code, HipFT. We discuss implementation and compilation details, including when/where using directive APIs for data movement is needed/desired compared to using a unified memory system. The performance achieved on both data center and consumer platforms is shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07843v2</guid>
      <category>cs.PL</category>
      <category>astro-ph.SR</category>
      <category>cs.CE</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SCW63240.2024.00240</arxiv:DOI>
      <dc:creator>Ronald M. Caplan, Miko M. Stulajter, Jon A. Linker, Jeff Larkin, Henry A. Gabb, Shiquan Su, Ivan Rodriguez, Zachary Tschirhart, Nicholas Malaya</dc:creator>
    </item>
  </channel>
</rss>
