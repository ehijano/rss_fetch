<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in Microstructure-Properties of Ceramic Aerogels</title>
      <link>https://arxiv.org/abs/2501.13255</link>
      <description>arXiv:2501.13255v1 Announce Type: new 
Abstract: Deep learning surrogate models have become pivotal in enabling model-driven materials discovery to achieve exceptional properties. However, ensuring the accuracy and reliability of predictions from these models, trained on limited and sparse material datasets remains a significant challenge. This study introduces an integrated deep learning framework for predicting the synthesis, microstructure, and mechanical properties of ceramic aerogels, leveraging physics-based models such as Lattice Boltzmann simulations for microstructure formation and stochastic finite element methods for mechanical property calculations. To address the computational demands of repeated physics-based simulations required for experimental calibration and material design, a linked surrogate model is developed, leveraging Convolutional Neural Networks (CNNs) for stochastic microstructure generation and microstructure-to-mechanical property mapping. To overcome challenges associated with limited training datasets from expensive physical modeling, CNN training is formulated within a Bayesian inference framework, enabling robust uncertainty quantification in predictions. Numerical results highlight the strengths and limitations of the linked surrogate framework, demonstrating its effectiveness in predicting properties of aerogels with pore sizes and morphologies similar to the training data (in-distribution) and its ability to interpolate to new microstructural features between training data (out-of-distribution).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13255v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Azharul Islama, Dwyer Deighan, Shayan Bhattacharjee, Daniel Tantalo, Pratyush Kumar Singh, David Salac, Danial Faghihi</dc:creator>
    </item>
    <item>
      <title>Generative Multi-Form Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2501.13337</link>
      <description>arXiv:2501.13337v1 Announce Type: new 
Abstract: Many real-world problems, such as airfoil design, involve optimizing a black-box expensive objective function over complex structured input space (e.g., discrete space or non-Euclidean space). By mapping the complex structured input space into a latent space of dozens of variables, a two-stage procedure labeled as generative model based optimization (GMO) in this paper, shows promise in solving such problems. However, the latent dimension of GMO is hard to determine, which may trigger the conflicting issue between desirable solution accuracy and convergence rate. To address the above issue, we propose a multi-form GMO approach, namely generative multi-form optimization (GMFoO), which conducts optimization over multiple latent spaces simultaneously to complement each other. More specifically, we devise a generative model which promotes positive correlation between latent spaces to facilitate effective knowledge transfer in GMFoO. And further, by using Bayesian optimization (BO) as the optimizer, we propose two strategies to exchange information between these latent spaces continuously. Experimental results are presented on airfoil and corbel design problems and an area maximization problem as well to demonstrate that our proposed GMFoO converges to better designs on a limited computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13337v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TCYB.2022.3165044</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Transactions on Cybernetics, vol. 53, no. 7, pp. 4347-4360, July 2023</arxiv:journal_reference>
      <dc:creator>Zhendong Guo, Haitao Liu, Yew-Soon Ong, Xinghua Qu, Yuzhe Zhang, Jianmin Zheng</dc:creator>
    </item>
    <item>
      <title>Concentration in Governance Control Across Decentralised Finance Protocols</title>
      <link>https://arxiv.org/abs/2501.13377</link>
      <description>arXiv:2501.13377v1 Announce Type: new 
Abstract: Blockchain-based systems are frequently governed through tokens that grant their holders voting rights over core protocol functions and funds. The centralisation occurring in Decentralised Finance (DeFi) protocols' token-based voting systems is typically analysed by examining token holdings' distribution across addresses. In this paper, we expand this perspective by exploring shared token holdings of addresses across multiple DeFi protocols. We construct a Statistically Validated Network (SVN) based on shared governance token holdings among addresses. Using the links within the SVN, we identify influential addresses that shape these connections and we conduct a post-hoc analysis to examine their characteristics and behaviour. Our findings reveal persistent influential links over time, predominantly involving addresses associated with institutional investors who maintain significant token supplies across the sampled protocols. Finally, we observe that token holding patterns and concentrations tend to shift in response to speculative market cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13377v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Eisermann, Carlo Campajola, Claudio J. Tessone, Andreia Sofia Teixeira</dc:creator>
    </item>
    <item>
      <title>ARCADE: An interactive playground for real-time immersed topology optimization</title>
      <link>https://arxiv.org/abs/2501.13564</link>
      <description>arXiv:2501.13564v1 Announce Type: new 
Abstract: Topology optimization (TO) has found applications across a wide range of disciplines but remains underutilized in practice. Key barriers to broader adoption include the absence of versatile commercial software, the need for specialized expertise, and high computational demands. Additionally, challenges such as ensuring manufacturability, optimizing hyper-parameters, and integrating subjective design elements like aesthetics further hinder its widespread use.
  Emerging technologies like augmented reality (AR) and virtual reality (VR) offer transformative potential for TO. By enabling intuitive, gesture-based human-computer interactions, these immersive tools bridge the gap between human intuition and computational processes. They provide the means to integrate subjective human judgment into optimization workflows in real time, creating a paradigm shift toward interactive and immersive design.
  Here we introduce the concept of immersive topology optimization (ITO) as a novel design paradigm that leverages AR environments for TO. To demonstrate this concept, we present ARCADE: Augmented Reality Computational Analysis and Design Environment. Developed in Swift for the Apple Vision Pro mixed reality headset, ARCADE enables users to define, manipulate, and solve structural optimization problems within an augmented reality setting. By incorporating real-time human interaction and visualization of the design in its intended target location, ARCADE has the potential to reduce lead times, enhance manufacturability, and improve design integration. Although initially developed for structural optimization, ARCADE's framework could be extended to other disciplines, paving the way for a new era of interactive and immersive computational design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13564v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro M. Arag\'on, Hendrik J. Algra</dc:creator>
    </item>
    <item>
      <title>Analysis of Eccentric Coaxial Waveguides Filled with Lossy Anisotropic Media via Finite Difference</title>
      <link>https://arxiv.org/abs/2501.13706</link>
      <description>arXiv:2501.13706v1 Announce Type: new 
Abstract: This study presents a finite difference method (FDM) to model the electromagnetic field propagation in eccentric coaxial waveguides filled with lossy uniaxially anisotropic media. The formulation utilizes conformal transformation to map the eccentric circular waveguide into an equivalent concentric one. In the concentric problem, we introduce a novel normalized Helmholtz equation to decouple TM and TE modes, and we solve this non-homogeneous partial differential equation using the finite difference in cylindrical coordinates. The proposed approach was validated against perturbation-based, spectral element-based, and finite-integration-based numerical solutions. The preliminary results show that our solution is superior in computational time. Furthermore, our FDM formulation can be extended with minimal adaptations to model complex media problems, such as metamaterial devices, optical fibers, and geophysical exploration sensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13706v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raul O. Ribeiro, Maria A. Martinez, Guilherme S. Rosa, Rafael A. Penchel</dc:creator>
    </item>
    <item>
      <title>The Lock Generative Adversarial Network for Medical Waveform Anomaly Detection</title>
      <link>https://arxiv.org/abs/2501.13858</link>
      <description>arXiv:2501.13858v1 Announce Type: new 
Abstract: Waveform signal analysis is a complex and important task in medical care. For example, mechanical ventilators are critical life-support machines, but they can cause serious injury to patients if they are out of synchronization with the patients' own breathing reflex. This asynchrony is revealed by the waveforms showing flow and pressure histories. Likewise, electrocardiograms record the electrical activity of a patients' heart as a set of waveforms, and anomalous waveforms can reveal important disease states. In both cases, subtle variations in a complex waveform are important information for patient care; signals which may be missed or mis-interpreted by human caregivers.
  We report on the design of a novel Lock Generative Adversarial Network architecture for anomaly detection in raw or summarized medical waveform data. The proposed architecture uses alternating optimization of the generator and discriminator networks to solve the convergence dilemma. Furthermore, the fidelity of the generator networks' outputs to the actual distribution of anomalous data is improved via synthetic minority oversampling. We evaluate this new architecture on one ventilator asynchrony dataset, and two electrocardiogram datasets, finding that the performance was either equal or superior to the state-of-the art on all three.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13858v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Xu, Scott Dick</dc:creator>
    </item>
    <item>
      <title>Lost in Siting: The Hidden Carbon Cost of Inequitable Residential Solar Installations</title>
      <link>https://arxiv.org/abs/2501.13868</link>
      <description>arXiv:2501.13868v1 Announce Type: new 
Abstract: The declining cost of solar photovoltaics (PV) combined with strong federal and state-level incentives have resulted in a high number of residential solar PV installations in the US. However, these installations are concentrated in particular regions, such as California, and demographics, such as high-income Asian neighborhoods. This inequitable distribution creates an illusion that further increasing residential solar installations will become increasingly challenging. Furthermore, while the inequity in solar installations has received attention, no prior comprehensive work has been done on understanding whether our current trajectory of residential solar adoption is energy- and carbon-efficient. In this paper, we reveal the hidden energy and carbon cost of the inequitable distribution of existing installations. Using US-based data on carbon offset potential, the amount of avoided carbon emissions from using rooftop PV instead of electric grid energy, and the number of existing solar installations, we surprisingly observe that locations and demographics with a higher carbon offset potential have fewer existing installations. For instance, neighborhoods with relatively higher black population have 7.4% higher carbon offset potential than average but 36.7% fewer installations; lower-income neighborhoods have 14.7% higher potential and 47% fewer installations. We propose several equity- and carbon-aware solar siting strategies. In evaluating these strategies, we develop Sunsight, a toolkit that combines simulation/visualization tools and our relevant datasets, which we are releasing publicly. Our projections show that a multi-objective siting strategy can address two problems at once; namely, it can improve societal outcomes in terms of distributional equity and simultaneously improve the carbon-efficiency (i.e., climate impact) of current installation trends by up to 39.8%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13868v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cooper Sigrist, Adam Lechowicz, Jovan Champ, Noman Bashir, Mohammad Hajiesmaili</dc:creator>
    </item>
    <item>
      <title>ProtChatGPT: Towards Understanding Proteins with Large Language Models</title>
      <link>https://arxiv.org/abs/2402.09649</link>
      <description>arXiv:2402.09649v2 Announce Type: replace 
Abstract: Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09649v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang</dc:creator>
    </item>
    <item>
      <title>A novel design update framework for topology optimization with quantum annealing: Application to truss and continuum structures</title>
      <link>https://arxiv.org/abs/2406.18833</link>
      <description>arXiv:2406.18833v2 Announce Type: replace 
Abstract: This paper presents a novel design update strategy for topology optimization, as an iterative optimization. The key contribution lies in incorporating a design updater concept with quantum annealing, applicable to both truss and continuum structures. To align with density-based approaches in topology optimization, these updaters are formulated through a multiplicative relationship to represent the design material and serve as design variables. Specifically, structural analysis is conducted on a classical computer using the finite element method, while quantum annealing is utilized for topology updates. The primary objective of the framework is to minimize compliance under a volume constraint. An encoding formulation for the design variables is derived, and the penalty method along with a slack variable is employed to transform the inequality volume constraint. Subsequently, the optimization problem for determining the updater is formulated as a Quadratic Unconstrained Binary Optimization (QUBO) model. To demonstrate its performance, the developed design framework is tested on different computing platforms to perform design optimization for truss structures, as well as 2D and 3D continuum structures. Numerical results indicate that the proposed framework successfully finds optimal topologies similar to benchmark results. Furthermore, the results show the advantage of reduced time in finding an optimal design using quantum annealing compared to simulated annealing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18833v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>quant-ph</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2025.117746</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering Volume 437, 15 March 2025, 117746</arxiv:journal_reference>
      <dc:creator>Naruethep Sukulthanasorn, Junsen Xiao, Koya Wagatsuma, Reika Nomura, Shuji Moriguchi, Kenjiro Terada</dc:creator>
    </item>
    <item>
      <title>HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer</title>
      <link>https://arxiv.org/abs/2408.01271</link>
      <description>arXiv:2408.01271v4 Announce Type: replace 
Abstract: In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful "sentences" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01271v4</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyan Xu, Rundong Wang, Chen Li, Yonghong Hu, Zhonghua Lu</dc:creator>
    </item>
    <item>
      <title>Foil Conductor Model for Efficient Simulation of HTS Coils in Large Scale Applications</title>
      <link>https://arxiv.org/abs/2410.05121</link>
      <description>arXiv:2410.05121v2 Announce Type: replace 
Abstract: Homogenization techniques are an appealing approach to reduce computational complexity in systems containing coils with large numbers of high temperature superconductor (HTS) tapes. Resolving all the coated conductor layers and turns in coils is often computationally prohibitive. In this paper, we extend the foil conductor model, well-known in normal conducting applications, to applications with insulated HTS coils. To enhance the numerical performance of the model, the conventional formulation based on A-V is extended to J-A-V. The model is verified to be suitable for simulations of superconductors and to accelerate the calculations compared to resolving all the individual layers. The performance of both the A-V and J-A-V formulated models is examined, and the J-A-V variant is concluded to be advantageous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05121v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Paakkunainen, Louis Denis, Christophe Geuzaine, Paavo Rasilo, Sebastian Sch\"ops</dc:creator>
    </item>
    <item>
      <title>Block Vecchia Approximation for Scalable and Efficient Gaussian Process Computations</title>
      <link>https://arxiv.org/abs/2410.04477</link>
      <description>arXiv:2410.04477v2 Announce Type: replace-cross 
Abstract: Gaussian Processes (GPs) are vital for modeling and predicting irregularly-spaced, large geospatial datasets. However, their computations often pose significant challenges in large-scale applications. One popular method to approximate GPs is the Vecchia approximation, which approximates the full likelihood via a series of conditional probabilities. The classical Vecchia approximation uses univariate conditional distributions, which leads to redundant evaluations and memory burdens. To address this challenge, our study introduces block Vecchia, which evaluates each multivariate conditional distribution of a block of observations, with blocks formed using the K-means algorithm. The proposed GPU framework for the block Vecchia uses varying batched linear algebra operations to compute multivariate conditional distributions concurrently, notably diminishing the frequent likelihood evaluations. Diving into the factor affecting the accuracy of the block Vecchia, the neighbor selection criterion is investigated, where we found that the random ordering markedly enhances the approximated quality as the block count becomes large. To verify the scalability and efficiency of the algorithm, we conduct a series of numerical studies and simulations, demonstrating their practical utility and effectiveness compared to the exact GP. Moreover, we tackle large-scale real datasets using the block Vecchia method, i.e., high-resolution 3D profile wind speed with a million points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04477v2</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qilong Pan, Sameh Abdulah, Marc G. Genton, Ying Sun</dc:creator>
    </item>
  </channel>
</rss>
