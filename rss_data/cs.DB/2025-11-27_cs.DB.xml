<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference</title>
      <link>https://arxiv.org/abs/2511.21160</link>
      <description>arXiv:2511.21160v1 Announce Type: new 
Abstract: The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21160v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wu Sai, Xia Ruichen, Yang Dingyu, Wang Rui, Lai Huihang, Guan Jiarui, Bai Jiameng, Zhang Dongxiang, Tang Xiu, Xie Zhongle, Lu Peng, Chen Gang</dc:creator>
    </item>
    <item>
      <title>HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads</title>
      <link>https://arxiv.org/abs/2511.21307</link>
      <description>arXiv:2511.21307v1 Announce Type: new 
Abstract: Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21307v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyi Zhang, Liang Liang, Anastasia Ailamaki, Jianliang Xu</dc:creator>
    </item>
    <item>
      <title>Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation</title>
      <link>https://arxiv.org/abs/2511.21607</link>
      <description>arXiv:2511.21607v1 Announce Type: new 
Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21607v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zarin Tahia Hossain, Mostafa Milani</dc:creator>
    </item>
    <item>
      <title>Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic</title>
      <link>https://arxiv.org/abs/2511.20677</link>
      <description>arXiv:2511.20677v1 Announce Type: cross 
Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20677v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saleh Almohaimeed, May Alsofyani, Saad Almohaimeed, Mansour Al Ghanim, Liqiang Wang</dc:creator>
    </item>
    <item>
      <title>LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data</title>
      <link>https://arxiv.org/abs/2511.20691</link>
      <description>arXiv:2511.20691v1 Announce Type: cross 
Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20691v1</guid>
      <category>cs.CL</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lijun Shang, Yadong Yu, Wenqiang Kang, Jian Zhou, Dongyue Gao, Pan Xiang, Zhe Liu, Mengyan Dai, Zhonglu Guo, Zhimei Sun</dc:creator>
    </item>
    <item>
      <title>Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM</title>
      <link>https://arxiv.org/abs/2511.21413</link>
      <description>arXiv:2511.21413v1 Announce Type: cross 
Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21413v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3774902.3776632</arxiv:DOI>
      <dc:creator>Tim Trappen, Robert Ke{\ss}ler, Roland Pabel, Viktor Achter, Stefan Wesner</dc:creator>
    </item>
    <item>
      <title>Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework</title>
      <link>https://arxiv.org/abs/2511.21448</link>
      <description>arXiv:2511.21448v1 Announce Type: cross 
Abstract: Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21448v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebeka Toth, Tamas Bisztray, Richard Dubniczky</dc:creator>
    </item>
    <item>
      <title>AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI</title>
      <link>https://arxiv.org/abs/2511.21661</link>
      <description>arXiv:2511.21661v1 Announce Type: cross 
Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21661v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beth Plale, Neelesh Karthikeyan, Isuru Gamage, Joe Stubbs, Sachith Withana</dc:creator>
    </item>
    <item>
      <title>GenRewrite: Query Rewriting via Large Language Models</title>
      <link>https://arxiv.org/abs/2403.09060</link>
      <description>arXiv:2403.09060v2 Announce Type: replace 
Abstract: Query rewriting is an effective technique for refining poorly written queries before they reach the query optimizer. However, manual rewriting is not scalable, as it is prone to errors and requires deep expertise. Traditional query rewriting algorithms fall short too: rule-based approaches fail to generalize to new query patterns, while synthesis-based methods struggle with complex queries. Fortunately, Large Language Models (LLMs) already possess broad knowledge and advanced reasoning capabilities, making them a promising solution for tackling these longstanding challenges. In this paper, we present GenRewrite, the first holistic system that leverages LLMs for query rewriting beyond traditional rules. We introduce the notion of Natural Language Rewrite Rules (NLR2s), which serve as hints for the LLM while also a means of knowledge transfer from rewriting one query to another, allowing GenRewrite to become smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the LLM costs and the manual effort required for verification. Across the standard TPC-DS and JOB benchmarks and their SQLStorm-generated variants, GenRewrite consistently optimizes more queries at every speedup threshold than all baselines. At the &gt;=2x threshold on TPC-DS, GenRewrite improves 25 queries-1.35x more than LLM-driven baselines and 2.6x more than LLM-enhanced rule-based baselines-and the gap widens further on TPC-DS (SQLStorm); on JOB and its SQLStorm variant, where queries are simpler, absolute gains are smaller but GenRewrite still leads by a notable margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09060v2</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Liu, Barzan Mozafari</dc:creator>
    </item>
    <item>
      <title>MatBase Algorithm for Translating Entity-Relationship Data Models into (Elementary) Mathematical Data Model Schemes</title>
      <link>https://arxiv.org/abs/2504.07326</link>
      <description>arXiv:2504.07326v4 Announce Type: replace 
Abstract: This paper presents a pseudocode algorithm for translating Entity-Relationship data models into (Elementary) Mathematical Data Model schemes. We prove that this algorithm is linear, sound, complete, and optimal. As an example, we apply this algorithm to an Entity-Relationship data model for a teaching sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07326v4</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.56831/PSEN-07-236</arxiv:DOI>
      <arxiv:journal_reference>Primera Scientific Engineering 7(6): 04-11, Dec. 2025</arxiv:journal_reference>
      <dc:creator>Christian Mancas, Diana Christina Mancas</dc:creator>
    </item>
    <item>
      <title>Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising</title>
      <link>https://arxiv.org/abs/2510.09084</link>
      <description>arXiv:2510.09084v2 Announce Type: replace-cross 
Abstract: We study the problem of minimizing regret in multi-mode advertisement settings, where an influence provider allocates advertising resources such as social network seeds and billboard slots to multiple advertisers with specified influence demands and payments. Unlike prior work focusing on a single mode of advertising, we consider the interplay between online and offline modes and introduce a novel regret model that captures their interaction effect. This leads to a regret minimization problem that is non-monotone, non-submodular, and NP-hard to approximate within any constant factor. To address this, we propose a monotone, approximately bisubmodular influence model and develop two algorithmic solutions: Projected Subgradient Method based on the Lov\'asz extension of the regret function, and an Approximate Bisubmodular Local Search algorithm with provable guarantees. Experiments on large-scale real-world datasets, including billboard and trajectory data from major U.S. cities, as well as social network graphs, demonstrate that our methods outperform existing baselines in minimizing total regret while satisfying advertiser demands. Our framework is broadly applicable to other resource allocation scenarios beyond advertising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09084v2</guid>
      <category>cs.GT</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Benerjee, Yamuna Prasad</dc:creator>
    </item>
  </channel>
</rss>
