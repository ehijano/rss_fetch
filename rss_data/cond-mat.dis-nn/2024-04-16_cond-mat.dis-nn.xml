<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:04:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Problem Of Image Super-Resolution, Denoising And Some Image Restoration Methods In Deep Learning Models</title>
      <link>https://arxiv.org/abs/2404.09817</link>
      <description>arXiv:2404.09817v1 Announce Type: new 
Abstract: In this article, we address the challenges of image super-resolution and noise reduction, which are crucial for enhancing the quality of images derived from low-resolution or noisy data. We compared and assessed several approaches for upgrading low-resolution images to higher resolutions and for eliminating unwanted noise, all while maintaining the essential characteristics of the original images and recovering images from poor quality or damaged data using deep learning models. Our analysis and the experimental outcomes on image quality metrics indicate that the EDCNN neural network model, enhanced with pretrained weights, significantly outperforms other methods with a Train PSNR of 31.215, a Valid PSNR of 29.493, and a Test PSNR of 31.6632.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09817v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ngoc-Giau Pham, Thanh-Hai Tong Le, Van-Hieu Duong, Hong-Ngoc Tran, Phuoc-Hung Vo</dc:creator>
    </item>
    <item>
      <title>Squish Jamming</title>
      <link>https://arxiv.org/abs/2404.09773</link>
      <description>arXiv:2404.09773v1 Announce Type: cross 
Abstract: A wide range of disordered materials, from biological to geological assemblies, feature discrete elements undergoing large shape changes. How significant geometrical variations at the microscopic scale affect the response of the assembly, in particular rigidity transitions, is an ongoing challenge in soft matter physics. However, the lack of a model granular-like experimental system featuring large and versatile particle deformability impedes advances. Here, we explore the oscillatory shear response of a sponge-like granular assembly composed of highly compressible elastic rings. We highlight a progressive rigidity transition, switching from a fluid-like to a solid-like response by increasing density or decreasing shear amplitude. The rearranging fluid state consists of crystal clusters separated by melted regions; in contrast, the solid state remains amorphous and absorbs all imposed shear elastically. We rationalise this transition by uncovering an effective, attractive shear force between rings that emerges from a friction-geometry interplay. If friction is sufficiently high compared to shear, the extent of the contacts between rings, captured analytically by elementary geometry, controls the rigidity transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09773v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Poincloux, Kazumasa A. Takeuchi</dc:creator>
    </item>
    <item>
      <title>A replica analysis of under-bagging</title>
      <link>https://arxiv.org/abs/2404.09779</link>
      <description>arXiv:2404.09779v1 Announce Type: cross 
Abstract: A sharp asymptotics of the under-bagging (UB) method, which is a popular ensemble learning method for training classifiers from an imbalanced data, is derived and used to compare with several other standard methods for learning from imbalanced data, in the scenario where a linear classifier is trained from a binary mixture data. The methods compared include the under-sampling (US) method, which trains a model using a single realization of the subsampled dataset, and the simple weighting (SW) method, which trains a model with a weighted loss on the entire data. It is shown that the performance of UB is improved by increasing the size of the majority class, even if the class imbalance can be large, especially when the size of the minority class is small. This is in contrast to US, whose performance does not change as the size of the majority class increases, and SW, whose performance decreases as the imbalance increases. These results are different from the case of the naive bagging in training generalized linear models without considering the structure of class imbalance, indicating the intrinsic difference between the ensembling and the direct regularization on the parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09779v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Takahashi</dc:creator>
    </item>
    <item>
      <title>Normalizing flows as an enhanced sampling method for atomistic supercooled liquids</title>
      <link>https://arxiv.org/abs/2404.09914</link>
      <description>arXiv:2404.09914v1 Announce Type: cross 
Abstract: Normalizing flows can transform a simple prior probability distribution into a more complex target distribution. Here, we evaluate the ability and efficiency of generative machine learning methods to sample the Boltzmann distribution of an atomistic model for glass-forming liquids. This is a notoriously difficult task, as it amounts to ergodically exploring the complex free energy landscape of a disordered and frustrated many-body system. We optimize a normalizing flow model to successfully transform high-temperature configurations of a dense liquid into low-temperature ones, near the glass transition. We perform a detailed comparative analysis with established enhanced sampling techniques developed in the physics literature to assess and rank the performance of normalizing flows against state-of-the-art algorithms. We demonstrate that machine learning methods are very promising, showing a large speedup over conventional molecular dynamics. Normalizing flows show performances comparable to parallel tempering and population annealing, while still falling far behind the swap Monte Carlo algorithm. Our study highlights the potential of generative machine learning models in scientific computing for complex systems, but also points to some of its current limitations and the need for further improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09914v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerhard Jung, Giulio Biroli, Ludovic Berthier</dc:creator>
    </item>
    <item>
      <title>Hybrid spinodals for long-range cascades</title>
      <link>https://arxiv.org/abs/2401.09313</link>
      <description>arXiv:2401.09313v2 Announce Type: replace 
Abstract: Cascades are self-reinforcing processes underlying the systemic risk of many complex systems. Understanding the universal aspects of these phenomena is of fundamental interest, yet typically bound to numerical observations in ad-hoc models and limited insights. Here, we develop a unifying approach and show that cascades induced by a long-range propagation of local perturbations are characterized by two universality classes determined by the parity invariance of the underlying process. We provide hyperscaling arguments predicting hybrid critical exponents given by a combination of both mean-field spinodal exponents and $d$-dimensional corrections and we show how global symmetries influence the geometry and lifetime of avalanches. Simulations encompassing classic and novel cascade models validate our predictions, revealing fundamental principles of cascade phenomena amenable to experimental validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09313v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>I. Bonamassa, B. Gross, J. Kert\'esz, S. Havlin</dc:creator>
    </item>
    <item>
      <title>Thermodynamics based on Neural Networks</title>
      <link>https://arxiv.org/abs/2311.13799</link>
      <description>arXiv:2311.13799v3 Announce Type: replace-cross 
Abstract: We present three different neural network algorithms to calculate thermodynamic properties as well as dynamic correlation functions at finite temperatures for quantum lattice models. The first method is based on purification, which allows for the exact calculation of the operator trace. The second one is based on a sampling of the trace using minimally entangled states, whereas the third one makes use of quantum typicality. In the latter case, we approximate a typical infinite-temperature state by wave functions which are given by a product of a projected pair and a neural network part and evolve this typical state in imaginary time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13799v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Phys. Rev. B 109, 155128 (2024)</arxiv:journal_reference>
      <dc:creator>D. Wagner, A. Kl\"umper, J. Sirker</dc:creator>
    </item>
  </channel>
</rss>
