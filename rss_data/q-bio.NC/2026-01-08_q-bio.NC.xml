<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 05:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Emergent togetherness in collaborative dance improvisation: neural and motor synchronization reveal a coupling-decoupling paradox</title>
      <link>https://arxiv.org/abs/2601.03478</link>
      <description>arXiv:2601.03478v1 Announce Type: new 
Abstract: Collective improvisation in dance provides a rich natural laboratory for studying emergent coordination in coupled neuro-motor systems. Here, we investigate how training shapes spontaneous synchronization patterns in both movement and brain signals during collaborative performance. Using a dual-recording protocol integrating 3D motion capture and hyperscanning EEG, participants engaged in free, interaction-driven, and rule-based improvisation before and after a program of generative dance, grounded in cellular-automata. Motor behavior was modeled through a time-resolved {\alpha}-exponent derived from Movement Element Decomposition scaling between mean velocity and displacement, revealing fluctuations in energetic strategies and degrees of freedom. Synchronization events were quantified using Motif Synchronization (biomechanical data) and multilayer Time-Varying Graphs (neural data), enabling the detection of nontrivial lead-lag dependencies beyond zero-lag entrainment. Results indicate that training produced an intriguing dissociation: inter-brain synchronization increased, particularly within the frontal lobe, while interpersonal motor synchrony decreased. This opposite trend suggests that enhanced participatory sense-making fosters neural alignment while simultaneously expanding individual motor explorations, thereby reducing coupling in movement. Our findings position collaborative improvisation as a complex dynamical regime in which togetherness emerges not from identical motor outputs but from shared neural intentionality distributed across multilayer interaction networks, exemplifying the coupling-decoupling paradox, whereby increasing inter-brain synchrony supports the exploration of broader and mutually divergent motor trajectories. These results highlight the nonlinear nature of social coordination, offering new avenues for modeling creative joint action in human systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03478v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yago Emanoel Ramos, Raphael Silva do Ros\'ario, Adriana de Faria Gehres, Maria Jo\~ao Alves, Ana Maria Leit\~ao, Cec\'ilia Bastos da Costa Accioly, Fatima Wachowicz, Ivani L\'ucia Oliveira de Santana, Jos\'e Garcia Vivas Miranda</dc:creator>
    </item>
    <item>
      <title>A Quantifiable Information-Processing Hierarchy Provides a Necessary Condition for Detecting Agency</title>
      <link>https://arxiv.org/abs/2601.03498</link>
      <description>arXiv:2601.03498v1 Announce Type: new 
Abstract: As intelligent systems are developed across diverse substrates - from machine learning models and neuromorphic hardware to in vitro neural cultures - understanding what gives a system agency has become increasingly important. Existing definitions, however, tend to rely on top-down descriptions that are difficult to quantify. We propose a bottom-up framework grounded in a system's information-processing order: the extent to which its transformation of input evolves over time. We identify three orders of information processing. Class I systems are reactive and memoryless, mapping inputs directly to outputs. Class II systems incorporate internal states that provide memory but follow fixed transformation rules. Class III systems are adaptive; their transformation rules themselves change as a function of prior activity. While not sufficient on their own, these dynamics represent necessary informational conditions for genuine agency. This hierarchy offers a measurable, substrate-independent way to identify the informational precursors of agency. We illustrate the framework with neurophysiological and computational examples, including thermostats and receptor-like memristors, and discuss its implications for the ethical and functional evaluation of systems that may exhibit agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03498v1</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brett J. Kagan, Valentina Baccetti, Brian D. Earp, J. Lomax Boyd, Julian Savulescu, Adeel Razi</dc:creator>
    </item>
    <item>
      <title>Data-driven inference of brain dynamical states from the r-spectrum of correlation matrices</title>
      <link>https://arxiv.org/abs/2601.03796</link>
      <description>arXiv:2601.03796v1 Announce Type: new 
Abstract: We present a data-driven framework to characterize large-scale brain dynamical states directly from correlation matrices at the single-subject level. By treating correlation thresholding as a percolation-like probe of connectivity, the approach tracks multiple cluster- and network-level observables and identifies a characteristic percolation threshold, rc, at which these signatures converge. We use $r_c$ as an operational and physically interpretable descriptor of large-scale brain dynamical state. Applied to resting-state fMRI data from a large cohort of healthy individuals (N = 996), the method yields stable, subject-specific estimates that covary systematically with established dynamical indicators such as temporal autocorrelations. Numerical simulations of a whole-brain model with a known critical regime further show that $r_c$ tracks changes in collective dynamics under controlled variations of excitability. By replacing arbitrary threshold selection with a criterion intrinsic to correlation structure, the r-spectra provides a physically grounded approach for comparing brain dynamical states across individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03796v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Gabaldon, Adria Mulero, Rong Wang, Daniel A. Martin, Sabrina Camargo, Qian-Yuan Tang, Ignacio Cifre, Changsong Zhou, Dante R. Chialvo</dc:creator>
    </item>
    <item>
      <title>Stigmergic optimal transport</title>
      <link>https://arxiv.org/abs/2601.04111</link>
      <description>arXiv:2601.04111v1 Announce Type: new 
Abstract: Efficient navigation in swarms often relies on the emergence of decentralized approaches that minimize traversal time or energy. Stigmergy, where agents modify a shared environment that then modifies their behavior, is a classic mechanism that can encode this strategy. We develop a theoretical framework for stigmergic transport by casting it as a stochastic optimal control problem: agents (collectively) lay and (individually) follow trails while minimizing expected traversal time. Simulations and analysis reveal two emergent behaviors: path straightening in homogeneous environments and path refraction at material interfaces, both consistent with experimental observations of insect trails. While reminiscent of Fermat's principle, our results show how local, noisy agent+field interactions can give rise to geodesic trajectories in heterogeneous environments, without centralized coordination or global knowledge, relying instead on an embodied slow fast dynamical mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04111v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>nlin.AO</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishaal Krishnan, L. Mahadevan</dc:creator>
    </item>
    <item>
      <title>Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models</title>
      <link>https://arxiv.org/abs/2502.20408</link>
      <description>arXiv:2502.20408v2 Announce Type: replace 
Abstract: In recent years, the rapid advancement of large language models (LLMs) in natural language processing has sparked significant interest among researchers to understand their mechanisms and functional characteristics. Although prior studies have attempted to explain LLM functionalities by identifying and interpreting specific neurons, these efforts mostly focus on individual neuron contributions, neglecting the fact that human brain functions are realized through intricate interaction networks. Inspired by research on functional brain networks (FBNs) in the field of neuroscience, we utilize similar methodologies estabilished in FBN analysis to explore the "functional networks" within LLMs in this study. Experimental results highlight that, much like the human brain, LLMs exhibit certain functional networks that recur frequently during their operation. Further investigation reveals that these functional networks are indispensable for LLM performance. Inhibiting key functional networks severely impairs the model's capabilities. Conversely, amplifying the activity of neurons within these networks can enhance either the model's overall performance or its performance on specific tasks. This suggests that these functional networks are strongly associated with either specific tasks or the overall performance of the LLM. Code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20408v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiheng Liu, Zhengliang Liu, Zihao Wu, Junhao Ning, Haiyang Sun, Sichen Xia, Yang Yang, Xiaohui Gao, Ning Qiang, Bao Ge, Tianming Liu, Junwei Han, Xintao Hu</dc:creator>
    </item>
    <item>
      <title>SPD Matrix Learning for Neuroimaging Analysis: Perspectives, Methods, and Challenges</title>
      <link>https://arxiv.org/abs/2504.18882</link>
      <description>arXiv:2504.18882v2 Announce Type: replace-cross 
Abstract: Neuroimaging provides essential tools for characterizing brain activity by quantifying connectivity strength between remote regions, using different modalities that capture different aspects of connectivity. Yet, decoding meaningful neural signatures must contend with modality-specific challenges, including measurement noise, spatial and temporal distortions, heterogeneous acquisition protocols, and limited sample sizes. A unifying perspective emerges when these data are expressed through symmetric positive definite (SPD)-valued representations: across neuroimaging modalities, SPD-valued representations naturally give rise to SPD matrices that capture dependencies between sensors or brain regions. Endowing the SPD space with Riemannian metrics equips it with a non-Euclidean geometric structure, enabling principled statistical modeling and machine learning on the resulting manifold.
  This review consolidates machine learning methodologies that operate on the SPD manifold under a unified framework termed SPD matrix learning. SPD matrix learning brings conceptual clarity across multiple modalities, establishes continuity with decades of geometric statistics in neuroimaging, and positions SPD modeling as a methodological bridge between classical analysis and emerging AI-driven paradigms. We show that (i) modeling on the SPD manifold is mathematically natural and numerically stable, preserving symmetry and positive definiteness while avoiding degeneracies inherent to Euclidean embeddings; (ii) SPD matrix learning extends a broad family of established geometric statistical tools used across neuroimaging; and (iii) SPD matrix learning integrates new-generation AI technologies, driving a new class of neuroimaging problems that were previously out of reach. Taken together, SPD matrix learning offers a principled and forward-looking framework for next-generation neuroimaging analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18882v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ce Ju, Reinmar Kobler, Antoine Collas, Motoaki Kawanabe, Cuntai Guan, Bertrand Thirion</dc:creator>
    </item>
  </channel>
</rss>
