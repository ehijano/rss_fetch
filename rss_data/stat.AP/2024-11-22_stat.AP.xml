<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Nov 2024 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Causal wavelet analysis of ozone pollution contingencies in the Mexico City Metropolitan Area</title>
      <link>https://arxiv.org/abs/2411.13568</link>
      <description>arXiv:2411.13568v1 Announce Type: new 
Abstract: In the recent two decades, the Mexico City Metropolitan Area (MCMA) has been plagued by high concentrations of air pollutants, risking the health integrity of its inhabitants. Although some policies have been undertaken, they have been insufficient to deplete high air pollutants. Environmental contingencies are commonly imposed when the ozone concentration overpasses a certain threshold, which is well above the recommended maximum by the WHO. This work used a causal version of a generalized Morlet wavelet to characterize the dynamics of daily ozone concentration in the MCMA. The results indicated that the formation of dangerous ozone concentration levels is a consequence of accumulation and incomplete dissipation effects acting over a wide range of time scales. Ozone contingencies occurred when the wavelet coefficient power is increasing, which was linked to an inti-persistence behavior. It was proposed that the wavelet methodology could be used as a further tool for signaling the potential formation of adverse ozone pollution scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13568v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>J. A. Mart\'inez-Cadena, J. M. S\'anchez-Cerritos, A. Marin-Lopez, M. Meraz, J. Alvarez-Ramirez</dc:creator>
    </item>
    <item>
      <title>Integrating Dynamic Correlation Shifts and Weighted Benchmarking in Extreme Value Analysis</title>
      <link>https://arxiv.org/abs/2411.13608</link>
      <description>arXiv:2411.13608v1 Announce Type: new 
Abstract: This paper presents an innovative approach to Extreme Value Analysis (EVA) by introducing the Extreme Value Dynamic Benchmarking Method (EVDBM). EVDBM integrates extreme value theory to detect extreme events and is coupled with the novel Dynamic Identification of Significant Correlation (DISC)-Thresholding algorithm, which enhances the analysis of key variables under extreme conditions. By integrating return values predicted through EVA into the benchmarking scores, we are able to transform these scores to reflect anticipated conditions more accurately. This provides a more precise picture of how each case is projected to unfold under extreme conditions. As a result, the adjusted scores offer a forward-looking perspective, highlighting potential vulnerabilities and resilience factors for each case in a way that static historical data alone cannot capture. By incorporating both historical and probabilistic elements, the EVDBM algorithm provides a comprehensive benchmarking framework that is adaptable to a range of scenarios and contexts. The methodology is applied to real PV data, revealing critical low - production scenarios and significant correlations between variables, which aid in risk management, infrastructure design, and long-term planning, while also allowing for the comparison of different production plants. The flexibility of EVDBM suggests its potential for broader applications in other sectors where decision-making sensitivity is crucial, offering valuable insights to improve outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13608v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios P. Panagoulias, Elissaios Sarmas, Vangelis Marinakis, Maria Virvou, George A. Tsihrintzis</dc:creator>
    </item>
    <item>
      <title>Accounting carbon emissions from electricity generation: a review and comparison of emission factor-based methods</title>
      <link>https://arxiv.org/abs/2411.13663</link>
      <description>arXiv:2411.13663v1 Announce Type: new 
Abstract: Accurate estimation of greenhouse gas (GHG) is essential to meet carbon neutrality targets, particularly through the calculation of direct CO2 emissions from electricity generation. This work reviews and compares emission factor-based methods for accounting direct carbon emissions from electricity generation. The emission factor approach is commonly worldwide used. Empirical comparisons are based on emission factors computed using data from the Italian electricity market. The analyses reveal significant differences in the CO2 estimates according to different methods. This, in turn, highlights the need to select an appropriate method for reliable emissions, which could support effective regulatory compliance and informed policy-making. As concerns, in particular, the market zones of the Italian electricity market, the results underscore the importance of tailoring emission factors to accurately capture regional fuel variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13663v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marina Bertolini, Pierdomenico Duttilo, Francesco Lisi</dc:creator>
    </item>
    <item>
      <title>The Fast and the Furious: Tracking the Effect of the Tomoa Skip on Speed Climbing</title>
      <link>https://arxiv.org/abs/2411.13696</link>
      <description>arXiv:2411.13696v1 Announce Type: new 
Abstract: Sport climbing is an athletic discipline comprised of three sub-disciplines -- lead climbing, bouldering, and speed climbing. These three sub-disciplines have distinct goals, resulting in specialization of athletes into one of the three events. The year 2020 marked the first inclusion of sport climbing in the Olympic Games. While this decision was met with excitement from the climbing community, it was not without controversy. The International Olympic Committee had allocated one set of medals for the entire sport, necessitating the combination of sub-disciplines into one competition. As a result, athletes who specialized in lead and bouldering were forced to train and compete in speed for the first time in their careers. One such athlete was Tomoa Narasaki, a World Champion boulderer, who introduced a new method of approaching the speed event. This approach, deemed the Tomoa Skip (TS), was subsequently adopted by many of the top speed climbers. Concurrently, speed records fell rapidly (from 5.48s in 2017 to 4.90s in 2023). Speed climbing involves ascending a 15m wall containing the same pattern of obstacles. Thus, records can be compared across time. In this paper we investigate the effect of the TS on speed climbing by answering two questions: (1) Did the TS result in a decrease in speed times? and (2) Do climbers who utilize the TS show less consistency? The success of the TS highlights the potential of collaboration between different disciplines of sport, showing athletes of diverse backgrounds may contribute to the evolution of competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13696v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb Chou, Andee Kaplan</dc:creator>
    </item>
    <item>
      <title>Modelling and prediction of the wildfire data using fractional Poisson process</title>
      <link>https://arxiv.org/abs/2411.13995</link>
      <description>arXiv:2411.13995v1 Announce Type: new 
Abstract: Modelling wildfire events has been studied in the literature using the Poisson process, which essentially assumes the independence of wildfire events. In this paper, we use the fractional Poisson process to model the wildfire occurrences in California between June 2019 - April 2023 and predict the wildfire events that explains the underlying memory between these events. We introduce method of moments and maximum likelihood estimate approaches to estimate the parameters of the fractional Poisson process, which is an alternative to the method proposed by Cahoy (2010). We obtain the estimates of the fractional parameter as 0.8, proving that the wildfire events are dependent. The proposed model has reduced prediction error by 90\% compared to the classical Poisson process model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13995v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sudeep R. Bapat, Aditya Maheshwari</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Working Memory via Moment Neural Networks</title>
      <link>https://arxiv.org/abs/2411.14196</link>
      <description>arXiv:2411.14196v1 Announce Type: cross 
Abstract: Humans possess a finely tuned sense of uncertainty that helps anticipate potential errors, vital for adaptive behavior and survival. However, the underlying neural mechanisms remain unclear. This study applies moment neural networks (MNNs) to explore the neural mechanism of uncertainty quantification in working memory (WM). The MNN captures nonlinear coupling of the first two moments in spiking neural networks (SNNs), identifying firing covariance as a key indicator of uncertainty in encoded information. Trained on a WM task, the model demonstrates coding precision and uncertainty quantification comparable to human performance. Analysis reveals a link between the probabilistic and sampling-based coding for uncertainty representation. Transferring the MNN's weights to an SNN replicates these results. Furthermore, the study provides testable predictions demonstrating how noise and heterogeneity enhance WM performance, highlighting their beneficial role rather than being mere biological byproducts. These findings offer insights into how the brain effectively manages uncertainty with exceptional accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14196v1</guid>
      <category>physics.bio-ph</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengyuan Ma, Wenlian Lu, Jianfeng Feng</dc:creator>
    </item>
    <item>
      <title>Indiscriminate Disruption of Conditional Inference on Multivariate Gaussians</title>
      <link>https://arxiv.org/abs/2411.14351</link>
      <description>arXiv:2411.14351v1 Announce Type: cross 
Abstract: The multivariate Gaussian distribution underpins myriad operations-research, decision-analytic, and machine-learning models (e.g., Bayesian optimization, Gaussian influence diagrams, and variational autoencoders). However, despite recent advances in adversarial machine learning (AML), inference for Gaussian models in the presence of an adversary is notably understudied. Therefore, we consider a self-interested attacker who wishes to disrupt a decisionmaker's conditional inference and subsequent actions by corrupting a set of evidentiary variables. To avoid detection, the attacker also desires the attack to appear plausible wherein plausibility is determined by the density of the corrupted evidence. We consider white- and grey-box settings such that the attacker has complete and incomplete knowledge about the decisionmaker's underlying multivariate Gaussian distribution, respectively. Select instances are shown to reduce to quadratic and stochastic quadratic programs, and structural properties are derived to inform solution methods. We assess the impact and efficacy of these attacks in three examples, including, real estate evaluation, interest rate estimation and signals processing. Each example leverages an alternative underlying model, thereby highlighting the attacks' broad applicability. Through these applications, we also juxtapose the behavior of the white- and grey-box attacks to understand how uncertainty and structure affect attacker behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14351v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William N. Caballero, Matthew LaRosa, Alexander Fisher, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>Demystifying and avoiding the OLS "weighting problem": Unmodeled heterogeneity and straightforward solutions</title>
      <link>https://arxiv.org/abs/2403.03299</link>
      <description>arXiv:2403.03299v3 Announce Type: replace-cross 
Abstract: Researchers have long run regressions of an outcome variable (Y) on a treatment (D) and covariates (X) to estimate treatment effects. Even absent unobserved confounding, the regression coefficient on D in this setup reports a conditional variance weighted average of strata-wise average effects, not generally equal to the average treatment effect (ATE). Numerous proposals have been offered to cope with this "weighting problem", including interpretational tools to help characterize the weights and diagnostic aids to help researchers assess the potential severity of this problem. We make two contributions that together suggest an alternative direction for researchers and this literature. Our first contribution is conceptual, demystifying these weights. Simply put, under heterogeneous treatment effects (and varying probability of treatment), the linear regression of Y on D and X will be misspecified. The "weights" of regression offer one characterization for the coefficient from regression that helps to clarify how it will depart from the ATE. We also derive a more general expression for the weights than what is usually referenced. Our second contribution is practical: as these weights simply characterize misspecification bias, we suggest simply avoiding them through an approach that tolerate heterogeneous effects. A wide range of longstanding alternatives (regression-imputation/g-computation, interacted regression, and balancing weights) relax specification assumptions to allow heterogeneous effects. We make explicit the assumption of "separate linearity", under which each potential outcome is separately linear in X. This relaxation of conventional linearity offers a common justification for all of these methods and avoids the weighting problem, at an efficiency cost that will be small when there are few covariates relative to sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03299v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Hazlett, Tanvi Shinkre</dc:creator>
    </item>
    <item>
      <title>GRAMEP: an alignment-free method based on the Maximum Entropy Principle for identifying SNPs</title>
      <link>https://arxiv.org/abs/2405.01715</link>
      <description>arXiv:2405.01715v2 Announce Type: replace-cross 
Abstract: Background: Advances in high throughput sequencing technologies provide a huge number of genomes to be analyzed. Thus, computational methods play a crucial role in analyzing and extracting knowledge from the data generated. Investigating genomic mutations is critical because of their impact on chromosomal evolution, genetic disorders, and diseases. It is common to adopt aligning sequences for analyzing genomic variations. However, this approach can be computationally expensive and restrictive in scenarios with large datasets. Results: We present a novel method for identifying single nucleotide polymorphisms (SNPs) in DNA sequences from assembled genomes. This study proposes GRAMEP, an alignment-free approach that adopts the principle of maximum entropy to discover the most informative k-mers specific to a genome or set of sequences under investigation. The informative k-mers enable the detection of variant-specific mutations in comparison to a reference genome or other set of sequences. In addition, our method offers the possibility of classifying novel sequences with no need for organism-specific information. GRAMEP demonstrated high accuracy in both in silico simulations and analyses of viral genomes, including Dengue, HIV, and SARS-CoV-2. Our approach maintained accurate SARS-CoV-2 variant identification while demonstrating a lower computational cost compared to methods with the same purpose. Conclusions: GRAMEP is an open and user-friendly software based on maximum entropy that provides an efficient alignment-free approach to identifying and classifying unique genomic subsequences and SNPs with high accuracy, offering advantages over comparative methods. The instructions for use, applicability, and usability of GRAMEP are open access at https://github.com/omatheuspimenta/GRAMEP</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01715v2</guid>
      <category>q-bio.GN</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matheus Henrique Pimenta-Zanon, Andr\'e Yoshiaki Kashiwabara, Andr\'e Lu\'is Laforga Vanzela, Fabricio Martins Lopes</dc:creator>
    </item>
    <item>
      <title>Integration of Active Learning and MCMC Sampling for Efficient Bayesian Calibration of Mechanical Properties</title>
      <link>https://arxiv.org/abs/2411.13361</link>
      <description>arXiv:2411.13361v2 Announce Type: replace-cross 
Abstract: Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate modelling have significantly enhanced the feasibility of Bayesian analysis across engineering fields. However, the selection and integration of surrogate models and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A systematic assessment of their combined influence on analytical accuracy and efficiency is notably lacking. The present work offers a comprehensive comparative study, employing a scalable case study in computational mechanics focused on the inference of spatially varying material parameters, that sheds light on the impact of methodological choices for surrogate modelling and sampling. We show that a priori training of the surrogate model introduces large errors in the posterior estimation even in low to moderate dimensions. We introduce a simple active learning strategy based on the path of the MCMC algorithm that is superior to all a priori trained models, and determine its training data requirements. We demonstrate that the choice of the MCMC algorithm has only a small influence on the amount of training data but no significant influence on the accuracy of the resulting surrogate model. Further, we show that the accuracy of the posterior estimation largely depends on the surrogate model, but not even a tailored surrogate guarantees convergence of the MCMC.Finally, we identify the forward model as the bottleneck in the inference process, not the MCMC algorithm. While related works focus on employing advanced MCMC algorithms, we demonstrate that the training data requirements render the surrogate modelling approach infeasible before the benefits of these gradient-based MCMC algorithms on cheap models can be reaped.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13361v2</guid>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Riccius, Iuri B. C. M. Rocha, Joris Bierkens, Hanne Kekkonen, Frans P. van der Meer</dc:creator>
    </item>
  </channel>
</rss>
