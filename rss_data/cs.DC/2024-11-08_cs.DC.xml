<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>OpenFLAME: Building a large scale federated localization and mapping service</title>
      <link>https://arxiv.org/abs/2411.04271</link>
      <description>arXiv:2411.04271v1 Announce Type: new 
Abstract: The widespread availability of maps has enabled the development of numerous location-based applications, including navigation, ride-sharing, fitness tracking, gaming, robotics, and augmented reality. Today, the maps that power these services are predominantly controlled by a few large corporations and mostly cover outdoor spaces. As the use of these applications expands and indoor localization technologies advance, we are seeing the need for a scalable, federated location management system that can extend into private spaces.
  We introduce OpenFLAME (Open Federated Localization and Mapping Engine), the first federated and decentralized localization service. OpenFLAME links servers that handle localization for specific regions, providing applications with a seamless global view. Creating a federated localization system poses challenges, such as discovering the appropriate servers for a region and integrating services managed by independent providers. To address these issues and ensure scalability, we leverage Domain Name System (DNS) for service discovery and implement map abstractions to retrieve and merge locations across different maps. Our trace-driven study demonstrates that federated localization across remote servers is feasible with acceptable query latencies. To highlight the potential of the system, we developed an augmented reality navigation application for a large indoor space, showing that OpenFLAME can successfully power location-based applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04271v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagar Bharadwaj, Luke Wang, Michael Liang, Harrison Williams, Ivan Liang, Srinivasan Seshan, Anthony Rowe</dc:creator>
    </item>
    <item>
      <title>Intersections of Web3 and AI -- View in 2024</title>
      <link>https://arxiv.org/abs/2411.04318</link>
      <description>arXiv:2411.04318v1 Announce Type: new 
Abstract: This paper summarises the intersection of Web3 and AI technologies, synergies between these technologies, and gaps that we suggest exist in the conception of the possible integrations of these technologies. The summary is informed by a comprehensive literature review of current academic and industry papers, analyst reports, and Ethereum research community blogposts. We focus our contribution on the perceived gaps and detail some novel approaches that would benefit the blockchain/Web3 ecosystem. We believe that the overview presented in this paper will help guide researchers interested in the intersection of Web3 and AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04318v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David Hyland-Wood, Sandra Johnson</dc:creator>
    </item>
    <item>
      <title>Analysis of Blockchain Assisted Energy Sharing Algorithms with Realistic Data Across Microgrids</title>
      <link>https://arxiv.org/abs/2411.04538</link>
      <description>arXiv:2411.04538v1 Announce Type: new 
Abstract: With escalating energy demands, innovative solutions have emerged to supply energy affordably and sustainably. Energy sharing has also been proposed as a solution, addressing affordability issues while reducing consumers' greed. In this paper, we analyse the feasibility of two energy sharing algorithms, centralized and peer-to-peer, within two scenarios, between microgrids within a county, and between microgrids across counties. In addition, we propose a new sharing algorithm named Selfish Sharing, where prosumers take advantage of consumers' batteries in return for letting them consume part of the shared energy. The results for sharing between microgrids across counties show that the dependency on the grid could be reduced by approximately 5.72%, 6.12%, and 5.93% using the centralized, peer-to-peer and selfish sharing algorithms respectively, compared to trading only. The scenario of sharing between microgrids within a county has an average decrease in dependency on the grid by 5.66%, 6.0%, and 5.80% using the centralized, peer-to-peer and selfish algorithms respectively, compared to trading without sharing. We found that trading with batteries and the proposed sharing algorithms prove to be beneficial in the sharing between microgrids case. More specifically, the case of trading and sharing energy between microgrids across counties outperforms sharing within a county, with P2P sharing appearing to be superior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04538v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdulrezzak Zekiye, Ozan Sina Bankaoglu, Ouns Bouachir, Oznur Ozkasap, Moayad Aloqaily</dc:creator>
    </item>
    <item>
      <title>Joint wireless and computing resource management with optimal slice selection in in-network-edge metaverse system</title>
      <link>https://arxiv.org/abs/2411.04561</link>
      <description>arXiv:2411.04561v1 Announce Type: new 
Abstract: This paper presents an approach to joint wireless and computing resource management in slice-enabled metaverse networks, addressing the challenges of inter-slice and intra-slice resource allocation in the presence of in-network computing. We formulate the problem as a mixed-integer nonlinear programming (MINLP) problem and derive an optimal solution using standard optimization techniques. Through extensive simulations, we demonstrate that our proposed method significantly improves system performance by effectively balancing the allocation of radio and computing resources across multiple slices. Our approach outperforms existing benchmarks, particularly in scenarios with high user demand and varying computational tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04561v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulaiman Muhammad Rashid, Ibrahim Aliyu, Abubakar Isah, Jihoon Lee, Sangwon Oh, Minsoo Hahn, Jinsul Kim</dc:creator>
    </item>
    <item>
      <title>RainCloud: Decentralized Coordination and Communication in Heterogeneous IoT Swarms</title>
      <link>https://arxiv.org/abs/2411.04593</link>
      <description>arXiv:2411.04593v1 Announce Type: new 
Abstract: The increasing volume and complexity of IoT systems demand a transition from the cloud-centric model to a decentralized IoT architecture in the so-called Computing Continuum, with no or minimal reliance on central servers. This paradigm shift, however, raises novel research concerns for decentralized coordination, calling for accurate policies. However, building such strategies is not trivial. Our work aims to relieve the DevOps engineers from this concern and propose a solution for autonomous, decentralized task allocation at runtime for IoT systems. To this end, we present a semantic communication approach and an ad-hoc lightweight coordination strategy based on Ant Colony Optimization (ACO). We compare the ACO strategy with Random Search and Gossip protocol-based algorithms. We conduct accurate experiments with up to a hundred nodes in both a static and a dynamic environment, i.e., with device outages. We show that ACO finds a matching node with the smallest hops and messages sent. While the Gossip strategy can allocate the most tasks successfully, ACO scales better, thus being a promising candidate for decentralized task coordination in IoT clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04593v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Filip Loisel, Geri Zeqo, Andrea Morichetta, Anna Lackinger, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Precision-Aware Iterative Algorithms Based on Group-Shared Exponents of Floating-Point Numbers</title>
      <link>https://arxiv.org/abs/2411.04686</link>
      <description>arXiv:2411.04686v1 Announce Type: new 
Abstract: Iterative solvers are frequently used in scientific applications and engineering computations. However, the memory-bound Sparse Matrix-Vector (SpMV) kernel computation hinders the efficiency of iterative algorithms. As modern hardware increasingly supports low-precision computation, the mixed-precision optimization of iterative algorithms has garnered widespread attention. Nevertheless, existing mixed-precision methods pose challenges, including format conversion overhead, tight coupling between storage and computation representation, and the need to store multiple precision copies of data. This paper proposes a floating-point representation based on the group-shared exponent and segmented storage of the mantissa, enabling higher bit utilization of the representation vector and fast switches between different precisions without needing multiple data copies. Furthermore, a stepped mixed-precision iterative algorithm is proposed. Our experimental results demonstrate that, compared with existing floating-point formats, our approach significantly improves iterative algorithms' performance and convergence residuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04686v1</guid>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhua Gao, Jiayuan Shen, Yuxiang Zhang, Weixing Ji, Hua Huang</dc:creator>
    </item>
    <item>
      <title>Cooperation and Personalization on a Seesaw: Choice-based FL for Safe Cooperation in Wireless Networks</title>
      <link>https://arxiv.org/abs/2411.04159</link>
      <description>arXiv:2411.04159v1 Announce Type: cross 
Abstract: Federated learning (FL) is an innovative distributed artificial intelligence (AI) technique. It has been used for interdisciplinary studies in different fields such as healthcare, marketing and finance. However the application of FL in wireless networks is still in its infancy. In this work, we first overview benefits and concerns when applying FL to wireless networks. Next, we provide a new perspective on existing personalized FL frameworks by analyzing the relationship between cooperation and personalization in these frameworks. Additionally, we discuss the possibility of tuning the cooperation level with a choice-based approach. Our choice-based FL approach is a flexible and safe FL framework that allows participants to lower the level of cooperation when they feel unsafe or unable to benefit from the cooperation. In this way, the choice-based FL framework aims to address the safety and fairness concerns in FL and protect participants from malicious attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04159v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Zhang, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference</title>
      <link>https://arxiv.org/abs/2411.04975</link>
      <description>arXiv:2411.04975v1 Announce Type: cross 
Abstract: We present SuffixDecoding, a novel model-free approach to accelerating large language model (LLM) inference through speculative decoding. Unlike existing methods that rely on draft models or specialized decoding heads, SuffixDecoding leverages suffix trees built from previously generated outputs to efficiently predict candidate token sequences. Our approach enables flexible tree-structured speculation without the overhead of maintaining and orchestrating additional models. SuffixDecoding builds and dynamically updates suffix trees to capture patterns in the generated text, using them to construct speculation trees through a principled scoring mechanism based on empirical token frequencies. SuffixDecoding requires only CPU memory which is plentiful and underutilized on typical LLM serving nodes. We demonstrate that SuffixDecoding achieves competitive speedups compared to model-based approaches across diverse workloads including open-domain chat, code generation, and text-to-SQL tasks. For open-ended chat and code generation tasks, SuffixDecoding achieves up to $1.4\times$ higher output throughput than SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to $2.9\times$ higher output throughput and $3\times$ lower latency than speculative decoding. Our evaluation shows that SuffixDecoding maintains high acceptance rates even with small reference corpora of 256 examples, while continuing to improve performance as more historical outputs are incorporated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04975v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao</dc:creator>
    </item>
    <item>
      <title>Reaching Agreement in Competitive Microbial Systems</title>
      <link>https://arxiv.org/abs/2103.07450</link>
      <description>arXiv:2103.07450v3 Announce Type: replace 
Abstract: We study distributed agreement in microbial distributed systems under stochastic population dynamics and competitive interactions. Motivated by recent applications in synthetic biology, we examine how the presence and absence of direct competition among microbial species influences their ability to reach majority consensus. In this problem, two species are designated as input species, and the goal is to guarantee that eventually only the input species which had the highest initial count prevails.
  We show that direct competition dynamics reach majority consensus with high probability even when the initial gap between the species is small, i.e., $\Omega(\sqrt{n\log n})$, where $n$ is the initial population size. In contrast, we show that absence of direct competition is not robust: solving majority consensus with constant probability requires a large initial gap of $\Omega(n)$. To corroborate our analytical results, we use simulations to show that these consensus dynamics occur within practical biological time scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.07450v3</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Andaur, Janna Burman, Matthias F\"ugger, Manish Kushwaha, Bilal Manssouri, Thomas Nowak, Joel Rybicki</dc:creator>
    </item>
    <item>
      <title>HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO Computation Redundancy</title>
      <link>https://arxiv.org/abs/2411.01288</link>
      <description>arXiv:2411.01288v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) has emerged as a practical approach to scale up parameters for the Transformer model to achieve better generalization while maintaining a sub-linear increase in computation overhead. Current MoE models are mainly built with expert parallelism on distributed devices. However, it usually depends on homogeneous devices to deploy and suffers from heavy communication overhead and computation redundancy. In this paper, we explore developing a \texttt{H}eterogeneous-aware \texttt{EX}pert \texttt{A}llocation framework, \textbf{\texttt{HEXA-MoE}}, with significantly enhanced computing efficiency. It contains two components: ($1$) \textit{Expert-Specific Operators}. We replace the typical general matrix multiplication or grouped matrix multiplication interfaces with our operators, which allows the computing to be performed in an in-place manner with \textbf{ZERO} redundancy. ($2$) \textit{Adaptive Data- and Model-Centric Configurations} for different workload scales. Specifically, we introduce a pipeline-shared cache on each device to tackle the heavy memory consumption in the existing data-centric MoE library. Comprehensive experiments on the Swin-MoE benchmark consistently reveal the effectiveness of our \texttt{HEXA-MoE} framework, i.e., reducing $10\%\sim48\%$ memory consumption and achieving $0.5\sim4.3\times$ speed up compared to current state-of-the-art MoE libraries. Furthermore, we examine our \texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric settings. Promising results show that employing optimal parallel configuration with \texttt{HEXA-MoE} on heterogeneous devices can substantially minimize overall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01288v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuqing Luo, Jie Peng, Pingzhi Li, Tianlong Chen</dc:creator>
    </item>
    <item>
      <title>Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2402.11653</link>
      <description>arXiv:2402.11653v2 Announce Type: replace-cross 
Abstract: Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy. Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server. Moreover, existing multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function. We proposed a novel combinatorial client-master MADRL (CCM\_MADRL) algorithm for task offloading in MEC (CCM\_MADRL\_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs. CCM\_MADRL\_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs. By taking advantage of the combinatorial action selection, CCM\_MADRL\_MEC has shown superior convergence over existing MADDPG and heuristic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11653v2</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tesfay Zemuy Gebrekidan, Sebastian Stein, Timothy J. Norman</dc:creator>
    </item>
    <item>
      <title>Enabling Efficient On-Device Fine-Tuning of LLMs Using Only Inference Engines</title>
      <link>https://arxiv.org/abs/2409.15520</link>
      <description>arXiv:2409.15520v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are currently pre-trained and fine-tuned on large cloud servers. The next frontier is LLM personalization, where a foundation model can be fine-tuned with user/task-specific data. Given the sensitive nature of such private data, it is desirable to fine-tune these models on edge devices to improve user trust. However, fine-tuning on resource-constrained edge devices presents significant challenges due to substantial memory and computational demands, as well as limited infrastructure support. We observe that inference engines (e.g., ExecuTorch) can be repurposed for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses multiple forward passes to approximate gradients. However, directly applying ZO methods on edge devices is impractical due to the high computational cost of multiple model perturbations required to achieve accuracy improvements. Based on these observations, we propose a memory- and computation-efficient LLM fine-tuning method for edge devices. Our approach has three key innovations: (1) We introduce a parallelized randomized gradient estimation (P-RGE) technique that achieves high parallel efficiency by leveraging outer-loop and inner-loop parallelization. This enables multiple function queries and forward passes to be executed in parallel, reducing training time. (2) We integrate P-RGE with parameter-efficient fine-tuning methods (e.g. LoRA) to further reduce computational and memory overhead. (3) We implement a P-RGE LoRA-FA module that fully supports fine-tuning with ExecuTorch. Our approach requires no modifications to ExecuTorch's runtime code, as it can be implemented with server-side code changes only. Experiments demonstrate that P-RGE achieves substantial runtime speedups and memory savings while improving fine-tuning accuracy, paving the way for practical deployment of LLMs in real-time, on-device applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15520v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lei Gao, Amir Ziashahabi, Yue Niu, Salman Avestimehr, Murali Annavaram</dc:creator>
    </item>
  </channel>
</rss>
