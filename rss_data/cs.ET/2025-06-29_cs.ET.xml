<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles</title>
      <link>https://arxiv.org/abs/2506.22052</link>
      <description>arXiv:2506.22052v1 Announce Type: new 
Abstract: V2X communication has become crucial for enhancing road safety, especially for Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the increasing number of devices communicating on the same channels will lead to significant channel load. To address this issue this study evaluates the effectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM), focusing specifically on cyclists. The objective of RM is to minimize the transmission of redundant information. We conducted a simulation study using a urban scenario with a high bicycle density based on traffic data from Hannover, Germany. This study assessed the impact of RM on channel load, measured by Channel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in simulation. To evaluate the accuracy and reliability of the RM mechanisms, we analyzed the actual differences in position, speed, and heading between the ego VRU and the VRU, which was assumed to be redundant. Our findings indicate that while RM can reduce channel congestion, it also leads to a decrease in VPR. The analysis of actual differences revealed that the RM mechanism standardized by ETSI often uses outdated information, leading to significant discrepancies in position, speed, and heading, which could result in dangerous situations. To address these limitations, we propose an adapted RM mechanism that improves the balance between reducing channel load and maintaining VRU awareness. The adapted approach shows a significant reduction in maximum CBR and a less significant decrease in VPR compared to the standardized RM. Moreover, it demonstrates better performance in the actual differences in position, speed, and heading, thereby enhancing overall safety. Our results highlight the need for further research to optimize RM techniques and ensure they effectively enhance V2X communication without compromising the safety of VRUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22052v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nico Ostendorf, Keno Garlichs, Lars Wolf</dc:creator>
    </item>
    <item>
      <title>Unified Memcapacitor-Memristor Memory for Synaptic Weights and Neuron Temporal Dynamics</title>
      <link>https://arxiv.org/abs/2506.22227</link>
      <description>arXiv:2506.22227v1 Announce Type: new 
Abstract: We present a fabricated and experimentally characterized memory stack that unifies memristive and memcapacitive behavior. Exploiting this dual functionality, we design a circuit enabling simultaneous control of spatial and temporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware simulations highlight its promise for efficient neuromorphic processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22227v1</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone D'Agostino, Marco Massarotto, Tristan Torchet, Filippo Moro, Niccol\`o Castellani, Laurent Grenouillet, Yann Beilliard, David Esseni, Melika Payvand, Elisa Vianello</dc:creator>
    </item>
    <item>
      <title>In situ fine-tuning of in silico trained Optical Neural Networks</title>
      <link>https://arxiv.org/abs/2506.22122</link>
      <description>arXiv:2506.22122v1 Announce Type: cross 
Abstract: Optical Neural Networks (ONNs) promise significant advantages over traditional electronic neural networks, including ultrafast computation, high bandwidth, and low energy consumption, by leveraging the intrinsic capabilities of photonics. However, training ONNs poses unique challenges, notably the reliance on simplified in silico models whose trained parameters must subsequently be mapped to physical hardware. This process often introduces inaccuracies due to discrepancies between the idealized digital model and the physical ONN implementation, particularly stemming from noise and fabrication imperfections.
  In this paper, we analyze how noise misspecification during in silico training impacts ONN performance and we introduce Gradient-Informed Fine-Tuning (GIFT), a lightweight algorithm designed to mitigate this performance degradation. GIFT uses gradient information derived from the noise structure of the ONN to adapt pretrained parameters directly in situ, without requiring expensive retraining or complex experimental setups. GIFT comes with formal conditions under which it improves ONN performance.
  We also demonstrate the effectiveness of GIFT via simulation on a five-layer feed forward ONN trained on the MNIST digit classification task. GIFT achieves up to $28\%$ relative accuracy improvement compared to the baseline performance under noise misspecification, without resorting to costly retraining. Overall, GIFT provides a practical solution for bridging the gap between simplified digital models and real-world ONN implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22122v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Kosmella, Ripalta Stabile, Jaron Sanders</dc:creator>
    </item>
    <item>
      <title>On Drug Delivery System Parameter Optimisation via Semantic Information Theory</title>
      <link>https://arxiv.org/abs/2506.22137</link>
      <description>arXiv:2506.22137v1 Announce Type: cross 
Abstract: We investigate the application of semantic information theory to drug delivery systems (DDS) within the molecular communication (MC) framework. To operationalise this, we observe a DDS as a molecular concentration-based channel. Semantic information is defined as the amount of information required for a DDS to achieve its therapeutic goal in a dynamic environment. We derive it by introducing interventions, defined as modifications to DDS parameters, a viability function, and system-environment correlations quantified via the channel capacity. Here, the viability function represents DDS performance based on a drug dose-response relationship. Our model considers a system capable of inducing functional changes in a receiver cancer cell, where exceeding critical DDS parameter values can significantly reduce performance or cost-effectiveness. By analysing the MC-based DDS model through a semantic information perspective, we examine how correlations between the internalised particle concentration $(Y)$ and the particle concentration in the extracellular environment $(X)$ evolve under interventions. The final catalogue of results provides a quantitative basis for DDS design and optimisation, offering a method to determine optimal DDS parameter values under constraints such as chemical budget, desired effect and accuracy. Thus, the proposed framework can serve as a novel tool for guiding DDS design and optimisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22137v1</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>math.IT</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milica Leki\'c, Mohammad Zoofaghari, Ilangko Balasingham, Mladen Veleti\'c</dc:creator>
    </item>
    <item>
      <title>From Data Quality for AI to AI for Data Quality: A Systematic Review of Tools for AI-Augmented Data Quality Management in Data Warehouses</title>
      <link>https://arxiv.org/abs/2406.10940</link>
      <description>arXiv:2406.10940v3 Announce Type: replace-cross 
Abstract: While high data quality (DQ) is critical for analytics, compliance, and AI performance, data quality management (DQM) remains a complex, resource-intensive, and often manual process. This study investigates the extent to which existing tools support AI-augmented data quality management (DQM) in data warehouse environments. To this end, we conduct a systematic review of 151 DQ tools to evaluate their automation capabilities, particularly in detecting and recommending DQ rules in data warehouses -- a key component of modern data ecosystems. Using a multi-phase screening process based on functionality, trialability, regulatory compliance (e.g., GDPR), and architectural compatibility with data warehouses, only 10 tools met the criteria for AI-augmented DQM. The analysis reveals that most tools emphasize data cleansing and preparation for AI, rather than leveraging AI to improve DQ itself. Although metadata- and ML-based rule detection techniques are present, features such as SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations remain scarce. This study offers practical guidance for tool selection and outlines critical design requirements for next-generation AI-driven DQ solutions -- advocating a paradigm shift from ``data quality for AI'' to ``AI for data quality management''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10940v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heidi Carolina Tamm, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Generative AI for Software Architecture. Applications, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2503.13310</link>
      <description>arXiv:2503.13310v2 Announce Type: replace-cross 
Abstract: Context: Generative Artificial Intelligence (GenAI) is transforming much of software development, yet its application in software architecture is still in its infancy, and no prior study has systematically addressed the topic. Aim: We aim to systematically synthesize the use, rationale, contexts, usability, and future challenges of GenAI in software architecture. Method: We performed a multivocal literature review (MLR), analyzing peer-reviewed and gray literature, identifying current practices, models, adoption contexts, and reported challenges, extracting themes via open coding. Results: Our review identified significant adoption of GenAI for architectural decision support and architectural reconstruction. OpenAI GPT models are predominantly applied, and there is consistent use of techniques such as few-shot prompting and retrieved-augmented generation (RAG). GenAI has been applied mostly to initial stages of the Software Development Life Cycle (SDLC), such as Requirements-to-Architecture and Architecture-to-Code. Monolithic and microservice architectures were the dominant targets. However, rigorous testing of GenAI outputs was typically missing from the studies. Among the most frequent challenges are model precision, hallucinations, ethical aspects, privacy issues, lack of architecture-specific datasets, and the absence of sound evaluation frameworks. Conclusions: GenAI shows significant potential in software design, but several challenges remain on its path to greater adoption. Research efforts should target designing general evaluation methodologies, handling ethics and precision, increasing transparency and explainability, and promoting architecture-specific datasets and benchmarks to bridge the gap between theoretical possibilities and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13310v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Xiaozhou Li, Sergio Moreschini, Noman Ahmad, Tomas Cerny, Karthik Vaidhyanathan, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
  </channel>
</rss>
