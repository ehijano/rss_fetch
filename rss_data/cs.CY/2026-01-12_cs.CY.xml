<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 03:30:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The LLM Mirage: Economic Interests and the Subversion of Weaponization Controls</title>
      <link>https://arxiv.org/abs/2601.05307</link>
      <description>arXiv:2601.05307v1 Announce Type: new 
Abstract: U.S. AI security policy is increasingly shaped by an $\textit{LLM Mirage}$, the belief that national security risks scale in proportion to the compute used to train frontier language models. That premise fails in two ways. It miscalibrates strategy because adversaries can obtain weaponizable capabilities with task-specific systems that use specialized data, algorithmic efficiency, and widely available hardware, while compute controls harden only a high-end perimeter. It also destabilizes regulation because, absent a settled definition of "AI weaponization," compute thresholds are easily renegotiated as domestic priorities shift, turning security policy into a proxy contest over industrial competitiveness. We analyze how the LLM Mirage took hold, propose an intent-and-capability definition of AI weaponization grounded in effects and international humanitarian law, and outline measurement infrastructure based on live benchmarks across the full AI Triad (data, algorithms, compute) for weaponization-relevant capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05307v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Gupta, Andrew W. Reddie</dc:creator>
    </item>
    <item>
      <title>Research Integrity and Academic Authority in the Age of Artificial Intelligence: From Discovery to Curation?</title>
      <link>https://arxiv.org/abs/2601.05574</link>
      <description>arXiv:2601.05574v1 Announce Type: new 
Abstract: Artificial intelligence is reshaping the organization and practice of research in ways that extend far beyond gains in productivity. AI systems now accelerate discovery, reorganize scholarly labour, and mediate access to expanding scientific literatures. At the same time, generative models capable of producing text, images, and data at scale introduce new epistemic and institutional vulnerabilities. They exacerbate challenges of reproducibility, blur lines of authorship and accountability, and place unprecedented pressure on peer review and editorial systems. These risks coincide with a deeper political-economic shift: the centre of gravity in AI research has moved decisively from universities to private laboratories with privileged access to data, compute, and engineering talent. As frontier models become increasingly proprietary and opaque, universities face growing difficulty interrogating, reproducing, or contesting the systems on which scientific inquiry increasingly depends.
  This article argues that these developments challenge research integrity and erode traditional bases of academic authority, understood as the institutional capacity to render knowledge credible, contestable, and independent of concentrated power. Rather than competing with corporate laboratories at the technological frontier, universities can sustain their legitimacy by strengthening roles that cannot be readily automated or commercialized: exercising judgement over research quality in an environment saturated with synthetic outputs; curating the provenance, transparency, and reproducibility of knowledge; and acting as ethical and epistemic counterweights to private interests. In an era of informational abundance, the future authority of universities lies less in maximizing discovery alone than in sustaining the institutional conditions under which knowledge can be trusted and publicly valued.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05574v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Chesterman, Loy Hui Chieh</dc:creator>
    </item>
    <item>
      <title>Cross-National Evidence of Disproportionate Media Visibility for the Radical Right in the 2024 European Elections</title>
      <link>https://arxiv.org/abs/2601.05826</link>
      <description>arXiv:2601.05826v1 Announce Type: new 
Abstract: This study provides a systematic comparative analysis of media visibility of different political families during the 2024 European Parliament elections. We analyzed close to 21,500 unique news from leading national outlets in Austria, Germany, Ireland, Poland, and Portugal - countries with diverse political contexts and levels of media trust. Combining computational and human classification, we identified parties, political leaders, and groups from the article's URLs and titles, and clustered them according to European Parliament political families and broad political leanings. Cross-country comparison shows that the Mainstream and the Radical Right were mentioned more often than the other political groups. Moreover, the Radical Right received disproportionate attention relative to electoral results (from 2019 or 2024) and electoral projections, particularly in Austria, Germany, and Ireland. This imbalance increased in the final weeks of the campaign, when media influence on undecided voters is greatest. Outlet-level analysis shows that coverage of right-leaning entities dominated across news sources, especially those generating the highest traffic, suggesting a structural rather than outlet-specific pattern. Media visibility is a central resource, and this systematic mapping of online coverage highlights how traditional media can contribute to structural asymmetries in democratic competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05826v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Iris Dami\~ao, Jo\~ao Franco, Mariana Silva, Paulo Almeida, Pedro C. Magalh\~aes, Joana Gon\c{c}alves-S\'a</dc:creator>
    </item>
    <item>
      <title>Can AI mediation improve democratic deliberation?</title>
      <link>https://arxiv.org/abs/2601.05904</link>
      <description>arXiv:2601.05904v1 Announce Type: new 
Abstract: The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this "trilemma" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05904v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Knight Institute for the First Amendment at Columbia University Symposium on "AI and Democratic Freedoms", April 10-11, 2025</arxiv:journal_reference>
      <dc:creator>Michael Henry Tessler, Georgina Evans, Michiel A. Bakker, Iason Gabriel, Sophie Bridgers, Rishub Jain, Raphael Koster, Verena Rieser, Anca Dragan, Matthew Botvinick, Christopher Summerfield</dc:creator>
    </item>
    <item>
      <title>Navigating the Sociotechnical Imaginaries of Brazilian Tech Workers</title>
      <link>https://arxiv.org/abs/2601.05961</link>
      <description>arXiv:2601.05961v1 Announce Type: new 
Abstract: This chapter examines the sociotechnical imaginaries of Brazilian tech workers, a group often overlooked in digital labor research despite their role in designing the digital systems that shape everyday life. Grounded in the idea of sociotechnical imaginaries as collectively constructed visions that guide technology development and governance, the chapter argues that looking from the Global South helps challenge data universalism and foregrounds locally situated values, constraints, and futures. Drawing on semi-structured interviews with 26 Brazilian professionals conducted between July and December 2023, it maps how workers make sense of responsibility, bias, and power in AI and platform development. The findings highlight recurring tensions between academic and industry discourse on algorithmic bias, the limits of corporate accountability regarding user harm and surveillance, and the contested meanings of digital sovereignty, including grassroots initiatives that seek alternative technological futures aligned with marginalized communities needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05961v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Ergin Bulut, Julie Chen, Rafael Grohmann, and Kylie Jarrett (eds.) (2025). SAGE Handbook of Digital Labour. SAGE: 314-324.9781529669831</arxiv:journal_reference>
      <dc:creator>Kenzo Soares Seto</dc:creator>
    </item>
    <item>
      <title>The Causal Effect of First-Time Academic Failure on University Dropout: Evidence from a Regression Discontinuity Design</title>
      <link>https://arxiv.org/abs/2601.05987</link>
      <description>arXiv:2601.05987v1 Announce Type: new 
Abstract: University dropout remains a persistent challenge in higher education systems, yet causal evidence on the mechanisms triggering early disengagement is limited. This study estimates the causal effect of first-time academic failure on subsequent university attrition. Exploiting a sharp institutional grading threshold on a 0-10 scale, we implement a regression discontinuity design (RDD) comparing students who narrowly fail to those who narrowly pass their first attempt. Using longitudinal administrative data spanning multiple cohorts and degree programmes, we estimate local average treatment effects (LATE) for students at the margin of success and examine dropout outcomes within 12 and 24 months following the initial evaluation. Contrary to conventional assumptions, the results indicate that marginal first-time failure is associated with a lower probability of subsequent dropout relative to marginal passing at both horizons. A comprehensive battery of robustness checks - including donut RDD specifications, placebo cutoffs, and formal density tests - supports the validity of the identification strategy. These findings suggest that early academic failure may function as a salient signal that prompts behavioural adjustment or reorientation, while marginal passing may sustain a state of "fragile persistence". The study provides causal evidence on the non-linear effects of early academic performance and highlights the importance of carefully designed institutional responses at critical evaluation thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05987v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. R. Paz</dc:creator>
    </item>
    <item>
      <title>The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques</title>
      <link>https://arxiv.org/abs/2601.05358</link>
      <description>arXiv:2601.05358v1 Announce Type: cross 
Abstract: Public debates about "left-" or "right-wing" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a "table of media-bias elements". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05358v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Menzner, Jochen L. Leidner</dc:creator>
    </item>
    <item>
      <title>Conformity and Social Impact on AI Agents</title>
      <link>https://arxiv.org/abs/2601.05384</link>
      <description>arXiv:2601.05384v1 Announce Type: cross 
Abstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05384v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alessandro Bellina, Giordano De Marzo, David Garcia</dc:creator>
    </item>
    <item>
      <title>On the Transition to an Auction-based Intelligent Parking Assignment System</title>
      <link>https://arxiv.org/abs/2601.05429</link>
      <description>arXiv:2601.05429v1 Announce Type: cross 
Abstract: Finding a free parking space in a city has become a challenging task over the past decades. A recently proposed auction-based parking assignment can alleviate cruising for parking and also set a market-driven, demand-responsive parking price. However, the wide acceptance of such a system is far from certain.
  To evaluate the merits of auction-based parking assignment, we assume that drivers have access to a smartphone-based reservation system prior to its mandatory introduction and thus have the opportunity to test and experience its merits voluntarily. We set our experiment as Eclipse SUMO simulations with different rates of participants and non-participants to check how different market penetration levels affect the traffic flow, the performance of the auction-based assignment system, and the financial outcomes. The results show that the auction-based system improves traffic flow with increasing penetration rates, allowing participants to park gradually closer to their preferred parking lots. However, it comes with a price; the system also increases parking expenditures for participants. Interestingly, non-participating drivers will face even higher parking prices. Consequently, they will be motivated to use the new system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05429v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Levente Alekszejenk\'o, Dobrowiecki Tadeusz</dc:creator>
    </item>
    <item>
      <title>Good Allocations from Bad Estimates</title>
      <link>https://arxiv.org/abs/2601.05597</link>
      <description>arXiv:2601.05597v1 Announce Type: cross 
Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $\epsilon &gt; 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/\epsilon^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $\epsilon$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/\epsilon)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05597v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ilvia Casacuberta, Moritz Hardt</dc:creator>
    </item>
    <item>
      <title>Advancing credit mobility through stakeholder-informed AI design and adoption</title>
      <link>https://arxiv.org/abs/2601.05666</link>
      <description>arXiv:2601.05666v1 Announce Type: cross 
Abstract: Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05666v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yerin Kwak, Siddharth Adelkar, Zachary A. Pardos</dc:creator>
    </item>
    <item>
      <title>Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law</title>
      <link>https://arxiv.org/abs/2601.05879</link>
      <description>arXiv:2601.05879v1 Announce Type: cross 
Abstract: Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05879v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Harasta, Matej Vasina, Martin Kornel, Tomas Foltynek</dc:creator>
    </item>
    <item>
      <title>Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates</title>
      <link>https://arxiv.org/abs/2601.05909</link>
      <description>arXiv:2601.05909v1 Announce Type: cross 
Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05909v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayoub Ajarra, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset</title>
      <link>https://arxiv.org/abs/2601.05918</link>
      <description>arXiv:2601.05918v1 Announce Type: cross 
Abstract: On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05918v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tianshi Li</dc:creator>
    </item>
    <item>
      <title>AI-Educational Development Loop (AI-EDL): A Conceptual Framework to Bridge AI Capabilities with Classical Educational Theories</title>
      <link>https://arxiv.org/abs/2508.00970</link>
      <description>arXiv:2508.00970v2 Announce Type: replace 
Abstract: This study introduces the AI-Educational Development Loop (AI-EDL), a theory-driven framework that integrates classical learning theories with human-in-the-loop artificial intelligence (AI) to support reflective, iterative learning. Implemented in EduAlly, an AI-assisted platform for writing-intensive and feedback-sensitive tasks, the framework emphasizes transparency, self-regulated learning, and pedagogical oversight. A mixed-methods study was piloted at a comprehensive public university to evaluate alignment between AI-generated feedback, instructor evaluations, and student self-assessments; the impact of iterative revision on performance; and student perceptions of AI feedback. Quantitative results demonstrated statistically significant improvement between first and second attempts, with agreement between student self-evaluations and final instructor grades. Qualitative findings indicated students valued immediacy, specificity, and opportunities for growth that AI feedback provided. These findings validate the potential to enhance student learning outcomes through developmentally grounded, ethically aligned, and scalable AI feedback systems. The study concludes with implications for future interdisciplinary applications and refinement of AI-supported educational technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00970v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ning Yu, Jie Zhang, Sandeep Mitra, Rebecca Smith, Adam Rich</dc:creator>
    </item>
    <item>
      <title>Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy</title>
      <link>https://arxiv.org/abs/2508.04281</link>
      <description>arXiv:2508.04281v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are gaining traction as a method to generate consensus statements and aggregate preferences in digital democracy experiments. Yet, LLMs could introduce critical vulnerabilities in these systems. Here, we examine the vulnerability and robustness of off-the-shelf consensus-generating LLMs to prompt-injection attacks, in which texts are injected to amplify particular viewpoints, erase certain opinions, or divert consensus toward unrelated or irrelevant topics. We construct attack-free and adversarial variants of prompts containing public policy questions and opinion texts, classify opinion and consensus valences with a fine-tuned BERT model, and estimate Attack Success Rates (ASR) from $3\times3$ confusion matrices conditional on matching human majorities. Across topics, default LLaMA 3.1 8B Instruct, GPT-4.1 Nano, and Apertus 8B exhibit widespread vulnerability, with especially high ASR for economically and socially conservative parties and for rational, instruction-like rhetorical strategies. A robustness pipeline combining GPT-OSS-SafeGuard injection detection, structured opinion representations, and GSPO-based reinforcement learning reduces ASR to near zero across parties and policy clusters when restricting attention to non-ambiguous consensus outcomes. These findings advance our understanding of both the vulnerabilities and the potential defenses of consensus-generating LLMs in digital democracy applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04281v3</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jairo Gudi\~no-Rosero, Cl\'ement Contet, Umberto Grandi, C\'esar A. Hidalgo</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approach to Capitation Reform in Rwanda</title>
      <link>https://arxiv.org/abs/2510.21851</link>
      <description>arXiv:2510.21851v2 Announce Type: replace 
Abstract: As part of Rwanda's transition toward universal health coverage, the national Community-Based Health Insurance (CBHI) scheme is moving from retrospective fee-for-service reimbursements to prospective capitation payments for public primary healthcare providers. This work outlines a data-driven approach to designing, calibrating, and monitoring the capitation model using individual-level claims data from the Intelligent Health Benefits System (IHBS). We introduce a transparent, interpretable formula for allocating payments to Health Centers and their affiliated Health Posts. The formula is based on catchment population, service utilization patterns, and patient inflows, with parameters estimated via regression models calibrated on national claims data. Repeated validation exercises show the payment scheme closely aligns with historical spending while promoting fairness and adaptability across diverse facilities. In addition to payment design, the same dataset enables actionable behavioral insights. We highlight the use case of monitoring antibiotic prescribing patterns, particularly in pediatric care, to flag potential overuse and guideline deviations. Together, these capabilities lay the groundwork for a learning health financing system: one that connects digital infrastructure, resource allocation, and service quality to support continuous improvement and evidence-informed policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21851v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babaniyi Olaniyi, Ina Kalisa, Ana Fern\'andez del R\'io, Jean Marie Vianney Hakizayezu, Enric Jan\'e, Eniola Olaleye, Juan Francisco Garamendi, Ivan Nazarov, Aditya Rastogi, Mateo Diaz-Quiroz, \'Africa Peri\'a\~nez, Regis Hitimana</dc:creator>
    </item>
    <item>
      <title>The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning</title>
      <link>https://arxiv.org/abs/2601.04094</link>
      <description>arXiv:2601.04094v2 Announce Type: replace 
Abstract: The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of the regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders -- ranging from the EU Commission at the macro level to AI developers at the micro level -- within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes (AITSes) as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04094v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Deckenbrunnen, Alessio Buscemi, Marco Almada, Alfredo Capozucca, German Castignani</dc:creator>
    </item>
    <item>
      <title>Neither Consent nor Property: A Policy Lab for Data Law</title>
      <link>https://arxiv.org/abs/2510.26727</link>
      <description>arXiv:2510.26727v2 Announce Type: replace-cross 
Abstract: Regulators currently govern the AI data economy based on intuition rather than evidence, struggling to choose between inconsistent regimes of informed consent, immunity, and liability. To fill this policy vacuum, this paper develops a novel computational policy laboratory: a spatially explicit Agent-Based Model (ABM) of the data market. To solve the problem of missing data, we introduce a two-stage methodological pipeline. First, we translate decision rules from multi-year fieldwork (2022-2025) into agent constraints. This ensures the model reflects actual bargaining frictions rather than theoretical abstractions. Second, we deploy Large Language Models (LLMs) as "subjects" in a Discrete Choice Experiment (DCE). This novel approach recovers precise preference primitives, such as willingness-to-pay elasticities, which are empirically unobservable in the wild. Calibrated by these inputs, our model places rival legal institutions side-by-side to simulate their welfare effects. The results challenge the dominant regulatory paradigm. We find that property-rule mechanisms, such as informed consent, fail to maximize welfare. Counterintuitively, social welfare peaks when liability for substantive harm is shifted to the downstream buyer. This aligns with the "least cost avoider" principle, because downstream users control post-acquisition safeguards, they are best positioned to mitigate risk efficiently. By "de-romanticizing" seller-centric frameworks, this paper provides an economic justification for emerging doctrines of downstream reachability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26727v2</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyi Zhang, Tianyi Zhu</dc:creator>
    </item>
    <item>
      <title>MajinBook: An open catalogue of digital world literature with likes</title>
      <link>https://arxiv.org/abs/2511.11412</link>
      <description>arXiv:2511.11412v4 Announce Type: replace-cross 
Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11412v4</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.OT</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Mazi\`eres, Thierry Poibeau</dc:creator>
    </item>
    <item>
      <title>Measuring Memecoin Fragility</title>
      <link>https://arxiv.org/abs/2512.00377</link>
      <description>arXiv:2512.00377v2 Announce Type: replace-cross 
Abstract: Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows. To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability. We apply ME2F to representative tokens (over 65% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00377v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuexin Xiang, Qishuang Fu, Yuquan Li, Qin Wang, Tsz Hon Yuen, Jiangshan Yu</dc:creator>
    </item>
  </channel>
</rss>
