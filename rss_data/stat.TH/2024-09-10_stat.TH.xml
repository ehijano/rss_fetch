<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:49:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Precise Asymptotics for Linear Mixed Models with Crossed Random Effects</title>
      <link>https://arxiv.org/abs/2409.05066</link>
      <description>arXiv:2409.05066v1 Announce Type: new 
Abstract: We obtain an asymptotic normality result that reveals the precise asymptotic behavior of the maximum likelihood estimators of parameters for a very general class of linear mixed models containing cross random effects. In achieving the result, we overcome theoretical difficulties that arise from random effects being crossed as opposed to the simpler nested random effects case. Our new theory is for a class of Gaussian response linear mixed models which includes crossed random slopes that partner arbitrary multivariate predictor effects and does not require the cell counts to be balanced. Statistical utilities include confidence interval construction, Wald hypothesis test and sample size calculations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05066v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiming Jiang, Matt P. Wand, Swarnadip Ghosh</dc:creator>
    </item>
    <item>
      <title>Common or specific source, features or scores; it is all a matter of information</title>
      <link>https://arxiv.org/abs/2409.05403</link>
      <description>arXiv:2409.05403v1 Announce Type: new 
Abstract: We show that the incorporation of any new piece of information allows for improved decision making in the sense that the expected costs of an optimal decision decrease (or, in boundary cases where no or not enough new information is incorporated, stays the same) whenever this is done by the appropriate update of the probabilities of the hypotheses. Versions of this result have been stated before. However, previous proofs rely on auxiliary constructions with proper scoring rules. We, instead, offer a direct and completely general proof by considering elementary properties of likelihood ratios only. We do point out the relation to proper scoring rules. We apply our results to make a contribution to the debates about the use of score based/feature based and common/specific source likelihood ratios. In the literature these are often presented as different ``LR-systems''. We argue that deciding which LR to compute is simply a matter of the available information. There is no such thing as different ``LR-systems'', there are only differences in the available information. In particular, despite claims to the contrary, scores can very well be used in forensic practice and we illustrate this with an extensive example in DNA kinship context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05403v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aafko Boonstra, Ronald Meester, Klaas Slooten</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for fractional stochastic heat equations : Berry-Ess\'een bounds in CLTs</title>
      <link>https://arxiv.org/abs/2409.05416</link>
      <description>arXiv:2409.05416v1 Announce Type: new 
Abstract: The aim of this work is to estimate the drift coefficient of a fractional heat equation driven by an additive space-time noise using the Maximum likelihood estimator (MLE). In the first part of the paper, the first $N$ Fourier modes of the solution are observed continuously over a finite time interval $[0, T ]$. The explicit upper bounds for the Wasserstein distance for the central limit theorem of the MLE is provided when $N \rightarrow \infty$ and/or $T \rightarrow \infty$. While in the second part of the paper, the $N$ Fourier modes are observed at uniform time grid : $t_i = i \frac{T}{M}$, $i=0,..,M,$ where $M$ is the number of time grid points. The consistency and asymptotic normality are studied when $T,M,N \rightarrow + \infty$ in addition to the rate of convergence in law in the CLT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05416v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soukaina Douissi, Fatimah Alshahrani</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Estimators based on the Block Maxima Method</title>
      <link>https://arxiv.org/abs/2409.05529</link>
      <description>arXiv:2409.05529v1 Announce Type: new 
Abstract: The block maxima method is a standard approach for analyzing the extremal behavior of a potentially multivariate time series. It has recently been found that the classical approach based on disjoint block maxima may be universally improved by considering sliding block maxima instead. However, the asymptotic variance formula for estimators based on sliding block maxima involves an integral over the covariance of a certain family of multivariate extreme value distributions, which makes its estimation, and inference in general, an intricate problem. As an alternative, one may rely on bootstrap approximations: we show that naive block-bootstrap approaches from time series analysis are inconsistent even in i.i.d.\ situations, and provide a consistent alternative based on resampling circular block maxima. As a by-product, we show consistency of the classical resampling bootstrap for disjoint block maxima, and that estimators based on circular block maxima have the same asymptotic variance as their sliding block maxima counterparts. The finite sample properties are illustrated by Monte Carlo experiments, and the methods are demonstrated by a case study of precipitation extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05529v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Torben Staud</dc:creator>
    </item>
    <item>
      <title>Empirical likelihood for generalized smoothly trimmed mean</title>
      <link>https://arxiv.org/abs/2409.05631</link>
      <description>arXiv:2409.05631v1 Announce Type: new 
Abstract: This paper introduces a new version of the smoothly trimmed mean with a more general version of weights, which can be used as an alternative to the classical trimmed mean. We derive its asymptotic variance and to further investigate its properties we establish the empirical likelihood for the new estimator. As expected from previous theoretical investigations we show in our simulations a clear advantage of the proposed estimator over the classical trimmed mean estimator. Moreover, the empirical likelihood method gives an additional advantage for data generated from contaminated models. For the classical trimmed mean it is generally recommended in practice to use symmetrical 10\% or 20\% trimming. However, if the trimming is done close to data gaps, it can even lead to spurious results, as known from the literature and verified by our simulations. Instead, for practical data examples, we choose the smoothing parameters by an optimality criterion that minimises the variance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05631v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elina Kresse, Emils Silins, Janis Valeinis</dc:creator>
    </item>
    <item>
      <title>A Continuous Generalization of Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2409.05654</link>
      <description>arXiv:2409.05654v1 Announce Type: new 
Abstract: Testing has developed into the fundamental statistical framework for falsifying hypotheses. Unfortunately, tests are binary in nature: a test either rejects a hypothesis or not. Such binary decisions do not reflect the reality of many scientific studies, which often aim to present the evidence against a hypothesis and do not necessarily intend to establish a definitive conclusion. To solve this, we propose the continuous generalization of a test, which we use to measure the evidence against a hypothesis. Such a continuous test can be interpreted as a non-randomized interpretation of the classical 'randomized test'. This offers the benefits of a randomized test, without the downsides of external randomization. Another interpretation is as a literal measure, which measures the amount of binary tests that reject the hypothesis. Our work also offers a new perspective on the $e$-value: the $e$-value is recovered as a continuous test with $\alpha \to 0$, or as an unbounded measure of the amount of rejections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05654v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>Uniform Estimation and Inference for Nonparametric Partitioning-Based M-Estimators</title>
      <link>https://arxiv.org/abs/2409.05715</link>
      <description>arXiv:2409.05715v1 Announce Type: new 
Abstract: This paper presents uniform estimation and inference theory for a large class of nonparametric partitioning-based M-estimators. The main theoretical results include: (i) uniform consistency for convex and non-convex objective functions; (ii) optimal uniform Bahadur representations; (iii) optimal uniform (and mean square) convergence rates; (iv) valid strong approximations and feasible uniform inference methods; and (v) extensions to functional transformations of underlying estimators. Uniformity is established over both the evaluation point of the nonparametric functional parameter and a Euclidean parameter indexing the class of loss functions. The results also account explicitly for the smoothness degree of the loss function (if any), and allow for a possibly non-identity (inverse) link function. We illustrate the main theoretical and methodological results with four substantive applications: quantile regression, distribution regression, $L_p$ regression, and Logistic regression; many other possibly non-smooth, nonlinear, generalized, robust M-estimation settings are covered by our theoretical results. We provide detailed comparisons with the existing literature and demonstrate substantive improvements: we achieve the best (in some cases optimal) known results under improved (in some cases minimal) requirements in terms of regularity conditions and side rate restrictions. The supplemental appendix reports other technical results that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05715v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Boris Shigida</dc:creator>
    </item>
    <item>
      <title>Efficient estimation with incomplete data via generalised ANOVA decomposition</title>
      <link>https://arxiv.org/abs/2409.05729</link>
      <description>arXiv:2409.05729v1 Announce Type: new 
Abstract: We study the efficient estimation of a class of mean functionals in settings where a complete multivariate dataset is complemented by additional datasets recording subsets of the variables of interest. These datasets are allowed to have a general, in particular non-monotonic, structure. Our main contribution is to characterise the asymptotic minimal mean squared error for these problems and to introduce an estimator whose risk approximately matches this lower bound. We show that the efficient rescaled variance can be expressed as the minimal value of a quadratic optimisation problem over a function space, thus establishing a fundamental link between these estimation problems and the theory of generalised ANOVA decompositions. Our estimation procedure uses iterated nonparametric regression to mimic an approximate influence function derived through gradient descent. We prove that this estimator is approximately normally distributed, provide an estimator of its variance and thus develop confidence intervals of asymptotically minimal width. Finally we study a more direct estimator, which can be seen as a U-statistic with a data-dependent kernel, showing that it is also efficient under stronger regularity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05729v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas B. Berrett</dc:creator>
    </item>
    <item>
      <title>Markov Chain Variance Estimation: A Stochastic Approximation Approach</title>
      <link>https://arxiv.org/abs/2409.05733</link>
      <description>arXiv:2409.05733v1 Announce Type: new 
Abstract: We consider the problem of estimating the asymptotic variance of a function defined on a Markov chain, an important step for statistical inference of the stationary mean. We design the first recursive estimator that requires $O(1)$ computation at each step, does not require storing any historical samples or any prior knowledge of run-length, and has optimal $O(\frac{1}{n})$ rate of convergence for the mean-squared error (MSE) with provable finite sample guarantees. Here, $n$ refers to the total number of samples generated. The previously best-known rate of convergence in MSE was $O(\frac{\log n}{n})$, achieved by jackknifed estimators, which also do not enjoy these other desirable properties. Our estimator is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation.
  We generalize our estimator in several directions, including estimating the covariance matrix for vector-valued functions, estimating the stationary variance of a Markov chain, and approximately estimating the asymptotic variance in settings where the state space of the underlying Markov chain is large. We also show applications of our estimator in average reward reinforcement learning (RL), where we work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference type algorithm tailored for policy evaluation in this context. We consider both the tabular and linear function approximation settings. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05733v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhada Agrawal, Prashanth L. A., Siva Theja Maguluri</dc:creator>
    </item>
    <item>
      <title>On integer partitions and the Wilcoxon rank-sum statistic</title>
      <link>https://arxiv.org/abs/2409.05741</link>
      <description>arXiv:2409.05741v1 Announce Type: new 
Abstract: In the literature, derivations of exact null distributions of rank-sum statistics is often avoided in cases where one or more ties exist in the data. By deriving the null distribution in the no-ties case with the aid of classical $q$-series results of Euler and Rothe, we demonstrate how a natural generalization of the method may be employed to derive exact null distributions even when one or more ties are present in the data. It is suggested that this method could be implemented in a computer algebra system, or even a more primitive computer language, so that the normal approximation need not be employed in the case of small sample sizes, when it is less likely to be very accurate. Several algorithms for determining exact distributions of the rank-sum statistic (possibly with ties) have been given in the literature (see Streitberg and R\"ohmel (1986) and Marx et al. (2016)), but none seem as simple as the procedure discussed here which amounts to multiplying out a certain polynomial, extracting coefficients, and finally dividing by a binomal coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05741v1</guid>
      <category>math.ST</category>
      <category>math.NT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/03610926.2024.2315297</arxiv:DOI>
      <dc:creator>Andrew V. Sills</dc:creator>
    </item>
    <item>
      <title>Jackknife Empirical Likelihood Ratio Test for Cauchy Distribution</title>
      <link>https://arxiv.org/abs/2409.05764</link>
      <description>arXiv:2409.05764v1 Announce Type: new 
Abstract: Heavy-tailed distributions, such as the Cauchy distribution, are acknowledged for providing more accurate models for financial returns, as the normal distribution is deemed insufficient for capturing the significant fluctuations observed in real-world assets. Data sets characterized by outlier sensitivity are critically important in diverse areas, including finance, economics, telecommunications, and signal processing. This article addresses a goodness-of-fit test for the Cauchy distribution. The proposed test utilizes empirical likelihood methods, including the jackknife empirical likelihood (JEL) and adjusted jackknife empirical likelihood (AJEL). Extensive Monte Carlo simulation studies are conducted to evaluate the finite sample performance of the proposed test. The application of the proposed test is illustrated through the analysing two real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05764v1</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Avhad Ganesh Vishnu, Ananya Lahiri, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Privacy enhanced collaborative inference in the Cox proportional hazards model for distributed data</title>
      <link>https://arxiv.org/abs/2409.04716</link>
      <description>arXiv:2409.04716v1 Announce Type: cross 
Abstract: Data sharing barriers are paramount challenges arising from multicenter clinical studies where multiple data sources are stored in a distributed fashion at different local study sites. Particularly in the case of time-to-event analysis when global risk sets are needed for the Cox proportional hazards model, access to a centralized database is typically necessary. Merging such data sources into a common data storage for a centralized statistical analysis requires a data use agreement, which is often time-consuming. Furthermore, the construction and distribution of risk sets to participating clinical centers for subsequent calculations may pose a risk of revealing individual-level information. We propose a new collaborative Cox model that eliminates the need for accessing the centralized database and constructing global risk sets but needs only the sharing of summary statistics with significantly smaller dimensions than risk sets. Thus, the proposed collaborative inference enjoys maximal protection of data privacy. We show theoretically and numerically that the new distributed proportional hazards model approach has little loss of statistical power when compared to the centralized method that requires merging the entire data. We present a renewable sieve method to establish large-sample properties for the proposed method. We illustrate its performance through simulation experiments and a real-world data example from patients with kidney transplantation in the Organ Procurement and Transplantation Network (OPTN) to understand the factors associated with the 5-year death-censored graft failure (DCGF) for patients who underwent kidney transplants in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04716v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengtong Hu, Xu Shi, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Exact exponential tail estimation for sums of independent centered random variables, under natural norming, with applications to the theory of U-statistics</title>
      <link>https://arxiv.org/abs/2409.05083</link>
      <description>arXiv:2409.05083v1 Announce Type: cross 
Abstract: We derive in this short report the exact exponential decreasing tail of distribution for naturel normed sums of independent centered random variables (r.v.), applying the theory of Grand Lebesgue Spaces (GLS). We consider also some applications into the theory of U statistics, where we deduce alike for the independent variables the refined exponential tail estimates for ones under natural norming sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05083v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. R. Formica, E. Ostrovsky, L. Sirota</dc:creator>
    </item>
    <item>
      <title>Estimation and inference for the Wasserstein distance between mixing measures in topic models</title>
      <link>https://arxiv.org/abs/2206.12768</link>
      <description>arXiv:2206.12768v3 Announce Type: replace 
Abstract: The Wasserstein distance between mixing measures has come to occupy a central place in the statistical analysis of mixture models. This work proposes a new canonical interpretation of this distance and provides tools to perform inference on the Wasserstein distance between mixing measures in topic models. We consider the general setting of an identifiable mixture model consisting of mixtures of distributions from a set $\mathcal{A}$ equipped with an arbitrary metric $d$, and show that the Wasserstein distance between mixing measures is uniquely characterized as the most discriminative convex extension of the metric $d$ to the set of mixtures of elements of $\mathcal{A}$. The Wasserstein distance between mixing measures has been widely used in the study of such models, but without axiomatic justification. Our results establish this metric to be a canonical choice. Specializing our results to topic models, we consider estimation and inference of this distance. Though upper bounds for its estimation have been recently established elsewhere, we prove the first minimax lower bounds for the estimation of the Wasserstein distance in topic models. We also establish fully data-driven inferential tools for the Wasserstein distance in the topic model context. Our results apply to potentially sparse mixtures of high-dimensional discrete probability distributions. These results allow us to obtain the first asymptotically valid confidence intervals for the Wasserstein distance in topic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12768v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Florentina Bunea, Jonathan Niles-Weed</dc:creator>
    </item>
    <item>
      <title>Fractionally integrated curve time series with cointegration</title>
      <link>https://arxiv.org/abs/2212.04071</link>
      <description>arXiv:2212.04071v3 Announce Type: replace 
Abstract: We introduce methods and theory for fractionally cointegrated curve time series. We develop a variance-ratio test to determine the dimensions associated with the nonstationary and stationary subspaces. For each subspace, we apply a local Whittle estimator to estimate the long-memory parameter and establish its consistency. A Monte Carlo study of finite-sample performance is included, along with two empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04071v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won-Ki Seo, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>On Closed-Form Expressions for the Fisher-Rao Distance</title>
      <link>https://arxiv.org/abs/2304.14885</link>
      <description>arXiv:2304.14885v3 Announce Type: replace 
Abstract: The Fisher-Rao distance is the geodesic distance between probability distributions in a statistical manifold equipped with the Fisher metric, which is a natural choice of Riemannian metric on such manifolds. It has recently been applied to supervised and unsupervised problems in machine learning, in various contexts. Finding closed-form expressions for the Fisher-Rao distance is generally a non-trivial task, and those are only available for a few families of probability distributions. In this survey, we collect examples of closed-form expressions for the Fisher-Rao distance of both discrete and continuous distributions, aiming to present them in a unified and accessible language. In doing so, we also: illustrate the relation between negative multinomial distributions and the hyperbolic model, include a few new examples, and write a few more in the standard form of elliptical distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.14885v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrique K. Miyamoto, F\'abio C. C. Meneghetti, Julianna Pinele, Sueli I. R. Costa</dc:creator>
    </item>
    <item>
      <title>Intrinsic Bayesian Cram\'er-Rao Bound with an Application to Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2311.04748</link>
      <description>arXiv:2311.04748v3 Announce Type: replace 
Abstract: This paper presents a new performance bound for estimation problems where the parameter to estimate lies in a Riemannian manifold (a smooth manifold endowed with a Riemannian metric) and follows a given prior distribution. In this setup, the chosen Riemannian metric induces a geometry for the parameter manifold, as well as an intrinsic notion of the estimation error measure. Performance bound for such error measure were previously obtained in the non-Bayesian case (when the unknown parameter is assumed to deterministic), and referred to as \textit{intrinsic} Cram\'er-Rao bound. The presented result then appears either as: \textit{a}) an extension of the intrinsic Cram\'er-Rao bound to the Bayesian estimation framework; \textit{b}) a generalization of the Van-Trees inequality (Bayesian Cram\'er-Rao bound) that accounts for the aforementioned geometric structures. In a second part, we leverage this formalism to study the problem of covariance matrix estimation when the data follow a Gaussian distribution, and whose covariance matrix is drawn from an inverse Wishart distribution. Performance bounds for this problem are obtained for both the mean squared error (Euclidean metric) and the natural Riemannian distance for Hermitian positive definite matrices (affine invariant metric). Numerical simulation illustrate that assessing the error with the affine invariant metric is revealing of interesting properties of the maximum a posteriori and minimum mean square error estimator, which are not observed when using the Euclidean metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04748v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v4 Announce Type: replace 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood estimation in the ergodic Volterra Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2404.05554</link>
      <description>arXiv:2404.05554v2 Announce Type: replace 
Abstract: We study statistical inference of the drift parameters for the Volterra Ornstein-Uhlenbeck process on R in the ergodic regime. For continuous-time observations, we derive the corresponding maximum likelihood estimators and show that they are strongly consistent and asymptotically normal locally uniformly in the parameters. For the case of discrete high-frequency observations, we prove similar results by discretization of the continuous-time maximum likelihood estimator. Finally, for discrete low-frequency observations, we show that the method of moments is consistent. Our proofs are crucially based on the law of large numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05554v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Martin Friesen, Jonas Kremer</dc:creator>
    </item>
    <item>
      <title>Estimating odds and log odds with guaranteed accuracy</title>
      <link>https://arxiv.org/abs/2404.17705</link>
      <description>arXiv:2404.17705v2 Announce Type: replace 
Abstract: Two sequential estimators are proposed for the odds p/(1-p) and log odds log(p/(1-p)) respectively, using independent Bernoulli random variables with parameter p as inputs. The estimators are unbiased, and guarantee that the variance of the estimation error divided by the true value of the odds, or the variance of the estimation error of the log odds, are less than a target value for any p in (0,1). The estimators are close to optimal in the sense of Wolfowitz's bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17705v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Mendo</dc:creator>
    </item>
    <item>
      <title>Weak-instrument-robust subvector inference in instrumental variables regression: A subvector Lagrange multiplier test and properties of subvector Anderson-Rubin confidence sets</title>
      <link>https://arxiv.org/abs/2407.15256</link>
      <description>arXiv:2407.15256v2 Announce Type: replace 
Abstract: We propose a weak-instrument-robust subvector Lagrange multiplier test for instrumental variables regression. We show that it is asymptotically size-correct under a technical condition. This is the first weak-instrument-robust subvector test for instrumental variables regression to recover the degrees of freedom of the commonly used non-weak-instrument-robust Wald test. Additionally, we provide a closed-form solution for subvector confidence sets obtained by inverting the subvector Anderson-Rubin test. We show that they are centered around a k-class estimator. Also, we show that the subvector confidence sets for single coefficients of the causal parameter are jointly bounded if and only if Anderson's likelihood-ratio test rejects the hypothesis that the first-stage regression parameter is of reduced rank, that is, that the causal parameter is not identified. Finally, we show that if a confidence set obtained by inverting the Anderson-Rubin test is bounded and nonempty, it is equal to a Wald-based confidence set with a data-dependent confidence level. We explicitly compute this Wald-based confidence test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15256v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Londschien, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Approximate independence of permutation mixtures</title>
      <link>https://arxiv.org/abs/2408.09341</link>
      <description>arXiv:2408.09341v2 Announce Type: replace 
Abstract: We prove bounds on statistical distances between high-dimensional exchangeable mixture distributions (which we call permutation mixtures) and their i.i.d. counterparts. Our results are based on a novel method for controlling $\chi^2$ divergences between exchangeable mixtures, which is tighter than the existing methods of moments or cumulants. At a technical level, a key innovation in our proofs is a new Maclaurin-type inequality for elementary symmetric polynomials of variables that sum to zero and an upper bound on permanents of doubly-stochastic positive semidefinite matrices. Our results imply a de Finetti-style theorem (in the language of Diaconis and Freedman, 1987) and general asymptotic results for compound decision problems, generalizing and strengthening a result of Hannan and Robbins (1955).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09341v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjun Han, Jonathan Niles-Weed</dc:creator>
    </item>
    <item>
      <title>Local Fr\'echet regression with circular predictors</title>
      <link>https://arxiv.org/abs/2408.10118</link>
      <description>arXiv:2408.10118v3 Announce Type: replace 
Abstract: Fr\'echet regression extends the principles of linear regression to accommodate responses valued in generic metric spaces. While this approach has primarily focused on exploring relationships between Euclidean predictors and non-Euclidean responses, our work introduces a novel statistical method for handling random objects with circular predictors. We concentrate on local constant and local linear Fr\'echet regression, providing rigorous proofs for the upper bounds of both bias and stochastic deviation of the estimators under mild conditions. This research lays the groundwork for broadening the application of Fr\'echet regression to scenarios involving non-Euclidean covariates, thereby expanding its utility in complex data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10118v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Jun Im, Jeong Min Jeon</dc:creator>
    </item>
    <item>
      <title>Improved Catoni-Type Confidence Sequences for Estimating the Mean When the Variance Is Infinite</title>
      <link>https://arxiv.org/abs/2409.04198</link>
      <description>arXiv:2409.04198v2 Announce Type: replace 
Abstract: We consider a discrete time stochastic model with infinite variance and study the mean estimation problem as in Wang and Ramdas (2023). We refine the Catoni-type confidence sequence (abbr. CS) and use an idea of Bhatt et al. (2022) to achieve notable improvements of some currently existing results for such model.
  Specifically, for given $\alpha \in (0, 1]$, we assume that there is a known upper bound $\nu_{\alpha} &gt; 0$ for the $(1 + \alpha)$-th central moment of the population distribution that the sample follows. Our findings replicate and `optimize' results in the above references for $\alpha = 1$ (i.e., in models with finite variance) and enhance the results for $\alpha &lt; 1$. Furthermore, by employing the stitching method, we derive an upper bound on the width of the CS as $\mathcal{O} \left(((\log \log t)/t)^{\frac{\alpha}{1+\alpha}}\right)$ for the shrinking rate as $t$ increases, and $\mathcal{O}(\left(\log (1/\delta)\right)^{\frac{\alpha }{1+\alpha}})$ for the growth rate as $\delta$ decreases. These bounds are improving upon the bounds found in Wang and Ramdas (2023). Our theoretical results are illustrated by results from a series of simulation experiments. Comparing the performance of our improved $\alpha$-Catoni-type CS with the bound in the above cited paper indicates that our CS achieves tighter width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04198v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chengfu Wei, Jordan Stoyanov, Yiming Chen, Zijun Chen</dc:creator>
    </item>
    <item>
      <title>Universality of Approximate Message Passing algorithms and tensor networks</title>
      <link>https://arxiv.org/abs/2206.13037</link>
      <description>arXiv:2206.13037v5 Announce Type: replace-cross 
Abstract: Approximate Message Passing (AMP) algorithms provide a valuable tool for studying mean-field approximations and dynamics in a variety of applications. Although these algorithms are often first derived for matrices having independent Gaussian entries or satisfying rotational invariance in law, their state evolution characterizations are expected to hold over larger universality classes of random matrix ensembles.
  We develop several new results on AMP universality. For AMP algorithms tailored to independent Gaussian entries, we show that their state evolutions hold over broadly defined generalized Wigner and white noise ensembles, including matrices with heavy-tailed entries and heterogeneous entrywise variances that may arise in data applications. For AMP algorithms tailored to rotational invariance in law, we show that their state evolutions hold over delocalized sign-and-permutation-invariant matrix ensembles that have a limit distribution over the diagonal, including sensing matrices composed of subsampled Hadamard or Fourier transforms and diagonal operators.
  We establish these results via a simplified moment-method proof, reducing AMP universality to the study of products of random matrices and diagonal tensors along a tensor network. As a by-product of our analyses, we show that the aforementioned matrix ensembles satisfy a notion of asymptotic freeness with respect to such tensor networks, which parallels usual definitions of freeness for traces of matrix products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13037v5</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianhao Wang, Xinyi Zhong, Zhou Fan</dc:creator>
    </item>
    <item>
      <title>Principles of Statistical Inference in Online Problems</title>
      <link>https://arxiv.org/abs/2209.05399</link>
      <description>arXiv:2209.05399v2 Announce Type: replace-cross 
Abstract: To investigate a dilemma of statistical and computational efficiency faced by long-run variance estimators, we propose a decomposition of kernel weights in a quadratic form and some online inference principles. These proposals allow us to characterize efficient online long-run variance estimators. Our asymptotic theory and simulations show that this principle-driven approach leads to online estimators with a uniformly lower mean squared error than all existing works. We also discuss practical enhancements such as mini-batch and automatic updates to handle fast streaming data and optimal parameters tuning. Beyond variance estimation, we consider the proposals in the context of online quantile regression, online change point detection, Markov chain Monte Carlo convergence diagnosis, and stochastic approximation. Substantial improvements in computational cost and finite-sample statistical properties are observed when we apply our principle-driven variance estimator to original and modified inference procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05399v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Fung Leung, Kin Wai Chan</dc:creator>
    </item>
    <item>
      <title>Computable error bounds for quasi-Monte Carlo using points with non-negative local discrepancy</title>
      <link>https://arxiv.org/abs/2309.04209</link>
      <description>arXiv:2309.04209v2 Announce Type: replace-cross 
Abstract: Let $f:[0,1]^d\to\mathbb{R}$ be a completely monotone integrand as defined by Aistleitner and Dick (2015) and let points $\boldsymbol{x}_0,\dots,\boldsymbol{x}_{n-1}\in[0,1]^d$ have a non-negative local discrepancy (NNLD) everywhere in $[0,1]^d$. We show how to use these properties to get a non-asymptotic and computable upper bound for the integral of $f$ over $[0,1]^d$. An analogous non-positive local discrepancy (NPLD) property provides a computable lower bound. It has been known since Gabai (1967) that the two dimensional Hammersley points in any base $b\ge2$ have non-negative local discrepancy. Using the probabilistic notion of associated random variables, we generalize Gabai's finding to digital nets in any base $b\ge2$ and any dimension $d\ge1$ when the generator matrices are permutation matrices. We show that permutation matrices cannot attain the best values of the digital net quality parameter when $d\ge3$. As a consequence the computable absolutely sure bounds we provide come with less accurate estimates than the usual digital net estimates do in high dimensions. We are also able to construct high dimensional rank one lattice rules that are NNLD. We show that those lattices do not have good discrepancy properties: any lattice rule with the NNLD property in dimension $d\ge2$ either fails to be projection regular or has all its points on the main diagonal. Complete monotonicity is a very strict requirement that for some integrands can be mitigated via a control variate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04209v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Gnewuch, Peter Kritzer, Art B. Owen, Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Self-convolved Bootstrap for M-regression under Complex Temporal Dynamics</title>
      <link>https://arxiv.org/abs/2310.11724</link>
      <description>arXiv:2310.11724v3 Announce Type: replace-cross 
Abstract: The paper considers simultaneous nonparametric inference for a wide class of M-regression models with time-varying coefficients. The covariates and errors of the regression model are tackled as a general class of nonstationary time series and are allowed to be cross-dependent. A novel and easy-to-implement self-convolved bootstrap procedure is proposed. With only one tuning parameter, the bootstrap facilitates a $\sqrt{n}$-consistent inference of the cumulative regression function for the M-estimators under complex temporal dynamics, even under the possible presence of breakpoints in time series. Our methodology leads to a unified framework to conduct general classes of Exact Function Tests, Lack-of-fit Tests, and Qualitative Tests for the time-varying coefficients. These tests enable one to, among many others, conduct variable selection, check for constancy and linearity, as well as verify shape assumptions, including monotonicity and convexity. As applications, our method is utilized to study the time-varying properties of global climate data and Microsoft stock return, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11724v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaoshiqi Liu, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification</title>
      <link>https://arxiv.org/abs/2405.15132</link>
      <description>arXiv:2405.15132v2 Announce Type: replace-cross 
Abstract: The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. In the presented framework, to estimate the density it is necessary to know the ID, therefore, this condition is imposed self-consistently. We derive theoretical guarantees and illustrate the usefulness and robustness of this procedure by benchmarks on artificial and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15132v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Iuri Macocco, Aldo Glielmo, Alessandro Laio, Antonietta Mira</dc:creator>
    </item>
  </channel>
</rss>
