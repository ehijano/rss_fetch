<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jan 2026 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Constrained Multi-Objective Genetic Algorithm Variants for Design and Optimization of Tri-Band Microstrip Patch Antenna loaded CSRR for IoT Applications: A Comparative Case Study</title>
      <link>https://arxiv.org/abs/2601.17513</link>
      <description>arXiv:2601.17513v1 Announce Type: new 
Abstract: This paper presents an automated antenna design and optimization framework employing multi-objective genetic algorithms (MOGAs) to investigate various evolutionary optimization approaches, with a primary emphasis on multi-band frequency optimization. Five MOGA variants were implemented and compared: the Pareto genetic algorithm (PGA), non-dominated sorting genetic algorithm with niching (NSGA-I), non-dominated sorting genetic algorithm with elitism (NSGA-II), non-dominated sorting genetic algorithm using reference points (NSGA-III), and strength Pareto evolutionary algorithm (SPEA). These algorithms are employed to design and optimize microstrip patch antennas loaded with complementary split-ring resonators (CSRRs). A weighted-sum scalarization approach was adopted within a single-objective genetic algorithm framework enhanced with domain-specific constraint handling mechanisms. The optimization addresses the conflicting objectives of minimizing the return loss ($S_{11} &lt; -10$~dB) and achieving multi-band resonance at 2.4~GHz, 3.6~GHz, and 5.2~GHz. The proposed method delivers a superior overall performance by aggregating these objectives into a unified fitness function encompassing $S_{11}$(2.4~GHz), $S_{11}$(3.6~GHz), and $S_{11}$(5.2~GHz). This approach effectively balances all three frequency bands simultaneously, rather than exploring trade-off solutions typical of traditional multi-objective approaches. The antenna was printed on a Rogers RT5880 substrate with a dielectric constant of 2.2 , loss tangent of 0.0009 , and thickness of 1.57~mm . Scalarization approach achieved return loss values of $-21.56$~dB, $-16.60$~dB, and $-27.69$~dB, with corresponding gains of 1.96~dBi, 2.6~dB, and 3.99~dBi at 2.4~GHz, 3.6~GHz, and 5.2~GHz, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17513v1</guid>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moahmed Hamza Boulaich, Said Ohamouddou, Mohammed Ali Ennasar, Abdelatif El Afia</dc:creator>
    </item>
    <item>
      <title>Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments</title>
      <link>https://arxiv.org/abs/2601.17598</link>
      <description>arXiv:2601.17598v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) has driven major advances in autonomous control. Still, standard Deep Q-Network (DQN) agents tend to rely on fixed learning rates and uniform update scaling, even as updates are modulated by temporal-difference (TD) error. This rigidity destabilizes convergence, especially in sparse-reward settings where feedback is infrequent. We introduce Deep Intrinsic Surprise-Regularized Control (DISRC), a biologically inspired augmentation to DQN that dynamically scales Q-updates based on latent-space surprise. DISRC encodes states via a LayerNorm-based encoder and computes a deviation-based surprise score relative to a moving latent setpoint. Each update is then scaled in proportion to both TD error and surprise intensity, promoting plasticity during early exploration and stability as familiarity increases. We evaluate DISRC on two sparse-reward MiniGrid environments, which included MiniGrid-DoorKey-8x8 and MiniGrid-LavaCrossingS9N1, under identical settings as a vanilla DQN baseline. In DoorKey, DISRC reached the first successful episode (reward &gt; 0.8) 33% faster than the vanilla DQN baseline (79 vs. 118 episodes), with lower reward standard deviation (0.25 vs. 0.34) and higher reward area under the curve (AUC: 596.42 vs. 534.90). These metrics reflect faster, more consistent learning - critical for sparse, delayed reward settings. In LavaCrossing, DISRC achieved a higher final reward (0.95 vs. 0.93) and the highest AUC of all agents (957.04), though it converged more gradually. These preliminary results establish DISRC as a novel mechanism for regulating learning intensity in off-policy agents, improving both efficiency and stability in sparse-reward domains. By treating surprise as an intrinsic learning signal, DISRC enables agents to modulate updates based on expectation violations, enhancing decision quality when conventional value-based methods fall short.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17598v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yash Kini, Shiv Davay, Shreya Polavarapu</dc:creator>
    </item>
    <item>
      <title>Motif Diversity in Human Liver ChIP-seq Data Using MAP-Elites</title>
      <link>https://arxiv.org/abs/2601.17808</link>
      <description>arXiv:2601.17808v1 Announce Type: new 
Abstract: Motif discovery is a core problem in computational biology, traditionally formulated as a likelihood optimization task that returns a single dominant motif from a DNA sequence dataset. However, regulatory sequence data admit multiple plausible motif explanations, reflecting underlying biological heterogeneity. In this work, we frame motif discovery as a quality-diversity problem and apply the MAP-Elites algorithm to evolve position weight matrix motifs under a likelihood-based fitness objective while explicitly preserving diversity across biologically meaningful dimensions. We evaluate MAP-Elites using three complementary behavioral characterizations that capture trade-offs between motif specificity, compositional structure, coverage, and robustness. Experiments on human CTCF liver ChIP-seq data aligned to the human reference genome compare MAP-Elites against a standard motif discovery tool, MEME, under matched evaluation criteria across stratified dataset subsets. Results show that MAP-Elites recovers multiple high-quality motif variants with fitness comparable to MEME's strongest solutions while revealing structured diversity obscured by single-solution approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17808v1</guid>
      <category>cs.NE</category>
      <category>q-bio.GN</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Medina, Mary Lauren Benton</dc:creator>
    </item>
    <item>
      <title>Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization</title>
      <link>https://arxiv.org/abs/2601.17899</link>
      <description>arXiv:2601.17899v1 Announce Type: new 
Abstract: Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17899v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhao Qiu, Xin Chen, Liang Ge, Liyong Lin, Zhichao Lu, Qingfu Zhang</dc:creator>
    </item>
    <item>
      <title>TEFormer: Structured Bidirectional Temporal Enhancement Modeling in Spiking Transformers</title>
      <link>https://arxiv.org/abs/2601.18274</link>
      <description>arXiv:2601.18274v1 Announce Type: new 
Abstract: In recent years, Spiking Neural Networks (SNNs) have achieved remarkable progress, with Spiking Transformers emerging as a promising architecture for energy-efficient sequence modeling. However, existing Spiking Transformers still lack a principled mechanism for effective temporal fusion, limiting their ability to fully exploit spatiotemporal dependencies. Inspired by feedforward-feedback modulation in the human visual pathway, we propose TEFormer, the first Spiking Transformer framework that achieves bidirectional temporal fusion by decoupling temporal modeling across its core components. Specifically, TEFormer employs a lightweight and hyperparameter-free forward temporal fusion mechanism in the attention module, enabling fully parallel computation, while incorporating a backward gated recurrent structure in the MLP to aggregate temporal information in reverse order and reinforce temporal consistency. Extensive experiments across a wide range of benchmarks demonstrate that TEFormer consistently and significantly outperforms strong SNN and Spiking Transformer baselines under diverse datasets. Moreover, through the first systematic evaluation of Spiking Transformers under different neural encoding schemes, we show that the performance gains of TEFormer remain stable across encoding choices, indicating that the improved temporal modeling directly translates into reliable accuracy improvements across varied spiking representations. These results collectively establish TEFormer as an effective and general framework for temporal modeling in Spiking Transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18274v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sicheng Shen, Mingyang Lv, Bing Han, Dongcheng Zhao, Guobin Shen, Feifei Zhao, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?</title>
      <link>https://arxiv.org/abs/2601.18446</link>
      <description>arXiv:2601.18446v1 Announce Type: new 
Abstract: Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18446v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinmeng Yu, Tao Jiang, Ran Cheng, Yaochu Jin, Kay Chen Tan</dc:creator>
    </item>
    <item>
      <title>How Information Evolves: Stability-Driven Assembly and the Emergence of a Natural Genetic Algorithm</title>
      <link>https://arxiv.org/abs/2601.17061</link>
      <description>arXiv:2601.17061v1 Announce Type: cross 
Abstract: Information can evolve as a physical consequence of non-equilibrium dynamics, even in the absence of genes, replication, or predefined fitness functions. We present Stability-Driven Assembly (SDA), a framework in which stochastic assembly combined with differential persistence biases populations toward longer-lived motifs. Assemblies that persist longer become more frequent and are therefore more likely to participate in subsequent interactions, generating feedback that reshapes the population distribution and implements fitness-proportional sampling, realizing evolution as a natural, emergent genetic algorithm (SDA/GA) driven solely by stability. We apply SDA/GA to chemical symbol space using SMILES fragments with recombination, mutation, and a heuristic stability function. Simulations show hallmark features of evolutionary search, including scaffold-level dominance, sustained novelty, and entropy reduction, yielding open-ended dynamics absent from equilibrium models with fixed transition rates. These results motivate an evolutionary ladder hypothesis where persistence-driven selection precedes genetic replication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17061v1</guid>
      <category>q-bio.PE</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Adler</dc:creator>
    </item>
    <item>
      <title>Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces</title>
      <link>https://arxiv.org/abs/2601.17749</link>
      <description>arXiv:2601.17749v1 Announce Type: cross 
Abstract: The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17749v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyriakos Stylianopoulos, Mattia Fabiani, Giulia Torcolacci, Davide Dardari, George C. Alexandropoulos</dc:creator>
    </item>
    <item>
      <title>Resonant Sparse Geometry Networks</title>
      <link>https://arxiv.org/abs/2601.18064</link>
      <description>arXiv:2601.18064v1 Announce Type: cross 
Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k &lt;&lt; n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18064v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasi Hays</dc:creator>
    </item>
    <item>
      <title>EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization</title>
      <link>https://arxiv.org/abs/2601.18067</link>
      <description>arXiv:2601.18067v1 Announce Type: cross 
Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18067v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>cs.PL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei-Po Hsin, Ren-Hao Deng, Yao-Ting Hsieh, En-Ming Huang, Shih-Hao Hung</dc:creator>
    </item>
    <item>
      <title>Global Optimization of Atomic Clusters via Physically-Constrained Tensor Train Decomposition</title>
      <link>https://arxiv.org/abs/2601.18592</link>
      <description>arXiv:2601.18592v1 Announce Type: cross 
Abstract: The global optimization of atomic clusters represents a fundamental challenge in computational chemistry and materials science due to the exponential growth of local minima with system size (i.e., the curse of dimensionality). We introduce a novel framework that overcomes this limitation by exploiting the low-rank structure of potential energy surfaces through Tensor Train (TT) decomposition. Our approach combines two complementary TT-based strategies: the algebraic TTOpt method, which utilizes maximum volume sampling, and the probabilistic PROTES method, which employs generative sampling. A key innovation is the development of physically-constrained encoding schemes that incorporate molecular constraints directly into the discretization process. We demonstrate the efficacy of our method by identifying global minima of Lennard-Jones clusters containing up to 45 atoms. Furthermore, we establish its practical applicability to real-world systems by optimizing 20-atom carbon clusters using a machine-learned Moment Tensor Potential, achieving geometries consistent with quantum-accurate simulations. This work establishes TT-decomposition as a powerful tool for molecular structure prediction and provides a general framework adaptable to a wide range of high-dimensional optimization problems in computational material science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18592v1</guid>
      <category>math.OC</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantin Sozykin, Nikita Rybin, Andrei Chertkov, Anh-Huy Phan, Ivan Oseledets, Alexander Shapeev, Ivan Novikov, Gleb Ryzhakov</dc:creator>
    </item>
    <item>
      <title>SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model</title>
      <link>https://arxiv.org/abs/2601.18707</link>
      <description>arXiv:2601.18707v1 Announce Type: cross 
Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18707v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Hagnberger, Mathias Niepert</dc:creator>
    </item>
    <item>
      <title>Fractional-order Spiking Neural Network</title>
      <link>https://arxiv.org/abs/2507.16937</link>
      <description>arXiv:2507.16937v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) draw inspiration from biological neurons to enable brain-like computation, demonstrating effectiveness in processing temporal information with energy efficiency and biological realism. Most existing SNNs are based on neural dynamics such as the (leaky) integrate-and-fire (IF/LIF) models, which are described by first-order ordinary differential equations (ODEs) with Markovian characteristics. This means the potential state at any time depends solely on its immediate past value, potentially limiting network expressiveness. Empirical studies of real neurons, however, reveal long-range correlations and fractal dendritic structures, suggesting non-Markovian behavior better modeled by fractional-order ODEs.Motivated by this, we propose a fractional-order spiking neural network (f-SNN) framework that strictly generalizes integer-order SNNs and captures long-term dependencies in membrane potential and spike trains via fractional dynamics, enabling richer temporal patterns. We also release an open-source toolbox to support the f-SNN framework, applicable to diverse architectures and real-world tasks. Experimentally, fractional adaptations of established SNNs into the f-SNN framework achieve superior accuracy, comparable energy efficiency, and improved robustness to noise, underscoring the promise of f-SNNs as an effective extension of traditional SNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16937v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengjie Ge, Yufeng Peng, Zihao Li, Qiyu Kang, Xueyang Fu, Xuhao Li, Qixin Zhang, Junhao Ren, Zheng-Jun Zha</dc:creator>
    </item>
    <item>
      <title>Network-Optimised Spiking Neural Network for Event-Driven Networking</title>
      <link>https://arxiv.org/abs/2509.23516</link>
      <description>arXiv:2509.23516v4 Announce Type: replace 
Abstract: Delay-coupled systems often require low-latency decisions from sparse telemetry, where dense fixed-step neural inference is wasteful and can degrade near stability margins. We introduce Network-Optimised Spiking (NOS), a trainable two-state event-driven dynamical unit for delayed, graph-coupled streams, whose states map to a fast load variable and a slower recovery resource. NOS uses bounded excitability for finite buffers, explicit leak terms for service and damping, and graph-local coupling with per-link gates and communication delays, with differentiable resets compatible with surrogate-gradient training and neuromorphic execution. We prove existence and uniqueness of subthreshold equilibria, derive Jacobian-based stability conditions, and obtain a scalar network stability threshold that separates topology from node dynamics via a Perron-mode spectral condition. A stochastic arrival model aligned with telemetry smoothing explains increased variability as systems approach stability boundaries. On delayed graph forecasting and early-warning tasks from queue telemetry, NOS improves detection F1 and detection latency over MLP, RNN/GRU, and temporal GNN baselines under a common residual-based protocol, while providing calibration rules for resource-constrained deployments. Code and Demos: https://mbilal84.github.io/nos-snn-networking/</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23516v4</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.OC</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Bilal</dc:creator>
    </item>
    <item>
      <title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title>
      <link>https://arxiv.org/abs/2510.19764</link>
      <description>arXiv:2510.19764v2 Announce Type: replace 
Abstract: The majority of research in both training Artificial Neural Networks (ANNs) and modeling learning in biological brains focuses on synaptic plasticity, where learning equates to changing the strength of existing connections. However, in biological brains, structural plasticity - where new connections are created and others removed - is also vital, not only for effective learning but also for recovery from damage and optimal resource usage. Inspired by structural plasticity, pruning is often used in machine learning to remove weak connections from trained models to reduce the computational requirements of inference. However, the machine learning frameworks typically used for backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs) are optimized for dense connectivity, meaning that pruning does not help reduce the training costs of ever-larger models. The GeNN simulator already supports efficient GPU-accelerated simulation of sparse SNNs for computational neuroscience and machine learning. Here, we present a new flexible framework for implementing GPU-accelerated structural plasticity rules and demonstrate this first using the e-prop supervised learning rule and DEEP R to train efficient, sparse SNN classifiers and then, in an unsupervised learning context, to learn topographic maps. Compared to baseline dense models, our sparse classifiers reduce training time by up to 10x while the DEEP R rewiring enables them to perform as well as the original models. We demonstrate topographic map formation in faster-than-realtime simulations, provide insights into the connectivity evolution, and measure simulation speed versus network size. The proposed framework will enable further research into achieving and maintaining sparsity in network structure and neural communication, as well as exploring the computational benefits of sparsity in a range of neuromorphic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19764v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James C. Knight, Johanna Senk, Thomas Nowotny</dc:creator>
    </item>
    <item>
      <title>Space as Time Through Neuron Position Learning</title>
      <link>https://arxiv.org/abs/2511.01632</link>
      <description>arXiv:2511.01632v2 Announce Type: replace 
Abstract: Biological neural networks exist in physical space where distance influences communication delays: a fundamental coupling between space and time absent in most artificial neural networks. While recent work has separately explored spatial embeddings and learnable synaptic delays in spiking neural networks, we unify these approaches through a novel neuron position learning algorithm where delays relate to the Euclidean distances between neurons. We derive gradients with respect to neuron positions and demonstrate that this biologically-motivated constraint acts as an inductive bias: networks trained on temporal classification tasks spontaneously self-organize into local, clustered topologies and a modular, efficiently wired structure emerges if connection costs are distance-dependent. Remarkably, we observe functional specialization aligned with spatial clustering without explicitly enforcing it. These findings lay the groundwork for networks in which space and time are intrinsically coupled, offering new avenues for mechanistic interpretability, biologically inspired modelling, and efficient implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01632v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bal\'azs M\'esz\'aros, James C. Knight, Danyal Akarca, Thomas Nowotny</dc:creator>
    </item>
    <item>
      <title>ToxSearch: Evolving Prompts for Toxicity Search in Large Language Models</title>
      <link>https://arxiv.org/abs/2511.12487</link>
      <description>arXiv:2511.12487v2 Announce Type: replace 
Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12487v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Onkar Shelar, Travis Desell</dc:creator>
    </item>
    <item>
      <title>Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization</title>
      <link>https://arxiv.org/abs/2601.01832</link>
      <description>arXiv:2601.01832v2 Announce Type: replace 
Abstract: We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01832v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>SB Danush Vikraman, Hannah Abigail, Prasanna Kesavraj, Gajanan V Honnavar</dc:creator>
    </item>
    <item>
      <title>GENPACK: KPI-Guided Multi-Objective Genetic Algorithm for Industrial 3D Bin Packing</title>
      <link>https://arxiv.org/abs/2601.11325</link>
      <description>arXiv:2601.11325v2 Announce Type: replace 
Abstract: The three-dimensional bin packing problem (3D-BPP) is a longstanding challenge in operations research and logistics. Classical heuristics and constructive methods can generate packings quickly, but often fail to address industrial constraints such as stability, balance, and handling feasibility. Metaheuristics such as genetic algorithms (GAs) provide flexibility and the ability to optimize across multiple objectives; however, pure GA approaches frequently struggle with efficiency, parameter sensitivity, and scalability to industrial order sizes. This gap is especially evident when scaling to real-world pallet dimensions, where even state-of-the-art algorithms often fail to achieve robust, deployable solutions. We propose a KPI-driven GA-based pipeline for industrial 3D-BPP that integrates key performance indicators directly into a multi-objective fitness function. The methodology combines a layer-based chromosome representation with domain-specific operators and constructive heuristics to balance efficiency and feasibility. On the BED-BPP benchmark of 1,500 real-world orders, our Hybrid-GA pipeline consistently outperforms heuristic- and learning-based state-of-the-art methods, achieving up to 35% higher space utilization and 15 to 20% stronger surface support, with lower variance across orders. These improvements come at a modest runtime cost but remain feasible for batch-scale deployment, yielding stable, balanced, and space-efficient packings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11325v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dheeraj Poolavaram, Carsten Markgraf, Sebastian Dorn</dc:creator>
    </item>
    <item>
      <title>Fully tensorial approach to hypercomplex-valued neural networks</title>
      <link>https://arxiv.org/abs/2407.00449</link>
      <description>arXiv:2407.00449v5 Announce Type: replace-cross 
Abstract: A fully tensorial theoretical framework for hypercomplex-valued neural networks is presented. The proposed approach enables neural network architectures to operate on data defined over arbitrary finite-dimensional algebras. The central observation is that algebra multiplication can be represented by a rank-three tensor, which allows all algebraic operations in neural network layers to be formulated in terms of standard tensor contractions, permutations, and reshaping operations.
  This tensor-based formulation provides a unified and dimension-independent description of hypercomplex-valued dense and convolutional layers and is directly compatible with modern deep learning libraries supporting optimized tensor operations. The proposed framework recovers existing constructions for four-dimensional algebras as a special case.
  Within this setting, a tensor-based version of the universal approximation theorem for single-layer hypercomplex-valued perceptrons is established under mild non-degeneracy assumptions on the underlying algebra, thereby providing a rigorous theoretical foundation for the considered class of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00449v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agnieszka Niemczynowicz, Rados{\l}aw Antoni Kycia</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Nature-Inspired Algorithms for Autism Spectrum Disorder Detection Using 3D Video Dataset</title>
      <link>https://arxiv.org/abs/2501.01202</link>
      <description>arXiv:2501.01202v2 Announce Type: replace-cross 
Abstract: Autism Spectrum Disorder (ASD) is a chronic neurodevelopmental condition characterized by repetitive behaviors and impairments in social and communication skills. Despite the clear manifestation of these symptoms, many individuals with ASD remain undiagnosed. This paper proposes a methodology for ASD detection using a three-dimensional walking video dataset, leveraging supervised machine learning classification algorithms combined with nature-inspired optimization algorithms for feature extraction. The approach employs supervised classifiers to identify ASD cases, while nature-inspired optimization techniques select the most relevant features, enhanced by the use of ranking coefficients to identify initial leading particles. This strategy significantly reduces computational time, thereby improving efficiency and accuracy. Experimental evaluation with various algorithmic combinations demonstrates an exceptional classification accuracy of 100% in the best case when using the Random Forest classifier coupled with the Gravitational Search Algorithm for feature selection. The methodology's application to additional datasets promises improved robustness and generalizability. With its high accuracy and reduced computational requirements, the proposed framework offers significant contributions to both medical and academic fields, providing a foundation for future advances in ASD diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01202v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneesh Panchal, Kainat Khan, Rahul Katarya</dc:creator>
    </item>
  </channel>
</rss>
