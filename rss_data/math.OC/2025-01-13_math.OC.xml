<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Switched Optimal Control with Dwell Time Constraints</title>
      <link>https://arxiv.org/abs/2501.05548</link>
      <description>arXiv:2501.05548v1 Announce Type: new 
Abstract: This paper presents an embedding-based approach for solving switched optimal control problems (SOCPs) with dwell time constraints. At first, an embedded optimal control problem (EOCP) is defined by replacing the discrete switching signal with a continuous embedded variable that can take intermediate values between the discrete modes. While embedding enables solutions of SOCPs via conventional techniques, optimal solutions of EOCPs often involve nonexistent modes and thus may not be feasible for the SOCP. In the modified EOCP (MEOCP), a concave function is added to the cost function to enforce a bang-bang solution in the embedded variable, which results in feasible solutions for the SOCP. However, the MEOCP cannot guarantee the satisfaction of dwell-time constraints.
  In this paper, a MEOCP is combined with a filter layer to remove switching times that violate the dwell time constraint. Insertion gradients are used to minimize the effect of the filter on the optimal cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05548v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud S. Sakha, Rushikesh Kamalapurkar</dc:creator>
    </item>
    <item>
      <title>Equivariant Perturbation in Gomory and Johnson's Infinite Group Problem. IV. The General Unimodular Two-Dimensional Case</title>
      <link>https://arxiv.org/abs/2501.05582</link>
      <description>arXiv:2501.05582v1 Announce Type: new 
Abstract: We study an abstract setting for cutting planes for integer programming called the infinite group problem. In this abstraction, cutting planes are computed via cut generating function that act on the simplex tableau. In this function space, cut generating functions are classified as minimal, extreme, and facets as a proxy for understanding the strength or potential importance of these functions. Prior work developed algorithms for testing minimality, extremality, and facetness for cut generating functions applied to 1-row tableau and to some 2-row tableau in a restricted setting. We complement and generalize this work by giving an algorithm for testing the extremality of a large class of minimal valid functions for the two-dimensional infinite group problem. Along the way, we develop results of independent interest on functional equations and infinite systems of linear equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05582v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Robert Hildebrand, Matthias K\"oppe, Luze Xu</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Two-Stage Distributionally Robust Optimization over 1-Wasserstein and 2-Wasserstein Balls</title>
      <link>https://arxiv.org/abs/2501.05619</link>
      <description>arXiv:2501.05619v1 Announce Type: new 
Abstract: This paper investigates advantages of using 2-Wasserstein ambiguity sets over 1-Wasserstein sets in two-stage distributionally robust optimization with right-hand side uncertainty. We examine the worst-case distributions within 1- and 2-Wasserstein balls under both unrestricted and nonnegative orthant supports, highlighting a pathological behavior arising in 1-Wasserstein balls. Closed-form solutions for a single-scenario newsvendor problem illustrate that 2-Wasserstein balls enable more informed decisions. Additionally, a penalty-based dual interpretation suggests that 2-Wasserstein balls may outperform 1-Wasserstein balls across a broader range of Wasserstein radii, even with general support sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05619v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geunyeong Byeon</dc:creator>
    </item>
    <item>
      <title>A Quadratically-Constrained Convex Approximation for the AC Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2501.05623</link>
      <description>arXiv:2501.05623v1 Announce Type: new 
Abstract: We introduce a quadratically-constrained approximation (QCAC) of the AC optimal power flow (AC-OPF) problem. Unlike existing approximations like the DC-OPF, our model does not rely on typical assumptions such as high reactance-to-resistance ratio, near-nominal voltage magnitudes, or small angle differences, and preserves the structural sparsity of the original AC power flow equations, making it suitable for decentralized power systems optimization problems. To achieve this, we reformulate the AC-OPF problem as a quadratically constrained quadratic program. The nonconvex terms are expressed as differences of convex functions, which are then convexified around a base point derived from a warm start of the nodal voltages. If this linearization results in a non-empty constraint set, the convexified constraints form an inner convex approximation. Our experimental results, based on Power Grid Library instances of up to 30,000 buses, demonstrate the effectiveness of the QCAC approximation with respect to other well-documented conic relaxations and a linear approximation. We further showcase its potential advantages over the well-documented second-order conic relaxation of the power flow equations in two proof-of-concept case studies: optimal reactive power dispatch in transmission networks and PV hosting capacity in distribution grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05623v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo E. Constante-Flores, Can Li</dc:creator>
    </item>
    <item>
      <title>FIRM: Federated Image Reconstruction using Multimodal Tomographic Data</title>
      <link>https://arxiv.org/abs/2501.05642</link>
      <description>arXiv:2501.05642v1 Announce Type: new 
Abstract: We propose a federated algorithm for reconstructing images using multimodal tomographic data sourced from dispersed locations, addressing the challenges of traditional unimodal approaches that are prone to noise and reduced image quality. Our approach formulates a joint inverse optimization problem incorporating multimodality constraints and solves it in a federated framework through local gradient computations complemented by lightweight central operations, ensuring data decentralization. Leveraging the connection between our federated algorithm and the quadratic penalty method, we introduce an adaptive step-size rule with guaranteed sublinear convergence and further suggest its extension to augmented Lagrangian framework. Numerical results demonstrate its superior computational efficiency and improved image reconstruction quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05642v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geunyeong Byeon, Minseok Ryu, Zichao Wendy Di, Kibaek Kim</dc:creator>
    </item>
    <item>
      <title>Single-Loop Variance-Reduced Stochastic Algorithm for Nonconvex-Concave Minimax Optimization</title>
      <link>https://arxiv.org/abs/2501.05677</link>
      <description>arXiv:2501.05677v1 Announce Type: new 
Abstract: Nonconvex-concave (NC-C) finite-sum minimax problems have broad applications in decentralized optimization and various machine learning tasks. However, the nonsmooth nature of NC-C problems makes it challenging to design effective variance reduction techniques. Existing vanilla stochastic algorithms using uniform samples for gradient estimation often exhibit slow convergence rates and require bounded variance assumptions. In this paper, we develop a novel probabilistic variance reduction updating scheme and propose a single-loop algorithm called the probabilistic variance-reduced smoothed gradient descent-ascent (PVR-SGDA) algorithm. The proposed algorithm achieves an iteration complexity of $O(\epsilon^{-4})$, surpassing the best-known rates of stochastic algorithms for NC-C minimax problems and matching the performance of the best deterministic algorithms in this context. Finally, we demonstrate the effectiveness of the proposed algorithm through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05677v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Jiang, Linglingzhi Zhu, Taoli Zheng, Anthony Man-Cho So</dc:creator>
    </item>
    <item>
      <title>Robust Adaptive Supplementary Control for Damping Weak-Grid SSOs Involving IBRs</title>
      <link>https://arxiv.org/abs/2501.05693</link>
      <description>arXiv:2501.05693v1 Announce Type: new 
Abstract: Subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) connected to weak grids are a relatively new phenomena observed in modern power systems. SSOs are further exacerbated when grids become weaker because lines are disconnected due to maintenance or following faults. Such undesirable oscillations have also led to curtailment of inverter-based resource (IBR) outputs. In contrast to most literature addressing the issue by retuning/redesigning of standard IBR controllers, we propose a robust adaptive supplementary control for damping of such SSOs while keeping standard controls unaltered. As a result, uncertainty in system conditions can be handled without negatively impacting the nominal IBR performance. To that end, the adaptive control law is derived for a GFLC connected to the grid, where the grid is modeled by the Thevenin's equivalent representation with uncertainty and disturbances. The theoretical result provides dissipativity certificate for the closed-loop error dynamics with sufficient conditions for stability. The effectiveness of the developed controller is validated with several case studies conducted on a single-GFLC-infinite-bus test system, the IEEE $2$-area test system, wherein some of the synchronous generators are replaced by GFLCs, and a modified IEEE $5$-area test system with two GFLCs. The findings demonstrate that under very weak grid conditions, the proposed robust adaptive control performs well in stabilizing SSO modes, which a classical state-feedback control method fails to address.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05693v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sina Ameli, Lilan Karunaratne, Nilanjan Ray Chaudhuri, Constantino Lagoa</dc:creator>
    </item>
    <item>
      <title>A Two-timescale Primal-dual Algorithm for Decentralized Optimization with Compression</title>
      <link>https://arxiv.org/abs/2501.05701</link>
      <description>arXiv:2501.05701v1 Announce Type: new 
Abstract: This paper proposes a two-timescale compressed primal-dual (TiCoPD) algorithm for decentralized optimization with improved communication efficiency over prior works on primal-dual decentralized optimization. The algorithm is built upon the primal-dual optimization framework and utilizes a majorization-minimization procedure. The latter naturally suggests the agents to share a compressed difference term during the iteration. Furthermore, the TiCoPD algorithm incorporates a fast timescale mirror sequence for agent consensus on nonlinearly compressed terms, together with a slow timescale primal-dual recursion for optimizing the objective function. We show that the TiCoPD algorithm converges with a constant step size. It also finds an O(1 /T ) stationary solution after T iterations. Numerical experiments on decentralized training of a neural network validate the efficacy of TiCoPD algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05701v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoming Liu, Chung-Yiu Yau, Hoi-To Wai</dc:creator>
    </item>
    <item>
      <title>Efficient Gradient Tracking Algorithms for Distributed Optimization Problems with Inexact Communication</title>
      <link>https://arxiv.org/abs/2501.05737</link>
      <description>arXiv:2501.05737v1 Announce Type: new 
Abstract: Distributed optimization problems usually face inexact communication issues induced by communication quantization, differential privacy protection, or channels noise. Most existing algorithms need two-timescale setting of the stepsize of gradient descent and the parameter of noise suppression to ensure the convergence to the optimal solution. In this paper, we propose two single-timescale algorithms, VRA-DGT and VRA--DSGT, for distributed deterministic and stochastic optimization problems with inexact communication respectively. VRA-DGT integrates the Variance-Reduced Aggregation (VRA) mechanism with the distributed gradient tracking framework, which achieves a convergence rate of $\mathcal{O}\left(k^{-1}\right)$ in the mean-square sense when the objective function is strongly convex and smooth. For distributed stochastic optimization problem,VRA-DSGT, where a hybrid variance reduction technique has been introduced in VRA-DGT,
  VRA-DGT,, maintains the convergence rate of $\mathcal{O}\left(k^{-1}\right)$ for strongly convex and smooth objective function. Simulated experiments on logistic regression problem with real-world data verify the effectiveness of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05737v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengchao Zhaoa, Yongchao Liu</dc:creator>
    </item>
    <item>
      <title>Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks</title>
      <link>https://arxiv.org/abs/2501.05930</link>
      <description>arXiv:2501.05930v1 Announce Type: new 
Abstract: We present a framework to define a large class of neural networks for which, by construction, training by gradient flow provably reaches arbitrarily low loss when the number of parameters grows. Distinct from the fixed-space global optimality of non-convex optimization, this new form of convergence, and the techniques introduced to prove such convergence, pave the way for a usable deep learning convergence theory in the near future, without overparameterization assumptions relating the number of parameters and training samples. We define these architectures from a simple computation graph and a mechanism to lift it, thus increasing the number of parameters, generalizing the idea of increasing the widths of multi-layer perceptrons. We show that architectures similar to most common deep learning models are present in this class, obtained by sparsifying the weight tensors of usual architectures at initialization. Leveraging tools of algebraic topology and random graph theory, we use the computation graph's geometry to propagate properties guaranteeing convergence to any precision for these large sparse models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05930v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David A. R. Robin (DI-ENS), Kevin Scaman (DI-ENS), Marc Lelarge (DI-ENS)</dc:creator>
    </item>
    <item>
      <title>Designing a Robust and Cost-Efficient Electrified Bus Network with Sparse Energy Consumption Data</title>
      <link>https://arxiv.org/abs/2501.05939</link>
      <description>arXiv:2501.05939v1 Announce Type: new 
Abstract: This paper addresses the challenges of charging infrastructure design (CID) for electrified public transport networks using Battery Electric Buses (BEBs) under conditions of sparse energy consumption data. Accurate energy consumption estimation is critical for cost-effective and reliable electrification but often requires costly field experiments, resulting in limited data. To address this issue, we propose two mathematical models designed to handle uncertainty and data sparsity in energy consumption. The first is a robust optimization model with box uncertainty, addressing variability in energy consumption. The second is a data-driven distributionally robust optimization model that leverages observed data to provide more flexible and informed solutions. To evaluate these models, we apply them to the Rotterdam bus network. Our analysis reveals three key insights: (1) Ignoring variations in energy consumption can result in operational unreliability, with up to 55\% of scenarios leading to infeasible trips. (2) Designing infrastructure based on worst-case energy consumption increases costs by 67\% compared to using average estimates. (3) The data-driven distributionally robust optimization model reduces costs by 28\% compared to the box uncertainty model while maintaining reliability, especially in scenarios where extreme energy consumption values are rare and data exhibit skewness. In addition to cost savings, this approach provides robust protection against uncertainty, ensuring reliable operation under diverse conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05939v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sara Momen, Yousef Maknoon, Bart van Arem, Shadi Sharif Azadeh</dc:creator>
    </item>
    <item>
      <title>Distributed Generalized Nash Equilibria Learning for Online Stochastic Aggregative Games</title>
      <link>https://arxiv.org/abs/2501.06023</link>
      <description>arXiv:2501.06023v1 Announce Type: new 
Abstract: This paper investigates online stochastic aggregative games subject to local set constraints and time-varying coupled inequality constraints, where each player possesses a time-varying expectation-valued cost function relying on not only its own decision variable but also an aggregation of all the players' variables. Each player can only access its local individual cost function and constraints, necessitating partial information exchanges with neighboring players through time-varying unbalanced networks. Additionally, local cost functions and constraint functions are not prior knowledge and only revealed gradually. To learn generalized Nash equilibria of such games, a novel distributed online stochastic algorithm is devised based on push-sum and primal-dual strategies. Through rigorous analysis, high probability bounds on the regret and constraint violation are provided by appropriately selecting decreasing stepsizes. Moreover, for a time-invariant stochastic strongly monotone game, it is shown that the generated sequence by the designed algorithm converges to its variational generalized Nash equilibrium (GNE) almost surely, and the time-averaged sequence converges sublinearly with high probability. Finally, the derived theoretical results are illustrated by numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06023v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaixin Du, Min Meng</dc:creator>
    </item>
    <item>
      <title>Rank conditions for exactness of semidefinite relaxations in polynomial optimization</title>
      <link>https://arxiv.org/abs/2501.06052</link>
      <description>arXiv:2501.06052v1 Announce Type: new 
Abstract: We consider the Moment-SOS hierarchy in polynomial optimization. We first provide a sufficient condition to solve the truncated K-moment problem associated with a given degree-$2n$ pseudo-moment sequence $\phi$ n and a semi-algebraic set $K \subset \mathbb{R}^d$. Namely, let $2v$ be the maximum degree of the polynomials that describe $K$. If the rank $r$ of its associated moment matrix is less than $nv + 1$, then $\phi^n$ has an atomic representing measure supported on at most $r$ points of $K$. When used at step-$n$ of the Moment-SOS hierarchy, it provides a sufficient condition to guarantee its finite convergence (i.e., the optimal value of the corresponding degree-n semidefinite relaxation of the hierarchy is the global minimum). For Quadratic Constrained Quadratic Problems (QCQPs) one may also recover global minimizers from the optimal pseudo-moment sequence. Our condition is in the spirit of Blekherman's rank condition and while on the one-hand it is more restrictive, on the other hand it applies to constrained POPs as it provides a localization on $K$ for the representing measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06052v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean B Lasserre (LAAS-POP,TSE-R)</dc:creator>
    </item>
    <item>
      <title>Set-valued evenly convex functions: characterizations and c-conjugacy</title>
      <link>https://arxiv.org/abs/2501.06079</link>
      <description>arXiv:2501.06079v1 Announce Type: new 
Abstract: In this work we deal with set-valued functions with values in the power set of a separated locally convex space where a nontrivial pointed convex cone induces a partial order relation. A set-valued function is evenly convex if its epigraph is an evenly convex set, i.e., it is the intersection of an arbitrary family of open half-spaces. In this paper we characterize evenly convex set-valued functions as the pointwise supremum of its set-valued e-affine minorants. Moreover, a suitable conjugation pattern will be developed for these functions, as well as the counterpart of the biconjugation Fenchel-Moreau theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06079v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11228-021-00621-0</arxiv:DOI>
      <arxiv:journal_reference>Set Valued Var. Anal. 30 827-846 (2022)</arxiv:journal_reference>
      <dc:creator>M. D. Fajardo</dc:creator>
    </item>
    <item>
      <title>Averaged Adam accelerates stochastic optimization in the training of deep neural network approximations for partial differential equation and optimal control problems</title>
      <link>https://arxiv.org/abs/2501.06081</link>
      <description>arXiv:2501.06081v1 Announce Type: new 
Abstract: Deep learning methods - usually consisting of a class of deep neural networks (DNNs) trained by a stochastic gradient descent (SGD) optimization method - are nowadays omnipresent in data-driven learning problems as well as in scientific computing tasks such as optimal control (OC) and partial differential equation (PDE) problems. In practically relevant learning tasks, often not the plain-vanilla standard SGD optimization method is employed to train the considered class of DNNs but instead more sophisticated adaptive and accelerated variants of the standard SGD method such as the popular Adam optimizer are used. Inspired by the classical Polyak-Ruppert averaging approach, in this work we apply averaged variants of the Adam optimizer to train DNNs to approximately solve exemplary scientific computing problems in the form of PDEs and OC problems. We test the averaged variants of Adam in a series of learning problems including physics-informed neural network (PINN), deep backward stochastic differential equation (deep BSDE), and deep Kolmogorov approximations for PDEs (such as heat, Black-Scholes, Burgers, and Allen-Cahn PDEs), including DNN approximations for OC problems, and including DNN approximations for image classification problems (ResNet for CIFAR-10). In each of the numerical examples the employed averaged variants of Adam outperform the standard Adam and the standard SGD optimizers, particularly, in the situation of the scientific machine learning problems. The Python source codes for the numerical experiments associated to this work can be found on GitHub at https://github.com/deeplearningmethods/averaged-adam.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06081v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Dereich, Arnulf Jentzen, Adrian Riekert</dc:creator>
    </item>
    <item>
      <title>Blackwell Equilibrium in Repeated Games</title>
      <link>https://arxiv.org/abs/2501.05481</link>
      <description>arXiv:2501.05481v1 Announce Type: cross 
Abstract: We apply Blackwell optimality to repeated games. An equilibrium whose strategy profile is sequentially rational for all high enough discount factors simultaneously is a Blackwell (subgame-perfect, perfect public, etc.) equilibrium. The bite of this requirement depends on the monitoring structure. Under perfect monitoring, a ``folk'' theorem holds relative to an appropriate notion of minmax. Under imperfect public monitoring, absent a public randomization device, any perfect public equilibrium generically involves pure action profiles or stage-game Nash equilibria only. Under private conditionally independent monitoring, in a class of games that includes the prisoner's dilemma, the stage-game Nash equilibrium is played in every round.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05481v1</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Costas Cavounidis, Sambuddha Ghosh, Johannes H\"orner, Eilon Solan, Satoru Takahashi</dc:creator>
    </item>
    <item>
      <title>An Efficient Dual ADMM for Huber Regression with Fused Lasso Penalty</title>
      <link>https://arxiv.org/abs/2501.05676</link>
      <description>arXiv:2501.05676v1 Announce Type: cross 
Abstract: The ordinary least squares estimate in linear regression is sensitive to the influence of errors with large variance, which reduces its robustness, especially when dealing with heavy-tailed errors or outliers frequently encountered in real-world scenarios. To address this issue and accommodate the sparsity of coefficients along with their sequential disparities, we combine the adaptive robust Huber loss function with a fused lasso penalty. This combination yields a robust estimator capable of simultaneously achieving estimation and variable selection. Furthermore, we utilize an efficient alternating direction method of multipliers to solve this regression model from a dual perspective. The effectiveness and efficiency of our proposed approach is demonstrated through numerical experiments carried out on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05676v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjiao Shi, Yunhai Xiao</dc:creator>
    </item>
    <item>
      <title>Soft regression trees: a model variant and a decomposition training algorithm</title>
      <link>https://arxiv.org/abs/2501.05942</link>
      <description>arXiv:2501.05942v1 Announce Type: cross 
Abstract: Decision trees are widely used for classification and regression tasks in a variety of application fields due to their interpretability and good accuracy. During the past decade, growing attention has been devoted to globally optimized decision trees with deterministic or soft splitting rules at branch nodes, which are trained by optimizing the error function over all the tree parameters. In this work, we propose a new variant of soft multivariate regression trees (SRTs) where, for every input vector, the prediction is defined as the linear regression associated to a single leaf node, namely, the leaf node obtained by routing the input vector from the root along the branches with higher probability. SRTs exhibit the conditional computational property, i.e., each prediction depends on a small number of nodes (parameters), and our nonlinear optimization formulation for training them is amenable to decomposition. After showing a universal approximation result for SRTs, we present a decomposition training algorithm including a clustering-based initialization procedure and a heuristic for reassigning the input vectors along the tree. Under mild assumptions, we establish asymptotic convergence guarantees. Experiments on 15 wellknown datasets indicate that our SRTs and decomposition algorithm yield higher accuracy and robustness compared with traditional soft regression trees trained using the nonlinear optimization formulation of Blanquero et al., and a significant reduction in training times as well as a slightly better average accuracy compared with the mixed-integer optimization approach of Bertsimas and Dunn. We also report a comparison with the Random Forest ensemble method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05942v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Consolo, Edoardo Amaldi, Andrea Manno</dc:creator>
    </item>
    <item>
      <title>Nonlinear port-Hamiltonian system identification from input-state-output data</title>
      <link>https://arxiv.org/abs/2501.06118</link>
      <description>arXiv:2501.06118v1 Announce Type: cross 
Abstract: A framework for identifying nonlinear port-Hamiltonian systems using input-state-output data is introduced. The framework utilizes neural networks' universal approximation capacity to effectively represent complex dynamics in a structured way. We show that using the structure helps to make long-term predictions compared to baselines that do not incorporate physics. We also explore different architectures based on MLPs, KANs, and using prior information. The technique is validated through examples featuring nonlinearities in either the skew-symmetric terms, the dissipative terms, or the Hamiltonian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06118v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karim Cherifi, Achraf El Messaoudi, Hannes Gernandt, Marco Roschkowski</dc:creator>
    </item>
    <item>
      <title>Meta-Learning for Physically-Constrained Neural System Identification</title>
      <link>https://arxiv.org/abs/2501.06167</link>
      <description>arXiv:2501.06167v1 Announce Type: cross 
Abstract: We present a gradient-based meta-learning framework for rapid adaptation of neural state-space models (NSSMs) for black-box system identification. When applicable, we also incorporate domain-specific physical constraints to improve the accuracy of the NSSM. The major benefit of our approach is that instead of relying solely on data from a single target system, our framework utilizes data from a diverse set of source systems, enabling learning from limited target data, as well as with few online training iterations. Through benchmark examples, we demonstrate the potential of our approach, study the effect of fine-tuning subnetworks rather than full fine-tuning, and report real-world case studies to illustrate the practical application and generalizability of the approach to practical problems with physical-constraints. Specifically, we show that the meta-learned models result in improved downstream performance in model-based state estimation in indoor localization and energy systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06167v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ankush Chakrabarty, Gordon Wichern, Vedang M. Deshpande, Abraham P. Vinod, Karl Berntorp, Christopher R. Laughman</dc:creator>
    </item>
    <item>
      <title>Best Response Convergence for Zero-sum Stochastic Dynamic Games with Partial and Asymmetric Information</title>
      <link>https://arxiv.org/abs/2501.06181</link>
      <description>arXiv:2501.06181v1 Announce Type: cross 
Abstract: We analyze best response dynamics for finding a Nash equilibrium of an infinite horizon zero-sum stochastic linear quadratic dynamic game (LQDG) with partial and asymmetric information. We derive explicit expressions for each player's best response within the class of pure linear dynamic output feedback control strategies where the internal state dimension of each control strategy is an integer multiple of the system state dimension. With each best response, the players form increasingly higher-order belief states, leading to infinite-dimensional internal states. However, we observe in extensive numerical experiments that the game's value converges after just a few iterations, suggesting that strategies associated with increasingly higher-order belief states eventually provide no benefit. To help explain this convergence, our numerical analysis reveals rapid decay of the controllability and observability Gramian eigenvalues and Hankel singular values in higher-order belief dynamics, indicating that the higher-order belief dynamics become increasingly difficult for both players to control and observe. Consequently, the higher-order belief dynamics can be closely approximated by low-order belief dynamics with bounded error, and thus feedback strategies with limited internal state dimension can closely approximate a Nash equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06181v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiang Guan, Iman Shames, Tyler H. Summers</dc:creator>
    </item>
    <item>
      <title>An automatic system to detect equivalence between iterative algorithms</title>
      <link>https://arxiv.org/abs/2105.04684</link>
      <description>arXiv:2105.04684v4 Announce Type: replace 
Abstract: When are two algorithms the same? How can we be sure a recently proposed algorithm is novel, and not a minor twist on an existing method? In this paper, we present a framework for reasoning about equivalence between a broad class of iterative algorithms, with a focus on algorithms designed for convex optimization. We propose several notions of what it means for two algorithms to be equivalent, and provide computationally tractable means to detect equivalence. Our main definition, oracle equivalence, states that two algorithms are equivalent if they result in the same sequence of calls to the function oracles (for suitable initialization). Borrowing from control theory, we use state-space realizations to represent algorithms and characterize algorithm equivalence via transfer functions. Our framework can also identify and characterize some algorithm transformations including permutations of the update equations, repetition of the iteration, and conjugation of some of the function oracles in the algorithm. To support the paper, we have developed a software package named Linnaeus that implements the framework to identify other iterative algorithms that are equivalent to an input algorithm. More broadly, this framework and software advances the goal of making mathematics searchable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.04684v4</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shipu Zhao, Laurent Lessard, Madeleine Udell</dc:creator>
    </item>
    <item>
      <title>Counter-examples in first-order optimization: a constructive approach</title>
      <link>https://arxiv.org/abs/2303.10503</link>
      <description>arXiv:2303.10503v3 Announce Type: replace 
Abstract: While many approaches were developed for obtaining worst-case complexity bounds for first-order optimization methods in the last years, there remain theoretical gaps in cases where no such bound can be found. In such cases, it is often unclear whether no such bound exists (e.g., because the algorithm might fail to systematically converge) or simply if the current techniques do not allow finding them.
  In this work, we propose an approach to automate the search for cyclic trajectories generated by first-order methods. This provides a constructive approach to show that no appropriate complexity bound exists, thereby complementing the approaches providing sufficient conditions for convergence. Using this tool, we provide ranges of parameters for which some of the famous heavy-ball, Nesterov accelerated gradient, inexact gradient descent, and three-operator splitting algorithms fail to systematically converge, and show that it nicely complements existing tools searching for Lyapunov functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10503v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Baptiste Goujaud, Aymeric Dieuleveut, Adrien Taylor</dc:creator>
    </item>
    <item>
      <title>Designing a Framework for Solving Multiobjective Simulation Optimization Problems</title>
      <link>https://arxiv.org/abs/2304.06881</link>
      <description>arXiv:2304.06881v3 Announce Type: replace 
Abstract: Multiobjective simulation optimization (MOSO) problems are optimization problems with multiple conflicting objectives, where evaluation of at least one of the objectives depends on a black-box numerical code or real-world experiment, which we refer to as a simulation. While an extensive body of research is dedicated to developing new algorithms and methods for solving these and related problems, it is challenging and time consuming to integrate these techniques into real world production-ready solvers. This is partly due to the diversity and complexity of modern state-of-the-art MOSO algorithms and methods and partly due to the complexity and specificity of many real-world problems and their corresponding computing environments. The complexity of this problem is only compounded when introducing potentially complex and/or domain-specific surrogate modeling techniques, problem formulations, design spaces, and data acquisition functions. This paper carefully surveys the current state-of-the-art in MOSO algorithms, techniques, and solvers; as well as problem types and computational environments where MOSO is commonly applied. We then present several key challenges in the design of a Parallel Multiobjective Simulation Optimization framework (ParMOO) and how they have been addressed. Finally, we provide two case studies demonstrating how customized ParMOO solvers can be quickly built and deployed to solve real-world MOSO problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06881v3</guid>
      <category>math.OC</category>
      <category>cs.MS</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/ijoc.2023.0250</arxiv:DOI>
      <dc:creator>Tyler H. Chang, Stefan M. Wild</dc:creator>
    </item>
    <item>
      <title>On Lie-Bracket Averaging for a Class of Hybrid Dynamical Systems with Applications to Model-Free Control and Optimization</title>
      <link>https://arxiv.org/abs/2308.15732</link>
      <description>arXiv:2308.15732v2 Announce Type: replace 
Abstract: The stability of dynamical systems with oscillatory behaviors and well-defined average vector fields has traditionally been studied using averaging theory. These tools have also been applied to hybrid dynamical systems, which combine continuous and discrete dynamics. However, most averaging results for hybrid systems are limited to first-order methods, hindering their use in systems and algorithms that require high-order averaging techniques, such as hybrid Lie-bracket-based extremum seeking algorithms and hybrid vibrational controllers. To address this limitation, we introduce a novel high-order averaging theorem for analyzing the stability of hybrid dynamical systems with high-frequency periodic flow maps. These systems incorporate set-valued flow maps and jump maps, effectively modeling well-posed differential and difference inclusions. By imposing appropriate regularity conditions, we establish results on $(T,\varepsilon)$-closeness of solutions and semi-global practical asymptotic stability for sets. These theoretical results are then applied to the study of three distinct applications in the context of hybrid model-free control and optimization via Lie-bracket averaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15732v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahmoud Abdelgalil, Jorge I. Poveda</dc:creator>
    </item>
    <item>
      <title>Monotone Causality in Opportunistically Stochastic Shortest Path Problems</title>
      <link>https://arxiv.org/abs/2310.14121</link>
      <description>arXiv:2310.14121v3 Announce Type: replace 
Abstract: When traveling through a graph with an accessible deterministic path to a target, is it ever preferable to resort to stochastic node-to-node transitions instead? And if so, what are the conditions guaranteeing that such a stochastic optimal routing policy can be computed efficiently? We aim to answer these questions here by defining a class of Opportunistically Stochastic Shortest Path (OSSP) problems and deriving sufficient conditions for applicability of non-iterative label-setting methods. The usefulness of this framework is demonstrated in two very different contexts: numerical analysis and autonomous vehicle routing. We use OSSPs to derive causality conditions for semi-Lagrangian discretizations of anisotropic Hamilton-Jacobi equations. We also use a Dijkstra-like method to solve OSSPs optimizing the timing and urgency of lane change maneuvers for an autonomous vehicle navigating road networks with a heterogeneous traffic load.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14121v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mallory E. Gaspard, Alexander Vladimirsky</dc:creator>
    </item>
    <item>
      <title>Regularized MIP Model for Integrating Energy Storage Systems and its Application for Solving a Trilevel Interdiction Problem</title>
      <link>https://arxiv.org/abs/2402.04406</link>
      <description>arXiv:2402.04406v2 Announce Type: replace 
Abstract: Incorporating energy storage systems (ESS) into power systems has been studied in many recent works, where binary variables are often introduced to model the complementary nature of battery charging and discharging. A conventional approach for these ESS optimization problems is to relax binary variables and convert the problem into a linear program. However, such linear programming relaxation models can yield unrealistic fractional solutions, such as simultaneous charging and discharging. In this paper, we develop a regularized Mixed-Integer Programming (MIP) model for the ESS optimal power flow (OPF) problem. We prove that under mild conditions, the proposed regularized model admits a zero integrality gap with its linear programming relaxation; hence, it can be solved efficiently. By studying the properties of the regularized MIP model, we show that its optimal solution is also near-optimal to the original ESS OPF problem, thereby providing a valid and tight upper bound for the ESS OPF problem. The use of the regularized MIP model allows us to solve a trilevel min-max-min network contingency problem which is otherwise intractable to solve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04406v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dahye Han, Nan Jiang, Santanu S. Dey, Weijun Xie</dc:creator>
    </item>
    <item>
      <title>Gradient descent for unbounded convex functions on Hadamard manifolds and its applications to scaling problems</title>
      <link>https://arxiv.org/abs/2404.09746</link>
      <description>arXiv:2404.09746v2 Announce Type: replace 
Abstract: In this paper, we study asymptotic behaviors of continuous-time and discrete-time gradient flows of a ``lower-unbounded" convex function $f$ on a Hadamard manifold $M$, particularly, their convergence properties to the boundary $M^{\infty}$ at infinity of $M$. We establish a duality theorem that the infimum of the gradient-norm $\|\nabla f(x)\|$ of $f$ over $M$ is equal to the supremum of the negative of the recession function $f^{\infty}$ of $f$ over the boundary $M^{\infty}$, provided the infimum is positive. Further, the infimum and the supremum are obtained by the limits of the gradient flows of $f$, Our results feature convex-optimization ingredients of the moment-weight inequality for reductive group actions by Georgoulas, Robbin, and Salamon,and are applied to noncommutative optimization by B\"urgisser et al. FOCS 2019. We show that the gradient descent of the Kempf-Ness function for an unstable orbit converges to a 1-parameter subgroup in the Hilbert-Mumford criterion, and the associated moment-map sequence converges to the mimimum-norm point of the moment polytope. We show further refinements for operator scaling -- the left-right action on a matrix tuple $A= (A_1,A_2,\ldots,A_N)$. We characterize the gradient-flow limit of operator scaling by a vector-space generalization of the classical Dulmage-Mendelsohn decomposition of a bipartite graph. Also, for a special case of $N = 2$, we reveal that this limit determines the Kronecker canonical form of matrix pencils $s A_1+A_2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09746v2</guid>
      <category>math.OC</category>
      <category>math.DG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroshi Hirai, Keiya Sakabe</dc:creator>
    </item>
    <item>
      <title>A stochastic first-order method with multi-extrapolated momentum for highly smooth unconstrained optimization</title>
      <link>https://arxiv.org/abs/2412.14488</link>
      <description>arXiv:2412.14488v2 Announce Type: replace 
Abstract: In this paper, we consider an unconstrained stochastic optimization problem where the objective function exhibits high-order smoothness. Specifically, we propose a new stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum update based on these extrapolations. We demonstrate that the proposed SFOM can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Assuming that the $p$th-order derivative of $f$ is Lipschitz continuous for some $p\ge2$, and under additional mild assumptions, we establish that our method achieves a sample complexity of $\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})$ for finding a point $x$ such that $\mathbb{E}[\|\nabla f(x)\|]\le\epsilon$. To the best of our knowledge, this is the first SFOM to leverage arbitrary-order smoothness of the objective function for acceleration, resulting in a sample complexity that improves upon the best-known results without assuming the mean-squared smoothness condition. Preliminary numerical experiments validate the practical performance of our method and support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14488v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuan He</dc:creator>
    </item>
    <item>
      <title>Cyber Risk Assessment for Capital Management</title>
      <link>https://arxiv.org/abs/2205.08435</link>
      <description>arXiv:2205.08435v4 Announce Type: replace-cross 
Abstract: This paper introduces a two-pillar cyber risk management framework to address the pervasive challenges in managing cyber risk. The first pillar, cyber risk assessment, combines insurance frequency-severity models with cybersecurity cascade models to capture the unique nature of cyber risk. The second pillar, cyber capital management, facilitates informed allocation of capital for a balanced cyber risk management strategy, including cybersecurity investments, insurance coverage, and reserves. A case study, based on historical cyber incident data and realistic assumptions, demonstrates the necessity of comprehensive cost-benefit analysis for budget-constrained companies with competing objectives in cyber risk management. In addition, sensitivity analysis highlights the dependence of the optimal strategy on factors such as the price of cybersecurity controls and their effectiveness. The framework's implementation across a diverse range of companies yields general insights on cyber risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.08435v4</guid>
      <category>q-fin.RM</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wing Fung Chong, Runhuan Feng, Hins Hu, Linfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent</title>
      <link>https://arxiv.org/abs/2401.11940</link>
      <description>arXiv:2401.11940v3 Announce Type: replace-cross 
Abstract: This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11940v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSP.2024.3504292</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Signal Processing, Year:2024, Volume: 72, Pages: 5470 - 5483</arxiv:journal_reference>
      <dc:creator>Zhiyu Liu, Zhi Han, Yandong Tang, Xi-Le Zhao, Yao Wang</dc:creator>
    </item>
    <item>
      <title>Generic controllability of equivariant systems and applications to particle systems and neural networks</title>
      <link>https://arxiv.org/abs/2404.08289</link>
      <description>arXiv:2404.08289v2 Announce Type: replace-cross 
Abstract: There exist many examples of systems which have some symmetries, and which one may monitor with symmetry preserving controls. Since symmetries are preserved along the evolution, full controllability is not possible, and controllability has to be considered inside sets of states with same symmetries. We prove that generic systems with symmetries are controllable in this sense. This result has several applications, for instance:  (i) generic controllability of particle systems when the kernel of interaction between particles plays the role of a mean-field control;  (ii)  generic controllability for families of vector fields on manifolds with boundary; (iii) universal interpolation for neural networks architectures with "generic" self attention-type layers - a type of layers ubiquitous in recent neural networks architectures, e.g., in the Transformers architecture. The tools we develop could help address various other questions of control of equivariant systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08289v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Agrachev (SISSA / ISAS, CaGE), Cyril Letrouit (LMO, INSMI-CNRS)</dc:creator>
    </item>
    <item>
      <title>Log-Scale Quantization in Distributed First-Order Methods: Gradient-based Learning from Distributed Data</title>
      <link>https://arxiv.org/abs/2406.00621</link>
      <description>arXiv:2406.00621v3 Announce Type: replace-cross 
Abstract: Decentralized strategies are of interest for learning from large-scale data over networks. This paper studies learning over a network of geographically distributed nodes/agents subject to quantization. Each node possesses a private local cost function, collectively contributing to a global cost function, which the considered methodology aims to minimize. In contrast to many existing papers, the information exchange among nodes is log-quantized to address limited network-bandwidth in practical situations. We consider a first-order computationally efficient distributed optimization algorithm (with no extra inner consensus loop) that leverages node-level gradient correction based on local data and network-level gradient aggregation only over nearby nodes. This method only requires balanced networks with no need for stochastic weight design. It can handle log-scale quantized data exchange over possibly time-varying and switching network setups. We study convergence over both structured networks (for example, training over data-centers) and ad-hoc multi-agent networks (for example, training over dynamic robotic networks). Through experimental validation, we show that (i) structured networks generally result in a smaller optimality gap, and (ii) log-scale quantization leads to a smaller optimality gap compared to uniform quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00621v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Doostmohammadian, Muhammad I. Qureshi, Mohammad Hossein Khalesi, Hamid R. Rabiee, Usman A. Khan</dc:creator>
    </item>
    <item>
      <title>Prophet Inequalities: Competing with the Top $\ell$ Items is Easy</title>
      <link>https://arxiv.org/abs/2408.07616</link>
      <description>arXiv:2408.07616v2 Announce Type: replace-cross 
Abstract: We explore a prophet inequality problem, where the values of a sequence of items are drawn i.i.d. from some distribution, and an online decision maker must select one item irrevocably. We establish that $\mathrm{CR}_{\ell}$ the worst-case competitive ratio between the expected optimal performance of an online decision maker compared to that of a prophet who uses the average of the top $\ell$ items is exactly the solution to an integral equation. This quantity $\mathrm{CR}_{\ell}$ is larger than $1-e^{-\ell}$. This implies that the bound converges exponentially fast to $1$ as $\ell$ grows. In particular for $\ell=2$, $\mathrm{CR}_{2} \approx 0.966$ which is much closer to $1$ than the classical bound of $0.745$ for $\ell=1$. Additionally, we prove asymptotic lower bounds for the competitive ratio of a more general scenario, where the decision maker is permitted to select $k$ items. This subsumes the $k$ multi-unit i.i.d. prophet problem and provides the current best asymptotic guarantees, as well as enables broader understanding in the more general framework. Finally, we prove a tight asymptotic competitive ratio when only static threshold policies are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07616v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Molina, Nicolas Gast, Patrick Loiseau, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of wide shallow neural operators within the framework of Neural Tangent Kernel</title>
      <link>https://arxiv.org/abs/2412.05545</link>
      <description>arXiv:2412.05545v3 Announce Type: replace-cross 
Abstract: Neural operators are aiming at approximating operators mapping between Banach spaces of functions, achieving much success in the field of scientific computing. Compared to certain deep learning-based solvers, such as Physics-Informed Neural Networks (PINNs), Deep Ritz Method (DRM), neural operators can solve a class of Partial Differential Equations (PDEs). Although much work has been done to analyze the approximation and generalization error of neural operators, there is still a lack of analysis on their training error. In this work, we conduct the convergence analysis of gradient descent for the wide shallow neural operators and physics-informed shallow neural operators within the framework of Neural Tangent Kernel (NTK). The core idea lies on the fact that over-parameterization and random initialization together ensure that each weight vector remains near its initialization throughout all iterations, yielding the linear convergence of gradient descent. In this work, we demonstrate that under the setting of over-parametrization, gradient descent can find the global minimum regardless of whether it is in continuous time or discrete time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05545v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xianliang Xu, Ye Li, Zhongyi Huang</dc:creator>
    </item>
    <item>
      <title>Wait-Less Offline Tuning and Re-solving for Online Decision Making</title>
      <link>https://arxiv.org/abs/2412.09594</link>
      <description>arXiv:2412.09594v2 Announce Type: replace-cross 
Abstract: Online linear programming (OLP) has found broad applications in revenue management and resource allocation. State-of-the-art OLP algorithms achieve low regret by repeatedly solving linear programming (LP) subproblems that incorporate updated resource information. However, LP-based methods are computationally expensive and often inefficient for large-scale applications. In contrast, recent first-order OLP algorithms are more computationally efficient but typically suffer from worse regret guarantees. To address these shortcomings, we propose a new algorithm that combines the strengths of LP-based and first-order OLP methods. The algorithm re-solves the LP subproblems periodically at a predefined frequency $f$ and uses the latest dual prices to guide online decision-making. In addition, a first-order method runs in parallel during each interval between LP re-solves, smoothing resource consumption. Our algorithm achieves $\mathscr{O}(\log (T/f) + \sqrt{f})$ regret, delivering a "wait-less" online decision-making process that balances the computational efficiency of first-order methods and the superior regret guarantee of LP-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09594v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jingruo Sun, Wenzhi Gao, Ellen Vitercik, Yinyu Ye</dc:creator>
    </item>
  </channel>
</rss>
