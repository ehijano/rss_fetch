<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hierarchical Maximum Likelihood Estimation for Time-Resolved NMR Data</title>
      <link>https://arxiv.org/abs/2508.14902</link>
      <description>arXiv:2508.14902v1 Announce Type: cross 
Abstract: Metabolic monitoring and reaction rate estimation using hyperpolarized NMR technology requires accurate quantitative analysis of multidimensional data scenarios. Currently, this analysis is often performed in a two-stage procedure, which is prone to errors in uncertainty propagation and estimation. We propose an approach derived from a Bayesian hierarchical model that intrinsically propagates uncertainties and operates on the full data to maximize the precision at minimal uncertainty. In an analytic treatment, we reduce the estimation procedure to a least-squares optimization problem which can be understood as an extension of the Variable Projection (VarPro) approach for data scenarios with two predictors. We investigate the method's efficacy in two experiments with hyperpolarized metabolites recorded with conventional high-field NMR devices and a micronscale NMR setup using Nitrogen-Vacancy centers in diamond for detection, respectively. In both examples, the new approach improves estimates compared to Fourier methods and proves operational advantages over a two-stage procedure employing VarPro. While the approach presented is motivated by NMR analysis, it is straightforwardly applicable to further estimation scenarios with similar data structure, such as time-resolved photospectroscopy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14902v1</guid>
      <category>q-bio.QM</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lennart H. Bosch, Pernille R. Jensen, Nico Striegler, Thomas Unden, Jochen Scharpf, Usman Qureshi, Philipp Neumann, Martin Gierse, John W. Blanchard, Stephan Knecht, Jochen Scheuer, Ilai Schwart, Martin B. Plenio</dc:creator>
    </item>
    <item>
      <title>Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony</title>
      <link>https://arxiv.org/abs/2506.03302</link>
      <description>arXiv:2506.03302v2 Announce Type: replace-cross 
Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with interpretability, making them valuable for scientific modeling. However, it is unclear a priori how deep a network needs to be for any given task, and deeper KANs can be difficult to optimize and interpret. Here we introduce multi-exit KANs, where each layer includes its own prediction branch, enabling the network to make accurate predictions at multiple depths simultaneously. This architecture provides deep supervision that improves training while discovering the right level of model complexity for each task. Multi-exit KANs consistently outperform standard, single-exit versions on synthetic functions, dynamical systems, and real-world datasets. Remarkably, the best predictions often come from earlier, simpler exits, revealing that these networks naturally identify smaller, more parsimonious and interpretable models without sacrificing accuracy. To automate this discovery, we develop a differentiable "learning-to-exit" algorithm that balances contributions from exits during training. Our approach offers scientists a practical way to achieve both high performance and interpretability, addressing a fundamental challenge in machine learning for scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03302v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-2153/adf9bd</arxiv:DOI>
      <arxiv:journal_reference>Mach. Learn.: Sci. Technol. 6 035037 (2025)</arxiv:journal_reference>
      <dc:creator>James Bagrow, Josh Bongard</dc:creator>
    </item>
  </channel>
</rss>
