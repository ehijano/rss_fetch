<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL</title>
      <link>https://arxiv.org/abs/2508.20738</link>
      <description>arXiv:2508.20738v1 Announce Type: new 
Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof assistant. It attempts to close the current goal using a given list of lemmas. Typically these lemmas are found by Sledgehammer, a tool that integrates external automatic provers. We present a new tool that analyzes successful Metis proofs to derive variable instantiations. These increase Sledgehammer's success rate, improve the speed of Sledgehammer-generated proofs, and help users understand why a goal follows from the lemmas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20738v1</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-99984-0_30</arxiv:DOI>
      <dc:creator>Lukas Bartl, Jasmin Blanchette, Tobias Nipkow</dc:creator>
    </item>
    <item>
      <title>P2C: Path to Counterfactuals</title>
      <link>https://arxiv.org/abs/2508.20371</link>
      <description>arXiv:2508.20371v1 Announce Type: cross 
Abstract: Machine-learning models are increasingly driving decisions in high-stakes settings, such as finance, law, and hiring, thus, highlighting the need for transparency. However, the key challenge is to balance transparency -- clarifying `why' a decision was made -- with recourse: providing actionable steps on `how' to achieve a favourable outcome from an unfavourable outcome. Counterfactual explanations reveal `why' an undesired outcome occurred and `how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore causal dependencies between features, and 2) they typically assume all interventions can happen simultaneously, an unrealistic assumption in practical scenarios where actions are typically taken in a sequence. As a result, these counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that produces a plan (ordered sequence of actions) converting an unfavourable outcome to a causally consistent favourable outcome. P2C addresses both limitations by 1) Explicitly modelling causal relationships between features and 2) Ensuring that each intermediate state in the plan is feasible and causally valid. P2C uses the goal-directed Answer Set Programming system s(CASP) to generate the plan accounting for feature changes that happen automatically due to causal dependencies. Furthermore, P2C refines cost (effort) computation by only counting changes actively made by the user, resulting in realistic cost estimates. Finally, P2C highlights how its causal planner outperforms standard planners, which lack causal knowledge and thus can generate illegal actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20371v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sopam Dasgupta, Sadaf MD Halim, Joaqu\'in Arias, Elmer Salazar, Gopal Gupta</dc:creator>
    </item>
    <item>
      <title>Formal equivalence between global optimization consistency and random search</title>
      <link>https://arxiv.org/abs/2508.20671</link>
      <description>arXiv:2508.20671v1 Announce Type: cross 
Abstract: We formalize a proof that any stochastic and iterative global optimization algorithm is consistent over Lipschitz continuous functions if and only if it samples the whole search space. To achieve this, we use the L$\exists$$\forall$N theorem prover and the Mathlib library. The major challenge of this formalization, apart from the technical aspects of the proof itself, is to converge to a definition of a stochastic and iterative global optimization algorithm that is both general enough to encompass all algorithms of this type and specific enough to be used in a formal proof. We define such an algorithm as a pair of an initial probability measure and a sequence of Markov kernels that describe the distribution of the next point sampled by the algorithm given the previous points and their evaluations. We then construct a probability measure on finite and infinite sequences of iterations of the algorithm using the Ionescu-Tulcea theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20671v1</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ga\"etan Serr\'e (ENS Paris Saclay, CB)</dc:creator>
    </item>
    <item>
      <title>Efficient Neuro-Symbolic Learning of Constraints and Objective</title>
      <link>https://arxiv.org/abs/2508.20978</link>
      <description>arXiv:2508.20978v1 Announce Type: cross 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20978v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marianne Defresne, Romain Gambardella, Sophie Barbe, Thomas Schiex</dc:creator>
    </item>
    <item>
      <title>Aczel-Mendler Bisimulations in a Regular Category</title>
      <link>https://arxiv.org/abs/2303.04442</link>
      <description>arXiv:2303.04442v5 Announce Type: replace 
Abstract: Aczel-Mendler bisimulations are a coalgebraic extension of a variety of computational relations between systems. It is usual to assume that the underlying category satisfies some form of the axiom of choice, so that the collection of bisimulations enjoys desirable properties, such as closure under composition. In this paper, we accommodate the definition in general regular categories and toposes. We show that this general definition: 1) is closed under composition without using the axiom of choice, 2) coincides with other types of coalgebraic formulations under milder conditions, 3) coincides with the usual definition when the category satisfies the regular axiom of choice. In particular, the case of toposes heavily relies on power-objects, for which we recover some favourable properties along the way. Finally, we describe several examples in Stone spaces, toposes for name-passing, and modules over a ring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04442v5</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Dubut</dc:creator>
    </item>
    <item>
      <title>A Characterization of Basic Feasible Functionals Through Higher-Order Rewriting and Tuple Interpretations</title>
      <link>https://arxiv.org/abs/2401.12385</link>
      <description>arXiv:2401.12385v5 Announce Type: replace 
Abstract: The class of type-two basic feasible functionals ($\mathtt{BFF}_2$) is the analogue of $\mathtt{FP}$ (polynomial time functions) for type-2 functionals, that is, functionals that can take (first-order) functions as arguments. $\mathtt{BFF}_2$ can be defined through Oracle Turing machines with running time bounded by second-order polynomials. On the other hand, higher-order term rewriting provides an elegant formalism for expressing higher-order computation. We address the problem of characterizing $\mathtt{BFF}_2$ by higher-order term rewriting. Various kinds of interpretations for first-order term rewriting have been introduced in the literature for proving termination and characterizing first-order complexity classes. In this paper, we consider a recently introduced notion of cost-size interpretations for higher-order term rewriting and see second order rewriting as ways of computing type-2 functionals. We then prove that the class of functionals represented by higher-order terms admitting polynomially bounded cost-size interpretations exactly corresponds to $\mathtt{BFF}_2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12385v5</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Patrick Baillot, Ugo Dal Lago, Cynthia Kop, Deivid Vale</dc:creator>
    </item>
    <item>
      <title>Satisfiability Modulo Exponential Integer Arithmetic</title>
      <link>https://arxiv.org/abs/2402.01501</link>
      <description>arXiv:2402.01501v4 Announce Type: replace 
Abstract: SMT solvers use sophisticated techniques for polynomial (linear or non-linear) integer arithmetic. In contrast, non-polynomial integer arithmetic has mostly been neglected so far. However, in the context of program verification, polynomials are often insufficient to capture the behavior of the analyzed system without resorting to approximations. In the last years, incremental linearization has been applied successfully to satisfiability modulo real arithmetic with transcendental functions. We adapt this approach to an extension of polynomial integer arithmetic with exponential functions. Here, the key challenge is to compute suitable lemmas that eliminate the current model from the search space if it violates the semantics of exponentiation. An empirical evaluation of our implementation shows that our approach is highly effective in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01501v4</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Frohn, J\"urgen Giesl</dc:creator>
    </item>
    <item>
      <title>Non-wellfounded parsimonious proofs and non-uniform complexity</title>
      <link>https://arxiv.org/abs/2404.03311</link>
      <description>arXiv:2404.03311v2 Announce Type: replace 
Abstract: In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality ! is interpreted as a constructor for streams over finite data. We present non-wellfounded parsimonious proof systems capturing the classes $\mathbf{FPTIME}$ and $\mathbf{FP}/\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination. Completeness relies on an encoding of polynomial Turing machines with advice.
  As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03311v2</guid>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Acclavio, Gianluca Curzi, Giulio Guerrieri</dc:creator>
    </item>
    <item>
      <title>Application of AI to formal methods - an analysis of current trends</title>
      <link>https://arxiv.org/abs/2411.14870</link>
      <description>arXiv:2411.14870v2 Announce Type: replace 
Abstract: Context: With artificial intelligence (AI) being well established within the daily lives of research communities, we turn our gaze toward formal methods (FM). FM aim to provide sound and verifiable reasoning about problems in computer science. Objective: We conduct a systematic mapping study to overview the current landscape of research publications that apply AI to FM. We aim to identify how FM can benefit from AI techniques and highlight areas for further research. Our focus lies on the previous five years (2019-2023) of research. Method: Following the proposed guidelines for systematic mapping studies, we searched for relevant publications in four major databases, defined inclusion and exclusion criteria, and applied extensive snowballing to uncover potential additional sources. Results: This investigation results in 189 entries which we explored to find current trends and highlight research gaps. We find a strong focus on AI in the area of theorem proving while other subfields of FM are less represented. Conclusions: The mapping study provides a quantitative overview of the modern state of AI application in FM. The current trend of the field is yet to mature. Many primary studies focus on practical application, yet we identify a lack of theoretical groundwork, standard benchmarks, or case studies. Further, we identify issues regarding shared training data sets and standard benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14870v2</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Stock, Jannik Dunkelau, Atif Mashkoor</dc:creator>
    </item>
    <item>
      <title>Rethinking meaning and ontologies from the perspective of ontological units</title>
      <link>https://arxiv.org/abs/2503.21661</link>
      <description>arXiv:2503.21661v2 Announce Type: replace-cross 
Abstract: Ontologies enable knowledge sharing and interdisciplinary collaboration by providing standardized, structured vocabularies for diverse communities. While logical axioms are a cornerstone of ontology design, natural language elements such as annotations are equally critical for conveying intended meaning and ensuring consistent term usage. This paper explores how meaning is represented in ontologies and how it can be effectively represented and communicated, addressing challenges such as indeterminacy of reference and meaning holism. To this end, instead of following the conventional approach of beginning with existing ontologies and working toward alignment or modularization, this article proposes a reversal of perspective: taking the ontological term as the starting point and introducing a new structure, named 'ontological unit', characterized by: a term-centered design; enhanced characterization of both formal and natural language statements; and an operationalizable definition of communicated meaning based on general assertions. By formalizing the meaning of ontological units, this work seeks to enhance the semantic robustness of terms, improving their clarity and accessibility across domains. Furthermore, it may offer a more effective foundation for ontology generation and significantly improves support for key maintenance tasks such as reuse and versioning. This article aims to establish the theoretical groundwork for the proposed approach and to lay the foundations for future applications in applied ontologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21661v2</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Fabry, Adrien Barton, Jean-Fran\c{c}ois \'Ethier</dc:creator>
    </item>
    <item>
      <title>Prover-Adversary games for systems over (non-deterministic) branching programs</title>
      <link>https://arxiv.org/abs/2508.16014</link>
      <description>arXiv:2508.16014v2 Announce Type: replace-cross 
Abstract: We introduce Pudlak-Buss style Prover-Adversary games to characterise proof systems reasoning over deterministic branching programs (BPs) and non-deterministic branching programs (NBPs). Our starting points are the proof systems eLDT and eLNDT, for BPs and NBPs respectively, previously introduced by Buss, Das and Knop. We prove polynomial equivalences between these proof systems and the corresponding games we introduce. This crucially requires access to a form of negation of branching programs which, for NBPs, requires us to formalise a non-uniform version of the Immerman-Szelepcsenyi theorem that coNL = NL. Thanks to the techniques developed, we further obtain a proof complexity theoretic version of Immerman-Szelepcsenyi, showing that eLNDT is polynomially equivalent to systems over boundedly alternating branching programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16014v2</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anupam Das, Avgerinos Delkos</dc:creator>
    </item>
    <item>
      <title>Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks (extended version)</title>
      <link>https://arxiv.org/abs/2508.19430</link>
      <description>arXiv:2508.19430v2 Announce Type: replace-cross 
Abstract: Formal verification is crucial for ensuring the robustness of security protocols against adversarial attacks. The Needham-Schroeder protocol, a foundational authentication mechanism, has been extensively studied, including its integration with Physical Layer Security (PLS) techniques such as watermarking and jamming. Recent research has used ProVerif to verify these mechanisms in terms of secrecy. However, the ProVerif-based approach limits the ability to improve understanding of security beyond verification results. To overcome these limitations, we re-model the same protocol using an Isabelle formalism that generates sound animation, enabling interactive and automated formal verification of security protocols. Our modelling and verification framework is generic and highly configurable, supporting both cryptography and PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy and authenticity in four different eavesdropper locations under both passive and active attacks) using our new web interface. Our findings not only successfully reproduce and reinforce previous results on secrecy but also reveal an uncommon but expected outcome: authenticity is preserved across all examined scenarios, even in cases where secrecy is compromised. We have proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and jamming, and our analysis shows that it is secure for deriving a session key with required authentication. These highlight the advantages of our novel approach, demonstrating its robustness in formally verifying security properties beyond conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19430v2</guid>
      <category>cs.CR</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangfeng Ye, Roberto Metere, Jim Woodcock, Poonam Yadav</dc:creator>
    </item>
  </channel>
</rss>
