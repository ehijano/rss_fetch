<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Feb 2026 02:33:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2602.13789</link>
      <description>arXiv:2602.13789v1 Announce Type: new 
Abstract: As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing "Newtonian" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, "Head-of-Line" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13789v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyan Chu</dc:creator>
    </item>
    <item>
      <title>ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies</title>
      <link>https://arxiv.org/abs/2602.14107</link>
      <description>arXiv:2602.14107v1 Announce Type: new 
Abstract: Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14107v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuze Liu, Shibo Chu, Tiehua Zhang, Hao Zhou, Zhishu Shen, Jinze Wang, Jianzhong Qi, Feng Xia</dc:creator>
    </item>
    <item>
      <title>Floe: Federated Specialization for Real-Time LLM-SLM Inference</title>
      <link>https://arxiv.org/abs/2602.14302</link>
      <description>arXiv:2602.14302v1 Announce Type: new 
Abstract: Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14302v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chunlin Tian, Kahou Tam, Yebo Wu, Shuaihang Zhong, Li Li, Nicholas D. Lane, Chengzhong Xu</dc:creator>
    </item>
    <item>
      <title>Efficient Multi-round LLM Inference over Disaggregated Serving</title>
      <link>https://arxiv.org/abs/2602.14516</link>
      <description>arXiv:2602.14516v1 Announce Type: new 
Abstract: With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.
  In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14516v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao He, Youhe Jiang, Penghao Zhao, Quanqing Xu, Eiko Yoneki, Bin Cui, Fangcheng Fu</dc:creator>
    </item>
    <item>
      <title>Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement</title>
      <link>https://arxiv.org/abs/2602.14704</link>
      <description>arXiv:2602.14704v1 Announce Type: new 
Abstract: Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14704v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zong Yu Lee, Xueyan Tang</dc:creator>
    </item>
    <item>
      <title>Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?</title>
      <link>https://arxiv.org/abs/2602.09937</link>
      <description>arXiv:2602.09937v1 Announce Type: cross 
Abstract: Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09937v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taeyoon Kim, Woohyeok Park, Hoyeong Yun, Kyungyong Lee</dc:creator>
    </item>
    <item>
      <title>MergePipe: A Budget-Aware Parameter Management System for Scalable LLM Merging</title>
      <link>https://arxiv.org/abs/2602.13273</link>
      <description>arXiv:2602.13273v1 Announce Type: cross 
Abstract: Large language model (LLM) merging has become a key technique in modern LLM development pipelines, enabling the integration of multiple task- or domain-specific expert models without retraining. However, as the number of experts grows, existing merging implementations treat model parameters as unstructured files and execute merges in a stateless, one-shot manner, leading to excessive disk I/O, redundant parameter scans, and poor scalability.
  In this paper, we present \textbf{MergePipe}, a parameter management system for scalable LLM merging. MergePipe is the first system that treats LLM merging as a data management and execution problem, and introduces a catalog-driven abstraction over model parameters, merge plans, and execution lineage. At its core, MergePipe employs a cost-aware planner that explicitly models expert parameter I/O and enforces user-specified I/O budgets, followed by a streaming execution engine that materializes merged models under transactional guarantees. Our key insight is that while base model reads and output writes are unavoidable, expert parameter reads dominate merge cost and constitute the primary optimization target. By making expert access budget-aware throughout planning and execution, MergePipe mitigates the $O(K)$ I/O growth of naive pipelines and achieves predictable scaling behavior. Experiments show that MergePipe reduces total I/O by up to an order of magnitude and delivers up to $11\times$ end-to-end speedups (up to 90\% wall-time reduction) over state-of-the-art LLM merging pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13273v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanyi Wang, Yanggan Gu, Zihao Wang, Kunxi Li, Yifan Yang, Zhaoyi Yan, Congkai Xie, Jianmin Wu, Hongxia Yang</dc:creator>
    </item>
    <item>
      <title>Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs</title>
      <link>https://arxiv.org/abs/2602.13277</link>
      <description>arXiv:2602.13277v1 Announce Type: cross 
Abstract: Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13277v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uma Mahesh Boda, Mallikharjuna Rao Nuka</dc:creator>
    </item>
    <item>
      <title>Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators</title>
      <link>https://arxiv.org/abs/2602.13334</link>
      <description>arXiv:2602.13334v1 Announce Type: cross 
Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13334v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Liu, Suhaib A. Fahmy</dc:creator>
    </item>
    <item>
      <title>Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.13486</link>
      <description>arXiv:2602.13486v1 Announce Type: cross 
Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13486v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</dc:creator>
    </item>
    <item>
      <title>SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity</title>
      <link>https://arxiv.org/abs/2602.13542</link>
      <description>arXiv:2602.13542v1 Announce Type: cross 
Abstract: Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.
  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.
  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13542v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George M. Gichuru, Zoe Aiyanna M. Cayetano</dc:creator>
    </item>
    <item>
      <title>Parallel Sparse and Data-Sparse Factorization-based Linear Solvers</title>
      <link>https://arxiv.org/abs/2602.14289</link>
      <description>arXiv:2602.14289v1 Announce Type: cross 
Abstract: Efficient solutions of large-scale, ill-conditioned and indefinite algebraic equations are ubiquitously needed in numerous computational fields, including multiphysics simulations, machine learning, and data science. Because of their robustness and accuracy, direct solvers are crucial components in building a scalable solver toolchain. In this article, we will review recent advances of sparse direct solvers along two axes: 1) reducing communication and latency costs in both task- and data-parallel settings, and 2) reducing computational complexity via low-rank and other compression techniques such as hierarchical matrix algebra. In addition to algorithmic principles, we also illustrate the key parallelization challenges and best practices to deliver high speed and reliability on modern heterogeneous parallel machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14289v1</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoye Sherry Li, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows</title>
      <link>https://arxiv.org/abs/2602.14849</link>
      <description>arXiv:2602.14849v1 Announce Type: cross 
Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14849v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bardia Mohammadi, Nearchos Potamitis, Lars Klein, Akhil Arora, Laurent Bindschaedler</dc:creator>
    </item>
    <item>
      <title>EdgeFaaS: A Function-based Framework for Edge Computing</title>
      <link>https://arxiv.org/abs/2210.01410</link>
      <description>arXiv:2210.01410v2 Announce Type: replace 
Abstract: The rapid growth of data generated from Internet of Things (IoTs) such as smart phones and smart home devices presents new challenges to cloud computing in transferring, storing, and processing the data. With increasingly more powerful edge devices, edge computing, on the other hand, has the potential to better responsiveness, privacy, and cost efficiency. However, resources across the cloud and edge are highly distributed and highly diverse. To address these challenges, this paper proposes EdgeFaaS, a Function-as-a-Service (FaaS) based computing framework that supports the flexible, convenient, and optimized use of distributed and heterogeneous resources across IoT, edge, and cloud systems. EdgeFaaS allows cluster resources and individual devices to be managed under the same framework and provide computational and storage resources for functions. It provides virtual function and virtual storage interfaces for consistent function management and storage management across heterogeneous compute and storage resources. It automatically optimizes the scheduling of functions and placement of data according to their performance and privacy requirements. EdgeFaaS is evaluated based on two edge workflows: video analytics workflow and federated learning workflow, both of which are representative edge applications and involve large amounts of input data generated from edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01410v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runyu Jin, Qirui Yang</dc:creator>
    </item>
    <item>
      <title>uBFT: Microsecond-scale BFT using Disaggregated Memory [Extended Version]</title>
      <link>https://arxiv.org/abs/2210.17174</link>
      <description>arXiv:2210.17174v5 Announce Type: replace 
Abstract: We propose uBFT, the first State-Machine Replication (SMR) system to achieve microsecond-scale latency in data centers, while using only $2f{+}1$ replicas to tolerate $f$ Byzantine failures. The Byzantine Fault Tolerance (BFT) provided by uBFT is essential as pure crashes appear to be a mere illusion with real-life systems reportedly failing in many unexpected ways. uBFT relies on a small non-tailored trusted computing base -- disaggregated memory -- and consumes a practically bounded amount of memory (both local and disaggregated). uBFT is based on a novel abstraction called Consistent Tail Broadcast, which we use to prevent equivocation while bounding memory. We implement uBFT using RDMA-based disaggregated memory and obtain an end-to-end latency of as little as 10us. This is at least 50$\times$ faster than MinBFT , a state of the art $2f{+}1$ BFT SMR based on Intel's SGX. We use uBFT to replicate two key-value stores (Memcached and Redis), as well as a financial order matching engine (Liquibook). These applications have low latency (up to 20us) and become Byzantine tolerant with as little as 10us more. The price for uBFT is a small amount of reliable disaggregated memory (less than 1 MiB), which in our prototype consists of a small number of memory servers connected through RDMA and replicated for fault tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17174v5</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3575693.3575732</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2023)</arxiv:journal_reference>
      <dc:creator>Marcos K. Aguilera, Naama Ben-David, Rachid Guerraoui, Antoine Murat, Athanasios Xygkis, Igor Zablotchi</dc:creator>
    </item>
    <item>
      <title>Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing</title>
      <link>https://arxiv.org/abs/2408.10746</link>
      <description>arXiv:2408.10746v2 Announce Type: replace 
Abstract: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10746v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Jingyi Li, Jiangsu Du, Xiaowen Chu, Guoliang Xing, Xu Chen</dc:creator>
    </item>
    <item>
      <title>Understanding GPU Resource Interference One Level Deeper</title>
      <link>https://arxiv.org/abs/2501.16909</link>
      <description>arXiv:2501.16909v3 Announce Type: replace 
Abstract: GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16909v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic</dc:creator>
    </item>
    <item>
      <title>Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems</title>
      <link>https://arxiv.org/abs/2505.07755</link>
      <description>arXiv:2505.07755v2 Announce Type: replace 
Abstract: Edge computing has emerged as a pivotal technology, offering significant advantages such as low latency, enhanced data security, and reduced reliance on centralized cloud infrastructure. These benefits are crucial for applications requiring real-time data processing or strict security measures. Despite these advantages, edge devices operating within edge clusters are often underutilized. This inefficiency is mainly due to the absence of a holistic performance profiling mechanism which can help dynamically adjust the desired system configuration for a given workload. Since edge computing environments involve a complex interplay between CPU frequency, power consumption, and application performance, a deeper understanding of these correlations is essential. By uncovering these relationships, it becomes possible to make informed decisions that enhance both computational efficiency and energy savings. To address this gap, this paper evaluates the power consumption and performance characteristics of a single processing node within an edge cluster using a synthetic microbenchmark by varying the workload size and CPU frequency. The results show how an optimal measure can lead to optimized usage of edge resources, given both performance and power consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07755v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Szydlo, Viacheslaw Horbanov, Devki Nandan Jha, Shashikant Ilager, Aleksander Slominski, Rajiv Ranjan</dc:creator>
    </item>
    <item>
      <title>KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider</title>
      <link>https://arxiv.org/abs/2506.02634</link>
      <description>arXiv:2506.02634v5 Announce Type: replace 
Abstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02634v5</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Optimizing Task Scheduling in Fog Computing with Deadline Awareness</title>
      <link>https://arxiv.org/abs/2509.07378</link>
      <description>arXiv:2509.07378v5 Announce Type: replace 
Abstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07378v5</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sadegh Sirjani, Mohammad Ahmad, Amir Mousavi, Erfan Nourbakhsh, Khoa Nguyen</dc:creator>
    </item>
    <item>
      <title>Transforming Lock-free Linked Lists into Distributed Lock-free Linked Lists</title>
      <link>https://arxiv.org/abs/2510.06387</link>
      <description>arXiv:2510.06387v2 Announce Type: replace 
Abstract: Modern databases use dynamic search structures that store an enormous amount of data, and often serve them using multi-threaded algorithms to support the ever-increasing throughput needs. When this throughput need exceeds the capacity of the machine hosting the structure, one either needs to replace the underlying hardware (an option that is typically not viable and introduces a long down time) or make the data structure distributed. Static partitioning of the data structure for distribution is not desirable, as it is prone to uneven load distribution over time, and having to change the partitioning scheme later will require downtime.
  The goal of this paper is to extend a concurrent data structure to distributed data structures that provide dynamic load balancing while preserving important properties such as lock freedom. With this intuition, first, we introduce the notion of conditional lock-freedom that extends the notion of lock-free computation with reasonable assumptions about communication between processes. Then, we present DiLi, a conditional lock-free, linearizable, and distributable linked list that can be asynchronously and dynamically (1) partitioned into multiple sublists and (2) load balanced by distributing sublists across multiple machines. DiLi contains primitives for these that also maintain the lock-free property of the underlying search structure that supports find, remove, and insert of a key as the client operations.
  We show that DiLi bridges the gap between concurrent data structures and distributed data structures. Specifically, DiLi provides comparable (and better in write-intensive workloads) performance to skip lists (which are typically the fastest data structures for search in a concurrent environment). Plus, it provides horizontal scaling by permitting dynamic load balancing on a distributed environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06387v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raaghav Ravishankar, Sandeep Kulkarni, Sathya Peri, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Leader Election via Unique Sink Orientation</title>
      <link>https://arxiv.org/abs/2511.19208</link>
      <description>arXiv:2511.19208v2 Announce Type: replace 
Abstract: A Locally Checkable Labeling (LCL) is a specification describing a set of labels that are valid with respect to a set of conditions that characterize a local part of a solution to a global problem. Conditions can only refer to nodes and labels within a constant radius neighborhood of each node. This work studies local labeling schemes whose global consistency implies solutions to two classical problems: leader election and spanning tree construction. For each problem, we present a local labeling scheme using one bit per edge or equivalently $O(\Delta)$ bits per node (where $\Delta$ is the maximum degree in the graph), with conditions checkable within the graph induced by the one neighborhood of each node. For leader election, we show that global satisfaction of the conditions implies the existence of a unique sink in the graph, which we define to be a leader, while in the spanning tree setting it implies that a specific subset of edges induces a spanning tree rooted at a given node. We show those implications for $K_4$-free dismantlable and chordal graphs in the former case and for dismantlable graphs in the latter, assuming a root is given. For chordal graphs, the labeling implying a unique sink additionally induces an acyclic orientation. This property is not generally locally verifiable with constant-size labels in arbitrary graphs. To the best of our knowledge, these are the first local labeling results tailored to ($K_4$-free) dismantlable graphs, potentially highlighting structural properties useful for designing LCLs for additional problems. Finally, we present a generic transformation that converts any local labeling scheme into a silent self-stabilizing algorithm by adding only one extra state, assuming a Gouda fair scheduler. This transformation may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19208v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emie Chalopin, Maria Kokkou</dc:creator>
    </item>
    <item>
      <title>Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks</title>
      <link>https://arxiv.org/abs/2512.06784</link>
      <description>arXiv:2512.06784v3 Announce Type: replace 
Abstract: The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06784v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Long Shi, Bingyan Ou, Kang Wei, Weihao Zhu, Zhe Wang, Zhiyong Chen</dc:creator>
    </item>
    <item>
      <title>Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</title>
      <link>https://arxiv.org/abs/2512.19131</link>
      <description>arXiv:2512.19131v2 Announce Type: replace 
Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19131v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>SlimEdge: Performance and Device Aware Distributed DNN Deployment on Resource-Constrained Edge Hardware</title>
      <link>https://arxiv.org/abs/2512.22136</link>
      <description>arXiv:2512.22136v2 Announce Type: replace 
Abstract: Distributed deep neural networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts, computational demands, and the probability of device failure. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respect hardware limitations, preserve task performance, and remain robust to partial system failures. Our method integrates structured model pruning with a multi-objective optimization framework to tailor network capacity for heterogeneous device constraints, while explicitly accounting for device availability and failure probability during deployment. We demonstrate this framework using Multi-View Convolutional Neural Networks (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets accordingly. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint, even under multiple simultaneous device failures. The inference time is reduced by factors up to 4.7x across diverse simulated device configurations. These findings suggest that performance-aware, view-adaptive, and failure-resilient compression provides a viable pathway for deploying complex vision models in distributed edge environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22136v2</guid>
      <category>cs.DC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Rounak Chatterjee, Amitava Mukherjee</dc:creator>
    </item>
    <item>
      <title>Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving</title>
      <link>https://arxiv.org/abs/2512.22420</link>
      <description>arXiv:2512.22420v2 Announce Type: replace 
Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22420v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai</dc:creator>
    </item>
    <item>
      <title>Low-Latency Federated Fine-Tuning for Large Language Models Over Wireless Networks</title>
      <link>https://arxiv.org/abs/2602.01024</link>
      <description>arXiv:2602.01024v2 Announce Type: replace 
Abstract: Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01024v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwen Pang, Kang Wei, Long Shi, Zhe Wang, Jun Li, Feng Shu</dc:creator>
    </item>
    <item>
      <title>LCLs Beyond Bounded Degrees</title>
      <link>https://arxiv.org/abs/2602.02340</link>
      <description>arXiv:2602.02340v2 Announce Type: replace 
Abstract: The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees.
  We focus on the \emph{polynomial regime} ($\Theta(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting depends on how LCLs are generalized beyond bounded degrees.
  There already exists a complex construction that shows that the polynomial gaps also vanish for LCLs on unbounded degree trees. Rather than stopping at this negative result, we give a much simpler set of problems that already contradicts the existence of any polynomial gaps. The insight obtained from this cleaner construction is that for gap results to exist, we cannot allow problems to distinguish infinitely many cases.
  This guides us to a natural class of problems for which polynomial gap results can still be recovered. We introduce \emph{Locally Finite Labelings} (LFLs), which formalize the intuition that \emph{every node must fall into one of finitely many local cases}.
  Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $\Pi$ on trees with unbounded degrees, the deterministic LOCAL complexity of $\Pi$ is either
  - $\Theta(n^{1/k})$ for some integer $k \geq 1$, or
  - $O(\log n)$.
  Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $\Pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02340v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustav Schmid</dc:creator>
    </item>
    <item>
      <title>Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware Expert Selection</title>
      <link>https://arxiv.org/abs/2411.08982</link>
      <description>arXiv:2411.08982v2 Announce Type: replace-cross 
Abstract: Selective parameter activation provided by Mixture-of-Expert (MoE) models have made them a popular choice in modern foundational models. However, MoEs face a fundamental tension when employed for serving. Batching, critical for performance in serving, forces the activation of all experts, thereby negating MoEs' benefits and exacerbating memory bandwidth bottlenecks. Existing work on efficient MoE inference are unable to resolve this tension even with extensive workload-specific tuning. We present LYNX, a system that enables efficient MoE inference in a workload-agnostic fashion. Exploiting several key observations that we uncover in this work, LYNX provides a light-weight run-time dynamic expert remapping technique that depends only on information already available in the models. Our evaluation of LYNX on four state-of-the-art model families across nine benchmarks shows that it achieves up to 1.23x improvement in throughput while simultaneously improving accuracy by up to 4% in the majority of the tasks, and incurs only a negligible accuracy loss of less than 1% points in significantly hard tasks. Further, LYNX is complementary to existing techniques where it additionally boosts their performance by up to 1.38x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08982v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vima Gupta, Jae Hyung Ju, Kartik Sinha, Ada Gavrilovska, Anand Padmanabha Iyer</dc:creator>
    </item>
    <item>
      <title>A Generalized Hierarchical Federated Learning Framework with Theoretical Guarantees</title>
      <link>https://arxiv.org/abs/2505.08145</link>
      <description>arXiv:2505.08145v2 Announce Type: replace-cross 
Abstract: Almost all existing hierarchical federated learning (FL) models are limited to two aggregation layers, restricting scalability and flexibility in complex, large-scale networks. In this work, we propose a Multi-Layer Hierarchical Federated Learning framework (QMLHFL), which appears to be the first study that generalizes hierarchical FL to arbitrary numbers of layers and network architectures through nested aggregation, while employing a layer-specific quantization scheme to meet communication constraints. We develop a comprehensive convergence analysis for QMLHFL and derive a general convergence condition and rate that reveal the effects of key factors, including quantization parameters, hierarchical architecture, and intra-layer iteration counts. Furthermore, we determine the optimal number of intra-layer iterations to maximize the convergence rate while meeting a deadline constraint that accounts for both communication and computation times. Our results show that QMLHFL consistently achieves high learning accuracy, even under high data heterogeneity, and delivers notably improved performance when optimized, compared to using randomly selected values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08145v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione</dc:creator>
    </item>
    <item>
      <title>Pareto-optimal Trade-offs Between Communication and Computation with Flexible Gradient Tracking</title>
      <link>https://arxiv.org/abs/2509.18129</link>
      <description>arXiv:2509.18129v2 Announce Type: replace-cross 
Abstract: This paper addresses distributed stochastic optimization problems under non-i.i.d. data, focusing on the inherent trade-offs between communication and computational efficiency. To this end, we propose FlexGT, a flexible snapshot gradient tracking method that enables tunable numbers of local updates and neighbor communications per round, thereby adapting efficiently to diverse system resource conditions. Leveraging a unified convergence analysis framework, we derive tight communication and computational complexity for FlexGT with explicit dependence on objective properties and certain tunable parameters. Moreover, we introduce an accelerated variant, termed Acc-FlexGT, and prove that, with prior knowledge of the graph, it achieves Pareto-optimal trade-offs between communication and computation. Particularly, in the nonconvex case, Acc-FlexGT achieves the optimal iteration complexity of $\tilde{\mathcal{O}}\left( \left( L\sigma ^2 \right) /\left( n\epsilon ^2 \right) +L/\left( \epsilon \sqrt{1-\sqrt{\rho _W}} \right) \right) $ and optimal communication complexity of $\tilde{\mathcal{O}}\left( L/\left( \epsilon \sqrt{1-\sqrt{\rho _W}} \right) \right)$ for appropriately chosen numbers of local updates, matching existing lower bounds up to logarithmic factors. And, it improves the existing results for the strongly convex case by a factor of $\tilde{\mathcal{O}} \left( 1/\sqrt{\epsilon} \right)$, where $\epsilon$ is the targeted accuracy, $n$ the number of nodes, $L$ the Lipschitz constant, $\rho_W$ the connectivity of the graph, and $\sigma$ the stochastic gradient variance. Numerical experiments corroborate the theoretical results and demonstrate the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18129v2</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Huang, Jinming Xu, Li Chai, Jiming Chen, Karl H. Johansson</dc:creator>
    </item>
    <item>
      <title>Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number</title>
      <link>https://arxiv.org/abs/2512.17885</link>
      <description>arXiv:2512.17885v2 Announce Type: replace-cross 
Abstract: Magnetic fields are critical at many scales to galactic dynamics and structure, including multiphase pressure balance, dust processing, and star formation. Dynamo action determines their dynamical structure and strength. Simulations of combined large- and small-scale dynamos have successfully developed mean fields with strength and topology consistent with observations but with turbulent fields much weaker than observed, while simulations of small-scale dynamos with parameters relevant to the interstellar medium yield turbulent fields an order of magnitude below the values observed or expected theoretically. We use the Pencil Code accelerated on GPUs with Astaroth to perform high-resolution simulations of a supernova-driven galactic dynamo including heating and cooling in a periodic domain. Our models show that the strength of the turbulent field produced by the small-scale dynamo approaches an asymptote at only modest magnetic Prandtl numbers. This allows us to use these models to suggest the essential characteristics of this constituent of the magnetic field for inclusion in global galactic models. The asymptotic limit occurs already at magnetic Prandtl number of only a few hundred, many orders of magnitude below physical values in the the interstellar medium and consistent with previous findings for isothermal compressible flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17885v2</guid>
      <category>astro-ph.GA</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederick A. Gent, Mordecai-Mark Mac Low, Maarit J. Korpi-Lagg, Touko Puro, Matthias Reinhardt</dc:creator>
    </item>
    <item>
      <title>Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</title>
      <link>https://arxiv.org/abs/2601.21972</link>
      <description>arXiv:2601.21972v3 Announce Type: replace-cross 
Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21972v3</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato</dc:creator>
    </item>
    <item>
      <title>TriCloudEdge: A multi-layer Cloud Continuum</title>
      <link>https://arxiv.org/abs/2602.02121</link>
      <description>arXiv:2602.02121v2 Announce Type: replace-cross 
Abstract: TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02121v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Violettas, Lefteris Mamatas</dc:creator>
    </item>
  </channel>
</rss>
