<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence</title>
      <link>https://arxiv.org/abs/2503.03901</link>
      <description>arXiv:2503.03901v1 Announce Type: new 
Abstract: Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective indicator of vegetation productivity and plant health. The global quantification of SIF and its associated uncertainties yields many important capabilities, including improving carbon flux estimation, improving the identification of carbon sources and sinks, monitoring a variety of ecosystems, and evaluating carbon sequestration efforts. Long-term, regional-to-global scale monitoring is now feasible with the availability of SIF estimates from multiple Earth-observing satellites. These efforts can be aided by a rigorous accounting of the sources of uncertainty present in satellite SIF data products. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for the estimation of SIF and associated uncertainties from Orbiting Carbon Observatory-2 (OCO-2) satellite observations at one-degree resolution with global coverage. The hierarchical structure of our modeling framework allows for convenient model specification, quantification of various sources of variation, and the incorporation of seasonal SIF information through Fourier terms in the regression model. The modeling framework leverages the predictable seasonality of SIF in most temperate land areas. The resulting data product complements existing atmospheric carbon dioxide estimates at the same spatio-temporal resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03901v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manju Johny, Jonathan Hobbs, Vineet Yadav, Margaret Johnson, Nicholas Parazoo, Hai Nguyen, Amy Braverman</dc:creator>
    </item>
    <item>
      <title>Joint modeling of longitudinal HRQoL data accounting for the risk of competing dropouts</title>
      <link>https://arxiv.org/abs/2503.03919</link>
      <description>arXiv:2503.03919v1 Announce Type: new 
Abstract: In cancer clinical trials, health-related quality of life (HRQoL) is an important endpoint, providing information about patients' well-being and daily functioning. However, missing data due to premature dropout can lead to biased estimates, especially when dropouts are informative. This paper introduces the extJMIRT approach, a novel tool that efficiently analyzes multiple longitudinal ordinal categorical data while addressing informative dropout. Within a joint modeling framework, this approach connects a latent variable, derived from HRQoL data, to cause-specific hazards of dropout. Unlike traditional joint models, which treat longitudinal data as a covariate in the survival submodel, our approach prioritizes the longitudinal data and incorporates the log baseline dropout risks as covariates in the latent process. This leads to a more accurate analysis of longitudinal data, accounting for potential effects of dropout risks. Through extensive simulation studies, we demonstrate that extJMIRT provides robust and unbiased parameter estimates and highlight the importance of accounting for informative dropout. We also apply this methodology to HRQoL data from patients with progressive glioblastoma, showcasing its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03919v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hortense Doms, Philippe Lambert, Catherine Legrand</dc:creator>
    </item>
    <item>
      <title>A Robust and Distribution-Fitting-Free Estimation Approach of Travel Time Percentile Function based on L-moments</title>
      <link>https://arxiv.org/abs/2503.04062</link>
      <description>arXiv:2503.04062v1 Announce Type: new 
Abstract: Travel time is one of the key indicators monitored by intelligent transportation systems, helping the systems to gain real-time insights into traffic situations, predict congestion, and identify network bottlenecks. Travel time exhibits variability, and thus suitable probability distributions are necessary to accurately capture full information of travel time variability. Considering the potential issues of insufficient sample size and the disturbance of outliers in actual observations, as well as the heterogeneity of travel time distributions, we propose a robust and distribution-fitting-free estimation approach of travel time percentile function using L-moments based Normal-Polynomial Transformation. We examine the proposed approach from perspectives of validity, robustness, and stability based on both theoretical probability distributions and real data. The results indicate that the proposed approach exhibits high estimation validity, accuracy and low volatility in dealing with outliers, even in scenarios with small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04062v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiya Chen, Xiangdong Xu, Jianqiang Li</dc:creator>
    </item>
    <item>
      <title>Spectral Extremal Connectivity of Two-State Seizure Brain Waves</title>
      <link>https://arxiv.org/abs/2503.04169</link>
      <description>arXiv:2503.04169v1 Announce Type: new 
Abstract: Coherence analysis plays a vital role in the study of functional brain connectivity. However, coherence captures only linear spectral associations, and thus can produce misleading findings when ignoring variations of connectivity in the tails of the distribution. This limitation becomes important when investigating extreme neural events that are characterized by large signal amplitudes. The focus of this paper is to examine connectivity in the tails of the distribution, as this reveals salient information that may be overlooked by standard methods. We develop a novel notion of spectral tail association of periodograms to study connectivity in the network of electroencephalogram (EEG) signals of seizure-prone neonates. We further develop a novel non-stationary extremal dependence model for multivariate time series that captures differences in extremal dependence during different brain phases, namely burst-suppression and non-burst-suppression. One advantage of our proposed approach is its ability to identify tail connectivity at key frequency bands that could be associated with outbursts of energy which may lead to seizures. We discuss these novel scientific findings alongside a comparison of the extremal behavior of brain signals for epileptic and non-epileptic patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04169v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mara Sherlin D. Talento, Jordan Richards, Marco Pinto-Orellana, Raphael Huser, Hernando C. Ombao</dc:creator>
    </item>
    <item>
      <title>A Protocol to Exposure Path Analysis for Multiple Stressors Associated with Cardiovascular Disease Risk: A Novel Approach Using NHANES Data</title>
      <link>https://arxiv.org/abs/2503.04365</link>
      <description>arXiv:2503.04365v1 Announce Type: new 
Abstract: Background: Multiple medical and non-medical stressors, along with the complicity of their exposure pathways, have posted significant challenges to the epidemiological interpretation of the non-communicable diseases, including cardiovascular disease (CVD). Objective: To develop a protocol for deconstructing the complex exposure pathways linking various stressors to adverse outcomes and to elucidate the sequential determinants contributing to CVD risk in depth. Methods: In this study, we developed a Path-Lasso approach, rooted in Adaptive Lasso regression, to construct the network and paths to interpret the determinants of CVD in an in-depth way by using data from the National Health and Nutrition Examination Survey (NHANES). Univariate logistic regression was initially employed to screen out all potential factors of influencing CVD. Then a programmed approach, using Path-Lasso technique, stratified covariates and established a causal network to predict CVD risk. Results: Age, smoking and waist circumference were identified as the most significant predictors of CVD risk. Other factors, such as race, marital status, physical activity, cadmium exposure and diabetes acted as the intermediary or proximal variables. All these stressors (or nodes) formed the network with paths (or edges to link the CVD), in which the latent layer variables that causally associate to the outcome are linearly formed by the stressors in each layer. Discussion: The Path-Lasso approach revealed the epidemiological pathways, linking covariates to CVD risk, which is instrumental in elucidating the inter-covariate transitions of their predication to the outcome, and providing the hierarchal network for foundation of the assessment of CVD risk and the beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04365v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangling Liu, Ya Liu, Banyun Zheng, Longjian Liu, Heqing Shen</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v1 Announce Type: new 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has previously been stunted by lack of data. Critically, many low- and middle-income countries, where open burning is of greatest concern, have little to no air quality monitoring. Here, we propose an approach, at the intersection of modern causal inference and environmental data science, to leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we use the case study of Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries in the East Asia &amp; Pacific region, including Indonesia. We tailor cutting-edge statistical methods to this setting, estimating effects of the increase in plastic waste imports on fine particulate matter near waste dump sites in Indonesia and allowing effects to vary as a function of the site's proximity to ports (from which international plastic waste enters the country), which serves as an induced continuous exposure or "dose" of treatment. We observe a statistically significant increase in monthly fine particulate matter concentrations near dump sites after China's ban took effect (2018-2019) compared to concentrations expected under business-as-usual (2012-2017), with increases ranging from 0.76--1.72$\mu$g/m$^3$ (15--34\% of the World Health Organization's recommended limit for exposure on an annual basis) depending on the site's port proximity, at sites with port proximity above the 20th quantile. Sites with lower port proximity had smaller and not statistically significant effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Method for recovering data on unreported low-severity crashes</title>
      <link>https://arxiv.org/abs/2503.04529</link>
      <description>arXiv:2503.04529v1 Announce Type: new 
Abstract: Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04529v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Morando</dc:creator>
    </item>
    <item>
      <title>Granular mortality modelling with temperature and epidemic shocks: a three-state regime-switching approach</title>
      <link>https://arxiv.org/abs/2503.04568</link>
      <description>arXiv:2503.04568v1 Announce Type: new 
Abstract: This paper develops a granular regime-switching framework to model mortality deviations from seasonal baseline trends driven by temperature- and epidemic-related shocks. The framework features three states: (1) a baseline state that captures observed seasonal mortality patterns, (2) an environmental shock state for heat waves, and (3) a respiratory shock state that addresses mortality deviations caused by strong outbreaks of respiratory diseases due to influenza and COVID-19. Transition probabilities between states are modeled using covariate-dependent multinomial logit functions. These functions incorporate, among others, lagged temperature and influenza incidence rates as predictors, allowing dynamic adjustments to evolving shocks. Calibrated on weekly mortality data across 21 French regions and six age groups, the regime-switching framework accounts for spatial and demographic heterogeneity. Under various projection scenarios for temperature and influenza, we quantify uncertainty in mortality forecasts through prediction intervals constructed using an extensive bootstrap approach. These projections can guide healthcare providers and hospitals in managing risks and planning resources for potential future shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04568v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Robben, Karim Barigou, Torsten Kleinow</dc:creator>
    </item>
    <item>
      <title>Rapid updating of multivariate resource models based on new information using EnKF-MDA and multi-Gaussian transformation</title>
      <link>https://arxiv.org/abs/2503.04694</link>
      <description>arXiv:2503.04694v1 Announce Type: new 
Abstract: Rapid resource model updating with real-time data is important for making timely decisions in resource management and mining operations. This requires optimal merging of models and observations, which can be achieved through data assimilation, and the ensemble Kalman filter (EnKF) has become a popular method for this task. However, the modelled resources in mining usually consist of multiple variables of interest with multivariate relationships of varying complexity. EnKF is not a multivariate approach, and even for univariate cases, there may be slight deviations between its outcomes and observations. This study presents a methodology for rapidly updating multivariate resource models using the EnKF with multiple data assimilations (EnKF-MDA) combined with rotation based iterative Gaussianisation (RBIG). EnKF-MDA improves the updating by assimilating the same data multiple times with an inflated measurement error, while RBIG quickly transforms the data into multi-Gaussian factors. The application of the proposed algorithm is validated by a real case study with nine cross-correlated variables. The combination of EnKF-MDA and RBIG successfully improves the accuracy of resource model updates, minimises uncertainty, and preserves the multivariate relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04694v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sultan Abulkhair, Peter A. Dowd, Chaoshui Xu</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects with competing intercurrent events in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2503.03049</link>
      <description>arXiv:2503.03049v1 Announce Type: cross 
Abstract: The analysis of randomized controlled trials is often complicated by intercurrent events--events that occur after treatment initiation and may impact outcome assessment. These events may lead to patients discontinuing their assigned treatment or dropping out of the trial entirely. In an analysis of data from two recent immunology trials, we categorize intercurrent events into two broad types: those unrelated to treatment (e.g., withdrawal from the study due to external factors like pandemics or relocation) and those related to treatment (e.g., adverse events or lack of efficacy). We adopt distinct strategies to handle each type, aiming to target a clinically more relevant estimand. For treatment-related intercurrent events, they often meaningfully describe the patient's outcome, we employ a composite variable strategy, where we attribute an outcome value that reflects the lack of treatment success. For treatment-unrelated intercurrent events, we adopt a hypothetical strategy that assumes these event times are conditionally independent of the outcome, given treatment and covariates, and envisions a scenario in which the intercurrent events do not occur. We establish the nonparametric identification and semiparametric estimation theory for the causal estimand and introduce doubly robust estimators. We illustrate our methods through the re-analysis of two randomized trials on baricitinib for Systemic Lupus Erythematosus. We classify intercurrent events, apply four estimators, and compare our approach with common ad-hoc methods, highlighting the robustness and practical implications of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Yanyao Yi, Yongming Qu, Huayu Karen Liu, Ting Ye, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Escalation dynamics and the severity of wars</title>
      <link>https://arxiv.org/abs/2503.03945</link>
      <description>arXiv:2503.03945v1 Announce Type: cross 
Abstract: Although very large wars remain an enduring threat in global politics, we lack a clear understanding of how some wars become large and costly, while most do not. There are three possibilities: large conflicts start with and maintain intense fighting, they persist over a long duration, or they escalate in intensity over time. Using detailed within-conflict data on civil and interstate wars 1946--2008, we show that escalation dynamics -- variations in fighting intensity within an armed conflict -- play a fundamental role in producing large conflicts and are a generic feature of both civil and interstate wars. However, civil wars tend to deescalate when they become very large, limiting their overall severity, while interstate wars exhibit a persistent risk of continual escalation. A non-parametric model demonstrates that this distinction in escalation dynamics can explain the differences in the historical sizes of civil vs. interstate wars, and explain Richardson's Law governing the frequency and severity of interstate conflicts over the past 200 years. Escalation dynamics also drive enormous uncertainty in forecasting the eventual sizes of both hypothetical and ongoing civil wars, indicating a need to better understand the causes of escalation and deescalation within conflicts. The close relationship between the size, and hence the cost, of an armed conflict and its potential for escalation has broad implications for theories of conflict onset or termination and for risk assessment in international relations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03945v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Clauset, Barbara F. Walter, Lars-Erik Cederman, Kristian Skrede Gleditsch</dc:creator>
    </item>
    <item>
      <title>Data-Driven Probabilistic Air-Sea Flux Parameterization</title>
      <link>https://arxiv.org/abs/2503.03990</link>
      <description>arXiv:2503.03990v1 Announce Type: cross 
Abstract: Accurately quantifying air-sea fluxes is important for understanding air-sea interactions and improving coupled weather and climate systems. This study introduces a probabilistic framework to represent the highly variable nature of air-sea fluxes, which is missing in deterministic bulk algorithms. Assuming Gaussian distributions conditioned on the input variables, we use artificial neural networks and eddy-covariance measurement data to estimate the mean and variance by minimizing negative log-likelihood loss. The trained neural networks provide alternative mean flux estimates to existing bulk algorithms, and quantify the uncertainty around the mean estimates. Stochastic parameterization of air-sea turbulent fluxes can be constructed by sampling from the predicted distributions. Tests in a single-column forced upper-ocean model suggest that changes in flux algorithms influence sea surface temperature and mixed layer depth seasonally. The ensemble spread in stochastic runs is most pronounced during spring restratification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03990v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiarong Wu, Pavel Perezhogin, David John Gagne, Brandon Reichl, Aneesh C. Subramanian, Elizabeth Thompson, Laure Zanna</dc:creator>
    </item>
    <item>
      <title>Fiducial Confidence Intervals for Agreement Measures Among Raters Under a Generalized Linear Mixed Effects Model</title>
      <link>https://arxiv.org/abs/2503.04117</link>
      <description>arXiv:2503.04117v1 Announce Type: cross 
Abstract: A generalization of the classical concordance correlation coefficient (CCC) is considered under a three-level design where multiple raters rate every subject over time, and each rater is rating every subject multiple times at each measuring time point. The ratings can be discrete or continuous. A methodology is developed for the interval estimation of the CCC based on a suitable linearization of the model along with an adaptation of the fiducial inference approach. The resulting confidence intervals have satisfactory coverage probabilities and shorter expected widths compared to the interval based on Fisher Z-transformation, even under moderate sample sizes. Two real applications available in the literature are discussed. The first application is based on a clinical trial to determine if various treatments are more effective than a placebo for treating knee pain associated with osteoarthritis. The CCC was used to assess agreement among the manual measurements of the joint space widths on plain radiographs by two raters, and the computer-generated measurements of digitalized radiographs. The second example is on a corticospinal tractography, and the CCC was once again applied in order to evaluate the agreement between a well-trained technologist and a neuroradiologist regarding the measurements of fiber number in both the right and left corticospinal tracts. Other relevant applications of our general approach are highlighted in many areas including artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04117v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Learning Causal Response Representations through Direct Effect Analysis</title>
      <link>https://arxiv.org/abs/2503.04358</link>
      <description>arXiv:2503.04358v1 Announce Type: cross 
Abstract: We propose a novel approach for learning causal response representations. Our method aims to extract directions in which a multidimensional outcome is most directly caused by a treatment variable. By bridging conditional independence testing with causal representation learning, we formulate an optimisation problem that maximises the evidence against conditional independence between the treatment and outcome, given a conditioning set. This formulation employs flexible regression models tailored to specific applications, creating a versatile framework. The problem is addressed through a generalised eigenvalue decomposition. We show that, under mild assumptions, the distribution of the largest eigenvalue can be bounded by a known $F$-distribution, enabling testable conditional independence. We also provide theoretical guarantees for the optimality of the learned representation in terms of signal-to-noise ratio and Fisher information maximisation. Finally, we demonstrate the empirical effectiveness of our approach in simulation and real-world experiments. Our results underscore the utility of this framework in uncovering direct causal effects within complex, multivariate settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04358v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Gustau Camps-Valls</dc:creator>
    </item>
    <item>
      <title>Non-parametric kernel density estimation of magnitude distribution for the analysis of seismic hazard posed by anthropogenic seismicity</title>
      <link>https://arxiv.org/abs/2503.04393</link>
      <description>arXiv:2503.04393v1 Announce Type: cross 
Abstract: Frequent significant deviations of the observed magnitude distribution of anthropogenic seismicity from the Gutenberg-Richter relation require alternative estimation methods for probabilistic seismic hazard assessments. We evaluate five nonparametric kernel density estimation (KDE) methods on simulated samples drawn from four magnitude distribution models: the exponential, concave and convex bi-exponential, and exponential-Gaussian distributions. The latter three represent deviations from the Gutenberg-Richter relation due to the finite thickness of the seismogenic crust and the effect of characteristic earthquakes. The assumed deviations from exponentiality are never more than those met in practice. The studied KDE methods include Silverman's and Scott's rules with Abramson's bandwidth adaptation, two diffusion-based methods (ISJ and diffKDE), and adaptiveKDE, which formulates the bandwidth estimation as an optimization problem. We assess their performance for magnitudes from 2 to 6 with sample sizes of 400 to 5000, using the mean integrated square error (MISE) over 100,000 simulations. Their suitability in hazard assessments is illustrated by the mean of the mean return period (MRP) for a sample size of 1000. Among the tested methods, diffKDE provides the most accurate cumulative distribution function estimates for larger magnitudes. Even when the data is drawn from an exponential distribution, diffKDE performs comparably to maximum likelihood estimation when the sample size is at least 1000. Given that anthropogenic seismicity often deviates from the exponential model, we recommend using diffKDE for probabilistic seismic hazard assessments whenever a sufficient sample size is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04393v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Tong, Stanis{\l}aw Lasocki, Beata Orlecka-Sikora</dc:creator>
    </item>
    <item>
      <title>A decision analysis model for colorectal cancer screening</title>
      <link>https://arxiv.org/abs/2502.21210</link>
      <description>arXiv:2502.21210v2 Announce Type: replace 
Abstract: Background and Objective. With minor differences, most national colorectal cancer (CRC) screening programs in Europe consist of one-size-fits-all aged-based strategies. This paper provides a decision analysis-based approach to personalized CRC screening, supporting decisions concerning whether and which screening method to consider and/or whether a colonoscopy should be administered.
  Methods. We use an influence diagram which characterizes CRC risk with respect to different variables of interest and includes comfort, costs, complications, and information as decision criteria, the last one assessed through information theory measures. The criteria are integrated with a multi-attribute utility model. Optimal screening policies are then computed.
  Results. The proposed model is used to support personalized individual screening based on relevant characteristics. It serves to assess existing national screening programs and design new ones. In particular, it suggests replacing current age-based strategies followed in many European countries by more personalized strategies based on the type of model proposed. Additionally, the model facilitates benchmarking of novel screening devices.
  Conclusions. This work creates a framework supporting personalized CRC screening improving upon current age-based screening strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21210v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Corrales, David R\'ios Insua, Marino J. Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v4 Announce Type: replace-cross 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Penetrance Estimation in Family-based Studies with the penetrance R package</title>
      <link>https://arxiv.org/abs/2411.18816</link>
      <description>arXiv:2411.18816v2 Announce Type: replace-cross 
Abstract: Reliable methods for penetrance (age-specific risk among those who carry a genetic variant) estimation are critical to improving clinical decision making and risk assessment for hereditary syndromes. We introduce penetrance, an open-source R package, to estimate age-specific penetrance using family-history pedigree data. The package employs a Bayesian estimation approach, allowing for the incorporation of prior knowledge through the specification of priors for the parameters of the carrier distribution. It also includes options to impute missing ages during the estimation process, addressing incomplete age information which is not uncommon in pedigree datasets. Our software provides a flexible and user-friendly tool for researchers to estimate penetrance in complex family-based studies, facilitating improved genetic risk assessment in hereditary syndromes. The penetrance R package is freely available on CRAN, with accompanying documentation and examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18816v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Kubista, Danielle Braun, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Methods for differential network estimation: an empirical comparison</title>
      <link>https://arxiv.org/abs/2412.17922</link>
      <description>arXiv:2412.17922v2 Announce Type: replace-cross 
Abstract: We provide a review and a comparison of methods for differential network estimation in Gaussian graphical models with focus on structure learning. We consider the case of two datasets from distributions associated with two graphical models. In our simulations, we use five different methods to estimate differential networks. We vary graph structure and sparsity to explore their influence on performance in terms of power and false discovery rate. We demonstrate empirically that presence of hubs proves to be a challenge for all the methods, as well as increased density. We suggest local and global properties that are associated with this challenge. Direct estimation with lasso penalized D-trace loss is shown to perform the best across all combinations of network structure and sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17922v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Plaksienko, Magne Thoresen, Vera Djordjilovi\'c</dc:creator>
    </item>
    <item>
      <title>Scenario Analysis with Multivariate Bayesian Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.08440</link>
      <description>arXiv:2502.08440v2 Announce Type: replace-cross 
Abstract: We present an econometric framework that adapts tools for scenario analysis, such as variants of conditional forecasts and impulse response functions, for use with dynamic nonparametric multivariate models. We demonstrate the utility of our approach with simulated data and three real-world applications: (1) scenario-based conditional forecasts aligned with Federal Reserve stress test assumptions, measuring (2) macroeconomic risk under varying financial conditions, and (3) asymmetric effects of US-based financial shocks and their international spillovers. Our results indicate the importance of nonlinearities and asymmetries in dynamic relationships between macroeconomic and financial variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08440v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Pfarrhofer, Anna Stelzer</dc:creator>
    </item>
  </channel>
</rss>
