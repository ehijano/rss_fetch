<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Automated HEMT Model Construction from Datasheets via Multi-Modal Intelligence and Prior-Knowledge-Free Optimization</title>
      <link>https://arxiv.org/abs/2507.21430</link>
      <description>arXiv:2507.21430v1 Announce Type: new 
Abstract: Parameter extraction for industry-standard device models like ASM-HEMT is crucial in circuit design workflows. However, many manufacturers do not provide such models, leaving users to build them using only datasheets. Unfortunately, datasheets lack sufficient information for standard step-by-step extraction. Moreover, manual data extraction from datasheets is highly time-consuming, and the absence of a fully automated method forces engineers to perform tedious manual work. To address this challenge, this paper introduces a novel, end-to-end framework that fully automates the generation of simulation-ready ASM-HEMT SPICE models directly from PDF datasheets. Our framework is founded on two core innovations: 1) a multi-modal AI pipeline that integrates computer vision with a large language model (LLM) to robustly parse heterogeneous datasheet layouts and digitize characteristic curves, and 2) a novel Iterative-Focusing Tree-structured Parzen Estimator (IF-TPE) optimization algorithm is specifically designed for device parameter extraction under the high-dimensional, sparse-data condition by adaptively refining the parameter search space. Experimental validation on a diverse set of 17 commercial HEMT devices from 10 manufacturers confirms the framework's accuracy and robustness. The generated models demonstrate excellent agreement with published DC and RF characteristics. As the first fully automated workflow of its kind, our proposed solution offers a transformative approach to device modeling, poised to significantly accelerate the circuit design cycle by eliminating the need for manual parameter extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21430v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuang Peng, Jiarui Zhong, Yang Zhang, Hong Cai Chen</dc:creator>
    </item>
    <item>
      <title>SLTarch: Towards Scalable Point-Based Neural Rendering by Taming Workload Imbalance and Memory Irregularity</title>
      <link>https://arxiv.org/abs/2507.21499</link>
      <description>arXiv:2507.21499v1 Announce Type: new 
Abstract: Rendering is critical in fields like 3D modeling, AR/VR, and autonomous driving, where high-quality, real-time output is essential. Point-based neural rendering (PBNR) offers a photorealistic and efficient alternative to conventional methods, yet it is still challenging to achieve real-time rendering on mobile platforms. We pinpoint two major bottlenecks in PBNR pipelines: LoD search and splatting. LoD search suffers from workload imbalance and irregular memory access, making it inefficient on off-the-shelf GPUs. Meanwhile, splatting introduces severe warp divergence across GPU threads due to its inherent sparsity.
  To tackle these challenges, we propose SLTarch, an algorithm-architecture co-designed framework. At its core, SLTarch introduces SLTree, a dedicated subtree-based data structure, and LTcore, a specialized hardware architecture tailored for efficient LoD search. Additionally, we co-design a divergence-free splatting algorithm with our simple yet principled hardware augmentation, SPcore, to existing PBNR accelerators. Compared to a mobile GPU, SLTarch achieves 3.9$\times$ speedup and 98\% energy savings with negligible architecture overhead. Compared to existing accelerator designs, SLTarch achieves 1.8$\times$ speedup with 54\% energy savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21499v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyang Li, Jie Jiang, Yu Feng, Yiming Gan, Jieru Zhao, Zihan Liu, Jingwen Leng, Minyi Guo</dc:creator>
    </item>
    <item>
      <title>No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for Real-time Rendering</title>
      <link>https://arxiv.org/abs/2507.21572</link>
      <description>arXiv:2507.21572v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and is getting increasing adoption in domains like autonomous driving and embodied intelligence. However, 3DGS still faces major efficiency challenges when faced with high frame rate requirements and resource-constrained edge deployment. To enable efficient 3DGS, in this paper, we propose LS-Gaussian, an algorithm/hardware co-design framework for lightweight streaming 3D rendering. LS-Gaussian is motivated by the core observation that 3DGS suffers from substantial computation redundancy and stalls. On one hand, in practical scenarios, high-frame-rate 3DGS is often applied in settings where a camera observes and renders the same scene continuously but from slightly different viewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian proposes a viewpoint transformation algorithm that leverages inter-frame continuity for efficient sparse rendering. On the other hand, as different tiles within an image are rendered in parallel but have imbalanced workloads, frequent hardware stalls also slow down the rendering process. LS-Gaussian predicts the workload for each tile based on viewpoint transformation to enable more balanced parallel computation and co-designs a customized 3DGS accelerator to support the workload-aware mapping in real-time. Experimental results demonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline on average and up to 17.3x speedup with the customized accelerator, while incurring only minimal visual quality degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21572v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linye Wei, Jiajun Tang, Fan Fei, Boxin Shi, Runsheng Wang, Meng Li</dc:creator>
    </item>
    <item>
      <title>A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation</title>
      <link>https://arxiv.org/abs/2507.21694</link>
      <description>arXiv:2507.21694v1 Announce Type: new 
Abstract: As large language models demonstrate enormous potential in the field of Electronic Design Automation (EDA), generative AI-assisted chip design is attracting widespread attention from academia and industry. Although these technologies have made preliminary progress in tasks such as code generation, their application in chip verification -- a critical bottleneck in the chip development cycle -- remains at an exploratory stage. This paper proposes an innovative Multi-Agent Verification Framework (MAVF) aimed at addressing the limitations of current single-LLM approaches in complex verification tasks. Our framework builds an automated transformation system from design specifications to testbench through the collaborative work of multiple specialized agents, including specification parsing, verification strategy generation, and code implementation. Through verification experiments on multiple chip modules of varying complexity, results show that MAVF significantly outperforms traditional manual methods and single-dialogue generative AI approaches in verification document parsing and generation, as well as automated testbench generation. This research opens new directions for exploring generative AI applications in verification automation, potentially providing effective approaches to solving the most challenging bottleneck issues in chip design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21694v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbo Liu, Forbes Hou, Jon Zhang, Hong Liu, Allen Lei</dc:creator>
    </item>
  </channel>
</rss>
