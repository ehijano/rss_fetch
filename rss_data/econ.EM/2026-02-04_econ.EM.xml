<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 05:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AI Assisted Economics Measurement From Survey: Evidence from Public Employee Pension Choice</title>
      <link>https://arxiv.org/abs/2602.02604</link>
      <description>arXiv:2602.02604v1 Announce Type: new 
Abstract: We develop an iterative framework for economic measurement that leverages large language models to extract measurement structure directly from survey instruments. The approach maps survey items to a sparse distribution over latent constructs through what we term a soft mapping, aggregates harmonized responses into respondent level sub dimension scores, and disciplines the resulting taxonomy through out of sample incremental validity tests and discriminant validity diagnostics. The framework explicitly integrates iteration into the measurement construction process. Overlap and redundancy diagnostics trigger targeted taxonomy refinement and constrained remapping, ensuring that added measurement flexibility is retained only when it delivers stable out of sample performance gains. Applied to a large scale public employee retirement plan survey, the framework identifies which semantic components contain behavioral signal and clarifies the economic mechanisms, such as beliefs versus constraints, that matter for retirement choices. The methodology provides a portable measurement audit of survey instruments that can guide both empirical analysis and survey design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02604v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiancheng Wang, Krishna Sharma</dc:creator>
    </item>
    <item>
      <title>The Innovation Tax: Generative AI Adoption, Productivity Paradox, and Systemic Risk in the U.S. Banking Sector</title>
      <link>https://arxiv.org/abs/2602.02607</link>
      <description>arXiv:2602.02607v1 Announce Type: new 
Abstract: This paper evaluates the causal impact of Generative Artificial Intelligence (GenAI) adoption on productivity and systemic risk in the U.S. banking sector. Using a novel dataset linking SEC 10-Q filings to Federal Reserve regulatory data for 809 financial institutions over 2018--2025, we employ two complementary identification strategies: Dynamic Spatial Durbin Models (DSDM) to capture network spillovers and Synthetic Difference-in-Differences (SDID) for causal inference using the November 2022 ChatGPT release as an exogenous shock. Our findings reveal a striking ``Productivity Paradox'': while DSDM estimates show that AI-adopting banks are high performers ($\beta &gt; 0$), the causal SDID analysis documents a significant ``Implementation Tax'' -- adopting banks experience a 428-basis-point decline in ROE as they absorb GenAI integration costs. This tax falls disproportionately on smaller institutions, with bottom-quartile banks suffering a 517-basis-point ROE decline compared to 129 basis points for larger banks, suggesting that economies of scale provide significant advantages in AI implementation. Most critically, our DSDM analysis reveals significant positive spillovers ($\theta = 0.161$ for ROA, $p &lt; 0.01$; $\theta = 0.679$ for ROE, $p &lt; 0.05$), with spillovers among large banks reaching $\theta = 3.13$ for ROE, indicating that the U.S. banking system is becoming ``algorithmically coupled.'' This synchronization of AI-driven decision-making creates a new channel for systemic contagion: a technical failure in widely-adopted AI models could trigger correlated shocks across the entire financial network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02607v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.CP</category>
      <category>q-fin.GN</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Predicting Well-Being with Mobile Phone Data: Evidence from Four Countries</title>
      <link>https://arxiv.org/abs/2602.02805</link>
      <description>arXiv:2602.02805v1 Announce Type: new 
Abstract: We provide systematic evidence on the potential for estimating household well-being from mobile phone data. Using data from four countries - Afghanistan, Cote d'Ivoire, Malawi, and Togo - we conduct parallel, standardized machine learning experiments to assess which measures of welfare can be most accurately predicted, which types of phone data are most useful, and how much training data is required. We find that long-term poverty measures such as wealth indices (Pearson's rho = 0.20-0.59) and multidimensional poverty (rho = 0.29-0.57) can be predicted more accurately than consumption (rho = 0.04 - 0.54); transient vulnerability measures like food security and mental health are very difficult to predict. Models using calls and text message behavior are more predictive than those using metadata on mobile internet usage, mobile money transactions, and airtime top-ups. Predictive accuracy improves rapidly through the first 1,000-2,000 training observations, with continued gains beyond 4,500 observations. Model performance depends strongly on sample heterogeneity: nationally-representative samples yield 20-70 percent higher accuracy than urban-only or rural-only samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02805v1</guid>
      <category>econ.EM</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Merritt Smith, Emily Aiken, Joshua E. Blumenstock, Sveta Milusheva</dc:creator>
    </item>
    <item>
      <title>Unbiased Estimation of Central Moments in Unbalanced Two- and Three-Level Models</title>
      <link>https://arxiv.org/abs/2602.03469</link>
      <description>arXiv:2602.03469v1 Announce Type: new 
Abstract: This paper derives closed-form unbiased estimators of central moments in multilevel random-effects models with unbalanced group sizes. In a two-level model, we provide unbiased estimators for the second, third, and fourth central moments under both group-level and observation-level averaging. In a three-level model, we provide unbiased estimators for the second and third central moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03469v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Ben-Moshe, David Genesove</dc:creator>
    </item>
    <item>
      <title>Global Testing in Multivariate Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2602.03819</link>
      <description>arXiv:2602.03819v1 Announce Type: new 
Abstract: Regression discontinuity (RD) designs with multiple running variables arise in a growing number of empirical applications, including geographic boundaries and multi-score assignment rules. Although recent methodological work has extended estimation and inference tools to multivariate settings, far less attention has been devoted to developing global testing methods that formally assess whether a discontinuity exists anywhere along a multivariate treatment boundary. Existing approaches perform well in large samples, but can exhibit severe size distortions in moderate or small samples due to the sparsity of observations near any particular boundary point. This paper introduces a complementary global testing procedure that mitigates the small-sample weaknesses of existing multivariate RD methods by integrating multivariate machine learning estimators with a distance-based aggregation strategy, yielding a test statistic that remains reliable with limited data. Simulations demonstrate that the proposed method maintains near-nominal size and strong power, including in settings where standard multivariate estimators break down. The procedure is applied to an empirical setting to demonstrate its implementation and to illustrate how it can complement existing multivariate RD estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03819v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artem Samiahulin</dc:creator>
    </item>
    <item>
      <title>Count Data Models with Heterogeneous Peer Effects under Rational Expectations</title>
      <link>https://arxiv.org/abs/2405.17290</link>
      <description>arXiv:2405.17290v5 Announce Type: replace 
Abstract: This paper develops a peer effect model for count responses under rational expectations. The model accounts for heterogeneity in peer effects across groups based on observed characteristics. Identification is based on the linear model condition that requires the presence of friends of friends who are not direct friends. I show that this identification condition extends to a broad class of nonlinear models. Parameters are estimated using a nested pseudo-likelihood approach. An empirical application to students' extracurricular participation reveals that females are more responsive to peers than males. An easy-to-use R package, CDatanet, is available for implementing the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17290v5</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aristide Houndetoungan</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference on Average Treatment Effect in Percentage Points under Heterogeneity</title>
      <link>https://arxiv.org/abs/2408.06624</link>
      <description>arXiv:2408.06624v3 Announce Type: replace 
Abstract: In semi-logarithmic regressions, treatment coefficients are often interpreted as approximations of the average treatment effect (ATE) in percentage points. This paper highlights the overlooked bias of this approximation under treatment effect heterogeneity, arising from Jensen's inequality. The issue is particularly relevant for difference-in-differences designs with log-transformed outcomes and staggered treatment adoption, where treatment effects often vary across groups and periods. This paper proposes new estimation and inference methods for an estimand that accounts for heterogeneity across observable subgroups and improves upon conventional measures. The estimand provides a lower bound on the ATE in percentage points, and coincides with it in the absence of within-group heterogeneity. I establish the methods' large-sample properties and study their finite-sample performance through Monte Carlo experiments, which reveal substantial discrepancies between conventional and proposed measures when systematic heterogeneity is large. Two empirical applications further underscore the practical importance of these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06624v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zeng</dc:creator>
    </item>
    <item>
      <title>Factorial Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2407.11937</link>
      <description>arXiv:2407.11937v5 Announce Type: replace-cross 
Abstract: We formulate factorial difference-in-differences (FDID), a research design that extends canonical difference-in-differences (DID) to settings in which an event affects all units. In many panel data applications, researchers exploit cross-sectional variation in a baseline factor alongside temporal variation in the event, but the corresponding estimand is often implicit and the justification for applying the DID estimator remains unclear. We frame FDID as a factorial design with two factors, the baseline factor $G$ and the exposure level $Z$, and define effect modification and causal moderation as the associative and causal effects of $G$ on the effect of $Z$, respectively. Under standard DID assumptions of no anticipation and parallel trends, the DID estimator identifies effect modification but not causal moderation. Identifying the latter requires an additional \emph{factorial parallel trends} assumption, that is, mean independence between $G$ and potential outcome trends. We extend the framework to conditionally valid assumptions and regression-based implementations, and further to repeated cross-sectional data and continuous $G$. We demonstrate the framework with an empirical application on the role of social capital in famine relief in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11937v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Anqi Zhao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Are Princelings Truly Busted? Evaluating Transaction Discounts in China's Land Market</title>
      <link>https://arxiv.org/abs/2502.07692</link>
      <description>arXiv:2502.07692v2 Announce Type: replace-cross 
Abstract: This paper replicates Chen and Kung's 2019 analysis ($The$ $Quarterly$ $Journal$ $of$ $Economics$ 134(1): 185-226). Inspecting the data reveals that nearly one-third of transactions (388,903 out of 1,208,621) are perfect duplicates of other rows, excluding the transaction number. The analysis on the data sans duplicates replicates their statistically significant princeling effect, robust across various specifications. Further analysis reveals a disagreement between Chen and Kung's text and code: the paper's ''logarithm of area'' is actually area ($\text{m}^2$) divided by one million. This therefore necessitates a reinterpretation of the estimation results, revealing that the princeling effect is extremely large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07692v2</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julia Manso</dc:creator>
    </item>
    <item>
      <title>How weak are weak factors? Uniform inference for signal strength in signal plus noise models</title>
      <link>https://arxiv.org/abs/2507.18554</link>
      <description>arXiv:2507.18554v2 Announce Type: replace-cross 
Abstract: The paper analyzes four classical signal-plus-noise models: the factor model, spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank perturbation, and canonical correlation analysis with low-rank dependencies. The objective is to construct confidence intervals for the signal strength that are uniformly valid across all regimes - strong, weak, and critical signals. We demonstrate that traditional Gaussian approximations fail in the critical regime. Instead, we introduce a universal transitional distribution that enables valid inference across the entire spectrum of signal strengths. The approach is illustrated through applications in macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18554v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Sasha Sodin</dc:creator>
    </item>
    <item>
      <title>Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data</title>
      <link>https://arxiv.org/abs/2601.20197</link>
      <description>arXiv:2601.20197v2 Announce Type: replace-cross 
Abstract: Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20197v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Langevin</dc:creator>
    </item>
  </channel>
</rss>
