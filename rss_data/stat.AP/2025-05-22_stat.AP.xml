<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy</title>
      <link>https://arxiv.org/abs/2505.15844</link>
      <description>arXiv:2505.15844v1 Announce Type: cross 
Abstract: Brain stroke remains one of the principal causes of death and disability worldwide, yet most tabular-data prediction models still hover below the 95% accuracy threshold, limiting real-world utility. Addressing this gap, the present work develops and validates a completely data-driven and interpretable machine-learning framework designed to predict strokes using ten routinely gathered demographic, lifestyle, and clinical variables sourced from a public cohort of 4,981 records. We employ a detailed exploratory data analysis (EDA) to understand the dataset's structure and distribution, followed by rigorous data preprocessing, including handling missing values, outlier removal, and class imbalance correction using Synthetic Minority Over-sampling Technique (SMOTE). To streamline feature selection, point-biserial correlation and random-forest Gini importance were utilized, and ten varied algorithms-encompassing tree ensembles, boosting, kernel methods, and a multilayer neural network-were optimized using stratified five-fold cross-validation. Their predictions based on probabilities helped us build the proposed model, which included Random Forest, XGBoost, LightGBM, and a support-vector classifier, with logistic regression acting as a meta-learner. The proposed model achieved an accuracy rate of 97.2% and an F1-score of 97.15%, indicating a significant enhancement compared to the leading individual model, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate that rigorous preprocessing, coupled with a diverse hybrid model, can convert low-cost tabular data into a nearly clinical-grade stroke-risk assessment tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15844v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yousuf Islam, Md. Jalal Uddin Chowdhury, Sumon Chandra Das</dc:creator>
    </item>
    <item>
      <title>What Lives? A meta-analysis of diverse opinions on the definition of life</title>
      <link>https://arxiv.org/abs/2505.15849</link>
      <description>arXiv:2505.15849v1 Announce Type: cross 
Abstract: The question of "what is life?" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15849v1</guid>
      <category>q-bio.OT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.BM</category>
      <category>q-bio.CB</category>
      <category>q-bio.SC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reed Bender, Karina Kofman, Blaise Ag\"uera y Arcas, Michael Levin</dc:creator>
    </item>
    <item>
      <title>Quantile Predictions for Equity Premium using Penalized Quantile Regression with Consistent Variable Selection across Multiple Quantiles</title>
      <link>https://arxiv.org/abs/2505.16019</link>
      <description>arXiv:2505.16019v1 Announce Type: cross 
Abstract: This paper considers equity premium prediction, for which mean regression can be problematic due to heteroscedasticity and heavy-tails of the error. We show advantages of quantile predictions using a novel penalized quantile regression that offers a model for a full spectrum analysis on the equity premium distribution. To enhance model interpretability and address the well-known issue of crossing quantile predictions in quantile regression, we propose a model that enforces the selection of a common set of variables across all quantiles. Such a selection consistency is achieved by simultaneously estimating all quantiles with a group penalty that ensures sparsity pattern is the same for all quantiles. Consistency results are provided that allow the number of predictors to increase with the sample size. A Huberized quantile loss function and an augmented data approach are implemented for computational efficiency. Simulation studies show the effectiveness of the proposed approach. Empirical results show that the proposed method outperforms several benchmark methods. Moreover, we find some important predictors reverse their relationship to the excess return from lower to upper quantiles, potentially offering interesting insights to the domain experts. Our proposed method can be applied to other fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16019v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaobo Li, Ben Sherwood</dc:creator>
    </item>
    <item>
      <title>Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics</title>
      <link>https://arxiv.org/abs/2505.16118</link>
      <description>arXiv:2505.16118v1 Announce Type: cross 
Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for travel decisions, yet analytical methods lack scalability. This study introduces a dual-method LLM framework: unsupervised expectation extraction from UGC paired with survey-informed supervised fine-tuning. Findings reveal leisure/social expectations drive engagement more than foundational natural/emotional factors. By establishing LLMs as precision tools for expectation quantification, we advance tourism analytics methodology and propose targeted strategies for experience personalization and social travel promotion. The framework's adaptability extends to consumer behavior research, demonstrating computational social science's transformative potential in marketing optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16118v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Lan, Yao Gao, Yujun Cheng, Wei Yuan, Kun Wang</dc:creator>
    </item>
    <item>
      <title>Utilizing citation index and synthetic quality measure to compare Wikipedia languages across various topics</title>
      <link>https://arxiv.org/abs/2505.16506</link>
      <description>arXiv:2505.16506v1 Announce Type: cross 
Abstract: This study presents a comparative analysis of 55 Wikipedia language editions employing a citation index alongside a synthetic quality measure. Specifically, we identified the most significant Wikipedia articles within distinct topical areas, selecting the top 10, top 25, and top 100 most cited articles in each topic and language version. This index was built on the basis of wikilinks between Wikipedia articles in each language version and in order to do that we processed 6.6 billion page-to-page link records. Next, we used a quality score for each Wikipedia article - a synthetic measure scaled from 0 to 100. This approach enabled quality comparison of Wikipedia articles even between language versions with different quality grading schemes. Our results highlight disparities among Wikipedia language editions, revealing strengths and gaps in content coverage and quality across topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16506v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W{\l}odzimierz Lewoniewski, Krzysztof W\k{e}cel, Witold Abramowicz</dc:creator>
    </item>
    <item>
      <title>Interpretable contour level selection for heat maps for gridded data</title>
      <link>https://arxiv.org/abs/2505.16788</link>
      <description>arXiv:2505.16788v1 Announce Type: cross 
Abstract: Gridded data formats, where the observed multivariate data are aggregated into grid cells, ensure confidentiality and reduce storage requirements, with the trade-off that access to the underlying point data is lost. Heat maps are a highly pertinent visualisation for gridded data, and heat maps with a small number of well-selected contour levels offer improved interpretability over continuous contour levels. There are many possible contour level choices. Amongst them, density contour levels are highly suitable in many cases, and their probabilistic interpretation form a rigorous statistical basis for further quantitative data analyses. Current methods for computing density contour levels requires access to the observed point data, so they are not applicable to gridded data. To remedy this, we introduce an approximation of density contour levels for gridded data. We then compare our proposed method to existing contour level selection methods, and conclude that our proposal provides improved interpretability for synthetic and experimental gridded data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16788v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>Fast return-level estimates for flood insurance via an improved Bennett inequality for random variables with differing upper bounds</title>
      <link>https://arxiv.org/abs/2311.10001</link>
      <description>arXiv:2311.10001v3 Announce Type: replace 
Abstract: Insurance losses due to flooding can be estimated by simulating and then summing losses over a large number of locations and a large set of hypothetical years of flood events. Replicated realisations lead to Monte Carlo return-level estimates and associated uncertainty. The procedure, however, is highly computationally intensive. We develop and use a new, Bennett-like concentration inequality to provide conservative but relatively accurate estimates of return levels. Bennett's inequality accounts for the different variances of each of the variables in a sum but uses a uniform upper bound on their support. Motivated by the variability in the total insured value of risks within a portfolio, we incorporate both individual upper bounds and variances and obtain tractable concentration bounds. Simulation studies and application to a representative portfolio demonstrate a substantial tightening compared with Bennett's bound. We then develop an importance-sampling procedure that repeatedly samples annual losses from the distributions implied by each year's concentration inequality, leading to conservative estimates of the return levels and their uncertainty using orders of magnitude less computation. This enables a simulation study of the sensitivity of the predictions to perturbations in quantities that are usually assumed fixed and known but, in truth, are not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10001v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Maria Barlow, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Lead Times in Flux: Analyzing Airbnb Booking Dynamics During Global Upheavals (2018-2022)</title>
      <link>https://arxiv.org/abs/2501.10535</link>
      <description>arXiv:2501.10535v3 Announce Type: replace 
Abstract: Short-term shifts in booking behaviors can disrupt forecasting in the travel and hospitality industry, especially during global crises. Traditional metrics like average or median lead times often overlook important distribution changes. This study introduces a normalized L1 (Manhattan) distance to assess Airbnb booking lead time divergences from 2018 to 2022, focusing on the COVID-19 pandemic across four major U.S. cities. We identify a two-phase disruption: an abrupt change at the pandemic's onset followed by partial recovery with persistent deviations from pre-2018 patterns. Our method reveals changes in travelers' planning horizons that standard statistics miss, highlighting the need to analyze the entire lead-time distribution for more accurate demand forecasting and pricing strategies. The normalized L1 metric provides valuable insights for tourism stakeholders navigating ongoing market volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10535v3</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Erica Savage, Peter Coles</dc:creator>
    </item>
    <item>
      <title>Non-Adaptive Multi-Stage Algorithm and Bounds for Group Testing with Prior Statistics</title>
      <link>https://arxiv.org/abs/2402.10018</link>
      <description>arXiv:2402.10018v4 Announce Type: replace-cross 
Abstract: In this paper, we propose an efficient multi-stage algorithm for non-adaptive Group Testing (GT) with general correlated prior statistics. The proposed solution can be applied to any correlated statistical prior represented in trellis, e.g., finite state machines and Markov processes. We introduce a variation of List Viterbi Algorithm (LVA) to enable accurate recovery using much fewer tests than objectives, which efficiently gains from the correlated prior statistics structure. We also provide a sufficiency bound to the number of pooled tests required by any Maximum A Posteriori (MAP) decoder with an arbitrary correlation between infected items. Our numerical results demonstrate that the proposed Multi-Stage GT (MSGT) algorithm can obtain the optimal MAP performance with feasible complexity in practical regimes, such as with COVID-19 and sparse signal recovery applications, and reduce in the scenarios tested the number of pooled tests by at least 25% compared to existing classical low complexity GT algorithms. Moreover, we analytically characterize the complexity of the proposed MSGT algorithm that guarantees its efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10018v4</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayelet C. Portnoy, Amit Solomon, Alejandro Cohen</dc:creator>
    </item>
    <item>
      <title>A new statistical approach for joint modeling of longitudinal outcomes measured in electronic health records with clinically informative presence and observation processes</title>
      <link>https://arxiv.org/abs/2410.13113</link>
      <description>arXiv:2410.13113v2 Announce Type: replace-cross 
Abstract: Biobanks with genetics-linked electronic health records (EHR) have opened up opportunities to study associations between genetic, social, or environmental factors and longitudinal lab biomarkers. However, in EHRs, the timing of patient visits and the recording of lab tests often depend on patient health status, referred to as informative presence (IP) and informative observation (IO), which can bias exposure-biomarker associations. Two gaps remain in EHR-based research: (1) the performance of existing IP-aware methods is unclear in real-world EHR settings, and (2) no existing methods handle IP and IO simultaneously. To address these challenges, we first conduct extensive simulation studies tailored to EHR-specific IP patterns to assess existing methods. We then propose a joint modeling framework, EHRJoint, that simultaneously models the visiting, observation, and longitudinal biomarker processes to address both IP and IO. We develop a computationally efficient estimation procedure based on estimating equations and provide asymptotically valid inference. Simulations show that EHRJoint yields unbiased exposure effect estimates under both IP and IO, while existing methods fail. We apply EHRJoint to the Michigan Genomics Initiative data to examine associations between repeated glucose measurements and two exposures: genetic variants and educational disadvantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13113v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacong Du, Xu Shi, Bhramar Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
