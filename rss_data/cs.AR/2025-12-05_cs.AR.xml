<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration</title>
      <link>https://arxiv.org/abs/2512.04527</link>
      <description>arXiv:2512.04527v1 Announce Type: new 
Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04527v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingyu Liu, Jiawei Liang, Linfeng Du, Yipu Zhang, Chaofang Ma, Hanwei Fan, Jiang Xu, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project</title>
      <link>https://arxiv.org/abs/2512.04867</link>
      <description>arXiv:2512.04867v1 Announce Type: new 
Abstract: This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04867v1</guid>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bychkov Oleksii, Senysh Taras</dc:creator>
    </item>
    <item>
      <title>Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming</title>
      <link>https://arxiv.org/abs/2512.04910</link>
      <description>arXiv:2512.04910v1 Announce Type: new 
Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04910v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Li</dc:creator>
    </item>
    <item>
      <title>A Spatial Array for Spectrally Agile Wireless Processing</title>
      <link>https://arxiv.org/abs/2512.04182</link>
      <description>arXiv:2512.04182v1 Announce Type: cross 
Abstract: Massive MIMO is a cornerstone of next-generation wireless communication, offering significant gains in capacity, reliability, and energy efficiency. However, to meet emerging demands such as high-frequency operation, wide bandwidths, co-existence, integrated sensing, and resilience to dynamic interference, future systems must exhibit both scalability and spectral agility. These requirements place increasing pressure on the underlying processing hardware to be both efficient and reconfigurable. This paper proposes a custom-designed spatial array architecture that serves as a reconfigurable, general-purpose core optimized for a class of wireless kernels that commonly arise in diverse communications and sensing tasks. The proposed spatial array is evaluated against specialized cores for each kernel using High-Level Synthesis (HLS). Both the reconfigurable and specialized designs are synthesized in a 32 nm process to assess latency, throughput, area, and power in realistic processes. The results identify conditions under which general-purpose systolic architectures can approach the efficiency of specialized cores, thereby paving the way toward more scalable and agile systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04182v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ali Rasteh, Andrew Hennessee, Ishaan Shivhare, Siddharth Garg, Sundeep Rangan, Brandon Reagen</dc:creator>
    </item>
    <item>
      <title>RRAM-Based Analog Matrix Computing for Massive MIMO Signal Processing: A Review</title>
      <link>https://arxiv.org/abs/2512.04365</link>
      <description>arXiv:2512.04365v1 Announce Type: cross 
Abstract: Resistive random-access memory (RRAM) provides an excellent platform for analog matrix computing (AMC), enabling both matrix-vector multiplication (MVM) and the solution of matrix equations through open-loop and closed-loop circuit architectures. While RRAM-based AMC has been widely explored for accelerating neural networks, its application to signal processing in massive multiple-input multiple-output (MIMO) wireless communication is rapidly emerging as a promising direction. In this Review, we summarize recent advances in applying AMC to massive MIMO, including DFT/IDFT computation for OFDM modulation and demodulation using MVM circuits; MIMO detection and precoding using MVM-based iterative algorithms; and rapid one-step solutions enabled by matrix inversion (INV) and generalized inverse (GINV) circuits. We also highlight additional opportunities, such as AMC-based compressed-sensing recovery for channel estimation and eigenvalue circuits for leakage-based precoding. Finally, we outline key challenges, including RRAM device reliability, analog circuit precision, array scalability, and data conversion bottlenecks, and discuss the opportunities for overcoming these barriers. With continued progress in device-circuit-algorithm co-design, RRAM-based AMC holds strong promise for delivering high-efficiency, high-reliability solutions to (ultra)massive MIMO signal processing in the 6G era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04365v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pushen Zuo, Zhong Sun</dc:creator>
    </item>
    <item>
      <title>Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems</title>
      <link>https://arxiv.org/abs/2512.04476</link>
      <description>arXiv:2512.04476v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04476v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Fan, Zhenyu Liu, Yunzhen Liu, Yayue Hou, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu</dc:creator>
    </item>
    <item>
      <title>Hardware-aware Neural Architecture Search of Early Exiting Networks on Edge Accelerators</title>
      <link>https://arxiv.org/abs/2512.04705</link>
      <description>arXiv:2512.04705v1 Announce Type: cross 
Abstract: Advancements in high-performance computing and cloud technologies have enabled the development of increasingly sophisticated Deep Learning (DL) models. However, the growing demand for embedded intelligence at the edge imposes stringent computational and energy constraints, challenging the deployment of these large-scale models. Early Exiting Neural Networks (EENN) have emerged as a promising solution, allowing dynamic termination of inference based on input complexity to enhance efficiency. Despite their potential, EENN performance is highly influenced by the heterogeneity of edge accelerators and the constraints imposed by quantization, affecting accuracy, energy efficiency, and latency. Yet, research on the automatic optimization of EENN design for edge hardware remains limited. To bridge this gap, we propose a hardware-aware Neural Architecture Search (NAS) framework that systematically integrates the effects of quantization and hardware resource allocation to optimize the placement of early exit points within a network backbone. Experimental results on the CIFAR-10 dataset demonstrate that our NAS framework can discover architectures that achieve over a 50\% reduction in computational costs compared to conventional static networks, making them more suitable for deployment in resource-constrained edge environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04705v1</guid>
      <category>cs.CC</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alaa Zniber, Arne Symons, Ouassim Karrakchou, Marian Verhelst, Mounir Ghogho</dc:creator>
    </item>
    <item>
      <title>David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?</title>
      <link>https://arxiv.org/abs/2512.05073</link>
      <description>arXiv:2512.05073v1 Announce Type: cross 
Abstract: Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05073v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa</dc:creator>
    </item>
    <item>
      <title>HLStrans: Dataset for C-to-HLS Hardware Code Synthesis</title>
      <link>https://arxiv.org/abs/2507.04315</link>
      <description>arXiv:2507.04315v3 Announce Type: replace 
Abstract: High-Level Synthesis (HLS) enables hardware design from C/C++ kernels but requires extensive transformations, such as restructuring code, inserting pragmas, adapting data types, and repairing non-synthesizable constructs, to achieve efficient FPGA implementations. While large language models (LLMs) show promise in automating these transformations, progress has been limited by the absence of large-scale, well-structured datasets. Existing HLS datasets focus primarily on resource estimation, lack paired C and HLS examples with testbenches, and cover only a narrow set of optimizations. We introduce HLStrans, the first benchmark-scale dataset for LLM-driven C-to-HLS synthesis. HLStrans contains over 124K paired C and HLS programs for real-world applications, with full testbenches and synthesis-based annotations of latency and resource usage. The dataset systematically captures five categories of transformations and is enriched by an automated augmentation pipeline combining LLMs, Monte Carlo Tree Search (MCTS), and Design Space Exploration (DSE). We benchmark state-of-the-art LLMs on HLStrans, demonstrating that retrieval and fine-tuning significantly improve success rates and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04315v3</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyun Zou, Nuo Chen, Yao Chen, Bingsheng He, WengFei Wong</dc:creator>
    </item>
    <item>
      <title>Efficient stereo matching on embedded GPUs with zero-means cross correlation</title>
      <link>https://arxiv.org/abs/2212.00476</link>
      <description>arXiv:2212.00476v2 Announce Type: replace-cross 
Abstract: Mobile stereo-matching systems have become an important part of many applications, such as automated-driving vehicles and autonomous robots. Accurate stereo-matching methods usually lead to high computational complexity; however, mobile platforms have only limited hardware resources to keep their power consumption low; this makes it difficult to maintain both an acceptable processing speed and accuracy on mobile platforms. To resolve this trade-off, we herein propose a novel acceleration approach for the well-known zero-means normalized cross correlation (ZNCC) matching cost calculation algorithm on a Jetson Tx2 embedded GPU. In our method for accelerating ZNCC, target images are scanned in a zigzag fashion to efficiently reuse one pixel's computation for its neighboring pixels; this reduces the amount of data transmission and increases the utilization of on-chip registers, thus increasing the processing speed. As a result, our method is 2X faster than the traditional image scanning method, and 26% faster than the latest NCC method. By combining this technique with the domain transformation (DT) algorithm, our system show real-time processing speed of 32 fps, on a Jetson Tx2 GPU for 1,280x384 pixel images with a maximum disparity of 128. Additionally, the evaluation results on the KITTI 2015 benchmark show that our combined system is more accurate than the same algorithm combined with census by 7.26%, while maintaining almost the same processing speed. Source Code: https://github.com/changqiong/Z2ZNCC.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.00476v2</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiong Chang, Aolong Zha, Weimin Wang, Xin Liu, Masaki Onishi, Lei Lei, Meng Joo Er, Tsutomu Maruyama</dc:creator>
    </item>
    <item>
      <title>PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification</title>
      <link>https://arxiv.org/abs/2506.12200</link>
      <description>arXiv:2506.12200v3 Announce Type: replace-cross 
Abstract: Register-Transfer Level (RTL) verification is a primary bottleneck, consuming 60-70% of development time. While Large Language Models (LLMs) show promise for RTL automation, their performance and research focus have overwhelmingly centered on RTL generation rather than verification. Current methods for RTL verification rely on large scale proprietary models (e.g., GPT-4o) to generate Python-based functional references, incurring a high cost and raising data-privacy risks. To date, an end-to-end open-source solution for autonomous verification remains absent.
  We introduce PRO-V-R1, the first trainable open-source agentic framework for autonomous RTL verification. Our contributions are threefold: (1) we design PRO-V sys, a modular agentic system that couples LLM-based reasoning with programmatic tool use for RTL verification; (2) we establish a data construction pipeline that leverages existing RTL datasets to build simulation-validated, expert-level trajectories tailored for supervised fine-tuning (SFT) RTL verification agents; and (3) we implement an efficient reinforcement learning (RL) algorithm that uses verification-specific rewards derived from program-tool feedback to optimize the end-to-end verification workflow. Our empirical evaluation demonstrates PRO-V-R1 achieves a 57.7% functional correctness rate and 34.0% in robust fault detection, significantly outperforming the base model's 25.7% and 21.8% (respectively) from the state-of-the-art (SOTA) automatic verification system. This configuration also outperforms large-scale proprietary LLMs in functional correctness and shows comparable robustness for fault detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12200v3</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Zhao, Zhijing Wu, Boqin Yuan, Zhongming Yu, Hejia Zhang, Wentao Ni, Chia-Tung Ho, Haoxing Ren, Jishen Zhao</dc:creator>
    </item>
    <item>
      <title>Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting</title>
      <link>https://arxiv.org/abs/2510.05497</link>
      <description>arXiv:2510.05497v2 Announce Type: replace-cross 
Abstract: Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.
  To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $&gt;$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05497v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongkai Yu, Yue Guan, Zihao Yu, Chenyang Zhou, Zhengding Hu, Shuyi Pei, Yangwook Kang, Yufei Ding, Po-An Tsai</dc:creator>
    </item>
    <item>
      <title>ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity</title>
      <link>https://arxiv.org/abs/2512.02403</link>
      <description>arXiv:2512.02403v2 Announce Type: replace-cross 
Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02403v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Liu, Zhifang Deng, Tong Pu, Shengli Lu</dc:creator>
    </item>
  </channel>
</rss>
