<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Mar 2024 04:02:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tensor Time Series Imputation through Tensor Factor Modelling</title>
      <link>https://arxiv.org/abs/2403.13153</link>
      <description>arXiv:2403.13153v1 Announce Type: new 
Abstract: We propose tensor time series imputation when the missing pattern in the tensor data can be general, as long as any two data positions along a tensor fibre are both observed for enough time points. The method is based on a tensor time series factor model with Tucker decomposition of the common component. One distinguished feature of the tensor time series factor model used is that there can be weak factors in the factor loadings matrix for each mode. This reflects reality better when real data can have weak factors which drive only groups of observed variables, for instance, a sector factor in financial market driving only stocks in a particular sector. Using the data with missing entries, asymptotic normality is derived for rows of estimated factor loadings, while consistent covariance matrix estimation enables us to carry out inferences. As a first in the literature, we also propose a ratio-based estimator for the rank of the core tensor under general missing patterns. Rates of convergence are spelt out for the imputations from the estimated tensor factor models. We introduce a new measure for gauging imputation performances, and simulation results show that our imputation procedure works well, with asymptotic normality and corresponding inferences also demonstrated. Re-imputation performances are also gauged when we demonstrate that using slightly larger rank then estimated gives superior re-imputation performances. An NYC taxi traffic data set is also analyzed by imposing general missing patterns and gauging the imputation performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13153v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Nonparametric density estimation for stationary processes under multiplicative measurement errors</title>
      <link>https://arxiv.org/abs/2403.13410</link>
      <description>arXiv:2403.13410v1 Announce Type: new 
Abstract: This paper focuses on estimating the invariant density function $f_X$ of the strongly mixing stationary process $X_t$ in the multiplicative measurement errors model $Y_t = X_t U_t$, where $U_t$ is also a strongly mixing stationary process. We propose a novel approach to handle non-independent data, typical in real-world scenarios. For instance, data collected from various groups may exhibit interdependencies within each group, resembling data generated from $m$-dependent stationary processes, a subset of stationary processes. This study extends the applicability of the model $Y_t = X_t U_t$ to diverse scientific domains dealing with complex dependent data. The paper outlines our estimation techniques, discusses convergence rates, establishes a lower bound on the minimax risk, and demonstrates the asymptotic normality of the estimator for $f_X$ under smooth error distributions. Through examples and simulations, we showcase the efficacy of our estimator. The paper concludes by providing proofs for the presented theoretical results.v</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13410v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duc Trong Dang, Van Ha Hoang, Phuc Hung Thai</dc:creator>
    </item>
    <item>
      <title>Adaptive estimation for Weakly Dependent Functional Times Series</title>
      <link>https://arxiv.org/abs/2403.13706</link>
      <description>arXiv:2403.13706v1 Announce Type: new 
Abstract: The local regularity of functional time series is studied under $L^p-m-$appro\-ximability assumptions. The sample paths are observed with error at possibly random design points. Non-asymptotic concentration bounds of the regularity estimators are derived. As an application, we build nonparametric mean and autocovariance functions estimators that adapt to the regularity and the design, which can be sparse or dense. We also derive the asymptotic normality of the mean estimator, which allows honest inference for irregular mean functions. Simulations and a real data application illustrate the performance of the new estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13706v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hassan Maissoro, Valentin Patilea, Myriam Vimond</dc:creator>
    </item>
    <item>
      <title>Quadratic Point Estimate Method for Probabilistic Moments Computation</title>
      <link>https://arxiv.org/abs/2403.13203</link>
      <description>arXiv:2403.13203v1 Announce Type: cross 
Abstract: This paper presents in detail the originally developed Quadratic Point Estimate Method (QPEM), aimed at efficiently and accurately computing the first four output moments of probabilistic distributions, using 2n^2+1 sample (or sigma) points, with n, the number of input random variables. The proposed QPEM particularly offers an effective, superior, and practical alternative to existing sampling and quadrature methods for low- and moderately-high-dimensional problems. Detailed theoretical derivations are provided proving that the proposed method can achieve a fifth or higher-order accuracy for symmetric input distributions. Various numerical examples, from simple polynomial functions to nonlinear finite element analyses with random field representations, support the theoretical findings and further showcase the validity, efficiency, and applicability of the QPEM, from low- to high-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13203v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhyeok Ko, Konstantinos G. Papakonstantinou</dc:creator>
    </item>
    <item>
      <title>Multifractal wavelet dynamic mode decomposition modeling for marketing time series</title>
      <link>https://arxiv.org/abs/2403.13361</link>
      <description>arXiv:2403.13361v1 Announce Type: cross 
Abstract: Marketing is the way we ensure our sales are the best in the market, our prices the most accessible, and our clients satisfied, thus ensuring our brand has the widest distribution. This requires sophisticated and advanced understanding of the whole related network. Indeed, marketing data may exist in different forms such as qualitative and quantitative data. However, in the literature, it is easily noted that large bibliographies may be collected about qualitative studies, while only a few studies adopt a quantitative point of view. This is a major drawback that results in marketing science still focusing on design, although the market is strongly dependent on quantities such as money and time. Indeed, marketing data may form time series such as brand sales in specified periods, brand-related prices over specified periods, market shares, etc. The purpose of the present work is to investigate some marketing models based on time series for various brands. This paper aims to combine the dynamic mode decomposition and wavelet decomposition to study marketing series due to both prices, and volume sales in order to explore the effect of the time scale on the persistence of brand sales in the market and on the forecasting of such persistence, according to the characteristics of the brand and the related market competition or competitors. Our study is based on a sample of Saudi brands during the period 22 November 2017 to 30 December 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13361v1</guid>
      <category>q-fin.MF</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mohamed Elshazli A. Zidan, Anouar Ben Mabrouk, Nidhal Ben Abdallah, Tawfeeq M. Alanazi</dc:creator>
    </item>
    <item>
      <title>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2403.13565</link>
      <description>arXiv:2403.13565v1 Announce Type: cross 
Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectiveness of the proposed method is validated using both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13565v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin He, Ying Sun, Jingyuan Liu, Runze Li</dc:creator>
    </item>
    <item>
      <title>Data integration of non-probability and probability samples with predictive mean matching</title>
      <link>https://arxiv.org/abs/2403.13750</link>
      <description>arXiv:2403.13750v1 Announce Type: cross 
Abstract: In this paper we study predictive mean matching mass imputation estimators to integrate data from probability and non-probability samples. We consider two approaches: matching predicted to observed ($\hat{y}-y$ matching) or predicted to predicted ($\hat{y}-\hat{y}$ matching) values. We prove the consistency of two semi-parametric mass imputation estimators based on these approaches and derive their variance and estimators of variance. Our approach can be employed with non-parametric regression techniques, such as kernel regression, and the analytical expression for variance can also be applied in nearest neighbour matching for non-probability samples. We conduct extensive simulation studies in order to compare the properties of this estimator with existing approaches, discuss the selection of $k$-nearest neighbours, and study the effects of model mis-specification. The paper finishes with empirical study in integration of job vacancy survey and vacancies submitted to public employment offices (admin and online data). Open source software is available for the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13750v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chlebicki Piotr, {\L}ukasz Chrostowski, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory in a class of directed random graph models with a differentially private bi-degree sequence</title>
      <link>https://arxiv.org/abs/2201.09648</link>
      <description>arXiv:2201.09648v2 Announce Type: replace 
Abstract: Although the asymptotic properties of the parameter estimator have been derived in the $p_{0}$ model for directed graphs with the differentially private bi-degree sequence, asymptotic theory in general models is still lacking. In this paper, we release the bi-degree sequence of directed graphs via the discrete Laplace mechanism, which satisfies differential privacy. We use the moment method to estimate the unknown model parameter. We establish a unified asymptotic result, in which consistency and asymptotic normality of the differentially private estimator holds. We apply the unified theoretical result to the Probit model. Simulations and a real data demonstrate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.09648v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Pan, Jianwei Hu, Peiyan Li</dc:creator>
    </item>
    <item>
      <title>Non-Independent Components Analysis</title>
      <link>https://arxiv.org/abs/2206.13668</link>
      <description>arXiv:2206.13668v4 Announce Type: replace 
Abstract: A seminal result in the ICA literature states that for $AY = \varepsilon$, if the components of $\varepsilon$ are independent and at most one is Gaussian, then $A$ is identified up to sign and permutation of its rows (Comon, 1994). In this paper we study to which extent the independence assumption can be relaxed by replacing it with restrictions on higher order moment or cumulant tensors of $\varepsilon$. We document new conditions that establish identification for several non-independent component models, e.g. common variance models, and propose efficient estimation methods based on the identification results. We show that in situations where independence cannot be assumed the efficiency gains can be significant relative to methods that rely on independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13668v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geert Mesters, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Distributed Estimation and Inference for Semi-parametric Binary Response Models</title>
      <link>https://arxiv.org/abs/2210.08393</link>
      <description>arXiv:2210.08393v3 Announce Type: replace 
Abstract: The development of modern technology has enabled data collection of unprecedented size, which poses new challenges to many statistical estimation and inference problems. This paper studies the maximum score estimator of a semi-parametric binary choice model under a distributed computing environment without pre-specifying the noise distribution. An intuitive divide-and-conquer estimator is computationally expensive and restricted by a non-regular constraint on the number of machines, due to the highly non-smooth nature of the objective function. We propose (1) a one-shot divide-and-conquer estimator after smoothing the objective to relax the constraint, and (2) a multi-round estimator to completely remove the constraint via iterative smoothing. We specify an adaptive choice of kernel smoother with a sequentially shrinking bandwidth to achieve the superlinear improvement of the optimization error over the multiple iterations. The improved statistical accuracy per iteration is derived, and a quadratic convergence up to the optimal statistical error rate is established. We further provide two generalizations to handle the heterogeneity of datasets and high-dimensional problems where the parameter of interest is sparse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08393v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Wenbo Jing, Weidong Liu, Yichen Zhang</dc:creator>
    </item>
    <item>
      <title>Asymptotically free sketched ridge ensembles: Risks, cross-validation, and tuning</title>
      <link>https://arxiv.org/abs/2310.04357</link>
      <description>arXiv:2310.04357v3 Announce Type: replace 
Abstract: We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an "ensemble trick" whereby the risk for unsketched ridge regression can be efficiently estimated via GCV using small sketched ridge ensembles. We empirically validate our theoretical results using both synthetic and real large-scale datasets with practical sketches including CountSketch and subsampled randomized discrete cosine transforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04357v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratik Patil, Daniel LeJeune</dc:creator>
    </item>
    <item>
      <title>Analysis of singular subspaces under random perturbations</title>
      <link>https://arxiv.org/abs/2403.09170</link>
      <description>arXiv:2403.09170v2 Announce Type: replace 
Abstract: We present a comprehensive analysis of singular vector and singular subspace perturbations in the context of the signal plus random Gaussian noise matrix model. Assuming a low-rank signal matrix, we extend the Davis-Kahan-Wedin theorem in a fully generalized manner, applicable to any unitarily invariant matrix norm, extending previous results of O'Rourke, Vu and the author. We also obtain the fine-grained results, which encompass the $\ell_\infty$ analysis of singular vectors, the $\ell_{2, \infty}$ analysis of singular subspaces, as well as the exploration of linear and bilinear functions related to the singular vectors. Moreover, we explore the practical implications of these findings, in the context of the Gaussian mixture model and the submatrix localization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09170v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Wang</dc:creator>
    </item>
    <item>
      <title>Are Ensembles Getting Better all the Time?</title>
      <link>https://arxiv.org/abs/2311.17885</link>
      <description>arXiv:2311.17885v2 Announce Type: replace-cross 
Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabilities that may be of independent interest. We illustrate our results on a medical prediction problem (diagnosing melanomas using neural nets) and a "wisdom of crowds" experiment (guessing the ratings of upcoming movies).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17885v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre-Alexandre Mattei, Damien Garreau</dc:creator>
    </item>
  </channel>
</rss>
