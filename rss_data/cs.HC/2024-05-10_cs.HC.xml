<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Study on Cognitive Effects of Canvas Size for Augmenting Drawing Skill</title>
      <link>https://arxiv.org/abs/2405.05284</link>
      <description>arXiv:2405.05284v1 Announce Type: new 
Abstract: In recent years, the field of generative artificial intelligence, particularly in the domain of image generation, has exerted a profound influence on society. Despite the capability of AI to produce images of high quality, the augmentation of users' drawing abilities through the provision of drawing support systems emerges as a challenging issue. In this study, we propose that a cognitive factor, specifically, the size of the canvas, may exert a considerable influence on the outcomes of imitative drawing sketches when utilizing reference images. To investigate this hypothesis, a web based drawing interface was utilized, designed specifically to evaluate the effect of the canvas size's proportionality to the reference image on the fidelity of the drawings produced. The findings from our research lend credence to the hypothesis that a drawing interface, featuring a canvas whose dimensions closely match those of the reference image, markedly improves the precision of user-generated sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05284v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jize Wang, Kazuhisa Nakano, Daiyannan Chen, Zhengyu Huang, Tsukasa Fukusato, Kazunori Miyata, Haoran Xie</dc:creator>
    </item>
    <item>
      <title>Generative AI as a metacognitive agent: A comparative mixed-method study with human participants on ICF-mimicking exam performance</title>
      <link>https://arxiv.org/abs/2405.05285</link>
      <description>arXiv:2405.05285v1 Announce Type: new 
Abstract: This study investigates the metacognitive capabilities of Large Language Models relative to human metacognition in the context of the International Coaching Federation ICF mimicking exam, a situational judgment test related to coaching competencies. Using a mixed method approach, we assessed the metacognitive performance, including sensitivity, accuracy in probabilistic predictions, and bias, of human participants and five advanced LLMs (GPT-4, Claude-3-Opus 3, Mistral Large, Llama 3, and Gemini 1.5 Pro). The results indicate that LLMs outperformed humans across all metacognitive metrics, particularly in terms of reduced overconfidence, compared to humans. However, both LLMs and humans showed less adaptability in ambiguous scenarios, adhering closely to predefined decision frameworks. The study suggests that Generative AI can effectively engage in human-like metacognitive processing without conscious awareness. Implications of the study are discussed in relation to development of AI simulators that scaffold cognitive and metacognitive aspects of mastering coaching competencies. More broadly, implications of these results are discussed in relation to development of metacognitive modules that lead towards more autonomous and intuitive AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05285v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jelena Pavlovic (University of Belgrade, Faculty of Philosophy,Koucing centar Resarch Lab), Jugoslav Krstic (Koucing centar Research Lab), Luka Mitrovic (Koucing centar Research Lab), Djordje Babic (Koucing centar Research Lab), Adrijana Milosavljevic (Koucing centar Research Lab), Milena Nikolic (Koucing centar Research Lab), Tijana Karaklic (Koucing centar Research Lab), Tijana Mitrovic (Koucing centar Research Lab)</dc:creator>
    </item>
    <item>
      <title>Smart Portable Computer</title>
      <link>https://arxiv.org/abs/2405.05292</link>
      <description>arXiv:2405.05292v1 Announce Type: new 
Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05292v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niladri Das</dc:creator>
    </item>
    <item>
      <title>Harmonizing Program Induction with Rate-Distortion Theory</title>
      <link>https://arxiv.org/abs/2405.05294</link>
      <description>arXiv:2405.05294v1 Announce Type: new 
Abstract: Many aspects of human learning have been proposed as a process of constructing mental programs: from acquiring symbolic number representations to intuitive theories about the world. In parallel, there is a long-tradition of using information processing to model human cognition through Rate Distortion Theory (RDT). Yet, it is still poorly understood how to apply RDT when mental representations take the form of programs. In this work, we adapt RDT by proposing a three way trade-off among rate (description length), distortion (error), and computational costs (search budget). We use simulations on a melody task to study the implications of this trade-off, and show that constructing a shared program library across tasks provides global benefits. However, this comes at the cost of sensitivity to curricula, which is also characteristic of human learners. Finally, we use methods from partial information decomposition to generate training curricula that induce more effective libraries and better generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05294v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanqi Zhou, David G. Nagy, Charley M. Wu</dc:creator>
    </item>
    <item>
      <title>Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology</title>
      <link>https://arxiv.org/abs/2405.05299</link>
      <description>arXiv:2405.05299v1 Announce Type: new 
Abstract: Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose into the stomach to deliver nutrition or medication. If not placed correctly, they can cause serious harm, even death to patients. Recent AI developments demonstrate the feasibility of robustly detecting NGT placement from Chest X-ray images to reduce risks of sub-optimally or critically placed NGTs being missed or delayed in their detection, but gaps remain in clinical practice integration. In this study, we present a human-centered approach to the problem and describe insights derived following contextual inquiry and in-depth interviews with 15 clinical stakeholders. The interviews helped understand challenges in existing workflows, and how best to align technical capabilities with user needs and expectations. We discovered the trade-offs and complexities that need consideration when choosing suitable workflow stages, target users, and design configurations for different AI proposals. We explored how to balance AI benefits and risks for healthcare staff and patients within broader organizational and medical-legal constraints. We also identified data issues related to edge cases and data biases that affect model training and evaluation; how data documentation practices influence data preparation and labelling; and how to measure relevant AI outcomes reliably in future evaluations. We discuss how our work informs design and development of AI applications that are clinically useful, ethical, and acceptable in real-world healthcare services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05299v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Thieme, Abhijith Rajamohan, Benjamin Cooper, Heather Groombridge, Robert Simister, Barney Wong, Nicholas Woznitza, Mark Ames Pinnock, Maria Teodora Wetscherek, Cecily Morrison, Hannah Richardson, Fernando P\'erez-Garc\'ia, Stephanie L. Hyland, Shruthi Bannur, Daniel C. Castro, Kenza Bouzid, Anton Schwaighofer, Mercy Ranjit, Harshita Sharma, Matthew P. Lungren, Ozan Oktay, Javier Alvarez-Valle, Aditya Nori, Stephen Harris, Joseph Jacob</dc:creator>
    </item>
    <item>
      <title>Permalife Of The Archive: Archaeogaming As Queergaming</title>
      <link>https://arxiv.org/abs/2405.05411</link>
      <description>arXiv:2405.05411v1 Announce Type: new 
Abstract: Archaeogaming and queer games studies have both grown as paradigms in the last decade. The former broadly refers to the archaeological study of games, while the later concerns the application of queer theory to the medium. To date, there has been limited engagement of archaeogamers with queer games scholarship, and vice versa. This article argues that there are epistomological parallels between the two; as they are both concerned with the limits and ethics of representation, the personal and political contexts of game development and engagement with video games through transgressive play.
  The paper is structured around an extended literature review and three vignettes that reflect on the author's personal experience of conducting archaeogaming research, an ethnographic study of Wurm Online, an archaeological survey of Elden Ring and a player study of the generative archaeology game Nothing Beside Remains. While archaeogaming can learn from the centring of subjective lived experience and labour in the queer games sphere, archaeogaming as a form of game preservation can also benefit queer games studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05411v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649921.3659843</arxiv:DOI>
      <dc:creator>Florence Smith Nicholls</dc:creator>
    </item>
    <item>
      <title>Studying Self-Care with Generative AI Tools: Lessons for Design</title>
      <link>https://arxiv.org/abs/2405.05458</link>
      <description>arXiv:2405.05458v1 Announce Type: new 
Abstract: The rise of generative AI presents new opportunities for the understanding and practice of self-care through its capability to generate varied content, including self-care suggestions via text and images, and engage in dialogue with users over time. However, there are also concerns about accuracy and trustworthiness of self-care advice provided via AI. This paper reports our findings from workshops, diaries, and interviews with five researchers and 24 participants to explore their experiences and use of generative AI for self-care. We analyze our findings to present a framework for the use of generative AI to support five types of self-care, - advice seeking, mentorship, resource creation, social simulation, and therapeutic self-expression - mapped across two dimensions - expertise and modality. We discuss how these practices shift the role of technologies for self-care from merely offering information to offering personalized advice and supporting creativity for reflection, and we offer suggestions for using the framework to investigate new self-care designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05458v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3661614</arxiv:DOI>
      <dc:creator>Tara Capel, Bernd Ploderer, Filip Bircanin, Simon Hanmer, Jamie Yates, Jiaxuan Wang, Kai Ling Khor, Tuck Wah Leong, Greg Wadley, Michelle Newcomb</dc:creator>
    </item>
    <item>
      <title>(Dis)placed Contributions: Uncovering Hidden Hurdles to Collaborative Writing Involving Non-Native Speakers, Native Speakers, and AI-Powered Editing Tools</title>
      <link>https://arxiv.org/abs/2405.05474</link>
      <description>arXiv:2405.05474v1 Announce Type: new 
Abstract: Content creation today often takes place via collaborative writing. A longstanding interest of CSCW research lies in understanding and promoting the coordination between co-writers. However, little attention has been paid to individuals who write in their non-native language and to co-writer groups involving them. We present a mixed-method study that fills the above gap. Our participants included 32 co-writer groups, each consisting of one native speaker (NS) of English and one non-native speaker (NNS) with limited proficiency. They performed collaborative writing adopting two different workflows: half of the groups began with NNSs taking the first editing turn and half had NNSs act after NSs. Our data revealed a "late-mover disadvantage" exclusively experienced by NNSs: an NNS's ideational contributions to the joint document were suppressed when their editing turn was placed after an NS's turn, as opposed to ahead of it. Surprisingly, editing help provided by AI-powered tools did not exempt NNSs from being disadvantaged. Instead, it triggered NSs' overestimation of NNSs' English proficiency and agency displayed in the writing, introducing unintended tensions into the collaboration. These findings shed light on the fair assessment and effective promotion of a co-writer's contributions in language diverse settings. In particular, they underscore the necessity of disentangling contributions made to the ideational, expressional, and lexical aspects of the joint writing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05474v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yimin Xiao, Yuewen Chen, Naomi Yamashita, Yuexi Chen, Zhicheng Liu, Ge Gao</dc:creator>
    </item>
    <item>
      <title>Predicting Cognitive Load Using Sensor Data in a Literacy Game</title>
      <link>https://arxiv.org/abs/2405.05543</link>
      <description>arXiv:2405.05543v1 Announce Type: new 
Abstract: Educational games are being increasingly used to support self-paced learning. However, educators and system designers often face challenges in monitoring student affect and cognitive load. Existing assessments in game-based learning environments (GBLEs) tend to focus more on outcomes rather than processes, potentially overlooking key aspects of the learning journey that include learner affect and cognitive load. To address this issue, we collected data and trained a model to track learner cognitive load while they used an online literacy game for English. We collected affect-related physiological data and pupil data during gameplay to enable the development of models that identify these latent characteristics of learner processes. Our model indicates the feasibility of using these data to track cognitive load in GBLEs. Our multimodal model distinguished different levels of cognitive load, achieving the highest Kappa (.417) and accuracy (70%). Our model reveals the importance of including affect-related features (i.e., EDA and heart rate) when predicting cognitive load and extends recent findings suggesting the benefit of using multiple channels when modeling latent aspects of learner processes. Findings also suggest that cognitive load tracking could now be used to facilitate the creation of personalized learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05543v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghao Cai, Carrie Demmans Epp</dc:creator>
    </item>
    <item>
      <title>Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis</title>
      <link>https://arxiv.org/abs/2405.05548</link>
      <description>arXiv:2405.05548v1 Announce Type: new 
Abstract: Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants' interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05548v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613905.3651042</arxiv:DOI>
      <dc:creator>Jiajing Guo, Vikram Mohanty, Jorge Piazentin Ono, Hongtao Hao, Liang Gou, Liu Ren</dc:creator>
    </item>
    <item>
      <title>Intelligent EC Rearview Mirror: Enhancing Driver Safety with Dynamic Glare Mitigation via Cloud Edge Collaboration</title>
      <link>https://arxiv.org/abs/2405.05579</link>
      <description>arXiv:2405.05579v1 Announce Type: new 
Abstract: Sudden glare from trailing vehicles significantly increases driving safety risks. Existing anti-glare technologies such as electronic, manually-adjusted, and electrochromic rearview mirrors, are expensive and lack effective adaptability in different lighting conditions. To address these issues, our research introduces an intelligent rearview mirror system utilizing novel all-liquid electrochromic technology. This system integrates IoT with ensemble and federated learning within a cloud edge collaboration framework, dynamically controlling voltage to effectively eliminate glare and maintain clear visibility. Utilizing an ensemble learning model, it automatically adjusts mirror transmittance based on light intensity, achieving a low RMSE of 0.109 on the test set. Furthermore, the system leverages federated learning for distributed data training across devices, which enhances privacy and updates the cloud model continuously. Distinct from conventional methods, our experiment utilizes the Schmidt-Clausen and Bindels de Boer 9-point scale with TOPSIS for comprehensive evaluation of rearview mirror glare. Designed to be convenient and costeffective, this system demonstrates how IoT and AI can significantly enhance rearview mirror anti-glare performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05579v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyi Yang, Zefei Xu, Huayi Lai, Hongjian Chen, Sifan Kong, Yutong Wu, Huan Yang</dc:creator>
    </item>
    <item>
      <title>One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations</title>
      <link>https://arxiv.org/abs/2405.05581</link>
      <description>arXiv:2405.05581v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study (N=252) in which participants were given one or more LLM-generated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants' perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05581v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3630106.3662681</arxiv:DOI>
      <dc:creator>Yoonjoo Lee, Kihoon Son, Tae Soo Kim, Jisu Kim, John Joon Young Chung, Eytan Adar, Juho Kim</dc:creator>
    </item>
    <item>
      <title>AI in Your Toolbox: A Plugin for Generating Renderings from 3D Models</title>
      <link>https://arxiv.org/abs/2405.05627</link>
      <description>arXiv:2405.05627v1 Announce Type: new 
Abstract: With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology. This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino's features. It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality. Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools. Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05627v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingming Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment</title>
      <link>https://arxiv.org/abs/2405.05678</link>
      <description>arXiv:2405.05678v1 Announce Type: new 
Abstract: AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction. The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems. In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication. By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent. This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05678v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yoonsu Kim, Kihoon Son, Seoyoung Kim, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma</title>
      <link>https://arxiv.org/abs/2405.05758</link>
      <description>arXiv:2405.05758v1 Announce Type: new 
Abstract: Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI). Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored. To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research. The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights. We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions. We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05758v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Expanding Accessibility in Immersive Virtual Spaces: A Comprehensive Approach for All Disabilities</title>
      <link>https://arxiv.org/abs/2405.05910</link>
      <description>arXiv:2405.05910v1 Announce Type: new 
Abstract: In the early stages of the COVID-19 pandemic, many events and conferences hastily converted to a virtual format, and many commercial ventures promptly developed tools promising seamless transitions to virtual spaces. In particular, efforts to expand and monetize augmented and virtual reality environments increased. While these spaces increased accessibility for some, others were left behind. In 2024, many events returned to on-site venues, yet virtual spaces remain central in academic and research communities, particularly for disabled scholars. As such, in this paper, we advocate for continued virtual access and improved virtual spaces; we also identify some potentially overlooked harms in immersive and embodied virtual spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05910v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cecilia Aragon, Melissa Vosen Callens, Stacy M. Branham, Cali Anicha, Brianna Blaser, Canan Bilen-Green</dc:creator>
    </item>
    <item>
      <title>Risk of Harm in VR Dating from the Perspective of Women and LGBTQIA+ Stakeholders</title>
      <link>https://arxiv.org/abs/2405.05914</link>
      <description>arXiv:2405.05914v1 Announce Type: new 
Abstract: Virtual reality (VR) dating introduces novel opportunities for romantic interactions, but it also raises concerns about new harms that typically occur separately in traditional dating apps and general-purpose social VR environments. Given the subjectivity in which VR dating experiences can be considered harmful it is imperative to involve user stakeholders in anticipating harms and formulating preventative designs. Towards this goal with conducted participatory design workshops with 17 stakeholders identified as women and/or LGBTQIA+; demographics that are at elevated risk of harm in online dating and social VR. Findings reveal that participants are concerned with two categories of harm in VR dating: those that occur through the transition of interaction across virtual and physical modalities, and harms stemming from expectations of sexual interaction in VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05914v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Devin Tebbe, Meryem Barkallah, Braeden Burger, Douglas Zytko</dc:creator>
    </item>
    <item>
      <title>From Virtual Gains to Real Pains: Potential Harms of Immersive Exergames</title>
      <link>https://arxiv.org/abs/2405.05915</link>
      <description>arXiv:2405.05915v1 Announce Type: new 
Abstract: Digitalization and virtualization are parts of our everyday lives in almost all aspects ranging from work, education, and communication to entertainment. A novel step in this direction is the widespread interest in extended reality (XR) [2]. The newest consumer-ready head-mounted displays (HMD) such as Meta Quest 3 or Apple Vision Pro, have reached unprecedented levels of visual fidelity, interaction capabilities, and computational power. The built-in pass-through features of these headsets enable both virtual reality (VR) and augmented reality (AR) with the same devices. However, the immersive nature of these experiences is not the only groundbreaking difference from established forms of media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05915v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Cmentowski, Sukran Karaosmanoglu, Frank Steinicke</dc:creator>
    </item>
    <item>
      <title>Leveraging Artificial Intelligence to Promote Awareness in Augmented Reality Systems</title>
      <link>https://arxiv.org/abs/2405.05916</link>
      <description>arXiv:2405.05916v1 Announce Type: new 
Abstract: Recent developments in artificial intelligence (AI) have permeated through an array of different immersive environments, including virtual, augmented, and mixed realities. AI brings a wealth of potential that centers on its ability to critically analyze environments, identify relevant artifacts to a goal or action, and then autonomously execute decision-making strategies to optimize the reward-to-risk ratio. However, the inherent benefits of AI are not without disadvantages as the autonomy and communication methodology can interfere with the human's awareness of their environment. More specifically in the case of autonomy, the relevant human-computer interaction literature cites that high autonomy results in an "out-of-the-loop" experience for the human such that they are not aware of critical artifacts or situational changes that require their attention. At the same time, low autonomy of an AI system can limit the human's own autonomy with repeated requests to approve its decisions. In these circumstances, humans enter into supervisor roles, which tend to increase their workload and, therefore, decrease their awareness in a multitude of ways. In this position statement, we call for the development of human-centered AI in immersive environments to sustain and promote awareness. It is our position then that we believe with the inherent risk presented in both AI and AR/VR systems, we need to examine the interaction between them when we integrate the two to create a new system for any unforeseen risks, and that it is crucial to do so because of its practical application in many high-risk environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05916v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wangfan Li, Rohit Mallick, Carlos Toxtli-Hernandez, Christopher Flathmann, Nathan J. McNeese</dc:creator>
    </item>
    <item>
      <title>Fostering Inclusive Virtual Reality Environments: Discussing Strategies for Promoting Group Dynamics and Mitigating Harassment</title>
      <link>https://arxiv.org/abs/2405.05917</link>
      <description>arXiv:2405.05917v1 Announce Type: new 
Abstract: The rapid evolution of social Virtual Reality (VR) platforms has significantly enhanced the way users interact and socialize in digital spaces, offering immersive experiences that closely mimic real-world interactions [1]. However, this technological advancement has brought new challenges, particularly in ensuring safety and preventing harassment [11]. Unlike traditional social media platforms, the immersive nature of social VR applications can intensify the impact of harassment, affecting users' emotions, experiences, and reactions at both mental and physical levels [2, 9].
  Group dynamics can play a pivotal role in both preventing and mitigating harassment [9]. Literature in group dynamics has provided insights into fostering and nurturing communities, with special emphasis on how enabling leadership, coordination, and cohesion among individuals can enable inclusive and safer social spaces [4]. For this workshop, we propose to discuss group-centric approaches to address harassment in social VR. We will discuss group strategies such as matching, interactions, and reporting mechanisms aimed at promoting safer and more supportive social VR spaces. By leveraging the social structures among users, we aim to empower users and communities to collectively counteract harassment and ensure a positive social experience for all users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05917v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Niloofar Sayadi, Diego G\'omez-Zar\'a</dc:creator>
    </item>
    <item>
      <title>Safeguarding People's Financial Health in Metaverse with Emotionally Intelligent Virtual Buddy</title>
      <link>https://arxiv.org/abs/2405.05918</link>
      <description>arXiv:2405.05918v1 Announce Type: new 
Abstract: The Metaverse, an immersive virtual world, has emerged as a shared space where people engage in various activities ranging from social interactions to commerce. Cryptocurrencies [3] and Non-Fungible Tokens (NFTs) [6] play pivotal roles within this virtual realm, reshaping interactions and transactions. Cryptocurrencies, utilizing cryptographic techniques for security, enable decentralized and secure transactions, and NFTs represent ownership or proof of authenticity of unique digital assets through the blockchain technology. While NFTs and cryptocurrencies offer innovative opportunities for ownership, trading, and monetization within the metaverse, their use also introduces potential risks and negative consequences, such as financial scams and fraud, highlighting the need for users to exercise caution and diligence in their virtual transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05918v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Syed Ali Asif, Emma Cao, Hang Chen, Chien-Chung Shen, Yan-Ming Chiou</dc:creator>
    </item>
    <item>
      <title>Protecting Human Users Against Cognitive Attacks in Immersive Environments</title>
      <link>https://arxiv.org/abs/2405.05919</link>
      <description>arXiv:2405.05919v1 Announce Type: new 
Abstract: Integrating mixed reality (MR) with artificial intelligence (AI) technologies, including vision, language, audio, reasoning, and planning, enables the AI-powered MR assistant [1] to substantially elevate human efficiency. This enhancement comes from situational awareness, quick access to essential information, and support in learning new skills in the right context throughout everyday tasks. This blend transforms interactions with both the virtual and physical environments, catering to a range of skill levels and personal preferences. For instance, computer vision enables the understanding of the user's environment, allowing for the provision of timely and relevant digital overlays in MR systems. At the same time, language models enhance comprehension of contextual information and support voice-activated dialogue to answer user questions. However, as AI-driven MR systems advance, they also unveil new vulnerabilities, posing a threat to user safety by potentially exposing them to grave dangers [5, 6].</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05919v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yan-Ming Chiou, Bob Price, Chien-Chung Shen, Syed Ali Asif</dc:creator>
    </item>
    <item>
      <title>Exploring Proactive Interventions toward Harmful Behavior in Embodied Virtual Spaces</title>
      <link>https://arxiv.org/abs/2405.05920</link>
      <description>arXiv:2405.05920v1 Announce Type: new 
Abstract: Technological advancements have undoubtedly revolutionized various aspects of human life, altering the ways we perceive the world, engage with others, build relationships, and conduct our daily work routines. Among the recent advancements, the proliferation of virtual and mixed reality technologies stands out as a significant leap forward, promising to elevate our experiences and interactions to unprecedented levels. However, alongside the benefits, these emerging technologies also introduce novel avenues for harm and misuse, particularly in virtual and embodied spaces such as Zoom and virtual reality (VR) environments.
  The immersive nature of virtual reality environments raises unique challenges regarding psychological and emotional well-being. While VR can offer captivating and immersive experiences, prolonged exposure to virtual environments may lead to phenomena like cybersickness, disorientation, and even psychological distress in susceptible individuals. Additionally, the blurring of boundaries between virtual and real-world interactions in VR raises ethical concerns regarding consent, harassment, and the potential for virtual experiences to influence real-life behavior. Additionally, the increasing integration of artificial intelligence (AI) and machine learning algorithms in virtual spaces introduces risks related to algorithmic bias, discrimination, and manipulation. In VR environments, AI-driven systems may inadvertently perpetuate stereotypes, amplify inequalities, or manipulate user behavior through personalized content recommendations and targeted advertising, posing ethical dilemmas and societal risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05920v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruchi Panchanadikar</dc:creator>
    </item>
    <item>
      <title>Understanding and Mitigating Harmful Design in User-Generated Virtual Worlds</title>
      <link>https://arxiv.org/abs/2405.05922</link>
      <description>arXiv:2405.05922v1 Announce Type: new 
Abstract: Virtual space offers innovative ways for individuals to engage with one another in a digital setting. Prominent virtual social platforms, such as Facebook Spaces, VR Chat, and AltspaceVR, facilitate social connections, allowing users to interact seamlessly. Additionally, certain video games, like Second Life and World of Warcraft, are set within these virtual spaces as well, providing immersive player experiences. As the popularity of virtual space grows, various companies have begun to democratize the process of creating these spaces, shifting the development from skilled professionals to hobbyist creators. Platforms like Minecraft, Roblox, and RecRoom enable users to create and publish their own virtual environments, hosting a wide range of interactions and narratives. This shift echoes the rise of user-generated content, where content creators create and publish content on platforms, such as social media platforms [6]. For example, YouTubers upload videos on YouTube and Reddit users post text-based content on Reddit. For a long time, user-generated content has predominantly contained text, videos, and images. However, with the emergence of virtual spaces, some platforms now allow creators to create and publish their own virtual spaces, leading to the emergence of user-generated virtual worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05922v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zinan Zhang, Xinning Gui, Yubo Kou</dc:creator>
    </item>
    <item>
      <title>Darkverse -- A New DarkWeb?</title>
      <link>https://arxiv.org/abs/2405.05923</link>
      <description>arXiv:2405.05923v1 Announce Type: new 
Abstract: The "Darkverse" could be the negative harmful area of the Metaverse; a new virtual immersive environment for the facilitation of illicit activity such as misinformation, fraud, harassment, and illegal marketplaces. This paper explores the potential for inappropriate activities within the Metaverse, and the similarities between the Darkverse and the Dark Web. Challenges and future directions for investigation are also discussed, including user identification, creation of privacy-preserving frameworks and other data monitoring methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05923v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raymond Chan, Benjamin W. J. Kwok, Adriel Yeo, Kan Chen, Jeannie S. Lee</dc:creator>
    </item>
    <item>
      <title>Privacy Protection and Video Manipulation in Immersive Media</title>
      <link>https://arxiv.org/abs/2405.05924</link>
      <description>arXiv:2405.05924v1 Announce Type: new 
Abstract: In comparison to traditional footage, 360{\deg} videos can convey engaging, immersive experiences and even be utilized to create interactive virtual environments. Like regular recordings, these videos need to consider the privacy of recorded people and could be targets for video manipulations. However, due to their properties like enhanced presence, the effects on users might differ from traditional, non-immersive content. Therefore, we are interested in how changes of real-world footage like adding privacy protection or applying video manipulations could mitigate or introduce harm in the resulting immersive media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05924v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Leslie W\"ohler, Satoshi Ikehata, Kiyoharu Aizawa</dc:creator>
    </item>
    <item>
      <title>New Harms Moderated by Immersive Experiences and Breaks in Them</title>
      <link>https://arxiv.org/abs/2405.05926</link>
      <description>arXiv:2405.05926v1 Announce Type: new 
Abstract: This proposal highlights the potential real-world consequences of harmful experiences in immersive and embodied spaces due to presence, which moderates experiences, intensifying positive or negative content. While positive experiences enhance social interactions, negative content can lead to harm. Also, presence is not continuous and may break. Understanding the threshold between harmful experiences and breaks in presence is essential. This proposal underscores the need to explore whether the level of immersion or the nature of harm defines the boundary, emphasizing key discussion points for the workshop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05926v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eugene Kukshinov</dc:creator>
    </item>
    <item>
      <title>Moderating Embodied Cyber Threats Using Generative AI</title>
      <link>https://arxiv.org/abs/2405.05928</link>
      <description>arXiv:2405.05928v1 Announce Type: new 
Abstract: The advancement in computing and hardware, like spatial computing and VR headsets (e.g., Apple's Vision Pro) [1], has boosted the popularity of social VR platforms (VRChat, Rec Room, Meta HorizonWorlds) [2, 3, 4]. Unlike traditional digital interactions, social VR allows for more immersive experiences, with avatars that mimic users' real-time movements and enable physical-like interactions. However, the immersive nature of social VR may introduce intensified and more physicalized cyber threats-we define as "embodied cyber threats", including trash-talking, virtual "groping", and such virtual harassment and assault. These new cyber threats are more realistic and invasive due to direct, virtual interactions, underscoring the urgent need for comprehensive understanding and practical strategies to enhance safety and security in virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05928v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Keyan Guo, Freeman Guo, Hongxin Hu</dc:creator>
    </item>
    <item>
      <title>Understanding Emotional Hijacking in Metaverse</title>
      <link>https://arxiv.org/abs/2405.05929</link>
      <description>arXiv:2405.05929v1 Announce Type: new 
Abstract: Emotions are an integral part of being human, and experiencing a range of emotions is what makes life rich and vibrant. From basic emotions like anger, fear, happiness, and sadness to more complex ones like excitement and grief, emotions help us express ourselves and connect with the world around us. In recent years, researchers have begun adopting virtual reality (VR) technology to evoke emotions as realistically as possible and quantify the strength of emotions from the electroencephalogram (EEG) signals measured from the brain to understand human emotions in realistic situations better. This is achieved by creating a sense of presence in the virtual environment, the feeling that the user is there. For instance, [6] studied the excitement of a rollercoaster ride in VR, and [5] studied the fear of navigating in a VR cave.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05929v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Syed Ali Asif, Philip Gable, Chien-Chung Shen, Yan-Ming Chiou</dc:creator>
    </item>
    <item>
      <title>Harms in Repurposing Real-World Sensory Cues for Mixed Reality: A Causal Perspective</title>
      <link>https://arxiv.org/abs/2405.05931</link>
      <description>arXiv:2405.05931v1 Announce Type: new 
Abstract: The rise of Mixed Reality (MR) stimulates new interactive techniques that seamlessly blend the virtual and physical environments. Just as virtual content could be overlayed onto the physical world for providing adaptive user interfaces [5, 8], emergent techniques "repurpose" everyday environments and sensory cues to support the virtual content [7, 9, 13-15]. For instance, a strong wind gust in the real world, rather than being distracting to the virtual experience, can be mapped with trees swaying in MR to achieve a unifying experience [15], as shown in Figure 1. Such techniques introduce stronger immersion, but they also expose users to overlooked perceptual manipulations, where safety risks arise from misperception of real-world events. In this work, we apply a causal inference perspective to understand the harms of repurposing real-world sensory cues for MR. We argue that by viewing the MR experience as a causal inference process of interpreting cues arising from both the virtual and physical world, MR designers and researchers can gain a new lens to understand potential perceptual manipulation harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05931v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yujie Tao, Sean Follmer</dc:creator>
    </item>
    <item>
      <title>Considering Avatar Crossing as Harm or Help for Adolescents in Social VR</title>
      <link>https://arxiv.org/abs/2405.05933</link>
      <description>arXiv:2405.05933v1 Announce Type: new 
Abstract: People leverage avatars to communicate nonverbal behaviors in immersive virtual reality (VR), like interpersonal distance [2, 6] and virtual touch [5]. However, violations of appropriate physical distancing and unsolicited intimate touching behavior in social virtual worlds represent potential social and psychological virtual harm to older adolescent users [4, 8]. Obtaining peer acceptance and social rewards, while avoiding social rejection can drive older adolescent behavior even in simulated virtual spaces [1, 3], and while "the beginning of adolescence is largely defined by a biological event, [...] the end of adolescence is often defined socially" [3] (p.912). Avatar crossing, the phenomenon of avatars walking through each other in virtual environments, is a unique capability of virtual embodiment, and others intriguing possibilities and ethical concerns for older adolescents experiencing social virtual spaces. For example, the ability to cross through and share positions with other avatars in a virtual classroom helps students concentrate on accessing and comprehending information without concerns about blocking others when navigating for better viewpoints [10]. However, the ability to cross through others in virtual spaces has been associated with a reduction in perceived presence and avatar realism, coupled with a greater level of discomfort and intimidation in comparison to avatar collisions [12]. In this article, we consider the potential benefits and harms of utilizing avatar crossing with adolescent users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05933v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jakki O. Bailey (Sally),  Xinyue (Sally),  You</dc:creator>
    </item>
    <item>
      <title>A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities</title>
      <link>https://arxiv.org/abs/2405.05947</link>
      <description>arXiv:2405.05947v1 Announce Type: new 
Abstract: Politics is the set of activities related to strategic decision-making in groups. Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions. While some decisions might alleviate social problems, others might lead to disasters such as war and conflict. Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts. However, political researchers' perspectives on data visualization are unclear. This paper examines political scientists' perspectives on visualization and how they apply data visualization in their research. We discovered a growing trend in the use of graphs in political science journals. However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers. To reduce this gap, we survey visualization techniques applicable to the political scientists' research and report the visual analytics systems implemented for and evaluated by political scientists. At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics. Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05947v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dongyun Han, Abdullah-Al-Raihan Nayeem, Jason Windett, Yaoyao Dai, Benjamin Radford, Isaac Cho</dc:creator>
    </item>
    <item>
      <title>QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums</title>
      <link>https://arxiv.org/abs/2405.05345</link>
      <description>arXiv:2405.05345v1 Announce Type: cross 
Abstract: Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting methodology and evaluation strategy. We applied this framework to analyze over one million comments from two Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05345v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, Andr\'es Monroy-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>"They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations</title>
      <link>https://arxiv.org/abs/2405.05378</link>
      <description>arXiv:2405.05378v1 Announce Type: cross 
Abstract: Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate "harm" as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05378v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra</dc:creator>
    </item>
    <item>
      <title>The Power of Absence: Thinking with Archival Theory in Algorithmic Design</title>
      <link>https://arxiv.org/abs/2405.05420</link>
      <description>arXiv:2405.05420v1 Announce Type: cross 
Abstract: This paper explores the value of archival theory as a means of grappling with bias in algorithmic design. Rather than seek to mitigate biases perpetuated by datasets and algorithmic systems, archival theory offers a reframing of bias itself. Drawing on a range of archival theory from the fields of history, literary and cultural studies, Black studies, and feminist STS, we propose absence-as power, presence, and productive-as a concept that might more securely anchor investigations into the causes of algorithmic bias, and that can prompt more capacious, creative, and joyful future work. This essay, in turn, can intervene into the technical as well as the social, historical, and political structures that serve as sources of bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05420v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3660690</arxiv:DOI>
      <dc:creator>Jihan Sherman, Romi Morrison, Lauren Klein, Daniela K. Rosner</dc:creator>
    </item>
    <item>
      <title>RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation</title>
      <link>https://arxiv.org/abs/2405.05792</link>
      <description>arXiv:2405.05792v1 Announce Type: cross 
Abstract: Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on "image segments", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a "continuous sense of a place", defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of "hops over segments" and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05792v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko S\"underhauf, Feras Dayoub, Ian Reid</dc:creator>
    </item>
    <item>
      <title>Warmth and competence in human-agent cooperation</title>
      <link>https://arxiv.org/abs/2201.13448</link>
      <description>arXiv:2201.13448v4 Announce Type: replace 
Abstract: Interaction and cooperation with humans are overarching aspirations of artificial intelligence (AI) research. Recent studies demonstrate that AI agents trained with deep reinforcement learning are capable of collaborating with humans. These studies primarily evaluate human compatibility through "objective" metrics such as task performance, obscuring potential variation in the levels of trust and subjective preference that different agents garner. To better understand the factors shaping subjective preferences in human-agent cooperation, we train deep reinforcement learning agents in Coins, a two-player social dilemma. We recruit $N = 501$ participants for a human-agent cooperation study and measure their impressions of the agents they encounter. Participants' perceptions of warmth and competence predict their stated preferences for different agents, above and beyond objective performance metrics. Drawing inspiration from social science and biology research, we subsequently implement a new ``partner choice'' framework to elicit revealed preferences: after playing an episode with an agent, participants are asked whether they would like to play the next episode with the same agent or to play alone. As with stated preferences, social perception better predicts participants' revealed preferences than does objective performance. Given these results, we recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.13448v4</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10458-024-09649-6</arxiv:DOI>
      <dc:creator>Kevin R. McKee, Xuechunzi Bai, Susan T. Fiske</dc:creator>
    </item>
    <item>
      <title>PodReels: Human-AI Co-Creation of Video Podcast Teasers</title>
      <link>https://arxiv.org/abs/2311.05867</link>
      <description>arXiv:2311.05867v3 Announce Type: replace 
Abstract: Video podcast teasers are short videos that can be shared on social media platforms to capture interest in the full episodes of a video podcast. These teasers enable long-form podcasters to reach new audiences and gain new followers. However, creating a compelling teaser from an hour-long episode is challenging. Selecting interesting clips requires significant mental effort; editing the chosen clips into a cohesive, well-produced teaser is time-consuming. To support the creation of video podcast teasers, we first investigate what makes a good teaser. We combine insights from both audience comments and creator interviews to determine a set of essential ingredients. We also identify a common workflow shared by creators during the process. Based on these findings, we introduce a human-AI co-creative tool called PodReels to assist video podcasters in creating teasers. Our user study shows that PodReels significantly reduces creators' mental demand and improves their efficiency in producing video podcast teasers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05867v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitong Wang, Zheng Ning, Anh Truong, Mira Dontcheva, Dingzeyu Li, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>"You Might Like It": How People Respond to Small Talk During Human-Robot Collaboration</title>
      <link>https://arxiv.org/abs/2312.07454</link>
      <description>arXiv:2312.07454v2 Announce Type: replace 
Abstract: Social communication between people and social robots has been studied extensively and found to have various notable benefits, including the enhancement of human-robot team cohesion and the development of rapport and trust. However, the potential of social communication between people and non-social robots, such as non-anthropomorphic robot manipulators commonly used in work settings (\eg warehouse and factory), is less explored and not well established. In this work, we investigate people's engagement and attitudes towards a non-anthropomorphic robot manipulator that initiates small talk during a collaborative assembly task and explore how the presence of negative team feedback may affect team dynamics and blame attribution. Through an in-person study with 20 participants, we observed a response rate of 77.60% in response to the robot's small talk attempts. Nine participants continued engaging with the robot by initiating their own questions, indicating sustained interest in the conversation. However, we also found that the first negative feedback decreased the participants' willingness to extend the conversation. We additionally present participants' initial perceptions of small talk for physical robot manipulators and discuss design implications for integrating small talk into non-social robots, along with various aspects of small talk that may influence physical human-robot interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07454v2</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlynn Taylor Pineda, Amama Mahmood, Juo-Tung Chen, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Human Factors in the LastPass Breach</title>
      <link>https://arxiv.org/abs/2405.01795</link>
      <description>arXiv:2405.01795v2 Announce Type: replace 
Abstract: This paper examines the complex nature of cyber attacks through an analysis of the LastPass breach. It argues for the integration of human-centric considerations into cybersecurity measures, focusing on mitigating factors such as goal-directed behavior, cognitive overload, human biases (e.g., optimism, anchoring), and risky behaviors. Findings from an analysis of this breach offers support to the perspective that addressing both the human and technical dimensions of cyber defense can significantly enhance the resilience of cyber systems against complex threats. This means maintaining a balanced approach while simultaneously simplifying user interactions, making users aware of biases, and discouraging risky practices are essential for preventing cyber incidents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01795v2</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Niroop Sugunaraj</dc:creator>
    </item>
    <item>
      <title>ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems</title>
      <link>https://arxiv.org/abs/2308.04588</link>
      <description>arXiv:2308.04588v2 Announce Type: replace-cross 
Abstract: Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribution. ML consumers and engineers can visually compare the salient features of test samples with training examples through the use of a ``hover callback'' to understand model uncertainty performance and decide follow up courses of action. We demonstrate the effectiveness of ScatterUQ to explain model uncertainty for a multiclass image classification on a distance-aware neural network trained on Fashion-MNIST and tested on Fashion-MNIST (in distribution) and MNIST digits (out of distribution), as well as a deep learning model for a cyber dataset. We quantitatively evaluate dimensionality reduction techniques to optimize our contextually driven UQ visualizations. Our results indicate that the ScatterUQ system should scale to arbitrary, multiclass datasets. Our code is available at https://github.com/mit-ll-responsible-ai/equine-webapp</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04588v2</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VIS54172.2023.00058</arxiv:DOI>
      <arxiv:journal_reference>2023 IEEE Visualization and Visual Analytics (VIS)</arxiv:journal_reference>
      <dc:creator>Harry Li, Steven Jorgensen, John Holodnak, Allan Wollaber</dc:creator>
    </item>
    <item>
      <title>On the Potential of an Independent Avatar to Augment Metaverse Social Networks</title>
      <link>https://arxiv.org/abs/2312.07077</link>
      <description>arXiv:2312.07077v2 Announce Type: replace-cross 
Abstract: We present a computational modelling approach which targets capturing the specifics on how to virtually augment a Metaverse user's available social time capacity via using an independent and autonomous version of her digital representation in the Metaverse. We motivate why this is a fundamental building block to model large-scale social networks in the Metaverse, and emerging properties herein. We envision a Metaverse-focused extension of the traditional avatar concept: An avatar can be as well programmed to operate independently when its user is not controlling it directly, thus turning it into an agent-based digital human representation. This way, we highlight how such an independent avatar could help its user to better navigate their social relationships and optimize their socializing time in the Metaverse by (partly) offloading some interactions to the avatar. We model the setting and identify the characteristic variables by using selected concepts from social sciences: ego networks, social presence, and social cues. Then, we formulate the problem of maximizing the user's non-avatar-mediated spare time as a linear optimization. Finally, we analyze the feasible region of the problem and we present some initial insights on the spare time that can be achieved for different parameter values of the avatar-mediated interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07077v2</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE ICCCN 2024</arxiv:journal_reference>
      <dc:creator>Theofanis P. Raptis, Chiara Boldrini, Marco Conti, Andrea Passarella</dc:creator>
    </item>
    <item>
      <title>A Decision Theoretic Framework for Measuring AI Reliance</title>
      <link>https://arxiv.org/abs/2401.15356</link>
      <description>arXiv:2401.15356v3 Announce Type: replace-cross 
Abstract: Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's recommendation from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies from literature, we demonstrate how our framework can be used to separate the loss due to mis-reliance from the loss due to not accurately differentiating the signals. We evaluate these losses by comparing to a baseline and a benchmark for complementary performance defined by the expected payoff achieved by a rational decision-maker facing the same decision task as the behavioral decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15356v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman</dc:creator>
    </item>
    <item>
      <title>PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers</title>
      <link>https://arxiv.org/abs/2403.02939</link>
      <description>arXiv:2403.02939v2 Announce Type: replace-cross 
Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02939v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642196</arxiv:DOI>
      <dc:creator>Yoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue</dc:creator>
    </item>
    <item>
      <title>Creative Beam Search: LLM-as-a-Judge For Improving Response Generation</title>
      <link>https://arxiv.org/abs/2405.00099</link>
      <description>arXiv:2405.00099v2 Announce Type: replace-cross 
Abstract: Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00099v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Franceschelli, Mirco Musolesi</dc:creator>
    </item>
  </channel>
</rss>
