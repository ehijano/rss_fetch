<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 02:50:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Random sampling of partitions and contingency tables: Two practical examples of the Burnside process</title>
      <link>https://arxiv.org/abs/2503.02818</link>
      <description>arXiv:2503.02818v1 Announce Type: new 
Abstract: The Burnside process is a general algorithm for sampling a uniformly chosen orbit of a finite group $G$ acting on a finite set $\mathcal{X}$. For example, if $\mathcal{X} = G$ and $G$ acts on itself by conjugation ($s^t = t^{-1}st$), then the orbits are conjugacy classes. When $G$ is the symmetric group $S_n$, the conjugacy classes are indexed by partitions of $n$, so the Burnside process gives a way to sample partitions. If $\mathcal{X}=S_n$ and $G$ is a product of symmetric groups, then the orbits are labeled by contingency tables: non-negative integer arrays with given row and column sums. Actually carrying out the Burnside process requires new combinatorics and group theory. This is worked out and illustrated for these two examples. For partitions, we also developed a new Markov chain called the reflected Burnside process which greatly improves the mixing of the Burnside process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02818v1</guid>
      <category>stat.CO</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Persi Diaconis, Michael Howes</dc:creator>
    </item>
    <item>
      <title>Generalized Tree-Informed Mixed Model Regression</title>
      <link>https://arxiv.org/abs/2503.02266</link>
      <description>arXiv:2503.02266v1 Announce Type: cross 
Abstract: The standard regression tree method applied to observations within clusters poses both methodological and implementation challenges. Effectively leveraging these data requires methods that account for both individual-level and sample-level effects. We propose Generalized Tree-Informed Mixed Model (GTIMM), which replaces the linear fixed effect in a generalized linear mixed model (GLMM) with the output of a regression tree. Traditional parameter estimation and prediction techniques, such as the expectation-maximization algorithm, scale poorly in high-dimensional settings, creating a computational bottleneck. To address this, we employ a quasi-likelihood framework with stochastic gradient descent for optimized parameter estimation. Additionally, we establish a theoretical bound for the mean squared prediction error. The predictive performance of our method is evaluated through simulations and compared with existing approaches. Finally, we apply our model to predict country-level GDP based on trade, foreign direct investment, unemployment, inflation, and geographic region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02266v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremiah Allis, Xin Jin, Riddhi Ghosh</dc:creator>
    </item>
    <item>
      <title>Fast and robust invariant generalized linear models</title>
      <link>https://arxiv.org/abs/2503.02611</link>
      <description>arXiv:2503.02611v1 Announce Type: cross 
Abstract: Statistical integration of diverse data sources is an essential step in the building of generalizable prediction tools, especially in precision health. The invariant features model is a new paradigm for multi-source data integration which posits that a small number of covariates affect the outcome identically across all possible environments. Existing methods for estimating invariant effects suffer from immense computational costs or only offer good statistical performance under strict assumptions. In this work, we provide a general framework for estimation under the invariant features model that is computationally efficient and statistically flexible. We also provide a robust extension of our proposed method to protect against possibly corrupted or misspecified data sources. We demonstrate the robust properties of our method via simulations, and use it to build a transferable prediction model for end stage renal disease using electronic health records from the All of Us research program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02611v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parker Knight, Ndey Isatou Jobe, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Auxiliary MCMC and particle Gibbs samplers for parallelisable inference in latent dynamical systems</title>
      <link>https://arxiv.org/abs/2303.00301</link>
      <description>arXiv:2303.00301v3 Announce Type: replace 
Abstract: Sampling from the full posterior distribution of high-dimensional non-linear, non-Gaussian latent dynamical models presents significant computational challenges. While Particle Gibbs (also known as conditional sequential Monte Carlo) is considered the gold standard for this task, it quickly degrades in performance as the latent space dimensionality increases. Conversely, globally Gaussian-approximated methods like extended Kalman filtering, though more robust, are seldom used for posterior sampling due to their inherent bias. We introduce novel auxiliary sampling approaches that address these limitations. By incorporating artificial observations of the system as auxiliary variables in our MCMC kernels, we develop both efficient exact Kalman-based samplers and enhanced Particle Gibbs algorithms that maintain performance in high-dimensional latent spaces. Some of our methods support parallelisation along the time dimension, achieving logarithmic scaling when implemented on GPUs. Empirical evaluations demonstrate superior statistical and computational performance compared to existing approaches for high-dimensional latent dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00301v3</guid>
      <category>stat.CO</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Simo S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2303.13429</link>
      <description>arXiv:2303.13429v3 Announce Type: replace 
Abstract: We develop a class of interacting particle systems for implementing a maximum marginal likelihood estimation (MMLE) procedure to estimate the parameters of a latent variable model. We achieve this by formulating a continuous-time interacting particle system which can be seen as a Langevin diffusion over an extended state space of parameters and latent variables. In particular, we prove that the parameter marginal of the stationary measure of this diffusion has the form of a Gibbs measure where number of particles acts as the inverse temperature parameter in classical settings for global optimisation. Using a particular rescaling, we then prove geometric ergodicity of this system and bound the discretisation error in a manner that is uniform in time and does not increase with the number of particles. The discretisation results in an algorithm, termed Interacting Particle Langevin Algorithm (IPLA) which can be used for MMLE. We further prove nonasymptotic bounds for the optimisation error of our estimator in terms of key parameters of the problem, and also extend this result to the case of stochastic gradients covering practical scenarios. We provide numerical experiments to illustrate the empirical behaviour of our algorithm in the context of logistic regression with verifiable assumptions. Our setting provides a straightforward way to implement a diffusion-based optimisation routine compared to more classical approaches such as the Expectation Maximisation (EM) algorithm, and allows for especially explicit nonasymptotic bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13429v3</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"O. Deniz Akyildiz, Francesca Romana Crucinio, Mark Girolami, Tim Johnston, Sotirios Sabanis</dc:creator>
    </item>
    <item>
      <title>Modelling pathwise uncertainty of Stochastic Differential Equations samplers via Probabilistic Numerics</title>
      <link>https://arxiv.org/abs/2401.03338</link>
      <description>arXiv:2401.03338v2 Announce Type: replace-cross 
Abstract: Probabilistic ordinary differential equation (ODE) solvers have been introduced over the past decade as uncertainty-aware numerical integrators. They typically proceed by assuming a functional prior to the ODE solution, which is then queried on a grid to form a posterior distribution over the ODE solution. As the queries span the integration interval, the approximate posterior solution then converges to the true deterministic one. Gaussian ODE filters, in particular, have enjoyed a lot of attention due to their computational efficiency, the simplicity of their implementation, as well as their provable fast convergence rates. In this article, we extend the methodology to stochastic differential equations (SDEs) and propose a probabilistic simulator for SDEs. Our approach involves transforming the SDE into a sequence of random ODEs using piecewise differentiable approximations of the Brownian motion. We then apply probabilistic ODE solvers to the individual ODEs, resulting in a pathwise probabilistic solution to the SDE\@. We establish worst-case strong $1.5$ local and $1.0$ global convergence orders for a specific instance of our method. We further show how we can marginalise the Brownian approximations, by incorporating its coefficients as part of the prior ODE model, allowing for computing exact transition densities under our model. Finally, we numerically validate the theoretical findings, showcasing reasonable weak convergence properties in the marginalised version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03338v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yvann Le Fay, Simo S\"arkk\"a, Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Graph sub-sampling for divide-and-conquer algorithms in large networks</title>
      <link>https://arxiv.org/abs/2409.06994</link>
      <description>arXiv:2409.06994v3 Announce Type: replace-cross 
Abstract: As networks continue to increase in size, current methods must be capable of handling large numbers of nodes and edges in order to be practically relevant. Instead of working directly with the entire (large) network, analyzing sub-networks has become a popular approach. Due to a network's inherent inter-connectedness, however, sub-sampling is not a trivial task. While this problem has gained popularity in recent years, it has not received sufficient attention from the statistics community. In this work, we provide a thorough comparison of seven graph sub-sampling algorithms by applying them to divide-and-conquer algorithms for community structure and core-periphery (CP) structure. After discussing the various algorithms and sub-sampling routines, we derive theoretical results for the mis-classification rate of the divide-and-conquer algorithm for CP structure under various sub-sampling schemes. We then perform extensive experiments on both simulated and real-world data to compare the various methods. For the community detection task, we found that sampling nodes uniformly at random yields the best performance, but that sometimes the base algorithm applied to the entire network yields better results both in terms of identification and computational time. For CP structure on the other hand, there was no single winner, but algorithms which sampled core nodes at a higher rate consistently outperformed other sampling routines, e.g., random edge sampling and random walk sampling. Unlike community detection, the CP divide-and-conquer algorithm tends to yield better identification results while also being faster than the base algorithm. The varying performance of the sampling algorithms on different tasks demonstrates the importance of carefully selecting a sub-sampling routine for the specific application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06994v3</guid>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko</dc:creator>
    </item>
  </channel>
</rss>
