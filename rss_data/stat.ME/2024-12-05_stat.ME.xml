<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 05:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Chain-linked Multiple Matrix Integration via Embedding Alignment</title>
      <link>https://arxiv.org/abs/2412.02791</link>
      <description>arXiv:2412.02791v1 Announce Type: new 
Abstract: Motivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures -- specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available. We propose the Chain-linked Multiple Matrix Integration (CMMI) procedure to efficiently combine the information that can be extracted from these individual noisy submatrices. CMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using overlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest. We establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates. Simulation studies and real data applications show that CMMI is computationally efficient and effective in recovering the full matrix, even when overlaps between the observed submatrices are minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02791v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Detection of Multiple Influential Observations on Model Selection</title>
      <link>https://arxiv.org/abs/2412.02945</link>
      <description>arXiv:2412.02945v1 Announce Type: new 
Abstract: Outlying observations are frequently encountered in a wide spectrum of scientific domains, posing significant challenges for the generalizability of statistical models and the reproducibility of downstream analysis. These observations can be identified through influential diagnosis, which refers to the detection of observations that are unduly influential on diverse facets of statistical inference. To date, methods for identifying observations influencing the choice of a stochastically selected submodel have been underdeveloped, especially in the high-dimensional setting where the number of predictors p exceeds the sample size n. Recently we proposed an improved diagnostic measure to handle this setting. However, its distributional properties and approximations have not yet been explored. To address this shortcoming, the notion of exchangeability is revived, and used to determine the exact finite- and large-sample distributions of our assessment metric. This forms the foundation for the introduction of both parametric and non-parametric approaches for its approximation and the establishment of thresholds for diagnosis. The resulting framework is extended to logistic regression models, followed by a simulation study conducted to assess the performance of various detection procedures. Finally the framework is applied to data from an fMRI study of thermal pain, with the goal of identifying outlying subjects that could distort the formulation of statistical models using functional brain activity in predicting physical pain ratings. Both linear and logistic regression models are used to demonstrate the benefits of detection and compare the performances of different detection procedures. In particular, two additional influential observations are identified, which are not discovered by previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02945v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongliang Zhang, Masoud Asgharian, Martin A. Lindquist</dc:creator>
    </item>
    <item>
      <title>Uncovering dynamics between SARS-CoV-2 wastewater concentrations and community infections via Bayesian spatial functional concurrent regression</title>
      <link>https://arxiv.org/abs/2412.02970</link>
      <description>arXiv:2412.02970v1 Announce Type: new 
Abstract: Monitoring wastewater concentrations of SARS-CoV-2 yields a low-cost, noninvasive method for tracking disease prevalence and provides early warning signs of upcoming outbreaks in the serviced communities. There is tremendous clinical and public health interest in understanding the exact dynamics between wastewater viral loads and infection rates in the population. As both data sources may contain substantial noise and missingness, in addition to spatial and temporal dependencies, properly modeling this relationship must address these numerous complexities simultaneously while providing interpretable and clear insights. We propose a novel Bayesian functional concurrent regression model that accounts for both spatial and temporal correlations while estimating the dynamic effects between wastewater concentrations and positivity rates over time. We explicitly model the time lag between the two series and provide full posterior inference on the possible delay between spikes in wastewater concentrations and subsequent outbreaks. We estimate a time lag likely between 5 to 11 days between spikes in wastewater levels and reported clinical positivity rates. Additionally, we find a dynamic relationship between wastewater concentration levels and the strength of its association with positivity rates that fluctuates between outbreaks and non-outbreaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas Y. Sun, Julia C. Schedler, Daniel R. Kowal, Rebecca Schneider, Lauren B. Stadler, Loren Hopkins, Katherine B. Ensor</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for Enhanced Estimation and Inference</title>
      <link>https://arxiv.org/abs/2412.02986</link>
      <description>arXiv:2412.02986v1 Announce Type: new 
Abstract: Transfer learning enhances model performance in a target population with limited samples by leveraging knowledge from related studies. While many works focus on improving predictive performance, challenges of statistical inference persist. Bayesian approaches naturally offer uncertainty quantification for parameter estimates, yet existing Bayesian transfer learning methods are typically limited to single-source scenarios or require individual-level data. We introduce TRansfer leArning via guideD horseshoE prioR (TRADER), a novel approach enabling multi-source transfer through pre-trained models in high-dimensional linear regression. TRADER shrinks target parameters towards a weighted average of source estimates, accommodating sources with different scales. Theoretical investigation shows that TRADER achieves faster posterior contraction rates than standard continuous shrinkage priors when sources align well with the target while preventing negative transfer from heterogeneous sources. The analysis of finite-sample marginal posterior behavior reveals that TRADER achieves desired frequentist coverage probabilities, even for coefficients with moderate signal strength--a scenario where standard continuous shrinkage priors struggle. Extensive numerical studies and a real-data application estimating the association between blood glucose and insulin use in the Hispanic diabetic population demonstrate that TRADER improves estimation and inference accuracy over continuous shrinkage priors using target data alone, while outperforming a state-of-the-art transfer learning method that requires individual-level data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02986v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daoyuan Lai, Oscar Hernan Madrid Padilla, Tian Gu</dc:creator>
    </item>
    <item>
      <title>On a penalised likelihood approach for joint modelling of longitudinal covariates and partly interval-censored data -- an application to the Anti-PD1 brain collaboration trial</title>
      <link>https://arxiv.org/abs/2412.03042</link>
      <description>arXiv:2412.03042v1 Announce Type: new 
Abstract: This article considers the joint modeling of longitudinal covariates and partly-interval censored time-to-event data. Longitudinal time-varying covariates play a crucial role in obtaining accurate clinically relevant predictions using a survival regression model. However, these covariates are often measured at limited time points and may be subject to measurement error. Further methodological challenges arise from the fact that, in many clinical studies, the event times of interest are interval-censored. A model that simultaneously accounts for all these factors is expected to improve the accuracy of survival model estimations and predictions. In this article, we consider joint models that combine longitudinal time-varying covariates with the Cox model for time-to-event data which is subject to interval censoring. The proposed model employs a novel penalised likelihood approach for estimating all parameters, including the random effects. The covariance matrix of the estimated parameters can be obtained from the penalised log-likelihood. The performance of the model is compared to an existing method under various scenarios. The simulation results demonstrated that our new method can provide reliable inferences when dealing with interval-censored data. Data from the Anti-PD1 brain collaboration clinical trial in advanced melanoma is used to illustrate the application of the new method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03042v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annabel Webb, Nan Zou, Serigne Lo, Jun Ma</dc:creator>
    </item>
    <item>
      <title>Information borrowing in Bayesian clinical trials: choice of tuning parameters for the robust mixture prior</title>
      <link>https://arxiv.org/abs/2412.03185</link>
      <description>arXiv:2412.03185v1 Announce Type: new 
Abstract: Borrowing historical data for use in clinical trials has increased in recent years. This is accomplished in the Bayesian framework by specification of informative prior distributions. One such approach is the robust mixture prior arising as a weighted mixture of an informative prior and a robust prior inducing dynamic borrowing that allows to borrow most when the current and external data are observed to be similar. The robust mixture prior requires the choice of three additional quantities: the mixture weight, and the mean and dispersion of the robust component. Some general guidance is available, but a case-by-case study of the impact of these quantities on specific operating characteristics seems lacking. We focus on evaluating the impact of parameter choices for the robust component of the mixture prior in one-arm and hybrid-control trials. The results show that all three quantities can strongly impact the operating characteristics. In particular, as already known, variance of the robust component is linked to robustness. Less known, however, is that its location can have a strong impact on Type I error rate and MSE which can even become unbounded. Further, the impact of the weight choice is strongly linked with the robust component's location and variance. Recommendations are provided for the choice of the robust component parameters, prior weight, alternative functional form for this component as well as considerations to keep in mind when evaluating operating characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03185v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vivienn Weru, Annette Kopp-Schneider, Manuel Wiesenfarth, Sebastian Weber, Silvia Calderazzo</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the Patient Weighted While-Alive Estimand</title>
      <link>https://arxiv.org/abs/2412.03246</link>
      <description>arXiv:2412.03246v1 Announce Type: new 
Abstract: In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). We focus in this paper on the patient weighted while-alive estimand represented as the expected number of events divided by the time alive within a target window and develop efficient estimation for this estimand. We derive its efficient influence function and develop a one-step estimator, initially applied to the irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, the one-step estimator is practically intractable. We therefore suggest an alternative estimator that is also expected to have high efficiency focusing on the randomized treatment setting. We compare the efficiency of these two estimators in the illness-death setting. Additionally, we apply our proposed estimator to a real-world case study involving metastatic colorectal cancer patients, demonstrating the practical applicability and benefits of the while-alive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03246v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Torben Martinussen, Thomas Scheike</dc:creator>
    </item>
    <item>
      <title>Coherent forecast combination for linearly constrained multiple time series</title>
      <link>https://arxiv.org/abs/2412.03429</link>
      <description>arXiv:2412.03429v1 Announce Type: new 
Abstract: Linearly constrained multiple time series may be encountered in many practical contexts, such as the National Accounts (e.g., GDP disaggregated by Income, Expenditure and Output), and multilevel frameworks where the variables are organized according to hierarchies or groupings, like the total energy consumption of a country disaggregated by region and energy sources. In these cases, when multiple incoherent base forecasts for each individual variable are available, a forecast combination-and-reconciliation approach, that we call coherent forecast combination, may be used to improve the accuracy of the base forecasts and achieve coherence in the final result. In this paper, we develop an optimization-based technique that combines multiple unbiased base forecasts while assuring the constraints valid for the series. We present closed form expressions for the coherent combined forecast vector and its error covariance matrix in the general case where a different number of forecasts is available for each variable. We also discuss practical issues related to the covariance matrix that is part of the optimal solution. Through simulations and a forecasting experiment on the daily Australian electricity generation hierarchical time series, we show that the proposed methodology, in addition to adhering to sound statistical principles, may yield in significant improvement on base forecasts, single-task combination and single-expert reconciliation approaches as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03429v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Visualisation for Exploratory Modelling Analysis of Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2412.03484</link>
      <description>arXiv:2412.03484v1 Announce Type: new 
Abstract: When developing Bayesian hierarchical models, selecting the most appropriate hierarchical structure can be a challenging task, and visualisation remains an underutilised tool in this context. In this paper, we consider visualisations for the display of hierarchical models in data space and compare a collection of multiple models via their parameters and hyper-parameter estimates. Specifically, with the aim of aiding model choice, we propose new visualisations to explore how the choice of Bayesian hierarchical modelling structure impacts parameter distributions. The visualisations are designed using a robust set of principles to provide richer comparisons that extend beyond the conventional plots and numerical summaries typically used. As a case study, we investigate five Bayesian hierarchical models fit using the brms R package, a high-level interface to Stan for Bayesian modelling, to model country mathematics trends from the PISA (Programme for International Student Assessment) database. Our case study demonstrates that by adhering to these principles, researchers can create visualisations that not only help them make more informed choices between Bayesian hierarchical model structures but also enable them to effectively communicate the rationale for those choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03484v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oluwayomi Akinfenwa, Niamh Cahill, Catherine Hurley</dc:creator>
    </item>
    <item>
      <title>Constrained Identifiability of Causal Effects</title>
      <link>https://arxiv.org/abs/2412.02869</link>
      <description>arXiv:2412.02869v1 Announce Type: cross 
Abstract: We study the identification of causal effects in the presence of different types of constraints (e.g., logical constraints) in addition to the causal graph. These constraints impose restrictions on the models (parameterizations) induced by the causal graph, reducing the set of models considered by the identifiability problem. We formalize the notion of constrained identifiability, which takes a set of constraints as another input to the classical definition of identifiability. We then introduce a framework for testing constrained identifiability by employing tractable Arithmetic Circuits (ACs), which enables us to accommodate constraints systematically. We show that this AC-based approach is at least as complete as existing algorithms (e.g., do-calculus) for testing classical identifiability, which only assumes the constraint of strict positivity. We use examples to demonstrate the effectiveness of this AC-based approach by showing that unidentifiable causal effects may become identifiable under different types of constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02869v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizuo Chen, Adnan Darwiche</dc:creator>
    </item>
    <item>
      <title>Modeling and Discovering Direct Causes for Predictive Models</title>
      <link>https://arxiv.org/abs/2412.02878</link>
      <description>arXiv:2412.02878v1 Announce Type: cross 
Abstract: We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models) by representing it using causal graphs. The framework enables us to define and identify features that directly cause the predictions, which has broad implications for data collection and model evaluation. We show two assumptions under which the direct causes can be discovered from data, one of which further simplifies the discovery process. In addition to providing sound and complete algorithms, we propose an optimization technique based on an independence rule that can be integrated with the algorithms to speed up the discovery process both theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02878v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizuo Chen, Amit Bhatia</dc:creator>
    </item>
    <item>
      <title>Removing Spurious Correlation from Neural Network Interpretations</title>
      <link>https://arxiv.org/abs/2412.02893</link>
      <description>arXiv:2412.02893v1 Announce Type: cross 
Abstract: The existing algorithms for identification of neurons responsible for undesired and harmful behaviors do not consider the effects of confounders such as topic of the conversation. In this work, we show that confounders can create spurious correlations and propose a new causal mediation approach that controls the impact of the topic. In experiments with two large language models, we study the localization hypothesis and show that adjusting for the effect of conversation topic, toxicity becomes less localized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02893v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milad Fotouhi, Mohammad Taha Bahadori, Oluwaseyi Feyisetan, Payman Arabshahi, David Heckerman</dc:creator>
    </item>
    <item>
      <title>Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications</title>
      <link>https://arxiv.org/abs/2412.03491</link>
      <description>arXiv:2412.03491v1 Announce Type: cross 
Abstract: Adequately generating and evaluating prediction models based on supervised machine learning (ML) is often challenging, especially for less experienced users in applied research areas. Special attention is required in settings where the model generation process involves hyperparameter tuning, i.e. data-driven optimization of different types of hyperparameters to improve the predictive performance of the resulting model. Discussions about tuning typically focus on the hyperparameters of the ML algorithm (e.g., the minimum number of observations in each terminal node for a tree-based algorithm). In this context, it is often neglected that hyperparameters also exist for the preprocessing steps that are applied to the data before it is provided to the algorithm (e.g., how to handle missing feature values in the data). As a consequence, users experimenting with different preprocessing options to improve model performance may be unaware that this constitutes a form of hyperparameter tuning - albeit informal and unsystematic - and thus may fail to report or account for this optimization. To illuminate this issue, this paper reviews and empirically illustrates different procedures for generating and evaluating prediction models, explicitly addressing the different ways algorithm and preprocessing hyperparameters are typically handled by applied ML users. By highlighting potential pitfalls, especially those that may lead to exaggerated performance claims, this review aims to further improve the quality of predictive modeling in ML applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03491v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Sauer, Anne-Laure Boulesteix, Luzia Han{\ss}um, Farina Hodiamont, Claudia Bausewein, Theresa Ullmann</dc:creator>
    </item>
    <item>
      <title>The R.O.A.D. to clinical trial emulation</title>
      <link>https://arxiv.org/abs/2412.03528</link>
      <description>arXiv:2412.03528v1 Announce Type: cross 
Abstract: Observational studies provide the only evidence on the effectiveness of interventions when randomized controlled trials (RCTs) are impractical due to cost, ethical concerns, or time constraints. While many methodologies aim to draw causal inferences from observational data, there is a growing trend to model observational study designs after RCTs, a strategy known as "target trial emulation." Despite its potential, causal inference through target trial emulation cannot fully address the confounding bias in real-world data due to the lack of randomization. In this work, we present a novel framework for target trial emulation that aims to overcome several key limitations, including confounding bias. The framework proceeds as follows: First, we apply the eligibility criteria of a specific trial to an observational cohort. We then "correct" this cohort by extracting a subset that matches both the distribution of covariates and the baseline prognosis of the control group in the target RCT. Next, we address unmeasured confounding by adjusting the prognosis estimates of the treated group to align with those observed in the trial. Following trial emulation, we go a step further by leveraging the emulated cohort to train optimal decision trees, to identify subgroups of patients with heterogeneity in treatment effects (HTE). The absence of confounding is verified using two external models, and the validity of the treatment recommendations is independently confirmed by the team responsible for the original trial we emulate. To our knowledge, this is the first framework to successfully address both observed and unobserved confounding, a challenge that has historically limited the use of randomized trial emulation and causal inference. Additionally, our framework holds promise in advancing precision medicine by identifying patient subgroups that benefit most from specific treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03528v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Bertsimas, Angelos G. Koulouras, Hiroshi Nagata, Carol Gao, Junki Mizusawa, Yukihide Kanemitsu, Georgios Antonios Margonis</dc:creator>
    </item>
    <item>
      <title>Bayesian inference on the order of stationary vector autoregressions</title>
      <link>https://arxiv.org/abs/2307.05708</link>
      <description>arXiv:2307.05708v2 Announce Type: replace 
Abstract: Vector autoregressions (VARs) are a widely used tool for modelling multivariate time-series. It is common to assume a VAR is stationary; this can be enforced by imposing the stationarity condition which restricts the parameter space of the autoregressive coefficients to the stationary region. However, implementing this constraint is difficult due to the complex geometry of the stationary region. Fortunately, recent work has provided a solution for autoregressions of fixed order $p$ based on a reparameterization in terms of a set of interpretable and unconstrained transformed partial autocorrelation matrices. In this work, focus is placed on the difficult problem of allowing $p$ to be unknown, developing a prior and computational inference that takes full account of order uncertainty. Specifically, the multiplicative gamma process is used to build a prior which encourages increasing shrinkage of the partial autocorrelations with increasing lag. Identifying the lag beyond which the partial autocorrelations become equal to zero then determines $p$. Based on classic time-series theory, a principled choice of truncation criterion identifies whether a partial autocorrelation matrix is effectively zero. Posterior inference utilizes Hamiltonian Monte Carlo via Stan. The work is illustrated in a substantive application to neural activity data to investigate ultradian brain rhythms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05708v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel L. Binks, Sarah E. Heaps, Mariella Panagiotopoulou, Yujiang Wang, Darren J. Wilkinson</dc:creator>
    </item>
    <item>
      <title>Likelihood Based Inference for ARMA Models</title>
      <link>https://arxiv.org/abs/2310.01198</link>
      <description>arXiv:2310.01198v5 Announce Type: replace 
Abstract: Autoregressive moving average (ARMA) models are widely used for analyzing time series data. However, standard likelihood-based inference methodology for ARMA models has avoidable limitations. We show that common ARMA likelihood maximization strategies often lead to sub-optimal parameter estimates. While this possibility has been previously identified, no routinely applicable algorithm has been developed to resolve the issue. We introduce a novel random initialization algorithm, designed to take advantage of the structure of the ARMA likelihood function, which overcomes these optimization problems. Additionally, we show that profile confidence intervals provide superior confidence intervals to those based on the Fisher information matrix. The efficacy of the proposed methodology is demonstrated through a data analysis example and a series of simulation studies. This work makes a significant contribution to statistical practice by identifying and resolving under-recognized shortcomings of existing procedures that frequently arise in scientific and industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01198v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Wheeler, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Assessing Influential Observations in Pain Prediction using fMRI Data</title>
      <link>https://arxiv.org/abs/2401.13208</link>
      <description>arXiv:2401.13208v2 Announce Type: replace 
Abstract: Neuroimaging data allows researchers to model the relationship between multivariate patterns of brain activity and outcomes related to mental states and behaviors. However, the existence of outlying participants can potentially undermine the generalizability of these models and jeopardize the validity of downstream statistical analysis. To date, the ability to detect and account for participants unduly influencing various model selection approaches have been sorely lacking. Motivated by a task-based functional magnetic resonance imaging (fMRI) study of thermal pain, we propose and establish the asymptotic distribution for a diagnostic measure applicable to a number of different model selectors. A high-dimensional clustering procedure is further combined with this measure to detect multiple influential observations. In a series of simulations, our proposed method demonstrates clear advantages over existing methods in terms of improved detection performance, leading to enhanced predictive and variable selection outcomes. Application of our method to data from the thermal pain study illustrates the influence of outlying participants, in particular with regards to differences in activation between low and intense pain conditions. This allows for the selection of an interpretable model with high prediction power after removal of the detected observations. Though inspired by the fMRI-based thermal pain study, our methods are broadly applicable to other high-dimensional data types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13208v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongliang Zhang, Masoud Asgharian, Martin A. Lindquist</dc:creator>
    </item>
    <item>
      <title>Spatial-Sign based Maxsum Test for High Dimensional Location Parameters</title>
      <link>https://arxiv.org/abs/2402.01381</link>
      <description>arXiv:2402.01381v2 Announce Type: replace 
Abstract: In this study, we explore a robust testing procedure for the high-dimensional location parameters testing problem. Initially, we introduce a spatial-sign based max-type test statistic, which exhibits excellent performance for sparse alternatives. Subsequently, we demonstrate the asymptotic independence between this max-type test statistic and the spatial-sign based sum-type test statistic (Feng and Sun, 2016). Building on this, we propose a spatial-sign based max-sum type testing procedure, which shows remarkable performance under varying signal sparsity. Our simulation studies underscore the superior performance of the procedures we propose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01381v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixuan Liu, Long Feng, Ping Zhao, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Interpretability Indices and Soft Constraints for Factor Models</title>
      <link>https://arxiv.org/abs/2409.11525</link>
      <description>arXiv:2409.11525v5 Announce Type: replace 
Abstract: Factor analysis is a way to characterize the relationships between many observable variables in terms of a smaller number of unobservable random variables. However, the application of factor models and its success can be subjective or difficult to gauge, since the factor model is not identifiable. Thus, there is a need to operationalize a criterion that measures how meaningful or "interpretable" a factor model is. While there are already techniques that address interpretability, new indices and methods are proposed to measure interpretability. The proposed methods can directly incorporate both loadings and semantics, and are generalized to incorporate any "prior information". Moreover, the indices allow for complete or partial specification of relationships at a pairwise level. Two other main benefits of the proposed methods are that they do not require the estimation of factor scores, which avoids the factor score indeterminacy problem, and that no additional explanatory variables are necessary. The implementation of the proposed methods is written in Python 3 and is made available together with several helper functions through the package interpretablefa on the Python Package Index. The methods' application is demonstrated here using data on the Experiences in Close Relationships Scale, obtained from the Open-Source Psychometrics Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11525v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Philip Tuazon, Gia Mizrane Abubo, Joemari Olea</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Statistical Models Based on Counting Processes</title>
      <link>https://arxiv.org/abs/2210.07114</link>
      <description>arXiv:2210.07114v4 Announce Type: replace-cross 
Abstract: Since the famous paper written by Kaplan and Meier in 1958, survival analysis has become one of the most important fields in statistics. Nowadays it is one of the most important statistical tools in analyzing epidemiological and clinical data including COVID-19 pandemic. This article reviews some of the most celebrated and important results and methods, including consistency, asymptotic normality, bias and variance estimation, in survival analysis and the treatment is parallel to the monograph Statistical Models Based on Counting Processes. Other models and results such as semi-Markov models and the Turnbull's estimator that jump out of the classical counting process martingale framework are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07114v4</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui</dc:creator>
    </item>
    <item>
      <title>Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression</title>
      <link>https://arxiv.org/abs/2405.04919</link>
      <description>arXiv:2405.04919v2 Announce Type: replace-cross 
Abstract: We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04919v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</title>
      <link>https://arxiv.org/abs/2406.06671</link>
      <description>arXiv:2406.06671v2 Announce Type: replace-cross 
Abstract: Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm$\unicode{x2014}$a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06671v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection</title>
      <link>https://arxiv.org/abs/2408.02060</link>
      <description>arXiv:2408.02060v2 Announce Type: replace-cross 
Abstract: We study the problem of finding the index of the minimum value of a vector from noisy observations. This problem is relevant in population/policy comparison, discrete maximum likelihood, and model selection. We develop an asymptotically normal test statistic, even in high-dimensional settings and with potentially many ties in the population mean vector, by integrating concepts and tools from cross-validation and differential privacy. The key technical ingredient is a central limit theorem for globally dependent data. We also propose practical ways to select the tuning parameter that adapts to the signal landscape. Numerical experiments and data examples demonstrate the ability of the proposed method to achieve a favorable bias-variance trade-off in practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02060v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Hao Lee, Jing Lei</dc:creator>
    </item>
  </channel>
</rss>
