<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 02:55:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Audience Impressions of Narrative Structures and Personal Language Style in Science Communication on Social Media</title>
      <link>https://arxiv.org/abs/2502.05287</link>
      <description>arXiv:2502.05287v1 Announce Type: new 
Abstract: Science communication increases public interest in science by educating, engaging, and encouraging everyday people to participate in the sciences. But traditional science communication is often too formal and inaccessible for general audiences. However, there is a growing trend on social media to make it more approachable using three techniques: relatable examples to make explanations concrete, step-by-step walkthroughs to improve understanding, and personal language to drive engagement. These techniques are flashy and often garner more engagement from social media users, but the effectiveness of these techniques in actually explaining the science is unknown. Furthermore, many scientists struggle with adopting these science communication strategies for social media, fearing it might undermine their authority. We conduct a reader study to understand how these science communication techniques on social media affect readers' understanding and engagement of the science. We found that while most readers prefer these techniques, they had diverse preferences for when and where these techniques are used. With these findings, we conducted a writer study to understand how scientists' varying comfort levels with these strategies can be supported by presenting different structure and style options. We found that the side-by-side comparison of options helped writers make editorial decisions. Instead of adhering to one direction of science communication, writers explored a continuum of options which helped them identify which communication strategies they wanted to implement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05287v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Grace Li, Yuanyang Teng, Juna Kawai Yue, Unaisah Ahmed, Anatta S. Tantiwongse, Jessica Y. Liang, Dorothy Zhang, Kynnedy Simone Smith, Tao Long, Mina Lee, Lydia B Chilton</dc:creator>
    </item>
    <item>
      <title>Speejis: Enhancing User Experience of Mobile Voice Messaging with Automatic Visual Speech Emotion Cues</title>
      <link>https://arxiv.org/abs/2502.05296</link>
      <description>arXiv:2502.05296v1 Announce Type: new 
Abstract: Mobile messaging apps offer an increasing range of emotional expressions, such as emojis to help users manually augment their texting experiences. Accessibility of such augmentations is limited in voice messaging. With the term "speejis" we refer to accessible emojis and other visual speech emotion cues that are created automatically from speech input alone. The paper presents an implementation of speejis and reports on a user study (N=12) comparing the UX of voice messaging with and without speejis. Results show significant differences in measures such as attractiveness and stimulation and a clear preference of all participants for messaging with speejis. We highlight the benefits of using paralinguistic speech processing and continuous emotion models to enable finer grained augmentations of emotion changes and transitions within a single message in addition to augmentations of the overall tone of the message.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05296v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilhan Aslan, Carla F. Griggio, Henning Pohl, Timothy Merritt, Niels van Berkel</dc:creator>
    </item>
    <item>
      <title>Supporting Contraceptive Decision-Making in the Intermediated Pharmacy Setting in Kenya</title>
      <link>https://arxiv.org/abs/2502.05323</link>
      <description>arXiv:2502.05323v1 Announce Type: new 
Abstract: Adolescent girls and young women (AGYW) in sub-Saharan Africa face unique barriers to contraceptive access and lack AGYW-centered contraceptive decision-support resources. To empower AGYW to make informed choices and improve reproductive health outcomes, we developed a tablet-based application to provide contraceptive education and decision-making support in the pharmacy setting - a key source of contraceptive services for AGYW - in Kenya. We conducted workshops with AGYW and pharmacy providers in Kenya to gather app feedback and understand how to integrate the intervention into the pharmacy setting. Our analysis highlights how intermediated interactions - a multiuser, cooperative effort to enable technology use and information access - could inform a successful contraceptive intervention in Kenya. The potential strengths of intermediation in our setting inform implications for technological health interventions in intermediated scenarios in \lrem{LMICs}\ladd{low- and middle-income countries}, including challenges and opportunities for extending impact to different populations and integrating technology into resource-constrained healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05323v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713508</arxiv:DOI>
      <dc:creator>Lisa Orii, Elizabeth K Harrington, Serah Gitome, Nelson Kiprotich Cheruiyot, Elizabeth Anne Bukusi, Sandy Cheng, Ariel Fu, Khushi Khandelwal, Shrimayee Narasimhan, Richard Anderson</dc:creator>
    </item>
    <item>
      <title>Atlas of AI Risks: Enhancing Public Understanding of AI Risks</title>
      <link>https://arxiv.org/abs/2502.05324</link>
      <description>arXiv:2502.05324v1 Announce Type: new 
Abstract: The prevailing methodologies for visualizing AI risks have focused on technical issues such as data biases and model inaccuracies, often overlooking broader societal risks like job loss and surveillance. Moreover, these visualizations are typically designed for tech-savvy individuals, neglecting those with limited technical skills. To address these challenges, we propose the Atlas of AI Risks-a narrative-style tool designed to map the broad risks associated with various AI technologies in a way that is understandable to non-technical individuals as well. To both develop and evaluate this tool, we conducted two crowdsourcing studies. The first, involving 40 participants, identified the design requirements for visualizing AI risks for decision-making and guided the development of the Atlas. The second study, with 140 participants reflecting the US population in terms of age, sex, and ethnicity, assessed the usability and aesthetics of the Atlas to ensure it met those requirements. Using facial recognition technology as a case study, we found that the Atlas is more user-friendly than a baseline visualization, with a more classic and expressive aesthetic, and is more effective in presenting a balanced assessment of the risks and benefits of facial recognition. Finally, we discuss how our design choices make the Atlas adaptable for broader use, allowing it to generalize across the diverse range of technology applications represented in a database that reports various AI incidents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05324v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/hcomp.v12i1.31598</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 12(1), 33-43, 2024</arxiv:journal_reference>
      <dc:creator>Edyta Bogucka, Sanja \v{S}\'cepanovi\'c, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>The Role of Human Creativity in the Presence of AI Creativity Tools at Work: A Case Study on AI-Driven Content Transformation in Journalism</title>
      <link>https://arxiv.org/abs/2502.05347</link>
      <description>arXiv:2502.05347v1 Announce Type: new 
Abstract: As AI becomes more capable, it is unclear how human creativity will remain essential in jobs that incorporate AI. We conducted a 14-week study of a student newsroom using an AI tool to convert web articles into social media videos. Most treated the tool as a creative springboard, yet still had to edit many AI outputs. The tool enabled the team to publish successful content, receiving over 500,000 views. Yet creators sometimes treated AI as an unquestioned expert, accepting flawed suggestions. Editorial critique was essential to spot errors and guide creative solutions when AI failed. We discuss how AI's inherent gaps ensure human creativity remains vital.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05347v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitong Wang, Jocelyn McKinnon-Crowley, Tao Long, Kian Loong Lua, Keren Henderson, Kevin Crowston, Jeffrey V. Nickerson, Mark Hansen, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>Evaluating Cross-Subject and Cross-Device Consistency in Visual Fixation Prediction</title>
      <link>https://arxiv.org/abs/2502.05554</link>
      <description>arXiv:2502.05554v1 Announce Type: new 
Abstract: Understanding cross-subject and cross-device consistency in visual fixation prediction is essential for advancing eye-tracking applications, including visual attention modeling and neuroprosthetics. This study evaluates fixation consistency using an embedded eye tracker integrated into regular-sized glasses, comparing its performance with high-end standalone eye-tracking systems. Nine participants viewed 300 images from the MIT1003 dataset in subjective experiments, allowing us to analyze cross-device and cross-subject variations in fixation patterns with various evaluation metrics. Our findings indicate that average visual fixations can be reliably transferred across devices for relatively simple stimuli. However, individual-to-average consistency remains weak, highlighting the challenges of predicting individual fixations across devices. These results provide an empirical foundation for leveraging predicted average visual fixation data to enhance neuroprosthetic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05554v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuli Wu, Henning Konermann, Emil Mededovic, Peter Walter, Johannes Stegmaier</dc:creator>
    </item>
    <item>
      <title>Rambler in the Wild: A Diary Study of LLM-Assisted Writing With Speech</title>
      <link>https://arxiv.org/abs/2502.05612</link>
      <description>arXiv:2502.05612v1 Announce Type: new 
Abstract: Speech-to-text technologies have been shown to improve text input efficiency and potentially lower the barriers to writing. Recent LLM-assisted dictation tools aim to support writing with speech by bridging the gaps between speaking and traditional writing. This case study reports on the real-world writing experiences of twelve academic or creative writers using one such tool, Rambler, to write various pieces such as blog posts, diaries, screenplays, notes, or fictional stories, etc. Through a ten-day diary study, we identified the participants' in-context writing strategies using Rambler, such as how they expanded from an outline or organized their loose thoughts for different writing goals. The interviews uncovered the psychological and productivity affordances of writing with speech, pointing to future directions of designing for this writing modality and the utilization of AI support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05612v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuyu Yang, Wengxi Li, Matthew G. Lee, Zhuoyang Li, J. D. Zamfirescu-Pereira, Can Liu</dc:creator>
    </item>
    <item>
      <title>Towards AI-driven Sign Language Generation with Non-manual Markers</title>
      <link>https://arxiv.org/abs/2502.05661</link>
      <description>arXiv:2502.05661v1 Announce Type: new 
Abstract: Sign languages are essential for the Deaf and Hard-of-Hearing (DHH) community. Sign language generation systems have the potential to support communication by translating from written languages, such as English, into signed videos. However, current systems often fail to meet user needs due to poor translation of grammatical structures, the absence of facial cues and body language, and insufficient visual and motion fidelity. We address these challenges by building on recent advances in LLMs and video generation models to translate English sentences into natural-looking AI ASL signers. The text component of our model extracts information for manual and non-manual components of ASL, which are used to synthesize skeletal pose sequences and corresponding video frames. Our findings from a user study with 30 DHH participants and thorough technical evaluations demonstrate significant progress and identify critical areas necessary to meet user needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05661v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Han Zhang, Rotem Shalev-Arkushin, Vasileios Baltatzis, Connor Gillis, Gierad Laput, Raja Kushalnagar, Lorna Quandt, Leah Findlater, Abdelkareem Bedri, Colin Lea</dc:creator>
    </item>
    <item>
      <title>Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies</title>
      <link>https://arxiv.org/abs/2502.05731</link>
      <description>arXiv:2502.05731v1 Announce Type: new 
Abstract: Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05731v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sam Yu-Te Lee, Cheng-Wei Hung, Mei-Hua Yuan, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>RECOVER: Designing a Large Language Model-based Remote Patient Monitoring System for Postoperative Gastrointestinal Cancer Care</title>
      <link>https://arxiv.org/abs/2502.05740</link>
      <description>arXiv:2502.05740v1 Announce Type: new 
Abstract: Cancer surgery is a key treatment for gastrointestinal (GI) cancers, a group of cancers that account for more than 35% of cancer-related deaths worldwide, but postoperative complications are unpredictable and can be life-threatening. In this paper, we investigate how recent advancements in large language models (LLMs) can benefit remote patient monitoring (RPM) systems through clinical integration by designing RECOVER, an LLM-powered RPM system for postoperative GI cancer care. To closely engage stakeholders in the design process, we first conducted seven participatory design sessions with five clinical staff and interviewed five cancer patients to derive six major design strategies for integrating clinical guidelines and information needs into LLM-based RPM systems. We then designed and implemented RECOVER, which features an LLM-powered conversational agent for cancer patients and an interactive dashboard for clinical staff to enable efficient postoperative RPM. Finally, we used RECOVER as a pilot system to assess the implementation of our design strategies with four clinical staff and five patients, providing design implications by identifying crucial design elements, offering insights on responsible AI, and outlining opportunities for future LLM-powered RPM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05740v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziqi Yang, Yuxuan Lu, Jennifer Bagdasarian, Vedant Das Swain, Ritu Agarwal, Collin Campbell, Waddah Al-Refaire, Jehan El-Bayoumi, Guodong Gao, Dakuo Wang, Bingsheng Yao, Nawar Shara</dc:creator>
    </item>
    <item>
      <title>Understanding the Practices, Perceptions, and (Dis)Trust of Generative AI among Instructors: A Mixed-methods Study in the U.S. Higher Education</title>
      <link>https://arxiv.org/abs/2502.05770</link>
      <description>arXiv:2502.05770v1 Announce Type: new 
Abstract: Generative AI (GenAI) has brought opportunities and challenges for higher education as it integrates into teaching and learning environments. As instructors navigate this new landscape, understanding their engagement with and attitudes toward GenAI is crucial. We surveyed 178 instructors from a single U.S. university to examine their current practices, perceptions, trust, and distrust of GenAI in higher education in March 2024. While most surveyed instructors reported moderate to high familiarity with GenAI-related concepts, their actual use of GenAI tools for direct instructional tasks remained limited. Our quantitative results show that trust and distrust in GenAI are related yet distinct; high trust does not necessarily imply low distrust, and vice versa. We also found significant differences in surveyed instructors' familiarity with GenAI across different trust and distrust groups. Our qualitative results show nuanced manifestations of trust and distrust among surveyed instructors and various approaches to support calibrated trust in GenAI. We discuss practical implications focused on (dis)trust calibration among instructors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05770v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhan Lyu (Rachel), Shuang Zhang (Rachel),  Tingting (Rachel),  Chung, Yifan Sun, Yixuan Zhang</dc:creator>
    </item>
    <item>
      <title>WatchGuardian: Enabling User-Defined Personalized Just-in-Time Intervention on Smartwatch</title>
      <link>https://arxiv.org/abs/2502.05783</link>
      <description>arXiv:2502.05783v1 Announce Type: new 
Abstract: While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. For the model to detect new actions based on limited new data samples, we developed a few-shot learning pipeline that finetuned a pre-trained inertial measurement unit (IMU) model on public hand-gesture datasets. We then designed a data augmentation and synthesis process to train additional classification layers for customization. Our offline evaluation with 26 participants showed that with three, five, and ten examples, our approach achieved an average accuracy of 76.8%, 84.7%, and 87.7%, and an F1 score of 74.8%, 84.2%, and 87.2% We then conducted a four-hour intervention study to compare WatchGuardian against a rule-based intervention. Our results demonstrated that our system led to a significant reduction by 64.0 +- 22.6% in undesirable actions, substantially outperforming the baseline by 29.0%. Our findings underscore the effectiveness of a customizable, AI-driven JITI system for individuals in need of behavioral intervention in personal undesirable actions. We envision that our work can inspire broader applications of user-defined personalized intervention with advanced AI solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05783v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Lei, Yancheng Cao, Will Wang, Yuanzhe Dong, Changchang Yin, Weidan Cao, Ping Zhang, Jingzhen Yang, Bingsheng Yao, Yifan Peng, Chunhua Weng, Randy Auerbach, Lena Mamykina, Dakuo Wang, Yuntao Wang, Xuhai Xu</dc:creator>
    </item>
    <item>
      <title>Seamless Integration: The Evolution, Design and Future Impact of Wearable Technology</title>
      <link>https://arxiv.org/abs/2502.05797</link>
      <description>arXiv:2502.05797v1 Announce Type: new 
Abstract: The rapid evolution of wearable technology marks a transformative phase in human-computer interaction, seamlessly integrating digital functionality into daily life. This paper explores the historical trajectory, current advancements, and future potential of wearables, emphasizing their impact on healthcare, productivity, and personal well-being. Key developments include the integration of artificial intelligence (AI), Internet of Things (IoT), and augmented reality (AR), driving personalization, real-time adaptability, and enhanced user experiences. The study highlights user-centered design principles, ethical considerations, and interdisciplinary collaboration as critical factors in creating wearables that are intuitive, inclusive, and secure. Furthermore, the paper examines sustainability trends, such as modular designs and eco-friendly materials, aligning innovation with environmental responsibility. By addressing challenges like data privacy, algorithmic bias, and usability, wearable technology is poised to redefine the interaction between humans and technology, offering unprecedented opportunities for enrichment and empowerment in diverse contexts. This comprehensive analysis provides a roadmap for advancing wearables to meet emerging societal needs while fostering ethical and sustainable growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05797v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Pearl, James Intriligator, William Liu</dc:creator>
    </item>
    <item>
      <title>Understanding Design Fixation in Generative AI</title>
      <link>https://arxiv.org/abs/2502.05870</link>
      <description>arXiv:2502.05870v1 Announce Type: new 
Abstract: Generative AI (GenAI) provides new opportunities for creativity support, but the phenomenon of GenAI design fixation remains underexplored. While human design fixation typically constrains ideas to familiar or existing solutions, our findings reveal that GenAI similarly experience design fixation, limiting its ability to generate novel and diverse design outcomes. To advance understanding of GenAI design fixation, we propose a theoretical framework includes the definition, causes, manifestations, and impacts of GenAI design fixation for creative design. We also conducted an experimental study to investigate the characteristics of GenAI design fixation in practice. We summarize how GenAI design fixation manifests in text generation model and image generation model respectively. Furthermore, we propose methods for mitigating GenAI design fixation for future creativity support tool design. We recommend adopting the lens of GenAI design fixation for creativity-oriented HCI research, as the unique perspectives and insights it provides.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05870v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liuqing Chen, Yaxuan Song, Chunyuan Zheng, Qianzhi Jing, Preben Hansen, Lingyun Sun</dc:creator>
    </item>
    <item>
      <title>Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction</title>
      <link>https://arxiv.org/abs/2502.05935</link>
      <description>arXiv:2502.05935v1 Announce Type: new 
Abstract: Neuromorphic HCI is a new theoretical approach to designing better UX inspired by the neurophysiology of the brain. Here, we apply the neuroscientific theory of Active Inference to HCI, postulating that users perform Bayesian inference on progress and goal distributions to predict their next action (Interactive Inference). We show how Bayesian surprise between goal and progress distributions follows a mean square error function of the signal-to-noise ratio (SNR) of the task. However, capacity to process Bayesian surprise follows the logarithm of SNR, and errors occur when average capacity is exceeded. Our model allows the quantitative analysis of performance and error in one framework with real-time estimation of mental load. We show through mathematical theorems how three basic laws of HCI, Hick's Law, Fitts' Law and the Power Law fit our model. We then test the validity of the general model by empirically measuring how well it predicts human performance in a car following task. Results suggest that driver processing capacity indeed is a logarithmic function of the SNR of the distance to a lead car. This positive result provides initial evidence that Interactive Interference can work as a new theoretical underpinning for HCI, deserving further exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05935v1</guid>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roel Vertegaal, Timothy Merritt, Saul Greenberg, Aneesh P. Tarun, Zhen Li, Zafeiros Fountas</dc:creator>
    </item>
    <item>
      <title>Cyri: A Conversational AI-based Assistant for Supporting the Human User in Detecting and Responding to Phishing Attacks</title>
      <link>https://arxiv.org/abs/2502.05951</link>
      <description>arXiv:2502.05951v1 Announce Type: new 
Abstract: This work introduces Cyri, an AI-powered conversational assistant designed to support a human user in detecting and analyzing phishing emails by leveraging Large Language Models. Cyri has been designed to scrutinize emails for semantic features used in phishing attacks, such as urgency, and undesirable consequences, using an approach that unifies features already established in the literature with others by Cyri features extraction methodology. Cyri can be directly plugged into a client mail or webmail, ensuring seamless integration with the user's email workflow while maintaining data privacy through local processing. By performing analyses on the user's machine, Cyri eliminates the need to transmit sensitive email data over the internet, reducing associated security risks. The Cyri user interface has been designed to reduce habituation effects and enhance user engagement. It employs dynamic visual cues and context-specific explanations to keep users alert and informed while using emails. Additionally, it allows users to explore identified malicious semantic features both through conversation with the agent and visual exploration, obtaining the advantages of both modalities for expert or non-expert users. It also allows users to keep track of the conversation, supports the user in solving additional questions on both computed features or new parts of the mail, and applies its detection on demand. To evaluate Cyri, we crafted a comprehensive dataset of 420 phishing emails and 420 legitimate emails. Results demonstrate high effectiveness in identifying critical phishing semantic features fundamental to phishing detection. A user study involving 10 participants, both experts and non-experts, evaluated Cyri's effectiveness and usability. Results indicated that Cyri significantly aided users in identifying phishing emails and enhanced their understanding of phishing tactics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05951v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antonio La Torre, Marco Angelini</dc:creator>
    </item>
    <item>
      <title>Pencils to Pixels: A Systematic Study of Creative Drawings across Children, Adults and AI</title>
      <link>https://arxiv.org/abs/2502.05999</link>
      <description>arXiv:2502.05999v1 Announce Type: new 
Abstract: Can we derive computational metrics to quantify visual creativity in drawings across intelligent agents, while accounting for inherent differences in technical skill and style? To answer this, we curate a novel dataset consisting of 1338 drawings by children, adults and AI on a creative drawing task. We characterize two aspects of the drawings -- (1) style and (2) content. For style, we define measures of ink density, ink distribution and number of elements. For content, we use expert-annotated categories to study conceptual diversity, and image and text embeddings to compute distance measures. We compare the style, content and creativity of children, adults and AI drawings and build simple models to predict expert and automated creativity scores. We find significant differences in style and content in the groups -- children's drawings had more components, AI drawings had greater ink density, and adult drawings revealed maximum conceptual diversity. Notably, we highlight a misalignment between creativity judgments obtained through expert and automated ratings and discuss its implications. Through these efforts, our work provides, to the best of our knowledge, the first framework for studying human and artificial creativity beyond the textual modality, and attempts to arrive at the domain-agnostic principles underlying creativity. Our data and scripts are available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05999v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Surabhi S Nath, Guiomar del Cuvillo y Schr\"oder, Claire E. Stevenson</dc:creator>
    </item>
    <item>
      <title>Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage</title>
      <link>https://arxiv.org/abs/2502.06009</link>
      <description>arXiv:2502.06009v1 Announce Type: new 
Abstract: Mainstream media, through their decisions on what to cover and how to frame the stories they cover, can mislead readers without using outright falsehoods. Therefore, it is crucial to have tools that expose these editorial choices underlying media bias. In this paper, we introduce the Media Bias Detector, a tool for researchers, journalists, and news consumers. By integrating large language models, we provide near real-time granular insights into the topics, tone, political lean, and facts of news articles aggregated to the publisher level. We assessed the tool's impact by interviewing 13 experts from journalism, communications, and political science, revealing key insights into usability and functionality, practical applications, and AI's role in powering media bias tools. We explored this in more depth with a follow-up survey of 150 news consumers. This work highlights opportunities for AI-driven tools that empower users to critically engage with media content, particularly in politically charged environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06009v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713716</arxiv:DOI>
      <dc:creator>Jenny S Wang, Samar Haider, Amir Tohidi, Anushkaa Gupta, Yuxuan Zhang, Chris Callison-Burch, David Rothschild, Duncan J Watts</dc:creator>
    </item>
    <item>
      <title>Living Bento: Heartbeat-Driven Noodles for Enriched Dining Dynamics</title>
      <link>https://arxiv.org/abs/2502.06069</link>
      <description>arXiv:2502.06069v1 Announce Type: new 
Abstract: To enhance focused eating and dining socialization, previous Human-Food Interaction research has indicated that external devices can support these dining objectives and immersion. However, methods that focus on the food itself and the diners themselves have remained underdeveloped. In this study, we integrated biofeedback with food, utilizing diners' heart rates as a source of the food's appearance to promote focused eating and dining socialization. By employing LED lights, we dynamically displayed diners' real-time physiological signals through the transparency of the food. Results revealed significant effects on various aspects of dining immersion, such as awareness perceptions, attractiveness, attentiveness to each bite, and emotional bonds with the food. Furthermore, to promote dining socialization, we established a "Sharing Bio-Sync Food" dining system to strengthen emotional connections between diners. Based on these findings, we developed tableware that integrates biofeedback into the culinary experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06069v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713108</arxiv:DOI>
      <dc:creator>Weijen Chen, Qingyuan Gao, Zheng Hu, Kouta Minamizawa, Yun Suen Pai</dc:creator>
    </item>
    <item>
      <title>Deconstructing Depression Stigma: Integrating AI-driven Data Collection and Analysis with Causal Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2502.06075</link>
      <description>arXiv:2502.06075v1 Announce Type: new 
Abstract: Mental-illness stigma is a persistent social problem, hampering both treatment-seeking and recovery. Accordingly, there is a pressing need to understand it more clearly, but analyzing the relevant data is highly labor-intensive. Therefore, we designed a chatbot to engage participants in conversations; coded those conversations qualitatively with AI assistance; and, based on those coding results, built causal knowledge graphs to decode stigma. The results we obtained from 1,002 participants demonstrate that conversation with our chatbot can elicit rich information about people's attitudes toward depression, while our AI-assisted coding was strongly consistent with human-expert coding. Our novel approach combining large language models (LLMs) and causal knowledge graphs uncovered patterns in individual responses and illustrated the interrelationships of psychological constructs in the dataset as a whole. The paper also discusses these findings' implications for HCI researchers in developing digital interventions, decomposing human psychological constructs, and fostering inclusive attitudes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06075v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Meng, Renwen Zhang, Ganyi Wang, Yitian Yang, Peinuan Qin, Jungup Lee, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Actual Achieved Gain and Optimal Perceived Gain: Modeling Human Take-over Decisions Towards Automated Vehicles' Suggestions</title>
      <link>https://arxiv.org/abs/2502.06179</link>
      <description>arXiv:2502.06179v1 Announce Type: new 
Abstract: Driver decision quality in take-overs is critical for effective human-Autonomous Driving System (ADS) collaboration. However, current research lacks detailed analysis of its variations. This paper introduces two metrics--Actual Achieved Gain (AAG) and Optimal Perceived Gain (OPG)--to assess decision quality, with OPG representing optimal decisions and AAG reflecting actual outcomes. Both are calculated as weighted averages of perceived gains and losses, influenced by ADS accuracy. Study 1 (N=315) used a 21-point Thurstone scale to measure perceived gains and losses-key components of AAG and OPG-across typical tasks: route selection, overtaking, and collision avoidance. Studies 2 (N=54) and 3 (N=54) modeled decision quality under varying ADS accuracy and decision time. Results show with sufficient time (&gt;3.5s), AAG converges towards OPG, indicating rational decision-making, while limited time leads to intuitive and deterministic choices. Study 3 also linked AAG-OPG deviations to irrational behaviors. An intervention study (N=8) and a pilot (N=4) employing voice alarms and multi-modal alarms based on these deviations demonstrated AAG's potential to improve decision quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06179v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuning Zhang, Xin Yi, Shixuan Li, Chuye Hong, Gujun Chen, Jiarui Liu, Xueyang Wang, Yongquan Hu, Yuntao Wang, Hewu Li</dc:creator>
    </item>
    <item>
      <title>Wearable AR in Everyday Contexts: Insights from a Digital Ethnography of YouTube Videos</title>
      <link>https://arxiv.org/abs/2502.06191</link>
      <description>arXiv:2502.06191v1 Announce Type: new 
Abstract: With growing investment in consumer augmented reality (AR) headsets and glasses, wearable AR is moving from niche applications to everyday use. However, current research primarily examines AR in controlled settings, offering limited insights into its use in real-world daily life. To address this gap, we adopt a digital ethnographic approach, analysing 27 hours of 112 YouTube videos featuring early adopters. These videos capture usage ranging from continuous periods of hours to intermittent use over weeks and months. Our analysis shows that currently, wearable AR is primarily used for media consumption and gaming. While productivity is a desired use case, frequent use is constrained by current hardware limitations and the nascent application ecosystem. Users seek continuity in their digital experience, desiring functionalities similar to those on smartphones, tablets, or computers. We propose implications for everyday AR development that promote adoption while ensuring safe, ethical, and socially-aware integration into daily life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06191v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713572</arxiv:DOI>
      <dc:creator>Tram Thi Minh Tran, Shane Brown, Oliver Weidlich, Soojeong Yoo, Callum Parker</dc:creator>
    </item>
    <item>
      <title>Timing Matters: How Using LLMs at Different Timings Influences Writers' Perceptions and Ideation Outcomes in AI-Assisted Ideation</title>
      <link>https://arxiv.org/abs/2502.06197</link>
      <description>arXiv:2502.06197v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been widely used to support ideation in the writing process. However, whether generating ideas with the help of LLMs leads to idea fixation or idea expansion is unclear. This study examines how different timings of LLM usage - either at the beginning or after independent ideation - affect people's perceptions and ideation outcomes in a writing task. In a controlled experiment with 60 participants, we found that using LLMs from the beginning reduced the number of original ideas and lowered creative self-efficacy and self-credit, mediated by changes in autonomy and ownership. We discuss the challenges and opportunities associated with using LLMs to assist in idea generation. We propose delaying the use of LLMs to support ideation while considering users' self-efficacy, autonomy, and ownership of the ideation outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06197v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peinuan Qin, Chi-Lan Yang, Jingshu Li, Jing Wen, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Amplifying Minority Voices: AI-Mediated Devil's Advocate System for Inclusive Group Decision-Making</title>
      <link>https://arxiv.org/abs/2502.06251</link>
      <description>arXiv:2502.06251v1 Announce Type: new 
Abstract: Group decision-making often benefits from diverse perspectives, yet power imbalances and social influence can stifle minority opinions and compromise outcomes. This prequel introduces an AI-mediated communication system that leverages the Large Language Model to serve as a devil's advocate, representing underrepresented viewpoints without exposing minority members' identities. Rooted in persuasive communication strategies and anonymity, the system aims to improve psychological safety and foster more inclusive decision-making. Our multi-agent architecture, which consists of a summary agent, conversation agent, AI duplicate checker, and paraphrase agent, encourages the group's critical thinking while reducing repetitive outputs. We acknowledge that reliance on text-based communication and fixed intervention timings may limit adaptability, indicating pathways for refinement. By focusing on the representation of minority viewpoints anonymously in power-imbalanced settings, this approach highlights how AI-driven methods can evolve to support more divergent and inclusive group decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06251v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708557.3716334</arxiv:DOI>
      <dc:creator>Soohwan Lee, Mingyu Kim, Seoyeong Hwang, Dajung Kim, Kyungho Lee</dc:creator>
    </item>
    <item>
      <title>Acceptance of an Augmented Society: Initial Explorations into the Acceptability of Augmenting Real World Locations</title>
      <link>https://arxiv.org/abs/2502.06378</link>
      <description>arXiv:2502.06378v1 Announce Type: new 
Abstract: Augmented reality (AR) will enable individuals to share and experience content augmented at real world locations with ease. But what protections and restrictions should be in place? Should, for example, anyone be able to post any content they wish at a place of religious or cultural significance? We developed a smartphone app to give individuals hands-on experience posting and sharing AR content. After using our app, we investigated their attitudes towards posting different types of AR content (of an artistic, protest, social, informative, and commercial nature) in a variety of locations (cultural sites, religious sites, residential areas, public spaces, government buildings, and tourist points of interests). Our results show individuals expect restrictions to be in place to control who can post AR content at some locations, in particular those of religious and cultural significance. We also report individuals prefer augmentations to fit contextually within the environment they are posted, and expect the posting and sharing of AR content to adhere to the same regulations/legislation as social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06378v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Joy, Joseph O'Hagan</dc:creator>
    </item>
    <item>
      <title>Comparing Pass-Through Quality of Mixed Reality Devices: A User Experience Study During Real-World Tasks</title>
      <link>https://arxiv.org/abs/2502.06382</link>
      <description>arXiv:2502.06382v1 Announce Type: new 
Abstract: In extended reality, pass-through enables users to view their real-world surroundings via cameras on the headset, displaying live video inside the device. This study compared the pass-through quality of three devices: Apple Vision Pro, Meta Quest 3, and Varjo XR3. Thirtyone participants performed two tasks, reading a text and solving a puzzle, while using each headset with the pass-through feature activated. Participants then rated their experiences, focusing on workload and cybersickness. Results showed that the Apple Vision Pro outperformed the Meta Quest 3 and Varjo XR3, receiving the highest ratings for pass-through quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06382v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Vona, Julia Schorlemmer, Michael Stern, Navid Ashrafi, Maurizio Vergari, Tanja Kojic, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>Content-Driven Local Response: Supporting Sentence-Level and Message-Level Mobile Email Replies With and Without AI</title>
      <link>https://arxiv.org/abs/2502.06430</link>
      <description>arXiv:2502.06430v1 Announce Type: new 
Abstract: Mobile emailing demands efficiency in diverse situations, which motivates the use of AI. However, generated text does not always reflect how people want to respond. This challenges users with AI involvement tradeoffs not yet considered in email UIs. We address this with a new UI concept called Content-Driven Local Response (CDLR), inspired by microtasking. This allows users to insert responses into the email by selecting sentences, which additionally serves to guide AI suggestions. The concept supports combining AI for local suggestions and message-level improvements. Our user study (N=126) compared CDLR with manual typing and full reply generation. We found that CDLR supports flexible workflows with varying degrees of AI involvement, while retaining the benefits of reduced typing and errors. This work contributes a new approach to integrating AI capabilities: By redesigning the UI for workflows with and without AI, we can empower users to dynamically adjust AI involvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06430v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713890</arxiv:DOI>
      <dc:creator>Tim Zindulka, Sven Goller, Florian Lehmann, Daniel Buschek</dc:creator>
    </item>
    <item>
      <title>Social Media Isn't Just Instagram: A Youth-Envisioned Platform for Meaningful Social Connections</title>
      <link>https://arxiv.org/abs/2502.06696</link>
      <description>arXiv:2502.06696v1 Announce Type: new 
Abstract: We conducted co-design workshops with 23 participants (ages 15-24) to explore how youth envision an ideal remote social connection. Using the Fictional Inquiry (FI) method within a Harry Potter-inspired narrative, we found that youth perceive a disconnect between platforms labeled as "social media" (like Instagram) and those where they actually experience meaningful connections (like Minecraft or Discord). Participants envisioned an immersive 3D platform that would bridge this gap by prioritizing meaningful social connections, enabling time that feels well spent through presence and immersion, natural individual expression, intuitive social navigation that leverages physical-world norms, and playful, low-stakes opportunities for gradual friendship development. We introduce the design framework of spatial integrity, which encompasses four dimensions of spatial affordances that facilitate meaningful social connections online. The FI method proved effective in generating innovative ideas while empowering youth by fostering a sense of hope and agency over the future of social media through their own design contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06696v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JaeWon Kim, Hyunsung Cho, Fannie Liu, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>The Odyssey of the Fittest: Can Agents Survive and Still Be Good?</title>
      <link>https://arxiv.org/abs/2502.05442</link>
      <description>arXiv:2502.05442v1 Announce Type: cross 
Abstract: As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This paper examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent play a simulated, LLM generated text based adventure game. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent's decisions, uncovering the tradeoffs they navigate to survive. Specifically, analysis finds that when danger increases, agents ignore ethical considerations and opt for unethical behavior. The agents' collective behavior, trading ethics for survival, suggests that prioritizing survival increases the risk of unethical behavior. In the context of AGI, designing agents to prioritize survival may amplify the likelihood of unethical decision making and unintended emergent behaviors, raising fundamental questions about goal design in AI safety research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05442v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dylan Waldner, Risto Miikkulainen</dc:creator>
    </item>
    <item>
      <title>Implicit Communication of Contextual Information in Human-Robot Collaboration</title>
      <link>https://arxiv.org/abs/2502.05775</link>
      <description>arXiv:2502.05775v1 Announce Type: cross 
Abstract: Implicit communication is crucial in human-robot collaboration (HRC), where contextual information, such as intentions, is conveyed as implicatures, forming a natural part of human interaction. However, enabling robots to appropriately use implicit communication in cooperative tasks remains challenging. My research addresses this through three phases: first, exploring the impact of linguistic implicatures on collaborative tasks; second, examining how robots' implicit cues for backchanneling and proactive communication affect team performance and perception, and how they should adapt to human teammates; and finally, designing and evaluating a multi-LLM robotics system that learns from human implicit communication. This research aims to enhance the natural communication abilities of robots and facilitate their integration into daily collaborative activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05775v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Mul2MAR: A Multi-Marker Mobile Augmented Reality Application for Improved Visual Perception</title>
      <link>https://arxiv.org/abs/2502.05953</link>
      <description>arXiv:2502.05953v1 Announce Type: cross 
Abstract: This paper presents an inexpensive Augmented Reality (AR) application which is aimed to use with mobile devices. Our application is a marker based AR application, and it can be used by inexpensive three dimensional (3D) red-cyan glasses. In our AR application, we combine left and right views without creating any uncomfortable situation for human eyes. We validate our mobile AR application on several objects, scenes, and views. We show that 3D AR perception can be obtained by using our inexpensive AR application [G\"ung\"or and Kurt 2014].</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05953v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murat Kurt</dc:creator>
    </item>
    <item>
      <title>Make the Fastest Faster: Importance Mask for Interactive Volume Visualization using Reconstruction Neural Networks</title>
      <link>https://arxiv.org/abs/2502.06053</link>
      <description>arXiv:2502.06053v1 Announce Type: cross 
Abstract: Visualizing a large-scale volumetric dataset with high resolution is challenging due to the high computational time and space complexity. Recent deep-learning-based image inpainting methods significantly improve rendering latency by reconstructing a high-resolution image for visualization in constant time on GPU from a partially rendered image where only a small portion of pixels go through the expensive rendering pipeline. However, existing methods need to render every pixel of a predefined regular sampling pattern. In this work, we provide Importance Mask Learning (IML) and Synthesis (IMS) networks which are the first attempts to learn importance regions from the sampling pattern to further minimize the number of pixels to render by jointly considering the dataset, user's view parameters, and the downstream reconstruction neural networks. Our solution is a unified framework to handle various image inpainting-based visualization methods through the proposed differentiable compaction/decompaction layers. Experiments show our method can further improve the overall rendering latency of state-of-the-art volume visualization methods using reconstruction neural network for free when rendering scientific volumetric datasets. Our method can also directly optimize the off-the-shelf pre-trained reconstruction neural networks without elongated retraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06053v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka</dc:creator>
    </item>
    <item>
      <title>Mixed Reality Outperforms Virtual Reality for Remote Error Resolution in Pick-and-Place Tasks</title>
      <link>https://arxiv.org/abs/2502.06141</link>
      <description>arXiv:2502.06141v1 Announce Type: cross 
Abstract: This study evaluates the performance and usability of Mixed Reality (MR), Virtual Reality (VR), and camera stream interfaces for remote error resolution tasks, such as correcting warehouse packaging errors. Specifically, we consider a scenario where a robotic arm halts after detecting an error, requiring a remote operator to intervene and resolve it via pick-and-place actions. Twenty-one participants performed simulated pick-and-place tasks using each interface. A linear mixed model (LMM) analysis of task resolution time, usability scores (SUS), and mental workload scores (NASA-TLX) showed that the MR interface outperformed both VR and camera interfaces. MR enabled significantly faster task completion, was rated higher in usability, and was perceived to be less cognitively demanding. Notably, the MR interface, which projected a virtual robot onto a physical table, provided superior spatial understanding and physical reference cues. Post-study surveys further confirmed participants' preference for MR over other interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06141v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Advay Kumar, Stephanie Simangunsong, Pamela Carreno-Medrano, Akansel Cosgun</dc:creator>
    </item>
    <item>
      <title>Facial Analysis Systems and Down Syndrome</title>
      <link>https://arxiv.org/abs/2502.06341</link>
      <description>arXiv:2502.06341v1 Announce Type: cross 
Abstract: The ethical, social and legal issues surrounding facial analysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimination, particularly against marginalized groups. We contribute to this field of research by reporting on the limitations of facial analysis systems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far. This study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling. The results show an overall lower accuracy of prediction in the experimental group, and other specific patterns of performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome were more often incorrectly labelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with women, and labels related to education level and skills more often associated with men. These results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syndrome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06341v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-74630-7_10</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning and Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2023. Communications in Computer and Information Science, vol 2133. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Marco Rondina, Fabiana Vinci, Antonio Vetr\`o, Juan Carlos De Martin</dc:creator>
    </item>
    <item>
      <title>Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain</title>
      <link>https://arxiv.org/abs/2502.06439</link>
      <description>arXiv:2502.06439v1 Announce Type: cross 
Abstract: Context. As software systems become more integrated into society's infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety, privacy, and, increasingly, non-discrimination.
  Motivation. Fairness in pricing algorithms grants equitable access to basic services without discriminating on the basis of protected attributes.
  Method. We replicate a previous empirical study that used black box testing to audit pricing algorithms used by Italian car insurance companies, accessible through a popular online system. With respect to the previous study, we enlarged the number of tests and the number of demographic variables under analysis.
  Results. Our work confirms and extends previous findings, highlighting the problematic permanence of discrimination across time: demographic variables significantly impact pricing to this day, with birthplace remaining the main discriminatory factor against individuals not born in Italian cities. We also found that driver profiles can determine the number of quotes available to the user, denying equal opportunities to all.
  Conclusion. The study underscores the importance of testing for non-discrimination in software systems that affect people's everyday lives. Performing algorithmic audits over time makes it possible to evaluate the evolution of such algorithms. It also demonstrates the role that empirical software engineering can play in making software systems more accountable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06439v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Rondina, Antonio Vetr\`o, Riccardo Coppola, Oumaima Regragrui, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Juan Carlos De Martin</dc:creator>
    </item>
    <item>
      <title>People will agree what I think: Investigating LLM's False Consensus Effect</title>
      <link>https://arxiv.org/abs/2407.12007</link>
      <description>arXiv:2407.12007v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been recently adopted in interactive systems requiring communication. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Psychologists especially focus on the False Consensus Effect (FCE), a cognitive bias where individuals overestimate the extent to which others share their beliefs or behaviors, because FCE can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when FCE becomes more or less prevalent compared to normal usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12007v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyuk Choi, Yeseon Hong, Bugeun Kim</dc:creator>
    </item>
    <item>
      <title>DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions with Large Language Models</title>
      <link>https://arxiv.org/abs/2408.02574</link>
      <description>arXiv:2408.02574v4 Announce Type: replace 
Abstract: Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford. Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments. To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows. Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance. Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors. Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions. Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors. Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02574v4</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3710954</arxiv:DOI>
      <dc:creator>Siying Hu, Huanchen Wang, Yu Zhang, Piaohong Wang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing</title>
      <link>https://arxiv.org/abs/2409.09760</link>
      <description>arXiv:2409.09760v2 Announce Type: replace 
Abstract: d/Deaf and hearing song-signers have become prevalent across video-sharing platforms, but translating songs into sign language remains cumbersome and inaccessible. Our formative study revealed the challenges song-signers face, including semantic, syntactic, expressive, and rhythmic considerations in translations. We present ELMI, an accessible song-signing tool that assists in translating lyrics into sign language. ELMI enables users to edit glosses line-by-line, with real-time synced lyric and music video snippets. Users can also chat with a large language model-driven AI to discuss meaning, glossing, emoting, and timing. Through an exploratory study with 13 song-signers, we examined how ELMI facilitates their workflows and how song-signers leverage and receive an LLM-driven chat for translation. Participants successfully adopted ELMI to song-signing, with active discussions throughout. They also reported improved confidence and independence in their translations, finding ELMI encouraging, constructive, and informative. We discuss research and design implications for accessible and culturally sensitive song-signing translation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09760v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Suhyeon Yoo, Khai N. Truong, Young-Ho Kim</dc:creator>
    </item>
    <item>
      <title>ChainBuddy: An AI Agent System for Generating LLM Pipelines</title>
      <link>https://arxiv.org/abs/2409.13588</link>
      <description>arXiv:2409.13588v2 Announce Type: replace 
Abstract: As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-defined tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the "blank page problem." ChainBuddy, an AI workflow generation assistant built into the ChainForge platform, aims to tackle this issue. From a single prompt or chat, ChainBuddy generates a starter evaluative LLM pipeline in ChainForge aligned to the user's requirements. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior and make the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants reported a less demanding workload, felt more confident, and produced higher quality pipelines evaluating LLM behavior. However, we also uncover a mismatch between subjective and objective ratings of performance: participants rated their successfulness similarly across conditions, while independent experts rated participant workflows significantly higher with AI assistance. Drawing connections to the Dunning-Kruger effect, we draw design implications for the future of workflow generation assistants to mitigate the risk of over-reliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13588v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714085</arxiv:DOI>
      <dc:creator>Jingyue Zhang, Ian Arawjo</dc:creator>
    </item>
    <item>
      <title>DynEx: Dynamic Code Synthesis with Structured Design Exploration for Accelerated Exploratory Programming</title>
      <link>https://arxiv.org/abs/2410.00400</link>
      <description>arXiv:2410.00400v3 Announce Type: replace 
Abstract: Recent advancements in large language models have significantly expedited the process of generating front-end code. This allows users to rapidly prototype user interfaces and ideate through code, a process known as exploratory programming. However, existing LLM code generation tools focus more on technical implementation details rather than finding the right design given a particular problem. We present DynEx, an LLM-based method for design exploration in accelerated exploratory programming. DynEx introduces a technique to explore the design space through a structured Design Matrix before creating the prototype with a modular, stepwise approach to LLM code generation. Code is generated sequentially, and users can test and approve each step before moving onto the next. A user study of 10 experts found that DynEx increased design exploration and enabled the creation of more complex and varied prototypes compared to a Claude Artifact baseline. We conclude with a discussion of the implications of design exploration for exploratory programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00400v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Ma, Karthik Sreedhar, Vivian Liu, Pedro Alejandro Perez, Sitong Wang, Riya Sahni, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>JumpStarter: Getting Started on Personal Goals with Adaptive Personal Context Curation</title>
      <link>https://arxiv.org/abs/2410.03882</link>
      <description>arXiv:2410.03882v3 Announce Type: replace 
Abstract: Everyone aspires to achieve personal goals. However, getting started is often complex and daunting, especially for large projects. AI has the potential to create plans and help jumpstart progress, but it often lacks sufficient personal context to be useful. Personal context includes background information, preferences, and resources that can help tailor AI responses to individuals and their goals. We introduce JumpStarter, an AI-powered system that creates action plans and drafts by adaptively soliciting and integrating personal context. JumpStarter assists users by posing questions to elicit relevant context, breaking down goals into manageable steps based on context, and identifying relevant context to draft working solutions. A technical evaluation indicates that context curation results in plans and working solutions of higher quality. A user study demonstrates that compared to ChatGPT, JumpStarter significantly reduces mental load and increases efficiency for users. We discuss the design implications of adaptive personal context curation to better support users in complex problem solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03882v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitong Wang, Xuanming Zhang, Jenny Ma, Alyssa Hwang, Zhou Yu, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>The Impact of Surface Co-location and Eye-tracking on Mixed Reality Typing</title>
      <link>https://arxiv.org/abs/2410.04177</link>
      <description>arXiv:2410.04177v2 Announce Type: replace 
Abstract: Accuracy and speed are pivotal when typing. We hypothesized that the lack of tactile feedback on midair mixed reality keyboards may adversely impact performance. Our first experiment investigated providing tactile feedback to users typing in mixed reality by co-locating the virtual keyboard on a table or a wall. The keyboard was deterministic (without auto-correct), allowed mixed case typing with symbols, and relied only on the hand-tracking provided by a commodity headset's egocentric cameras. Users preferred and had the highest entry rate of 12 words-per-minute using a midair keyboard. Error rates were similar in all conditions. Based on user feedback, our second experiment explored ten-finger typing. We used a novel eye-tracking technique to mitigate accidental key presses. This technique was preferred for ten-finger typing and halved the number of times backspace was pressed. However, participants were faster using only their index fingers without eye-tracking at 11 words-per-minute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04177v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cecilia Schmitz, Joshua Reynolds, Scott Kuhl, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Mining Hierarchies with Conviction: Constructing the CS1 Skill Hierarchy with Pairwise Comparisons over Skill Distributions</title>
      <link>https://arxiv.org/abs/2410.12967</link>
      <description>arXiv:2410.12967v2 Announce Type: replace 
Abstract: Background and Context: Some skills taught in introductory programming courses are categorized into 1) explaining code, 2) arranging lines of code in correct sequence, 3) tracing through the execution of a program, and 4) writing code from scratch. Objective: Knowing if a programming skill is a prerequisite to another would benefit teachers in properly planning the course and structuring the order in which they present activities relating to new content. Prior attempts to establish a skill hierarchy have suffered from methodological issues. Method: In this study, we used the conviction measure from association rule mining to perform pair-wise comparisons of five skills: Write, Trace, Reverse trace, Sequence, and Explain code. We used the data from four exams with more than 600 participants where students solved programming assignments of different skills for several programming topics. Findings: Our findings matched the previous finding that tracing is a prerequisite for students to learn to write code. Contradicting the previous claims, our analysis showed that using the mean threshold writing code is a prerequisite to explaining code. However, there is no clear relationship when we change the threshold to the median. Unlike prior work, we did not find a clear prerequisite relationship between sequencing code and writing or explaining code. Implications: Our research can help instructors by systematically arranging the skills students exercise when encountering a new topic. The goal is to help instructors properly teach and assess programming in a fashion most effective for learning by leveraging the relationship between skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12967v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dip Kiran Pradhan Newar, Max Fowler, David H. Smith IV, Seth Poulsen</dc:creator>
    </item>
    <item>
      <title>Orbit: A Framework for Designing and Evaluating Multi-objective Rankers</title>
      <link>https://arxiv.org/abs/2411.04798</link>
      <description>arXiv:2411.04798v2 Announce Type: replace 
Abstract: Machine learning in production needs to balance multiple objectives: This is particularly evident in ranking or recommendation models, where conflicting objectives such as user engagement, satisfaction, diversity, and novelty must be considered at the same time. However, designing multi-objective rankers is inherently a dynamic wicked problem -- there is no single optimal solution, and the needs evolve over time. Effective design requires collaboration between cross-functional teams and careful analysis of a wide range of information. In this work, we introduce Orbit, a conceptual framework for Objective-centric Ranker Building and Iteration. The framework places objectives at the center of the design process, to serve as boundary objects for communication and guide practitioners for design and evaluation. We implement Orbit as an interactive system, which enables stakeholders to interact with objective spaces directly and supports real-time exploration and evaluation of design trade-offs. We evaluate Orbit through a user study involving twelve industry practitioners, showing that it supports efficient design space exploration, leads to more informed decision-making, and enhances awareness of the inherent trade-offs of multiple objectives. Orbit (1) opens up new opportunities of an objective-centric design process for any multi-objective ML models, as well as (2) sheds light on future designs that push practitioners to go beyond a narrow metric-centric or example-centric mindset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04798v2</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyang Yang, Tesi Xiao, Michael Shavlovsky, Christian K\"astner, Tongshuang Wu</dc:creator>
    </item>
    <item>
      <title>SimulataR: Rapid Assisted Reality Prototyping using Design-Blended Videos</title>
      <link>https://arxiv.org/abs/2501.16515</link>
      <description>arXiv:2501.16515v2 Announce Type: replace 
Abstract: Assisted Reality (aR) is a subfield of Augmented Reality (AR) that overlays information onto a user's immediate view via see-through head-mounted displays (OST-HMDs). This technology has proven to be effective and energy-efficient to support the user and information interaction for everyday wearable intelligent systems. The aR viewing experience, however, is affected by varying real-world backgrounds, lighting, and user movements, which makes designing for aR challenging. Designers have to test their designs in-situ across multiple real-world settings, which can be time-consuming and labor-intensive. We propose SimulataR, a cost-effective desktop-based approach for rapid aR prototyping using first-person-view context videos blended with design prototypes to simulate an aR experience. A field study involving 12 AR users comparing SimulataR to real OST-HMDs found that SimulataR can approximate the aR experience, particularly for indoors and in low-to-moderate lit outdoor environments. Case studies with two designers who used SimulataR in their design process demonstrates the potential of design-blended videos for rapid aR prototyping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16515v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwin Ram, Yue Gu, Bowen Wang, Sneha Jaikumar, Youqi Wu, Benjamin Tan Kuan Wei, Qingyang Xu, Haiming Liu, Shengdong Zhao</dc:creator>
    </item>
    <item>
      <title>Model Human Learners: Computational Models to Guide Instructional Design</title>
      <link>https://arxiv.org/abs/2502.02456</link>
      <description>arXiv:2502.02456v3 Announce Type: replace 
Abstract: Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions. To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions. This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments -- one testing a problem sequencing intervention and the other testing an item design intervention. It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective. These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02456v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. MacLellan</dc:creator>
    </item>
    <item>
      <title>CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</title>
      <link>https://arxiv.org/abs/2502.03069</link>
      <description>arXiv:2502.03069v2 Announce Type: replace 
Abstract: Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI's contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03069v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713720</arxiv:DOI>
      <dc:creator>Julian Rasch, Julia T\"ows, Teresa Hirzle, Florian M\"uller, Martin Schmitz</dc:creator>
    </item>
    <item>
      <title>Automatic Prompt Optimization Techniques: Exploring the Potential for Synthetic Data Generation</title>
      <link>https://arxiv.org/abs/2502.03078</link>
      <description>arXiv:2502.03078v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) advancement is heavily dependent on access to large-scale, high-quality training data. However, in specialized domains such as healthcare, data acquisition faces significant constraints due to privacy regulations, ethical considerations, and limited availability. While synthetic data generation offers a promising solution, conventional approaches typically require substantial real data for training generative models. The emergence of large-scale prompt-based models presents new opportunities for synthetic data generation without direct access to protected data. However, crafting effective prompts for domain-specific data generation remains challenging, and manual prompt engineering proves insufficient for achieving output with sufficient precision and authenticity. We review recent developments in automatic prompt optimization, following PRISMA guidelines. We analyze six peer-reviewed studies published between 2020 and 2024 that focus on automatic data-free prompt optimization methods. Our analysis reveals three approaches: feedback-driven, error-based, and control-theoretic. Although all approaches demonstrate promising capabilities in prompt refinement and adaptation, our findings suggest the need for an integrated framework that combines complementary optimization techniques to enhance synthetic data generation while minimizing manual intervention. We propose future research directions toward developing robust, iterative prompt optimization frameworks capable of improving the quality of synthetic data. This advancement can be particularly crucial for sensitive fields and in specialized domains where data access is restricted, potentially transforming how we approach synthetic data generation for AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03078v2</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina Freise, Marius Heitlinger, Ruben Nuredini, Gerrit Meixner</dc:creator>
    </item>
    <item>
      <title>Code Shaping: Iterative Code Editing with Free-form AI-Interpreted Sketching</title>
      <link>https://arxiv.org/abs/2502.03719</link>
      <description>arXiv:2502.03719v2 Announce Type: replace 
Abstract: We introduce the concept of code shaping, an interaction paradigm for editing code using free-form sketch annotations directly on top of the code and console output. To evaluate this concept, we conducted a three-stage design study with 18 different programmers to investigate how sketches can communicate intended code edits to an AI model for interpretation and execution. The results show how different sketches are used, the strategies programmers employ during iterative interactions with AI interpretations, and interaction design principles that support the reconciliation between the code editor and sketches. Finally, we demonstrate the practical application of the code shaping concept with two use case scenarios, illustrating design implications from the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03719v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Yen, Jian Zhao, Daniel Vogel</dc:creator>
    </item>
    <item>
      <title>ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</title>
      <link>https://arxiv.org/abs/2209.08199</link>
      <description>arXiv:2209.08199v4 Announce Type: replace-cross 
Abstract: We introduce ScreenQA, a novel benchmarking dataset designed to advance screen content understanding through question answering. The existing screen datasets are focused either on low-level structural and component understanding, or on a much higher-level composite task such as navigation and task completion for autonomous agents. ScreenQA attempts to bridge this gap. By annotating 86k question-answer pairs over the RICO dataset, we aim to benchmark the screen reading comprehension capacity, thereby laying the foundation for vision-based automation over screenshots. Our annotations encompass full answers, short answer phrases, and corresponding UI contents with bounding boxes, enabling four subtasks to address various application scenarios. We evaluate the dataset's efficacy using both open-weight and proprietary models in zero-shot, fine-tuned, and transfer learning settings. We further demonstrate positive transfer to web applications, highlighting its potential beyond mobile applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08199v4</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Srinivas Sunkara, Victor Carbune, Jason Lin, Maria Wang, Yun Zhu, Jindong Chen</dc:creator>
    </item>
    <item>
      <title>Interactive Task Planning with Language Models</title>
      <link>https://arxiv.org/abs/2310.10645</link>
      <description>arXiv:2310.10645v2 Announce Type: replace-cross 
Abstract: An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals and distinct tasks, even during execution. However, most traditional methods require predefined module design, making it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models by incorporating both high-level planning and low-level skill execution through function calling, leveraging pretrained vision models to ground the scene in language. We verify the robustness of our system on the real world task of making milk tea drinks. Our system is able to generate novel high-level instructions for unseen objectives and successfully accomplishes user tasks. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Our approach is easy to adapt to different tasks by simply substituting the task guidelines, without the need for additional complex prompt engineering. Please check more details on our https://wuphilipp.github.io/itp_site and https://youtu.be/TrKLuyv26_g.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10645v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik</dc:creator>
    </item>
    <item>
      <title>Panza: Design and Analysis of a Fully-Local Personalized Text Writing Assistant</title>
      <link>https://arxiv.org/abs/2407.10994</link>
      <description>arXiv:2407.10994v4 Announce Type: replace-cross 
Abstract: The availability of powerful open-source large language models (LLMs) opens exciting use-cases, such as using personal data to fine-tune these models to imitate a user's unique writing style. Two key requirements for such assistants are personalization - in the sense that the assistant should recognizably reflect the user's own writing style - and privacy - users may justifiably be wary of uploading extremely personal data, such as their email archive, to a third-party service. In this paper, we present a new design and evaluation for such an automated assistant, for the specific use case of email generation, which we call Panza. Panza's personalization features are based on a combination of fine-tuning using a variant of the Reverse Instructions technique together with Retrieval-Augmented Generation (RAG). We demonstrate that this combination allows us to fine-tune an LLM to reflect a user's writing style using limited data, while executing on extremely limited resources, e.g. on a free Google Colab instance. Our key methodological contribution is the first detailed study of evaluation metrics for this personalized writing task, and of how different choices of system components--the use of RAG and of different fine-tuning approaches-impact the system's performance. Additionally, we demonstrate that very little data - under 100 email samples - are sufficient to create models that convincingly imitate humans. This finding showcases a previously-unknown attack vector in language models - that access to a small number of writing samples can allow a bad actor to cheaply create generative models that imitate a target's writing style. We are releasing the full Panza code as well as three new email datasets licensed for research use at https://github.com/IST-DASLab/PanzaMail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10994v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armand Nicolicioiu, Eugenia Iofinova, Andrej Jovanovic, Eldar Kurtic, Mahdi Nikdan, Andrei Panferov, Ilia Markov, Nir Shavit, Dan Alistarh</dc:creator>
    </item>
    <item>
      <title>EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders</title>
      <link>https://arxiv.org/abs/2407.11442</link>
      <description>arXiv:2407.11442v3 Announce Type: replace-cross 
Abstract: Numerous fairness metrics have been proposed and employed by artificial intelligence (AI) experts to quantitatively measure bias and define fairness in AI models. Recognizing the need to accommodate stakeholders' diverse fairness understandings, efforts are underway to solicit their input. However, conveying AI fairness metrics to stakeholders without AI expertise, capturing their personal preferences, and seeking a collective consensus remain challenging and underexplored. To bridge this gap, we propose a new framework, EARN Fairness, which facilitates collective metric decisions among stakeholders without requiring AI expertise. The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics, Ask stakeholders' personal metric preferences, Review metrics collectively, and Negotiate a consensus on metric selection. To gather empirical results, we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without AI knowledge. We identify their personal metric preferences and their acceptable level of unfairness in individual sessions. Subsequently, we uncovered how they reached metric consensus in team sessions. Our work shows that the EARN Fairness framework enables stakeholders to express personal preferences and reach consensus, providing practical guidance for implementing human-centered AI fairness in high-risk contexts. Through this approach, we aim to harmonize fairness expectations of diverse stakeholders, fostering more equitable and inclusive AI fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11442v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf</dc:creator>
    </item>
    <item>
      <title>Can Unconfident LLM Annotations Be Used for Confident Conclusions?</title>
      <link>https://arxiv.org/abs/2408.15204</link>
      <description>arXiv:2408.15204v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15204v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina Gligori\'c, Tijana Zrnic, Cinoo Lee, Emmanuel J. Cand\`es, Dan Jurafsky</dc:creator>
    </item>
    <item>
      <title>"So Am I Dr. Frankenstein? Or Were You a Monster the Whole Time?": Mitigating Software Project Failure With Loss-Aversion-Aware Development Methodologies</title>
      <link>https://arxiv.org/abs/2410.20696</link>
      <description>arXiv:2410.20696v2 Announce Type: replace-cross 
Abstract: Case studies have shown that software disasters snowball from technical issues to catastrophes through humans covering up problems rather than addressing them and empirical research has found the psychological safety of software engineers to discuss and address problems to be foundational to improving project success. However, the failure to do so can be attributed to psychological factors like loss aversion. We conduct a large-scale study of the experiences of 600 software engineers in the UK and USA on project success experiences. Empirical evaluation finds that approaches like ensuring clear requirements before the start of development, when loss aversion is at its lowest, correlated to 97% higher project success. The freedom of software engineers to discuss and address problems correlates with 87% higher success rates. The findings support the development of software development methodologies with a greater focus on human factors in preventing failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20696v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junade Ali</dc:creator>
    </item>
    <item>
      <title>Legacy Procurement Practices Shape How U.S. Cities Govern AI: Understanding Government Employees' Practices, Challenges, and Needs</title>
      <link>https://arxiv.org/abs/2411.04994</link>
      <description>arXiv:2411.04994v2 Announce Type: replace-cross 
Abstract: Most AI tools adopted by governments are not developed internally, but instead are acquired from third-party vendors in a process called public procurement. In this paper, we conduct the first empirical study of how United States cities' procurement practices shape critical decisions surrounding public sector AI. We conduct semi-structured interviews with 19 city employees who oversee AI procurement across 7 U.S. cities. We found that cities' legacy procurement practices, which are shaped by decades-old laws and norms, establish infrastructure that determines which AI is purchased, and which actors hold decision-making power over procured AI. We characterize the emerging actions cities have taken to adapt their purchasing practices to address algorithmic harms. From employees' reflections on real-world AI procurements, we identify three key challenges that motivate but are not fully addressed by existing AI procurement reform initiatives. Based on these findings, we discuss implications and opportunities for the FAccT community to support cities in foreseeing and preventing AI harms throughout the public procurement processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04994v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nari Johnson, Elise Silva, Harrison Leon, Motahhare Eslami, Beth Schwanke, Ravit Dotan, Hoda Heidari</dc:creator>
    </item>
    <item>
      <title>Influencing Humans to Conform to Preference Models for RLHF</title>
      <link>https://arxiv.org/abs/2501.06416</link>
      <description>arXiv:2501.06416v2 Announce Type: replace-cross 
Abstract: Designing a reinforcement learning from human feedback (RLHF) algorithm to approximate a human's unobservable reward function requires assuming, implicitly or explicitly, a model of human preferences. A preference model that poorly describes how humans generate preferences risks learning a poor approximation of the human's reward function. In this paper, we conduct three human studies to asses whether one can influence the expression of real human preferences to more closely conform to a desired preference model. Importantly, our approach does not seek to alter the human's unobserved reward function. Rather, we change how humans use this reward function to generate preferences, such that they better match whatever preference model is assumed by a particular RLHF algorithm. We introduce three interventions: showing humans the quantities that underlie a preference model, which is normally unobservable information derived from the reward function; training people to follow a specific preference model; and modifying the preference elicitation question. All intervention types show significant effects, providing practical tools to improve preference data quality and the resultant alignment of the learned reward functions. Overall we establish a novel research direction in model alignment: designing interfaces and training interventions to increase human conformance with the modeling assumptions of the algorithm that will learn from their input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06416v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephane Hatgis-Kessell, W. Bradley Knox, Serena Booth, Scott Niekum, Peter Stone</dc:creator>
    </item>
    <item>
      <title>Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by Level of Detail (LOD) Theory</title>
      <link>https://arxiv.org/abs/2501.10428</link>
      <description>arXiv:2501.10428v2 Announce Type: replace-cross 
Abstract: Objective: This study explores a novel deep learning approach for EEG analysis and perceptual state guidance, inspired by Level of Detail (LOD) theory. The goal is to improve perceptual state identification accuracy and advance personalized psychological therapy. Methods: Portable EEG devices and music rhythm signals were used for data collection. LOD theory was applied to dynamically adjust EEG signal processing, extracting core perceptual features. A Unity-based software system integrated EEG data with audio materials. The deep learning model combined a CNN for feature extraction and classification, and a DQN for reinforcement learning to optimize rhythm adjustments. Results: The CNN achieved 94.05% accuracy in perceptual state classification. The DQN guided subjects to target states with a 92.45% success rate, averaging 13.2 rhythm cycles. However, only 50% of users reported psychological alignment with the target state, indicating room for improvement. Discussion: The results validate the potential of LOD-based EEG biofeedback. Limitations include dataset source, label subjectivity, and reward function optimization. Future work will expand to diverse subjects, incorporate varied musical elements, and refine reward functions for better generalization and personalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10428v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>BG Tong</dc:creator>
    </item>
    <item>
      <title>Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems</title>
      <link>https://arxiv.org/abs/2501.11613</link>
      <description>arXiv:2501.11613v5 Announce Type: replace-cross 
Abstract: This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof-of-concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom functions (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include CR evaluation methods based on prompt engineering frameworks driven by goal-oriented grading criteria, improving scalability for complex multi-agent interactions, and enhancing system robustness to address the identified limitations across diverse business applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11613v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giorgio Robino</dc:creator>
    </item>
    <item>
      <title>CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation</title>
      <link>https://arxiv.org/abs/2501.16609</link>
      <description>arXiv:2501.16609v2 Announce Type: replace-cross 
Abstract: While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16609v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faria Huq, Zora Zhiruo Wang, Frank F. Xu, Tianyue Ou, Shuyan Zhou, Jeffrey P. Bigham, Graham Neubig</dc:creator>
    </item>
  </channel>
</rss>
