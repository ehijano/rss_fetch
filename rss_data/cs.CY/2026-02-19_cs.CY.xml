<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 02:34:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Should There be a Teacher In-the-Loop? A Study of Generative AI Personalized Tasks Middle School</title>
      <link>https://arxiv.org/abs/2602.15876</link>
      <description>arXiv:2602.15876v1 Announce Type: new 
Abstract: Adapting instruction to the fine-grained needs of individual students is a powerful application of recent advances in large language models. These generative AI models can create tasks that correspond to students' interests and enact context personalization, enhancing students' interest in learning academic content. However, when there is a teacher in-the-loop creating or modifying tasks with generative AI, it is unclear how efficient this process might be, despite commercial generative AI tools' claims that they will save teachers time. In the present study, we teamed 7 middle school mathematics teachers with ChatGPT to create personalized versions of problems in their curriculum, to correspond to their students' interests. We look at the prompting moves teachers made, their efficiency when creating problems, and the reactions of their 521 7th grade students who received the personalized assignments. We find that having a teacher-in-the-loop results in generative AI-enhanced personalization being enacted at a relatively broad grain size, whereas students tend to prefer a smaller grain size where they receive specific popular culture references that interest them. Teachers spent a lot of effort adjusting popular culture references and addressing issues with the depth or realism of the problems generated, giving higher or lower levels of ownership to the generative AI. Teachers were able to improve in their ability to craft interesting problems in partnership with generative AI, but this process did not appear to become particularly time efficient as teachers learned and reflected on their students' data, iterating their approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15876v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Candace Walkington, Mingyu Feng, Itffini Pruitt-Britton, Theodora Beauchamp, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>Pluralism in AI Governance: Toward Sociotechnical Alignment and Normative Coherence</title>
      <link>https://arxiv.org/abs/2602.15881</link>
      <description>arXiv:2602.15881v1 Announce Type: new 
Abstract: This paper examines the challenge of embedding public values into national artificial intelligence (AI) governance frameworks, a task complicated by the sociotechnical nature of contemporary systems. As AI permeates domains such as healthcare, justice, and public administration, legitimacy depends not only on technical correctness but on alignment with societal norms, democratic principles, and human dignity. Traditional paradigms focused on model safety or market efficiency neglect broader institutional contexts. To address this, the study synthesises frameworks including Full-Stack Alignment, Thick Models of Value, Value Sensitive Design, and Public Constitutional AI, alongside comparative analysis of jurisdictions such as the EU, US, China, UK, Brazil, and South Africa (SA). It introduces a layered framework linking values, mechanisms, and strategies, and maps tensions such as fairness versus efficiency, transparency versus security, and privacy versus equity. Findings reveal a pluralism of regulatory philosophies, with SA sovereignty-oriented approach offering a distinctive counterpoint. The study contributes a holistic, value-sensitive model of AI governance, reframing regulation as a proactive mechanism for embedding public values into sociotechnical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15881v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Wa Nkongolo</dc:creator>
    </item>
    <item>
      <title>Queer NLP: A Critical Survey on Literature Gaps, Biases and Trends</title>
      <link>https://arxiv.org/abs/2602.16151</link>
      <description>arXiv:2602.16151v1 Announce Type: new 
Abstract: Natural language processing (NLP) technologies are rapidly reshaping how language is created, processed, and analyzed by humans. With current and potential applications in hiring, law, healthcare, and other areas that impact people's lives, understanding and mitigating harms towards marginalized groups is critical. In this survey, we examine NLP research papers that explicitly address the relationship between LGBTQIA+ communities and NLP technologies. We systematically review all such papers published in the ACL Anthology, to answer the following research questions: (1) What are current research trends? (2) What gaps exist in terms of topics and methods? (3) What areas are open for future work? We find that while the number of papers on queer NLP has grown within the last few years, most papers take a reactive rather than a proactive approach, pointing out bias more often than mitigating it, and focusing on shortcomings of existing systems rather than creating new solutions. Our survey uncovers many opportunities for future work, especially regarding stakeholder involvement, intersectionality, interdisciplinarity, and languages other than English. We also offer an outlook from a queer studies perspective, highlighting understudied topics and gaps in the harms addressed in NLP papers. Beyond being a roadmap of what has been done, this survey is a call to action for work towards more just and inclusive NLP technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16151v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sabine Weber, Angelina Wang, Ankush Gupta, Arjun Subramonian, Dennis Ulmer, Eshaan Tanwar, Geetanjali Aich, Hannah Devinney, Jacob Hobbs, Jennifer Mickel, Joshua Tint, Mae Sosto, Ray Groshan, Simone Astarita, Vagrant Gautam, Verena Blaschke, William Agnew, Wilson Y Lee, Yanan Long</dc:creator>
    </item>
    <item>
      <title>Generative AI Usage of University Students: Navigating Between Education and Business</title>
      <link>https://arxiv.org/abs/2602.16307</link>
      <description>arXiv:2602.16307v1 Announce Type: new 
Abstract: This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16307v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>20th International Conference on Wirtschaftsinformatik, September 2025, Muenster, Germany</arxiv:journal_reference>
      <dc:creator>Fabian Walke, Veronika F\"oller</dc:creator>
    </item>
    <item>
      <title>Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship</title>
      <link>https://arxiv.org/abs/2602.16553</link>
      <description>arXiv:2602.16553v1 Announce Type: new 
Abstract: The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16553v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Ranisch, Sabine Salloch</dc:creator>
    </item>
    <item>
      <title>Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data</title>
      <link>https://arxiv.org/abs/2602.16561</link>
      <description>arXiv:2602.16561v1 Announce Type: new 
Abstract: Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16561v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roya Shomali, Nick Freeman, Greg Bott, Iman Dayarian, Jason Parton</dc:creator>
    </item>
    <item>
      <title>Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology</title>
      <link>https://arxiv.org/abs/2602.16703</link>
      <description>arXiv:2602.16703v1 Announce Type: new 
Abstract: Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a "typical" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16703v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Zhou Hong, Alex Kleinman, Alyssa Mathiowetz, Adam Howes, Julian Cohen, Suveer Ganta, Alex Letizia, Dora Liao, Deepika Pahari, Xavier Roberts-Gaal, Luca Righetti, Joe Torres</dc:creator>
    </item>
    <item>
      <title>NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey</title>
      <link>https://arxiv.org/abs/2602.15866</link>
      <description>arXiv:2602.15866v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review 203 peer-reviewed papers and propose the NLP Privacy Risk Identification in Social Media (NLP-PRISM) framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from 0.58-0.84, but incur a 1% - 23% drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis (16), emotion detection (14), offensive language identification (19), code-mixed processing (39), native language identification (29), and dialect detection (24) revealing substantial gaps in privacy research. We further found a (reduced by 2% - 9%) trade-off in model utility, MIA AUC (membership inference attacks) 0.81, AIA accuracy 0.75 (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15866v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026</arxiv:journal_reference>
      <dc:creator>Dhiman Goswami, Jai Kruthunz Naveen Kumar, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content</title>
      <link>https://arxiv.org/abs/2602.15871</link>
      <description>arXiv:2602.15871v1 Announce Type: cross 
Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15871v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diletta Abbonato</dc:creator>
    </item>
    <item>
      <title>BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation</title>
      <link>https://arxiv.org/abs/2602.15879</link>
      <description>arXiv:2602.15879v1 Announce Type: cross 
Abstract: Exercise recommendation focuses on personalized exercise selection conditioned on students' learning history, personal interests, and other individualized characteristics. Despite notable progress, most existing methods represent student learning solely as exercise sequences, overlooking rich behavioral interaction information. This limited representation often leads to biased and unreliable estimates of learning progress. Moreover, fixed-length sequence segmentation limits the incorporation of early learning experiences, thereby hindering the modeling of long-term dependencies and the accurate estimation of knowledge mastery. To address these limitations, we propose BamaER, a Behavior-aware memory-augmented Exercise Recommendation framework that comprises three core modules: (i) the learning progress prediction module that captures heterogeneous student interaction behaviors via a tri-directional hybrid encoding scheme; (ii) the memory-augmented knowledge tracing module that maintains a dynamic memory matrix to jointly model historical and current knowledge states for robust mastery estimation; and (iii) the exercise filtering module that formulates candidate selection as a diversity-aware optimization problem, solved via the Hippopotamus Optimization Algorithm to reduce redundancy and improve recommendation coverage. Experiments on five real-world educational datasets show that BamaER consistently outperforms state-of-the-art baselines across a range of evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15879v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qing Yang, Yuhao Jiang, Rui Wang, Jipeng Guo, Yejiang Wang, Xinghe Cheng, Zezheng Wu, Jiapu Wang, Jingwei Zhang</dc:creator>
    </item>
    <item>
      <title>From Reflection to Repair: A Scoping Review of Dataset Documentation Tools</title>
      <link>https://arxiv.org/abs/2602.15968</link>
      <description>arXiv:2602.15968v1 Announce Type: cross 
Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15968v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791344</arxiv:DOI>
      <dc:creator>Pedro Reynolds-Cu\'ellar (Robotics,AI Institute), Marisol Wong-Villacres (Escuela Superior Polit\'ecnica del Litoral), Adriana Alvarado Garcia (IBM Research), Heila Precel (Robotics,AI Institute)</dc:creator>
    </item>
    <item>
      <title>Surgical Activation Steering via Generative Causal Mediation</title>
      <link>https://arxiv.org/abs/2602.16080</link>
      <description>arXiv:2602.16080v1 Announce Type: cross 
Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16080v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aruna Sankaranarayanan, Amir Zur, Atticus Geiger, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications</title>
      <link>https://arxiv.org/abs/2602.16201</link>
      <description>arXiv:2602.16201v1 Announce Type: cross 
Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16201v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Badhe, Deep Shah, Nehal Kathrotia</dc:creator>
    </item>
    <item>
      <title>Machine Learning in Epidemiology</title>
      <link>https://arxiv.org/abs/2602.16352</link>
      <description>arXiv:2602.16352v1 Announce Type: cross 
Abstract: In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16352v1</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-1-4614-6625-3_81-1</arxiv:DOI>
      <arxiv:journal_reference>In: Ahrens, W., Pigeot, I. (Eds.) Handbook of Epidemiology. Springer, New York (2025)</arxiv:journal_reference>
      <dc:creator>Marvin N. Wright, Lukas Burk, Pegah Golchian, Jan Kapar, Niklas Koenen, Sophie Hanna Langbein</dc:creator>
    </item>
    <item>
      <title>Towards a Science of AI Agent Reliability</title>
      <link>https://arxiv.org/abs/2602.16666</link>
      <description>arXiv:2602.16666v1 Announce Type: cross 
Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16666v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</dc:creator>
    </item>
    <item>
      <title>Fairness Dynamics in Digital Economy Platforms with Biased Ratings</title>
      <link>https://arxiv.org/abs/2602.16695</link>
      <description>arXiv:2602.16695v1 Announce Type: cross 
Abstract: The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16695v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.65109/CEJT6762</arxiv:DOI>
      <dc:creator>J. Martin Smit, Fernando P. Santos</dc:creator>
    </item>
    <item>
      <title>Protocol as Poetry: A Case Study of Pak's Smart Contract-Based Protocol Art</title>
      <link>https://arxiv.org/abs/2505.12393</link>
      <description>arXiv:2505.12393v2 Announce Type: replace 
Abstract: Protocol art has recently proliferated through blockchain-based smart contracts, building on a century-long lineage of conceptual, participatory, interactive, systematic, algorithmic, and generative art practices. Few studies have examined the characteristics and appreciation of this emerging art form. To address this gap, this paper presents an annotated portfolio analysis of protocol artworks by Pak, a pioneering and influential pseudonymous artist who treats smart contracts as medium and collective participation through protocol as message. Tracing the evolution from early open-edition releases of The Fungible (2021) and the dynamic mechanics of Merge (2021) to the soul-bound messaging of Censored (2022) and the reflective absence of Not Found (2023), we examine how Pak choreographs distributed agency across collectors and autonomous code, demonstrating how programmable protocols become a social fabric in artistic meaning-making. Through thematic analysis of Pak's works, we identify seven core characteristics distinguishing protocol art from other art forms: (1) system-centric rather than object-centric composition, (2) autonomous governance enabling open-ended control, (3) distributed agency and communal authorship, (4) temporal dynamism and lifecycle aesthetics, (5) economy-driven engagement, (6) poetic message embedded in interaction rituals, and (7) interoperability enabling composability for emergent complexity. We then discuss how these features set protocol art apart from adjacent movements such as conceptual, generative, participatory, interactive, and performance art. By analyzing principles grounded in Pak's practice, we contribute to the emerging literature on protocol art (or "protocolism") and offer design implications for future artists exploring this evolving form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12393v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.MM</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3773699.3773918</arxiv:DOI>
      <dc:creator>Botao Amber Hu</dc:creator>
    </item>
    <item>
      <title>Dirty Bits in Low-Earth Orbit: The Carbon Footprint of Launching Computers</title>
      <link>https://arxiv.org/abs/2508.06250</link>
      <description>arXiv:2508.06250v3 Announce Type: replace 
Abstract: Low-Earth Orbit (LEO) satellites are increasingly proposed for communication and in-orbit computing, achieving low-latency global services. However, their sustainability remains largely unexamined. This paper investigates the carbon footprint of computing in space, focusing on lifecycle emissions from launch over orbital operation to re-entry. We present ESpaS, a lightweight tool for estimating carbon intensities across CPU usage, memory, and networking in orbital vs. terrestrial settings. Three worked examples compare (i) launch technologies (state-of-the-art rocket vs. potential next generation), (ii) operational emissions of data center workloads in orbit and on the ground and, (iii) in-orbit aggregation with raw data transmission. Results show that, even under optimistic assumptions, in-orbit systems incur significantly higher carbon costs - primarily due to embodied emissions from launch and re-entry. Our findings advocate for carbon-aware design principles and regulatory oversight in developing sustainable digital infrastructure in orbit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06250v3</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757892.3757896</arxiv:DOI>
      <arxiv:journal_reference>ACM SIGENERGY Energy Inform. Rev., Volume 5 Issue 2, July 2025</arxiv:journal_reference>
      <dc:creator>Robin Ohs, Gregory F. Stock, Andreas Schmidt, Juan A. Fraire, Holger Hermanns</dc:creator>
    </item>
    <item>
      <title>The Hands-Up Problem and How to Deal With It: Secondary School Teachers' Experiences of Debugging in the Classroom</title>
      <link>https://arxiv.org/abs/2508.18861</link>
      <description>arXiv:2508.18861v3 Announce Type: replace 
Abstract: Debugging is a vital but challenging skill for beginner programmers to learn. It is also a difficult skill to teach. For secondary school teachers, who may lack time or programming experience, honing students' understanding of debugging can be a daunting task. Despite this, little research has explored their perspectives of debugging. To this end, we investigated secondary teachers' experiences of debugging in the classroom, with a focus on text-based programming. Through thematic analysis of nine semi-structured interviews, we identified a common reliance on the teacher for debugging support, embodied by many raised hands. We call this phenomenon the "hands-up problem". While more experienced and confident teachers discussed strategies they use to counteract this, less confident teachers discussed the negative consequences of this problem. We recommend further research into debugging-specific pedagogical content knowledge and professional development to help less confident teachers develop approaches for supporting their students with debugging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18861v3</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurie Gale, Sue Sentance</dc:creator>
    </item>
    <item>
      <title>Bizarre Love Triangle: Generative AI, Art, and Kitsch</title>
      <link>https://arxiv.org/abs/2602.11353</link>
      <description>arXiv:2602.11353v3 Announce Type: replace 
Abstract: Generative artificial intelligence (GenAI) has engrossed the mainstream culture, expanded AI's creative user base, and catalyzed economic, legal, and aesthetic issues that stir a lively public debate. Unsurprisingly, GenAI tools proliferate kitsch in the hands of amateurs and hobbyists, but various shortcomings also induce kitsch into a more ambitious, professional artists' production with GenAI. I explore them in this paper. Following the introductory outline of digital kitsch and AI art, I review GenAI artworks that manifest five interrelated types of kitsch-engendering expressive flaws: the superficial foregrounding or faulty circumvention of generative models' formal signatures, the feeble critique of AI, the mimetics, and the unacknowledged poetic similarities, all marked by an overreliance on AI as a cultural signifier. I discuss the normalization of these blunders through GenAI art's good standing within the art world and keen relationship with the AI industry, which contributes to the adulteration of AI discourse and the possible corruption of artistic literacy. In conclusion, I emphasize that recognizing different facets of artists' uncritical embrace of techno-cultural trends, comprehending their functions, and anticipating their unintended effects is crucial for reaching relevance and responsibility in AI art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11353v3</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.34624/jdmi.v8i20.40441</arxiv:DOI>
      <arxiv:journal_reference>Journal of Digital Media &amp; Interaction, vol. 8, issue 20, pp. 65-80 (2025)</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Peaceful Anarcho-Accelerationism: Decentralized Full Automation for a Society of Universal Care</title>
      <link>https://arxiv.org/abs/2602.13154</link>
      <description>arXiv:2602.13154v4 Announce Type: replace 
Abstract: Foundational results in machine learning like the universal approximation theorem and deep reinforcement learning convergence and the progressive scale of technology imply that the vast majority of instrumental labor may be progressively automated. Consequently, as this process accelerates, the critical question becomes one of governance: who controls the machines, hence the labor and capital, and toward what ends? Predicting a post-capitalism future, this paper introduces an alternative system for society: anarcho-accelerationism. Concretely, it is a sociotechnical framework in which full automation is decentralized, commons-governed, and oriented toward universal care. To make it happen, I propose the Liberation Stack, a layered architecture of energy, manufacturing, food, communication, knowledge, and governance commons, powered by frontier clean energy technologies within an accelerationist ecologism that achieves sustainability through abundance rather than degrowth. As safety net, this system introduces Universal Desired Resources (UDR) as a post-monetary design principle and show that UDR constitutes the most comprehensive intersectional intervention yet proposed: by eliminating the material basis of oppression, it dissolves all axes of structural inequality simultaneously. Drawing on Maslow's hierarchy, I show that the Liberation Stack satisfies basic needs universally, enabling what the accelerationis literature terms synthetic liberty, the positive freedom that emerges when commons infrastructure provides the material conditions for genuine autonomy. Finally, given a set of assumptions and constraints, I propose a progressive transition from Universal Basic Income to UDR with a phased roadmap and present empirical evidence from Linux, Wikipedia, Mondragon and Rojava confirming that commons-based systems operate at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13154v4</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo C. Garrido-Merch\'an</dc:creator>
    </item>
    <item>
      <title>FOCUS on Contamination: Hydrology-Informed Noise-Aware Learning for Geospatial PFAS Mapping</title>
      <link>https://arxiv.org/abs/2502.14894</link>
      <description>arXiv:2502.14894v4 Announce Type: replace-cross 
Abstract: Per- and polyfluoroalkyl substances (PFAS) are persistent environmental contaminants with significant public health impacts, yet large-scale monitoring remains severely limited due to the high cost and logistical challenges of field sampling. The lack of samples leads to difficulty simulating their spread with physical models and limited scientific understanding of PFAS transport in surface waters. Yet, rich geospatial and satellite-derived data describing land cover, hydrology, and industrial activity are widely available. We introduce FOCUS, a geospatial deep learning framework for PFAS contamination mapping that integrates sparse PFAS observations with large-scale environmental context, including priors derived from hydrological connectivity, land cover, source proximity, and sampling distance. These priors are integrated into a principled, noise-aware loss, yielding a robust training objective under sparse labels. Across extensive ablations, robustness analyses, and real-world validation, FOCUS consistently outperforms baselines including sparse segmentation, Kriging, and pollutant transport simulations, while preserving spatial coherence and scalability over large regions. Our results demonstrate how AI can support environmental science by providing screening-level risk maps that prioritize follow-up sampling and help connect potential sources to surface-water contamination patterns in the absence of complete physical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14894v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jowaria Khan, Alexa Friedman, Sydney Evans, Rachel Klein, Runzi Wang, Katherine E. Manz, Kaley Beins, David Q. Andrews, Elizabeth Bondi-Kelly</dc:creator>
    </item>
    <item>
      <title>Strategic Hiring under Algorithmic Monoculture</title>
      <link>https://arxiv.org/abs/2502.20063</link>
      <description>arXiv:2502.20063v2 Announce Type: replace-cross 
Abstract: We study the impact of strategic behavior in labor markets characterized by algorithmic monoculture, where firms compete for a shared pool of applicants using a common algorithmic evaluation. In this setting, "naive" hiring strategies lead to severe congestion, as firms collectively target the same high-scoring candidates. We model this competition as a game with capacity-constrained firms and fully characterize the set of Nash equilibria. We demonstrate that equilibrium strategies, which naturally diversify firms' interview targets, significantly outperform naive selection, increasing social welfare for both firms and applicants. Specifically, the Price of Naive Selection (welfare gain from strategy) grows linearly with the number of firms, while the Price of Anarchy (efficiency loss from decentralization) approaches 1, implying that the decentralized equilibrium is nearly socially optimal. Finally, we analyze convergence, and we show that a simple sequential best-response process converges to the desired equilibrium. However, we show that firms generally cannot infer the key input needed to compute best responses, namely congestion for specific candidates, from their own historical data alone. Consequently, to realize the welfare gains of strategic differentiation, algorithmic platforms must explicitly reveal congestion information to participating firms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20063v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackie Baek, Hamsa Bastani, Shihan Chen</dc:creator>
    </item>
    <item>
      <title>Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training of Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2507.04033</link>
      <description>arXiv:2507.04033v2 Announce Type: replace-cross 
Abstract: The ability to train Deep Neural Networks (DNNs) with constraints is instrumental in improving the fairness of modern machine-learning models. Many algorithms have been analysed in recent years, and yet there is no standard, widely accepted method for the constrained training of DNNs. In this paper, we provide a challenging benchmark of real-world large-scale fairness-constrained learning tasks, built on top of the US Census (Folktables). We point out the theoretical challenges of such tasks and review the main approaches in stochastic approximation algorithms. Finally, we demonstrate the use of the benchmark by implementing and comparing three recently proposed, but as-of-yet unimplemented, algorithms both in terms of optimization performance, and fairness improvement. We release the code of the benchmark as a Python package at https://github.com/humancompatible/train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04033v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>14th International Conference on Learning Representations, 2026</arxiv:journal_reference>
      <dc:creator>Andrii Kliachkin, Jana Lep\v{s}ov\'a, Gilles Bareilles, Jakub Mare\v{c}ek</dc:creator>
    </item>
    <item>
      <title>When Algorithms Meet Artists: Semantic Compression of Artists' Concerns in the Public AI-Art Debate</title>
      <link>https://arxiv.org/abs/2508.03037</link>
      <description>arXiv:2508.03037v4 Announce Type: replace-cross 
Abstract: Artists occupy a paradoxical position in generative AI: their work trains the models reshaping creative labor. We tested whether their concerns achieve proportional representation in public discourse shaping AI governance. Analyzing public AI-art discourse (news, podcasts, legal filings, research; 2013--2025) and projecting 1,259 survey-derived artist statements into this semantic space, we find stark compression: 95% of artist concerns cluster in 4 of 22 discourse topics, while 14 topics (62% of discourse) contain no artist perspective. This compression is selective - governance concerns (ownership, transparency) are 7x underrepresented; affective themes (threat, utility) show only 1.4x underrepresentation after style controls. The pattern indicates semantic, not stylistic, marginalization. These findings demonstrate a measurable representational gap: decision-makers relying on public discourse as a proxy for stakeholder priorities will systematically underweight those most affected. We introduce a consensus-based semantic projection methodology that is currently being validated across domains and generalizes to other stakeholder-technology contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03037v4</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariya Mukherjee-Gandhi, Oliver Muellerklein</dc:creator>
    </item>
    <item>
      <title>ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI</title>
      <link>https://arxiv.org/abs/2602.14135</link>
      <description>arXiv:2602.14135v2 Announce Type: replace-cross 
Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14135v2</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibo Tong, Feifei Zhao, Linghao Feng, Ruoyu Wu, Ruolin Chen, Lu Jia, Zhou Zhao, Jindong Li, Tenglong Li, Erliang Lin, Shuai Yang, Enmeng Lu, Yinqian Sun, Qian Zhang, Zizhe Ruan, Jinyu Fan, Zeyang Yue, Ping Wu, Huangrui Li, Chengyi Sun, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook</title>
      <link>https://arxiv.org/abs/2602.14299</link>
      <description>arXiv:2602.14299v2 Announce Type: replace-cross 
Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while the global average of semantic contents stabilizes rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop a stable structure and consensus due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14299v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Li, Xirui Li, Tianyi Zhou</dc:creator>
    </item>
    <item>
      <title>A Geometric Analysis of Small-sized Language Model Hallucinations</title>
      <link>https://arxiv.org/abs/2602.14778</link>
      <description>arXiv:2602.14778v2 Announce Type: replace-cross 
Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.
  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.
  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14778v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Ricco, Elia Onofri, Lorenzo Cima, Stefano Cresci, Roberto Di Pietro</dc:creator>
    </item>
  </channel>
</rss>
