<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SD updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SD</link>
    <description>cs.SD updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SD" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 05:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hookpad Aria: A Copilot for Songwriters</title>
      <link>https://arxiv.org/abs/2502.08122</link>
      <description>arXiv:2502.08122v1 Announce Type: new 
Abstract: We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.
  More information about Hookpad Aria is available at https://www.hooktheory.com/hookpad/aria</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08122v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun</dc:creator>
    </item>
    <item>
      <title>Methods for pitch analysis in contemporary popular music: highlighting pitch uncertainty in Primaal's commercial works</title>
      <link>https://arxiv.org/abs/2502.08131</link>
      <description>arXiv:2502.08131v1 Announce Type: new 
Abstract: We identify characteristic features of how pitch is manipulated for expressive purposes by Hyper Music, a mainstream commercial music company specialising in advertisement music for global corporations. The study shows that the use and organisation of pitch in the company's `Primaal' brand differs from Western classical music. Through interviews with producers and in-depth analysis of their work, we reveal that their methods centre on a conscious aim to construct a musical discourse based on pitch uncertainty, contrasting with the clear transmission of well-defined pitches in Western classical traditions. According to the Primaal producers, who acknowledge the influence of artists such as Kanye West and Daft Punk and use widely available technology, pitch uncertainty captures the listener's attention. We provide analyses of musical excerpts demonstrating their approach, alongside descriptions of the tools and methods employed to achieve their expressive goals. These goals and methods are placed in a broader historical context, contrasting with fundamental principles of pitch organisation in Western music. Techniques used by Hyper Music to introduce and control pitch uncertainty include boosting upper partials, expressive use of inharmonicity, continuous pitch distributions around 'poles' tied to specific 'modes', and continuously evolving pitch. We examine these techniques from a psychoacoustic perspective, and conduct listening tests corroborating some of the observations. The ultimate goal of the study is to introduce a set of methods suited to the analysis of pitch in contemporary popular music.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08131v1</guid>
      <category>cs.SD</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Deruty, Luc Leroy, Yann Mac\'e, David Meredith</dc:creator>
    </item>
    <item>
      <title>DualStream Contextual Fusion Network: Efficient Target Speaker Extraction by Leveraging Mixture and Enrollment Interactions</title>
      <link>https://arxiv.org/abs/2502.08191</link>
      <description>arXiv:2502.08191v1 Announce Type: new 
Abstract: Target speaker extraction focuses on extracting a target speech signal from an environment with multiple speakers by leveraging an enrollment. Existing methods predominantly rely on speaker embeddings obtained from the enrollment, potentially disregarding the contextual information and the internal interactions between the mixture and enrollment. In this paper, we propose a novel DualStream Contextual Fusion Network (DCF-Net) in the time-frequency (T-F) domain. Specifically, DualStream Fusion Block (DSFB) is introduced to obtain contextual information and capture the interactions between contextualized enrollment and mixture representation across both spatial and channel dimensions, and then rich and consistent representations are utilized to guide the extraction network for better extraction. Experimental results demonstrate that DCF-Net outperforms state-of-the-art (SOTA) methods, achieving a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 21.6 dB on the benchmark dataset, and exhibits its robustness and effectiveness in both noise and reverberation scenarios. In addition, the wrong extraction results of our model, called target confusion problem, reduce to 0.4%, which highlights the potential of DCF-Net for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08191v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Xue, Rongfei Fan, Shanping Yu, Chang Sun, Jianping An</dc:creator>
    </item>
    <item>
      <title>Sparse wavefield reconstruction and denoising with boostlets</title>
      <link>https://arxiv.org/abs/2502.08230</link>
      <description>arXiv:2502.08230v1 Announce Type: cross 
Abstract: Boostlets are spatiotemporal functions that decompose nondispersive wavefields into a collection of localized waveforms parametrized by dilations, hyperbolic rotations, and translations. We study the sparsity properties of boostlets and find that the resulting decompositions are significantly sparser than those of other state-of-the-art representation systems, such as wavelets and shearlets. This translates into improved denoising performance when hard-thresholding the boostlet coefficients. The results suggest that boostlets offer a natural framework for sparsely decomposing wavefields in unified space-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08230v1</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elias Zea, Marco Laudato, Joakim and\'en</dc:creator>
    </item>
    <item>
      <title>EnvId: A Metric Learning Approach for Forensic Few-Shot Identification of Unseen Environments</title>
      <link>https://arxiv.org/abs/2405.02119</link>
      <description>arXiv:2405.02119v2 Announce Type: replace 
Abstract: Audio recordings may provide important evidence in criminal investigations. One such case is the forensic association of a recorded audio to its recording location. For example, a voice message may be the only investigative cue to narrow down the candidate sites for a crime. Up to now, several works provide supervised classification tools for closed-set recording environment identification under relatively clean recording conditions. However, in forensic investigations, the candidate locations are case-specific. Thus, supervised learning techniques are not applicable without retraining a classifier on a sufficient amount of training samples for each case and respective candidate set. In addition, a forensic tool has to deal with audio material from uncontrolled sources with variable properties and quality. In this work, we therefore attempt a major step towards practical forensic application scenarios. We propose a representation learning framework called EnvId, short for environment identification. EnvId avoids case-specific retraining by modeling the task as a few-shot classification problem. We demonstrate that EnvId can handle forensically challenging material. It provides good quality predictions even under unseen signal degradations, out-of-distribution reverberation characteristics or recording position mismatches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02119v2</guid>
      <category>cs.SD</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denise Moussa, Germans Hirsch, Christian Riess</dc:creator>
    </item>
    <item>
      <title>Music for All: Exploring Multicultural Representations in Music Generation Models</title>
      <link>https://arxiv.org/abs/2502.07328</link>
      <description>arXiv:2502.07328v2 Announce Type: replace 
Abstract: The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07328v2</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury</dc:creator>
    </item>
    <item>
      <title>Testing Correctness, Fairness, and Robustness of Speech Emotion Recognition Models</title>
      <link>https://arxiv.org/abs/2312.06270</link>
      <description>arXiv:2312.06270v4 Announce Type: replace-cross 
Abstract: Machine learning models for speech emotion recognition (SER) can be trained for different tasks and are usually evaluated based on a few available datasets per task. Tasks could include arousal, valence, dominance, emotional categories, or tone of voice. Those models are mainly evaluated in terms of correlation or recall, and always show some errors in their predictions. The errors manifest themselves in model behaviour, which can be very different along different dimensions even if the same recall or correlation is achieved by the model. This paper introduces a testing framework to investigate behaviour of speech emotion recognition models, by requiring different metrics to reach a certain threshold in order to pass a test. The test metrics can be grouped in terms of correctness, fairness, and robustness. It also provides a method for automatically specifying test thresholds for fairness tests, based on the datasets used, and recommendations on how to select the remaining test thresholds. We evaluated a xLSTM-based and nine transformer-based acoustic foundation models against a convolutional baseline model, testing their performance on arousal, valence, dominance, and emotional category classification. The test results highlight, that models with high correlation or recall might rely on shortcuts -- such as text sentiment --, and differ in terms of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06270v4</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anna Derington, Hagen Wierstorf, Ali \"Ozkil, Florian Eyben, Felix Burkhardt, Bj\"orn W. Schuller</dc:creator>
    </item>
    <item>
      <title>Janssen 2.0: Audio Inpainting in the Time-frequency Domain</title>
      <link>https://arxiv.org/abs/2409.06392</link>
      <description>arXiv:2409.06392v2 Announce Type: replace-cross 
Abstract: The paper focuses on inpainting missing parts of an audio signal spectrogram. The autoregression-based Janssen algorithm, the state-of-the-art for the time-domain audio inpainting, is adapted for the time-frequency setting. This novel method, termed Janssen-TF, is compared to the deep-prior neural network approach using both objective metrics and a~subjective listening test, proving Janssen-TF to be superior in all the considered measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06392v2</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ond\v{r}ej Mokr\'y, Peter Balu\v{s}\'ik, Pavel Rajmic</dc:creator>
    </item>
    <item>
      <title>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment</title>
      <link>https://arxiv.org/abs/2502.04328</link>
      <description>arXiv:2502.04328v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04328v2</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.IV</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</dc:creator>
    </item>
  </channel>
</rss>
