<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Demography Meets Ministry of Health: The Case of the Family Planning Estimation Tool</title>
      <link>https://arxiv.org/abs/2501.00007</link>
      <description>arXiv:2501.00007v1 Announce Type: new 
Abstract: The Family Planning Estimation Tool (FPET) is used in low- and middle-income countries to produce estimates and short-term forecasts of family planning indicators, such as modern contraceptive use and unmet need for contraceptives. Estimates are obtained via a Bayesian statistical model that is fitted to country-specific data from surveys and service statistics data. The model has evolved over the last decade based on user inputs.
  This paper summarizes the main features of the statistical model used in FPET and introduces recent updates related to capturing contraceptive transitions, fitting to survey data that may be error prone, and the use of service statistics data. We use our experience with FPET to discuss lessons learned and open challenges related to the broader field of statistical modeling for monitoring of demographic and global health indicators to help further optimize the application of statistical modeling in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00007v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leontine Alkema, Herbert Susmann, Evan Ray, Shauna Mooney, Niamh Cahill, Kristin Bietsch, A. A. Jayachandran, Rogers Kagimu, Priya Emmart, Zenon Mujani, Khan Muhammad, Brighton Muzavazi, Rebecca Rosenberg, John Stover, Emily Sonneveldt</dc:creator>
    </item>
    <item>
      <title>Competitiveness of Formula 1 championship from 2012 to 2022 as measured by Kendall corrected evolutive coefficient</title>
      <link>https://arxiv.org/abs/2501.00126</link>
      <description>arXiv:2501.00126v1 Announce Type: new 
Abstract: In this paper we analyze the FIA formula one world championships from 2012 to 2022 taking into account the drivers classifications and the constructors teams classifications of each Grand Prix. The needed data consisted of 22 matrices of sizes ranging from $25 \times 20$ to $10 \times 19$ that have been elaborated from the GP classifications extracted from the official FIA site. We have used the Kendall corrected evolutive coefficient, recently introduced, as a measure of Competitive Balance (CB) to study the evolution of the competitiveness along the years in both drivers and teams championships. In addition, we have compared the CB of F1 championships and two major European football leagues from the seasons 2012-2013 to 2022-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00126v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Mathematical Modelling in Engineering &amp; Human Behaviour 2023 (MME&amp;HB 2023), UPV, Valencia, Spain, 2023. Pages 394-404</arxiv:journal_reference>
      <dc:creator>Francisco Pedroche</dc:creator>
    </item>
    <item>
      <title>Improving Policy-Oriented Agent-Based Modeling with History Matching: A Case Study</title>
      <link>https://arxiv.org/abs/2501.00616</link>
      <description>arXiv:2501.00616v1 Announce Type: new 
Abstract: Advances in computing power and data availability have led to growing sophistication in mechanistic mathematical models of social dynamics. Increasingly these models are used to inform real-world policy decision-making, often with significant time sensitivity. One such modeling approach is agent-based modeling, which offers particular strengths for capturing spatial and behavioral realism, and for in-silico experiments (varying input parameters and assumptions to explore their downstream impact on key outcomes). To be useful in the real world, these models must be able to qualitatively or quantitatively capture observed empirical phenomena, forming the starting point for subsequent experimentation. Computational constraints often form a significant hurdle to timely calibration and policy analysis in high resolution agent-based models. In this paper, we present a technical solution to address this bottleneck, substantially increasing efficiency and thus widening the range of utility for policy models. We illustrate our approach with a case study using a previously published and widely used epidemiological model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00616v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David O'Gara, Cliff C. Kerr, Daniel J. Klein, Micka\"el Binois, Roman Garnett, Ross A. Hammond</dc:creator>
    </item>
    <item>
      <title>Compositional data analysis for modeling and forecasting mortality with the {\alpha}-transformation</title>
      <link>https://arxiv.org/abs/2501.01129</link>
      <description>arXiv:2501.01129v1 Announce Type: new 
Abstract: Mortality forecasting is crucial for demographic planning and actuarial studies, particularly for predicting population ageing rates and future longevity risks. Traditional approaches largely rely on extrapolative methods, such as the Lee-Carter model and its variants which use mortality rates as inputs. In recent years, compositional data analysis (CoDA), which adheres to summability and non-negativity constraints, has gained increasing attention from researchers for its application in mortality forecasting. This study explores the use of the {\alpha}-transformation as an alternative to the commonly applied centered log-ratio (CLR) transformation for converting compositional data from the Aitchison simplex to unconstrained real space. The {\alpha}-transformation offers greater flexibility through the inclusion of the {\alpha} parameter, enabling better adaptation to the underlying data structure and handling of zero values, which are the limitations inherent to the CLR transformation. Using age-specific life table death counts for males and females in 31 selected European countries/regions from 1983 to 2018, the proposed method demonstrates comparable performance to the CLR transformation in most countries with improved forecast accuracy in some cases. These findings highlight the potential of the {\alpha}-transformation as a competitive alternative transformation technique for real-world mortality data within a non-functional CoDA framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01129v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Ying Lim, Dharini Pathmanathan, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>Generalized Heterogeneous Functional Model with Applications to Large-scale Mobile Health Data</title>
      <link>https://arxiv.org/abs/2501.01135</link>
      <description>arXiv:2501.01135v1 Announce Type: new 
Abstract: Physical activity is crucial for human health. With the increasing availability of large-scale mobile health data, strong associations have been found between physical activity and various diseases. However, accurately capturing this complex relationship is challenging, possibly because it varies across different subgroups of subjects, especially in large-scale datasets. To fill this gap, we propose a generalized heterogeneous functional method which simultaneously estimates functional effects and identifies subgroups within the generalized functional regression framework. The proposed method captures subgroup-specific functional relationships between physical activity and diseases, providing a more nuanced understanding of these associations. Additionally, we introduce a pre-clustering method that enhances computational efficiency for large-scale data through a finer partition of subjects compared to true subgroups. In the real data application, we examine the impact of physical activity on the risk of mental disorders and Parkinson's disease using the UK Biobank dataset, which includes over 79,000 participants. Our proposed method outperforms existing methods in future-day prediction accuracy, identifying four subgroups for mental disorder outcomes and three subgroups for Parkinson's disease diagnosis, with detailed scientific interpretations for each subgroup. We also demonstrate theoretical consistency of our methods. Supplementary materials are available online. Codes implementing the proposed method are available at: https://github.com/xiaojing777/GHFM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01135v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojing Sun, Bingxin Zhao, Fei Xue</dc:creator>
    </item>
    <item>
      <title>Marketing Mix Modeling in Lemonade</title>
      <link>https://arxiv.org/abs/2501.01276</link>
      <description>arXiv:2501.01276v1 Announce Type: new 
Abstract: Marketing mix modeling (MMM) is a widely used method to assess the effectiveness of marketing campaigns and optimize marketing strategies. Bayesian MMM is an advanced approach that allows for the incorporation of prior information, uncertainty quantification, and probabilistic predictions (1). In this paper, we describe the process of building a Bayesian MMM model for the online insurance company Lemonade. We first collected data on Lemonade's marketing activities, such as online advertising, social media, and brand marketing, as well as performance data. We then used a Bayesian framework to estimate the contribution of each marketing channel on total performance, while accounting for various factors such as seasonality, market trends, and macroeconomic indicators. To validate the model, we compared its predictions with the actual performance data from A/B-testing and sliding window holdout data (2). The results showed that the predicted contribution of each marketing channel is aligned with A/B test performance and is actionable. Furthermore, we conducted several scenario analyses using convex optimization to test the sensitivity of the model to different assumptions and to evaluate the impact of changes in the marketing mix on sales. The insights gained from the model allowed Lemonade to adjust their marketing strategy and allocate their budget more effectively. Our case study demonstrates the benefits of using Bayesian MMM for marketing attribution and optimization in a data-driven company like Lemonade. The approach is flexible, interpretable, and can provide valuable insights for decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01276v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Ravid</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Markov-switching Ordinary Differential Processes</title>
      <link>https://arxiv.org/abs/2501.00087</link>
      <description>arXiv:2501.00087v1 Announce Type: cross 
Abstract: We investigate the parameter recovery of Markov-switching ordinary differential processes from discrete observations, where the differential equations are nonlinear additive models. This framework has been widely applied in biological systems, control systems, and other domains; however, limited research has been conducted on reconstructing the generating processes from observations. In contrast, many physical systems, such as human brains, cannot be directly experimented upon and rely on observations to infer the underlying systems. To address this gap, this manuscript presents a comprehensive study of the model, encompassing algorithm design, optimization guarantees, and quantification of statistical errors. Specifically, we develop a two-stage algorithm that first recovers the continuous sample path from discrete samples and then estimates the parameters of the processes. We provide novel theoretical insights into the statistical error and linear convergence guarantee when the processes are $\beta$-mixing. Our analysis is based on the truncation of the latent posterior processes and demonstrates that the truncated processes approximate the true processes under mixing conditions. We apply this model to investigate the differences in resting-state brain networks between the ADHD group and normal controls, revealing differences in the transition rate matrices of the two groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00087v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine Tsai, Mladen Kolar, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>Post Launch Evaluation of Policies in a High-Dimensional Setting</title>
      <link>https://arxiv.org/abs/2501.00119</link>
      <description>arXiv:2501.00119v1 Announce Type: cross 
Abstract: A/B tests, also known as randomized controlled experiments (RCTs), are the gold standard for evaluating the impact of new policies, products, or decisions. However, these tests can be costly in terms of time and resources, potentially exposing users, customers, or other test subjects (units) to inferior options. This paper explores practical considerations in applying methodologies inspired by "synthetic control" as an alternative to traditional A/B testing in settings with very large numbers of units, involving up to hundreds of millions of units, which is common in modern applications such as e-commerce and ride-sharing platforms. This method is particularly valuable in settings where the treatment affects only a subset of units, leaving many units unaffected. In these scenarios, synthetic control methods leverage data from unaffected units to estimate counterfactual outcomes for treated units. After the treatment is implemented, these estimates can be compared to actual outcomes to measure the treatment effect. A key challenge in creating accurate counterfactual outcomes is interpolation bias, a well-documented phenomenon that occurs when control units differ significantly from treated units. To address this, we propose a two-phase approach: first using nearest neighbor matching based on unit covariates to select similar control units, then applying supervised learning methods suitable for high-dimensional data to estimate counterfactual outcomes. Testing using six large-scale experiments demonstrates that this approach successfully improves estimate accuracy. However, our analysis reveals that machine learning bias -- which arises from methods that trade off bias for variance reduction -- can impact results and affect conclusions about treatment effects. We document this bias in large-scale experimental settings and propose effective de-biasing techniques to address this challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00119v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shima Nassiri, Mohsen Bayati, Joe Cooprider</dc:creator>
    </item>
    <item>
      <title>Poisson Network SIR Epidemic Model</title>
      <link>https://arxiv.org/abs/2501.00187</link>
      <description>arXiv:2501.00187v1 Announce Type: cross 
Abstract: We extend the classical Susceptible-Infected-Recovered (SIR) model to a network-based framework where the degree distribution of nodes follows a Poisson distribution. This extension incorporates an additional parameter representing the mean node degree, allowing for the inclusion of heterogeneity in contact patterns. Using this enhanced model, we analyze epidemic data from the 2018-20 Ebola outbreak in the Democratic Republic of the Congo, employing a survival approach combined with the Hamiltonian Monte Carlo method. Our results suggest that network-based models can more effectively capture the heterogeneity of epidemic dynamics compared to traditional compartmental models, without introducing unduly overcomplicated compartmental framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00187v1</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Josephine K. Wairimu, Andrew Gothard, Grzegorz A. Rempala</dc:creator>
    </item>
    <item>
      <title>How Well Did U.S. Rail and Intermodal Freight Respond to the COVID-19 Pandemic vs. the Great Recession?</title>
      <link>https://arxiv.org/abs/2501.00218</link>
      <description>arXiv:2501.00218v1 Announce Type: cross 
Abstract: This paper analyzes and compares patterns of U.S. domestic rail freight volumes during, and after the disruptions caused by the 2007-2009 Great Recession and the COVID-19 pandemic in 2020. Trends in rail and intermodal shipment data are examined in conjunction with economic indicators, focusing on the extent of drop and recovery of freight volumes of various commodities and intermodal shipments, and the lead/lag time with respect to economic drivers. While impacts from and the rebound from the Great Recessions were slow to develop, COVID-19 produced both profound disruptions in the freight market and rapid rebound, with important variations across commodity types.
  Energy-related commodities (i.e., coal, petroleum, and fracking sand), dropped during the pandemic while demand for other commodities (i.e., grain products and lumber, and intermodal freight). rebounded rapidly and in some cases grew. Overall rail freight experienced a rapid rebound following the precipitous drop in traffic in March and April 2020, achieving a near-full recovery in five months. As the recovery proceeded through 2020, intermodal flow, containers moving by rail for their longest overland trips, rebounded strongly, some exceeding 2019 levels. In contrast, rail flows during the Great Recession changed slowly with the onset and recovery, extending over multiple years. Pandemic response reflected the impacts of quick shutdowns and a rapid shift in consumer purchasing patterns. Results for the pandemic illustrate the resilience of U.S. rail freight industry and the multifaceted role it plays in the overall logistics system. Amid a challenging logistical environment, freight rail kept goods moving when other methods of transport were constrained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00218v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1177/03611981221150444</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Record 2677.4 (2023): 597-610</arxiv:journal_reference>
      <dc:creator>Max T. M. Ng, Joseph Schofer, Hani S. Mahmassani</dc:creator>
    </item>
    <item>
      <title>Adventures in Demand Analysis Using AI</title>
      <link>https://arxiv.org/abs/2501.00382</link>
      <description>arXiv:2501.00382v1 Announce Type: cross 
Abstract: This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on \textit{Amazon.com}, we combine text descriptions, images, and tabular covariates to represent each product using transformer-based embedding models. These embeddings capture nuanced attributes, such as quality, branding, and visual characteristics, that traditional methods often struggle to summarize. Moreover, we fine-tune these embeddings for causal inference tasks. We show that the resulting embeddings substantially improve the predictive accuracy of sales ranks and prices and that they lead to more credible causal estimates of price elasticity. Notably, we uncover strong heterogeneity in price elasticity driven by these product-specific features. Our findings illustrate that AI-driven representations can enrich and modernize empirical demand analysis. The insights generated may also prove valuable for applied causal inference more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00382v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philipp Bach, Victor Chernozhukov, Sven Klaassen, Martin Spindler, Jan Teichert-Kluge, Suhas Vijaykumar</dc:creator>
    </item>
    <item>
      <title>Tensor Topic Modeling Via HOSVD</title>
      <link>https://arxiv.org/abs/2501.00535</link>
      <description>arXiv:2501.00535v1 Announce Type: cross 
Abstract: By representing documents as mixtures of topics, topic modeling has allowed the successful analysis of datasets across a wide spectrum of applications ranging from ecology to genetics. An important body of recent work has demonstrated the computational and statistical efficiency of probabilistic Latent Semantic Indexing (pLSI)-- a type of topic modeling -- in estimating both the topic matrix (corresponding to distributions over word frequencies), and the topic assignment matrix. However, these methods are not easily extendable to the incorporation of additional temporal, spatial, or document-specific information, thereby potentially neglecting useful information in the analysis of spatial or longitudinal datasets that can be represented as tensors. Consequently, in this paper, we propose using a modified higher-order singular value decomposition (HOSVD) to estimate topic models based on a Tucker decomposition, thus accommodating the complexity of tensor data. Our method exploits the strength of tensor decomposition in reducing data to lower-dimensional spaces and successfully recovers lower-rank topic and cluster structures, as well as a core tensor that highlights interactions among latent factors. We further characterize explicitly the convergence rate of our method in entry-wise $\ell_1$ norm. Experiments on synthetic data demonstrate the statistical efficiency of our method and its ability to better capture patterns across multiple dimensions. Additionally, our approach also performs well when applied to large datasets of research abstracts and in the analysis of vaginal microbiome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00535v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yating Liu, Claire Donnat</dc:creator>
    </item>
    <item>
      <title>Monty Hall and Optimized Conformal Prediction to Improve Decision-Making with LLMs</title>
      <link>https://arxiv.org/abs/2501.00555</link>
      <description>arXiv:2501.00555v1 Announce Type: cross 
Abstract: Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, they often make overconfident, incorrect predictions, which can be risky in high-stakes settings like healthcare and finance. To mitigate these risks, recent works have used conformal prediction (CP), a model-agnostic framework for distribution-free uncertainty quantification. CP transforms a \emph{score function} into prediction sets that contain the true answer with high probability. While CP provides this coverage guarantee for arbitrary scores, the score quality significantly impacts prediction set sizes. Prior works have relied on LLM logits or other heuristic scores, lacking quality guarantees. We address this limitation by introducing CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Furthermore, inspired by the Monty Hall problem, we extend CP's utility beyond uncertainty quantification to improve accuracy. We propose \emph{conformal revision of questions} (CROQ) to revise the problem by narrowing down the available choices to those in the prediction set. The coverage guarantee of CP ensures that the correct choice is in the revised question prompt with high probability, while the smaller number of choices increases the LLM's chances of answering it correctly. Experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with Gemma-2, Llama-3 and Phi-3 models show that CP-OPT significantly reduces set sizes while maintaining coverage, and CROQ improves accuracy over the standard inference, especially when paired with CP-OPT scores. Together, CP-OPT and CROQ offer a robust framework for improving both the safety and accuracy of LLM-driven decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00555v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harit Vishwakarma, Alan Mishler, Thomas Cook, Niccol\`o Dalmasso, Natraj Raman, Sumitra Ganesh</dc:creator>
    </item>
    <item>
      <title>Robust distribution-free tests for the linear model</title>
      <link>https://arxiv.org/abs/2501.00583</link>
      <description>arXiv:2501.00583v1 Announce Type: cross 
Abstract: Recently, there has been growing concern about heavy-tailed and skewed noise in biological data. We introduce RobustPALMRT, a flexible permutation framework for testing the association of a covariate of interest adjusted for control covariates. RobustPALMRT controls type I error rate for finite-samples, even in the presence of heavy-tailed or skewed noise. The new framework expands the scope of state-of-the-art tests in three directions. First, our method applies to robust and quantile regressions, even with the necessary hyper-parameter tuning. Second, by separating model-fitting and model-evaluation, we discover that performance improves when using a robust loss function in the model-evaluation step, regardless of how the model is fit. Third, we allow fitting multiple models to detect specialized features of interest in a distribution. To demonstrate this, we introduce DispersionPALRMT, which tests for differences in dispersion between treatment and control groups. We establish theoretical guarantees, identify settings where our method has greater power than existing methods, and analyze existing immunological data on Long-COVID patients. Using RobustPALMRT, we unveil novel differences between Long-COVID patients and others even in the presence of highly skewed noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00583v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Torey Hilbert, Steven MacEachern, Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>A novel unit-asymmetric distribution based on correlated Fr\'echet random variables</title>
      <link>https://arxiv.org/abs/2501.00970</link>
      <description>arXiv:2501.00970v1 Announce Type: cross 
Abstract: In this paper, we propose a new distribution with unitary support which can be characterized as a ratio of the type $W=X_1/(X_1+X_2)$, where $(X_1, X_2)^\top$ follows a bivariate extreme distribution with Fr\'echet margins, that is, $X_1$ and $X_2$ are two correlated Fr\'echet random variables. Some mathematical properties such as identifiability, symmetry, stochastic representation, characterization as a ratio, moments, stress-strength probability, quantiles, and the maximum likelihood method are rigorously analyzed. Two applications of the ratio distribution are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Felipe Quintino</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v1 Announce Type: cross 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not leverage surrogate data. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
    <item>
      <title>An Efficient Outlier Detection Algorithm for Data Streaming</title>
      <link>https://arxiv.org/abs/2501.01061</link>
      <description>arXiv:2501.01061v1 Announce Type: cross 
Abstract: The nature of modern data is increasingly real-time, making outlier detection crucial in any data-related field, such as finance for fraud detection and healthcare for monitoring patient vitals. Traditional outlier detection methods, such as the Local Outlier Factor (LOF) algorithm, struggle with real-time data due to the need for extensive recalculations with each new data point, limiting their application in real-time environments. While the Incremental LOF (ILOF) algorithm has been developed to tackle the challenges of online anomaly detection, it remains computationally expensive when processing large streams of data points, and its detection performance may degrade after a certain threshold of points have streamed in. In this paper, we propose a novel approach to enhance the efficiency of LOF algorithms for online anomaly detection, named the Efficient Incremental LOF (EILOF) algorithm. The EILOF algorithm only computes the LOF scores of new points without altering the LOF scores of existing data points. Although exact LOF scores have not yet been computed for the existing points in the new algorithm, datasets often contain noise, and minor deviations in LOF score calculations do not necessarily degrade detection performance. In fact, such deviations can sometimes enhance outlier detection. We systematically tested this approach on both simulated and real-world datasets, demonstrating that EILOF outperforms ILOF as the volume of streaming data increases across various scenarios. The EILOF algorithm not only significantly reduces computational costs, but also systematically improves detection accuracy when the number of additional points increases compared to the ILOF algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01061v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Hu (Zhilu),  Luc (Zhilu),  Chen, Yiwei Wang</dc:creator>
    </item>
    <item>
      <title>Does a Large Language Model Really Speak in Human-Like Language?</title>
      <link>https://arxiv.org/abs/2501.01273</link>
      <description>arXiv:2501.01273v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently emerged, attracting considerable attention due to their ability to generate highly natural, human-like text. This study compares the latent community structures of LLM-generated text and human-written text within a hypothesis testing procedure. Specifically, we analyze three text sets: original human-written texts ($\mathcal{O}$), their LLM-paraphrased versions ($\mathcal{G}$), and a twice-paraphrased set ($\mathcal{S}$) derived from $\mathcal{G}$. Our analysis addresses two key questions: (1) Is the difference in latent community structures between $\mathcal{O}$ and $\mathcal{G}$ the same as that between $\mathcal{G}$ and $\mathcal{S}$? (2) Does $\mathcal{G}$ become more similar to $\mathcal{O}$ as the LLM parameter controlling text variability is adjusted? The first question is based on the assumption that if LLM-generated text truly resembles human language, then the gap between the pair ($\mathcal{O}$, $\mathcal{G}$) should be similar to that between the pair ($\mathcal{G}$, $\mathcal{S}$), as both pairs consist of an original text and its paraphrase. The second question examines whether the degree of similarity between LLM-generated and human text varies with changes in the breadth of text generation. To address these questions, we propose a statistical hypothesis testing framework that leverages the fact that each text has corresponding parts across all datasets due to their paraphrasing relationship. This relationship enables the mapping of one dataset's relative position to another, allowing two datasets to be mapped to a third dataset. As a result, both mapped datasets can be quantified with respect to the space characterized by the third dataset, facilitating a direct comparison between them. Our results indicate that GPT-generated text remains distinct from human-authored text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01273v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mose Park, Yunjin Choi, Jong-June Jeon</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v1 Announce Type: cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Replicating and extending chain-ladder via an age-period-cohort structure on the claim development in a run-off triangle</title>
      <link>https://arxiv.org/abs/2301.03858</link>
      <description>arXiv:2301.03858v3 Announce Type: replace 
Abstract: This paper introduces yet another stochastic model replicating chain-ladder estimates and furthermore considers extensions that add flexibility to the modeling. In its simplest form, the proposed model replicates the chain-ladder's development factors using a GLM model with averaged hazard rates running in reversed development time as response. This is in contrast to the existing reserving literature within the GLM framework where claim amounts are modeled as response. Modeling the averaged hazard rate corresponds to modeling the claim development and is arguably closer to the actual chain-ladder algorithm. Furthermore, since exposure does not need to be modeled, the model only has half the number of parameters compared to when modeling the claim amounts. This lesser complexity can be used to easily introduce model extensions that may better fit the data. We provide a new R-package, $\texttt{clmplus}$, where the models are implemented and can be fed with run-off triangles. We conduct an empirical study on 30 publicly available run-off triangles making a case for the benefit of having $\texttt{clmplus}$ in the actuary's toolbox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03858v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriele Pittarello, Munir Hiabu, Andr\'es M. Villegas</dc:creator>
    </item>
    <item>
      <title>A tree-based model for addressing sparsity and taxa covariance in microbiome compositional count data</title>
      <link>https://arxiv.org/abs/2106.15051</link>
      <description>arXiv:2106.15051v5 Announce Type: replace-cross 
Abstract: Microbiome compositional data are often high-dimensional, sparse, and exhibit pervasive cross-sample heterogeneity. Generative modeling is a popular approach to analyze such data, and effective generative models must accurately characterize these key features. While high-dimensionality and abundance of zeros have received much attention, existing models often lack flexibility in capturing complex cross-sample variability. This limitation can affect statistical efficiency and lead to misleading conclusions in tasks like differential abundance analysis, clustering, and network analysis. We introduce a generative model, the "logistic-tree normal" (LTN) model, which addresses this issue and effectively captures key characteristics of microbiome data, including abundance of zeros. LTN employs a tree-based decomposition to aggregate sparse taxa counts and uses a (multivariate) logistic-normal distribution at tree splits, allowing for flexible covariance adjustments among taxa as needed. The latent Gaussian structure of LTN enables the incorporation of multivariate analysis tools that enforce sparsity or low-rank covariance assumptions. As a versatile, fully generative model, LTN supports a wide range of applications and offers efficient Bayesian inference computational recipes through conjugate blocked Gibbs sampling with P\'olya-Gamma augmentation. We demonstrate application of LTN in a compositional mixed-effects model for differential abundance analysis using numerical experiments and a reanalysis of the infant cohort in the DIABIMMUNE study. Our findings illustrate that LTN, by adequately accounting for cross-sample heterogeneity, appropriately generates the proportion of zeros without requiring an explicit zero-inflation component, confirming a recent viewpoint that "zero-inflation" in count-based sequencing data are often results of unaccounted cross-sample variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15051v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoqun Wang, Jialiang Mao, Li Ma</dc:creator>
    </item>
    <item>
      <title>Random matching in balanced bipartite graphs: The (un)fairness of draw mechanisms used in sports</title>
      <link>https://arxiv.org/abs/2303.09274</link>
      <description>arXiv:2303.09274v5 Announce Type: replace-cross 
Abstract: The draw of some knockout tournaments requires finding a perfect matching in a balanced bipartite graph. The problem becomes challenging with draw constraints: the two field-proven procedures used in sports are known to be non-uniformly distributed (the feasible matchings are not equally likely), which may threaten fairness. We compare the biases of both mechanisms, each of them having two forms, for reasonable subsets of balanced bipartite graphs up to 16 nodes. A mechanism is found to dominate all others in the draw of quarterfinals under reasonable restrictions. The UEFA Champions League Round of 16 draw is verified to apply the best design among the four available options between the 2003/04 and 2023/24 seasons. However, considerable scope remains to improve the performance of these randomisation procedures, especially because they tend to distort the probabilities in the same direction and roughly with the same magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09274v5</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Quantiles of Hidden Biases in Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2309.06459</link>
      <description>arXiv:2309.06459v2 Announce Type: replace-cross 
Abstract: Causal conclusions from observational studies may be sensitive to unmeasured confounding. In such cases, a sensitivity analysis is often conducted, which tries to infer the minimum amount of hidden biases or the minimum strength of unmeasured confounding needed in order to explain away the observed association between treatment and outcome. If the needed bias is large, then the treatment is likely to have significant effects. The Rosenbaum sensitivity analysis is a modern approach for conducting sensitivity analysis in matched observational studies. It investigates what magnitude the maximum of hidden biases from all matched sets needs to be in order to explain away the observed association. However, such a sensitivity analysis can be overly conservative and pessimistic, especially when investigators suspect that some matched sets may have exceptionally large hidden biases. In this paper, we generalize Rosenbaum's framework to conduct sensitivity analysis on quantiles of hidden biases from all matched sets, which are more robust than the maximum. Moreover, the proposed sensitivity analysis is simultaneously valid across all quantiles of hidden biases and is thus a free lunch added to the conventional sensitivity analysis. The proposed approach works for general outcomes, general matched studies and general test statistics. In addition, we demonstrate that the proposed sensitivity analysis also works for bounded null hypotheses when the test statistic satisfies certain properties. An R package implementing the proposed approach is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06459v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongxiao Wu, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v3 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
