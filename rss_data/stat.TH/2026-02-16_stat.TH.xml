<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finding Super-spreaders in SIS Epidemics</title>
      <link>https://arxiv.org/abs/2602.12568</link>
      <description>arXiv:2602.12568v1 Announce Type: new 
Abstract: In network epidemic models, controlling the spread of a disease often requires targeted interventions such as vaccinating high-risk individuals based on network structure. However, typical approaches assume complete knowledge of the underlying contact network, which is often unavailable. While network structure can be learned from observed epidemic dynamics, existing methods require long observation windows that may delay critical interventions.
  In this work, we show that full network reconstruction may not be necessary: control-relevant features, such as high-degree vertices (super-spreaders), can be learned far more efficiently than the complete structure. Specifically, we develop an algorithm to identify such vertices from the dynamics of a Susceptible-Infected-Susceptible (SIS) process. We prove that in an $n$-vertex graph, vertices of degree at least $n^\alpha$ can be identified over an observation window of size $\Omega (1/\alpha)$, for any $\alpha \in (0,1)$. In contrast, existing methods for exact network reconstruction requires an observation window that grows linearly with $n$. Simulations demonstrate that our approach accurately identifies super-spreaders and enables effective epidemic control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12568v1</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirudh Sridhar, Arnob Ghosh</dc:creator>
    </item>
    <item>
      <title>Berry-Esseen Bounds and Moderate Deviations for Catoni-Type Robust Estimation</title>
      <link>https://arxiv.org/abs/2602.12589</link>
      <description>arXiv:2602.12589v1 Announce Type: new 
Abstract: A powerful robust mean estimator introduced by Catoni (2012) allows for mean estimation of heavy-tailed data while achieving the performance characteristics of classical mean estimator for sub-Gaussian data. While Catoni's framework has been widely extended across statistics, stochastic algorithms, and machine learning, fundamental asymptotic questions regarding the Central Limit Theorem and rare event deviations remain largely unaddressed. In this paper, we investigate Catoni-type robust estimators in two contexts: (i) mean estimation for heavy-tailed data, and (ii) linear regression with heavy-tailed innovations. For the first model, we establish the Berry--Esseen bound and moderate deviation principles, addressing both known and unknown variance settings. For the second model, we demonstrate that the associated estimator is consistent and satisfies a multi-dimensional Berry-Esseen bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12589v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhijun Cai, Xiang Li, Lihu Xu</dc:creator>
    </item>
    <item>
      <title>Differentially Private Two-Stage Empirical Risk Minimization and Applications to Individualized Treatment Rule</title>
      <link>https://arxiv.org/abs/2602.12604</link>
      <description>arXiv:2602.12604v1 Announce Type: new 
Abstract: Differential Privacy (DP) provides a rigorous framework for deriving privacy-preserving estimators by injecting calibrated noise to mask individual contributions while preserving population-level insights. Its central challenge lies in the privacy-utility trade-off: calibrating noise levels to ensure robust protection without compromising statistical performance. Standard DP methods struggle with a particular class of two-stage problems prevalent in individualized treatment rules (ITRs) and causal inference. In these settings, data-dependent weights are first computed to satisfy distributional constraints, such as covariate balance, before the final parameter of interest is estimated. Current DP approaches often privatize stages independently, which either degrades weight efficacy-leading to biased and inconsistent estimates-or introduces excessive noise to account for worst-case scenarios.
  To address these challenges, we propose the Differentially Private Two-Stage Empirical Risk Minimization (DP-2ERM), a framework that injects a carefully calibrated noise only into the second stage while maintaining privacy for the entire pipeline and preserving the integrity of the first stage weights. Our theoretical contributions include deterministic bounds on weight perturbations across various widely used weighting methods, and probabilistic bounds on sensitivity for the final estimator. Simulations and real-world applications in ITR demonstrate that DP-2ERM significantly enhances utility over existing methods while providing rigorous privacy guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12604v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joowon Lee, Guanhua Chen</dc:creator>
    </item>
    <item>
      <title>Many-sample tests for the dimensionality hypothesis for large covariance matrices among groups</title>
      <link>https://arxiv.org/abs/2602.12653</link>
      <description>arXiv:2602.12653v1 Announce Type: new 
Abstract: In this paper, we consider procedures for testing hypotheses on the dimension of the linear span generated by a growing number of $p\times p$ covariance matrices from independent $q$ populations. Under a proper limiting scheme where all the parameters, $q$, $p$, and the sample sizes from the $q$ populations, are allowed to increase to infinity, we derive the asymptotic normality of the proposed test statistics. The proposed test procedures show satisfactory performance in finite samples under both the null and the alternative. We also apply the proposed many-sample dimensionality test to investigate a matrix-valued gene dataset from the Mouse Aging Project and gain some new knowledge about its covariance structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12653v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianxing Mei, Chen Wang, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>On the relation between Global VAR Models and Matrix Time Series Models with Multiple Terms</title>
      <link>https://arxiv.org/abs/2602.12710</link>
      <description>arXiv:2602.12710v1 Announce Type: new 
Abstract: Matrix valued time series (MaTS) and global vector autoregressive (GVAR) models both impose restrictions on the general VAR for multidimensional data sets, in order to bring down the number of parameters. Both models are motivated from a different viewpoint such that on first sight they do not have much in common. When investigating the models more closely, however, one notices many connections between the two model sets. This paper investigates the relations between the restrictions imposed by the two models. We show that under appropriate restrictions in both models we obtain a joint framework allowing to gain insight into the nature of GVARs from the viewpoint of MaTS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12710v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dietmar Bauer Kurtulus Kidik</dc:creator>
    </item>
    <item>
      <title>Quantile characterization of univariate unimodality</title>
      <link>https://arxiv.org/abs/2602.12874</link>
      <description>arXiv:2602.12874v1 Announce Type: new 
Abstract: Unimodal univariate distributions can be characterized as piecewise convex-concave cumulative distribution functions. In this note we transfer this shape constraint characterization to the quantile function. We show that this characterization comes with the upside that the quantile function of a unimodal distribution is always absolutely continuous and consequently unimodality is equivalent to the quasi-convexity of its Radon-Nikodym derivative, i.e., the quantile density. Our analysis is based on the theory of generalized inverses of non-decreasing functions and relies on a version of the inverse function rule for non-decreasing functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12874v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Zobel, Axel Munk</dc:creator>
    </item>
    <item>
      <title>When Stein-Type Test Detects Equilibrium Distributions of Finite N-Body Systems</title>
      <link>https://arxiv.org/abs/2602.12297</link>
      <description>arXiv:2602.12297v1 Announce Type: cross 
Abstract: Starting from the probability distribution of finite N-body systems, which maximises the Havrda--Charv\'at entropy, we build a Stein-type goodness-of-fit test. The Maxwell--Boltzmann distribution is exact only in the thermodynamic limit, where the system is composed of infinitely many particles as N approaches infinity. For an isolated system with a finite number of particles, the equilibrium velocity distribution is compact and markedly non-Gaussian, being restricted by the fixed total energy. Using Stein's method, we first obtain a differential operator that characterises the target density. Its eigenfunctions are symmetric Jacobi polynomials, whose orthogonality yields a simple, parameter-free statistic. Under the null hypothesis that the data follows the finite-N distribution, the statistic converges to a chi-squared law, so critical values are available in closed form. Large-scale Monte Carlo experiments confirm exact size control and give a clear picture of the power. These findings quantify how quickly a finite system approaches the classical limit and provide a practical tool for testing kinetic models in regimes where normality cannot be assumed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12297v1</guid>
      <category>math-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae Wan Shim</dc:creator>
    </item>
    <item>
      <title>Reconstruction of finite Quasi-Probability and Probability from Principles: The Role of Syntactic Locality</title>
      <link>https://arxiv.org/abs/2602.12334</link>
      <description>arXiv:2602.12334v1 Announce Type: cross 
Abstract: Quasi-probabilities appear across diverse areas of physics, but their conceptual foundations remain unclear: they are often treated merely as computational tools, and operations like conditioning and Bayes' theorem become ambiguous. We address both issues by developing a principled framework that derives quasi-probabilities and their conditional calculus from structural consistency requirements on how statements are valued across different universes of discourse, understood as finite Boolean algebras of statements.We begin with a universal valuation that assigns definite (possibly complex) values to all statements. The central concept is Syntactic Locality: every universe can be embedded within a larger ambient one, and the universal valuation must behave coherently under such embeddings and restrictions. From a set of structural principles, we prove a representation theorem showing that every admissible valuation can be re-expressed as a finitely additive measure on mutually exclusive statements, mirroring the usual probability sum rule. We call such additive representatives pre-probabilities. This representation is unique up to an additive regraduation freedom. When this freedom can be fixed canonically, pre-probabilities reduce to finite quasi-probabilities, thereby elevating quasi-probability theory from a computational device to a uniquely determined additive representation of universal valuations. Classical finite probabilities arise as the subclass of quasi-probabilities stable under relativisation, i.e., closed under restriction to sub-universes. Finally, the same framework enables us to define a coherent theory of conditionals, yielding a well-defined generalized Bayes' theorem applicable to both pre-probabilities and quasi-probabilities. We conclude by discussing additional regularity conditions, including the role of rational versus irrational probabilities in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12334v1</guid>
      <category>quant-ph</category>
      <category>math.LO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Surace</dc:creator>
    </item>
    <item>
      <title>Linear Regression with Unknown Truncation Beyond Gaussian Features</title>
      <link>https://arxiv.org/abs/2602.12534</link>
      <description>arXiv:2602.12534v1 Announce Type: cross 
Abstract: In truncated linear regression, samples $(x,y)$ are shown only when the outcome $y$ falls inside a certain survival set $S^\star$ and the goal is to estimate the unknown $d$-dimensional regressor $w^\star$. This problem has a long history of study in Statistics and Machine Learning going back to the works of (Galton, 1897; Tobin, 1958) and more recently in, e.g., (Daskalakis et al., 2019; 2021; Lee et al., 2023; 2024). Despite this long history, however, most prior works are limited to the special case where $S^\star$ is precisely known. The more practically relevant case, where $S^\star$ is unknown and must be learned from data, remains open: indeed, here the only available algorithms require strong assumptions on the distribution of the feature vectors (e.g., Gaussianity) and, even then, have a $d^{\mathrm{poly} (1/\varepsilon)}$ run time for achieving $\varepsilon$ accuracy.
  In this work, we give the first algorithm for truncated linear regression with unknown survival set that runs in $\mathrm{poly} (d/\varepsilon)$ time, by only requiring that the feature vectors are sub-Gaussian. Our algorithm relies on a novel subroutine for efficiently learning unions of a bounded number of intervals using access to positive examples (without any negative examples) under a certain smoothness condition. This learning guarantee adds to the line of works on positive-only PAC learning and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12534v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros Kouridakis, Anay Mehrotra, Alkis Kalavasis, Constantine Caramanis</dc:creator>
    </item>
    <item>
      <title>Some bivariate distributions on a discrete torus with application to wind direction datasets</title>
      <link>https://arxiv.org/abs/2602.12842</link>
      <description>arXiv:2602.12842v1 Announce Type: cross 
Abstract: Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12842v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brajesh Kumar Dhakad, Jayant Jha, Debepsita Mukherjee</dc:creator>
    </item>
    <item>
      <title>A unified testing approach for log-symmetry using Fourier methods</title>
      <link>https://arxiv.org/abs/2602.12900</link>
      <description>arXiv:2602.12900v1 Announce Type: cross 
Abstract: Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12900v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ganesh Vishnu Avhad, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Multidimensional Dickman distribution and operator selfdecomposability</title>
      <link>https://arxiv.org/abs/2602.12988</link>
      <description>arXiv:2602.12988v1 Announce Type: cross 
Abstract: The one-dimensional Dickman distribution arises in various stochastic models across number theory, combinatorics, physics, and biology. Recently, a definition of the multidimensional Dickman distribution has appeared in the literature, together with its application to approximating the small jumps of multidimensional L\'evy processes. In this paper, we extend this definition to a class of vector-valued random elements, which we characterise as fixed points of a specific affine transformation involving a random matrix obtained from the matrix exponential of a uniformly distributed random variable. We prove that these new distributions possess the key properties of infinite divisibility and operator selfdecomposability. Furthermore, we identify several cases where this new distribution arises as a limiting distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12988v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasiia S. Kovtun, Nikolai N. Leonenko, Andrey Pepelyshev</dc:creator>
    </item>
    <item>
      <title>Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes</title>
      <link>https://arxiv.org/abs/2602.12992</link>
      <description>arXiv:2602.12992v1 Announce Type: cross 
Abstract: In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12992v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reagan Mozer, Nicole E. Pashley, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Random Forests as Statistical Procedures: Design, Variance, and Dependence</title>
      <link>https://arxiv.org/abs/2602.13104</link>
      <description>arXiv:2602.13104v1 Announce Type: cross 
Abstract: Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed dataset. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13104v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel S. O'Connell</dc:creator>
    </item>
    <item>
      <title>Detecting Parameter Instabilities in Functional Concurrent Linear Regression</title>
      <link>https://arxiv.org/abs/2602.13152</link>
      <description>arXiv:2602.13152v1 Announce Type: cross 
Abstract: We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under H\"older regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13152v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rupsa Basu, Sven Otto</dc:creator>
    </item>
    <item>
      <title>Minmax Trend Filtering: Generalizations of Total Variation Denoising via a Local Minmax/Maxmin Formula</title>
      <link>https://arxiv.org/abs/2410.03041</link>
      <description>arXiv:2410.03041v4 Announce Type: replace 
Abstract: Total Variation Denoising (TVD) is a fundamental denoising and smoothing method. In this article, we identify a new local minmax/maxmin formula producing two estimators which sandwich the univariate TVD estimator at every point. Operationally, this formula gives a local definition of TVD as a minmax/maxmin of a simple function of local averages. Moreover we find that this minmax/maxmin formula is generalizeable and can be used to define other TVD like estimators. In this article we propose and study higher order polynomial versions of TVD which are defined pointwise lying between minmax and maxmin optimizations of penalized local polynomial regressions over intervals of different scales. These appear to be new nonparametric regression methods, different from usual Trend Filtering and any other existing method in the nonparametric regression toolbox. We call these estimators Minmax Trend Filtering (MTF). We show how the proposed local definition of TVD/MTF estimator makes it tractable to bound pointwise estimation errors in terms of a local bias variance like trade-off. This type of local analysis of TVD/MTF is new and arguably simpler than existing analyses of TVD/Trend Filtering. In particular, apart from minimax rate optimality over bounded variation and piecewise polynomial classes, our pointwise estimation error bounds also enable us to derive local rates of convergence for (locally) Holder Smooth signals. These local rates offer a new pointwise explanation of local adaptivity of TVD/MTF instead of global (MSE) based justifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03041v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee</dc:creator>
    </item>
    <item>
      <title>Improving variable selection properties with data integration and transfer learning</title>
      <link>https://arxiv.org/abs/2502.15584</link>
      <description>arXiv:2502.15584v3 Announce Type: replace 
Abstract: We study variable selection (also called support recovery) in high-dimensional sparse linear regression when one has external information on which variables are likely to be associated with the response. Consistent recovery is only possible under somewhat restrictive conditions on sample size, dimension, signal strength, and sparsity. We investigate how these conditions can be relaxed by incorporating said external information. A key application that we consider is structural transfer learning, where variables selected in one or more source datasets are used to guide variable selection in a target dataset. We introduce a family of likelihood penalties that depend on the external information, motivated by connections to Bayesian variable selection. We show that these methods achieve variable selection consistency in regimes where any method ignoring external information fails, and that they achieve consistency at faster rates. We first quantify the potential gains under ideal, oracle-chosen, penalties. We then propose computationally efficient empirical Bayes procedures that learn suitable penalties from the data. We prove that these procedures have improved variable selection properties compared to methods that do not use external information. We illustrate our approach using simulations and a genomics application, where results from mouse experiments are used to inform variable selection for gene expression data in humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15584v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Entropy-Regularized Inference: A Predictive Approach</title>
      <link>https://arxiv.org/abs/2512.21639</link>
      <description>arXiv:2512.21639v2 Announce Type: replace 
Abstract: Predictive inference requires balancing statistical accuracy against informational complexity, yet the choice of complexity measure is usually imposed rather than derived. We treat econometric objects as predictive rules, mappings from information to reported predictive distributions, and impose three structural requirements on evaluation: locality, strict propriety, and coherence under aggregation (coarsening/refinement) of outcome categories. These axioms characterize (uniquely, up to affine transformations) the logarithmic score and induce Shannon mutual information (Kullback-Leibler divergence) as the corresponding measure of predictive complexity. The resulting entropy-regularized prediction problem admits Gibbs-form optimal rules, and we establish an essentially complete-class result for the admissible rules we study under joint risk-complexity dominance. Rational inattention emerges as the constrained dual, corresponding to frontier points with binding information capacity. The entropy penalty contributes additive curvature to the predictive criterion; in weakly identified settings, such as weak instruments in IV regression, where the unregularized objective is flat, this curvature stabilizes the predictive criterion. We derive a local quadratic (LAQ) expansion connecting entropy regularization to classical weak-identification diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21639v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Polson, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns</title>
      <link>https://arxiv.org/abs/2511.21537</link>
      <description>arXiv:2511.21537v2 Announce Type: replace-cross 
Abstract: Real-world problems, for example in climate applications, often require causal reasoning on spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similarly at different Points in space and time, those variations that do exist are relevant twofold: They often encode important information in and of themselves. And they may negatively affect the stability and validity of results if not accounted for. We study the information encoded in changes of the causal graph, with stability in mind. Two core challenges arise, related to the complexity of encoding system-states and to statistical convergence properties in the presence of imperfectly recoverable non-stationary structure. We provide a framework realizing principles conceptually suitable to overcome these challenges - an interpretation supported by numerical experiments. Primarily, we modify constraint-based causal discovery approaches on the level of independence testing. This leads to a framework which is additionally highly modular, easily extensible and widely applicable. For example, it allows to leverage existing constraint-based causal discovery methods (demonstrated on PC, PC-stable, FCI, PCMCI, PCMCI+ and LPCMCI), and to systematically divide the problem into simpler subproblems that are easier to analyze and understand and relate more clearly to well-studied problems like change-point-detection, clustering, independence-testing and more. Code is available at https://github.com/martin-rabel/Causal_GLDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21537v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Martin Rabel, Jakob Runge</dc:creator>
    </item>
  </channel>
</rss>
