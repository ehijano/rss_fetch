<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Arbitrage equilibrium and the emergence of universal microstructure in deep neural networks</title>
      <link>https://arxiv.org/abs/2405.10955</link>
      <description>arXiv:2405.10955v1 Announce Type: new 
Abstract: Despite the stunning progress recently in large-scale deep neural network applications, our understanding of their microstructure, 'energy' functions, and optimal design remains incomplete. Here, we present a new game-theoretic framework, called statistical teleodynamics, that reveals important insights into these key properties. The optimally robust design of such networks inherently involves computational benefit-cost trade-offs that are not adequately captured by physics-inspired models. These trade-offs occur as neurons and connections compete to increase their effective utilities under resource constraints during training. In a fully trained network, this results in a state of arbitrage equilibrium, where all neurons in a given layer have the same effective utility, and all connections to a given layer have the same effective utility. The equilibrium is characterized by the emergence of two lognormal distributions of connection weights and neuronal output as the universal microstructure of large deep neural networks. We call such a network the Jaynes Machine. Our theoretical predictions are shown to be supported by empirical data from seven large-scale deep neural networks. We also show that the Hopfield network and the Boltzmann Machine are the same special case of the Jaynes Machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10955v1</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Venkat Venkatasubramanian, N Sanjeevrajan, Manasi Khandekar, Abhishek Sivaram, Collin Szczepanski</dc:creator>
    </item>
    <item>
      <title>Statistical Mechanics and Artificial Neural Networks: Principles, Models, and Applications</title>
      <link>https://arxiv.org/abs/2405.10957</link>
      <description>arXiv:2405.10957v1 Announce Type: new 
Abstract: The field of neuroscience and the development of artificial neural networks (ANNs) have mutually influenced each other, drawing from and contributing to many concepts initially developed in statistical mechanics. Notably, Hopfield networks and Boltzmann machines are versions of the Ising model, a model extensively studied in statistical mechanics for over a century. In the first part of this chapter, we provide an overview of the principles, models, and applications of ANNs, highlighting their connections to statistical mechanics and statistical learning theory.
  Artificial neural networks can be seen as high-dimensional mathematical functions, and understanding the geometric properties of their loss landscapes (i.e., the high-dimensional space on which one wishes to find extrema or saddles) can provide valuable insights into their optimization behavior, generalization abilities, and overall performance. Visualizing these functions can help us design better optimization methods and improve their generalization abilities. Thus, the second part of this chapter focuses on quantifying geometric properties and visualizing loss functions associated with deep ANNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10957v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas B\"ottcher, Gregory Wheeler</dc:creator>
    </item>
    <item>
      <title>Describing the critical behavior of the Anderson transition in infinite dimension by random-matrix ensembles: logarithmic multifractality and critical localization</title>
      <link>https://arxiv.org/abs/2405.10975</link>
      <description>arXiv:2405.10975v1 Announce Type: new 
Abstract: Due to their analytical tractability, random matrix ensembles serve as robust platforms for exploring exotic phenomena in systems that are computationally demanding. Building on a companion letter [arXiv:2312.17481], this paper investigates two random matrix ensembles tailored to capture the critical behavior of the Anderson transition in infinite dimension, employing both analytical techniques and extensive numerical simulations. Our study unveils two types of critical behaviors: logarithmic multifractality and critical localization. In contrast to conventional multifractality, the novel logarithmic multifractality features eigenstate moments scaling algebraically with the logarithm of the system size. Critical localization, characterized by eigenstate moments of order $q&gt;1/2$ converging to a finite value indicating localization, exhibits characteristic logarithmic finite-size or time effects, consistent with the critical behavior observed in random regular and Erd\"os-R\'enyi graphs of effective infinite dimensionality. Using perturbative methods, we establish the existence of logarithmic multifractality and critical localization in our models. Furthermore, we explore the emergence of novel scaling behaviors in the time dynamics and spatial correlation functions. Our models provide a valuable framework for studying infinite-dimensional quantum disordered systems, and the universality of our findings enables broad applicability to systems with pronounced finite-size effects and slow dynamics, including the contentious many-body localization transition, akin to the Anderson transition in infinite dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10975v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weitao Chen, Olivier Giraud, Jiangbin Gong, Gabriel Lemari\'e</dc:creator>
    </item>
    <item>
      <title>Limit theorems for Randic index for Erd\H{o}s-Renyi graphs</title>
      <link>https://arxiv.org/abs/2405.11097</link>
      <description>arXiv:2405.11097v1 Announce Type: new 
Abstract: We prove that the generalized Randic index over graphs following the Erd\H{o}s-Renyi model, for both the sparse and dense regimes, is concentrated around its mean when the number of vertices tends to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11097v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Eslava, Sayle Sigarreta, Arno Siri-Jegousse</dc:creator>
    </item>
    <item>
      <title>Crumpled-to-flat transition of quenched disordered membranes at two-loop order</title>
      <link>https://arxiv.org/abs/2405.11663</link>
      <description>arXiv:2405.11663v1 Announce Type: new 
Abstract: We investigate the effects of quenched elastic disorder on the nature of the crumpling-to-flat transition of $D$-dimensional polymerized membranes using a two-loop computation near the upper critical dimension $D_c=4$. While the pure system undergoes fluctuation-induced first order transitions below $D_c$ and for an embedding dimension $d&lt;d_{c,pure}\simeq 218.2$, one observes, in presence of disorder, the emergence of various regions of second order governed by a disordered stable fixed point for $d&lt;d_{c1}\sim d_{c,pure}$. This opens the possibility of a new universality class associated with the crumpling-to-flat transition of disordered membranes in $d=3$</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11663v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>hep-th</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. Delzescaux, D. Mouhanna, M. Tissier</dc:creator>
    </item>
    <item>
      <title>Random close packing of binary hard spheres predicts the stability of atomic nuclei</title>
      <link>https://arxiv.org/abs/2405.11268</link>
      <description>arXiv:2405.11268v1 Announce Type: cross 
Abstract: In spite of the empirical success of the Weizsaecker mass formula, a predictive, physics-based, analytical estimate of the stability chart of nuclei has remained elusive. We show that a recent analytical progress in the mathematical description of random close packing of spheres with different sizes provides an excellent description of the $Z$ versus $N$ slope in the nuclides chart. In particular, the theory is able to predict the most quoted ratio $Z/N\approx 0.75$ for stable nuclides, in agreement with experimental observations without any fitting parameter. This solves a 90-year old mystery in nuclear physics: until now, the stability of atomic nuclei was empirically described by the Bethe-Weizsaecker mass formula, which, only thanks to 4-5 adjustable fitting parameters, can capture the tendency of large nuclei to be rich in neutrons. The latter phenomenon was traditionally (qualitatively) explained in terms of a subtle balance between electrostatics and asymmetry effects due to the Pauli exclusion principle. We show that the larger stability of neutron-rich nuclei is the result of a maximized nuclear density, which is controlled by geometric packing considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11268v1</guid>
      <category>nucl-th</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>hep-th</category>
      <category>nucl-ex</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carmine Anzivino, Vinay Vaibhav, Alessio Zaccone</dc:creator>
    </item>
    <item>
      <title>Smooth Kolmogorov Arnold networks enabling structural knowledge representation</title>
      <link>https://arxiv.org/abs/2405.11318</link>
      <description>arXiv:2405.11318v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable alternative to traditional multi-layer perceptron (MLP) architectures due to their finite network topology. However, according to the results of Kolmogorov and Vitushkin, the representation of generic smooth functions by KAN implementations using analytic functions constrained to a finite number of cutoff points cannot be exact. Hence, the convergence of KAN throughout the training process may be limited. This paper explores the relevance of smoothness in KANs, proposing that smooth, structurally informed KANs can achieve equivalence to MLPs in specific function classes. By leveraging inherent structural knowledge, KANs may reduce the data required for training and mitigate the risk of generating hallucinated predictions, thereby enhancing model reliability and performance in computational biomedicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11318v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moein E. Samadi, Younes M\"uller, Andreas Schuppert</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory of in-context learning by linear attention</title>
      <link>https://arxiv.org/abs/2405.11751</link>
      <description>arXiv:2405.11751v1 Announce Type: cross 
Abstract: Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11751v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Application of time-series quantum generative model to financial data</title>
      <link>https://arxiv.org/abs/2405.11795</link>
      <description>arXiv:2405.11795v1 Announce Type: cross 
Abstract: Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems. In this study, a time-series generative model was applied as a quantum generative model to actual financial data. Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression. Furthermore, numerical experiments were performed to complete missing values. Based on the results, we evaluated the practical applications of the time-series quantum generation model. It was observed that fewer parameter values were required compared with the classical method. In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data. These results suggest that several parameters can be applied to various types of time-series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11795v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shun Okumura, Masayuki Ohzeki, Masaya Abe</dc:creator>
    </item>
    <item>
      <title>Nonequilbrium physics of generative diffusion models</title>
      <link>https://arxiv.org/abs/2405.11932</link>
      <description>arXiv:2405.11932v1 Announce Type: cross 
Abstract: Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently. Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory. This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11932v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhendong Yu, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Non-equilibrium orbital edge magnetization</title>
      <link>https://arxiv.org/abs/2405.11979</link>
      <description>arXiv:2405.11979v1 Announce Type: cross 
Abstract: Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current. The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness. This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces. In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction. In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11979v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Voss, I. A. Ado, M. Titov</dc:creator>
    </item>
    <item>
      <title>Stable Attractors for Neural networks classification via Ordinary Differential Equations (SA-nODE)</title>
      <link>https://arxiv.org/abs/2311.10387</link>
      <description>arXiv:2311.10387v2 Announce Type: replace 
Abstract: A novel approach for supervised classification is presented which sits at the intersection of machine learning and dynamical systems theory. At variance with other methodologies that employ ordinary differential equations for classification purposes, the untrained model is a priori constructed to accommodate for a set of pre-assigned stationary stable attractors. Classifying amounts to steer the dynamics towards one of the planted attractors, depending on the specificity of the processed item supplied as an input. Asymptotically the system will hence converge on a specific point of the explored multi-dimensional space, flagging the category of the object to be eventually classified. Working in this context, the inherent ability to perform classification, as acquired ex post by the trained model, is ultimately reflected in the shaped basin of attractions associated to each of the target stable attractors. The performance of the proposed method is here challenged against simple toy models crafted for the purpose, as well as by resorting to well established reference standards. Although this method does not reach the performance of state-of-the-art deep learning algorithms, it illustrates that continuous dynamical systems with closed analytical interaction terms can serve as high-performance classifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10387v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raffaele Marino, Lorenzo Giambagli, Lorenzo Chicchi, Lorenzo Buffoni, Duccio Fanelli</dc:creator>
    </item>
    <item>
      <title>An intriguing connection between the Bardeen-Moshe-Bander phenomenon and 2+p spin glasses</title>
      <link>https://arxiv.org/abs/2404.05436</link>
      <description>arXiv:2404.05436v2 Announce Type: replace 
Abstract: This paper aims to establish a close connection between the Bardeen-Moshe-Bander phenomenon and a p=2+3 spin-glass model with sextic confinement potential. This is made possible by the unconventional power-counting induced by the effective kinetics provided by the disorder coupling in the large N -limit. Because of the absence of epsilon expansion, our approach is more attractive than the previous one and may probably play a relevant role in the signal detection issue in nearly continuous spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05436v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>hep-th</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Lahoche, Dine Ousmane Samary</dc:creator>
    </item>
    <item>
      <title>Statistical Mechanics Calculations Using Variational Autoregressive Networks and Quantum Annealing</title>
      <link>https://arxiv.org/abs/2404.19274</link>
      <description>arXiv:2404.19274v2 Announce Type: replace 
Abstract: In statistical mechanics, computing the partition function is generally difficult. An approximation method using a variational autoregressive network (VAN) has been proposed recently. This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples. The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution. When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19274v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Tamura, Masayuki Ohzeki</dc:creator>
    </item>
    <item>
      <title>Engineered Ordinary Differential Equations as Classification Algorithm (EODECA): thorough characterization and testing</title>
      <link>https://arxiv.org/abs/2312.14681</link>
      <description>arXiv:2312.14681v2 Announce Type: replace-cross 
Abstract: EODECA (Engineered Ordinary Differential Equations as Classification Algorithm) is a novel approach at the intersection of machine learning and dynamical systems theory, presenting a unique framework for classification tasks [1]. This method stands out with its dynamical system structure, utilizing ordinary differential equations (ODEs) to efficiently handle complex classification challenges. The paper delves into EODECA's dynamical properties, emphasizing its resilience against random perturbations and robust performance across various classification scenarios. Notably, EODECA's design incorporates the ability to embed stable attractors in the phase space, enhancing reliability and allowing for reversible dynamics. In this paper, we carry out a comprehensive analysis by expanding on the work [1], and employing a Euler discretization scheme. In particular, we evaluate EODECA's performance across five distinct classification problems, examining its adaptability and efficiency. Significantly, we demonstrate EODECA's effectiveness on the MNIST and Fashion MNIST datasets, achieving impressive accuracies of $98.06\%$ and $88.21\%$, respectively. These results are comparable to those of a multi-layer perceptron (MLP), underscoring EODECA's potential in complex data processing tasks. We further explore the model's learning journey, assessing its evolution in both pre and post training environments and highlighting its ability to navigate towards stable attractors. The study also investigates the invertibility of EODECA, shedding light on its decision-making processes and internal workings. This paper presents a significant step towards a more transparent and robust machine learning paradigm, bridging the gap between machine learning algorithms and dynamical systems methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14681v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>nlin.PS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raffaele Marino, Lorenzo Buffoni, Lorenzo Chicchi, Lorenzo Giambagli, Duccio Fanelli</dc:creator>
    </item>
    <item>
      <title>Optimal input reverberation and homeostatic self-organization towards the edge of synchronization</title>
      <link>https://arxiv.org/abs/2402.05032</link>
      <description>arXiv:2402.05032v2 Announce Type: replace-cross 
Abstract: Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures. Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network. We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches. We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions. We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization. Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition. We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05032v2</guid>
      <category>nlin.AO</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0202743</arxiv:DOI>
      <arxiv:journal_reference>Chaos 34, 053127 (2024)</arxiv:journal_reference>
      <dc:creator>Sue L. Rh\^amidda, Mauricio Girardi-Schappo, Osame Kinouchi</dc:creator>
    </item>
    <item>
      <title>Thermally activated particle motion in biased correlated Gaussian disorder potentials</title>
      <link>https://arxiv.org/abs/2405.09850</link>
      <description>arXiv:2405.09850v2 Announce Type: replace-cross 
Abstract: Thermally activated particle motion in disorder potentials is controlled by the large-$\Delta V$ tail of the distribution of height $\Delta V$ of the potential barriers created by the disorder. We employ the optimal fluctuation method to evaluate this tail for correlated quenched Gaussian potentials in one dimension in the presence of a small bias of the potential. We focus on the mean escape time (MET) of overdamped particles averaged over the disorder. We show that the bias leads to a strong (exponential) reduction of the MET in the direction along the bias. The reduction depends both on the bias, and on detailed properties of the covariance of the disorder, such as its derivatives and asymptotic behavior at large distances. We verify our theoretical predictions, as well as earlier predictions for zero bias, by performing large-deviation simulations of the potential disorder. The simulations employ correlated random potential sampling based on the circulant embedding method and the Wang-Landau algorithm, which enable us to probe probability densities smaller than $10^{-1200}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09850v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Valov, Netanel Levi, Baruch Meerson</dc:creator>
    </item>
  </channel>
</rss>
