<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 01:29:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>$\beta$-integrated local depth and corresponding partitioned local depth representation</title>
      <link>https://arxiv.org/abs/2506.14108</link>
      <description>arXiv:2506.14108v1 Announce Type: new 
Abstract: A novel local depth definition, $\beta$-integrated local depth ($\beta$-ILD), is proposed as a generalization of the local depth introduced by Paindaveine and Van Bever \cite{paindaveine2013depth}, designed to quantify the local centrality of data points. $\beta$-ILD inherits desirable properties from global data depth and remains robust across varying locality levels. A partitioning approach for $\beta$-ILD is introduced, leading to the construction of a matrix that quantifies the contribution of one point to another's local depth, providing a new interpretable measure of local centrality. These concepts are applied to classification and outlier detection tasks, demonstrating significant improvements in the performance of depth-based algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14108v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Complete Characterization for Adjustment in Summary Causal Graphs of Time Series</title>
      <link>https://arxiv.org/abs/2506.14534</link>
      <description>arXiv:2506.14534v1 Announce Type: new 
Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14534v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cl\'ement Yvernes, Emilie Devijver, Eric Gaussier</dc:creator>
    </item>
    <item>
      <title>Differential Privacy and Survey Sampling</title>
      <link>https://arxiv.org/abs/2506.14620</link>
      <description>arXiv:2506.14620v1 Announce Type: new 
Abstract: The Horvitz-Thompson estimate of a total can be seen as as differentially private mechanism applied to this population total. We provide forumlae to compute the $\epsilon$ and $\delta$ parameter for this specific mecanism, coupled or not coupled with the addition of a Laplace or a Gaussian noise. This allows to determine the scale of the Laplace privacy mechanism to be added to reach a specified level of privacy, expressed in terms of $\epsilon,\delta$ differential privacy. In particular, we provide simple formulae for the special case of simple random sampling on binary data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14620v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Bernard Bonn\'ery, Julien Jamme</dc:creator>
    </item>
    <item>
      <title>The Sample Complexity of Distributed Simple Binary Hypothesis Testing under Information Constraints</title>
      <link>https://arxiv.org/abs/2506.13686</link>
      <description>arXiv:2506.13686v1 Announce Type: cross 
Abstract: This paper resolves two open problems from a recent paper, arXiv:2403.16981, concerning the sample complexity of distributed simple binary hypothesis testing under information constraints. The first open problem asks whether interaction reduces the sample complexity of distributed simple binary hypothesis testing. In this paper, we show that sequential interaction does not help. The second problem suggests tightening existing sample complexity bounds for communication-constrained simple binary hypothesis testing. We derive optimally tight bounds for this setting and resolve this problem. Our main technical contributions are: (i) a one-shot lower bound on the Bayes error in simple binary hypothesis testing that satisfies a crucial tensorisation property; (ii) a streamlined proof of the formula for the sample complexity of simple binary hypothesis testing without constraints, first established in arXiv:2403.16981; and (iii) a reverse data-processing inequality for Hellinger-$\lambda$ divergences, generalising the results from arXiv:1812.03031 and arXiv:2206.02765.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13686v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Kazemi, Ankit Pensia, Varun Jog</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects in Extreme and Unobserved Data</title>
      <link>https://arxiv.org/abs/2506.14051</link>
      <description>arXiv:2506.14051v1 Announce Type: cross 
Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14051v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiyuan Tan, Jose Blanchet, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Initial Heat States with Gaussian Series Priors</title>
      <link>https://arxiv.org/abs/2506.14241</link>
      <description>arXiv:2506.14241v1 Announce Type: cross 
Abstract: We consider the statistical linear inverse problem of recovering the unknown initial heat state from noisy interior measurements over an inhomogeneous domain of the solution to the heat equation at a fixed time instant. We employ nonparametric Bayesian procedures with Gaussian series priors defined on the Dirichlet-Laplacian eigenbasis, yielding convenient conjugate posterior distributions with explicit expressions for posterior inference. We review recent theoretical results that provide asymptotic performance guarantees (in the large sample size limit) for the resulting posterior-based point estimation and uncertainty quantification. We further provide an implementation of the approach, and illustrate it via a numerical simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14241v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Non-Parametric Goodness-of-Fit Tests Using Tsallis Entropy Measures</title>
      <link>https://arxiv.org/abs/2506.14242</link>
      <description>arXiv:2506.14242v1 Announce Type: cross 
Abstract: In this paper, we investigate new procedures for statistical testing based on Tsallis entropy, a parametric generalization of Shannon entropy. Focusing on multivariate generalized Gaussian and $q$-Gaussian distributions, we develop entropy-based goodness-of-fit tests based on maximum entropy formulations and nearest neighbour entropy estimators. Furthermore, we propose a novel iterative approach for estimating the shape parameters of the distributions, which is crucial for practical inference. This method extends entropy estimation techniques beyond traditional approaches, improving precision in heavy-tailed and non-Gaussian contexts. The numerical experiments are demonstrative of the statistical properties and convergence behaviour of the proposed tests. These findings are important for disciplines that require robust distributional tests, such as machine learning, signal processing, and information theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14242v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehmet S{\i}dd{\i}k \c{C}ad{\i}rc{\i}</dc:creator>
    </item>
    <item>
      <title>Ole E. Barndorff-Nielsen: Sand, Wind and Inference</title>
      <link>https://arxiv.org/abs/2506.14389</link>
      <description>arXiv:2506.14389v1 Announce Type: cross 
Abstract: This paper reviews Ole Eiler Barndorff-Nielsen's research in the first decades of his career. The focus is on topics that he kept returning to throughout his scientific life, and on papers that he built on in later important contributions. First his early contributions to the foundations of statistical inference are reviewed with focus on conditional inference and exponential families, two topics in which he had a lifelong interest. The second half of the paper reviews his research on wind blown sand and hyperbolic distributions and processes, including his early contributions to modelling of turbulent wind fields. This research laid the foundations for his later work on financial econometrics and ambit processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14389v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters</title>
      <link>https://arxiv.org/abs/2506.14530</link>
      <description>arXiv:2506.14530v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14530v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasis Kratsios, Tin Sum Cheng, Aurelien Lucchi, Haitz S\'aez de Oc\'ariz Borde</dc:creator>
    </item>
    <item>
      <title>Posterior contraction rates of computational methods for Bayesian data assimilation</title>
      <link>https://arxiv.org/abs/2506.14685</link>
      <description>arXiv:2506.14685v1 Announce Type: cross 
Abstract: In this paper, we analyze posterior consistency of a Bayesian data assimilation problem under discretization. We prove convergence rates for the discrete posterior to ground truth solution under both conforming discretization and finite element discretization (usually non-conforming). The analysis is based on the coupling of asymptotics between the number of samples and the dimension of discrete spaces. In the finite element discretization, tailor-made discrete priors, instead of the discretization of continuous priors, are used to generate an optimal convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14685v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Burman, Mingfei Lu</dc:creator>
    </item>
    <item>
      <title>Building A Theoretical Foundation for Combining Negative Controls and Replicates</title>
      <link>https://arxiv.org/abs/2310.01918</link>
      <description>arXiv:2310.01918v2 Announce Type: replace 
Abstract: Studies using assays to quantify the expression of thousands of genes on tens to thousands of cell samples have been carried out for over 20 years. Such assays are based on microarrays, DNA sequencing or other molecular technologies. All such studies involve unwanted variation, often called batch effects, associated with the cell samples and the assay process. Removing this unwanted variation is essential before the measurements can be used to address the questions that motivated the studies. Combining the results of replicate assays with measurements on negative control genes to estimate the unwanted variation and remove it has proved to be effective at this task. The main goal of this paper is to present asymptotic theory that explains this effectiveness. The approach can be widened by using pseudo-replicate sets of pseudo-samples, for use with studies having no replicate assays. Theory covering this case is also presented. The established theory is supported by results of empirical investigations, including simulation studies and a real-data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01918v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiming Jiang, Johann A. Gagnon-Bartsch, Terence P. Speed</dc:creator>
    </item>
    <item>
      <title>Distribution free M-estimation</title>
      <link>https://arxiv.org/abs/2505.22807</link>
      <description>arXiv:2505.22807v3 Announce Type: replace 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22807v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Areces, John C. Duchi</dc:creator>
    </item>
    <item>
      <title>On the attainment of the Wasserstein--Cramer--Rao lower bound</title>
      <link>https://arxiv.org/abs/2506.12732</link>
      <description>arXiv:2506.12732v2 Announce Type: replace 
Abstract: Recently, a Wasserstein analogue of the Cramer--Rao inequality has been developed using the Wasserstein information matrix (Otto metric). This inequality provides a lower bound on the Wasserstein variance of an estimator, which quantifies its robustness against additive noise. In this study, we investigate conditions for an estimator to attain the Wasserstein--Cramer--Rao lower bound (asymptotically), which we call the (asymptotic) Wasserstein efficiency. We show a condition under which Wasserstein efficient estimators exist for one-parameter statistical models. This condition corresponds to a recently proposed Wasserstein analogue of one-parameter exponential families (e-geodesics). We also show that the Wasserstein estimator, a Wasserstein analogue of the maximum likelihood estimator based on the Wasserstein score function, is asymptotically Wasserstein efficient in location-scale families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12732v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hayato Nishimori, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy</title>
      <link>https://arxiv.org/abs/2302.09049</link>
      <description>arXiv:2302.09049v2 Announce Type: replace-cross 
Abstract: We construct multiperiodic processes -- a simple example of stationary ergodic stochastic processes over natural numbers that enjoy the vanishing entropy rate under a mild condition. Multiperiodic processes are supported on randomly shifted deterministic sequences called multiperiodic sequences, which can be efficiently generated using an algorithm called the Infinite Clock. Under a suitable parameterization, multiperiodic sequences exhibit relative frequencies of particular numbers given by Zipf's law. Exactly in the same setting, the respective multiperiodic processes satisfy an asymptotic power-law growth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold for statistical language models, in particular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09049v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz D\k{e}bowski</dc:creator>
    </item>
    <item>
      <title>Testing Goodness-of-Fit for Conditional Distributions: A New Perspective based on Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2403.10352</link>
      <description>arXiv:2403.10352v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel goodness-of-fit test technique for parametric conditional distributions. The proposed tests are based on a residual marked empirical process, for which we develop a conditional Principal Component Analysis. The obtained components provide a basis for various types of new tests in addition to the omnibus one. Component tests that based on each component serve as experts in detecting certain directions. Smooth tests that assemble a few components are also of great use in practice. To further improve testing performance, we introduce a component selection approach, aiming to identify the most contributory components. The finite sample performance of the proposed tests is illustrated through Monte Carlo experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10352v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cui Rui, Li Yuhao</dc:creator>
    </item>
    <item>
      <title>When are dynamical systems learned from time series data statistically accurate?</title>
      <link>https://arxiv.org/abs/2411.06311</link>
      <description>arXiv:2411.06311v2 Announce Type: replace-cross 
Abstract: Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its \emph{physical} behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06311v2</guid>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.DS</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Park, Nicole Yang, Nisha Chandramoorthy</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Bow tie Neural Networks with Shrinkage</title>
      <link>https://arxiv.org/abs/2411.11132</link>
      <description>arXiv:2411.11132v3 Announce Type: replace-cross 
Abstract: Despite the dominant role of deep models in machine learning, limitations persist, including overconfident predictions, susceptibility to adversarial attacks, and underestimation of variability in predictions. The Bayesian paradigm provides a natural framework to overcome such issues and has become the gold standard for uncertainty estimation with deep models, also providing improved accuracy and a framework for tuning critical hyperparameters. However, exact Bayesian inference is challenging, typically involving variational algorithms that impose strong independence and distributional assumptions. Moreover, existing methods are sensitive to the architectural choice of the network. We address these issues by focusing on a stochastic relaxation of the standard feed-forward rectified neural network and using sparsity-promoting priors on the weights of the neural network for increased robustness to architectural design. Thanks to Polya-Gamma data augmentation tricks, which render a conditionally linear and Gaussian model, we derive a fast, approximate variational inference algorithm that avoids distributional assumptions and independence across layers. Suitable strategies to further improve scalability and account for multimodality are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11132v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>A New Proof for the Linear Filtering and Smoothing Equations, and Asymptotic Expansion of Nonlinear Filtering</title>
      <link>https://arxiv.org/abs/2501.16333</link>
      <description>arXiv:2501.16333v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a new asymptotic expansion approach for nonlinear filtering based on a small parameter in the system noise. This method expresses the filtering distribution as a power series in the noise level, where the coefficients can be computed by solving a system of ordinary differential equations. As a result, it addresses the trade-off between computational efficiency and accuracy inherent in existing methods such as Gaussian approximations and particle filters. In the course of our derivation, we also show that classical linear filtering and smoothing equations, namely Kalman-Bucy filter and Rauch-Tung-Striebel smoother, can be obtained in a unified and transparent manner from an explicit formula for the conditional distribution of the hidden path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16333v3</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kurisaki</dc:creator>
    </item>
    <item>
      <title>Generalization error bound for denoising score matching under relaxed manifold assumption</title>
      <link>https://arxiv.org/abs/2502.13662</link>
      <description>arXiv:2502.13662v3 Announce Type: replace-cross 
Abstract: We examine theoretical properties of the denoising score matching estimate. We model the density of observations with a nonparametric Gaussian mixture. We significantly relax the standard manifold assumption allowing the samples step away from the manifold. At the same time, we are still able to leverage a nice distribution structure. We derive non-asymptotic bounds on the approximation and generalization errors of the denoising score matching estimate. The rates of convergence are determined by the intrinsic dimension. Furthermore, our bounds remain valid even if we allow the ambient dimension grow polynomially with the sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13662v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Yakovlev, Nikita Puchkin</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v2 Announce Type: replace-cross 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
  </channel>
</rss>
