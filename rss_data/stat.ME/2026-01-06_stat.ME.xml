<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 02:33:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional</title>
      <link>https://arxiv.org/abs/2601.00904</link>
      <description>arXiv:2601.00904v1 Announce Type: new 
Abstract: Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00904v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Li, Shujian Yu, Liang Ma, Chen Ma, Jingyu Liu, Tulay Adali, Vince D. Calhoun</dc:creator>
    </item>
    <item>
      <title>Beyond P-Values: Importing Quantitative Finance's Risk and Regret Metrics for AI in Learning Health Systems</title>
      <link>https://arxiv.org/abs/2601.01116</link>
      <description>arXiv:2601.01116v1 Announce Type: new 
Abstract: The increasing deployment of artificial intelligence (AI) in clinical settings challenges foundational assumptions underlying traditional frameworks of medical evidence. Classical statistical approaches, centered on randomized controlled trials, frequentist hypothesis testing, and static confidence intervals, were designed for fixed interventions evaluated under stable conditions. In contrast, AI-driven clinical systems learn continuously, adapt their behavior over time, and operate in non-stationary environments shaped by evolving populations, practices, and feedback effects. In such systems, clinical harm arises less from average error rates than from calibration drift, rare but severe failures, and the accumulation of suboptimal decisions over time.
  In this perspective, we argue that prevailing notions of statistical significance are insufficient for characterizing evidence and safety in learning health systems. Drawing on risk-theoretic concepts from quantitative finance and online decision theory, we propose reframing medical evidence for adaptive AI systems in terms of time-indexed calibration stability, bounded downside risk, and controlled cumulative regret. We emphasize that this approach does not replace randomized trials or causal inference, but complements them by addressing dimensions of risk and uncertainty that emerge only after deployment. This framework provides a principled mathematical language for evaluating AI-driven clinical systems under continual learning and offers implications for clinical practice, research design, and regulatory oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01116v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richik Chakraborty</dc:creator>
    </item>
    <item>
      <title>Matrix Decomposition-Based Approach to Estimate the STARTS Model</title>
      <link>https://arxiv.org/abs/2601.01163</link>
      <description>arXiv:2601.01163v1 Announce Type: new 
Abstract: We propose a new estimation method for the Stable Trait, Auto Regressive Trait, and State (STARTS) model, which is well known for its frequent occurrence of improper solutions. The proposed approach is implemented through a two-stage estimation procedure that combines matrix decomposition factor analysis (MDFA) based on eigenvalue decomposition with conventional SEM estimation principles. By reformulating the STARTS model within a factor-analytic framework, this study presents a novel way of applying MDFA in the context of structural equation modeling (SEM). Through a simulation study and an empirical application to ToKyo Teen Cohort data, the proposed method was shown to entail a substantially lower risk of improper solutions than commonly used maximum likelihood, conditional ML, and (unweighted) least squares estimators, while tending to yield solutions similar to those obtained by ML. Compared with Bayesian estimation, the proposed method does not require the specification of appropriate (weakly informative) prior distributions and may effectively mitigate bias issues that arise when the number of time points is small. Applying the proposed method, as well as conducting sensitivity analyses informed by it, will enable researchers to more effectively delineate the range of plausible conclusions from data in estimating the STARTS model and other SEMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01163v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Usami</dc:creator>
    </item>
    <item>
      <title>A Modified Bayesian Criterion for Model Selection in Mixed and Hierarchical Frameworks</title>
      <link>https://arxiv.org/abs/2601.01190</link>
      <description>arXiv:2601.01190v1 Announce Type: new 
Abstract: In this work, we propose a modified Bayesian Information Criterion (BIC) specifically designed for mixture models and hierarchical structures. This criterion incorporates the determinant of the Hessian matrix of the log-likelihood function, thereby refining the classical Bayes Factor by accounting for the curvature of the likelihood surface. Such geometric information introduces a more nuanced penalization for model complexity. The proposed approach improves model selection, particularly under small-sample conditions or in the presence of noise variables. Through theoretical derivations and extensive simulation studies-including both linear and linear mixed models-we show that our criterion consistently outperforms traditional methods such as BIC, Akaike Information Criterion (AIC), and related variants. The results suggest that integrating curvature-based information from the likelihood landscape leads to more robust and accurate model discrimination in complex data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01190v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogenes de Jesus Ramirez, Anderson Melchor Hernandez, Isabel Cristina Ramirez, Luis Ra\'ul Pericchi</dc:creator>
    </item>
    <item>
      <title>A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data</title>
      <link>https://arxiv.org/abs/2601.01259</link>
      <description>arXiv:2601.01259v2 Announce Type: new 
Abstract: Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01259v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Taiane Schaedler Prass, Douglas Krauthein Verdum</dc:creator>
    </item>
    <item>
      <title>Adaptive Kernel Regression for Constrained Route Alignment: Theory and Iterative Data Sharpening</title>
      <link>https://arxiv.org/abs/2601.01344</link>
      <description>arXiv:2601.01344v1 Announce Type: new 
Abstract: Route alignment design in surveying and transportation engineering frequently involves fixed waypoint constraints, where a path must precisely traverse specific coordinates. While existing literature primarily relies on geometric optimization or control-theoretic spline frameworks, there is a lack of systematic statistical modeling approaches that balance global smoothness with exact point adherence. This paper proposes an Adaptive Nadaraya-Watson (ANW) kernel regression estimator designed to address the fixed waypoint problem. By incorporating waypoint-specific weight tuning parameters, the ANW estimator decouples global smoothing from local constraint satisfaction, avoiding the "jagged" artifacts common in naive local bandwidth-shrinking strategies. To further enhance estimation accuracy, we develop an iterative data sharpening algorithm that systematically reduces bias while maintaining the stability of the kernel framework. We establish the theoretical foundation for the ANW estimator by deriving its asymptotic bias and variance and proving its convergence properties under the internal constraint model. Numerical case studies in 1D and 2D trajectory planning demonstrate that the method effectively balances root mean square error (RMSE) and curvature smoothness. Finally, we validate the practical utility of the framework through empirical applications to railway and highway route planning. In sum, this work provides a stable, theoretically grounded, and computationally efficient solution for complex, constrained alignment design problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01344v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyin Du, Yiting Chen, Wenzhi Yang, Qiong Li, Xiaoping Shi</dc:creator>
    </item>
    <item>
      <title>Unsupervised dense random survival forests identify interpretable patient profiles with heterogeneous treatment benefit</title>
      <link>https://arxiv.org/abs/2601.01380</link>
      <description>arXiv:2601.01380v1 Announce Type: new 
Abstract: Precision oncology aims to prescribe the optimal cancer treatment to the right patients, maximizing therapeutic benefits. However, identifying patient subgroups that may benefit more from experimental cancer treatments based on randomized clinical trials presents a significant analytical challenge. To address this, we introduce a novel unsupervised machine learning approach based on very dense random survival forests (up to 100,000 trees), equipped with a new splitting rule that explicitly targets treatment-effect heterogeneity. This method is robust, interpretable, and effectively identifies responsive subgroups. Extensive simulations confirm its ability to detect heterogeneous patient responses and distinguish between datasets with and without heterogeneity, while maintaining a stringent Type I error rate of 1%. We further validate its performance using Phase III randomized clinical trial datasets, demonstrating significant patient heterogeneity in treatment response based on baseline characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01380v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Li, Qing Liu, Tony Jiang, Hong Amy Xia, Peng Wei, Brian P. Hobbs</dc:creator>
    </item>
    <item>
      <title>Personalizing black-box models for nonparametric regression with minimax optimality</title>
      <link>https://arxiv.org/abs/2601.01432</link>
      <description>arXiv:2601.01432v1 Announce Type: new 
Abstract: Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01432v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Li, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Reduced-Rank Autoregressive Model for High-Dimensional Multivariate Network Time Series</title>
      <link>https://arxiv.org/abs/2601.01510</link>
      <description>arXiv:2601.01510v1 Announce Type: new 
Abstract: Multivariate network time series are ubiquitous in modern systems, yet existing network autoregressive models typically treat nodes as scalar processes, ignoring cross-variable spillovers. To capture these complex interactions without the curse of dimensionality, we propose the Reduced-Rank Network Autoregressive (RRNAR) model. Our framework introduces a separable bilinear transition structure that couples the known network topology with a learnable low-rank variable subspace. We estimate the model using a novel Scaled Gradient Descent (ScaledGD) algorithm, explicitly designed to bridge the gap between rigid network scalars and flexible factor components. Theoretically, we establish non-asymptotic error bounds under a novel distance metric. A key finding is a network-induced blessing of dimensionality: for sparse networks, the estimation accuracy for network parameters improves as the network size grows. Applications to traffic and server monitoring networks demonstrate that RRNAR significantly outperforms univariate and unstructured benchmarks by identifying latent cross-channel propagation mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01510v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Lyu, Xiaoyu Zhang, Guodong Li, Di Wang</dc:creator>
    </item>
    <item>
      <title>Cubic lower record-based transmuted family of distributions: Theory, Estimation, Applications</title>
      <link>https://arxiv.org/abs/2601.01583</link>
      <description>arXiv:2601.01583v1 Announce Type: new 
Abstract: In this study, a family of distributions called cubic lower record-based transmuted is provided. A special case of this family is proposed as an alternative exponential distribution. Several statistical properties are explored. We utilize nine different methods to estimate the parameters of the suggested distribution. In order to compare the performances of these methods, we consider a comprehensive Monte-Carlo simulation study. As a result of simulation study, we conclude that minimum absolute distance estimator is a valuable alternative to maximum likelihood estimator. Then, we carried out two real-world data examples to evaluate the fits of introduced distribution as well as its potential competitor ones. The findings of real-world data analysis show that the best-fitting distribution for both datasets is our model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01583v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}</dc:creator>
    </item>
    <item>
      <title>Wasserstein Distributionally Robust Rare-Event Simulation</title>
      <link>https://arxiv.org/abs/2601.01642</link>
      <description>arXiv:2601.01642v1 Announce Type: new 
Abstract: Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01642v1</guid>
      <category>stat.ME</category>
      <category>q-fin.CP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dohyun Ahn, Huiyi Chen, Lewen Zheng</dc:creator>
    </item>
    <item>
      <title>Predictive Assessment and Comparison of Bayesian Survival Models for Cancer Recurrence</title>
      <link>https://arxiv.org/abs/2601.01662</link>
      <description>arXiv:2601.01662v1 Announce Type: new 
Abstract: Complex data features, such as unmodelled censored event times and variables with time-dependent effects, are common in cancer recurrence studies and pose challenges for Bayesian survival modelling. However, current methodologies for predictive model checking and comparison often fail to adequately address these features. This paper bridges that gap by introducing new, targeted recommendations for predictive assessment and comparison of Bayesian survival models for cancer recurrence. Our recommendations cover a variety of different scenarios and models. Accompanying code together with our implementations to open source software help in replicating the results and applying our recommendations in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01662v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saku Suorsa, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Bayesian mortality forecasting with a Conway--Maxwell--Poisson specification</title>
      <link>https://arxiv.org/abs/2601.01686</link>
      <description>arXiv:2601.01686v1 Announce Type: new 
Abstract: This paper presents a novel approach to stochastic mortality modelling by using the Conway--Maxwell--Poisson (CMP) distribution to model death counts. Unlike standard Poisson or negative binomial distributions, the CMP is a more adaptable choice because it can account for different levels of variability in the data, a feature known as dispersion. Specifically, it can handle data that are underdispersed (less variable than expected), equidispersed (as variable as expected), and overdispersed (more variable than expected). We develop a Bayesian formulation that treats the dispersion level as an unknown parameter, using a Gamma prior to enable a robust and coherent integration of the parameter, process, and distributional uncertainty. The model is calibrated using Markov chain Monte Carlo (MCMC) methods, with model performance evaluated using standard statistical criteria such as residual analysis and scoring rules. An empirical study using England and Wales male mortality data shows that our CMP-based models provide a better fit for both existing data and future predictions compared to traditional Poisson and negative binomial models, particularly when the data exhibit overdispersion. Finally, we conduct a sensitivity analysis with respect to prior specification to assess robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01686v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackie Siaw Tze Wong, Emiliano A. Valdez</dc:creator>
    </item>
    <item>
      <title>Varying-Coefficient Mixture of Experts Model</title>
      <link>https://arxiv.org/abs/2601.01699</link>
      <description>arXiv:2601.01699v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01699v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qicheng Zhao, Celia M. T. Greenwood, Qihuang Zhang</dc:creator>
    </item>
    <item>
      <title>On regional treatment effect assessment using robust MAP priors</title>
      <link>https://arxiv.org/abs/2601.01811</link>
      <description>arXiv:2601.01811v1 Announce Type: new 
Abstract: Bayesian dynamic borrowing has become an increasingly important tool for evaluating the consistency of regional treatment effects which is a key requirement for local regulatory approval of a new drug. It helps increase the precision of regional treatment effect estimate when regional and global data are similar, while guarding against potential bias when they differ. In practice, the two-component mixture prior, of which one mixture component utilizes the power prior to incorporate external data, is widely used. It allows convenient prior specification, analytical posterior computation, and fast evaluation of operating characteristics. Though the robust meta-analytical-predictive (MAP) prior is broadly used with multiple external data sources, it remains underutilized for regional treatment effect assessment (typically only one external data source is available) due to its inherit complexity in prior specification and posterior computation. In this article, we illustrate the applicability of the robust MAP prior in the regional treatment effect assessment by developing a closed-form approximation for its posterior distribution while leveraging its relationship with the power prior. The proposed methodology substantially reduces the computational burden of identifying prior parameters for desired operating characteristics. Moreover, we have demonstrated that the MAP prior is an attractive choice to construct the informative component of the mixture prior compared to the power prior. The advantage can be explained through a Bayesian hypothesis testing perspective. Using a real-world example, we illustrate how our proposed method enables efficient and transparent development of a Bayesian dynamic borrowing design to show regional consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01811v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Hui Zhang, Satrajit Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal modeling and forecasting with Fourier neural operators</title>
      <link>https://arxiv.org/abs/2601.01813</link>
      <description>arXiv:2601.01813v1 Announce Type: new 
Abstract: Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01813v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Nag, Andrew Zammit-Mangion, Sumeetpal Singh, Noel Cressie</dc:creator>
    </item>
    <item>
      <title>Causal Network Recovery in Perturb-seq Experiments Using Proxy and Instrumental Variables</title>
      <link>https://arxiv.org/abs/2601.01830</link>
      <description>arXiv:2601.01830v1 Announce Type: new 
Abstract: Emerging single-cell technologies that integrate CRISPR-based genetic perturbations with single-cell RNA sequencing, such as Perturb-seq, have substantially advanced our understanding of gene regulation and causal influence of genes. While Perturb-seq data provide valuable causal insights into gene-gene interactions, statistical concerns remain regarding unobserved confounders that may bias inference. These latent factors may arise not only from intrinsic molecular features of regulatory elements encoded in Perturb-seq experiments, but also from unobserved genes arising from cost-constrained experimental designs. Although methods for analyzing largescale Perturb-seq data are rapidly maturing, approaches that explicitly account for such unobserved confounders in learning the causal gene networks are still lacking. Here, we propose a novel method to recover causal gene networks from Perturb-seq experiments with robustness to arbitrarily omitted confounders. Our framework leverages proxy and instrumental variable strategies to exploit the rich information embedded in perturbations, enabling unbiased estimation of the underlying directed acyclic graph (DAG) of gene expressions. Simulation studies and analyses of CRISPR interference experiments of K562 cells demonstrate that our method outperforms baseline approaches that ignore unmeasured confounding, yielding more accurate and biologically relevant recovery of the true gene causal DAGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kwangmoon Park, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Causal inference for censored data with continuous marks</title>
      <link>https://arxiv.org/abs/2601.01854</link>
      <description>arXiv:2601.01854v1 Announce Type: new 
Abstract: This paper presents a framework for causal inference in the presence of censored data, where the failure time is marked by a continuous variable known as a mark. The mark can be viewed as an extension of the failure cause in the classical competing risks model where the cause of failure is replaced by a continuous mark only observed at uncensored failure times. Due to the continuous nature of the marks, observations at each specific mark are sparse, making the identification and estimation of causality a challenging task. To address this issue, we define a new mark-specific treatment effect within the potential outcomes framework and characterize its identifying conditions. We then propose a local smoothing causal estimand and establish its asymptotic properties. We evaluate our method using simulation studies as well as a real dataset from the Antibody Mediated Prevention trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01854v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lianqiang Qu, Long Lv, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Simulation of warping processes with applications to temperature data</title>
      <link>https://arxiv.org/abs/2601.02154</link>
      <description>arXiv:2601.02154v1 Announce Type: new 
Abstract: Curve registration plays a major role in functional data analysis by separating amplitude and phase variation through warping functions and the accurate simulation of warping processes is essential for developing statistical methods that properly account for phase variability in functional data. In this paper, we focus on the simulation of continuous warping processes with a prescribed expectation and a controllable variance. We study and compare three procedures, including two existing methods and a new algorithm based on randomized empirical cumulative distribution functions. For each approach, we provide an operational description and establish theoretical results for the first two moments of the simulated processes. A numerical study illustrates the theoretical findings and highlights the respective merits of the three methods. Finally, we present an application to the analysis of temperature distributions in Montreal based on simulated realizations from a warping process estimated from temperature quantile functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02154v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nolwenn Le M\'ehaut\'e (SVH), Jean-Fran\c{c}ois Coeurjolly (SVH), Marie-H\'el\`ene Descary (UQAM)</dc:creator>
    </item>
    <item>
      <title>A neighbour selection approach for identifying differential networks in conditional functional graphical models</title>
      <link>https://arxiv.org/abs/2601.02292</link>
      <description>arXiv:2601.02292v1 Announce Type: new 
Abstract: Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02292v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Mapelli, Laura Carini, Francesca Ieva, Sara Sommariva</dc:creator>
    </item>
    <item>
      <title>Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction</title>
      <link>https://arxiv.org/abs/2601.02322</link>
      <description>arXiv:2601.02322v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02322v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuozhi Zuo, Yixin Wang</dc:creator>
    </item>
    <item>
      <title>Hierarchical topological clustering</title>
      <link>https://arxiv.org/abs/2601.00892</link>
      <description>arXiv:2601.00892v1 Announce Type: cross 
Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00892v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Carpio, Gema Duro</dc:creator>
    </item>
    <item>
      <title>Quotient EM under Misspecification:Tight Local Rates and Finite-Sample Bounds in General Integral Probability Metrics</title>
      <link>https://arxiv.org/abs/2601.01051</link>
      <description>arXiv:2601.01051v1 Announce Type: cross 
Abstract: We study the expectation-maximization (EM) algorithm for general latent-variable models under (i) distributional misspecification and (ii) nonidentifiability induced by a group action. We formulate EM on the quotient parameter space and measure error using an arbitrary integral probability metric (IPM). Our main results give (a) a sharp local linear convergence rate for population EM governed by the spectral radius of the linearization on a local slice, and (b) tight finite-sample bounds for sample EM obtained via perturbed contraction inequalities and generic chaining/entropy control of EM-induced empirical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01051v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Koustav Mallik</dc:creator>
    </item>
    <item>
      <title>Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events</title>
      <link>https://arxiv.org/abs/2601.01245</link>
      <description>arXiv:2601.01245v1 Announce Type: cross 
Abstract: This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01245v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>q-bio.TO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyuan Huang, Ling Zhou, Min Zhang, Peter X. K. Song</dc:creator>
    </item>
    <item>
      <title>SGD with Dependent Data: Optimal Estimation, Regret, and Inference</title>
      <link>https://arxiv.org/abs/2601.01371</link>
      <description>arXiv:2601.01371v1 Announce Type: cross 
Abstract: This work investigates the performance of the final iterate produced by stochastic gradient descent (SGD) under temporally dependent data. We consider two complementary sources of dependence: $(i)$ martingale-type dependence in both the covariate and noise processes, which accommodates non-stationary and non-mixing time series data, and $(ii)$ dependence induced by sequential decision making. Our formulation runs in parallel with classical notions of (local) stationarity and strong mixing, while neither framework fully subsumes the other. Remarkably, SGD is shown to automatically accommodate both independent and dependent information under a broad class of stepsize schedules and exploration rate schemes.
  Non-asymptotically, we show that SGD simultaneously achieves statistically optimal estimation error and regret, extending and improving existing results. In particular, our tail bounds remain sharp even for potentially infinite horizon $T=+\infty$. Asymptotically, the SGD iterates converge to a Gaussian distribution with only an $O_{\PP}(1/\sqrt{t})$ remainder, demonstrating that the supposed estimation-regret trade-off claimed in prior work can in fact be avoided. We further propose a new ``conic'' approximation of the decision region that allows the covariates to have unbounded support. For online sparse regression, we develop a new SGD-based algorithm that uses only $d$ units of storage and requires $O(d)$ flops per iteration, achieving the long term statistical optimality. Intuitively, each incoming observation contributes to estimation accuracy, while aggregated summary statistics guide support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01371v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Yichen Zhang, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Monte Carlo for (Physics) Dummies</title>
      <link>https://arxiv.org/abs/2601.01422</link>
      <description>arXiv:2601.01422v1 Announce Type: cross 
Abstract: Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01422v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arghya Mukherjee, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations</title>
      <link>https://arxiv.org/abs/2601.01442</link>
      <description>arXiv:2601.01442v1 Announce Type: cross 
Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01442v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongrong Li, Tianwei Yu, Xiaodan Fan</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning of Continuous Treatment Effects with General Instrumental Variables</title>
      <link>https://arxiv.org/abs/2601.01471</link>
      <description>arXiv:2601.01471v1 Announce Type: cross 
Abstract: Estimating causal effects of continuous treatments is a common problem in practice, for example, in studying dose-response functions. Classical analyses typically assume that all confounders are fully observed, whereas in real-world applications, unmeasured confounding often persists. In this article, we propose a novel framework for local identification of dose-response functions using instrumental variables, thereby mitigating bias induced by unobserved confounders. We introduce the concept of a uniform regular weighting function and consider covering the treatment space with a finite collection of open sets. On each of these sets, such a weighting function exists, allowing us to identify the dose-response function locally within the corresponding region. For estimation, we develop an augmented inverse probability weighting score for continuous treatments under a debiased machine learning framework with instrumental variables. We further establish the asymptotic properties when the dose-response function is estimated via kernel regression or empirical risk minimization. Finally, we conduct both simulation and empirical studies to assess the finite-sample performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01471v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Chen, Peng Zhang, Yifan Cui</dc:creator>
    </item>
    <item>
      <title>On lead-lag estimation of non-synchronously observed point processes</title>
      <link>https://arxiv.org/abs/2601.01871</link>
      <description>arXiv:2601.01871v1 Announce Type: cross 
Abstract: This paper introduces a new theoretical framework for analyzing lead-lag relationships between point processes, with a special focus on applications to high-frequency financial data. In particular, we are interested in lead-lag relationships between two sequences of order arrival timestamps. The seminal work of Dobrev and Schaumburg proposed model-free measures of cross-market trading activity based on cross-counts of timestamps. While their method is known to yield reliable results, it faces limitations because its original formulation inherently relies on discrete-time observations, an issue we address in this study. Specifically, we formulate the problem of estimating lead-lag relationships in two point processes as that of estimating the shape of the cross-pair correlation function (CPCF) of a bivariate stationary point process, a quantity well-studied in the neuroscience and spatial statistics literature. Within this framework, the prevailing lead-lag time is defined as the location of the CPCF's sharpest peak. Under this interpretation, the peak location in Dobrev and Schaumburg's cross-market activity measure can be viewed as an estimator of the lead-lag time in the aforementioned sense. We further propose an alternative lead-lag time estimator based on kernel density estimation and show that it possesses desirable theoretical properties and delivers superior numerical performance. Empirical evidence from high-frequency financial data demonstrates the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01871v1</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Shiotani, Takaki Hayashi, Yuta Koike</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2601.02305</link>
      <description>arXiv:2601.02305v1 Announce Type: cross 
Abstract: Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02305v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Didong Li, Aritra Halder, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Semismooth Newton Augmented Lagrangian Algorithm for Adaptive Lasso Penalized Least Squares in Semiparametric Regression</title>
      <link>https://arxiv.org/abs/2111.10766</link>
      <description>arXiv:2111.10766v3 Announce Type: replace 
Abstract: This paper is concerned with a partially linear semiparametric regression model containing an unknown regression coefficient, an unknown nonparametric function, and an unobservable Gaussian distributed random error. We focus on the case of simultaneous variable selection and estimation with a divergent number of covariates under the assumption that the regression coefficient is sparse. We consider the applications of the least squares to semiparametric regression and particularly present an adaptive lasso penalized least squares (PLS) method to select the regression coefficient. We note that there are many algorithms for PLS in various applications, but they seem to be rarely used in semiparametric regression. This paper focuses on using a semismooth Newton augmented Lagrangian (SSNAL) algorithm to solve the dual of PLS which is the sum of a smooth strongly convex function and an indicator function. At each iteration, there must be a strongly semismooth nonlinear system, which can be solved by semismooth Newton by making full use of the penalized term. We show that the algorithm offers a significant computational advantage, and the semismooth Newton method admits fast local convergence rate. Numerical experiments on simulated and real data have demonstrated the effectiveness of the PLS method and the progressiveness of the SSNAL algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10766v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peili Li, Yunhai Xiao, Meixia Yang, Hanbing Zhu</dc:creator>
    </item>
    <item>
      <title>MML Probabilistic Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2209.14559</link>
      <description>arXiv:2209.14559v3 Announce Type: replace 
Abstract: Principal component analysis (PCA) is perhaps the most widely used method for data dimensionality reduction. A key question in PCA is deciding how many factors to retain. This manuscript describes a new approach to automatically selecting the number of principal components based on the Bayesian minimum message length method of inductive inference. We derive a new estimate of the isotropic residual variance and demonstrate that it improves on the usual maximum likelihood approach. We also discuss extending this approach to finite mixture models of principal component analyzers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14559v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enes Makalic, Daniel F. Schmidt</dc:creator>
    </item>
    <item>
      <title>Data integration using covariate summaries from external sources</title>
      <link>https://arxiv.org/abs/2411.15691</link>
      <description>arXiv:2411.15691v3 Announce Type: replace 
Abstract: In modern data analysis, information is frequently collected from multiple sources, often leading to challenges such as data heterogeneity and imbalanced sample sizes across datasets. Robust and efficient data integration methods are crucial for improving the generalization and transportability of statistical findings. In this work, we address scenarios where, in addition to having full access to individualized data from a primary source, supplementary covariate information from external sources is also available. While traditional data integration methods typically require individualized covariates from external sources, such requirements can be impractical due to limitations related to accessibility, privacy, storage, and cost. Instead, we propose novel data integration techniques that rely solely on external summary statistics, such as sample means and covariances, to construct robust estimators for the mean outcome under both homogeneous and heterogeneous data settings. Additionally, we extend this framework to causal inference, enabling the estimation of average treatment effects for both generalizability and transportability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15691v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Facheng Yu, Zhen Qi, Yuqian Zhang</dc:creator>
    </item>
    <item>
      <title>Inside Out: Externalizing Assumptions in Data Analysis as Validation Checks</title>
      <link>https://arxiv.org/abs/2501.04296</link>
      <description>arXiv:2501.04296v2 Announce Type: replace 
Abstract: In data analysis, unexpected results often prompt researchers to revisit their procedures to identify potential issues. While some researchers may struggle to identify the root causes, experienced researchers can often quickly diagnose problems by checking a few key assumptions. These checked assumptions, or expectations, are typically informal, difficult to trace, and rarely discussed in publications. In this paper, we introduce the term *analysis validation checks* to formalize and externalize these informal assumptions. We then introduce a procedure to identify a subset of checks that best predict the occurrence of unexpected outcomes, based on simulations of the original data. The checks are evaluated in terms of accuracy, determined by binary classification metrics, and independence, which measures the shared information among checks. We demonstrate this approach with a toy example using step count data and a generalized linear model example examining the effect of particulate matter air pollution on daily mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04296v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Data Science, 2026</arxiv:journal_reference>
      <dc:creator>H. Sherry Zhang, Roger D. Peng</dc:creator>
    </item>
    <item>
      <title>Exploratory Hierarchical Factor Analysis with an Application to Psychological Measurement</title>
      <link>https://arxiv.org/abs/2505.09043</link>
      <description>arXiv:2505.09043v3 Announce Type: replace 
Abstract: Hierarchical factor models, which include the bifactor model as a special case, are useful in social and behavioural sciences for measuring hierarchically structured constructs. Specifying a hierarchical factor model involves imposing hierarchically structured zero constraints on a factor loading matrix, which is often challenging. Therefore, an exploratory analysis is needed to learn the hierarchical factor structure from data. Unfortunately, there does not exist an identifiability theory for the learnability of this hierarchical structure, nor a computationally efficient method with provable performance. The method of Schmid-Leiman transformation, which is often regarded as the default method for exploratory hierarchical factor analysis, is flawed and likely to fail. The contribution of this paper is three-fold. First, an identifiability result is established for general hierarchical factor models, which shows that the hierarchical factor structure is learnable under mild regularity conditions. Second, a computationally efficient divide-and-conquer approach is proposed for learning the hierarchical factor structure. Finally, asymptotic theory is established for the proposed method, showing that it can consistently recover the true hierarchical factor structure as the sample size grows to infinity. The power of the proposed method is shown via simulation studies and a real data application to a personality test. The computation code for the proposed method is publicly available at https://github.com/EmetSelch97/EHFA/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09043v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Constrained Regression</title>
      <link>https://arxiv.org/abs/2512.12953</link>
      <description>arXiv:2512.12953v2 Announce Type: replace 
Abstract: We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12953v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhav Sankaranarayanan, Yana Hrytsenko, Jerome I. Rotter, Tamar Sofer, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Nonparametric Survival Estimation with Contaminated and Adjudicated Events</title>
      <link>https://arxiv.org/abs/2512.14959</link>
      <description>arXiv:2512.14959v2 Announce Type: replace 
Abstract: We study the conditional expert Kaplan-Meier estimator, an extension of the classical Kaplan--Meier estimator designed for time-to-event data subject to both right-censoring and contamination. Such contamination, where observed events may not reflect true outcomes, is common in applied settings, including insurance and credit risk, where expert opinion is often used to adjudicate uncertain events. Building on previous work, we develop a comprehensive asymptotic theory for the conditional version incorporating covariates through kernel smoothing. We establish functional consistency and weak convergence under suitable regularity conditions and quantify the bias induced by imperfect expert information. The results show that unbiased expert judgments ensure consistency, while systematic deviations lead to a deterministic asymptotic bias that can be explicitly characterized. We examine finite-sample properties through simulation studies and illustrate the practical use of the estimator with an application to loan default data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14959v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Kristian Vilhelm Dinesen</dc:creator>
    </item>
    <item>
      <title>On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective</title>
      <link>https://arxiv.org/abs/2304.13836</link>
      <description>arXiv:2304.13836v4 Announce Type: replace-cross 
Abstract: Approaches for appraising feature importance approximations, alternatively referred to as attribution methods, have been established across an extensive array of contexts. The development of resilient techniques for performance benchmarking constitutes a critical concern in the sphere of explainable deep learning. This study scrutinizes the dependability of the RemOve-And-Retrain (ROAR) procedure, which is prevalently employed for gauging the performance of feature importance estimates. The insights gleaned from our theoretical foundation and empirical investigations reveal that attributions containing lesser information about the decision function may yield superior results in ROAR benchmarks, contradicting the original intent of ROAR. This occurrence is similarly observed in the recently introduced variant RemOve-And-Debias (ROAD), and we posit a persistent pattern of blurriness bias in ROAR attribution metrics. Our findings serve as a warning against indiscriminate use on ROAR metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13836v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhwa Song, Keumgang Cha, Junghoon Seo</dc:creator>
    </item>
    <item>
      <title>Echo State Networks for Spatio-Temporal Area-Level Data</title>
      <link>https://arxiv.org/abs/2410.10641</link>
      <description>arXiv:2410.10641v2 Announce Type: replace-cross 
Abstract: Spatio-temporal area-level datasets play a critical role in official statistics, providing valuable insights for policy-making and regional planning. Accurate modeling and forecasting of these datasets can be extremely useful for policymakers to develop informed strategies for future planning. Echo State Networks (ESNs) are efficient methods for capturing nonlinear temporal dynamics and generating forecasts. However, ESNs lack a direct mechanism to account for the neighborhood structure inherent in area-level data. Ignoring these spatial relationships can significantly compromise the accuracy and utility of forecasts. In this paper, we incorporate approximate graph spectral filters at the input stage of the ESN, thereby improving forecast accuracy while preserving the model's computational efficiency during training. We demonstrate the effectiveness of our approach using Eurostat's tourism occupancy dataset and show how it can support more informed decision-making in policy and planning contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10641v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenhua Wang, Scott H. Holan, Christopher K. Wikle</dc:creator>
    </item>
    <item>
      <title>An AI-powered Bayesian generative modeling approach for causal inference in observational studies</title>
      <link>https://arxiv.org/abs/2501.00755</link>
      <description>arXiv:2501.00755v3 Announce Type: replace-cross 
Abstract: Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome. The core innovation is to estimate the individual treatment effect (ITE) by learning the individual-specific distribution of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This individualized posterior representation yields estimates of the individual treatment effect (ITE) together with well-calibrated posterior intervals while mitigating confounding effect. CausalBGM is fitted through an iterative algorithm to update the model parameters and the latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in a wide range of modern applications. The code for CausalBGM is available at https://github.com/liuq-lab/bayesgm. The tutorial for CausalBGM is available at https://causalbgm.readthedocs.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00755v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qiao Liu, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>GRAND: Graph Release with Assured Node Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.00402</link>
      <description>arXiv:2507.00402v3 Announce Type: replace-cross 
Abstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00402v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqing Liu, Xuan Bi, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Ideal Observer for Segmentation of Dead Leaves Images</title>
      <link>https://arxiv.org/abs/2512.05539</link>
      <description>arXiv:2512.05539v2 Announce Type: replace-cross 
Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05539v2</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swantje Mahncke, Malte Ott</dc:creator>
    </item>
    <item>
      <title>Sharp Structure-Agnostic Lower Bounds for General Linear Functional Estimation</title>
      <link>https://arxiv.org/abs/2512.17341</link>
      <description>arXiv:2512.17341v2 Announce Type: replace-cross 
Abstract: We establish a general statistical optimality theory for estimation problems where the target parameter is a linear functional of an unknown nuisance component that must be estimated from data. This formulation covers many causal and predictive parameters and has applications to numerous disciplines. We adopt the structure-agnostic framework introduced by \citet{balakrishnan2023fundamental}, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we first prove the statistical optimality of the celebrated and widely used doubly robust estimators for the Average Treatment Effect (ATE), the most central parameter in causal inference. We then characterize the minimax optimal rate under the general formulation. Notably, we differentiate between two regimes in which double robustness can and cannot be achieved and in which first-order debiasing yields different error rates. Our result implies that first-order debiasing is simultaneously optimal in both regimes. We instantiate our theory by deriving optimal error rates that recover existing results and extend to various settings of interest, including the case when the nuisance is defined by generalized regressions and when covariate shift exists for training and test distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17341v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2512.24139</link>
      <description>arXiv:2512.24139v2 Announce Type: replace-cross 
Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-\alpha \pm \delta\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24139v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors</title>
      <link>https://arxiv.org/abs/2512.24478</link>
      <description>arXiv:2512.24478v2 Announce Type: replace-cross 
Abstract: Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (&lt;10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24478v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunjun Kim</dc:creator>
    </item>
  </channel>
</rss>
