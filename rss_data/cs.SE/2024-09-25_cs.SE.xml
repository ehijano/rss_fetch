<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 11:15:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exploring Requirements Elicitation from App Store User Reviews Using Large Language Models</title>
      <link>https://arxiv.org/abs/2409.15473</link>
      <description>arXiv:2409.15473v1 Announce Type: new 
Abstract: Mobile applications have become indispensable companions in our daily lives. Spanning over the categories from communication and entertainment to healthcare and finance, these applications have been influential in every aspect. Despite their omnipresence, developing apps that meet user needs and expectations still remains a challenge. Traditional requirements elicitation methods like user interviews can be time-consuming and suffer from limited scope and subjectivity. This research introduces an approach leveraging the power of Large Language Models (LLMs) to analyze user reviews for automated requirements elicitation. We fine-tuned three well-established LLMs BERT, DistilBERT, and GEMMA, on a dataset of app reviews labeled for usefulness. Our evaluation revealed BERT's superior performance, achieving an accuracy of 92.40% and an F1-score of 92.39%, demonstrating its effectiveness in accurately classifying useful reviews. While GEMMA displayed a lower overall performance, it excelled in recall (93.39%), indicating its potential for capturing a comprehensive set of valuable user insights. These findings suggest that LLMs offer a promising avenue for streamlining requirements elicitation in mobile app development, leading to the creation of more user-centric and successful applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15473v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanmai Kumar Ghosh, Atharva Pargaonkar, Nasir U. Eisty</dc:creator>
    </item>
    <item>
      <title>Developer Reactions to Protestware in Open Source Software: The cases of color.js and es5.ext</title>
      <link>https://arxiv.org/abs/2409.15674</link>
      <description>arXiv:2409.15674v1 Announce Type: new 
Abstract: There is growing concern about maintainers self-sabotaging their work in order to take political or economic stances, a practice referred to as "protestware". Our objective is to understand the discourse around discussions on such an attack, how it is received by the community, and whether developers respond to the attack in a timely manner. We study two notable protestware cases i.e., colors.js and es5-ext. Results indicate that protestware discussions spread rapidly, though not as quickly as security vulnerabilities, with a lower speed when compared to ua-parser and log4j. By establishing a taxonomy of protestware discussions, we identify posts such as expressing stances and providing technical mitigation instructions. A thematic analysis identified five major themes during the discussions: i. disseminate and response, ii. stance, iii. reputation, iv. communicative styles, v. rights and ethics. This work sheds light on the nuanced landscape of protestware discussions, offering insights for both researchers and developers into maintaining a healthy balance between the political or social actions of developers and the collective well-being of the open-source community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15674v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youmei Fan, Dong Wang, Supatsara Wattanakriengkrai, Hathaichanok Damrongsiri, Christoph Treude, Hideaki Hata, Raula Gaikovina Kula</dc:creator>
    </item>
    <item>
      <title>Toward Conceptual Modeling for Propositional Logic: Propositions as Events</title>
      <link>https://arxiv.org/abs/2409.15705</link>
      <description>arXiv:2409.15705v1 Announce Type: new 
Abstract: Applying logic in the area of conceptual modeling has been investigated widely, yet there has been limited uptake of logic-based conceptual modeling in industry. According to some researchers, another formalization of such tools as EER or UML class diagrams in logic may only marginally contribute to the body of knowledge. This paper reflects on applying propositional logic language to a high-level diagrammatic representation called the thinging machines (TM) model. We explore the relationship between conceptual modeling and logic, including such issues as: What logical constructs model? How does truth fit into the picture produced in conceptual modeling as a representation of some piece of the world it is about? The ultimate research objective is a quest for a thorough semantic alignment of TM modeling and propositional logic into a single structure. Examples that involve the application of propositional logic in certain areas of reality are TM remodeled, where propositions are viewed as TM regions or events. As it turned out, TM seems to shed light on the semantics of propositions. In such a conceptual framework, logical truth is a matter of how things are in actuality and how falsehood is in subsistence. The results show that propositional logic enriches the rigorousness of conceptual descriptions and that the TM semantic apparatus complements propositional logic by providing a background to the given set of propositions. Semantics matters are applied to propositional constructs such as negative propositions, disjunctions, and conjunctions with negative terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15705v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabah Al-Fedaghi</dc:creator>
    </item>
    <item>
      <title>LLM-Cure: LLM-based Competitor User Review Analysis for Feature Enhancement</title>
      <link>https://arxiv.org/abs/2409.15724</link>
      <description>arXiv:2409.15724v1 Announce Type: new 
Abstract: The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target apps reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based Competitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically generate suggestion s for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided suggestions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15724v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maram Assi, Safwat Hassan, Ying Zou</dc:creator>
    </item>
    <item>
      <title>Automatic Bottom-Up Taxonomy Construction: A Software Application Domain Study</title>
      <link>https://arxiv.org/abs/2409.15881</link>
      <description>arXiv:2409.15881v1 Announce Type: new 
Abstract: Previous research in software application domain classification has faced challenges due to the lack of a proper taxonomy that explicitly models relations between classes. As a result, current solutions are less effective for real-world usage. This study aims to develop a comprehensive software application domain taxonomy by integrating multiple datasources and leveraging ensemble methods. The goal is to overcome the limitations of individual sources and configurations by creating a more robust, accurate, and reproducible taxonomy. This study employs a quantitative research design involving three different datasources: an existing Computer Science Ontology (CSO), Wikidata, and LLMs. The study utilises a combination of automated and human evaluations to assess the quality of a taxonomy. The outcome measures include the number of unlinked terms, self-loops, and overall connectivity of the taxonomy. The results indicate that individual datasources have advantages and drawbacks: the CSO datasource showed minimal variance across different configurations, but a notable issue of missing technical terms and a high number of self-loops. The Wikipedia datasource required significant filtering during construction to improve metric performance. LLM-generated taxonomies demonstrated better performance when using context-rich prompts. An ensemble approach showed the most promise, successfully reducing the number of unlinked terms and self-loops, thus creating a more connected and comprehensive taxonomy. The study addresses the construction of a software application domain taxonomy relying on pre-existing resources. Our results indicate that an ensemble approach to taxonomy construction can effectively address the limitations of individual datasources. Future work should focus on refining the ensemble techniques and exploring additional datasources to enhance the taxonomy's accuracy and completeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15881v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cezar Sas, Andrea Capiluppi</dc:creator>
    </item>
    <item>
      <title>Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation</title>
      <link>https://arxiv.org/abs/2409.15895</link>
      <description>arXiv:2409.15895v1 Announce Type: new 
Abstract: Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.
  To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28% on EM, 13% on BLEU, and 6.8% on CodeBLEU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15895v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Gao, Yun Xiong, Deze Wang, Zhenhan Guan, Zejian Shi, Haofen Wang, Shanshan Li</dc:creator>
    </item>
    <item>
      <title>MOSS: Enabling Code-Driven Evolution and Context Management for AI Agents</title>
      <link>https://arxiv.org/abs/2409.16120</link>
      <description>arXiv:2409.16120v1 Announce Type: new 
Abstract: Developing AI agents powered by large language models (LLMs) faces significant challenges in achieving true Turing completeness and adaptive, code-driven evolution. Current approaches often generate code independently of its runtime context, relying heavily on the LLM's memory, which results in inefficiencies and limits adaptability. Manual protocol development in sandbox environments further constrains the agent's autonomous adaptability. Crucially, achieving consistency in code and context across multi-turn interactions and ensuring isolation of local variables within each interaction remains an unsolved problem.
  We introduce MOSS (llM-oriented Operating System Simulation), a novel framework that addresses these challenges by integrating code generation with a dynamic context management system. MOSS ensures consistency and adaptability by using a mechanism that maintains the Python context across interactions, including isolation of local variables and preservation of runtime integrity. At its core, the framework employs an Inversion of Control (IoC) container in conjunction with decorators to enforce the least knowledge principle, allowing agents to focus on abstract interfaces rather than concrete implementations. This facilitates seamless integration of new tools and libraries, enables runtime instance replacement, and reduces prompt complexity, providing a "what you see is what you get" environment for the agent.
  Through a series of case studies, we show how this framework can enhance the efficiency and capabilities of agent development and highlight its advantages in moving towards Turing-complete agents capable of evolving through code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16120v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Zhu, Yi Zhou</dc:creator>
    </item>
    <item>
      <title>Metamorphic Debugging for Accountable Software</title>
      <link>https://arxiv.org/abs/2409.16140</link>
      <description>arXiv:2409.16140v1 Announce Type: new 
Abstract: As the laws have become more complicated and enormous, the role of software systems in navigating and understanding these intricacies has become more critical. Given their socio-economic and legally critical implications, ensuring software accountability -- encompassing qualities such as legal compliance, explainability, perceptions of procedural justice, fairness of outcomes, and confidentiality/privacy -- is of paramount social importance. Moreover, software that accurately interprets its requirements, complies with legal standards and upholds social fairness can serve as a surrogate for legal and social norms, enabling policymakers to inquire about the law as seamlessly as a software engineer conducts a test. However, ensuring software accountability faces three key challenges: i) Translating legalese into formal specifications, ii) Lack of a definitive 'truth' for queries (the oracle problem), and iii) Scarcity of trustworthy datasets due to privacy and legal concerns.
  Drawing from the experiences in debugging U.S. tax preparation software, we propose that these challenges can be tackled by focusing on relational specifications. While the exact output for a given input may be unknown, the relationship between the outputs of two related inputs may be easier to express. This observation resembles i) the legal doctrine of precedent, meaning that similar cases must yield similar rulings; and ii) metamorphic relation (MR) in software engineering that requires a specific relation between software inputs and outputs. We propose metamorphic debugging as the foundation for detecting, explaining, and repairing socio-legal software for these relations. We showcase recent results that leverage metamorphic debugging to detect and explain accountability bugs in tax prep and poverty management software systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16140v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Tizpaz-Niari, Shiva Darian, Ashutosh Trivedi</dc:creator>
    </item>
    <item>
      <title>Refactoring-aware Block Tracking in Commit History</title>
      <link>https://arxiv.org/abs/2409.16185</link>
      <description>arXiv:2409.16185v1 Announce Type: new 
Abstract: Tracking statements in the commit history of a project is in many cases useful for supporting various software maintenance, comprehension, and evolution tasks. A high level of accuracy can facilitate the adoption of code tracking tools by developers and researchers. To this end, we propose CodeTracker, a refactoring-aware tool that can generate the commit change history for code blocks. To evaluate its accuracy, we created an oracle with the change history of 1,280 code blocks found within 200 methods from 20 popular open-source project repositories. Moreover, we created a baseline based on the current state-of-the-art Abstract Syntax Tree diff tool, namely GumTree 3.0, in order to compare the accuracy and execution time. Our experiments have shown that CodeTracker has a considerably higher precision/recall and faster execution time than the GumTree-based baseline, and can extract the complete change history of a code block with a precision and recall of 99.5% within 3.6 seconds on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16185v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Tayeeb Hasan, Nikolaos Tsantalis, Pouria Alikhanifard</dc:creator>
    </item>
    <item>
      <title>ChatDBG: An AI-Powered Debugging Assistant</title>
      <link>https://arxiv.org/abs/2403.16354</link>
      <description>arXiv:2403.16354v2 Announce Type: replace 
Abstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like `why is x null?'. To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded roughly 50,000 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16354v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund</dc:creator>
    </item>
    <item>
      <title>Using Agile Story Points and Game Theory Together: Better Software Planning and Development in Agile Software Development</title>
      <link>https://arxiv.org/abs/2409.12196</link>
      <description>arXiv:2409.12196v2 Announce Type: replace 
Abstract: In the realm of Agile software development, precise user story point estimation is crucial for effectual project timeline and resource management. Despite its significance, the method is often marred by issues stemming from cognitive biases, disparities in individual judgment, and hurdles related to both collaboration and competition. In addressing these challenges, this study employs a comprehensive literature review, integrating key concepts from Agile software development, Story Point estimation, and Game Theory. Through rigorous examination of existing literature and relevant case studies, we identified pervasive issues in Agile and Story Point estimation. In response, we proposed the application of game theoretic strategies, notably the Vickrey Auction and Stag Hunt Game, aiming to refine these estimations. The resultant methodology not only promotes the use of game-theory inspired mechanisms but also accentuates their potential to enhance software development planning, team cohesion, and conflict resolution. Preliminary results from our research underscore the transformative potential of these games when incorporated into Agile methodologies, especially during planning and retrospective phases. The overarching goal is to achieve improved accuracy in planning, foster team collaboration, and a discernible uplift in software product quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12196v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatih Bildirici, Keziban Seckin Codal, Tunc Durmus Medeni</dc:creator>
    </item>
    <item>
      <title>RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal Reinforcement and Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2409.13122</link>
      <description>arXiv:2409.13122v2 Announce Type: replace 
Abstract: In real-world software engineering tasks, solving a problem often requires understanding and modifying multiple functions, classes, and files across a large codebase. Therefore, on the repository level, it is crucial to extract the relevant information to achieve accurate code completion effectively. Existing code completion tools have achieved some success, but they struggle to optimize the retrieval and generation process dynamically. In this paper, we propose RepoGenReflex, a generic, dynamic, effective framework to address this challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal results for repository-level code completion. RepoGenReflex uses Reflector to give directional feedback to the next loop. RepoGenReflex chooses the optimal results stored in the Experience cache based on the RAG-VRL loop. To validate the framework's generalization ability, we propose a new benchmark RepoGenEval, which consists of the latest, high-quality real-world repositories in line completion scenarios. Our experiments demonstrate that RepoGenReflex achieves significant improvements after optimizing the Reflector component, resulting in enhanced accuracy and relevance of code completions. Additionally, RepoGenReflex consistently demonstrates superior performance and effectiveness across standard code completion tasks, highlighting the robustness and adaptability of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13122v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jicheng Wang, Yifeng He, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Multi-Grained Specifications for Distributed System Model Checking and Verification</title>
      <link>https://arxiv.org/abs/2409.14301</link>
      <description>arXiv:2409.14301v2 Announce Type: replace 
Abstract: This paper presents our experience specifying and verifying the correctness of ZooKeeper, a complex and evolving distributed coordination system. We use TLA+ to model fine-grained behaviors of ZooKeeper and use the TLC model checker to verify its correctness properties; we also check conformance between the model and code. The fundamental challenge is to balance the granularity of specifications and the scalability of model checking -- fine-grained specifications lead to state-space explosion, while coarse-grained specifications introduce model-code gaps. To address this challenge, we write specifications with different granularities for composable modules, and compose them into mixed-grained specifications based on specific scenarios. For example, to verify code changes, we compose fine-grained specifications of changed modules and coarse-grained specifications that abstract away details of unchanged code with preserved interactions. We show that writing multi-grained specifications is a viable practice and can cope with model-code gaps without untenable state space, especially for evolving software where changes are typically local and incremental. We detected six severe bugs that violate five types of invariants and verified their code fixes; the fixes have been merged to ZooKeeper. We also improve the protocol design to make it easy to implement correctly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14301v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingzhi Ouyang, Xudong Sun, Ruize Tang, Yu Huang, Madhav Jivrajani, Xiaoxing Ma, Tianyin Xu</dc:creator>
    </item>
    <item>
      <title>Tumbling Down the Rabbit Hole: How do Assisting Exploration Strategies Facilitate Grey-box Fuzzing?</title>
      <link>https://arxiv.org/abs/2409.14541</link>
      <description>arXiv:2409.14541v2 Announce Type: replace 
Abstract: Many assisting exploration strategies have been proposed to assist grey-box fuzzers in exploring program states guarded by tight and complex branch conditions such as equality constraints. Although they have shown promising results in their original papers, their evaluations seldom follow equivalent protocols, e.g., they are rarely evaluated on identical benchmarks. Moreover, there is a lack of sufficient investigations on the specifics of the program states explored by these strategies which can obfuscate the future application and development of such strategies. Consequently, there is a pressing need for a comprehensive study of assisting exploration strategies on their effectiveness, versatility, and limitations to enlighten their future development. To this end, we perform the first comprehensive study about the assisting exploration strategies for grey-box fuzzers. Specifically, we first collect nine recent fuzzers representing the mainstream assisting exploration strategies as our studied subjects and 21 real-world projects to form our benchmark suite. After evaluating the subjects on the benchmark suite, we then surprisingly find that the dictionary strategy is the most promising since it not only achieves similar or even slightly better performance over the other studied assisting exploration strategies in terms of exploring program states but also is more practical to be enhanced. Accordingly, we propose CDFUZZ, which generates a customized dictionary for each seed upon the baseline fuzzer AFL to improve over the original dictionary strategy. The evaluation results demonstrate that CDFUZZ increases the edge coverage by 16.1% on average for all benchmark projects over the best performer in our study (i.e., AFL++ with the dictionary strategy). CDFUZZ also successfully exposed 37 previously unknown bugs, with nine confirmed and seven fixed by the corresponding developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14541v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyuan Wu, Jiahong Xiang, Kunqiu Chen, Peng DI, Shin Hwei Tan, Heming Cui, Yuqun Zhang</dc:creator>
    </item>
    <item>
      <title>Racing the Market: An Industry Support Analysis for Pricing-Driven DevOps in SaaS</title>
      <link>https://arxiv.org/abs/2409.15150</link>
      <description>arXiv:2409.15150v2 Announce Type: replace 
Abstract: The SaaS paradigm has popularized the usage of pricings, allowing providers to offer customers a wide range of subscription possibilities. This creates a vast configuration space for users, enabling them to choose the features and support guarantees that best suit their needs. Regardless of the reasons why changes in these pricings are made, the frequency of changes within the elements of pricings continues to increase. Therefore, for those responsible for the development and operation of SaaS, it would be ideal to minimize the time required to transfer changes in SaaS pricing to the software and underlying infrastructure, without compromising the quality and reliability.% of the service; %i.e., this development and operation should be Pricing-Driven. This work explores the support offered by the industry for this need. By modeling over 150 pricings from 30 different SaaS over six years, we reveal that the configuration space grows exponentially with the number of add-ons and linearly with the number of plans. We also evaluate 21 different feature toggling solutions, finding that feature toggling, particularly permission toggles, is a promising technique for enabling rapid adaptation to pricing changes. Our results suggest that developing automated solutions with minimal human intervention could effectively reduce the time-to-market for SaaS updates driven by pricing changes, especially with the adoption of a standard for serializing pricings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15150v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alejandro Garcia-Fern\'andez, Jos\'e Antonio Parejo, Francisco Javier Cavero, Antonio Ruiz-Cort\'es</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models</title>
      <link>https://arxiv.org/abs/2409.15228</link>
      <description>arXiv:2409.15228v2 Announce Type: replace 
Abstract: Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15228v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixi Wu (Peter), Pengfei He (Peter), Zehao Wang (Peter), Shaowei Wang (Peter), Yuan Tian (Peter),  Tse-Hsun (Peter),  Chen</dc:creator>
    </item>
    <item>
      <title>KernJC: Automated Vulnerable Environment Generation for Linux Kernel Vulnerabilities</title>
      <link>https://arxiv.org/abs/2404.11107</link>
      <description>arXiv:2404.11107v3 Announce Type: replace-cross 
Abstract: Linux kernel vulnerability reproduction is a critical task in system security. To reproduce a kernel vulnerability, the vulnerable environment and the Proof of Concept (PoC) program are needed. Most existing research focuses on the generation of PoC, while the construction of environment is overlooked. However, establishing an effective vulnerable environment to trigger a vulnerability is challenging. Firstly, it is hard to guarantee that the selected kernel version for reproduction is vulnerable, as the vulnerability version claims in online databases can occasionally be spurious. Secondly, many vulnerabilities can not be reproduced in kernels built with default configurations. Intricate non-default kernel configurations must be set to include and trigger a kernel vulnerability, but less information is available on how to recognize these configurations.
  To solve these challenges, we propose a patch-based approach to identify real vulnerable kernel versions and a graph-based approach to identify necessary configs for activating a specific vulnerability. We implement these approaches in a tool, KernJC, automating the generation of vulnerable environments for kernel vulnerabilities. To evaluate the efficacy of KernJC, we build a dataset containing 66 representative real-world vulnerabilities with PoCs from kernel vulnerability research in the past five years. The evaluation shows that KernJC builds vulnerable environments for all these vulnerabilities, 48.5% of which require non-default configs, and 4 have incorrect version claims in the National Vulnerability Database (NVD). Furthermore, we conduct large-scale spurious version detection on kernel vulnerabilities and identify 128 vulnerabilities which have spurious version claims in NVD. To foster future research, we release KernJC with the dataset in the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11107v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3678890.3678891</arxiv:DOI>
      <dc:creator>Bonan Ruan, Jiahao Liu, Chuqi Zhang, Zhenkai Liang</dc:creator>
    </item>
    <item>
      <title>VulZoo: A Comprehensive Vulnerability Intelligence Dataset</title>
      <link>https://arxiv.org/abs/2406.16347</link>
      <description>arXiv:2406.16347v2 Announce Type: replace-cross 
Abstract: Software vulnerabilities pose critical security and risk concerns for many software systems. Many techniques have been proposed to effectively assess and prioritize these vulnerabilities before they cause serious consequences. To evaluate their performance, these solutions often craft their own experimental datasets from limited information sources, such as MITRE CVE and NVD, lacking a global overview of broad vulnerability intelligence. The repetitive data preparation process further complicates the verification and comparison of new solutions. To resolve this issue, in this paper, we propose VulZoo, a comprehensive vulnerability intelligence dataset that covers 17 popular vulnerability information sources. We also construct connections among these sources, enabling more straightforward configuration and adaptation for different vulnerability assessment tasks (e.g., vulnerability type prediction). Additionally, VulZoo provides utility scripts for automatic data synchronization and cleaning, relationship mining, and statistics generation. We make VulZoo publicly available and maintain it with incremental updates to facilitate future research. We believe that VulZoo serves as a valuable input to vulnerability assessment and prioritization studies. The dataset with utility scripts is available at https://github.com/NUS-Curiosity/VulZoo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16347v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonan Ruan, Jiahao Liu, Weibo Zhao, Zhenkai Liang</dc:creator>
    </item>
  </channel>
</rss>
