<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:01:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Drift Optimization of Regulated Stochastic Models Using Sample Average Approximation</title>
      <link>https://arxiv.org/abs/2506.06723</link>
      <description>arXiv:2506.06723v1 Announce Type: new 
Abstract: This paper introduces a drift optimization model of stochastic optimization problems driven by regulated stochastic processes. A broad range of problems across operations research, machine learning, and statistics can be viewed as optimizing the "drift" associated with a process by minimizing a cost functional, while respecting path constraints imposed by a Lipschitz continuous regulator. Towards an implementable solution to such infinite-dimensional problems, we develop the fundamentals of a Sample Average Approximation (SAA) method that incorporates (i) path discretization, (ii) function-space discretization, and (iii) Monte Carlo sampling, and that is solved using an optimization recursion such as mirror descent. We start by constructing pathwise directional derivatives for use within the SAA method, followed by consistency and complexity calculations. The characterized complexity is expressed as a function of the number of optimization steps, and the computational effort involved in (i)--(iii), leading to guidance on how to trade-off the computational effort allocated to optimization steps versus the "dimension reduction" steps in (i)--(iii).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06723v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihe Zhou, Harsha Honnappa, Raghu Pasupathy</dc:creator>
    </item>
    <item>
      <title>The Euler-Lagrange and Legendre Necessary Conditions for Fractional Calculus of Variations</title>
      <link>https://arxiv.org/abs/2506.06736</link>
      <description>arXiv:2506.06736v1 Announce Type: new 
Abstract: In this paper, we study the problems of minimizing a functional depending on the Caputo fractional derivative of order $0&lt; \alpha \leq 1$ and the Riemann- Liouville fractional integral of order $\beta &gt;0$ under certain constraints. A fractional analogue of the Du Bois-Reymond lemma is proved. Using this lemma for various weak local minimum problems, the Euler-Lagrange equation is derived in integral form. Some serious works in the literature claim that the standard proof of the Legendre condition in the classical case $\alpha=1$ cannot be adapted to the fractional case $0&lt;\alpha &lt;1$ with final constraints. In spite of this, we prove the Legendre conditions using the standard classical method. The obtained necessary conditions are illustrated by appropriate examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06736v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shikhi Sh. Yusubov, Shakir Sh. Yusubov, Elimhan N. Mahmudov</dc:creator>
    </item>
    <item>
      <title>The Weierstrass necessary condition for fractional calculus of variations</title>
      <link>https://arxiv.org/abs/2506.06741</link>
      <description>arXiv:2506.06741v1 Announce Type: new 
Abstract: In this paper, we study problems of minimization of a functional depending on the fractional Caputo derivative of order $0&lt;\alpha \leq 1$ and the fractional Riemann- Liouville integral of order $\beta &gt; 0$ at fixed endpoints. A fractional analogue of the Du Bois-Reymond lemma is proved, and the Euler-Lagrange conditions are proved for the simplest problem of fractional variational calculus with fixed ends and for the fractional isoperimetric problem. An approach is proposed to obtain the necessary first-order conditions for the strong and weak extrema, and the necessary optimality conditions are obtained. From these necessary conditions, as a consequence, we obtain the Weierstrass condition and its local modification.
  It should be noted that some papers in the literature claim that the standard proof of the Legendre condition in the classical case $\alpha=1$ cannot be adapted to the fractional case $0&lt; \alpha &lt;1$ with final constraints. Despite this, we prove the Legendre conditions by the standard classical method via the Weierstrass condition. In addition, the necessary Weierstrass-Erdmann conditions at the corner points are obtained. Examples are provided to illustrate the significance of the main results obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06741v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shakir Sh. Yusubov, Shikhi Sh. Yusubov, Elimhan N. Mahmudov</dc:creator>
    </item>
    <item>
      <title>CatMADS: Mesh Adaptive Direct Search for constrained blackbox optimization with categorical variables</title>
      <link>https://arxiv.org/abs/2506.06937</link>
      <description>arXiv:2506.06937v1 Announce Type: new 
Abstract: Solving optimization problems in which functions are blackboxes and variables involve different types poses significant theoretical and algorithmic challenges. Nevertheless, such settings frequently occur in simulation-based engineering design and machine learning. This paper extends the Mesh Adaptive Direct Search (MADS) algorithm to address mixed-variable problems with categorical, integer and continuous variables. MADS is a robust derivative-free optimization framework with a well-established convergence analysis for constrained quantitative problems. CatMADS generalizes MADS by incorporating categorical variables through distance-induced neighborhoods. A detailed convergence analysis of CatMADS is provided, with flexible choices balancing computational cost and local optimality strength. Four types of mixed-variable local minima are introduced, corresponding to progressively stronger notions of local optimality. CatMADS integrates the progressive barrier strategy for handling constraints, and ensures Clarke stationarity. An instance of \catmads employs cross-validation to construct problem-specific categorical distances. This instance is compared to state-of-the-art solvers on 32 mixed-variable problems, half of which are constrained. Data profiles show that CatMADS achieves the best results, demonstrating that the framework is empirically efficient in addition to having strong theoretical foundations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06937v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Audet, Youssef Diouane, Edward Hall\'e-Hannan, S\'ebastien Le Digabel, Christophe Tribes</dc:creator>
    </item>
    <item>
      <title>Stochastic Push-Pull for Decentralized Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2506.07021</link>
      <description>arXiv:2506.07021v1 Announce Type: new 
Abstract: To understand the convergence behavior of the Push-Pull method for decentralized optimization with stochastic gradients (Stochastic Push-Pull), this paper presents a comprehensive analysis. Specifically, we first clarify the algorithm's underlying assumptions, particularly those regarding the network structure and weight matrices. Then, to establish the convergence rate under smooth nonconvex objectives, we introduce a general analytical framework that not only encompasses a broad class of decentralized optimization algorithms, but also recovers or enhances several state-of-the-art results for distributed stochastic gradient tracking methods. A key highlight is the derivation of a sufficient condition under which the Stochastic Push-Pull algorithm achieves linear speedup, matching the scalability of centralized stochastic gradient methods -- a result not previously reported. Extensive numerical experiments validate our theoretical findings, demonstrating the algorithm's effectiveness and robustness across various decentralized optimization scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07021v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runze You, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Optimizing rake-links independently of timetables in railway operations</title>
      <link>https://arxiv.org/abs/2506.07024</link>
      <description>arXiv:2506.07024v1 Announce Type: new 
Abstract: This study addresses optimal rake-link formation in large-scale timetabled rail operations by modeling the problem as a directed acyclic graph and solving it via the minimum path cover algorithm. It enables efficient rake-to-service assignment while minimizing fleet size. Crucially, it decouples rake-link optimization from the timetable planning process, allowing planners to evaluate feasible rake configurations independently. The model incorporates operational constraints such as deadhead limits, service balance, and slack allowances. Applied to real-world data from Indian Railways, the results reveal clustered Pareto fronts in the decision space, indicating robust and redundant solutions. The approach lays a foundation for resilient, adaptive rail management via digital twin systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07024v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Dey</dc:creator>
    </item>
    <item>
      <title>A Triple-Bregman Balanced Primal-Dual Algorithm for Saddle Point Problems</title>
      <link>https://arxiv.org/abs/2506.07117</link>
      <description>arXiv:2506.07117v1 Announce Type: new 
Abstract: The primal-dual hybrid gradient (PDHG) method is one of the most popular algorithms for solving saddle point problems. However, when applying the PDHG method and its many variants to some real-world models commonly encountered in signal processing, imaging sciences, and statistical learning, there often exists an imbalance between the two subproblems, with the dual subproblem typically being easier to solve than the primal one. In this paper, we propose a flexible triple-Bregman balanced primal-dual algorithm (TBDA) to solve a class of (not necessarily smooth) convex-concave saddle point problems with a bilinear coupling term. Specifically, our TBDA mainly consists of two dual subproblems and one primal subproblem. Moreover, three Bregman proximal terms, each one with an individual Bregman kernel function, are embedded into the respective subproblems. In this way, it effectively enables us to strike a practical balance between the primal and dual subproblems. More interestingly, it provides us a flexible algorithmic framework to understand some existing iterative schemes and to produce customized structure-exploiting algorithms for applications. Theoretically, we first establish the global convergence and ergodic convergence rate of the TBDA under some mild conditions. In particular, our TBDA allows larger step sizes than the PDHG method under appropriate parameter settings. Then, when the requirements on objective functions are further strengthened, we accordingly introduce two improved versions with better convergence rates than the original TBDA. Some numerical experiments on synthetic and real datasets demonstrate that our TBDA performs better than the PDHG method and some other efficient variants in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07117v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jintao Yu, Hongjin He</dc:creator>
    </item>
    <item>
      <title>Adaptive Consensus with Exponential Decay</title>
      <link>https://arxiv.org/abs/2506.07203</link>
      <description>arXiv:2506.07203v1 Announce Type: new 
Abstract: This paper addresses the adaptive consensus problem in uncertain multi-agent systems, particularly under challenges posed by quantized communication. We consider agents with general linear dynamics subject to nonlinear uncertainties and propose an adaptive consensus control framework that integrates concurrent learning. Unlike traditional methods relying solely on instantaneous data, concurrent learning leverages stored historical data to enhance parameter estimation without requiring persistent excitation. We establish that the proposed controller ensures exponential convergence of both consensus and parameter estimation. Furthermore, we extend the analysis to scenarios where inter-agent communication is quantized using a uniform quantizer. We prove that the system still achieves consensus up to an error proportional to the quantization level, with exponential convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07203v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Woocheol Choi, Piljae Jang</dc:creator>
    </item>
    <item>
      <title>Stochastic Moving Anchor Algorithms and a Popov's Scheme with Moving Anchor</title>
      <link>https://arxiv.org/abs/2506.07290</link>
      <description>arXiv:2506.07290v1 Announce Type: new 
Abstract: Since their introduction, anchoring methods in extragradient-type saddlepoint problems have inspired a flurry of research due to their ability to provide order-optimal rates of accelerated convergence in very general problem settings. Such guarantees are especially important as researchers consider problems in artificial intelligence (AI) and machine learning (ML), where large problem sizes demand immense computational power. Much of the more recent works explore theoretical aspects of this new acceleration framework, connecting it to existing methods and order-optimal convergence rates from the literature. However, in practice introducing stochastic oracles allows for more computational efficiency given the size of many modern optimization problems. To this end, this work provides the moving anchor variants [1] of the original anchoring algorithms [36] with stochastic implementations and robust analyses to bridge the gap from deterministic to stochastic algorithm settings. In particular, we demonstrate that an accelerated convergence rate theory for stochastic oracles also exists for our moving anchor scheme, itself a generalization of the original fixed anchor algorithms, and provide numerical results that validate our theoretical findings. We also develop a tentative moving anchor Popov scheme based on the work in [33], with promising numerical results pointing towards an as-of-yet uncovered general convergence theory for such methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07290v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Alcala, Yat Tin Chow, Mahesh Sunkula</dc:creator>
    </item>
    <item>
      <title>Stochastic Quadratic Dynamic Programming</title>
      <link>https://arxiv.org/abs/2506.07314</link>
      <description>arXiv:2506.07314v1 Announce Type: new 
Abstract: We introduce an algorithm called SQDP (Stochastic Quadratic Dynamic Programming) to solve some multistage stochastic optimization problems having strongly convex recourse functions. The algorithm extends the classical Stochastic Dual Dynamic Programming (SDDP) method replacing affine cuts by quadratic cuts. We provide conditions ensuring strong convexity of the recourse functions and prove the convergence of SQDP. In the special case of a single stage deterministic problem, we call QCSC (Quadratic Cuts for Strongly Convex optimization) the method and prove its complexity. Numerical experiments illustrate the performance and correctness of SQDP, with SQDP being much quicker than SDDP for large values of the constants of strong convexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07314v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Adriana Washington</dc:creator>
    </item>
    <item>
      <title>Bregman level proximal subdifferentials and new characterizations of Bregman proximal operators</title>
      <link>https://arxiv.org/abs/2506.07333</link>
      <description>arXiv:2506.07333v1 Announce Type: new 
Abstract: Classic subdifferentials in variational analysis may fail to fully represent the Bregman proximal operator in the absence of convexity. In this paper, we fill this gap by introducing the left and right \emph{Bregman level proximal subdifferentials} and investigate them systematically. Every Bregman proximal operator turns out to be the resolvent of a Bregman level proximal subdifferential under a standard range assumption, even without convexity. Aided by this pleasant feature, we establish new correspondences among useful properties of the Bregman proximal operator, the underlying function, and the (left) Bregman level proximal subdifferential, generalizing classical equivalences in the Euclidean case. Unlike the classical setting, asymmetry and duality gap emerge as natural consequences of the Bregman distance. Along the way, we improve results by Kan and Song and by Wang and Bauschke on Bregman proximal operators. We also characterize the existence and single-valuedness of the Bregman level proximal subdifferential, investigate coincidence results, and make an interesting connection to relative smoothness. Abundant examples are provided to justify the necessity of our assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07333v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyuan Wang, Andreas Themelis</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Self-Adaptive Projection Method for Variational Inequalities with Non-Lipschitz Continuous Operators</title>
      <link>https://arxiv.org/abs/2506.07349</link>
      <description>arXiv:2506.07349v1 Announce Type: new 
Abstract: In this paper, we employ Tseng's extragradient method with the self-adaptive stepsize to solve variational inequality problems involving non-Lipschitz continuous and quasimonotone operators in real Hilbert spaces. The convergence of the proposed method is analyzed under some mild assumptions. The key advantages of the method are that it does not require the operator associated with the variational inequality to be Lipschitz continuous and that it adopts the self-adaptive stepsize. Numerical experiments are also provided to illustrate the effectiveness and superiority of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07349v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meiying Wang, Hongwei Liu, Jun Yang</dc:creator>
    </item>
    <item>
      <title>Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking</title>
      <link>https://arxiv.org/abs/2506.07351</link>
      <description>arXiv:2506.07351v1 Announce Type: new 
Abstract: This paper considers the problem of decentralized optimization on compact submanifolds, where a finite sum of smooth (possibly non-convex) local functions is minimized by $n$ agents forming an undirected and connected graph. However, the efficiency of distributed optimization is often hindered by communication bottlenecks. To mitigate this, we propose the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local variables using quantized gradients. The introduction of quantization noise allows our algorithm to bypass the constraints of the accurate Riemannian projection operator (such as retraction), further improving iterative efficiency. To the best of our knowledge, this is the first algorithm to achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization, matching the convergence rate of methods without quantization. Additionally, we explicitly derive lower bounds on decentralized consensus associated with a function of quantization levels. Numerical experiments demonstrate that Q-RGT performs comparably to non-quantized methods while reducing communication bottlenecks and computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07351v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Chen, Lina Liu, Tianyi Zhu, Yong Liu, Guang Dai, Yunliang Jiang, Ivor W. Tsang</dc:creator>
    </item>
    <item>
      <title>Doubly Smoothed Optimistic Gradients: A Universal Approach for Smooth Minimax Problems</title>
      <link>https://arxiv.org/abs/2506.07397</link>
      <description>arXiv:2506.07397v1 Announce Type: new 
Abstract: Smooth minimax optimization problems play a central role in a wide range of applications, including machine learning, game theory, and operations research. However, existing algorithmic frameworks vary significantly depending on the problem structure -- whether it is convex-concave, nonconvex-concave, convex-nonconcave, or even nonconvex-nonconcave with additional regularity conditions. In particular, this diversity complicates the tuning of step-sizes since even verifying convexity (or concavity) assumptions is challenging and problem-dependent. We introduce a universal and single-loop algorithm, Doubly Smoothed Optimistic Gradient Descent Ascent (DS-OGDA), that applies to a broad class of smooth minimax problems. Specifically, this class includes convex-concave, nonconvex-concave, convex-nonconcave, and nonconvex-nonconcave minimax optimization problems satisfying a one-sided Kurdyka-Lojasiewicz (KL) property. DS-OGDA works with a universal single set of parameters for all problems in this class, eliminating the need for prior structural knowledge to determine step-sizes. Moreover, when a particular problem structure in our class is specified, DS-OGDA achieves optimal or best-known performance guarantees. Overall, our results provide a comprehensive and versatile framework for smooth minimax optimization, bridging the gap between convex and nonconvex problem structures and simplifying the choice of algorithmic strategies across diverse applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07397v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taoli Zheng, Anthony Man-Cho So, Jiajin Li</dc:creator>
    </item>
    <item>
      <title>Extended mean field games with terminal constraint via decoupling fields</title>
      <link>https://arxiv.org/abs/2506.07485</link>
      <description>arXiv:2506.07485v1 Announce Type: new 
Abstract: We consider a class of extended mean field games with common noises, where there exists a strictly terminal constraint. We solve the problem by reducing it to an unconstrained control problem by adding a penalized term in the cost functional and then taking a limit. Using the stochastic maximum principle, we characterize the solution of the unconstrained control problem in terms of a conditional mean field forward-backward stochastic differential equation (FBSDE). We obtain the wellposedness results of the FBSDE and the monotonicity property of its decoupling field. Based on that, we solve the original constrained problem and characterize its solution in terms of a system of coupled conditional mean field FBSDE with a free backward part. In particular, we obtain the solvability of a new type of coupled conditional mean field FBSDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07485v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianjiao Hua, Peng Luo</dc:creator>
    </item>
    <item>
      <title>Brockett cost function for symplectic eigenvalues</title>
      <link>https://arxiv.org/abs/2506.07560</link>
      <description>arXiv:2506.07560v1 Announce Type: new 
Abstract: The symplectic eigenvalues and corresponding eigenvectors of symmetric positive-definite matrices in the sense of Williamson's theorem can be computed via minimization of a trace cost function under the symplecticity constraint. The optimal solution to this problem only offers a symplectic basis for a symplectic eigenspace corresponding to the sought symplectic eigenvalues. In this paper, we introduce a Brockett cost function and investigate the connection between its properties and the symplectic eigenvalues and eigenvectors, specifically prove that any critical point consists of symplectic eigenvectors. Surprisingly, the trace minimization theorem for the symplectic eigenvalues can be deduced from our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07560v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Thanh Son</dc:creator>
    </item>
    <item>
      <title>Set-valued regression and cautious suboptimization: From noisy data to optimality</title>
      <link>https://arxiv.org/abs/2506.07622</link>
      <description>arXiv:2506.07622v1 Announce Type: new 
Abstract: This paper deals with the problem of finding suboptimal values of an unknown function on the basis of measured data corrupted by bounded noise. As a prior, we assume that the unknown function is parameterized in terms of a number of basis functions. Inspired by the informativity approach, we view the problem as the suboptimization of the worst-case estimate of the function. The paper provides closed form solutions and convexity results for this function, which enables us to solve the problem. After this, an online implementation is investigated, where we iteratively measure the function and perform a suboptimization. This nets a procedure that is safe at each step, and which, under mild assumptions, converges to the true optimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07622v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaap Eising, Jorge Cortes</dc:creator>
    </item>
    <item>
      <title>Shape optimization under width constraint: the Cheeger constant and the torsional rigidity</title>
      <link>https://arxiv.org/abs/2506.07708</link>
      <description>arXiv:2506.07708v1 Announce Type: new 
Abstract: In this article it is shown that the equilateral triangle maximizes the Cheeger constant and minimizes the torsional rigidity among shapes having a fixed minimal width. The proof techniques use direct comparisons with simpler shapes, consisting of disks with three disjoint caps. Comparison results for harmonic functions help establish that in non-equilateral configurations the shape derivative has an appropriate sign, contradicting optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07708v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Beniamin Bogosel</dc:creator>
    </item>
    <item>
      <title>Controllability of induced bilinear systems on the sphere</title>
      <link>https://arxiv.org/abs/2506.07749</link>
      <description>arXiv:2506.07749v1 Announce Type: new 
Abstract: In this paper, we investigate the controllability of bilinear control systems of the form $\dot{s} = As + uBs$, where $s \in \mathbb{S}^2$ and $A, B \in gl(3, \mathbb{R})$ are skew-symmetric matrices. First, we prove that the algebraic condition $[A, B] \neq 0$ ensures that the Lie algebra rank condition is satisfied for these systems. Next, we show that this same condition implies the controllability of the system. Finally, in an explicit and descriptive manner, we demonstrate controllability by exhibiting trajectories that transfer a given initial state to another.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07749v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Marco A. Colque-Choquecallata, Efrain Cruz-Mullisaca, Victor H. Patty-Yujra</dc:creator>
    </item>
    <item>
      <title>Analyticity of Exponential Dirichlet Series and Applications to the Approximate Controllability of Parabolic Equations</title>
      <link>https://arxiv.org/abs/2506.07892</link>
      <description>arXiv:2506.07892v1 Announce Type: new 
Abstract: In this paper, we investigate the analyticity of a class of exponential Dirichlet series. We then explicitly determine the coefficients of their power series decomposition and provide an estimate for the remainder. As an application, we study the approximate controllability property of linear parabolic equations with locally distributed or lumped controls by employing the moment method, which relies on the exponential Dirichlet series associated with the spectrum of the system's operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07892v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Ouzahra</dc:creator>
    </item>
    <item>
      <title>Discrete and Continuous Difference of Submodular Minimization</title>
      <link>https://arxiv.org/abs/2506.07952</link>
      <description>arXiv:2506.07952v1 Announce Type: new 
Abstract: Submodular functions, defined on continuous or discrete domains, arise in numerous applications. We study the minimization of the difference of two submodular (DS) functions, over both domains, extending prior work restricted to set functions. We show that all functions on discrete domains and all smooth functions on continuous domains are DS. For discrete domains, we observe that DS minimization is equivalent to minimizing the difference of two convex (DC) functions, as in the set function case. We propose a novel variant of the DC Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable theoretical guarantees as in the set function case. The algorithm can be applied to continuous domains via discretization. Experiments demonstrate that our method outperforms baselines in integer compressive sensing and integer least squares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07952v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025</arxiv:journal_reference>
      <dc:creator>George Orfanides, Tim Hoheisel, Marwa El Halabi</dc:creator>
    </item>
    <item>
      <title>El0ps: An Exact L0-regularized Problems Solver</title>
      <link>https://arxiv.org/abs/2506.06373</link>
      <description>arXiv:2506.06373v1 Announce Type: cross 
Abstract: This paper presents El0ps, a Python toolbox providing several utilities to handle L0-regularized problems related to applications in machine learning, statistics, and signal processing, among other fields. In contrast to existing toolboxes, El0ps allows users to define custom instances of these problems through a flexible framework, provides a dedicated solver achieving state-of-the-art performance, and offers several built-in machine learning pipelines. Our aim with El0ps is to provide a comprehensive tool which opens new perspectives for the integration of L0-regularized problems in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06373v1</guid>
      <category>cs.MS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Guyard, C\'edric Herzet, Cl\'ement Elvira</dc:creator>
    </item>
    <item>
      <title>Optimizing Optimizations: Case Study on Detecting Specific Types of Mathematical Optimization Constraints with E-Graphs in JijModeling</title>
      <link>https://arxiv.org/abs/2506.06495</link>
      <description>arXiv:2506.06495v1 Announce Type: cross 
Abstract: In solving mathematical optimization problems efficiently, it is crucial to make use of information about specific types of constraints, such as the one-hot or Special-Ordered Set (SOS) constraints. In many cases, exploiting such information gives asymptotically better execution time. JijModeling, an industrial-strength mathematical optimization modeller, achieves this by separating the symbolic representation of an optimization problem from the input data. In this paper, we will report a real-world case study on a constraint detection mechanism modulo the algebraic congruence using e-graphs, and describe heuristic criteria for designing rewriting systems. We give benchmarking result that shows the performance impact of the constraint detection mechanism.
  We also introduce egg_recursive, a utility library for writing egg-terms as recursive abstract syntax trees, reducing the burden of writing and maintaining complex terms in S-expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06495v1</guid>
      <category>cs.PL</category>
      <category>cs.MS</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiromi Ishii (Jij, Inc), Taro Shimizu (Jij, Inc), Toshiki Teramura (Jij, Inc)</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient-Descent Calibration of Pyragas Delayed-Feedback Control for Chaos Suppression in the Sprott Circuit</title>
      <link>https://arxiv.org/abs/2506.06639</link>
      <description>arXiv:2506.06639v1 Announce Type: cross 
Abstract: This paper investigates chaos control in the Sprott circuit, a minimal electronic system exhibiting complex nonlinear dynamics. Using the third-order nonlinear differential equation from Kaveh Merat paper, we model the circuit and implement delayed feedback control to suppress chaos. Experimental voltage data were extracted from published figures via WebPlotDigitizer. Then we explore two calibration techniques: Minimizing sum of squared errors (SSE), and stochastic gradient descent (SGD) with finite differences. Joint optimization of control parameters and the variable resistor achieves the best alignment with experimental data, accurately capturing phase and amplitude. SGD outperforms grid search in phase synchronization, though amplitude discrepancies persist due to model simplifications. The trade-off between accuracy and computational cost is analyzed, revealing scalability challenges in chaotic system calibration. Phase space analysis validates the model ability to replicate the chaotic attractor geometry, despite minor deviations. Overall, Stochastic Gradient Descent based calibration of chaotic nonlinear systems shows significant potential for advancing mathematical modeling and electrical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06639v1</guid>
      <category>nlin.CD</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adib Kabir, Onil Morshed, Oishi Kabir</dc:creator>
    </item>
    <item>
      <title>Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.07134</link>
      <description>arXiv:2506.07134v1 Announce Type: cross 
Abstract: Despite decades of research, it remains challenging to correctly use Reinforcement Learning (RL) algorithms with function approximation. A prime example is policy iteration, whose fundamental guarantee of monotonic improvement collapses even under linear function approximation. To address this issue, we introduce Reliable Policy Iteration (RPI). It replaces the common projection or Bellman-error minimization during policy evaluation with a Bellman-based constrained optimization. We prove that not only does RPI confer textbook monotonicity on its value estimates but these estimates also lower bound the true return. Also, their limit partially satisfies the unprojected Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first algorithm with such monotonicity and convergence guarantees under function approximation. For practical use, we provide a model-free variant of RPI that amounts to a novel critic. It can be readily integrated into primary model-free PI implementations such as DQN and DDPG. In classical control tasks, such RPI-enhanced variants consistently maintain their lower-bound guarantee while matching or surpassing the performance of all baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07134v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Eshwar S. R., Gugan Thoppe, Aditya Gopalan, Gal Dalal</dc:creator>
    </item>
    <item>
      <title>A Generalization of a Classical Geometric Extremum Problem</title>
      <link>https://arxiv.org/abs/2506.07252</link>
      <description>arXiv:2506.07252v1 Announce Type: cross 
Abstract: Let $\partial \,\mathcal{C}$ be the boundary of a compact convex body $\mathcal{C}$ in $\mathbb{R}^n,\, n\geq 2$, and $O$ be an interior point of $\mathcal C$. Every straight line $l$ containing $O$ cuts from $\mathcal{C}$ a segment $[AB]$ with end-points on $\partial \,\mathcal{C}$. It is shown that if $[AB]$ is the shortest such segment, then $\partial \,\mathcal{C}$ is smooth at the points $A$ and $ B$ (i.e. at both of them there is only one supporting hyperplane for $\mathcal{C}$) and, something more, the normals to the unique supporting hyperplanes at the points $A$ and $B$ intersect at a point belonging to the hiperplane through $O$ which is orthogonal to $[AB]$.
  If $\mathcal{C}$ is a smooth compact convex body in $\mathbb{R}^n,\, n\geq 2$, the above property holds also when $[AB]$ is the longest such segment. Similar results have place also when $O$ is outside the set $\mathcal{C}$. The ``local versions'' of these results (when the length $|AB|$ of the segment $[AB]$ is locally maximal or locally minimal) also have a place. More specific results are obtained in the particular case when $\mathcal{C}$ is a convex polytope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07252v1</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petar Kenderov, Oleg Mushkarov, Nikolai Nikolov</dc:creator>
    </item>
    <item>
      <title>BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field</title>
      <link>https://arxiv.org/abs/2506.07325</link>
      <description>arXiv:2506.07325v1 Announce Type: cross 
Abstract: Model Predictive Path Integral (MPPI) controller is used to solve unconstrained optimal control problems and Control Barrier Function (CBF) is a tool to impose strict inequality constraints, a.k.a, barrier constraints. In this work, we propose an integration of these two methods that employ CBF-like conditions to guide the control sampling procedure of MPPI. CBFs provide an inequality constraint restricting the rate of change of barrier functions by a classK function of the barrier itself. We instead impose the CBF condition as an equality constraint by choosing a parametric linear classK function and treating this parameter as a state in an augmented system. The time derivative of this parameter acts as an additional control input that is designed by MPPI. A cost function is further designed to reignite Nagumo's theorem at the boundary of the safe set by promoting specific values of classK parameter to enforce safety. Our problem formulation results in an MPPI subject to multiple state and control-dependent equality constraints which are non-trivial to satisfy with randomly sampled control inputs. We therefore also introduce state transformations and control projection operations, inspired by the literature on path planning for manifolds, to resolve the aforementioned issue. We show empirically through simulations and experiments on quadrotor that our proposed algorithm exhibits better sampled efficiency and enhanced capability to operate closer to the safe set boundary over vanilla MPPI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07325v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hardik Parwana, Taekyung Kim, Kehan Long, Bardh Hoxha, Hideki Okamoto, Georgios Fainekos, Dimitra Panagou</dc:creator>
    </item>
    <item>
      <title>Flowing Datasets with Wasserstein over Wasserstein Gradient Flows</title>
      <link>https://arxiv.org/abs/2506.07534</link>
      <description>arXiv:2506.07534v1 Announce Type: cross 
Abstract: Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07534v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Bonet, Christophe Vauthier, Anna Korba</dc:creator>
    </item>
    <item>
      <title>Data-Informed Mathematical Characterization of Absorption Properties in Artificial and Natural Porous Materials</title>
      <link>https://arxiv.org/abs/2506.07656</link>
      <description>arXiv:2506.07656v1 Announce Type: cross 
Abstract: In this work, we characterize the water absorption properties of selected porous materials through a combined approach that integrates laboratory experiments and mathematical modeling. Specifically, experimental data from imbibition tests on marble, travertine, wackestone and mortar mock-ups are used to inform and validate the mathematical and simulation frameworks. First, a monotonicity-preserving fitting procedure is developed to preprocess the measurements, aiming to reduce noise and mitigate instrumental errors. The imbibition process is then simulated through a partial differential equation model, with parameters calibrated against rough and smoothed data. The proposed procedure appears particularly effective to characterize absorption properties of different materials and it represents a reliable tool for the study and preservation of cultural heritage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07656v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elishan C. Braun, Gabriella Bretti, Melania Di Fazio, Laura Medeghini, Mario Pezzella</dc:creator>
    </item>
    <item>
      <title>Online Control with Adversarial Disturbance for Continuous-time Linear Systems</title>
      <link>https://arxiv.org/abs/2306.01952</link>
      <description>arXiv:2306.01952v5 Announce Type: replace 
Abstract: We study online control for continuous-time linear systems with finite sampling rates, where the objective is to design an online procedure that learns under non-stochastic noise and performs comparably to a fixed optimal linear controller. We present a novel two-level online algorithm, by integrating a higher-level learning strategy and a lower-level feedback control strategy. This method offers a practical and robust solution for online control, which achieves sublinear regret. Our work provides the first nonasymptotic results for controlling continuous-time linear systems with finite number of interactions with the system. Moreover, we examine how to train an agent in domain randomization environments from a non-stochastic control perspective. By applying our method to the SAC (Soft Actor-Critic) algorithm, we achieved improved results in multiple reinforcement learning tasks within domain randomization environments. Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems. Furthermore, our work brings practical intuition into controller learning under non-stochastic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01952v5</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Li, Jing Dong, Can Chang, Baoxiang Wang, Jingzhao Zhang</dc:creator>
    </item>
    <item>
      <title>Stochastic forward-backward-half forward splitting algorithm with variance reduction</title>
      <link>https://arxiv.org/abs/2312.00272</link>
      <description>arXiv:2312.00272v2 Announce Type: replace 
Abstract: In this paper, we present a stochastic forward-backward-half forward splitting algorithm with variance reduction for solving the structured monotone inclusion problem composed of a maximally monotone operator, a maximally monotone operator and a cocoercive operator in a separable real Hilbert space. By deffining a Lyapunov function, we establish the weak almost sure convergence of the proposed algorithm, and obtain the linear convergence when one of the maximally monotone operators is strongly monotone. Numerical examples are provided to show the performance of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00272v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liqian Qin, Yaxuan Zhang, Qiao-Li Dong, Michael Th. Rassias</dc:creator>
    </item>
    <item>
      <title>Theoretical smoothing frameworks for nonsmooth simple bilevel problems</title>
      <link>https://arxiv.org/abs/2401.17852</link>
      <description>arXiv:2401.17852v2 Announce Type: replace 
Abstract: Bilevel programming has recently received a great deal of attention due to its abundant applications in many areas. The optimal value function approach provides a useful reformulation of the bilevel problem, but its utility is often limited due to the nonsmoothness of the value function even in cases when the associated lower-level function is smooth. In this paper, we present two smoothing strategies for the value function associated with lower-level functions that are not necessarily smooth but are Lipschitz continuous. The first method employs quadratic regularization for partially convex lower-level functions, while the second utilizes entropic regularization for general lower-level objective functions. Meanwhile, the property known as gradient consistency is crucial in ensuring that a designed smoothing algorithm is globally subsequentially convergent to stationary points of the value function reformulation. With this motivation, we prove that the proposed smooth approximations satisfy the gradient consistent property under certain conditions on the lower-level function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17852v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Harold Alcantara, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>The Inverse Carson's Equations Problem: Definition, Implementation and Numerical Experiments</title>
      <link>https://arxiv.org/abs/2404.08210</link>
      <description>arXiv:2404.08210v2 Announce Type: replace 
Abstract: In recent years, with the increase in renewable energy and storage penetration, power flow studies in low-voltage networks have become of interest in both industry and academia. Many studies use impedance represented by sequence components due to the lack of datasets with fully parameterized impedance matrices. This assumes that the network impedance is balanced, which is typically not the case in the low-voltage network and therefore risks the accuracy of the study. This paper proposes a methodology for the recovery of more detailed impedance data from sequence components as an inverse problem, i.e. the inverse Carson's equations problem, for both overhead lines and cables. We consider discrete properties like material and configuration of conductors common in the distribution network and investigate what data can be reliably recovered from only sequence components using nonlinear optimisation models. Presented results include uniqueness of recovered variables and the likelihood of mismatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08210v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ching Hong Tam, Frederik Geth, Nadarajah Mithulananthan</dc:creator>
    </item>
    <item>
      <title>Doubly relaxed forward-Douglas--Rachford splitting for the sum of two nonconvex and a DC function</title>
      <link>https://arxiv.org/abs/2405.08485</link>
      <description>arXiv:2405.08485v2 Announce Type: replace 
Abstract: In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form. This problem class covers several important structures in the literature including the sum of three functions and the general DC program. We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem. The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part. Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings. We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on a low rank matrix completion problem and a simutaneously sparse and low-rank matrix estimation problem. Our algorithm exhibits very competitive results compared to notable algorithms in the literature, on both synthetic data and public dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08485v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10915-025-02950-w</arxiv:DOI>
      <arxiv:journal_reference>Journal of Scientific Computing 104 (2025), no. 2, Paper No. 35</arxiv:journal_reference>
      <dc:creator>Minh N. Dao, Tan Nhat Pham, Phan Thanh Tung</dc:creator>
    </item>
    <item>
      <title>Continuous Relaxation of Discontinuous Shrinkage Operator: Proximal Inclusion and Conversion</title>
      <link>https://arxiv.org/abs/2409.05316</link>
      <description>arXiv:2409.05316v3 Announce Type: replace 
Abstract: We present a principled way of deriving a continuous relaxation of a given discontinuous shrinkage operator, which is based on two fundamental results, proximal inclusion and conversion. Using our results, the discontinuous operator is converted, via double inversion, to a continuous operator; more precisely, the associated ``set-valued'' operator is converted to a ``single-valued'' Lipschitz continuous operator. The first illustrative example is the firm shrinkage operator which can be derived as a continuous relaxation of the hard shrinkage operator. We also derive a new operator as a continuous relaxation of the discontinuous shrinkage operator associated with the so-called reversely ordered weighted L1 (ROWL) penalty. Numerical examples demonstrate potential advantages of the continuous relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05316v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Yukawa</dc:creator>
    </item>
    <item>
      <title>Tight Lower Bounds under Asymmetric High-Order H\"older Smoothness and Uniform Convexity</title>
      <link>https://arxiv.org/abs/2409.10773</link>
      <description>arXiv:2409.10773v3 Announce Type: replace 
Abstract: In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order H\"older smooth and uniformly convex functions. Specifically, for a function whose $p^{th}$-order derivatives are H\"older continuous with degree $\nu$ and parameter $H$, and that is uniformly convex with degree $q$ and parameter $\sigma$, we focus on two asymmetric cases: (1) $q &gt; p + \nu$, and (2) $q &lt; p+\nu$. Given up to $p^{th}$-order oracle access, we establish worst-case oracle complexities of $\Omega\left( \left( \frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}\left( \frac{\sigma}{\epsilon}\right)^\frac{2(q-p-\nu)}{q(3(p+\nu)-2)}\right)$ in the first case with an $\ell_\infty$-ball-truncated-Gaussian smoothed hard function and $\Omega\left(\left(\frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}+ \log\log\left(\left(\frac{\sigma^{p+\nu}}{H^q}\right)^\frac{1}{p+\nu-q}\frac{1}{\epsilon}\right)\right)$ in the second case, for reaching an $\epsilon$-approximate solution in terms of the optimality gap. Our analysis generalizes previous lower bounds for functions under first- and second-order smoothness as well as those for uniformly convex functions, and furthermore our results match the corresponding upper bounds in this general setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10773v3</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025: https://openreview.net/forum?id=fMTPkDEhLQ</arxiv:journal_reference>
      <dc:creator>Cedar Site Bai, Brian Bullins</dc:creator>
    </item>
    <item>
      <title>Learning to Stop: Deep Learning for Mean Field Optimal Stopping</title>
      <link>https://arxiv.org/abs/2410.08850</link>
      <description>arXiv:2410.08850v2 Announce Type: replace 
Abstract: Optimal stopping is a fundamental problem in optimization with applications in risk management, finance, robotics, and machine learning. We extend the standard framework to a multi-agent setting, named multi-agent optimal stopping (MAOS), where agents cooperate to make optimal stopping decisions in a finite-space, discrete-time environment. Since solving MAOS becomes computationally prohibitive as the number of agents is very large, we study the mean-field optimal stopping (MFOS) problem, obtained as the number of agents tends to infinity. We establish that MFOS provides a good approximation to MAOS and prove a dynamic programming principle (DPP) based on mean-field control theory. We then propose two deep learning approaches: one that learns optimal stopping decisions by simulating full trajectories and another that leverages the DPP to compute the value function and to learn the optimal stopping rule using backward induction. Both methods train neural networks to approximate optimal stopping policies. We demonstrate the effectiveness and the scalability of our work through numerical experiments on 6 different problems in spatial dimension up to 300. To the best of our knowledge, this is the first work to formalize and computationally solve MFOS in discrete time and finite space, opening new directions for scalable MAOS methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08850v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Magnino, Yuchen Zhu, Mathieu Lauri\`ere</dc:creator>
    </item>
    <item>
      <title>Optimal transport and regularity of weak Kantorovich potentials on a globally hyperbolic spacetime</title>
      <link>https://arxiv.org/abs/2412.01012</link>
      <description>arXiv:2412.01012v2 Announce Type: replace 
Abstract: We consider the optimal transportation problem on a globally hyperbolic spacetime for some cost function $c_2$, which corresponds to the optimal transportation problem on a complete Riemannian manifold where the cost function is the Riemannian distance squared. Building on insights from previous studies on the Riemannian and Lorentzian case, our main goal is to investigate the regularity of $\pi$-solutions (weak versions of Kantorovich potentials), from which we can conclude, in a classical way, the existence, uniqueness and structure of an optimal transport map between given Borel probability measures $\mu$ and $\nu$, under suitable assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01012v2</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.DG</category>
      <category>math.MP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec Metsch</dc:creator>
    </item>
    <item>
      <title>An Adaptively Inexact Method for Bilevel Learning Using Primal-Dual Style Differentiation</title>
      <link>https://arxiv.org/abs/2412.06436</link>
      <description>arXiv:2412.06436v3 Announce Type: replace 
Abstract: We consider a bilevel learning framework for learning linear operators. In this framework, the learnable parameters are optimized via a loss function that also depends on the minimizer of a convex optimization problem (denoted lower-level problem). We utilize an iterative algorithm called `piggyback' to compute the gradient of the loss and minimizer of the lower-level problem. Given that the lower-level problem is solved numerically, the loss function and thus its gradient can only be computed inexactly. To estimate the accuracy of the computed hypergradient, we derive an a-posteriori error bound, which provides guides for setting the tolerance for the lower-level problem, as well as the piggyback algorithm. To efficiently solve the upper-level optimization, we also propose an adaptive method for choosing a suitable step-size. To illustrate the proposed method, we consider a few learned regularizer problems, such as training an input-convex neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06436v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Bogensperger, Matthias J. Ehrhardt, Thomas Pock, Mohammad Sadegh Salehi, Hok Shing Wong</dc:creator>
    </item>
    <item>
      <title>Preference Robust Ordinal Priority Approach with Preference Elicitation under Incomplete Information for Multi-Attribute Robust Ranking and Selection</title>
      <link>https://arxiv.org/abs/2412.12690</link>
      <description>arXiv:2412.12690v2 Announce Type: replace 
Abstract: Ordinal Priority Approach (OPA) has recently been proposed to determine the weights of experts, attributes, and alternatives using ordinal preference without precise information for multi-attribute ranking and selection (MARS). This study extends OPA with preference elicitation under incomplete information to counter the parametric and preference uncertainty within MARS. Specifically, we propose Preference Robust Ordinal Priority Approach (OPA-PR) within a two-stage optimization framework to generalize marginal utility structure and resolve ambiguity in ranking parameters and utility preferences. In the first stage, the worst-case marginal utility functions are elicited from utility preference ambiguity sets, characterized by monotonicity, normalization, concavity, and Lipschitz continuity for global information, and moment-type preference elicitation for the local. In the second stage, decision weights are optimized based on the elicited marginal utility functions, considering the ranking parameters within norm-, budget-, and conditional value-at-risk-based ambiguity sets. We derive tractable reformulations of OPA-PR, especially through piecewise linear approximation for the marginal utility preference ambiguity sets for the first stage. This approximation is verified by the error bounds for both stages, establishing the foundation of preference elicitation strategy design. The proposed approach is demonstrated through a numerical experiment on the emergency supplier selection problem, including the case, sensitivity, and comparison tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12690v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renlong Wang</dc:creator>
    </item>
    <item>
      <title>Discrete lossless convexification for pointing constraints</title>
      <link>https://arxiv.org/abs/2501.06931</link>
      <description>arXiv:2501.06931v3 Announce Type: replace 
Abstract: Discrete Lossless Convexification (DLCvx) formulates a convex relaxation for a specific class of discrete-time non-convex optimal control problems. It establishes sufficient conditions under which the solution of the relaxed problem satisfies the original non-convex constraints at specified time grid points. Furthermore, it provides an upper bound on the number of time grid points where these sufficient conditions may not hold, and thus the original constraints could be violated. This paper extends DLCvx to problems with control pointing constraints. Additionally, it introduces a novel DLCvx formulation for mixed-integer optimal control problems in which the control is either inactive or constrained within an annular sector. This formulation broadens the feasible space for problems with pointing constraints. A numerical example is provided to illustrate its application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06931v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dayou Luo, Fabio Spada, Beh\c{c}et A\c{c}{\i}kme\c{s}e</dc:creator>
    </item>
    <item>
      <title>On the Instability of Nesterov's ODE under Non-Conservative Vector Fields</title>
      <link>https://arxiv.org/abs/2501.13244</link>
      <description>arXiv:2501.13244v2 Announce Type: replace 
Abstract: We study the instability properties of Nesterov's ODE in non-conservative settings, where the driving term is not necessarily the gradient of a potential function. While convergence properties under Nesterov's ODE are well-characterized for optimization settings with gradient-based driving terms, we show that the presence of arbitrarily small non-conservative terms can lead to instability, a phenomenon previously observed empirically via numerical studies in optimization and game-theoretic problems. Our instability analysis combines multi-time scale techniques, such as averaging via variations-of-constants formula, and Floquet Theory, focusing on systems where the vector field is linear and its Helmholtz decomposition reveals a non-vanishing non-conservative component. To resolve the instability issue, the dynamics under non-vanishing non-conservative components, we study a regularization mechanism based on restarting. The resulting system is a hybrid dynamical system that mirrors Nesterov's ODE during intervals of flow, and implements resets of the momentum state through discrete periodic jumps. For this hybrid system, we establish novel explicit bounds on the resetting period that ensure the decrease of a suitable Lyapunov function, guaranteeing not only stability but also "accelerated" convergence rates under suitable smoothness and strong monotonicity properties on the driving term. Numerical simulations support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13244v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Ochoa, Mahmoud Abdelgalil, Jorge I. Poveda</dc:creator>
    </item>
    <item>
      <title>Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties via Sequential Covariance Steering</title>
      <link>https://arxiv.org/abs/2502.01907</link>
      <description>arXiv:2502.01907v2 Announce Type: replace 
Abstract: Spacecraft operations are influenced by uncertainties such as dynamics modeling, navigation, and maneuver execution errors. Although mission design has traditionally incorporated heuristic safety margins to mitigate the effect of uncertainties, particularly before/after crucial events, it is yet unclear whether this practice will scale in the cislunar region, which features locally chaotic nonlinear dynamics and involves frequent lunar flybys. This paper applies chance-constrained covariance steering and sequential convex programming to simultaneously design an optimal trajectory and trajectory correction policy that can probabilistically guarantee safety constraints under the assumed physical/navigational error models. The results show that the proposed method can effectively control the state uncertainty in a highly nonlinear environment. The framework allows faster computation and lossless convexification of linear covariance propagation compared to existing methods, enabling a rapid and accurate comparison of $\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the algorithm on several transfers in the Earth-Moon Circular Restricted Three Body Problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01907v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoya Kumagai, Kenshiro Oguri</dc:creator>
    </item>
    <item>
      <title>A relax-fix-and-exclude algorithm for an MINLP problem with multilinear interpolations</title>
      <link>https://arxiv.org/abs/2502.21249</link>
      <description>arXiv:2502.21249v2 Announce Type: replace 
Abstract: This paper introduces a novel algorithm for Mixed-Integer Nonlinear Programming (MINLP) problems with multilinear interpolations of look-up tables. These problems arise when objective or constraints contain black-box functions only known at a finite set of evaluations on a predefined grid. We derive a piecewise-linear relaxation for the multilinear constraints resulting from the multilinear interpolations used to approximate the true functions. Supported by the fact that our proposed relaxation defines the convex hull of the original problem, we propose a novel algorithm that iteratively solves the MILP relaxation and refines the solution space through variable fixing and exclusion strategies. This approach ensures convergence to an optimal solution, which we demonstrate, while maintaining computational efficiency. We apply the proposed algorithm to a real-world offshore oil production optimization problem. In comparison to the Gurobi solver, our algorithm was able to find the optimal solution at least four times faster, and to consistently provide better incumbents under limited time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21249v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Machado Pacheco, Pedro Marcolin Antunes, Eduardo Camponogara, Laio Oriel Seman, Vin\'icius Ramos Rosa, Bruno Ferreira Vieira, Cesar Longhi</dc:creator>
    </item>
    <item>
      <title>The global convergence time of stochastic gradient descent in non-convex landscapes: Sharp estimates via large deviations</title>
      <link>https://arxiv.org/abs/2503.16398</link>
      <description>arXiv:2503.16398v2 Announce Type: replace 
Abstract: In this paper, we examine the time it takes for stochastic gradient descent (SGD) to reach the global minimum of a general, non-convex loss function. We approach this question through the lens of randomly perturbed dynamical systems and large deviations theory, and we provide a tight characterization of the global convergence time of SGD via matching upper and lower bounds. These bounds are dominated by the most "costly" set of obstacles that the algorithm may need to overcome in order to reach a global minimizer from a given initialization, coupling in this way the global geometry of the underlying loss landscape with the statistics of the noise entering the process. Finally, motivated by applications to the training of deep neural networks, we also provide a series of refinements and extensions of our analysis for loss functions with shallow local minima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16398v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wa\"iss Azizian, Franck Iutzeler, J\'er\^ome Malick, Panayotis Mertikopoulos</dc:creator>
    </item>
    <item>
      <title>The tangent cone to the real determinantal variety: various expressions and a proof</title>
      <link>https://arxiv.org/abs/2504.11382</link>
      <description>arXiv:2504.11382v2 Announce Type: replace 
Abstract: The set of real matrices of upper-bounded rank is a real algebraic variety called the real generic determinantal variety. An explicit description of the tangent cone to that variety is given in Theorem 3.2 of Schneider and Uschmajew [SIAM J. Optim., 25 (2015), pp. 622-646]. The present paper shows that the proof therein is incomplete and provides a proof. It also reviews equivalent descriptions of the tangent cone to that variety. Moreover, it shows that the tangent cone and the algebraic tangent cone to that variety coincide, which is not true for all real algebraic varieties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11382v2</guid>
      <category>math.OC</category>
      <category>math.AG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Olikier, Petar Mlinari\'c, P. -A. Absil, Andr\'e Uschmajew</dc:creator>
    </item>
    <item>
      <title>Preference Disaggregation Analysis with Criteria Selection in a Regularization Framework</title>
      <link>https://arxiv.org/abs/2505.20111</link>
      <description>arXiv:2505.20111v2 Announce Type: replace 
Abstract: Limited by cognitive abilities, decision-makers (DMs) may struggle to evaluate decision alternatives based on all criteria in multiple criteria decision-making problems. This paper proposes an embedded criteria selection method derived from preference disaggregation technique and regularization theory. The method aims to infer the criteria and value functions used by the DM to evaluate decision alternatives. It measures the quality of criteria subsets by investigating both the empirical error (fitting ability of value functions to preference information) and generalization error (complexity of value functions). Unlike existing approaches that consider only the deviation from linearity as a measure of complexity, we argue that the number of marginal value functions also affects complexity. To address this, we use 0-1 variables to indicate whether a criterion is selected in the value function or not, and construct a criteria selection model with the trade-off between empirical and generalization errors as the objective function. If the criteria are sufficiently discriminative, we identify all supporting criteria sets that can restore preference information without unnecessary criteria. We further analyze the likelihood of criteria being selected by the DM. Finally, the effectiveness of the proposed method is demonstrated by applying it to an example of the green supplier selection problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20111v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.omega.2024.103252</arxiv:DOI>
      <arxiv:journal_reference>Omega, 133, 103252 (2025)</arxiv:journal_reference>
      <dc:creator>Kun Zhou, Zaiwu Gong, Guo Wei, Roman Slowinski</dc:creator>
    </item>
    <item>
      <title>Disjunctive Benders Decomposition</title>
      <link>https://arxiv.org/abs/2506.03561</link>
      <description>arXiv:2506.03561v2 Announce Type: replace 
Abstract: We propose a novel enhancement to Benders Decomposition (BD) that generates valid inequalities for the convex hull of the Benders reformulation, addressing a key limitation of conventional BD-its cuts are typically tight only for the continuous relaxation. Our method efficiently integrates disjunctive programming theory with BD, introducing a new routine that leverages existing cut-generating oracles for uncovering constraints required to construct valid inequalities for the convex hull. For mixed-binary linear programs, this approach eliminates the need to solve the master problem as a mixed-integer program. Additionally, we extend the a posteriori strengthening and lifting procedure for lift-and-project cuts into the BD framework, and present an approximate routine for generating lift-and-project cuts. Numerical results on large-scale instances show that our approach significantly reduces the number of branch-and-bound nodes required to reach the lower bound achieved by conventional BD, often by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03561v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiwen Fang, Inho Sin, Geunyeong Byeon</dc:creator>
    </item>
    <item>
      <title>Analysis of Thompson Sampling for Controlling Unknown Linear Diffusion Processes</title>
      <link>https://arxiv.org/abs/2206.09977</link>
      <description>arXiv:2206.09977v2 Announce Type: replace-cross 
Abstract: Linear diffusion processes serve as canonical continuous-time models for dynamic decision-making under uncertainty. These systems evolve according to drift matrices that specify the instantaneous rates of change in the expected system state, while also experiencing continuous random disturbances modeled by Brownian noise. For instance, in medical applications such as artificial pancreas systems, the drift matrices represent the internal dynamics of glucose concentrations. Classical results in stochastic control provide optimal policies under perfect knowledge of the drift matrices. However, practical decision-making scenarios typically feature uncertainty about the drift; in medical contexts, such parameters are patient-specific and unknown, requiring adaptive policies for efficiently learning the drift matrices while ensuring system stability and optimal performance.
  We study the Thompson sampling (TS) algorithm for decision-making in linear diffusion processes with unknown drift matrices. For this algorithm that designs control policies as if samples from a posterior belief about the parameters fully coincide with the unknown truth, we establish efficiency. That is, Thompson sampling learns optimal control actions fast, incurring only a square-root of time regret, and also learns to stabilize the system in a short time period. To our knowledge, this is the first such result for TS in a diffusion process control problem. Moreover, our empirical simulations in three settings that involve blood-glucose and flight control demonstrate that TS significantly improves regret, compared to the state-of-the-art algorithms, suggesting it explores in a more guarded fashion. Our theoretical analysis includes characterization of a certain optimality manifold that relates the geometry of the drift matrices to the optimal control of the diffusion process, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09977v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamad Kazem Shirani Faradonbeh, Sadegh Shirani, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity</title>
      <link>https://arxiv.org/abs/2210.16402</link>
      <description>arXiv:2210.16402v3 Announce Type: replace-cross 
Abstract: We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing clients to perform multiple local gradient-type training steps before communication. In a recent breakthrough, Mishchenko et al. (2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their ProxSkip method requires all clients to take the same number of local training steps in each communication round. We propose a redesign of the ProxSkip method, allowing clients with ``less important'' data to get away with fewer local training steps without impacting the overall communication complexity of the method. In particular, we prove that our modified method, GradSkip, converges linearly under the same assumptions and has the same accelerated communication complexity, while the number of local gradient steps can be reduced relative to a local condition number. We further generalize our method by extending the randomness of probabilistic alternations to arbitrary unbiased compression operators and by considering a generic proximable regularizer. This generalization, which we call GradSkip+, recovers several related methods in the literature as special cases. Finally, we present an empirical study on carefully designed toy problems that confirm our theoretical claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.16402v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artavazd Maranjyan, Mher Safaryan, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding</title>
      <link>https://arxiv.org/abs/2304.03907</link>
      <description>arXiv:2304.03907v5 Announce Type: replace-cross 
Abstract: This paper proposes an approach, Spectral Dynamics Embedding Control (SDEC), to optimal control for nonlinear stochastic systems. This method reveals an infinite-dimensional feature representation induced by the system's nonlinear stochastic dynamics, enabling a linear representation of the state-action value function. For practical implementation, this representation is approximated using finite-dimensional trucations, specifically via two prominent kernel approximation methods: random feature truncation and Nystrom approximation. To characterize the effectiveness of these approximations, we provide an in-depth theoretical analysis to characterize the approximation error arising from the finite-dimension truncation and statistical error due to finite-sample approximation in both policy evaluation and policy optimization. Empirically, our algorithm performs favorably against existing stochastic control algorithms on several benchmark problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03907v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaolin Ren, Tongzheng Ren, Haitong Ma, Na Li, Bo Dai</dc:creator>
    </item>
    <item>
      <title>TS-RSR: A provably efficient approach for batch Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2403.04764</link>
      <description>arXiv:2403.04764v4 Announce Type: replace-cross 
Abstract: This paper presents a new approach for batch Bayesian Optimization (BO) called Thompson Sampling-Regret to Sigma Ratio directed sampling (TS-RSR), where we sample a new batch of actions by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our sampling objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. Theoretically, we provide rigorous convergence guarantees on our algorithm's regret, and numerically, we demonstrate that our method attains state-of-the-art performance on a range of challenging synthetic and realistic test functions, where it outperforms several competitive benchmark batch BO algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04764v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaolin Ren, Na Li</dc:creator>
    </item>
    <item>
      <title>Improved Sample Complexity for Private Nonsmooth Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2410.05880</link>
      <description>arXiv:2410.05880v2 Announce Type: replace-cross 
Abstract: We study differentially private (DP) optimization algorithms for stochastic and empirical objectives which are neither smooth nor convex, and propose methods that return a Goldstein-stationary point with sample complexity bounds that improve on existing works. We start by providing a single-pass $(\epsilon,\delta)$-DP algorithm that returns an $(\alpha,\beta)$-stationary point as long as the dataset is of size $\widetilde{\Omega}(\sqrt{d}/\alpha\beta^{3}+d/\epsilon\alpha\beta^{2})$, which is $\Omega(\sqrt{d})$ times smaller than the algorithm of Zhang et al. [2024] for this task, where $d$ is the dimension. We then provide a multi-pass polynomial time algorithm which further improves the sample complexity to $\widetilde{\Omega}\left(d/\beta^2+d^{3/4}/\epsilon\alpha^{1/2}\beta^{3/2}\right)$, by designing a sample efficient ERM algorithm, and proving that Goldstein-stationary points generalize from the empirical loss to the population loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05880v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guy Kornowski, Daogao Liu, Kunal Talwar</dc:creator>
    </item>
    <item>
      <title>Improving the convergence of Markov chains via permutations and projections</title>
      <link>https://arxiv.org/abs/2411.08295</link>
      <description>arXiv:2411.08295v3 Announce Type: replace-cross 
Abstract: This paper aims at improving the convergence to equilibrium of finite ergodic Markov chains via permutations and projections. First, we prove that a specific mixture of permuted Markov chains arises naturally as a projection under the KL divergence or the squared-Frobenius norm. We then compare various mixing properties of the mixture with other competing Markov chain samplers and demonstrate that it enjoys improved convergence. This geometric perspective motivates us to propose samplers based on alternating projections to combine different permutations and to analyze their rate of convergence. We give necessary, and under some additional assumptions also sufficient, conditions for the projection to achieve stationarity in the limit in terms of the trace of the transition matrix. We proceed to discuss tuning strategies of the projection samplers when these permutations are viewed as parameters. Along the way, we reveal connections between the mixture and a Markov chain Sylvester's equation as well as assignment problems, and highlight how these can be used to understand and improve Markov chain mixing. We provide two examples as illustrations. In the first example, the projection sampler (with a suitable choice of the permutation) improves upon Metropolis-Hastings in a discrete bimodal distribution with a reduced relaxation time from exponential to polynomial in the system size, while in the second example, the mixture of permuted Markov chain yields a mixing time that is logarithmic in system size (with high probability under random permutation), compared to a linear mixing time in the Diaconis-Holmes-Neal sampler. Finally, we provide numerical experiments on simple statistical physics models to illustrate the improved mixing performance of the proposed projection samplers over standard Metropolis-Hastings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08295v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Max Hird, Youjia Wang</dc:creator>
    </item>
    <item>
      <title>Safe Navigation in Dynamic Environments using Density Functions</title>
      <link>https://arxiv.org/abs/2411.12206</link>
      <description>arXiv:2411.12206v2 Announce Type: replace-cross 
Abstract: This work presents a density-based framework for safe navigation in dynamic environments characterized by time-varying obstacle sets and time-varying target regions. We propose an analytical construction of time-varying density functions that enables the synthesis of a feedback controller defined as the positive gradient of the resulting density field. The primary contribution of this paper is a rigorous convergence proof demonstrating almost-everywhere safe navigation under the proposed framework, specifically for systems governed by single-integrator dynamics. To the best of our knowledge, these are the first analytical guarantees of their kind for navigation in dynamic environments using density functions. We illustrate the applicability of the framework to systems with more complex dynamics, including multi-agent systems and robotic manipulators, using standard control design techniques such as backstepping and inverse dynamics. These results provide a foundation for extending density-based navigation methods to a broad class of robotic systems operating in time-varying environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12206v2</guid>
      <category>cs.RO</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sriram S. K. S Narayanan, Joseph Moyalan, Umesh Vaidya</dc:creator>
    </item>
    <item>
      <title>Turbulence modeling over riblets via domain transformation</title>
      <link>https://arxiv.org/abs/2501.03542</link>
      <description>arXiv:2501.03542v2 Announce Type: replace-cross 
Abstract: Numerical and experimental studies have demonstrated the drag-reducing potential of carefully designed streamwise-elongated riblets in lowering skin-friction drag. To support the systematic design of such surface corrugations, recent efforts have integrated simplified versions of the governing equations with innovative methods for representing the effects of rough boundaries on flow dynamics. Notably, the statistical response of the eddy-viscosity-enhanced linearized Navier-Stokes equations has been shown to effectively capture the ability of riblets in suppressing turbulence, quantify the influence of background turbulence on the mean velocity, and reproduce established drag-reduction trends. In this paper, we enhance the flexibility and computational efficiency of this simulation-free approach by implementing a domain transformation for surface representation, along with a perturbation analysis on a small geometric parameter of the riblets. While domain transformation complicates the differential equations, it provides accurate boundary representations and facilitates the analysis of complex riblet shapes at high Reynolds numbers by enabling perturbation analysis to simplify the dimensional complexity of the governing equations. Our method successfully predicts drag reduction trends for triangular and scalloped riblets, consistent with existing literature. We further utilize our framework to investigate flow mechanisms influenced by riblets and extend our study to channel flows with friction Reynolds numbers up to 2003. Our findings reveal the emergence of K-H rollers over large and sharp scalloped riblets, contributing to the degradation of drag reduction in these geometries. Additionally, we examine the impact of riblets on near-wall flow structures, focusing on their suppression of streamwise-elongated structures in flows over large riblets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03542v2</guid>
      <category>physics.flu-dyn</category>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadamin Naseri, Armin Zare</dc:creator>
    </item>
    <item>
      <title>GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models</title>
      <link>https://arxiv.org/abs/2501.12956</link>
      <description>arXiv:2501.12956v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQ's ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup over the baseline, advancing memory and inference efficiency in LLM deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12956v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengxiang Zhao, Xiaoming Yuan</dc:creator>
    </item>
    <item>
      <title>On the kernel learning problem</title>
      <link>https://arxiv.org/abs/2502.11665</link>
      <description>arXiv:2502.11665v2 Announce Type: replace-cross 
Abstract: The classical kernel ridge regression problem aims to find the best fit for the output $Y$ as a function of the input data $X\in \mathbb{R}^d$, with a fixed choice of regularization term imposed by a given choice of a reproducing kernel Hilbert space, such as a Sobolev space. Here we consider a generalization of the kernel ridge regression problem, by introducing an extra matrix parameter $U$, which aims to detect the scale parameters and the feature variables in the data, and thereby improve the efficiency of kernel ridge regression. This naturally leads to a nonlinear variational problem to optimize the choice of $U$. We study various foundational mathematical aspects of this variational problem, and in particular how this behaves in the presence of multiscale structures in the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11665v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.CA</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Li, Feng Ruan</dc:creator>
    </item>
    <item>
      <title>MAGPIE: Multilevel-Adaptive-Guided Solver for Ptychographic Phase Retrieval</title>
      <link>https://arxiv.org/abs/2504.10118</link>
      <description>arXiv:2504.10118v4 Announce Type: replace-cross 
Abstract: We introduce MAGPIE (Multilevel-Adaptive-Guided Ptychographic Iterative Engine), a stochastic multigrid solver for the ptychographic phase-retrieval problem. The ptychographic phase-retrieval problem is inherently nonconvex and ill-posed. To address these challenges, we reformulate the original nonlinear and nonconvex inverse problem as the iterative minimization of a quadratic surrogate model that majorizes the original objective. This surrogate not only ensures favorable convergence properties but also generalizes the Ptychographic Iterative Engine (PIE) family of algorithms. By solving the surrogate model using a multigrid method, MAGPIE achieves substantial gains in convergence speed and reconstruction quality over traditional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10118v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Borong Zhang, Qin Li, Zichao Wendy Di</dc:creator>
    </item>
    <item>
      <title>Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy</title>
      <link>https://arxiv.org/abs/2505.07614</link>
      <description>arXiv:2505.07614v2 Announce Type: replace-cross 
Abstract: Recent advancements in machine learning have improved performance while also increasing computational demands. While federated and distributed setups address these issues, their structure is vulnerable to malicious influences. In this paper, we address a specific threat, Byzantine attacks, where compromised clients inject adversarial updates to derail global convergence. We combine the trust scores concept with trial function methodology to dynamically filter outliers. Our methods address the critical limitations of previous approaches, allowing functionality even when Byzantine nodes are in the majority. Moreover, our algorithms adapt to widely used scaled methods like Adam and RMSProp, as well as practical scenarios, including local training and partial participation. We validate the robustness of our methods by conducting extensive experiments on both synthetic and real ECG data collected from medical institutions. Furthermore, we provide a broad theoretical analysis of our algorithms and their extensions to aforementioned practical setups. The convergence guarantees of our methods are comparable to those of classical algorithms developed without Byzantine interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07614v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gleb Molodtsov, Daniil Medyakov, Sergey Skorik, Nikolas Khachaturov, Shahane Tigranyan, Vladimir Aletov, Aram Avetisyan, Martin Tak\'a\v{c}, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>On long-duration storage, weather uncertainty and limited foresight</title>
      <link>https://arxiv.org/abs/2505.12538</link>
      <description>arXiv:2505.12538v2 Announce Type: replace-cross 
Abstract: Long-duration energy storage (LDES) is a key component for fully renewable, sector-coupled energy systems based on wind and solar. While capacity expansion planning has begun to take into account interannual weather variability, it often ignores weather uncertainty and limited foresight in capacity and operational decisions. We build a stochastic capacity expansion model for fully decarbonized energy systems with LDES in Europe accounting for weather uncertainty - isolating the effect of limited foresight by comparing it to a perfect foresight benchmark. Under limited foresight, LDES acts as a hedge against extreme system states operating defensively and exhibiting a stockpiling effect absent under perfect foresight. Solar PV gains in system value for its higher predictability with up to 25\% higher capacities versus the benchmark while onshore wind capacities are lower. We shed light on the underlying mechanisms by deriving implicit LDES bidding curves. We show that LDES bids reflect the costs and the weather-dependent probability of extreme system states conditional on the current system state. This has important implications for the price formation on renewable electricity markets, as a wide and continuous range of probabilistic LDES bids alleviates concerns of extreme price disparity at high renewable shares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12538v2</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Schmidt</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Learning in Survival Analysis</title>
      <link>https://arxiv.org/abs/2506.01348</link>
      <description>arXiv:2506.01348v2 Announce Type: replace-cross 
Abstract: We introduce an innovative approach that incorporates a Distributionally Robust Learning (DRL) approach into Cox regression to enhance the robustness and accuracy of survival predictions. By formulating a DRL framework with a Wasserstein distance-based ambiguity set, we develop a variant Cox model that is less sensitive to assumptions about the underlying data distribution and more resilient to model misspecification and data perturbations. By leveraging Wasserstein duality, we reformulate the original min-max DRL problem into a tractable regularized empirical risk minimization problem, which can be computed by exponential conic programming. We provide guarantees on the finite sample behavior of our DRL-Cox model. Moreover, through extensive simulations and real world case studies, we demonstrate that our regression model achieves superior performance in terms of prediction accuracy and robustness compared with traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01348v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeping Jin, Lauren Wise, Ioannis Ch. Paschalidis</dc:creator>
    </item>
  </channel>
</rss>
