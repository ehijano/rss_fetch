<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 04:04:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the statistical nature of Betti numbers and Euler characteristic of smooth random fields</title>
      <link>https://arxiv.org/abs/2507.06255</link>
      <description>arXiv:2507.06255v1 Announce Type: new 
Abstract: We represent excursion sets of smooth random fields as unions of a topological basis consisting of a sequence of simply and multiply connected compact subsets of the underlying manifold. The associated coefficients, which are non-negative discrete random variables, reflect the randomness of the field. Betti numbers of the excursion sets can be expressed as summations over the coefficients, and the Euler characteristic and the sum of Betti numbers can also be expressed as their (alternating) sum. This enables understanding their statistical properties as sums (or differences) of discrete random variables. We examine the conditions under which each topological statistic can be asymptotically Gaussian as the size of the manifold and the resolution increase. The coefficients of the basis elements are then modeled as Binomial variables, and the statistical natures of Betti numbers, Euler character and sum of Betti numbers follow from this fundamental property. We test the validity of the modeling using numerical calculations, and identify threshold regimes where the topological statistics can be approximated as Gaussian variables. The new representation of excursion sets thus maps the properties of topological statistics to combinatorial structures, thereby providing mathematical clarity on their use for physical inference, particularly in cosmology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06255v1</guid>
      <category>math.ST</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pravabati Chingangbam</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic confidence regions on RKHS. The Paley-Wiener and standard Sobolev space cases</title>
      <link>https://arxiv.org/abs/2507.06657</link>
      <description>arXiv:2507.06657v1 Announce Type: new 
Abstract: We consider the problem of constructing a global, probabilistic, and non-asymptotic confidence region for an unknown function observed on a random design. The unknown function is assumed to lie in a reproducing kernel Hilbert space (RKHS). We show that this construction can be reduced to accurately estimating the RKHS norm of the unknown function. Our analysis primarily focuses both on the Paley-Wiener and on the standard Sobolev space settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06657v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrice Gamboa (IMT, RT-UQ, ANITI), Olivier Roustant (IMT, INSA Toulouse, RT-UQ, ANITI)</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Inference for Stochastic Reaction-Diffusion Equations</title>
      <link>https://arxiv.org/abs/2507.06857</link>
      <description>arXiv:2507.06857v1 Announce Type: new 
Abstract: We consider the Bayesian nonparametric estimation of a nonlinear reaction function in a reaction-diffusion stochastic partial differential equation (SPDE). The likelihood is well-defined and tractable by the infinite-dimensional Girsanov theorem, and the posterior distribution is analysed in the growing domain asymptotic. Based on a Gaussian wavelet prior, the contraction of the posterior distribution around the truth at the minimax optimal rate is proved. The analysis of the posterior distribution is complemented by a semiparametric Bernstein--von Mises theorem. The proofs rely on the sub-Gaussian concentration of spatio-temporal averages of transformations of the SPDE, which is derived by combining the Clark-Ocone formula with bounds for the derivatives of the (marginal) densities of the SPDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06857v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Randolf Altmeyer, Sascha Gaudlitz</dc:creator>
    </item>
    <item>
      <title>Binomiality of colored Gaussian models</title>
      <link>https://arxiv.org/abs/2507.06437</link>
      <description>arXiv:2507.06437v1 Announce Type: cross 
Abstract: Following earlier work by Coons-Maraj-Misra-Sorea and Misra-Sullivant, we study colored, undirected Gaussian graphical models, and present a necessary and sufficient condition for such a model to have binomial vanishing ideal. These conditions involve Jordan schemes, a variant of association schemes, well-known structures in algebraic combinatorics. Using association schemes without transitive group action, we refute the conjecture by Coons-Maraj-Misra-Sorea that binomiality implies that the color classes must be orbits under the automorphism group of the colored graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06437v1</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Biaggi, Jan Draisma, Magdal\'ena Mi\v{s}inov\'a</dc:creator>
    </item>
    <item>
      <title>A powerful transformation of quantitative responses for biobank-scale association studies</title>
      <link>https://arxiv.org/abs/2507.06496</link>
      <description>arXiv:2507.06496v1 Announce Type: cross 
Abstract: In linear regression models with non-Gaussian errors, transformations of the response variable are widely used in a broad range of applications. Motivated by various genetic association studies, transformation methods for hypothesis testing have received substantial interest. In recent years, the rise of biobank-scale genetic studies, which feature a vast number of participants that could be around half a million, spurred the need for new transformation methods that are both powerful for detecting weak genetic signals and computationally efficient for large-scale data. In this work, we propose a novel transformation method that leverages the information of the error density. This transformation leads to locally most powerful tests and therefore has strong power for detecting weak signals. To make the computation scalable to biobank-scale studies, we harnessed the nature of weak genetic signals and proposed a consistent and computationally efficient estimator of the transformation function. Through extensive simulations and a gene-based analysis of spirometry traits from the UK Biobank, we validate that our approach maintains stringent control over type I error rates and significantly enhances statistical power over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06496v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yaowu Liu, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2507.06525</link>
      <description>arXiv:2507.06525v1 Announce Type: cross 
Abstract: Differential privacy has been proven effective for stochastic gradient descent; however, existing methods often suffer from performance degradation in high-dimensional settings, as the scale of injected noise increases with dimensionality. To tackle this challenge, we propose AdaDPIGU--a new differentially private SGD framework with importance-based gradient updates tailored for deep neural networks. In the pretraining stage, we apply a differentially private Gaussian mechanism to estimate the importance of each parameter while preserving privacy. During the gradient update phase, we prune low-importance coordinates and introduce a coordinate-wise adaptive clipping mechanism, enabling sparse and noise-efficient gradient updates. Theoretically, we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy and retains convergence guarantees. Extensive experiments on standard benchmarks validate the effectiveness of AdaDPIGU. All results are reported under a fixed retention ratio of 60%. On MNIST, our method achieves a test accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at $\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating that adaptive sparsification can enhance both privacy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06525v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huiqi Zhang, Fang Xie</dc:creator>
    </item>
    <item>
      <title>Spectra of high-dimensional sparse random geometric graphs</title>
      <link>https://arxiv.org/abs/2507.06556</link>
      <description>arXiv:2507.06556v1 Announce Type: cross 
Abstract: We analyze the spectral properties of the high-dimensional random geometric graph $ G(n, d, p)$, formed by sampling $n$ i.i.d vectors $\{v_i\}_{i=1}^{n}$ uniformly on a $d$-dimensional unit sphere and connecting each pair $\{i,j\}$ whenever $\langle v_i, v_j \rangle \geq \tau$ so that $p=\mathbb P(\langle v_i,v_j\rangle \geq \tau)$. This model defines a nonlinear random matrix ensemble with dependent entries. We show that if $d =\omega( np\log^{2}(1/p))$ and $np\to\infty$, the limiting spectral distribution of the normalized adjacency matrix $\frac{A}{\sqrt{np(1-p)}}$ is the semicircle law. To our knowledge, this is the first such result for $G(n, d, p)$ in the sparse regime. In the constant sparsity case $p=\alpha/n$, we further show that if $d=\omega(\log^2(n))$ the limiting spectral distribution of $A$ in $G(n,d, \alpha/n)$ coincides with that of the Erd\H{o}s-R\'{e}nyi graph $ G(n,\alpha/n)$. Our approach combines the classical moment method in random matrix theory with a novel recursive decomposition of closed walk graphs, leveraging block cut trees and ear decompositions, to control $\mathbb E \mathrm{tr}(A^k)$. A refined high trace analysis further yields a near-optimal bound on the second eigenvalue when $np=\Omega(\log^4 (n))$, removing technical conditions previously imposed in (Liu et al. 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06556v1</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cao, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>On the rate of convergence to the Boolean extreme value distribution under the von Mises condition</title>
      <link>https://arxiv.org/abs/2507.06580</link>
      <description>arXiv:2507.06580v1 Announce Type: cross 
Abstract: We investigate the rate of convergence toward the Boolean extreme value distribution which is the universal limiting law for the normalized spectral maximum of Boolean independent positive operators, under the von Mises condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06580v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Ueda</dc:creator>
    </item>
    <item>
      <title>Conformal Link Prediction with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2507.07025</link>
      <description>arXiv:2507.07025v1 Announce Type: cross 
Abstract: We propose a new method for predicting multiple missing links in partially observed networks while controlling the false discovery rate (FDR), a largely unresolved challenge in network analysis. The main difficulty lies in handling complex dependencies and unknown, heterogeneous missing patterns. We introduce conformal link prediction ({\tt clp}), a distribution-free procedure grounded in the exchangeability structure of weighted graphon models. Our approach constructs conformal p-values via a novel multi-splitting strategy that restores exchangeability within local test sets, thereby ensuring valid row-wise FDR control, even under unknown missing mechanisms. To achieve FDR control across all missing links, we further develop a new aggregation scheme based on e-values, which accommodates arbitrary dependence across network predictions. Our method requires no assumptions on the missing rates, applies to weighted, unweighted, undirected, and bipartite networks, and enjoys finite-sample theoretical guarantees. Extensive simulations and real-world data study confirm the effectiveness and robustness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07025v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Wenqin Du, Wanteng Ma, Dong Xia, Yuan Zhang, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Heavy-tailed max-linear structural equation models in networks with hidden nodes</title>
      <link>https://arxiv.org/abs/2306.15356</link>
      <description>arXiv:2306.15356v2 Announce Type: replace 
Abstract: Recursive max-linear vectors provide models for causal dependence between large values of random variables that are supported on directed acyclic graphs, but the standard assumption that all nodes of such a graph are observed can be unrealistic. We give necessary and sufficient conditions for a partially observed recursive max-linear vector to be representable as a recursive max-linear (sub-)model and provide a graphical algorithm to construct the latter. Our conditions concern the max-weighted paths of a directed acyclic graph and its minimal representation, which play a key role for such models. In the framework of regular variation we translate these conditions into checkable criteria and establish a connection between max-weighted paths and the extremal dependence measure of transformed variables for pairs of nodes. We propose a statistical algorithm to detect bivariate regularly varying recursive max-linear models among the node variables of a directed acyclic graph and show consistency and asymptotic normality of the estimators of the extremal dependence measure under a thresholding procedure. Simulations show that our algorithm performs satisfactorily. We apply it to nutrition intake data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15356v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Krali, Anthony C. Davison, Claudia Kl\"uppelberg</dc:creator>
    </item>
    <item>
      <title>Online Quantile Regression</title>
      <link>https://arxiv.org/abs/2402.04602</link>
      <description>arXiv:2402.04602v3 Announce Type: replace 
Abstract: This paper addresses the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of features is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only \textsf{ short-term} sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well-controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for squared loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04602v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Dong Xia, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>On the Low-Temperature MCMC threshold: the cases of sparse tensor PCA, sparse regression, and a geometric rule</title>
      <link>https://arxiv.org/abs/2408.00746</link>
      <description>arXiv:2408.00746v3 Announce Type: replace 
Abstract: Over the last years, there has been a significant amount of work studying the power of specific classes of computationally efficient estimators for multiple statistical parametric estimation tasks, including the estimators classes of low-degree polynomials, spectral methods, and others. Despite that, our understanding of the important class of MCMC methods remains quite poorly understood. For instance, for many models of interest, the performance of even zero-temperature (greedy-like) MCMC methods that simply maximize the posterior remains elusive.
  In this work, we provide an easy to check condition under which the low-temperature Metropolis chain maximizes the posterior in polynomial-time with high probability. The result is generally applicable, and in this work, we use it to derive positive MCMC results for two classical sparse estimation tasks: the sparse tensor PCA model and sparse regression. Interestingly, in both cases, we also leverage the Overlap Gap Property framework for inference (Gamarnik, Zadik AoS '22) to prove that our results are tight: no low-temperature local MCMC method can achieve better performance. In particular, our work identifies the "low-temperature (local) MCMC threshold" for both sparse models. Interestingly, in the sparse tensor PCA model our results indicate that low-temperature local MCMC methods significantly underperform compared to other studied time-efficient methods, such as the class of low-degree polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00746v3</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongchen Chen, Conor Sheehan, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank approximations for linear Gaussian inverse problems on Hilbert spaces, Part I: posterior covariance approximation</title>
      <link>https://arxiv.org/abs/2411.01112</link>
      <description>arXiv:2411.01112v4 Announce Type: replace 
Abstract: For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. Using the Feldman--Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\'enyi divergences, the Amari $\alpha$-divergences for $\alpha\in(0,1)$, the Hellinger metric and the Kullback--Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretisation independent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01112v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>On noncentral Wishart mixtures of noncentral Wisharts and their use for testing random effects in factorial design models</title>
      <link>https://arxiv.org/abs/2502.13711</link>
      <description>arXiv:2502.13711v2 Announce Type: replace 
Abstract: It is shown that a noncentral Wishart mixture of noncentral Wishart distributions with the same degrees of freedom yields a noncentral Wishart distribution, thereby extending the main result of Jones and Marchand [Stat 10 (2021), Paper No. e398, 7 pp.] from the chi-square to the Wishart setting. To illustrate its use, this fact is then employed to derive the finite-sample distribution of test statistics for random effects in a two-factor factorial design model with $d$-dimensional normal data, thereby broadening the findings of Bilodeau [ArXiv (2022), 6 pp.], who treated the case $d = 1$. The same approach makes it possible to test random effects in more general factorial design models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13711v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Anne MacKay, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank approximations for linear Gaussian inverse problems on Hilbert spaces, Part II: posterior mean approximation</title>
      <link>https://arxiv.org/abs/2503.24209</link>
      <description>arXiv:2503.24209v2 Announce Type: replace 
Abstract: In this work, we construct optimal low-rank approximations for the Gaussian posterior distribution in linear Gaussian inverse problems with possibly infinite-dimensional separable Hilbert parameter spaces and finite-dimensional data spaces. We consider different approximation families for the posterior. We first consider approximate posteriors in which the means vary among a class of either structure-preserving or structure-ignoring low-rank transformations of the data, and in which the posterior covariance is kept fixed. We give necessary and sufficient conditions for these approximating posteriors to be equivalent to the exact posterior, for all possible realisations of the data simultaneously. For such approximations, we measure approximation error with the Kullback--Leibler, R\'enyi and Amari $\alpha$-divergences for $\alpha\in(0,1)$, and with the Hellinger distance, all averaged over the data distribution. With these losses, we find the optimal approximations and formulate an equivalent condition for their uniqueness, extending the work in finite dimensions of Spantini et al. (SIAM J. Sci. Comput. 2015). We then consider joint approximation of the mean and covariance, by also varying the posterior covariance over the low-rank updates considered in Part I of this work. For the reverse Kullback--Leibler divergence, we show that the separate optimal approximations of the mean and of the covariance can be combined to yield an optimal joint approximation of the mean and covariance. In addition, we interpret the joint approximation with the optimal structure-ignoring approximate mean in terms of an optimal projector in parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24209v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Eigenstructure inference for high-dimensional covariance with generalized shrinkage inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2505.20668</link>
      <description>arXiv:2505.20668v2 Announce Type: replace 
Abstract: In multivariate statistics, estimating the covariance matrix is essential for understanding the interdependence among variables. In high-dimensional settings, where the number of covariates increases with the sample size, it is well known that the eigenstructure of the sample covariance matrix is inconsistent. The inverse-Wishart prior, a standard choice for covariance estimation in Bayesian inference, also suffers from posterior inconsistency. To address the issue of eigenvalue dispersion in high-dimensional settings, the shrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its conceptual appeal and empirical success, the asymptotic justification for the SIW prior has remained limited. In this paper, we propose a generalized shrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance modeling. By extending the SIW framework, the gSIW prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices. In particular, under the spiked covariance assumption, we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices. This direct evaluation provides insights into the large-sample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems. Finally, simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure, particularly for spiked eigenvalues, achieving narrower credible intervals and higher coverage probabilities compared to existing methods. For spiked eigenvectors, the performance is generally comparable to that of competing approaches, including the sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20668v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Kwangmin Lee, Sewon Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Filtering of partially observed polynomial processes in discrete and continuous time</title>
      <link>https://arxiv.org/abs/2503.05588</link>
      <description>arXiv:2503.05588v2 Announce Type: replace-cross 
Abstract: This paper is devoted to filtering, smoothing, and prediction of polynomial processes that are partially observed. These problems are known to allow for an explicit solution in the simpler case of linear Gaussian state space models. The key insight underlying the present piece of research is that in filtering applications polynomial processes and their discrete counterpart are indistinguishable from Gaussian processes sharing their first two moments. We describe the construction of these Gaussian equivalents of polynomial processes and explicitly compute optimal linear filters, predictors and smoothers for polynomial processes in discrete and continuous time. The consideration of Gaussian equivalents also opens the door to parameter estimation and linear-quadratic optimal control in the context of polynomial processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05588v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Kallsen, Ivo Richert</dc:creator>
    </item>
    <item>
      <title>Mixing and Merging Metric Spaces using Directed Graphs</title>
      <link>https://arxiv.org/abs/2505.06405</link>
      <description>arXiv:2505.06405v2 Announce Type: replace-cross 
Abstract: Let $(X_1,d_1),\dots, (X_N,d_N)$ be metric spaces, where $d_i: X_i \times X_i \rightarrow [0,1]$ is a distance function for $i=1,\dots,N$. Let $\mathcal{X}$ denote the set theoretic product $X_1\times \cdots \times X_N$. Let $\mathcal{G} = \left(\mathcal{V},\mathcal{E}\right)$ be a directed graph with vertex set $\mathcal{V} =\{1,\dots, N\}$, and let $\mathcal{P} = \{p_{ij}\}$ be a collection of weights, where each $p_{ij}\in (0, 1]$ is associated with the edge $(i,j) \in \mathcal{E}$. We introduce the function $d_{\mathcal{X},\mathcal{G},\mathcal{P}}: \mathcal{X}\times \mathcal{X} \to [0,1]$ defined by \begin{align*} d_{\mathcal{X},\mathcal{G},\mathcal{P}}(\mathbf{g},\mathbf{h}) := \left(1 - \frac{1}{N}\sum_{j=1}^N \prod_{i=1}^N \left[1- d_i(g_i,h_i)\right]^{\frac{1}{p_{ji}}} \right), \end{align*} for all $\mathbf{g},\mathbf{h} \in \mathcal{X}$. In this paper we show that $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ defines a metric space over $\mathcal{X}$. Then we determine how this distance behaves under various graph operations, including disjoint unions and Cartesian products. We investigate two limiting cases: (a) when $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ is defined over a finite field, leading to a broad generalization of graph-based distances commonly studied in error-correcting code theory; and (b) when the metric is extended to graphons, enabling the measurement of distances in a continuous graph limit setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06405v2</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahir Bilen Can, Shantanu Chakrabartty</dc:creator>
    </item>
    <item>
      <title>Wild refitting for black box prediction</title>
      <link>https://arxiv.org/abs/2506.21460</link>
      <description>arXiv:2506.21460v2 Announce Type: replace-cross 
Abstract: We describe and analyze a computionally efficient refitting procedure for computing high-probability upper bounds on the instance-wise mean-squared prediction error of penalized nonparametric estimates based on least-squares minimization. Requiring only a single dataset and black box access to the prediction method, it consists of three steps: computing suitable residuals, symmetrizing and scaling them with a pre-factor $\rho$, and using them to define and solve a modified prediction problem recentered at the current estimate. We refer to it as wild refitting, since it uses Rademacher residual symmetrization as in a wild bootstrap variant. Under relatively mild conditions allowing for noise heterogeneity, we establish a high probability guarantee on its performance, showing that the wild refit with a suitably chosen wild noise scale $\rho$ gives an upper bound on prediction error. This theoretical analysis provides guidance into the design of such procedures, including how the residuals should be formed, the amount of noise rescaling in the wild sub-problem needed for upper bounds, and the local stability properties of the block-box procedure. We illustrate the applicability of this procedure to various problems, including non-rigid structure-from-motion recovery with structured matrix penalties; plug-and-play image restoration with deep neural network priors; and randomized sketching with kernel methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21460v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin J. Wainwright</dc:creator>
    </item>
  </channel>
</rss>
