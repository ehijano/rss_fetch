<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Mar 2025 02:16:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>fastfrechet: An R package for fast implementation of Fr\'echet regression with distributional responses</title>
      <link>https://arxiv.org/abs/2503.06401</link>
      <description>arXiv:2503.06401v1 Announce Type: new 
Abstract: Distribution-as-response regression problems are gaining wider attention, especially within biomedical settings where observation-rich patient specific data sets are available, such as feature densities in CT scans (Petersen et al., 2021) actigraphy (Ghosal et al., 2023), and continuous glucose monitoring (Coulter et al., 2024; Matabuena et al., 2021). To accommodate the complex structure of such problems, Petersen and M\"uller (2019) proposed a regression framework called Fr\'echet regression which allows non-Euclidean responses, including distributional responses. This regression framework was further extended for variable selection by Tucker et al. (2023), and Coulter et al. (2024) (arXiv:2403.00922 [stat.AP]) developed a fast variable selection algorithm for the specific setting of univariate distributional responses equipped with the 2-Wasserstein metric (2-Wasserstein space). We present "fastfrechet", an R package providing fast implementation of these Fr\'echet regression and variable selection methods in 2-Wasserstein space, with resampling tools for automatic variable selection. "fastfrechet" makes distribution-based Fr\'echet regression with resampling-supplemented variable selection readily available and highly scalable to large data sets, such as the UK Biobank (Doherty et al., 2017).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06401v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Coulter, Rebecca Lee, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Model-based edge clustering for weighted networks with a noise component</title>
      <link>https://arxiv.org/abs/2503.06822</link>
      <description>arXiv:2503.06822v1 Announce Type: new 
Abstract: Clustering is a fundamental task in network analysis, essential for uncovering hidden structures within complex systems. Edge clustering, which focuses on relationships between nodes rather than the nodes themselves, has gained increased attention in recent years. However, existing edge clustering algorithms often overlook the significance of edge weights, which can represent the strength or capacity of connections, and fail to account for noisy edges--connections that obscure the true structure of the network. To address these challenges, the Weighted Edge Clustering Adjusting for Noise (WECAN) model is introduced. This novel algorithm integrates edge weights into the clustering process and includes a noise component that filters out spurious edges. WECAN offers a data-driven approach to distinguishing between meaningful and noisy edges, avoiding the arbitrary thresholding commonly used in network analysis. Its effectiveness is demonstrated through simulation studies and applications to real-world datasets, showing significant improvements over traditional clustering methods. Additionally, the R package ``WECAN'' has been developed to facilitate its practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06822v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haomin Li, Daniel K. Sewell</dc:creator>
    </item>
    <item>
      <title>Bypassing orthogonalization in the quantum DPP sampler</title>
      <link>https://arxiv.org/abs/2503.05906</link>
      <description>arXiv:2503.05906v1 Announce Type: cross 
Abstract: Given an $n\times r$ matrix $X$ of rank $r$, consider the problem of sampling $r$ integers $\mathtt{C}\subset \{1, \dots, n\}$ with probability proportional to the squared determinant of the rows of $X$ indexed by $\mathtt{C}$. The distribution of $\mathtt{C}$ is called a projection determinantal point process (DPP). The vanilla classical algorithm to sample a DPP works in two steps, an orthogonalization in $\mathcal{O}(nr^2)$ and a sampling step of the same cost. The bottleneck of recent quantum approaches to DPP sampling remains that preliminary orthogonalization step. For instance, (Kerenidis and Prakash, 2022) proposed an algorithm with the same $\mathcal{O}(nr^2)$ orthogonalization, followed by a $\mathcal{O}(nr)$ classical step to find the gates in a quantum circuit. The classical $\mathcal{O}(nr^2)$ orthogonalization thus still dominates the cost. Our first contribution is to reduce preprocessing to normalizing the columns of $X$, obtaining $\mathsf{X}$ in $\mathcal{O}(nr)$ classical operations. We show that a simple circuit inspired by the formalism of Kerenidis et al., 2022 samples a DPP of a type we had never encountered in applications, which is different from our target DPP. Plugging this circuit into a rejection sampling routine, we recover our target DPP after an expected $1/\det \mathsf{X}^\top\mathsf{X} = 1/a$ preparations of the quantum circuit. Using amplitude amplification, our second contribution is to boost the acceptance probability from $a$ to $1-a$ at the price of a circuit depth of $\mathcal{O}(r\log n/\sqrt{a})$ and $\mathcal{O}(\log n)$ extra qubits. Prepending a fast, sketching-based classical approximation of $a$, we obtain a pipeline to sample a projection DPP on a quantum computer, where the former $\mathcal{O}(nr^2)$ preprocessing bottleneck has been replaced by the $\mathcal{O}(nr)$ cost of normalizing the columns and the cost of our approximation of $a$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05906v1</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha\"el Fanuel, R\'emi Bardenet</dc:creator>
    </item>
    <item>
      <title>Black Box Causal Inference: Effect Estimation via Meta Prediction</title>
      <link>https://arxiv.org/abs/2503.05985</link>
      <description>arXiv:2503.05985v1 Announce Type: cross 
Abstract: Causal inference and the estimation of causal effects plays a central role in decision-making across many areas, including healthcare and economics. Estimating causal effects typically requires an estimator that is tailored to each problem of interest. But developing estimators can take significant effort for even a single causal inference setting. For example, algorithms for regression-based estimators, propensity score methods, and doubly robust methods were designed across several decades to handle causal estimation with observed confounders. Similarly, several estimators have been developed to exploit instrumental variables (IVs), including two-stage least-squares (TSLS), control functions, and the method-of-moments. In this work, we instead frame causal inference as a dataset-level prediction problem, offloading algorithm design to the learning process. The approach we introduce, called black box causal inference (BBCI), builds estimators in a black-box manner by learning to predict causal effects from sampled dataset-effect pairs. We demonstrate accurate estimation of average treatment effects (ATEs) and conditional average treatment effects (CATEs) with BBCI across several causal inference problems with known identification, including problems with less developed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05985v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucius E. J. Bynum, Aahlad Manas Puli, Diego Herrero-Quevedo, Nhi Nguyen, Carlos Fernandez-Granda, Kyunghyun Cho, Rajesh Ranganath</dc:creator>
    </item>
    <item>
      <title>Analysis of Patterns in Recorded Signals of Software Systems With a Variance Based Segmentation Algorithm</title>
      <link>https://arxiv.org/abs/2503.06290</link>
      <description>arXiv:2503.06290v1 Announce Type: cross 
Abstract: Due to the increasing complexity and interconnectedness of different components in modern automotive software systems there is a great number of interactions between these system components and their environment. These interactions result in unique temporal behaviors we call underlying scenarios. The signal data from all system components, which is recorded during runtime, can be processed and analyzed by observing changes in their runtime. Different system behaviors can be characterized by dividing the whole data spectrum into appropriate segments with consistent behavior, classifying these segments, and mapping them to different scenarios. These disjunctive scenarios can be analyzed for their specific behavior which may divert from the expected average system behavior. We state the emerging problem of data segmentation as follows: divide a multivariate data set into a suitable amount of segments with consistent internal behavior. The problem can be divided into 2 subproblems: "How many segments are present in the data set?", and "What are good segmentation indices for the underlying segments?". The complexity of the problem still needs to be assessed, however, at this point we expect it to be NP-hard, as both the number of segments and the segmentation points are unknown. We are in search of appropriate metrics to quantify the quality of a given segmentation of a whole data set. In this paper, we discuss the segmentation of multivariate data, but not the classification of segments into scenario classes. In the following, we investigate segmentation algorithms for solving the subproblem of finding suitable segmentation indices by constant amount of segments. The algorithms are investigated towards effectivity and efficiency by applying them to a data set taken out of a real system trace provided by our automotive partners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06290v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bojan Luki\'c, Thorben Knust, Andreas Rausch</dc:creator>
    </item>
    <item>
      <title>Landscape computations for the edge of chaos in nonlinear dynamical systems</title>
      <link>https://arxiv.org/abs/2503.06393</link>
      <description>arXiv:2503.06393v1 Announce Type: cross 
Abstract: We propose a stochastic sampling approach to identify stability boundaries in general dynamical systems. The global landscape of Lyapunov exponent in multi-dimensional parameter space provides transition boundaries for stable/unstable trajectories, i.e., the edge of chaos. Despite its usefulness, it is generally difficult to derive analytically. In this study, we reveal the transition boundaries by leveraging the Markov chain Monte Carlo algorithm coupled directly with the numerical integration of nonlinear differential/difference equation. It is demonstrated that a posteriori modeling for parameter subspace along the edge of chaos determines an inherent constrained dynamical system to flexibly activate or de-activate the chaotic tra jectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06393v1</guid>
      <category>nlin.CD</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Motoki Nakata, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>matrixdist: An R Package for Statistical Analysis of Matrix Distributions</title>
      <link>https://arxiv.org/abs/2101.07987</link>
      <description>arXiv:2101.07987v4 Announce Type: replace 
Abstract: The matrixdist R package provides a comprehensive suite of tools for the statistical analysis of matrix distributions, including phase-type, inhomogeneous phase-type, discrete phase-type, and related multivariate distributions. This paper introduces the package and its key features, including the estimation of these distributions and their extensions through expectation-maximisation algorithms, as well as the implementation of regression through the proportional intensities and mixture-of-experts models. Additionally, the paper provides an overview of the theoretical background, discusses the algorithms and methods implemented in the package, and offers practical examples to illustrate the application of matrixdist in real-world actuarial problems. The matrixdist R package aims to provide researchers and practitioners a wide set of tools for analysing and modelling complex data using matrix distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.07987v4</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Alaric Mueller, Jorge Yslas</dc:creator>
    </item>
    <item>
      <title>lpcde: Estimation and Inference for Local Polynomial Conditional Density Estimators</title>
      <link>https://arxiv.org/abs/2204.10375</link>
      <description>arXiv:2204.10375v3 Announce Type: replace 
Abstract: This paper discusses the R package lpcde, which stands for local polynomial conditional density estimation. It implements the kernel-based local polynomial smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2024) for statistical estimation and inference of conditional distributions, densities, and derivatives thereof. The package offers mean square error optimal bandwidth selection and associated point estimators, as well as uncertainty quantification based on robust bias correction both pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands) over evaluation points. The methods implemented are boundary adaptive whenever the data is compactly supported. The package also implements regularized conditional density estimation methods, ensuring the resulting density estimate is non-negative and integrates to one. We contrast the functionalities of lpcde with existing open-source packages for conditional density estimation, and showcase its main features using simulated and real datasets. An abbreviated version of this article is published in Cattaneo, Chandak, Jansson, Ma (2025 JOSS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10375v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rajita Chandak, Michael Jansson, Xinwei Ma</dc:creator>
    </item>
    <item>
      <title>Squintability and Other Metrics for Assessing Projection Pursuit Indexes, and Guiding Optimization Choices</title>
      <link>https://arxiv.org/abs/2407.13663</link>
      <description>arXiv:2407.13663v3 Announce Type: replace 
Abstract: The projection pursuit (PP) guided tour optimizes a criterion function, known as the PP index, to gradually reveal projections of interest from high-dimensional data through animation. Optimization of some PP indexes can be non-trivial, if they are non-smooth functions, or when the optimum has a small "squint angle", detectable only from close proximity. Here, measures for calculating the smoothness and squintability properties of the PP index are defined. These are used to investigate the performance of a recently introduced swarm-based algorithm, Jellyfish Search Optimizer (JSO), for optimizing PP indexes. The performance of JSO in detecting the target pattern (pipe shape) is compared with existing optimizers in PP. Additionally, JSO's performance on detecting the sine-wave shape is evaluated using different PP indexes (hence different smoothness and squintability) across various data dimensions (d = 4, 6, 8, 10, 12) and JSO hyper-parameters. We observe empirically that higher squintability improves the success rate of the PP index optimization, while smoothness has no significant effect. The JSO algorithm has been implemented in the R package, `tourr`, and functions to calculate smoothness and squintability measures are implemented in the `ferrn` package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13663v3</guid>
      <category>stat.CO</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Sherry Zhang, Dianne Cook, Nicolas Langren\'e, Jessica Wai Yin Leung</dc:creator>
    </item>
    <item>
      <title>Change-point regression with a smooth additive disturbance</title>
      <link>https://arxiv.org/abs/2112.03878</link>
      <description>arXiv:2112.03878v2 Announce Type: replace-cross 
Abstract: We assume a nonparametric regression model where the signal is given by the sum of a piecewise constant function and a smooth function. To detect the change-points and estimate the regression functions, we propose PCpluS, a combination of the fused Lasso and kernel smoothing. In contrast to existing approaches, it explicitly uses the additive decomposition of the signal when detecting change-points. This is motivated by several applications and by theoretical results about partial linear model. We show how the use of the Epanechnikov kernel in the linear smoother results in very fast computation. Simulations demonstrate that our approach has a small mean squared error and detects change-points well. We also apply the methodology to genome sequencing data to detect copy number variations. Finally, we demonstrate its flexibility by proposing extensions to multivariate and filtered data. An R-package called PCpluS is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03878v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Pein, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Adaptive truncation of infinite sums: applications to Statistics</title>
      <link>https://arxiv.org/abs/2202.06121</link>
      <description>arXiv:2202.06121v2 Announce Type: replace-cross 
Abstract: It is often the case in Statistics that one needs to compute sums of infinite series, especially in marginalising over discrete latent variables. This has become more relevant with the popularization of gradient-based techniques (e.g. Hamiltonian Monte Carlo) in the Bayesian inference context, for which discrete latent variables are hard or impossible to deal with. For many commonly used infinite series, custom algorithms have been developed which exploit specific features of each problem. General techniques, suitable for a large class of problems with limited input from the user are less established. We employ basic results from the theory of infinite series to investigate general, problem-agnostic algorithms to truncate infinite sums within an arbitrary tolerance $\varepsilon &gt; 0$ and provide robust computational implementations with provable guarantees. We compare three tentative solutions to estimating the infinite sum of interest: (i) a "naive" approach that sums terms until the terms are below the threshold $\varepsilon$; (ii) a `bounding pair' strategy based on trapping the true value between two partial sums; and (iii) a `batch' strategy that computes the partial sums in regular intervals and stops when their difference is less than $\varepsilon$. We show under which conditions each strategy guarantees the truncated sum is within the required tolerance and compare the error achieved by each approach, as well as the number of function evaluations necessary for each one. A detailed discussion of numerical issues in practical implementations is also provided. The paper provides some theoretical discussion of a variety of statistical applications, including raw and factorial moments and count models with observation error. Finally, detailed illustrations in the form noisy MCMC for Bayesian inference and maximum marginal likelihood estimation are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06121v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luiz Max Carvalho, Wellington J. Silva, Guido A. Moreira</dc:creator>
    </item>
    <item>
      <title>lsirm12pl: An R package for latent space item response modeling</title>
      <link>https://arxiv.org/abs/2205.06989</link>
      <description>arXiv:2205.06989v3 Announce Type: replace-cross 
Abstract: The item response model in latent space (LSIRM; Jeon et al., 2021) uncovers unobserved interactions between respondents and items in the item response data by embedding both in a shared latent metric space. The R package lsirm12pl implements Bayesian estimation of the LSIRM and its extensions for various response types, base model specifications, and missing data handling. Furthermore, lsirm12pl package provides methods to improve model utilization and interpretation, such as clustering item positions on an estimated interaction map. The package also offers convenient summary and plotting options to evaluate and process the estimated results. In this paper, we provide an overview of the LSIRM's methodological foundation and describe several extensions included in the package. We then demonstrate the use of the package with real data examples contained within it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.06989v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyoung Go, Gwanghee Kim, Jina Park, Junyong Park, Minjeong Jeon, Ick Hoon Jin</dc:creator>
    </item>
    <item>
      <title>Channelling Multimodality Through a Unimodalizing Transport: Warp-U Sampler and Stochastic Bridge Sampling Estimator</title>
      <link>https://arxiv.org/abs/2401.00667</link>
      <description>arXiv:2401.00667v2 Announce Type: replace-cross 
Abstract: Monte Carlo integration is a powerful tool for scientific and statistical computation, but faces significant challenges when the integrand is a multi-modal distribution, even when the mode locations are known. This work introduces novel Monte Carlo sampling and integration estimation strategies for the multi-modal context by leveraging a generalized version of the stochastic Warp-U transformation Wang et al. [2022]. We propose two flexible classes of Warp-U transformations, one based on a general location-scale-skew mixture model and a second using neural ordinary differential equations. We develop an efficient sampling strategy called Warp-U sampling, which applies a Warp-U transformation to map a multi-modal density into a uni-modal one, then inverts the transformation with injected stochasticity. In high dimensions, our approach relies on information about the mode locations, but requires minimal tuning and demonstrates better mixing properties than conventional methods with identical mode information. To improve normalizing constant estimation once samples are obtained, we propose a stochastic Warp-U bridge sampling estimator, which we demonstrate has higher asymptotic precision per CPU second compared to the original approach proposed by Wang et al. [2022]. We also establish the ergodicity of our sampling algorithm. The effectiveness and current limitations of our methods are illustrated through simulation studies and an application to exoplanet detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00667v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Ding, Shiyuan He, David E. Jones, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>Moments by the Integrating the Moment-Generating Function</title>
      <link>https://arxiv.org/abs/2410.23587</link>
      <description>arXiv:2410.23587v2 Announce Type: replace-cross 
Abstract: We introduce a novel method for obtaining a wide variety of moments of a random variable with a well-defined moment-generating function (MGF). We derive new expressions for fractional moments and fractional absolute moments, both central and non-central moments. The new moment expressions are relatively simple integrals that involve the MGF, but do not require its derivatives. We label the new method CMGF because it uses a complex extension of the MGF and can be used to obtain complex moments. We illustrate the new method with three applications where the MGF is available in closed-form, while the corresponding densities and the derivatives of the MGF are either unavailable or very difficult to obtain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23587v2</guid>
      <category>econ.EM</category>
      <category>q-fin.CP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Reinhard Hansen, Chen Tong</dc:creator>
    </item>
    <item>
      <title>Posterior SBC: Simulation-Based Calibration Checking Conditional on Data</title>
      <link>https://arxiv.org/abs/2502.03279</link>
      <description>arXiv:2502.03279v2 Announce Type: replace-cross 
Abstract: Simulation-based calibration checking (SBC) refers to the validation of an inference algorithm and model implementation through repeated inference on data simulated from a generative model. In the original and commonly used approach, the generative model uses parameters drawn from the prior, and thus the approach is testing whether the inference works for simulated data generated with parameter values plausible under that prior. This approach is natural and desirable when we want to test whether the inference works for a wide range of datasets we might observe. However, after observing data, we are interested in answering whether the inference works conditional on that particular data. In this paper, we propose posterior SBC and demonstrate how it can be used to validate the inference conditionally on observed data. We illustrate the utility of posterior SBC in three case studies: (1) A simple multilevel model; (2) a model that is governed by differential equations; and (3) a joint integrative neuroscience model which is approximated via amortized Bayesian inference with neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03279v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teemu S\"ailynoja, Marvin Schmitt, Paul-Christian B\"urkner, Aki Vehtari</dc:creator>
    </item>
  </channel>
</rss>
