<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Mar 2025 04:01:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Real-time Bus Travel Time Prediction and Reliability Quantification: A Hybrid Markov Model</title>
      <link>https://arxiv.org/abs/2503.05907</link>
      <description>arXiv:2503.05907v1 Announce Type: new 
Abstract: Accurate and reliable bus travel time prediction in real-time is essential for improving the operational efficiency of public transportation systems. However, this remains a challenging task due to the limitations of existing models and data sources. This study proposed a hybrid Markovian framework for real-time bus travel time prediction, incorporating uncertainty quantification. Firstly, the bus link travel time distributions were modeled by integrating various influential factors while explicitly accounting for heteroscedasticity. Particularly, the parameters of the distributions were estimated using Maximum Likelihood Estimation, and the Fisher Information Matrix was then employed to calculate the 95\% uncertainty bounds for the estimated parameters, ensuring a robust and reliable quantification of prediction uncertainty of bus link travel times. Secondly, a Markovian framework with transition probabilities based on previously predicted bus link travel times was developed to predict travel times and their uncertainties from a current location to any future stop along the route. The framework was evaluated using the General Transit Feed Specification (GTFS) Static and Realtime data collected in 2023 from Gainesville, Florida. The results showed that the proposed model consistently achieved better prediction performance compared to the selected baseline approaches (including historical mean, statistical and AI-based models) while providing narrower uncertainty bounds. The model also demonstrated high interpretability, as the estimated coefficients provided insights into how different factors influencing bus travel times across links with varying characteristics. These findings suggest that the model could serve as a valuable tool for transit system performance evaluation and real-time trip planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05907v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuran Sun, James Spall, Wai Wong, Xilei Zhao</dc:creator>
    </item>
    <item>
      <title>Interactive Visualization Framework for Forensic Bullet Comparisons</title>
      <link>https://arxiv.org/abs/2503.05910</link>
      <description>arXiv:2503.05910v1 Announce Type: new 
Abstract: The current method for forensic analysis of bullet comparison relies on manual examination by forensic examiners to determine if bullets were discharged from the same firearm. This process is highly subjective, prompting the development of algorithmic methods to provide objective statistical support for comparisons. However, a gap exists between the technical understanding of these algorithms and the typical background of many forensic examiners. We present a visualization tool designed to bridge this gap, allowing for the presentation of statistical information in a more familiar format to forensic professionals. The forensic bullet comparison visualizer (FBCV) features a variety of plots that will enable the user to examine every step of the algorithmic comparison process. We demonstrate the utility of the FBCV by applying it to data from the Houston Science Lab, where it helped identify an error in the comparison process caused by mislabeling. This tool can be used for future investigations, such as examining how distance between shots affects scores. The FBCV offers a user-friendly way to convey complex statistical information to forensic examiners, facilitating their understanding and utilization of algorithmic comparison methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05910v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Rethwisch, Heike Hofmann</dc:creator>
    </item>
    <item>
      <title>Evaluating Multilevel Regression and Poststratification with Spatial Priors with a Big Data Behavioural Survey</title>
      <link>https://arxiv.org/abs/2503.05915</link>
      <description>arXiv:2503.05915v1 Announce Type: new 
Abstract: Multilevel regression and poststratification (MRP) is a computationally efficient indirect estimation method that can quickly produce improved population-adjusted estimates with limited data. Recent computational advancements allow efficient, relatively simple, and quick approximate Bayesian estimation for MRP. As population health outcomes of interest including vaccination uptake are known to have spatial structure, precision may be gained by including space in the model. We test a recently proposed spatial MRP method that includes a BYM2 spatial term that smooths across demographics and geographic areas using a large, unrepresentative survey. We produce California county-level estimates of first-dose COVID-19 vaccination up to June 2021 using classic and spatial MRP models, and poststratify using data from the American Community Survey (US Census Bureau). We assess validity using reported first-dose vaccination counts from the Centers for Disease Control (CDC). Neither classic nor spatial MRP models performed well, highlighting: 1. spatial MRP may be most appropriate for richer data contexts, 2. some demographics in the survey data are over-sampled and -aggregated, producing model over-smoothing, and 3. a need for survey producers to share user-representative metrics to better benchmark estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05915v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aja Sutton, Zack W. Almquist, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Analysis of Patterns in Recorded Signals of Software Systems With a Variance Based Segmentation Algorithm</title>
      <link>https://arxiv.org/abs/2503.06290</link>
      <description>arXiv:2503.06290v1 Announce Type: new 
Abstract: Due to the increasing complexity and interconnectedness of different components in modern automotive software systems there is a great number of interactions between these system components and their environment. These interactions result in unique temporal behaviors we call underlying scenarios. The signal data from all system components, which is recorded during runtime, can be processed and analyzed by observing changes in their runtime. Different system behaviors can be characterized by dividing the whole data spectrum into appropriate segments with consistent behavior, classifying these segments, and mapping them to different scenarios. These disjunctive scenarios can be analyzed for their specific behavior which may divert from the expected average system behavior. We state the emerging problem of data segmentation as follows: divide a multivariate data set into a suitable amount of segments with consistent internal behavior. The problem can be divided into 2 subproblems: "How many segments are present in the data set?", and "What are good segmentation indices for the underlying segments?". The complexity of the problem still needs to be assessed, however, at this point we expect it to be NP-hard, as both the number of segments and the segmentation points are unknown. We are in search of appropriate metrics to quantify the quality of a given segmentation of a whole data set. In this paper, we discuss the segmentation of multivariate data, but not the classification of segments into scenario classes. In the following, we investigate segmentation algorithms for solving the subproblem of finding suitable segmentation indices by constant amount of segments. The algorithms are investigated towards effectivity and efficiency by applying them to a data set taken out of a real system trace provided by our automotive partners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06290v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bojan Luki\'c, Thorben Knust, Andreas Rausch</dc:creator>
    </item>
    <item>
      <title>Adaptive multi-wave sampling for efficient chart validation</title>
      <link>https://arxiv.org/abs/2503.06308</link>
      <description>arXiv:2503.06308v1 Announce Type: new 
Abstract: Computable phenotypes are used to characterize patients and identify outcomes in studies conducted using healthcare claims and electronic health record data. Chart review studies establish reference labels against which computable phenotypes are compared to understand their measurement characteristics, the quantity of interest, for instance the positive predictive value. We describe a method to adaptively evaluate a quantity of interest over sequential samples of charts, with the goal to minimize the number of charts reviewed. With the help of a simultaneous confidence band, we stop the reviewing once the confidence band meets a pre-specified stopping threshold. The contribution of this article is threefold. First, we tested the use of an adaptive approach called Neyman's sampling of charts versus random or stratified random sampling. Second, we propose frequentist confidence bands and Bayesian credible intervals to sequentially evaluate the quantity of interest. Third, we propose a tool to predict the stopping time (defined as the number of charts reviewed) at which the chart review would be complete. We observe that Bayesian credible intervals proved to be tighter than its frequentist confidence band counterparts. Moreover, we observe that simple random sampling is often performing similarly to Neyman's sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06308v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georg Hahn, Sebastian Schneeweiss, Shirley Wang</dc:creator>
    </item>
    <item>
      <title>Heterogeneous network estimation for single-cell transcriptomic data via a joint regularized deep neural network</title>
      <link>https://arxiv.org/abs/2503.06389</link>
      <description>arXiv:2503.06389v1 Announce Type: new 
Abstract: Network estimation has been a critical component of single-cell transcriptomic data analysis, which can provide crucial insights into the complex interplay among genes, facilitating uncovering the biological basis of human life at single-cell resolution. Despite notable achievements, existing methodologies often falter in their practicality, primarily due to their narrow focus on simplistic linear relationships and inadequate handling of cellular heterogeneity. To bridge these gaps, we propose a joint regularized deep neural network method incorporating a Mahalanobis distance-based K-means clustering (JRDNN-KM) to estimate multiple networks for various cell subgroups simultaneously, accounting for both unknown cellular heterogeneity and zero-inflation and, more importantly, complex nonlinear relationships among genes. We innovatively introduce a selection layer for network construction and develop homogeneous and heterogeneous hidden layers to accommodate commonality and specificity across multiple networks. Through simulations and applications to real single-cell transcriptomic data for multiple tissues and species, we show that JRDNN-KM constructs networks with more accuracy and biological interpretability and, meanwhile, identifies more accurate cell subgroups compared to the state-of-the-art methods in the literature. Building on the network construction, we further find hub genes with important biological implications and modules with statistical enrichment of biological processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06389v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Jingyuan, Li Tao, Wang Tianyi, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Exponential-polynomial divergence based inference for nondestructive one-shot devices under progressive stress model</title>
      <link>https://arxiv.org/abs/2503.06414</link>
      <description>arXiv:2503.06414v1 Announce Type: new 
Abstract: Nondestructive one-shot device (NOSD) testing plays a crucial role in engineering, particularly in the reliability assessment of high-stakes systems such as aerospace components, medical devices, and semiconductor technologies. Accurate reliability prognosis of NOSD testing data is essential for ensuring product durability, safety, and performance optimization. The conventional estimation methods like maximum likelihood estimation (MLE) are sensitive to data contamination, leading to biased results. Consequently, this study develops robust inferential analysis for NOSD testing data under a progressive stress model. The lifetime of NOSD is assumed to follow Log-logistic distribution. The estimation procedure addresses robustness by incorporating Exponential-polynomial divergence (EPD). Equipped with three tuning parameters, EPD based estimation is proven to be more flexible than density power divergence estimation frequently used for one-shot device testing data analysis. Further, we explore the asymptotic behaviour of minimum EPD estimator (MEPDE) for large sample size. The robustness of MEPDE is analytically studied through influence function. Since tradeoff between efficiency and robustness of EPD based estimation is governed by three tuning parameters, a novel approach leveraging Concrete Score Matching (CSM) is introduced to optimize the tuning parameters of MEPDE. Moreover, a comparative study with the existing methods of finding tuning parameters is conducted through extensive simulation experiment and data analysis. Another aspect of this study is determining an optimal plan to ensure a successful ALT experiment within specified budget and time constraints. It is designed on A-optimality criteria subject to the given constraints and is executed using the constraint particle swarm optimization (CPSO) algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06414v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Multivariate spatial models for small area estimation of species-specific forest inventory parameters</title>
      <link>https://arxiv.org/abs/2503.07118</link>
      <description>arXiv:2503.07118v1 Announce Type: new 
Abstract: National Forest Inventories (NFIs) provide statistically reliable information on forest resources at national and other large spatial scales. As forest management and conservation needs become increasingly complex, NFIs are being called upon to provide forest parameter estimates at spatial scales smaller than current design-based estimation procedures can provide. This is particularly true when estimates are desired by species or species groups. Here we propose a multivariate spatial model for small area estimation of species-specific forest inventory parameters. The hierarchical Bayesian modeling framework accounts for key complexities in species-specific forest inventory data, such as zero-inflation, correlations among species, and residual spatial autocorrelation. Importantly, by fitting the model directly to the individual plot-level data, the framework enables estimates of species-level forest parameters, with associated uncertainty, across any user-defined small area of interest. A simulation study revealed minimal bias and higher accuracy of the proposed model-based approach compared to the design-based estimator and a non-parametric k-nearest neighbor (kNN) estimator. We applied the model to estimate species-specific county-level aboveground biomass for the 20 most abundant tree species in the southern United States using Forest Inventory and Analysis (FIA) data. Biomass estimates from the proposed model had high correlations with design-based estimates and kNN estimates. Importantly, the proposed model provided large gains in precision across all 20 species. On average across species, 91.5% of county-level biomass estimates had higher precision compared to the design-based estimates. The proposed framework improves the ability of NFI data users to generate species-level forest parameter estimates with reasonable precision at management-relevant spatial scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07118v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey W. Doser, Malcolm S. Itter, Grant M. Domke, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Further results on relative, divergence measures based on extropy and their applications</title>
      <link>https://arxiv.org/abs/2503.07123</link>
      <description>arXiv:2503.07123v1 Announce Type: new 
Abstract: This study explores information measures based on extropy, introducing dynamic relative extropy measures for residual and past lifetimes, and investigating their various properties. Furthermore, the study analyzes the relationships between extropy-based divergence with dynamic relative extropy and other extropy measures. A nonparametric estimator for relative extropy is developed, and its performance is assessed through numerical simulation studies. The practical applicability of the relative extropy is demonstrated through some real-life data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07123v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saranya P., Sunoj S. M.</dc:creator>
    </item>
    <item>
      <title>Robust a posteriori estimation of probit-lognormal seismic fragility curves via sequential design of experiments and constrained reference prior</title>
      <link>https://arxiv.org/abs/2503.07343</link>
      <description>arXiv:2503.07343v1 Announce Type: new 
Abstract: Seismic fragility curves express the probability of failure of a mechanical equipment conditional to an intensity measure derived from a seismic signal. Although based on a strong assumption, the probit-lognormal model is very popular among practitioners for estimating such curves, judging by its abundant use in the literature. However, as this model is likely to lead to biased estimates, its use should be limited to cases for which only few data are available. In practice, this involves having to resort to binary data which indicate the state of the structure when it has been subjected to a seismic loading, namely failure or non-failure. The question then arises of the choice of data that must be used to obtain an optimal estimate, that is to say the most precise possible with the minimum of data. To answer this question, we propose a methodology for design of experiments in a Bayesian framework based on the reference prior theory. This theory aims to define a so-called objective prior that favors data learning, which is slighty constrained in this work in order tackle the problems of likelihood degeneracy that are ubiquitous with small data sets. The novelty of our work is then twofold. First, we rigorously present the problem of likelihood degeneracy which hampers frequentist approaches such as the maximum likelihood estimation. Then, we propose our strategy inherited from the reference prior theory to build the data set. This strategy aims to maximize the impact of the data on the posterior distribution of the fragility curve. Our method is applied to a case study of the nuclear industry. The results demonstrate its ability to efficiently and robustly estimate the fragility curve, and to avoid degeneracy even with a limited number of experiments. Additionally, we demonstrate that the estimates quickly reach the model bias induced by the probit-lognormal modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07343v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck, Cl\'ement Gauchy, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Improving Statistical Postprocessing for Extreme Wind Speeds using Tuned Weighted Scoring Rules</title>
      <link>https://arxiv.org/abs/2503.07374</link>
      <description>arXiv:2503.07374v1 Announce Type: new 
Abstract: Recent statistical postprocessing methods for wind speed forecasts have incorporated linear models and neural networks to produce more skillful probabilistic forecasts in the low-to-medium wind speed range. At the same time, these methods struggle in the high-to-extreme wind speed range. In this work, we aim to increase the performance in this range by training using a weighted version of the continuous ranked probability score (wCRPS). We develop an approach using shifted Gaussian cdf weight functions, whose parameters are tuned using a multi-objective hyperparameter tuning algorithm that balances performance on low and high wind speed ranges. We explore this approach for both linear models and convolutional neural network models combined with various parametric distributions, namely the truncated normal, log-normal, and generalized extreme value distributions, as well as adaptive mixtures. We apply these methods to forecasts from KNMI's deterministic Harmonie-Arome numerical weather prediction model to obtain probabilistic wind speed forecasts in the Netherlands for 48 hours ahead. For linear models we observe that even with a tuned weight function, training using the wCRPS produces a strong body-tail trade-off, where increased performance on extremes comes at the price of lower performance for the bulk of the distribution. For the best models using convolutional neural networks, we find that using a tuned weight function the performance on extremes can be increased without a significant deterioration in performance on the bulk. The best-performing weight function is shown to be model-specific. Finally, the choice of distribution has no significant impact on the performance of our models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07374v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Hakvoort, Bastien Francois, Kirien Whan, Sjoerd Dirksen</dc:creator>
    </item>
    <item>
      <title>The Impact of Building-Induced Visibility Restrictions on Intersection Accidents</title>
      <link>https://arxiv.org/abs/2503.05706</link>
      <description>arXiv:2503.05706v1 Announce Type: cross 
Abstract: Traffic accidents, especially at intersections, are a major road safety concern. Previous research has extensively studied intersection-related accidents, but the effect of building-induced visibility restrictions at intersections on accident rates has been under-explored, particularly in urban contexts. Using OpenStreetMap data, the UK's geographic and accident datasets, and the UK Traffic Count Dataset, we formulated a novel approach to estimate accident risk at intersections. This method factors in the area visible to drivers, accounting for views blocked by buildings - a distinctive aspect in traffic accident analysis. Our findings reveal a notable correlation between the road visible percentage and accident frequency. In the model, the coefficient for "road visible percentage" is 1.7450, implying a strong positive relationship. Incorporating this visibility factor enhances the model's explanatory power, with increased R-square values and reduced AIC and BIC, indicating a better data fit. This study underscores the essential role of architectural layouts in road safety and suggests that urban planning strategies should consider building-induced visibility restrictions. Such consideration could be an effective approach to mitigate accident rates at intersections. This research opens up new avenues for innovative, data-driven urban planning and traffic management strategies, highlighting the importance of visibility enhancements for safer roads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05706v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanlin Tian, Yuxiang Feng, Wei Zhou,  Anupriya, Mohammed Quddus, Yiannis Demiris, Panagiotis Angeloudis</dc:creator>
    </item>
    <item>
      <title>Establishment and Solution of a Multi-Stage Decision Model Based on Hypothesis Testing and Dynamic Programming Algorithm</title>
      <link>https://arxiv.org/abs/2503.05807</link>
      <description>arXiv:2503.05807v1 Announce Type: cross 
Abstract: This paper introduces a novel multi-stage decision-making model that integrates hypothesis testing and dynamic programming algorithms to address complex decision-making scenarios.Initially,we develop a sampling inspection scheme that controls for both Type I and Type II errors using a simple random sampling method without replacement,ensuring the randomness and representativeness of the sample while minimizing selection bias.Through the application of hypothesis testing theory,a hypothesis testing model concerning the defect rate is established,and formulas for the approximate distribution of the sample defect rate and the minimum sample size required under two different scenarios are derived. Subsequently,a multi-stage dynamic programming decision model is constructed.This involves defining the state transition functions and stage-specific objective functions,followed by obtaining six optimal decision strategies under various conditions through backward recursion.The results demonstrate the model's potent capability for multi-stage decision-making and its high interpretability,offering significant advantages in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05807v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICIRDC65564.2024.00163</arxiv:DOI>
      <arxiv:journal_reference>Proc. ICIRDC 2024, pp. 883-884, ISBN 979-8-3315-3405-9 (2024)</arxiv:journal_reference>
      <dc:creator>Ziyang Liu, Yurui Hu, Yihan Deng</dc:creator>
    </item>
    <item>
      <title>Model-based bi-clustering using multivariate Poisson-lognormal with general block-diagonal covariance matrix and its applications</title>
      <link>https://arxiv.org/abs/2503.05961</link>
      <description>arXiv:2503.05961v1 Announce Type: cross 
Abstract: While several Gaussian mixture models-based biclustering approaches currently exist in the literature for continuous data, approaches to handle discrete data have not been well researched. A multivariate Poisson-lognormal (MPLN) model-based bi-clustering approach that utilizes a block-diagonal covariance structure is introduced to allow for a more flexible structure of the covariance matrix. Two variations of the algorithm are developed where the number of column clusters: 1) are assumed equal across groups or 2) can vary across groups. Variational Gaussian approximation is utilized for parameter estimation, and information criteria are used for model selection. The proposed models are investigated in the context of clustering multivariate count data. Using simulated data the models display strong accuracy and computational efficiency and is applied to breast cancer RNA-sequence data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05961v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caitlin Kral, Evan Chance, Ryan Browne, Sanjeena Dang</dc:creator>
    </item>
    <item>
      <title>Shiny-MAGEC: A Bayesian R Shiny Application for Meta-analysis of Censored Adverse Events</title>
      <link>https://arxiv.org/abs/2503.05982</link>
      <description>arXiv:2503.05982v1 Announce Type: cross 
Abstract: Accurate assessment of adverse event (AE) incidence is critical in clinical cancer research for drug safety evaluation and regulatory approval. While meta-analysis serves as an essential tool to comprehensively synthesize the evidence across multiple studies, incomplete AE reporting in clinical trials remains a persistent challenge. In particular, AEs occurring below study-specific reporting thresholds are often omitted from publications, leading to left-censored data. Failure to account for these censored AE counts can result in biased AE incidence estimates. We present an R Shiny application that implements a one-stage Bayesian meta-analysis model specifically designed to incorporate censored AE data into the estimation process. This interactive tool provides a user-friendly interface for researchers to conduct AE meta-analyses and estimate the AE incidence probability following the bias-correction methods proposed by Qi et al. (2024). It also enables direct comparisons between models that either incorporate or ignore censoring, highlighting the biases introduced by conventional approaches. This tutorial demonstrates the Shiny application's functionality through an illustrative example on meta-analysis of PD-1/PD-L1 inhibitor safety and highlights the importance of this tool in improving AE risk assessment. Ultimately, the new Shiny app facilitates more accurate and transparent drug safety evaluations. The Shiny-MAGEC app is available at: https://zihanzhou98.shinyapps.io/Shiny-MAGEC/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05982v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Zhou, Zizhong Tian, Christine B. Peterson, Le Bao, Shouhao Zhou</dc:creator>
    </item>
    <item>
      <title>VACT: A Video Automatic Causal Testing System and a Benchmark</title>
      <link>https://arxiv.org/abs/2503.06163</link>
      <description>arXiv:2503.06163v1 Announce Type: cross 
Abstract: With the rapid advancement of text-conditioned Video Generation Models (VGMs), the quality of generated videos has significantly improved, bringing these models closer to functioning as ``*world simulators*'' and making real-world-level video generation more accessible and cost-effective. However, the generated videos often contain factual inaccuracies and lack understanding of fundamental physical laws. While some previous studies have highlighted this issue in limited domains through manual analysis, a comprehensive solution has not yet been established, primarily due to the absence of a generalized, automated approach for modeling and assessing the causal reasoning of these models across diverse scenarios. To address this gap, we propose VACT: an **automated** framework for modeling, evaluating, and measuring the causal understanding of VGMs in real-world scenarios. By combining causal analysis techniques with a carefully designed large language model assistant, our system can assess the causal behavior of models in various contexts without human annotation, which offers strong generalization and scalability. Additionally, we introduce multi-level causal evaluation metrics to provide a detailed analysis of the causal performance of VGMs. As a demonstration, we use our framework to benchmark several prevailing VGMs, offering insight into their causal reasoning capabilities. Our work lays the foundation for systematically addressing the causal understanding deficiencies in VGMs and contributes to advancing their reliability and real-world applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06163v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotong Yang, Qingyuan Zheng, Yunjian Gao, Yongkun Yang, Yangbo He, Zhouchen Lin, Muhan Zhang</dc:creator>
    </item>
    <item>
      <title>Gaussian mixture copulas for flexible dependence modelling in the body and tails of joint distributions</title>
      <link>https://arxiv.org/abs/2503.06255</link>
      <description>arXiv:2503.06255v1 Announce Type: cross 
Abstract: Fully describing the entire data set is essential in multivariate risk assessment, since moderate levels of one variable can influence another, potentially leading it to be extreme. Additionally, modelling both non-extreme and extreme events within a single framework avoids the need to select a threshold vector used to determine an extremal region, or the requirement to add flexibility to bridge between separate models for the body and tail regions. We propose a copula model, based on a mixture of Gaussian distributions, as this model avoids the need to define an extremal region, it is scalable to dimensions beyond the bivariate case, and it can handle both asymptotic dependent and asymptotic independent extremal dependence structures. We apply the proposed model through simulations and to a 5-dimensional seasonal air pollution data set, previously analysed in the multivariate extremes literature. Through pairwise, trivariate and 5-dimensional analyses, we show the flexibility of the Gaussian mixture copula in capturing different joint distributional behaviours and its ability to identify potential graphical structure features, both of which can vary across the body and tail regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06255v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'idia M. Andr\'e, Jonathan A. Tawn</dc:creator>
    </item>
    <item>
      <title>Pulse Processing -- Overview and Challenges</title>
      <link>https://arxiv.org/abs/2503.06408</link>
      <description>arXiv:2503.06408v1 Announce Type: cross 
Abstract: The detection of irregularly spaced pulses of non-negligible width is a fascinating yet under-explored topic in signal processing. It sits adjacent to other core topics such as radar and symbol detection yet has its own distinctive challenges. Even modern techniques such as compressed sensing perform worse than may be expected on pulse processing problems. Real-world applications include nuclear spectroscopy, flow cytometry, seismic signal processing and neural spike sorting, and these in turn have applications to environmental radiation monitoring, surveying, diagnostic medicine, industrial imaging, biomedical imaging, top-down proteomics, and security screening, to name just a few. This overview paper endeavours to position the pulse processing problem in the context of signal processing. It also describes some current challenges in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06408v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonathan H. Manton</dc:creator>
    </item>
    <item>
      <title>Bayesian Synthetic Control with a Soft Simplex Constraint</title>
      <link>https://arxiv.org/abs/2503.06454</link>
      <description>arXiv:2503.06454v1 Announce Type: cross 
Abstract: Whether the synthetic control method should be implemented with the simplex constraint and how to implement it in a high-dimensional setting have been widely discussed. To address both issues simultaneously, we propose a novel Bayesian synthetic control method that integrates a soft simplex constraint with spike-and-slab variable selection. Our model is featured by a hierarchical prior capturing how well the data aligns with the simplex assumption, which enables our method to efficiently adapt to the structure and information contained in the data by utilizing the constraint in a more flexible and data-driven manner. A unique computational challenge posed by our model is that conventional Markov chain Monte Carlo sampling algorithms for Bayesian variable selection are no longer applicable, since the soft simplex constraint results in an intractable marginal likelihood. To tackle this challenge, we propose to update the regression coefficients of two predictors simultaneously from their full conditional posterior distribution, which has an explicit but highly complicated characterization. This novel Gibbs updating scheme leads to an efficient Metropolis-within-Gibbs sampler that enables effective posterior sampling from our model and accurate estimation of the average treatment effect. Simulation studies demonstrate that our method performs well across a wide range of settings, in terms of both variable selection and treatment effect estimation, even when the true data-generating process does not adhere to the simplex constraint. Finally, application of our method to two empirical examples in the economic literature yields interesting insights into the impact of economic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06454v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>How to improve the regression factor score predictor when individuals have different factor loadings</title>
      <link>https://arxiv.org/abs/2503.06742</link>
      <description>arXiv:2503.06742v1 Announce Type: cross 
Abstract: Previous research has shown that ignoring individual differences of factor loadings in conventional factor models may reduce the determinacy of factor score predictors. Therefore, the aim of the present study is to propose a heterogeneous regression factor score with larger determinacy than the conventional regression factor score when individuals have different factor loadings. First, a method for the estimation of individual loadings is proposed. The individual loading estimates are used to compute the heterogeneity-based regression factor score predictor. Then, a binomial test for loading heterogeneity of a factor is recommended to compute the heterogeneity-based regression factor score predictor only when the test is significant. Otherwise, the conventional regression factor score predictor should be used. A simulation study reveals that the heterogeneity-based regression factor score predictor has larger determinacy than the conventional regression factor score predictor in populations with substantial loading heterogeneity. An empirical example based on subsamples drawn randomly from a large sample of Big Five Markers indicates that the determinacy can be improved for the factor emotional stability when the heterogeneity-based regression factor score is computed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06742v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Beauducel, Norbert Hilger, Anneke C. Weide</dc:creator>
    </item>
    <item>
      <title>Robust local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.06837</link>
      <description>arXiv:2503.06837v1 Announce Type: cross 
Abstract: This paper investigates a robust empirical Bayes correction for Bayesian modeling. We show the application of the model on income distribution. Income shock includes temporal and permanent shocks. We aim to eliminate temporal shock and permanent shock using two-step local empirical correction method. Our results show that only 6.7% of the observed income shocks were permanent shock, and the posterior (permanent) mean weekly income was reduced from the observed income 415 pounds to 202 pounds for the United Kingdom using the Living Costs and Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers; Bayesian modeling</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06837v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Doubly robust omnibus sensitivity analysis of externally controlled trials with intercurrent events</title>
      <link>https://arxiv.org/abs/2503.06864</link>
      <description>arXiv:2503.06864v1 Announce Type: cross 
Abstract: Externally controlled trials are crucial in clinical development when randomized controlled trials are unethical or impractical. These trials consist of a full treatment arm with the experimental treatment and a full external control arm. However, they present significant challenges in learning the treatment effect due to the lack of randomization and a parallel control group. Besides baseline incomparability, outcome mean non-exchangeability, caused by differences in conditional outcome distributions between external controls and counterfactual concurrent controls, is infeasible to test and may introduce biases in evaluating the treatment effect. Sensitivity analysis of outcome mean non-exchangeability is thus critically important to assess the robustness of the study's conclusions against such assumption violations. Moreover, intercurrent events, which are ubiquitous and inevitable in clinical studies, can further confound the treatment effect and hinder the interpretation of the estimated treatment effects. This paper establishes a semi-parametric framework for externally controlled trials with intercurrent events, offering doubly robust and locally optimal estimators for primary and sensitivity analyses. We develop an omnibus sensitivity analysis that accounts for both outcome mean non-exchangeability and the impacts of intercurrent events simultaneously, ensuring root-n consistency and asymptotic normality under specified conditions. The performance of the proposed sensitivity analysis is evaluated in simulation studies and a real-data problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenyin Gao, Xiang Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Representative dietary behavior patterns and associations with cardiometabolic outcomes in Puerto Rico using a Bayesian latent class analysis for non-probability samples</title>
      <link>https://arxiv.org/abs/2503.07240</link>
      <description>arXiv:2503.07240v1 Announce Type: cross 
Abstract: There is limited understanding of how dietary behaviors cluster together and influence cardiometabolic health at a population level in Puerto Rico. Data availability is scarce, particularly outside of urban areas, and is often limited to non-probability sample (NPS) data where sample inclusion mechanisms are unknown. In order to generalize results to the broader Puerto Rican population, adjustments are necessary to account for selection bias but are difficult to implement for NPS data. Although Bayesian latent class models enable summaries of dietary behavior variables through underlying patterns, they have not yet been adapted to the NPS setting. We propose a novel Weighted Overfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN utilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS using Bayesian additive regression trees (BART) and a reference probability sample, and (2) integrate the pseudo-weights within a weighted pseudo-likelihood approach for Bayesian latent class analysis, while propagating pseudo-weight uncertainty into parameter estimation. A stacked sample approach is used to allow shared individuals between the NPS and the reference sample. We evaluate model performance through simulations and apply WOLCAN to data from the Puerto Rico Observational Study of Psychosocial, Environmental, and Chronic Disease Trends (PROSPECT). We identify dietary behavior patterns for adults in Puerto Rico aged 30 to 75 and examine their associations with type 2 diabetes, hypertension, and hypercholesterolemia. Our findings suggest that an out-of-home eating pattern is associated with a higher likelihood of these cardiometabolic outcomes compared to a nutrition-sensitive pattern. WOLCAN effectively reveals generalizable dietary behavior patterns and demonstrates relevant applications in studying diet-disease relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07240v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie M. Wu, Abrania Marrero, Matthew R. Williams, Terrance D. Savitsky, Josiemer Mattei, Jos\'e Rodr\'iguez-Orengo, Briana J. K. Stephenson</dc:creator>
    </item>
    <item>
      <title>Testing for Markovian character of transfer of fluctuations in solar wind turbulence on kinetic scales</title>
      <link>https://arxiv.org/abs/2503.07255</link>
      <description>arXiv:2503.07255v1 Announce Type: cross 
Abstract: We apply statistical analysis to search for processes responsible for turbulence in physical systems. In our previous studies, we have shown that solar wind turbulence in the inertial range of large magnetohydrodynamic scales exhibits Markov properties. We have recently extended this approach on much smaller kinetic scales. Here we are testing for the Markovian character of stochastic processes in a kinetic regime based on magnetic field and velocity fluctuations in the solar wind, measured onboard the Magnetospheric Multiscale (MMS) mission: behind the bow shock, inside the magnetosheath, and near the magnetopause. We have verified that the Chapman-Kolmogorov necessary conditions for Markov processes is satisfied for local transfer of energy between the magnetic and velocity fields also on kinetic scales. We have confirmed that for magnetic fluctuations, the first Kramers-Moyal coefficient is linear, while the second term is quadratic, corresponding to drift and diffusion processes in the resulting Fokker-Planck equation. It means that magnetic self-similar turbulence is described by generalized Ornstein-Uhlenbeck processes. We show that for the magnetic case, the Fokker-Planck equation leads to the probability density functions of the kappa distributions, which exhibit global universal scale invariance with a linear scaling and lack of intermittency. On the contrary, for velocity fluctuations, higher order Kramers-Moyal coefficients should be taken into account and hence scale invariance is not observed. However, the nonextensity parameter in Tsallis entropy provides a robust measure of the departure of the system from equilibrium. The obtained results are important for a better understanding of the physical mechanism governing turbulent systems in space and laboratory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07255v1</guid>
      <category>physics.plasm-ph</category>
      <category>astro-ph.SR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.110.025203</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 110, 025203 - Published 12 August, 2024</arxiv:journal_reference>
      <dc:creator>Dariusz W\'ojcik, Wies{\l}aw M. Macek</dc:creator>
    </item>
    <item>
      <title>Using Markov Boundary Approach for Interpretable and Generalizable Feature Selection</title>
      <link>https://arxiv.org/abs/2307.14327</link>
      <description>arXiv:2307.14327v2 Announce Type: replace 
Abstract: The perceived advantage of machine learning (ML) models is that they are flexible and can incorporate a large number of features. However, many of these are typically correlated or dependent, and incorporating all of them can hinder model stability and generalizability. In fact, it is desirable to do some form of feature screening and incorporate only the relevant features. The best approaches should involve subject-matter knowledge and information on causal relationships. This paper deals with an approach called Markov boundary (MB) that is related to causal discovery, using directed acyclic graphs to represent potential relationships and using statistical tests to determine the connections. An MB is the minimum set of features that guarantee that other potential predictors do not affect the target given the boundary while ensuring maximal predictive accuracy. Identifying the Markov boundary is straightforward under assumptions of Gaussianity on the features and linear relationships between them. But these assumptions are not satisfied in practice. This paper outlines common problems associated with identifying the Markov boundary in structured data when relationships are non-linear and the predictors are of mixed data type. We propose a multi-group forward-backward selection strategy that addresses these challenges and demonstrate its capabilities on simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14327v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anwesha Bhattacharyya, Yaqun Wang, Joel Vaughan, Vijayan N. Nair</dc:creator>
    </item>
    <item>
      <title>Choosing alpha post hoc: the danger of multiple standard significance thresholds</title>
      <link>https://arxiv.org/abs/2410.02306</link>
      <description>arXiv:2410.02306v2 Announce Type: replace 
Abstract: A fundamental assumption of classical hypothesis testing is that the significance threshold $\alpha$ is chosen independently from the data. The validity of confidence intervals likewise relies on choosing $\alpha$ beforehand. We point out that the independence of $\alpha$ is guaranteed in practice because, in most fields, there exists one standard $\alpha$ that everyone uses -- so that $\alpha$ is automatically independent of everything. However, there have been recent calls to decrease $\alpha$ from $0.05$ to $0.005$. We note that this may lead to multiple accepted standard thresholds within one scientific field. For example, different journals may require different significance thresholds. As a consequence, some researchers may be tempted to conveniently choose their $\alpha$ based on their p-value. We use examples to illustrate that this severely invalidates hypothesis tests, and mention some potential solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02306v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik, Nick W Koning</dc:creator>
    </item>
    <item>
      <title>Targeting mediating mechanisms of social disparities with an interventional effects framework, applied to the gender pay gap in Western Germany</title>
      <link>https://arxiv.org/abs/2411.07368</link>
      <description>arXiv:2411.07368v5 Announce Type: replace 
Abstract: The Oaxaca-Blinder decomposition is a widely used method to explain social disparities. However, assigning causal meaning to its estimated components requires strong assumptions that often lack explicit justification. This article emphasizes the importance of clearly defined estimands and their identification when targeting mediating mechanisms of social disparities. Three approaches are distinguished based on their scientific questions and assumptions: a mediation approach and two interventional approaches. The Oaxaca-Blinder decomposition and Monte Carlo simulation-based g-computation are discussed for estimation in relation to these approaches. The latter method is used in an interventional effects analysis of the observed gender pay gap in Western Germany, using data from the 2017 German Socio-Economic Panel. Ten mediators, including indicators of human capital and job characteristics, are considered. Key findings indicate that the gender pay gap in log hourly wages could be reduced by up to 86% if these mediators were equally distributed between women and men. Substantial reductions could be achieved by aligning full-time employment and work experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07368v5</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christiane Didden</dc:creator>
    </item>
    <item>
      <title>Change-point regression with a smooth additive disturbance</title>
      <link>https://arxiv.org/abs/2112.03878</link>
      <description>arXiv:2112.03878v2 Announce Type: replace-cross 
Abstract: We assume a nonparametric regression model where the signal is given by the sum of a piecewise constant function and a smooth function. To detect the change-points and estimate the regression functions, we propose PCpluS, a combination of the fused Lasso and kernel smoothing. In contrast to existing approaches, it explicitly uses the additive decomposition of the signal when detecting change-points. This is motivated by several applications and by theoretical results about partial linear model. We show how the use of the Epanechnikov kernel in the linear smoother results in very fast computation. Simulations demonstrate that our approach has a small mean squared error and detects change-points well. We also apply the methodology to genome sequencing data to detect copy number variations. Finally, we demonstrate its flexibility by proposing extensions to multivariate and filtered data. An R-package called PCpluS is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03878v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Pein, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Adaptive truncation of infinite sums: applications to Statistics</title>
      <link>https://arxiv.org/abs/2202.06121</link>
      <description>arXiv:2202.06121v2 Announce Type: replace-cross 
Abstract: It is often the case in Statistics that one needs to compute sums of infinite series, especially in marginalising over discrete latent variables. This has become more relevant with the popularization of gradient-based techniques (e.g. Hamiltonian Monte Carlo) in the Bayesian inference context, for which discrete latent variables are hard or impossible to deal with. For many commonly used infinite series, custom algorithms have been developed which exploit specific features of each problem. General techniques, suitable for a large class of problems with limited input from the user are less established. We employ basic results from the theory of infinite series to investigate general, problem-agnostic algorithms to truncate infinite sums within an arbitrary tolerance $\varepsilon &gt; 0$ and provide robust computational implementations with provable guarantees. We compare three tentative solutions to estimating the infinite sum of interest: (i) a "naive" approach that sums terms until the terms are below the threshold $\varepsilon$; (ii) a `bounding pair' strategy based on trapping the true value between two partial sums; and (iii) a `batch' strategy that computes the partial sums in regular intervals and stops when their difference is less than $\varepsilon$. We show under which conditions each strategy guarantees the truncated sum is within the required tolerance and compare the error achieved by each approach, as well as the number of function evaluations necessary for each one. A detailed discussion of numerical issues in practical implementations is also provided. The paper provides some theoretical discussion of a variety of statistical applications, including raw and factorial moments and count models with observation error. Finally, detailed illustrations in the form noisy MCMC for Bayesian inference and maximum marginal likelihood estimation are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06121v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luiz Max Carvalho, Wellington J. Silva, Guido A. Moreira</dc:creator>
    </item>
    <item>
      <title>lpcde: Estimation and Inference for Local Polynomial Conditional Density Estimators</title>
      <link>https://arxiv.org/abs/2204.10375</link>
      <description>arXiv:2204.10375v3 Announce Type: replace-cross 
Abstract: This paper discusses the R package lpcde, which stands for local polynomial conditional density estimation. It implements the kernel-based local polynomial smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2024) for statistical estimation and inference of conditional distributions, densities, and derivatives thereof. The package offers mean square error optimal bandwidth selection and associated point estimators, as well as uncertainty quantification based on robust bias correction both pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands) over evaluation points. The methods implemented are boundary adaptive whenever the data is compactly supported. The package also implements regularized conditional density estimation methods, ensuring the resulting density estimate is non-negative and integrates to one. We contrast the functionalities of lpcde with existing open-source packages for conditional density estimation, and showcase its main features using simulated and real datasets. An abbreviated version of this article is published in Cattaneo, Chandak, Jansson, Ma (2025 JOSS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10375v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rajita Chandak, Michael Jansson, Xinwei Ma</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning Methods for Estimating Average Treatment Effects: A Comparative Study</title>
      <link>https://arxiv.org/abs/2204.10969</link>
      <description>arXiv:2204.10969v5 Announce Type: replace-cross 
Abstract: Observational cohort studies are increasingly being used for comparative effectiveness research to assess the safety of therapeutics. Recently, various doubly robust methods have been proposed for average treatment effect estimation by combining the treatment model and the outcome model via different vehicles, such as matching, weighting, and regression. The key advantage of doubly robust estimators is that they require either the treatment model or the outcome model to be correctly specified to obtain a consistent estimator of average treatment effects, and therefore lead to a more accurate and often more precise inference. However, little work has been done to understand how doubly robust estimators differ due to their unique strategies of using the treatment and outcome models and how machine learning techniques can be combined to boost their performance, which we call double machine learning estimators. Here we examine multiple popular doubly robust methods and compare their performance using different treatment and outcome modeling via extensive simulations and a real-world application. We found that incorporating machine learning with doubly robust estimators such as the targeted maximum likelihood estimator gives the best overall performance. Practical guidance on how to apply doubly robust estimators is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10969v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Tan, Shu Yang, Wenyu Ye, Douglas E. Faries, Ilya Lipkovich, Zbigniew Kadziola</dc:creator>
    </item>
    <item>
      <title>Predicting Distributions of Physical Activity Profiles in the NHANES Database Using a Partially Linear Fr\'echet Single Index Model</title>
      <link>https://arxiv.org/abs/2302.07692</link>
      <description>arXiv:2302.07692v2 Announce Type: replace-cross 
Abstract: Object-oriented data analysis is a fascinating and evolving field in modern statistical science, with the potential to make significant contributions to biomedical applications. This statistical framework facilitates the development of new methods to analyze complex data objects that capture more information than traditional clinical biomarkers. This paper applies the object-oriented framework to analyze physical activity levels, measured by accelerometers, as response objects in a regression model. Unlike traditional summary metrics, we utilize a recently proposed representation of physical activity data as a distributional object, providing a more nuanced and complete profile of individual energy expenditure across all ranges of monitoring intensity. A novel hybrid Fr\'echet regression model is proposed and applied to US population accelerometer data from National Health and Nutrition Examination Survey (NHANES) 2011-2014. The semi-parametric nature of the model allows for the inclusion of nonlinear effects for critical variables, such as age, which are biologically known to have subtle impacts on physical activity. Simultaneously, the inclusion of linear effects preserves interpretability for other variables, particularly categorical covariates such as ethnicity and sex. The results obtained are valuable from a public health perspective and could lead to new strategies for optimizing physical activity interventions in specific American subpopulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07692v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Aritra Ghosal, Wendy Meiring, Alexander Petersen</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Framework for Evaluating Time to Event Predictions using the Restricted Mean Survival Time</title>
      <link>https://arxiv.org/abs/2306.16075</link>
      <description>arXiv:2306.16075v2 Announce Type: replace-cross 
Abstract: The restricted mean survival time (RMST) is a widely used quantity in survival analysis due to its straightforward interpretation. For instance, predicting the time to event based on patient attributes is of great interest when analyzing medical data. In this paper, we propose a novel framework for evaluating RMST estimations. A criterion that estimates the mean squared error of an RMST estimator using Inverse Probability Censoring Weighting (IPCW) is presented. A model-agnostic conformal algorithm adapted to right-censored data is also introduced to compute prediction intervals and to evaluate local variable importance. Finally, a model-agnostic statistical test is developed to assess global variable importance. Our framework is valid for any RMST estimator that is asymptotically convergent and works under model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16075v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/sjos.12766</arxiv:DOI>
      <arxiv:journal_reference>Scandinavian Journal of Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Ariane Cwiling (MAP5 - UMR 8145), Vittorio Perduca (MAP5 - UMR 8145), Olivier Bouaziz (MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>Optimal Cut-Point Estimation for Functional Digital Biomarkers: Application to Diabetes Risk Stratification via Continuous Glucose Monitoring</title>
      <link>https://arxiv.org/abs/2404.09716</link>
      <description>arXiv:2404.09716v2 Announce Type: replace-cross 
Abstract: Establishing optimal cut-offs for clinical biomarkers is a fundamental statistical problem in epidemiology, clinical trials, and drug discovery. While there is extensive literature regarding the definition of optimal cut-offs for scalar biomarkers, methodologies for analyzing random statistical objects in the more complex spaces associated with random functions and graphs - something increasingly required in the field of modern digital health applications - are lacking. This paper proposes a new, general, simple methodology for defining optimal cut-offs for random objects residing in separable Hilbert spaces. Its underlying motivation is the need to create new, digital health rules for the detection of diabetes mellitus, and thus better exploit the continuous high-dimensional functional information provided by continuous glucose monitors (CGM). A functional cut-off for identifying diabetes is offered, based on glucose distributional representations from CGM time series. This work may be a valuable resource for researchers interested in defining and validating new digital biomarkers for biosensor time series</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09716v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Lado-Baleato, Carla D\'iaz-Louza, Francisco Gude, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title>
      <link>https://arxiv.org/abs/2405.02551</link>
      <description>arXiv:2405.02551v2 Announce Type: replace-cross 
Abstract: Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02551v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>Climate Change in Austria: Precipitation and Dry Spells over the last 50 years</title>
      <link>https://arxiv.org/abs/2408.11497</link>
      <description>arXiv:2408.11497v2 Announce Type: replace-cross 
Abstract: We propose a statistical model for precipitation patterns that resolves small-scale local effects in the Austrian Alpine region. Despite the significance of accounting for elevation-dependent precipitation changes in the Alpine region, they have not been extensively explored in regional climate studies. We investigate changes in precipitation patterns between two 10-year periods over the past 50 years in Austria. Specifically, we analyse real precipitation data for three scenarios: monthly mean, monthly maximum precipitation, and the monthly maximum length of a dry spell. We compute temporal difference maps to visualise these changes by comparing the average monthly precipitation scenario across the two decades 1973-1982 and 2013-2022. Our findings are essential for detecting fine-scale precipitation changes in Austria, identifying thresholds across space and time and creating the basis for political decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11497v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold</dc:creator>
    </item>
    <item>
      <title>Age Group Sensitivity Analysis of Epidemic Models: Investigating the Impact of Contact Matrix Structure</title>
      <link>https://arxiv.org/abs/2502.19206</link>
      <description>arXiv:2502.19206v2 Announce Type: replace-cross 
Abstract: Understanding the role of different age groups in disease transmission is crucial for designing effective intervention strategies. A key parameter in age-structured epidemic models is the contact matrix, which defines the interaction structure between age groups. However, accurately estimating contact matrices is challenging, as different age groups respond differently to surveys and are accessible through different channels. This variability introduces significant epistemic uncertainty in epidemic models.
  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method, a novel framework for assessing the impact of age-structured contact patterns on epidemic outcomes. Our approach integrates age-stratified epidemic models with Latin Hypercube Sampling (LHS) and the Partial Rank Correlation Coefficient (PRCC) method, enabling a systematic sensitivity analysis of age-specific interactions. Additionally, we propose a new sensitivity aggregation technique that quantifies the contribution of each age group to key epidemic parameters.
  By identifying the age groups to which the model is most sensitive, AGSA helps pinpoint those that introduce the greatest epistemic uncertainty. This allows for targeted data collection efforts, focusing surveys and empirical studies on the most influential age groups to improve model accuracy. As a result, AGSA can enhance epidemic forecasting and inform the design of more effective and efficient public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19206v2</guid>
      <category>q-bio.QM</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsolt Vizi, Evans Kiptoo Korir, Norbert Bogya, Csaba Roszt\'oczy, G\'eza Makay, P\'eter Boldog</dc:creator>
    </item>
  </channel>
</rss>
