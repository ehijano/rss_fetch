<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 05:03:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Outcome Regression Methods for Analyzing Hybrid Control Studies: Balancing Bias and Variability</title>
      <link>https://arxiv.org/abs/2501.17358</link>
      <description>arXiv:2501.17358v1 Announce Type: new 
Abstract: There is growing interest in a hybrid control design in which a randomized controlled trial is augmented with an external control arm from a previous trial or real world data. Existing methods for analyzing hybrid control studies include various downweighting and propensity score methods as well as methods that combine downweighting with propensity score stratification. In this article, we describe and discuss methods that make use of an outcome regression model (possibly in addition to a propensity score model). Specifically, we consider an augmentation method, a G-computation method, and a weighted regression method, and note that the three methods provide different bias-variance trade-offs. The methods are compared with each other and with existing methods in a simulation study. Simulation results indicate that weighted regression compares favorably with other model-based methods that seek to improve efficiency by incorporating external control data. The methods are illustrated using two examples from urology and infectious disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17358v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhang, Jialuo Liu, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Gradient-free Importance Sampling Scheme for Efficient Reliability Estimation</title>
      <link>https://arxiv.org/abs/2501.17401</link>
      <description>arXiv:2501.17401v1 Announce Type: new 
Abstract: This work presents a novel gradient-free importance sampling-based framework for precisely and efficiently estimating rare event probabilities, often encountered in reliability analyses of engineering systems. The approach is formulated around our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) methodology. ASTPA uniquely constructs and directly samples an unnormalized target distribution, relaxing the optimal importance sampling distribution (ISD). The target's normalizing constant is then estimated using our inverse importance sampling (IIS) scheme, employing an ISD fitted based on the obtained samples. In this work, a gradient-free sampling method within ASTPA is developed through a guided dimension-robust preconditioned Crank-Nicolson (pCN) algorithm, particularly suitable for black-box computational models where analytical gradient information is not available. To boost the sampling efficiency of pCN in our context, a computationally effective, general discovery stage for the rare event domain is devised, providing (multi-modal) rare event samples used in initializing the pCN chains. A series of diverse test functions and engineering problems involving high dimensionality and strong nonlinearity is presented, demonstrating the advantages of the proposed framework compared to several state-of-the-art sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17401v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsayed Eshra, Konstantinos G. Papakonstantinou</dc:creator>
    </item>
    <item>
      <title>Geodesic Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2501.17436</link>
      <description>arXiv:2501.17436v1 Announce Type: new 
Abstract: Difference-in-differences (DID) is a widely used quasi-experimental design for causal inference, traditionally applied to scalar or Euclidean outcomes, while extensions to outcomes residing in non-Euclidean spaces remain limited. Existing methods for such outcomes have primarily focused on univariate distributions, leveraging linear operations in the space of quantile functions, but these approaches cannot be directly extended to outcomes in general metric spaces. In this paper, we propose geodesic DID, a novel DID framework for outcomes in geodesic metric spaces, such as distributions, networks, and manifold-valued data. To address the absence of algebraic operations in these spaces, we use geodesics as proxies for differences and introduce the geodesic average treatment effect on the treated (ATT) as the causal estimand. We establish the identification of the geodesic ATT and derive the convergence rate of its sample versions, employing tools from metric geometry and empirical process theory. This framework is further extended to the case of staggered DID settings, allowing for multiple time periods and varying treatment timings. To illustrate the practical utility of geodesic DID, we analyze health impacts of the Soviet Union's collapse using age-at-death distributions and assess effects of U.S. electricity market liberalization on electricity generation compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17436v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yidong Zhou, Daisuke Kurisu, Taisuke Otsu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Applying non-negative matrix factorization with covariates to multivariate time series data as a vector autoregression model</title>
      <link>https://arxiv.org/abs/2501.17446</link>
      <description>arXiv:2501.17446v1 Announce Type: new 
Abstract: Non-negative matrix factorization (NMF) is a powerful technique for dimensionality reduction, but its application to time series data remains limited. This paper proposes a novel framework that integrates NMF with a vector autoregression (VAR) model to capture both latent structure and temporal dependencies in multivariate time series data. By representing the NMF coefficient matrix as a VAR model, the framework leverages the interpretability of NMF while incorporating the dynamic characteristics of time series data. This approach allows for the extraction of meaningful features and accurate predictions in time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17446v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Nonparametric Smoothing of Directional and Axial Data</title>
      <link>https://arxiv.org/abs/2501.17463</link>
      <description>arXiv:2501.17463v1 Announce Type: new 
Abstract: We discuss generalized linear models for directional data where the conditional distribution of the response is a von Mises-Fisher distribution in arbitrary dimension or a Bingham distribution on the unit circle. To do this properly, we parametrize von Mises-Fisher distributions by Euclidean parameters and investigate computational aspects of this parametrization. Then we modify this approach for local polynomial regression as a means of nonparametric smoothing of distributional data. The methods are illustrated with simulated data and a data set from planetary sciences involving covariate vectors on a sphere with axial response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17463v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutz Duembgen, Caroline Haslebacher</dc:creator>
    </item>
    <item>
      <title>Semiparametric principal stratification analysis beyond monotonicity</title>
      <link>https://arxiv.org/abs/2501.17514</link>
      <description>arXiv:2501.17514v1 Announce Type: new 
Abstract: Intercurrent events, common in clinical trials and observational studies, affect the existence or interpretation of final outcomes. Principal stratification addresses these challenges by defining local average treatment effects within latent subpopulations, but often relies on restrictive assumptions such as monotonicity and counterfactual intermediate independence. To address these limitations, we propose a unified semiparametric framework for principal stratification analysis leveraging a margin-free, conditional odds ratio sensitivity parameter. Under principal ignorability, we derive nonparametric identification formulas and develop efficient estimation methods, including a conditionally doubly robust parametric estimator and a de-biased machine learning estimator with data-adaptive nuisance estimators. Simulations show that incorrectly assuming monotonicity can often lead to suboptimal inference, while specifying non-trivial odds ratio sensitivity parameter can enable approximately valid inference under monotonicity. We apply our methods to a critical care trial and further suggest a semiparametric sensitivity analysis approach under violation of principal ignorability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17514v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaqi Tong, Brennan Kahan, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian Integrative Mixed Modeling Framework for Analysis of the Adolescent Brain and Cognitive Development Study</title>
      <link>https://arxiv.org/abs/2501.17705</link>
      <description>arXiv:2501.17705v1 Announce Type: new 
Abstract: Integrating high-dimensional, heterogeneous data from multi-site cohort studies with complex hierarchical structures poses significant feature selection and prediction challenges. We extend the Bayesian Integrative Analysis and Prediction (BIP) framework to enable simultaneous feature selection and outcome modeling in data of nested hierarchical structure. We apply the proposed Bayesian Integrative Mixed Modeling (BIPmixed) framework to the Adolescent Brain Cognitive Development (ABCD) Study, leveraging multi-view data, including structural and functional MRI and early life adversity (ELA) metrics, to identify relevant features and predict the behavioral outcome. BIPmixed incorporates 2-level nested random effects, to enhance interpretability and make predictions in hierarchical data settings. Simulation studies illustrate BIPmixed's robustness in distinct random effect settings, highlighting its use for complex study designs. Our findings suggest that BIPmixed effectively integrates multi-view data while accounting for nested sampling, making it a valuable tool for analyzing large-scale studies with hierarchical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17705v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Neher, Apostolos Stamenos, Mark Fiecas, Sandra Safo, Thierry Chekouo</dc:creator>
    </item>
    <item>
      <title>An Estimator-Robust Design for Augmenting Randomized Controlled Trial with External Real-World Data</title>
      <link>https://arxiv.org/abs/2501.17835</link>
      <description>arXiv:2501.17835v1 Announce Type: new 
Abstract: Augmenting randomized controlled trials (RCTs) with external real-world data (RWD) has the potential to improve the finite sample efficiency of treatment effect estimators. We describe using adaptive targeted maximum likelihood estimation (A-TMLE) for estimating the average treatment effect (ATE) by decomposing the ATE estimand into two components: a pooled-ATE estimand that combines data from both the RCT and external sources, and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. This approach views the RCT data as the reference and corrects for inconsistencies of any kind between the RCT and the external data source. Given the growing abundance of external RWD from modern electronic health records, determining the optimal strategy to select candidate external patients for data integration remains an open yet critical problem. In this work, we begin by analyzing the robustness property of the A-TMLE estimator and then propose a matching-based sampling strategy that improves the robustness of the estimator with respect to the target estimand. Our proposed strategy is outcome-blind and involves matching based on two one-dimensional scores: the trial enrollment score and the propensity score in the external data. We demonstrate in simulations that our sampling strategy improves the coverage and shortens the widths of confidence intervals produced by A-TMLE. We illustrate our method with a case study of augmenting the DEVOTE cardiovascular safety trial by using the Optum Clinformatics claims database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17835v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sky Qiu, Jens Tarp, Andrew Mertens, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Constructing Simultaneous Confidence Bands for Errors-in-variables Curves with Application to the Lorenz Curve</title>
      <link>https://arxiv.org/abs/2501.17264</link>
      <description>arXiv:2501.17264v1 Announce Type: cross 
Abstract: Errors-in-variables curves are curves where errors exist not only in the independent variable but also in the dependent variable. We address the challenge of constructing simultaneous confidence bands (SCBs) for such curves. Our method finds application in the Lorenz curve, which represents the concentration of income or wealth. Unlike ordinary regression curves, the Lorenz curve incorporates errors in its explanatory variable and requires a fundamentally different treatment. To the best of our knowledge, the development of SCBs for such curves has not been explored in previous research. Using the Lorenz curve as a case study, this paper proposes a novel approach to address this challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17264v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziqing Dong, Francesco Bartolucci, Satoshi Kuriki, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Fundamental Computational Limits in Pursuing Invariant Causal Prediction and Invariance-Guided Regularization</title>
      <link>https://arxiv.org/abs/2501.17354</link>
      <description>arXiv:2501.17354v1 Announce Type: cross 
Abstract: Pursuing invariant prediction from heterogeneous environments opens the door to learning causality in a purely data-driven way and has several applications in causal discovery and robust transfer learning. However, existing methods such as ICP [Peters et al., 2016] and EILLS [Fan et al., 2024] that can attain sample-efficient estimation are based on exponential time algorithms. In this paper, we show that such a problem is intrinsically hard in computation: the decision problem, testing whether a non-trivial prediction-invariant solution exists across two environments, is NP-hard even for the linear causal relationship. In the world where P$\neq$NP, our results imply that the estimation error rate can be arbitrarily slow using any computationally efficient algorithm. This suggests that pursuing causality is fundamentally harder than detecting associations when no prior assumption is pre-offered.
  Given there is almost no hope of computational improvement under the worst case, this paper proposes a method capable of attaining both computationally and statistically efficient estimation under additional conditions. Furthermore, our estimator is a distributionally robust estimator with an ellipse-shaped uncertain set where more uncertainty is placed on spurious directions than invariant directions, resulting in a smooth interpolation between the most predictive solution and the causal solution by varying the invariance hyper-parameter. Non-asymptotic results and empirical applications support the claim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17354v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Gu, Cong Fang, Yang Xu, Zijian Guo, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Controlling FDR in selecting group-level simultaneous signals from multiple data sources with application to the National Covid Collaborative Cohort data</title>
      <link>https://arxiv.org/abs/2303.01599</link>
      <description>arXiv:2303.01599v2 Announce Type: replace 
Abstract: One challenge in exploratory association studies using observational data is that the associations between the predictors and the outcome are potentially weak and rare, and the candidate predictors have complex correlation structures. False discovery rate (FDR) controlling procedures can provide important statistical guarantees for replicability in predictor identification in exploratory research. In the recently established National COVID Collaborative Cohort (N3C), electronic health record (EHR) data on the same set of candidate predictors are independently collected in multiple different sites, offering opportunities to identify true associations by combining information from different sources. This paper presents a general knockoff-based variable selection algorithm to identify associations from unions of group-level conditional independence tests (simultaneous signals) with exact FDR control guarantees under finite sample settings. This algorithm can work with general regression settings, allowing heterogeneity of both the predictors and the outcomes across multiple data sources. We demonstrate the performance of this method with extensive numerical studies and an application to the N3C data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01599v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Runqiu Wang (on behalf of N3C consortium), Ran Dai (on behalf of N3C consortium), Hongying Dai (on behalf of N3C consortium), Evan French (on behalf of N3C consortium), Cheng Zheng (on behalf of N3C consortium)</dc:creator>
    </item>
    <item>
      <title>A Likelihood Perspective on Dose-Finding Study Designs in Oncology</title>
      <link>https://arxiv.org/abs/2304.12391</link>
      <description>arXiv:2304.12391v2 Announce Type: replace 
Abstract: Dose-finding studies in oncology often include an up-and-down dose transition rule that assigns a dose to each cohort of patients based on accumulating data on dose-limiting toxicity (DLT) events. In making a dose transition decision, a key scientific question is whether the true DLT rate of the current dose exceeds the target DLT rate, and the statistical question is how to evaluate the statistical evidence in the available DLT data with respect to that scientific question. This article introduces generalized likelihood ratios (GLRs) that can be used to measure statistical evidence and support dose transition decisions. Applying this approach to a single-dose likelihood leads to a GLR-based interval design with three parameters: the target DLT rate and two GLR cut-points representing the levels of evidence required for dose escalation and de-escalation. This design gives a likelihood interpretation to each existing interval design and provides a unified framework for comparing different interval designs in terms of how much evidence is required for escalation and de-escalation. A GLR-based comparison of commonly used interval designs reveals important differences and motivates alternative designs that reduce over-treatment while maintaining MTD estimation accuracy. The GLR-based approach can also be applied to a joint likelihood based on a nonparametric (e.g., isotonic regression) model or a parametric model. Simulation results indicate that the isotonic GLR performs similarly to the single-dose GLR but the GLR based on a parsimonious model can improve MTD estimation when the underlying model is correct.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12391v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric boundary detection for income areal data</title>
      <link>https://arxiv.org/abs/2312.13992</link>
      <description>arXiv:2312.13992v2 Announce Type: replace 
Abstract: Recent discussions on the future of metropolitan cities underscore the pivotal role of (social) equity, driven by demographic and economic trends. More equal policies can foster and contribute to a city's economic success and social stability. In this work, we focus on identifying metropolitan areas with distinct economic and social levels in the greater Los Angeles area, one of the most diverse yet unequal areas in the United States. Utilising American Community Survey data, we propose a Bayesian model for boundary detection based on areal income distributions. The model identifies areas with significant income disparities, offering actionable insights for policymakers to address social and economic inequalities. We have multiple observations (i.e., personal income of survey respondents) for each area, and our approach, formalised as a Bayesian structural learning framework, models areal densities through mixtures of finite mixtures. We address boundary detection by identifying boundaries for which the associated geographically contiguous areal densities are estimated as being very different without resorting to dissimilarity metrics or covariates. Efficient posterior computation is facilitated by a transdimensional Markov Chain Monte Carlo sampler. The methodology is validated via extensive simulations and applied to the income data in the greater Los Angeles area. We identify several boundaries in the income distributions, which can be explained ex-post in terms of the percentage of the population without health insurance, though not in terms of the total number of crimes, showing the usefulness of such an analysis to policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13992v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Gianella, Mario Beraha, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary Quantile and Count Data Regressions</title>
      <link>https://arxiv.org/abs/2403.03646</link>
      <description>arXiv:2403.03646v2 Announce Type: replace 
Abstract: Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS have been used for many decades in econometrics and more recently to investigate how poor air quality adversely affects human health. In this paper we describe how to expand the utility of these models for Bayesian inference by leveraging latent variables. In particular we explain how to perform binary regression to better handle imbalanced data, how to incorporate negative binomial regression, and how to estimate the probability of predictor inclusion. Extra parameters introduced through the DLM framework may require calibration for the MCMC algorithm, but this will not be the case in DLM-based analyses often seen in pollution exposure literature. In these cases, the parameters are inferred through a fully automatic Gibbs sampling procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03646v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Dempsey, Jason Wyse</dc:creator>
    </item>
    <item>
      <title>Applying Non-negative Matrix Factorization with Covariates to the Longitudinal Data as Growth Curve Model</title>
      <link>https://arxiv.org/abs/2403.05359</link>
      <description>arXiv:2403.05359v3 Announce Type: replace 
Abstract: Using Non-negative Matrix Factorization (NMF), the observed matrix can be approximated by the product of the basis and coefficient matrices. Moreover, if the coefficient vectors are explained by the covariates for each individual, the coefficient matrix can be written as the product of the parameter matrix and the covariate matrix, and additionally described in the framework of Non-negative Matrix tri-Factorization (tri-NMF) with covariates. Consequently, this is equal to the mean structure of the Growth Curve Model (GCM). The difference is that the basis matrix for GCM is given by the analyst, whereas that for NMF with covariates is unknown and optimized. In this study, we applied NMF with covariance to longitudinal data and compared it with GCM. We have also published an R package that implements this method, and we show how to use it through examples of data analyses including longitudinal measurement, spatiotemporal data and text data. In particular, we demonstrate the usefulness of Gaussian kernel functions as covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05359v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Admissible online closed testing must employ e-values</title>
      <link>https://arxiv.org/abs/2407.15733</link>
      <description>arXiv:2407.15733v2 Announce Type: replace 
Abstract: In contemporary research, data scientists often test an infinite sequence of hypotheses $H_1,H_2,\ldots $ one by one, and are required to make real-time decisions without knowing the future hypotheses or data. In this paper, we consider such an online multiple testing problem with the goal of providing simultaneous lower bounds for the number of true discoveries in data-adaptively chosen rejection sets. In offline multiple testing, it has been recently established that such simultaneous inference is admissible iff it proceeds through (offline) closed testing. We establish an analogous result in this paper using the recent online closure principle. In particular, we show that it is necessary to use an anytime-valid test for each intersection hypothesis. This connects two distinct branches of the literature: online testing of multiple hypotheses (where the hypotheses appear online), and sequential anytime-valid testing of a single hypothesis (where the data for a fixed hypothesis appears online). Motivated by this result, we construct a new online closed testing procedure and a corresponding short-cut with a true discovery guarantee based on multiplying sequential e-values. This general but simple procedure gives uniform improvements over the state-of-the-art methods but also allows to construct entirely new and powerful procedures. In addition, we introduce new ideas for hedging and boosting of sequential e-values that provably increase power. Finally, we also propose the first online true discovery procedures for exchangeable and arbitrarily dependent e-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15733v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Nonparametric methods controlling the median of the false discovery proportion</title>
      <link>https://arxiv.org/abs/2501.16985</link>
      <description>arXiv:2501.16985v2 Announce Type: replace 
Abstract: When testing many hypotheses, often we do not have strong expectations about the directions of the effects. In some situations however, the alternative hypotheses are that the parameters lie in a certain direction or interval, and it is in fact expected that most hypotheses are false. This is often the case when researchers perform multiple noninferiority or equivalence tests, e.g. when testing food safety with metabolite data. The goal is then to use data to corroborate the expectation that most hypotheses are false. We propose a nonparametric multiple testing approach that is powerful in such situations. If the user's expectations are wrong, our approach will still be valid but have low power. Of course all multiple testing methods become more powerful when appropriate one-sided instead of two-sided tests are used, but our approach has superior power then. The methods in this paper control the median of the false discovery proportion (FDP), which is the fraction of false discoveries among the rejected hypotheses. This approach is comparable to false discovery rate control, where one ensures that the mean rather than the median of the FDP is small. Our procedures make use of a symmetry property of the test statistics, do not require independence and are valid for finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16985v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik</dc:creator>
    </item>
    <item>
      <title>Uniform Inference on High-dimensional Spatial Panel Networks</title>
      <link>https://arxiv.org/abs/2105.07424</link>
      <description>arXiv:2105.07424v4 Announce Type: replace-cross 
Abstract: We propose employing a debiased-regularized, high-dimensional generalized method of moments (GMM) framework to perform inference on large-scale spatial panel networks. In particular, network structure with a flexible sparse deviation, which can be regarded either as latent or as misspecified from a predetermined adjacency matrix, is estimated using debiased machine learning approach. The theoretical analysis establishes the consistency and asymptotic normality of our proposed estimator, taking into account general temporal and spatial dependency inherent in the data-generating processes. A primary contribution of our study is the development of uniform inference theory that enables hypothesis testing on the parameters of interest, including zero or non-zero elements in the network structure. Additionally, the asymptotic properties for the estimator are derived for both linear and nonlinear moments. Simulations demonstrate superior performance of our proposed approach. Lastly, we apply our methodology to investigate the spatial network effect of stock returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.07424v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Chen Huang, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Dirichlet Process-based Robust Clustering using the Median-of-Means Estimator</title>
      <link>https://arxiv.org/abs/2311.15384</link>
      <description>arXiv:2311.15384v2 Announce Type: replace-cross 
Abstract: Clustering stands as one of the most prominent challenges in unsupervised machine learning. Among centroid-based methods, the classic $k$-means algorithm, based on Lloyd's heuristic, is widely used. Nonetheless, it is a well-known fact that $k$-means and its variants face several challenges, including heavy reliance on initial cluster centroids, susceptibility to converging into local minima of the objective function, and sensitivity to outliers and noise in the data. When data contains noise or outliers, the Median-of-Means (MoM) estimator offers a robust alternative for stabilizing centroid-based methods. On a different note, another limitation in many commonly used clustering methods is the need to specify the number of clusters beforehand. Model-based approaches, such as Bayesian nonparametric models, address this issue by incorporating infinite mixture models, which eliminate the requirement for predefined cluster counts. Motivated by these facts, in this article, we propose an efficient and automatic clustering technique by integrating the strengths of model-based and centroid-based methodologies. Our method mitigates the effect of noise on the quality of clustering; while at the same time, estimates the number of clusters. Statistical guarantees on an upper bound of clustering error, and rigorous assessment through simulated and real datasets, suggest the advantages of our proposed method over existing state-of-the-art clustering algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15384v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Supratik Basu, Jyotishka Ray Choudhury, Debolina Paul, Swagatam Das</dc:creator>
    </item>
    <item>
      <title>Positive Semidefinite Matrix Supermartingales</title>
      <link>https://arxiv.org/abs/2401.15567</link>
      <description>arXiv:2401.15567v4 Announce Type: replace-cross 
Abstract: We explore the asymptotic convergence and nonasymptotic maximal inequalities of supermartingales and backward submartingales in the space of positive semidefinite matrices. These are natural matrix analogs of scalar nonnegative supermartingales and backward nonnegative submartingales, whose convergence and maximal inequalities are the theoretical foundations for a wide and ever-growing body of results in statistics, econometrics, and theoretical computer science.
  Our results lead to new concentration inequalities for either martingale dependent or exchangeable random symmetric matrices under a variety of tail conditions, encompassing now-standard Chernoff bounds to self-normalized heavy-tailed settings. Further, these inequalities are usually expressed in the Loewner order, are sometimes valid simultaneously for all sample sizes or at an arbitrary data-dependent stopping time, and can often be tightened via an external randomization factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15567v4</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Algorithmic syntactic causal identification</title>
      <link>https://arxiv.org/abs/2403.09580</link>
      <description>arXiv:2403.09580v2 Announce Type: replace-cross 
Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing. Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09580v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dhurim Cakiqi, Max A. Little</dc:creator>
    </item>
    <item>
      <title>Compositional Models for Estimating Causal Effects</title>
      <link>https://arxiv.org/abs/2406.17714</link>
      <description>arXiv:2406.17714v2 Announce Type: replace-cross 
Abstract: Many real-world systems can be represented as sets of interacting components. Examples of such systems include computational systems such as query processors, natural systems such as cells, and social systems such as families. Many approaches have been proposed in traditional (associational) machine learning to model such structured systems, including statistical relational models and graph neural networks. Despite this prior work, existing approaches to estimating causal effects typically treat such systems as single units, represent them with a fixed set of variables and assume a homogeneous data-generating process. We study a compositional approach for estimating individual treatment effects (ITE) in structured systems, where each unit is represented by the composition of multiple heterogeneous components. This approach uses a modular architecture to model potential outcomes at each component and aggregates component-level potential outcomes to obtain the unit-level potential outcomes. We discover novel benefits of the compositional approach in causal inference - systematic generalization to estimate counterfactual outcomes of unseen combinations of components and improved overlap guarantees between treatment and control groups compared to the classical methods for causal effect estimation. We also introduce a set of novel environments for empirically evaluating the compositional approach and demonstrate the effectiveness of our approach using both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17714v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Purva Pruthi, David Jensen</dc:creator>
    </item>
    <item>
      <title>Golden Ratio-Based Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2410.19300</link>
      <description>arXiv:2410.19300v2 Announce Type: replace-cross 
Abstract: Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19300v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjing Yang, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>A Neyman-Orthogonalization Approach to the Incidental Parameter Problem</title>
      <link>https://arxiv.org/abs/2412.10304</link>
      <description>arXiv:2412.10304v2 Announce Type: replace-cross 
Abstract: A popular approach to perform inference on a target parameter in the presence of nuisance parameters is to construct estimating equations that are orthogonal to the nuisance parameters, in the sense that their expected first derivative is zero. Such first-order orthogonalization may, however, not suffice when the nuisance parameters are very imprecisely estimated. Leading examples where this is the case are models for panel and network data that feature fixed effects. In this paper, we show how, in the conditional-likelihood setting, estimating equations can be constructed that are orthogonal to any chosen order. Combining these equations with sample splitting yields higher-order bias-corrected estimators of target parameters. In an empirical application we apply our method to a fixed-effect model of team production and obtain estimates of complementarity in production and impacts of counterfactual re-allocations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10304v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Bonhomme, Koen Jochmans, Martin Weidner</dc:creator>
    </item>
  </channel>
</rss>
