<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jun 2025 01:40:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analysis of Multiple Long Run Relations in Panel Data Models with Applications to Financial Ratios</title>
      <link>https://arxiv.org/abs/2506.02135</link>
      <description>arXiv:2506.02135v1 Announce Type: new 
Abstract: This paper provides a new methodology for the analysis of multiple long run relations in panel data models where the cross section dimension, $n$, is large relative to the time series dimension, $T$. For panel data models with large $n$ researchers have focused on panels with a single long run relationship. The main difficulty has been to eliminate short run dynamics without generating significant uncertainty for identification of the long run. We overcome this problem by using non-overlapping sub-sample time averages as deviations from their full-sample counterpart and estimating the number of long run relations and their coefficients using eigenvalues and eigenvectors of the pooled covariance matrix of these sub-sample deviations. We refer to this procedure as pooled minimum eigenvalue (PME) and show that it applies to unbalanced panels generated from general linear processes with interactive stationary time effects and does not require knowing long run causal linkages. To our knowledge, no other estimation procedure exists for this setting. We show the PME estimator is consistent and asymptotically normal as $n$ and $T \rightarrow \infty$ jointly, such that $T\approx n^{d}$, with $d&gt;0$ for consistency and $d&gt;1/2$ for asymptotic normality. Extensive Monte Carlo studies show that the number of long run relations can be estimated with high precision and the PME estimates of the long run coefficients show small bias and RMSE and have good size and power properties. The utility of our approach is illustrated with an application to key financial variables using an unbalanced panel of US firms from merged CRSP-Compustat data set covering 2,000 plus firms over the period 1950-2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02135v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Chudik, M. Hashem Pesaran, Ron P. Smith</dc:creator>
    </item>
    <item>
      <title>Get me out of this hole: a profile likelihood approach to identifying and avoiding inferior local optima in choice models</title>
      <link>https://arxiv.org/abs/2506.02722</link>
      <description>arXiv:2506.02722v1 Announce Type: new 
Abstract: Choice modellers routinely acknowledge the risk of convergence to inferior local optima when using structures other than a simple linear-in-parameters logit model. At the same time, there is no consensus on appropriate mechanisms for addressing this issue. Most analysts seem to ignore the problem, while others try a set of different starting values, or put their faith in what they believe to be more robust estimation approaches. This paper puts forward the use of a profile likelihood approach that systematically analyses the parameter space around an initial maximum likelihood estimate and tests for the existence of better local optima in that space. We extend this to an iterative algorithm which then progressively searches for the best local optimum under given settings for the algorithm. Using a well known stated choice dataset, we show how the approach identifies better local optima for both latent class and mixed logit, with the potential for substantially different policy implications. In the case studies we conduct, an added benefit of the approach is that the new solutions exhibit properties that more closely adhere to the property of asymptotic normality, also highlighting the benefits of the approach in analysing the statistical properties of a solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02722v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephane Hess, David Bunch, Andrew Daly</dc:creator>
    </item>
    <item>
      <title>Orthogonality-Constrained Deep Instrumental Variable Model for Causal Effect Estimation</title>
      <link>https://arxiv.org/abs/2506.02790</link>
      <description>arXiv:2506.02790v1 Announce Type: new 
Abstract: OC-DeepIV is a neural network model designed for estimating causal effects. It characterizes heterogeneity by adding interaction features and reduces redundancy through orthogonal constraints. The model includes two feature extractors, one for the instrumental variable Z and the other for the covariate X*. The training process is divided into two stages: the first stage uses the mean squared error (MSE) loss function, and the second stage incorporates orthogonal regularization. Experimental results show that this model outperforms DeepIV and DML in terms of accuracy and stability. Future research directions include applying the model to real-world problems and handling scenarios with multiple processing variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02790v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shunxin Yao</dc:creator>
    </item>
    <item>
      <title>Deep Learning Enhanced Multivariate GARCH</title>
      <link>https://arxiv.org/abs/2506.02796</link>
      <description>arXiv:2506.02796v1 Announce Type: cross 
Abstract: This paper introduces a novel multivariate volatility modeling framework, named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep learning into multivariate GARCH processes. By combining the flexibility of recurrent neural networks with the econometric structure of BEKK models, our approach is designed to better capture nonlinear, dynamic, and high-dimensional dependence structures in financial return data. The proposed model addresses key limitations of traditional multivariate GARCH-based methods, particularly in capturing persistent volatility clustering and asymmetric co-movement across assets. Leveraging the data-driven nature of LSTMs, the framework adapts effectively to time-varying market conditions, offering improved robustness and forecasting performance. Empirical results across multiple equity markets confirm that the LSTM-BEKK model achieves superior performance in terms of out-of-sample portfolio risk forecast, while maintaining the interpretability from the BEKK models. These findings highlight the potential of hybrid econometric-deep learning models in advancing financial risk management and multivariate volatility forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02796v1</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Haoyuan Wang, Chen Liu, Minh-Ngoc Tran, Chao Wang</dc:creator>
    </item>
    <item>
      <title>Triple Difference Designs with Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2502.19620</link>
      <description>arXiv:2502.19620v2 Announce Type: replace 
Abstract: Triple difference designs have become increasingly popular in empirical economics. The advantage of a triple difference design is that, within a treatment group, it allows for another subgroup of the population -- potentially less impacted by the treatment -- to serve as a control for the subgroup of interest. While literature on difference-in-differences has discussed heterogeneity in treatment effects between treated and control groups or over time, little attention has been given to the implications of heterogeneity in treatment effects between subgroups. In this paper, I show that the parameter identified under the usual triple difference assumptions does not allow for causal interpretation of differences between subgroups when subgroups may differ in their underlying (unobserved) treatment effects. I propose a new parameter of interest, the causal difference in average treatment effects on the treated, which makes causal comparisons between subgroups. I discuss assumptions for identification and derive the semiparametric efficiency bounds for this parameter. I then propose doubly-robust, efficient estimators for this parameter. I use a simulation study to highlight the desirable finite-sample properties of these estimators, as well as to show the difference between this parameter and the usual triple difference parameter of interest. An empirical application shows the importance of considering treatment effect heterogeneity in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19620v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Caron</dc:creator>
    </item>
    <item>
      <title>Consumer Welfare under Individual Heterogeneity</title>
      <link>https://arxiv.org/abs/2303.01231</link>
      <description>arXiv:2303.01231v4 Announce Type: replace-cross 
Abstract: Welfare effects of price changes are often estimated with cross-sections; however, such data do not identify demand models with heterogeneous consumers. We develop a method that leverages moments of demand to construct a local approximation to the curvature of the expenditure function, which is essential for welfare analysis. Our method applies to arbitrary cross-sections and delivers a nonparametric approximation to the entire distribution of consumer welfare, robust to unobserved preference heterogeneity. We provide moment- and quantile-based estimation strategies that are simple to implement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01231v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Gauthier, Sebastiaan Maes, Raghav Malhotra</dc:creator>
    </item>
    <item>
      <title>High-Throughput Asset Pricing</title>
      <link>https://arxiv.org/abs/2311.10685</link>
      <description>arXiv:2311.10685v3 Announce Type: replace-cross 
Abstract: We apply empirical Bayes (EB) to mine data on 136,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This ``high-throughput asset pricing'' matches the out-of-sample performance of top journals while eliminating look-ahead bias. Naively mining for the largest Sharpe ratios leads to similar performance, consistent with our theoretical results, though EB uniquely provides unbiased predictions with transparent intuition. Predictability is concentrated in accounting strategies, small stocks, and pre-2004 periods, consistent with limited attention theories. Multiple testing methods popular in finance fail to identify most out-of-sample performers. High-throughput methods provide a rigorous, unbiased framework for understanding asset prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10685v3</guid>
      <category>q-fin.GN</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen, Chukwuma Dim</dc:creator>
    </item>
  </channel>
</rss>
