<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>QAMNet: Fast and Efficient Optical QAM Neural Networks</title>
      <link>https://arxiv.org/abs/2409.12305</link>
      <description>arXiv:2409.12305v1 Announce Type: new 
Abstract: The energy consumption of neural network inference has become a topic of paramount importance with the growing success and adoption of deep neural networks (DNNs). Analog optical neural networks (ONNs) can reduce the energy of matrix-vector multiplication in neural network inference below that of digital electronics. However, realizing this promise remains challenging due to digital-to-analog (DAC) conversion: even at low bit precisions $b$, encoding the $2^b$ levels of digital weights and inputs into the analog domain requires specialized and power-hungry electronics. Faced with similar challenges, the field of telecommunications has developed the complex-valued Quadrature-Amplitude Modulation (QAM), the workhorse modulation format for decades. QAM maximally exploits the complex amplitude to provide a quadratic $O(N^2) \to O(N)$ energy saving over intensity-only modulation. Inspired by this advantage, this work introduces QAMNet, an optical neural network hardware and architecture with superior energy consumption to existing ONNs, that fully utilizes the complex nature of the amplitude of light with QAM. When implemented with conventional telecommunications equipment, we show that QAMNet accelerates complex-valued deep neural networks with accuracies indistinguishable from digital hardware, based on physics-based simulations. Compared to standard ONNs, we find that QAMNet ONNs: (1) attain higher accuracy above moderate levels of total bit precision, (2) are more accurate above low energy budgets, and (3) are an optimal choice when hardware bit precision is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12305v1</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Gong Bacvanski, Sri Krishna Vadlamani, Kfir Sulimany, Dirk Robert Englund</dc:creator>
    </item>
    <item>
      <title>MPAI: A Co-Processing Architecture with MPSoC &amp; AI Accelerators for Vision Applications in Space</title>
      <link>https://arxiv.org/abs/2409.12258</link>
      <description>arXiv:2409.12258v1 Announce Type: cross 
Abstract: The emerging need for fast and power-efficient AI/ML deployment on-board spacecraft has forced the space industry to examine specialized accelerators, which have been successfully used in terrestrial applications. Towards this direction, the current work introduces a very heterogeneous co-processing architecture that is built around UltraScale+ MPSoC and its programmable DPU, as well as commercial AI/ML accelerators such as MyriadX VPU and Edge TPU. The proposed architecture, called MPAI, handles networks of different size/complexity and accommodates speed-accuracy-energy trade-offs by exploiting the diversity of accelerators in precision and computational power. This brief provides technical background and reports preliminary experimental results and outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12258v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Leon, Panagiotis Minaidis, Dimitrios Soudris, George Lentaris</dc:creator>
    </item>
    <item>
      <title>Hybrid quantum cycle generative adversarial network for small molecule generation</title>
      <link>https://arxiv.org/abs/2402.00014</link>
      <description>arXiv:2402.00014v2 Announce Type: replace-cross 
Abstract: The drug design process currently requires considerable time and resources to develop each new compound that enters the market. This work develops an application of hybrid quantum generative models based on the integration of parametrized quantum circuits into known molecular generative adversarial networks, and proposes quantum cycle architectures that improve model performance and stability during training. Through extensive experimentation on benchmark drug design datasets, QM9 and PC9, the introduced models are shown to outperform the previously achieved scores. Most prominently, the new scores indicate an increase of up to 30% in the quantitative estimation of druglikeness. The new hybrid quantum machine learning algorithms, as well as the achieved scores of pharmacokinetic properties, contribute to the development of fast and accurate drug discovery processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00014v2</guid>
      <category>q-bio.BM</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>physics.bio-ph</category>
      <category>quant-ph</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TQE.2024.3414264</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. Quantum Eng. 5, 2500514 (2024)</arxiv:journal_reference>
      <dc:creator>Matvei Anoshin, Asel Sagingalieva, Christopher Mansell, Dmitry Zhiganov, Vishal Shete, Markus Pflitsch, Alexey Melnikov</dc:creator>
    </item>
    <item>
      <title>Quantum Computing and Tensor Networks for Laminate Design: A Novel Approach to Stacking Sequence Retrieval</title>
      <link>https://arxiv.org/abs/2402.06455</link>
      <description>arXiv:2402.06455v3 Announce Type: replace-cross 
Abstract: As with many tasks in engineering, structural design frequently involves navigating complex and computationally expensive problems. A prime example is the weight optimization of laminated composite materials, which to this day remains a formidable task, due to an exponentially large configuration space and non-linear constraints. The rapidly developing field of quantum computation may offer novel approaches for addressing these intricate problems. However, before applying any quantum algorithm to a given problem, it must be translated into a form that is compatible with the underlying operations on a quantum computer. Our work specifically targets stacking sequence retrieval with lamination parameters. To adapt this problem for quantum computational methods, we map the possible stacking sequences onto a quantum state space. We further derive a linear operator, the Hamiltonian, within this state space that encapsulates the loss function inherent to the stacking sequence retrieval problem. Additionally, we demonstrate the incorporation of manufacturing constraints on stacking sequences as penalty terms in the Hamiltonian. This quantum representation is suitable for a variety of classical and quantum algorithms for finding the ground state of a quantum Hamiltonian. For a practical demonstration, we performed state-vector simulations of two variational quantum algorithms and additionally chose a classical tensor network algorithm, the DMRG algorithm, to numerically validate our approach. Although this work primarily concentrates on quantum computation, the application of tensor network algorithms presents a novel quantum-inspired approach for stacking sequence retrieval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06455v3</guid>
      <category>quant-ph</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.117380</arxiv:DOI>
      <arxiv:journal_reference>Comput. Methods Appl. Mech. Eng. 432 (2024) 117380</arxiv:journal_reference>
      <dc:creator>Arne Wulff, Boyang Chen, Matthew Steinberg, Yinglu Tang, Matthias M\"oller, Sebastian Feld</dc:creator>
    </item>
    <item>
      <title>FedQNN: Federated Learning using Quantum Neural Networks</title>
      <link>https://arxiv.org/abs/2403.10861</link>
      <description>arXiv:2403.10861v2 Announce Type: replace-cross 
Abstract: In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning. This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct datasets, proving its suitability for conducting various QML tasks. Our research not only identifies the limitations of classical paradigms but also presents a novel framework to propel the field of QML into a new era of secure and collaborative innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10861v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IJCNN60899.2024.10651123</arxiv:DOI>
      <arxiv:journal_reference>2024 International Joint Conference on Neural Networks (IJCNN), Yokohama, Japan, 2024, pp. 1-9</arxiv:journal_reference>
      <dc:creator>Nouhaila Innan, Muhammad Al-Zafar Khan, Alberto Marchisio, Muhammad Shafique, Mohamed Bennai</dc:creator>
    </item>
    <item>
      <title>ProcessTBench: An LLM Plan Generation Dataset for Process Mining</title>
      <link>https://arxiv.org/abs/2409.09191</link>
      <description>arXiv:2409.09191v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown significant promise in plan generation. Yet, existing datasets often lack the complexity needed for advanced tool use scenarios - such as handling paraphrased query statements, supporting multiple languages, and managing actions that can be done in parallel. These scenarios are crucial for evaluating the evolving capabilities of LLMs in real-world applications. Moreover, current datasets don't enable the study of LLMs from a process perspective, particularly in scenarios where understanding typical behaviors and challenges in executing the same process under different conditions or formulations is crucial. To address these gaps, we present the ProcessTBench synthetic dataset, an extension of the TaskBench dataset specifically designed to evaluate LLMs within a process mining framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09191v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Cosmin Redis, Mohammadreza Fani Sani, Bahram Zarrin, Andrea Burattin</dc:creator>
    </item>
  </channel>
</rss>
