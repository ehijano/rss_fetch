<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 02:44:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Do Data Center Network Metrics Predict Application-Facing Performance?</title>
      <link>https://arxiv.org/abs/2411.06004</link>
      <description>arXiv:2411.06004v1 Announce Type: new 
Abstract: Applications that run in large-scale data center networks (DCNs) rely on the DCN's ability to deliver application requests in a performant manner. DCNs expose a complex design and operational space, and network designers and operators care how different options along this space affect application performance. One might run controlled experiments and measure the corresponding application-facing performance, but such experiments become progressively infeasible at a large scale, and simulations risk yielding inaccurate or incomplete results. Instead, we show that we can predict application-facing performance through more easily measured network metrics. For example, network telemetry metrics (e.g., link utilization) can predict application-facing metrics (e.g., transfer latency). Through large-scale measurements of production networks, we study the correlation between the two types of metrics, and construct predictive, interpretable models that serve as a suggestive guideline to network designers and operators. We show that no single network metric is universally the best predictor (even though some prior work has focused on a single predictor). We found that simple linear models often have the lowest error, while queueing-based models are better in a few cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06004v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Chang, Jeffrey C. Mogul, Rui Wang, Mingyang Zhang, Aditya Akella</dc:creator>
    </item>
    <item>
      <title>Multi-hop RIS-aided Learning Model Sharing for Urban Air Mobility</title>
      <link>https://arxiv.org/abs/2411.06015</link>
      <description>arXiv:2411.06015v1 Announce Type: new 
Abstract: Urban Air Mobility (UAM), powered by flying cars, is poised to revolutionize urban transportation by expanding vehicle travel from the ground to the air. This advancement promises to alleviate congestion and enable faster commutes. However, the fast travel speeds mean vehicles will encounter vastly different environments during a single journey. As a result, onboard learning systems need access to extensive environmental data, leading to high costs in data collection and training. These demands conflict with the limited in-vehicle computing and battery resources. Fortunately, learning model sharing offers a solution. Well-trained local Deep Learning (DL) models can be shared with other vehicles, reducing the need for redundant data collection and training. However, this sharing process relies heavily on efficient vehicular communications in UAM. To address these challenges, this paper leverages the multi-hop Reconfigurable Intelligent Surface (RIS) technology to improve DL model sharing between distant flying cars. We also employ knowledge distillation to reduce the size of the shared DL models and enable efficient integration of non-identical models at the receiver. Our approach enhances model sharing and onboard learning performance for cars entering new environments. Simulation results show that our scheme improves the total reward by 85% compared to benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06015v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Xiong, Hanqing Yu, Supeng Leng, Chongwen Huang, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Advanced Network Planning in 6G Smart Radio Environments</title>
      <link>https://arxiv.org/abs/2411.06021</link>
      <description>arXiv:2411.06021v1 Announce Type: new 
Abstract: The growing demand for high-speed, reliable wireless connectivity in 6G networks necessitates innovative approaches to overcome the limitations of traditional Radio Access Network (RAN). Reconfigurable Intelligent Surface (RIS) and Network-Controlled Repeater (NCR) have emerged as promising technologies to address coverage challenges in high-frequency millimeter wave (mmW) bands by enhancing signal reach in environments susceptible to blockage and severe propagation losses. In this paper, we propose an optimized deployment framework aimed at minimizing infrastructure costs while ensuring full area coverage using only RIS and NCR. We formulate a cost-minimization optimization problem that integrates the deployment and configuration of these devices to achieve seamless coverage, particularly in dense urban scenarios. Simulation results confirm that this framework significantly reduces the network planning costs while guaranteeing full coverage, demonstrating RIS and NCR's viability as cost-effective solutions for next-generation network infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06021v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reza Aghazadeh Ayoubi, Marouan Mizmizi, Eugenio Moro, Ilario Filippini, Umberto Spagnolini</dc:creator>
    </item>
    <item>
      <title>A Multicast Scheme for Live Streaming Courses in Large-Scale, Geographically Dense Campus Networks</title>
      <link>https://arxiv.org/abs/2411.06334</link>
      <description>arXiv:2411.06334v1 Announce Type: new 
Abstract: Video courses have become a significant component of modern education. However, the increasing demand for live streaming video courses places considerable strain on the service capabilities of campus networks. The challenges associated with live streaming course videos in campus network environments exhibit distinct spatial distribution characteristics. The audience for specific video courses may be highly concentrated in certain areas, leading to a large number of users attempting to access the same live stream simultaneously. Utilizing a Content Delivery Network (CDN) to distribute videos in these campus scenarios creates substantial unicast pressure on edge CDN servers. This paper proposes a two-layer dynamic partitioning Recursive Bit String (RBS) virtual domain network layer multicast architecture specifically designed for large-scale, geographically dense multicast scenarios within campus networks. This approach reduces redundant multicast messages by approximately 10-30\% compared to the two-layer fixed partitioning method. Additionally, it establishes multicast source authentication capabilities based on Source Address Validation Improvement (SAVI) and facilitates secure multicast group key exchange using a concise exchange protocol within the WebRTC framework. In the next-generation data plane of programmable software-defined networks, the RBS stateless multicast technology can be integrated with the unique characteristics of large-scale, geographically dense campus network scenarios to dynamically and efficiently extend multicast coverage to every dormitory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06334v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Senxin Wu, Jinlong Hu, Ling Zhang</dc:creator>
    </item>
    <item>
      <title>On Resolving Non-Preemptivity in Multitask Scheduling: An Optimal Algorithm in Deterministic and Stochastic Worlds</title>
      <link>https://arxiv.org/abs/2411.06348</link>
      <description>arXiv:2411.06348v1 Announce Type: new 
Abstract: The efficient scheduling of multi-task jobs across multiprocessor systems has become increasingly critical with the rapid expansion of computational systems. This challenge, known as Multiprocessor Multitask Scheduling (MPMS), is essential for optimizing the performance and scalability of applications in fields such as cloud computing and deep learning. In this paper, we study the MPMS problem under both deterministic and stochastic models, where each job is composed of multiple tasks and can only be completed when all its tasks are finished. We introduce $\mathsf{NP}$-$\mathsf{SRPT}$, a non-preemptive variant of the Shortest Remaining Processing Time (SRPT) algorithm, designed to accommodate scenarios with non-preemptive tasks. Our algorithm achieves a competitive ratio of $\ln \alpha + \beta + 1$ for minimizing response time, where $\alpha$ represents the ratio of the largest to the smallest job workload, and $\beta$ captures the ratio of the largest non-preemptive task workload to the smallest job workload. We further establish that this competitive ratio is order-optimal when the number of processors is fixed. For stochastic systems modeled as M/G/N queues, where job arrivals follow a Poisson process and task workloads are drawn from a general distribution, we prove that $\mathsf{NP}$-$\mathsf{SRPT}$ achieves asymptotically optimal mean response time as the traffic intensity $\rho$ approaches $1$, assuming the task size distribution has finite support. Moreover, the asymptotic optimality extends to cases with infinite task size distributions under mild probabilistic assumptions, including the standard M/M/N model. Experimental results validate the effectiveness of $\mathsf{NP}$-$\mathsf{SRPT}$, demonstrating its asymptotic optimality in both theoretical and practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06348v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Li</dc:creator>
    </item>
    <item>
      <title>Enhancing Emergency Communication for Future Smart Cities with Random Forest Model</title>
      <link>https://arxiv.org/abs/2411.06455</link>
      <description>arXiv:2411.06455v1 Announce Type: new 
Abstract: This study aims to optimise the "spray and wait" protocol in delay tolerant networks (DTNs) to improve the performance of information transmission in emergency situations, especially in car accident scenarios. Due to the intermittent connectivity and dynamic environment of DTNs, traditional routing protocols often do not work effectively. In this study, a machine learning method called random forest was used to identify "high-quality" nodes. "High-quality" nodes refer to those with high message delivery success rates and optimal paths. The high-quality node data was filtered according to the node report of successful transmission generated by the One simulator. The node contact report generated by another One simulator was used to calculate the data of the three feature vectors required for training the model. The feature vectors and the high-quality node data were then fed into the model to train the random forest model, which was then able to identify high-quality nodes. The simulation experiment was carried out in the ONE simulator in the Helsinki city centre, with two categories of weekday and holiday scenarios, each with a different number of nodes. Three groups were set up in each category: the original unmodified group, the group with high-quality nodes, and the group with random nodes. The results show that this method of loading high-quality nodes significantly improves the performance of the protocol, increasing the success rate of information transmission and reducing latency. This study not only confirms the feasibility of using advanced machine learning techniques to improve DTN routing protocols, but also lays the foundation for future innovations in emergency communication network management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06455v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengkun Ye, Milena Radenkovic</dc:creator>
    </item>
    <item>
      <title>On-demand 5G Private Networks using a Mobile Cell</title>
      <link>https://arxiv.org/abs/2411.06597</link>
      <description>arXiv:2411.06597v1 Announce Type: new 
Abstract: This paper proposes the Mobile Cell (MC) concept for on-demand 5G private networks. The MC is designed to extend, restore, and reinforce 5G wireless coverage and network capacity on-demand, especially in areas with temporary communications needs or where it is costly or not possible to deploy a permanent fixed infrastructure. The design of the MC as well as the development, integration, and deployment in 5G private networks are discussed.
  The Mobile Cell concept can be applied in multiple real-world environments, including seaports and application scenarios. Similarly to critical hubs in the global supply chain, seaports require reliable, high-performance wireless communications to increase efficiency and manage dynamic operations in real-time. Current communications solutions in seaports typically rely on Wi-Fi and wired-based technologies. Wired-based technologies lack the necessary flexibility for dynamic environments. Wi-Fi is susceptible to interference from other systems operating in the same frequency bands. An MC operating in a licensed, interference-free spectrum is a promising solution to overcome these limitations and provide improved Quality of Service when using the 5G technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06597v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andr\'e Coelho, Jos\'e Ruela, Gon\c{c}alo Queir\'os, Ricardo Trancoso, Paulo Furtado Correia, Filipe Ribeiro, Helder Fontes, Rui Campos, Manuel Ricardo</dc:creator>
    </item>
    <item>
      <title>Loss-tolerant neural video codec aware congestion control for real time video communication</title>
      <link>https://arxiv.org/abs/2411.06742</link>
      <description>arXiv:2411.06742v1 Announce Type: new 
Abstract: Because of reinforcement learning's (RL) ability to automatically create more adaptive controlling logics beyond the hand-crafted heuristics, numerous effort has been made to apply RL to congestion control (CC) design for real time video communication (RTC) applications and has successfully shown promising benefits over the rule-based RTC CCs. Online reinforcement learning is often adopted to train the RL models so the models can directly adapt to real network environments. However, its trail-and-error manner can also cause catastrophic degradation of the quality of experience (QoE) of RTC application at run time. Thus, safeguard strategies such as falling back to hand-crafted heuristics can be used to run along with RL models to guarantee the actions explored in the training sensible, despite that these safeguard strategies interrupt the learning process and make it more challenging to discover optimal RL policies.
  The recent emergence of loss-tolerant neural video codecs (NVC) naturally provides a layer of protection for the online learning of RL-based congestion control because of its resilience to packet losses, but such packet loss resilience have not been fully exploited in prior works yet. In this paper, we present a reinforcement learning (RL) based congestion control which can be aware of and takes advantage of packet loss tolerance characteristic of NVCs via reward in online RL learning. Through extensive evaluation on various videos and network traces in a simulated environment, we demonstrate that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41\% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20\% to 77\% in comparison with other baseline RTC CCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06742v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengxu Xia, Hanchen Li, Junchen Jiang</dc:creator>
    </item>
    <item>
      <title>AI-Native Multi-Access Future Networks -- The REASON Architecture</title>
      <link>https://arxiv.org/abs/2411.06870</link>
      <description>arXiv:2411.06870v1 Announce Type: new 
Abstract: The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.
  This paper presents REASON's architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06870v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Katsaros, Ioannis Mavromatis, Kostantinos Antonakoglou, Saptarshi Ghosh, Dritan Kaleshi, Toktam Mahmoodi, Hamid Asgari, Anastasios Karousos, Iman Tavakkolnia, Hossein Safi, Harald Hass, Constantinos Vrontos, Amin Emami, Juan Parra Ullauri, Shadi Moazzeni, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>Sdn Intrusion Detection Using Machine Learning Method</title>
      <link>https://arxiv.org/abs/2411.05888</link>
      <description>arXiv:2411.05888v1 Announce Type: cross 
Abstract: Software-defined network (SDN) is a new approach that allows network control to become directly programmable, and the underlying infrastructure can be abstracted from applications and network services. Control plane). When it comes to security, the centralization that this demands is ripe for a variety of cyber threats that are not typically seen in other network architectures. The authors in this research developed a novel machine-learning method to capture infections in networks. We applied the classifier to the UNSW-NB 15 intrusion detection benchmark and trained a model with this data. Random Forest and Decision Tree are classifiers used to assess with Gradient Boosting and AdaBoost. Out of these best-performing models was Gradient Boosting with an accuracy, recall, and F1 score of 99.87%,100%, and 99.85%, respectively, which makes it reliable in the detection of intrusions for SDN networks. The second best-performing classifier was also a Random Forest with 99.38% of accuracy, followed by Ada Boost and Decision Tree. The research shows that the reason that Gradient Boosting is so effective in this task is that it combines weak learners and creates a strong ensemble model that can predict if traffic belongs to a normal or malicious one with high accuracy. This paper indicates that the GBDT-IDS model is able to improve network security significantly and has better features in terms of both real-time detection accuracy and low false positive rates. In future work, we will integrate this model into live SDN space to observe its application and scalability. This research serves as an initial base on which one can make further strides forward to enhance security in SDN using ML techniques and have more secure, resilient networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05888v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Zawad Mahmud, Shahran Rahman Alve, Samiha Islam, Mohammad Monirujjaman Khan</dc:creator>
    </item>
    <item>
      <title>Personalized Hierarchical Split Federated Learning in Wireless Networks</title>
      <link>https://arxiv.org/abs/2411.06042</link>
      <description>arXiv:2411.06042v1 Announce Type: cross 
Abstract: Extreme resource constraints make large-scale machine learning (ML) with distributed clients challenging in wireless networks. On the one hand, large-scale ML requires massive information exchange between clients and server(s). On the other hand, these clients have limited battery and computation powers that are often dedicated to operational computations. Split federated learning (SFL) is emerging as a potential solution to mitigate these challenges, by splitting the ML model into client-side and server-side model blocks, where only the client-side block is trained on the client device. However, practical applications require personalized models that are suitable for the client's personal task. Motivated by this, we propose a personalized hierarchical split federated learning (PHSFL) algorithm that is specially designed to achieve better personalization performance. More specially, owing to the fact that regardless of the severity of the statistical data distributions across the clients, many of the features have similar attributes, we only train the body part of the federated learning (FL) model while keeping the (randomly initialized) classifier frozen during the training phase. We first perform extensive theoretical analysis to understand the impact of model splitting and hierarchical model aggregations on the global model. Once the global model is trained, we fine-tune each client classifier to obtain the personalized models. Our empirical findings suggest that while the globally trained model with the untrained classifier performs quite similarly to other existing solutions, the fine-tuned models show significantly improved personalized performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06042v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md-Ferdous Pervej, Andreas F. Molisch</dc:creator>
    </item>
    <item>
      <title>Hermes: A Large Language Model Framework on the Journey to Autonomous Networks</title>
      <link>https://arxiv.org/abs/2411.06490</link>
      <description>arXiv:2411.06490v1 Announce Type: cross 
Abstract: The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or "telecommunications brain", is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses "blueprints" for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06490v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadhel Ayed, Ali Maatouk, Nicola Piovesan, Antonio De Domenico, Merouane Debbah, Zhi-Quan Luo</dc:creator>
    </item>
    <item>
      <title>A neural-network based anomaly detection system and a safety protocol to protect vehicular network</title>
      <link>https://arxiv.org/abs/2411.07013</link>
      <description>arXiv:2411.07013v1 Announce Type: cross 
Abstract: This thesis addresses the use of Cooperative Intelligent Transport Systems (CITS) to improve road safety and efficiency by enabling vehicle-to-vehicle communication, highlighting the importance of secure and accurate data exchange. To ensure safety, the thesis proposes a Machine Learning-based Misbehavior Detection System (MDS) using Long Short-Term Memory (LSTM) networks to detect and mitigate incorrect or misleading messages within vehicular networks. Trained offline on the VeReMi dataset, the detection model is tested in real-time within a platooning scenario, demonstrating that it can prevent nearly all accidents caused by misbehavior by triggering a defense protocol that dissolves the platoon if anomalies are detected. The results show that while the system can accurately detect general misbehavior, it struggles to label specific types due to varying traffic conditions, implying the difficulty of creating a universally adaptive protocol. However, the thesis suggests that with more data and further refinement, this MDS could be implemented in real-world CITS, enhancing driving safety by mitigating risks from misbehavior in cooperative driving networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07013v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Franceschini</dc:creator>
    </item>
    <item>
      <title>Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network</title>
      <link>https://arxiv.org/abs/2411.07168</link>
      <description>arXiv:2411.07168v1 Announce Type: cross 
Abstract: Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\% classification accuracy, while cloud-based inference reached 99\%. On-sensor inference reduced power consumption by approximately 44\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07168v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ra\'ul de la Fuente, Luciano Radrigan, Anibal S Morales</dc:creator>
    </item>
    <item>
      <title>Joint Age-State Belief is All You Need: Minimizing AoII via Pull-Based Remote Estimation</title>
      <link>https://arxiv.org/abs/2411.07179</link>
      <description>arXiv:2411.07179v1 Announce Type: cross 
Abstract: Age of incorrect information (AoII) is a recently proposed freshness and mismatch metric that penalizes an incorrect estimation along with its duration. Therefore, keeping track of AoII requires the knowledge of both the source and estimation processes. In this paper, we consider a time-slotted pull-based remote estimation system under a sampling rate constraint where the information source is a general discrete-time Markov chain (DTMC) process. Moreover, packet transmission times from the source to the monitor are non-zero which disallows the monitor to have perfect information on the actual AoII process at any time. Hence, for this pull-based system, we propose the monitor to maintain a sufficient statistic called {\em belief} which stands for the joint distribution of the age and source processes to be obtained from the history of all observations. Using belief, we first propose a maximum a posteriori (MAP) estimator to be used at the monitor as opposed to existing martingale estimators in the literature. Second, we obtain the optimality equations from the belief-MDP (Markov decision process) formulation. Finally, we propose two belief-dependent policies one of which is based on deep reinforcement learning, and the other one is a threshold-based policy based on the instantaneous expected AoII.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07179v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ismail Cosandal, Sennur Ulukus, Nail Akar</dc:creator>
    </item>
    <item>
      <title>Toward Standardized Performance Evaluation of Flow-guided Nanoscale Localization</title>
      <link>https://arxiv.org/abs/2303.07804</link>
      <description>arXiv:2303.07804v3 Announce Type: replace 
Abstract: Nanoscale devices with Terahertz (THz) communication capabilities are envisioned to be deployed within human bloodstreams. Such devices will enable fine-grained sensing-based applications for detecting early indications (i.e., biomarkers) of various health conditions, as well as actuation-based ones such as targeted drug delivery. Associating the locations of such events with the events themselves would provide an additional utility for precision diagnostics and treatment. This vision yielded a new class of in-body localization coined under the term "flow-guided nanoscale localization". Such localization can be piggybacked on THz communication for detecting body regions in which biological events were observed based on the duration of one circulation of a nanodevice in the bloodstream. From a decades-long research on objective benchmarking of "traditional" indoor localization, as well as its eventual standardization (e.g., ISO/IEC 18305:2016), we know that in early stages the reported performance results were often incomplete (e.g., targeting a subset of relevant performance metrics), carrying out benchmarking experiments in different evaluation environments and scenarios, and utilizing inconsistent performance indicators. To avoid such a "lock-in" in flow-guided localization, in this paper we propose a workflow for standardized performance evaluation of such localization. The workflow is implemented in the form of an open-source simulation framework that is able to jointly account for the mobility of the nanodevices, in-body THz communication between with on-body anchors, and energy-related and other technological constraints (e.g., pulse-based modulation) at the nanodevice level. Accounting for these constraints, the framework is able to generate the raw data that can be streamlined into different flow-guided localization solutions for generating standardized performance benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07804v3</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnau Brosa L\'opez, Filip Lemic, Jakob Struye, Jorge Torres G\'omez, Esteban Municio, Carmen Delgado, Gerard Calvo Bartra, Falko Dressler, Eduard Alarc\'on, Jeroen Famaey, Sergi Abadal, Xavier Costa P\'erez</dc:creator>
    </item>
    <item>
      <title>Virtuoso: High Resource Utilization and {\mu}s-scale Performance Isolation in a Shared Virtual Machine TCP Network Stack</title>
      <link>https://arxiv.org/abs/2309.14016</link>
      <description>arXiv:2309.14016v3 Announce Type: replace 
Abstract: Virtualization improves resource efficiency and ensures security and performance isolation for cloud applications. Today, operators use a layered architecture with separate network stack instances in each VM and container connected to a virtual switch. Decoupling through layering reduces complexity, but induces performance and resource overheads at odds with increasing demands for network bandwidth, connection scalability, and low latency.
  We present Virtuoso, a new software network stack for VMs and containers. Virtuoso re-organizes the network stack to maximize CPU utilization, enforce isolation, and minimize processing overheads. We maximize utilization by running one elastically shared network stack instance on dedicated cores; we enforce isolation by performing central and fine-grained per-packet resource accounting and scheduling; we reduce overheads by building a single-layer data path with a one-shot fast-path incorporating all processing from the TCP transport layer through network virtualization and virtual switching. Virtuoso improves resource efficiency by up to 82%, latencies by up to 58% compared to other virtualized network stacks without sacrificing isolation, and keeps processing overhead within 6.7% of unvirtualized stacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14016v3</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matheus Stolet, Liam Arzola, Simon Peter, Antoine Kaufmann</dc:creator>
    </item>
    <item>
      <title>Sustainable business decision modelling with blockchain and digital twins: A survey</title>
      <link>https://arxiv.org/abs/2405.12101</link>
      <description>arXiv:2405.12101v2 Announce Type: replace 
Abstract: Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12101v2</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyan Wickremasinghe, Siofra Frost, Karen Rafferty, Vishal Sharma</dc:creator>
    </item>
    <item>
      <title>Connection Performance Modeling and Analysis of a Radiosonde Network in a Typhoon</title>
      <link>https://arxiv.org/abs/2411.01906</link>
      <description>arXiv:2411.01906v3 Announce Type: replace 
Abstract: This paper is concerned with the theoretical modeling and analysis of uplink connection performance of a radiosonde network deployed in a typhoon. Similar to existing works, the stochastic geometry theory is leveraged to derive the expression of the uplink connection probability (CP) of a radiosonde. Nevertheless, existing works assume that network nodes are spherically or uniformly distributed. Different from the existing works, this paper investigates two particular motion patterns of radiosondes in a typhoon, which significantly challenges the theoretical analysis. According to their particular motion patterns, this paper first separately models the distributions of horizontal and vertical distances from a radiosonde to its receiver. Secondly, this paper derives the closed-form expressions of cumulative distribution function (CDF) and probability density function (PDF) of a radiosonde's three-dimensional (3D) propagation distance to its receiver. Thirdly, this paper derives the analytical expression of the uplink CP for any radiosonde in the network. Finally, extensive numerical simulations are conducted to validate the theoretical analysis, and the influence of various network design parameters are comprehensively discussed. Simulation results show that when the signal-to-interference-noise ratio (SINR) threshold is below -35 dB, and the density of radiosondes remains under 0.01/km^3, the uplink CP approaches 26%, 39%, and 50% in three patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01906v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyi Liu, Xianbin Cao, Peng Yang, Zehui Xiong, Tony Q. S. Quek, Dapeng Oliver Wu</dc:creator>
    </item>
    <item>
      <title>Orchestration Framework for Open System Models with Autonomous RISs and Oblivious Base Stations</title>
      <link>https://arxiv.org/abs/2304.10858</link>
      <description>arXiv:2304.10858v4 Announce Type: replace-cross 
Abstract: Autonomous reconfigurable intelligent surface (RIS) offers the potential to simplify deployment by reducing the need for real-time remote control between a base station (BS) and an RIS. However, we highlight two major challenges posed by autonomy. The first is implementation complexity, as autonomy requires hybrid RISs (HRISs) equipped with additional on-board hardware to monitor the propagation environment and conduct local channel estimation (CHEST), a process known as probing. The second challenge, termed probe distortion, reflects a form of the observer effect: during probing, an HRIS can inadvertently alter the propagation environment, potentially disrupting the operations of other communicating devices. While implementation complexity has been extensively studied, probe distortion remains largely unexplored. To further assess the potential of autonomous RISs, this paper comprehensively and pragmatically studies fundamental trade-offs posed by these challenges. We examine the robustness of an HRIS-assisted massive multiple-input multiple-output (mMIMO) system under minimal design choices that reflect the essential elements and stringent conditions, including (a) two extremes of implementation complexity realized through minimalist operational designs of two HRIS hardware architectures, and (b) an oblivious BS that fully embraces probe distortion. To make our analysis possible, we propose a physical-layer orchestration framework that aligns HRIS and mMIMO operations. We provide empirical evidence showing that autonomous RIS holds promise even under these strict conditions and propose new research directions, particularly for advancing the understanding of probe distortion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10858v4</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Croisfelt, Francesco Devoti, Fabio Saggese, Vincenzo Sciancalepore, Xavier Costa-P\'erez, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>F-KANs: Federated Kolmogorov-Arnold Networks</title>
      <link>https://arxiv.org/abs/2407.20100</link>
      <description>arXiv:2407.20100v3 Announce Type: replace-cross 
Abstract: In this paper, we present an innovative federated learning (FL) approach that utilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By utilizing the adaptive activation capabilities of KANs in a federated framework, we aim to improve classification capabilities while preserving privacy. The study evaluates the performance of federated KANs (F- KANs) compared to traditional Multi-Layer Perceptrons (MLPs) on classification task. The results show that the F-KANs model significantly outperforms the federated MLP model in terms of accuracy, precision, recall, F1 score and stability, and achieves better performance, paving the way for more efficient and privacy-preserving predictive analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20100v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Engin Zeydan, Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus, Abdullah Aydeger</dc:creator>
    </item>
    <item>
      <title>LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks</title>
      <link>https://arxiv.org/abs/2411.03354</link>
      <description>arXiv:2411.03354v2 Announce Type: replace-cross 
Abstract: In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100\% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6\% in both classification accuracy and recall.The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03354v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederic Adjewa, Moez Esseghir, Leila Merghem-Boulahia</dc:creator>
    </item>
  </channel>
</rss>
