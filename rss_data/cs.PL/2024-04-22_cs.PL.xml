<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Encoding Petri Nets into CCS (Technical Report)</title>
      <link>https://arxiv.org/abs/2404.14385</link>
      <description>arXiv:2404.14385v1 Announce Type: new 
Abstract: This paper explores the problem of determining which classes of Petri nets can be encoded into behaviourally-equivalent CCS processes. Most of the existing related literature focuses on the inverse problem (i.e., encoding process calculi belonging to the CCS family into Petri nets), or extends CCS with Petri net-like multi-synchronisation (Multi-CCS). In this work, our main focus are free-choice and workflow nets (which are widely used in process mining to describe system interactions) and our target is plain CCS. We present several novel encodings, including one from free-choice workflow nets (produced by process mining algorithms like the alpha-miner) into CCS processes, and we prove that our encodings produce CCS processes that are weakly bisimilar to the original net. Besides contributing new expressiveness results, our encodings open a door towards bringing analysis and verification techniques from the realm of process calculi into the realm of process mining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14385v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Bog{\o}, Andrea Burattin, Alceste Scalas</dc:creator>
    </item>
    <item>
      <title>Declarative Concurrent Data Structures</title>
      <link>https://arxiv.org/abs/2404.13359</link>
      <description>arXiv:2404.13359v1 Announce Type: cross 
Abstract: Implementing concurrent data structures is challenging and requires a deep understanding of concurrency concepts and careful design to ensure correctness, performance, and scalability. Further, composing operations on two or more concurrent data structures often requires a synchronization wrapper to ensure the operations are applied together atomically, resulting in serialization and, thereby, giving up the performance benefit of the individual data structures. DBMS provides generalized concurrency control (CC) and is a good fit for implementing concurrent data structures. However, DBMSs are over-generalized for this use case, which fails to match the performance of specialized implementations.
  This paper makes the case for the Declarative Concurrent Data Structures (DCDS) framework for automatically generating concurrent data structures from a serial specification. In DCDS, users declare the attributes and methods needed for their desired data structure through an embedded DSL at design time. DCDS automatically injects CC at build-time, generating a concurrent intermediate representation (IR) compiled into machine code. A declarative interface for designing data structure enables efficient composability through co-optimizing component structures; optimizations are applied to both the composed serial specification and the generated concurrent IR. We realize the DCDS framework in our prototype system Rosti and experimentally show that data structures declared in Rosti can be efficiently composed by co-optimizing their logical functionality and the generated CC protocol. Our evaluation shows that composing a map and a list to create an LRU container can benefit up to 2X performance scalability in Rosti compared to an open-source library. We demonstrate the applicability of DCDS as an in-process OLTP by comparing it with in-memory DBMS, Proteus, and showing up to 2X performance gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13359v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aun Raza, Hamish Nicholson, Ioanna Tsakalidou, Anna Herlihy, Prathamesh Tagore, Anastasia Ailamaki</dc:creator>
    </item>
    <item>
      <title>Proceedings 18th International Workshop on Logical and Semantic Frameworks, with Applications and 10th Workshop on Horn Clauses for Verification and Synthesis</title>
      <link>https://arxiv.org/abs/2404.13672</link>
      <description>arXiv:2404.13672v1 Announce Type: cross 
Abstract: This volume contains
  * The post-proceedings of the Eighteenth Logical and Semantic Frameworks with Applications (LSFA 2023). The meeting was held on July 1-2, 2023, organised by the Sapienza Universit\`a di Roma, Italy. LSFA aims to bring researchers and students interested in theoretical and practical aspects of logical and semantic frameworks and their applications. The covered topics include proof theory, type theory and rewriting theory, specification and deduction languages, and formal semantics of languages and systems.
  * The post-proceedings of the Tenth Workshop on Horn clauses for Verification and Synthesis (HCVS 2023). The meeting was held on April 23, 2023 at the Institut Henri Poincar\'e in Paris. HCVS aims to bring together researchers working in the two communities of constraint/ logic programming (e.g., ICLP and CP), program verification (e.g., CAV, TACAS, and VMCAI), and automated deduction (e.g., CADE, IJCAR), on the topics of Horn clause based analysis, verification, and synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13672v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.402</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 402, 2024</arxiv:journal_reference>
      <dc:creator>Temur Kutsia (RISC, Johannes Kepler University Linz), Daniel Ventura (INF, Universidade Federal de Goi\'as), David Monniaux (CNRS - Verimag), Jos\'e F. Morales (IMDEA)</dc:creator>
    </item>
    <item>
      <title>Error Credits: Resourceful Reasoning about Error Bounds for Higher-Order Probabilistic Programs</title>
      <link>https://arxiv.org/abs/2404.14223</link>
      <description>arXiv:2404.14223v1 Announce Type: cross 
Abstract: Probabilistic programs often trade accuracy for efficiency, and are thus only approximately correct. It is important to obtain precise error bounds for these approximations, but existing approaches rely on simplifications that make the error bounds excesively coarse, or only apply to first-order programs. In this paper we present Eris, a higher-order separation logic for probabilistic programs written in an expressive higher-order language.
  Our key novelty is the introduction of error credits, a separation logic resource that tracks the error bound of a program. By representing error bounds as a resource, we recover the benefits of separation logic, including compositionality, modularity, and dependency between errors and program terms, allowing for more precise specifications. Moreover, we enable novel reasoning principles such as expectation-preserving error composition, amortized error reasoning, and proving almost-sure termination by induction on the error.
  We illustrate the advantages of our approach by proving amortized error bounds on a range of examples, including collision probabilities in hash functions, which allows us to write more modular specifications for data structures that use them as clients. We also use our logic to prove correctness and almost-sure termination of rejection sampling algorithms. All of our results have been mechanized in the Coq proof assistant using the Iris separation logic framework and the Coquelicot real analysis library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14223v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Aguirre, Philipp G. Haselwarter, Markus de Medeiros, Kwing Hei Li, Simon Oddershede Gregersen, Joseph Tassarotti, Lars Birkedal</dc:creator>
    </item>
    <item>
      <title>Hybrid Intersection Types for PCF (Extended Version)</title>
      <link>https://arxiv.org/abs/2404.14340</link>
      <description>arXiv:2404.14340v1 Announce Type: cross 
Abstract: Intersection type systems have been independently applied to different evaluation strategies, such as call-by-name (CBN) and call-by-value (CBV). These type systems have been then generalized to different subsuming paradigms being able, in particular, to encode CBN and CBV in a unique unifying framework. However, there are no intersection type systems that explicitly enable CBN and CBV to cohabit together without making use of an encoding into a common target framework. This work proposes an intersection type system for PCF with a specific notion of evaluation, called PCFH. Evaluation in PCFH actually has a hybrid nature, in the sense that CBN and CBV operational behaviors cohabit together. Indeed, PCFH combines a CBV-like operational behavior for function application with a CBN-like behavior for recursion. This hybrid nature is reflected in the type system, which turns out to be sound and complete with respect to PCFH: not only typability implies normalization, but also the converse holds. Moreover, the type system is quantitative, in the sense that the size of typing derivations provides upper bounds for the length of the reduction sequences to normal form. This type system is then refined to a tight one, offering exact information regarding the length of normalization sequences. This is the first time that a sound and complete quantitative type system has been designed for a hybrid computational model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14340v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Barenbaum, Delia Kesner, Mariana Milicich</dc:creator>
    </item>
    <item>
      <title>Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation</title>
      <link>https://arxiv.org/abs/2404.14370</link>
      <description>arXiv:2404.14370v1 Announce Type: cross 
Abstract: The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14370v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G\'abor Antal, Rich\'ard Voz\'ar, Rudolf Ferenc</dc:creator>
    </item>
    <item>
      <title>Ozone: Fully Out-of-Order Choreographies</title>
      <link>https://arxiv.org/abs/2401.17403</link>
      <description>arXiv:2401.17403v2 Announce Type: replace 
Abstract: Choreographic programming is a paradigm for writing distributed applications. It allows programmers to write a single program, called a choreography, that can be compiled to generate correct implementations of each process in the application. Although choreographies provide good static guarantees, they can exhibit high latency when messages or processes are delayed. This is because processes in a choreography typically execute in a fixed, deterministic order, and cannot adapt to the order that messages arrive at runtime. In non-choreographic code, programmers can address this problem by allowing processes to execute out of order -- for instance by using futures or reactive programming. However, in choreographic code, out-of-order process execution can lead to serious and subtle bugs, called communication integrity violations (CIVs).
  In this paper, we develop a model of choreographic programming for out-of-order processes that guarantees absence of CIVs and deadlocks. As an application of our approach, we also introduce an API for safe non-blocking communication via futures in the choreographic programming language Choral. The API allows processes to execute out of order, participate in multiple choreographies concurrently, and to handle unordered data messages. We provide an illustrative evaluation of our API, showing that out-of-order execution can reduce latency and increase throughput by overlapping communication with computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17403v2</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Plyukhin, Marco Peressotti, Fabrizio Montesi</dc:creator>
    </item>
    <item>
      <title>AuDaLa is Turing Complete</title>
      <link>https://arxiv.org/abs/2404.12934</link>
      <description>arXiv:2404.12934v2 Announce Type: replace 
Abstract: AuDaLa is a recently introduced programming language that follows the new data autonomous paradigm. In this paradigm, small pieces of data execute functions autonomously. Considering the paradigm and the design choices of AuDaLa, it is interesting to determine the expressiveness of the language and to create verification methods for it. In this paper, we take our first steps to such a verification method by implementing Turing machines in AuDaLa and proving that implementation correct. This also proves that AuDaLa is Turing complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12934v2</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom T. P. Franken, Thomas Neele</dc:creator>
    </item>
  </channel>
</rss>
