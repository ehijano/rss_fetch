<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>sTiles: An Accelerated Computational Framework for Sparse Factorizations of Structured Matrices</title>
      <link>https://arxiv.org/abs/2501.02483</link>
      <description>arXiv:2501.02483v1 Announce Type: new 
Abstract: This paper introduces sTiles, a GPU-accelerated framework for factorizing sparse structured symmetric matrices. By leveraging tile algorithms for fine-grained computations, sTiles uses a structure-aware task execution flow to handle challenging arrowhead sparse matrices with variable bandwidths, common in scientific and engineering fields. It minimizes fill-in during Cholesky factorization using permutation techniques and employs a static scheduler to manage tasks on shared-memory systems with GPU accelerators. sTiles balances tile size and parallelism, where larger tiles enhance algorithmic intensity but increase floating-point operations and memory usage, while parallelism is constrained by the arrowhead structure. To expose more parallelism, a left-looking Cholesky variant breaks sequential dependencies in trailing submatrix updates via tree reductions. Evaluations show sTiles achieves speedups of up to 8.41X, 9.34X, 5.07X, and 11.08X compared to CHOLMOD, SymPACK, MUMPS, and PARDISO, respectively, and a 5X speedup compared to a 32-core AMD EPYC CPU on an NVIDIA A100 GPU. Our generic software framework imports well-established concepts from dense matrix computations but they all require customizations in their deployments on hybrid architectures to best handle factorizations of sparse matrices with arrowhead structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02483v1</guid>
      <category>cs.PF</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esmail Abdul Fattah, Hatem Ltaief, Havard Rue, David Keyes</dc:creator>
    </item>
    <item>
      <title>Latency and Privacy-Aware Resource Allocation in Vehicular Edge Computing</title>
      <link>https://arxiv.org/abs/2501.02804</link>
      <description>arXiv:2501.02804v1 Announce Type: new 
Abstract: The rapid increase in the number of connected vehicles has led to the generation of vast amounts of data. As a significant portion of this data pertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it is predominantly generated at the edge. Considering the enormous volume of data, real-time applications, and privacy concerns, it is crucial to process the data at the edge. Neglecting the management of processing resources in vehicular edge computing (VEC) could lead to numerous challenges as a substantial number of vehicles with diverse safety, economic, and entertainment applications, along with their data processing, emerge in the near future [1]. Previous research in VEC resource allocation has primarily focused on issues such as response time and privacy preservation techniques. However, an approach that takes into account privacy-aware resource allocation based on vehicular network architecture and application requirements has not yet been proposed. In this paper, we present a privacy and latency-aware approach for allocating processing resources at the edge of the vehicular network, considering the specific requirements of different applications. Our approach involves categorizing vehicular network applications based on their processing accuracy, real-time processing needs, and privacy preservation requirements. We further divide the vehicular network edge into two parts: the user layer (OBUs) is considered for processing applications with privacy requirements, while the allocation of resources in the RSUs and cloud layer is based on the specific needs of different applications. In this study, we evaluate the quality of service based on parameters such as privacy preservation, processing cost, meeting deadlines, and result quality. Comparative analyses demonstrate that our approach enhances service quality by 55% compared to existing state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02804v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>HHossein Ahmadvand, Fouzhan Foroutan</dc:creator>
    </item>
    <item>
      <title>The Race to Efficiency: A New Perspective on AI Scaling Laws</title>
      <link>https://arxiv.org/abs/2501.02156</link>
      <description>arXiv:2501.02156v1 Announce Type: cross 
Abstract: As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the "efficiency-doubling rate" parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02156v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chien-Ping Lu</dc:creator>
    </item>
    <item>
      <title>Automated MPI-X code generation for scalable finite-difference solvers</title>
      <link>https://arxiv.org/abs/2312.13094</link>
      <description>arXiv:2312.13094v5 Announce Type: replace-cross 
Abstract: Partial differential equations (PDEs) are crucial in modeling diverse phenomena across scientific disciplines, including seismic and medical imaging, computational fluid dynamics, image processing, and neural networks. Solving these PDEs at scale is an intricate and time-intensive process that demands careful tuning. This paper introduces automated code-generation techniques specifically tailored for distributed memory parallelism (DMP) to execute explicit finite-difference (FD) stencils at scale, a fundamental challenge in numerous scientific applications. These techniques are implemented and integrated into the Devito DSL and compiler framework, a well-established solution for automating the generation of FD solvers based on a high-level symbolic math input. Users benefit from modeling simulations for real-world applications at a high-level symbolic abstraction and effortlessly harnessing HPC-ready distributed-memory parallelism without altering their source code. This results in drastic reductions both in execution time and developer effort. A comprehensive performance evaluation of Devito's DMP via MPI demonstrates highly competitive strong and weak scaling on CPU and GPU clusters, proving its effectiveness and capability to meet the demands of large-scale scientific simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13094v5</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Bisbas, Rhodri Nelson, Mathias Louboutin, Fabio Luporini, Paul H. J. Kelly, Gerard Gorman</dc:creator>
    </item>
  </channel>
</rss>
