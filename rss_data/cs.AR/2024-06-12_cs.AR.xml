<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jun 2024 01:47:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pragmatic Formal Verification Methodology for Clock Domain Crossing (CDC)</title>
      <link>https://arxiv.org/abs/2406.06533</link>
      <description>arXiv:2406.06533v1 Announce Type: new 
Abstract: Modern System-on-Chip (SoC) designs are becoming more and more complex due to the technology upscaling. SoC designs often operate on multiple asynchronous clock domains, further adding to the complexity of the overall design. To make the devices power efficient, designers take a Globally-Asynchronous Locally-Synchronous (GALS) approach that creates multiple asynchronous domains. These Clock Domain Crossings (CDC) are prone to metastability effects, and functional verification of such CDC is very important to ensure that no bug escapes. Conventional verification methods, such as register transfer level (RTL) simulations and static timing analysis, are not enough to address these CDC issues, which may lead to verification gaps. Additionally, identifying these CDC-related bugs is very time-consuming and is one of the most common reasons for costly silicon re-spins. This paper is focused on the development of a pragmatic formal verification methodology to minimize the CDC issues by exercising Metastability Injection (MSI) in different CDC paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06533v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Kumar, Muhammad Ul Haque Khan, Bijitendra Mittra</dc:creator>
    </item>
    <item>
      <title>Apparate: Evading Memory Hierarchy with GodSpeed Wireless-on-Chip</title>
      <link>https://arxiv.org/abs/2406.06536</link>
      <description>arXiv:2406.06536v1 Announce Type: new 
Abstract: The rapid advancements in memory systems, CPU technology, and emerging technologies herald a transformative potential in computing, promising to revolutionize memory hierarchies. Innovations in DDR memory are delivering unprecedented bandwidth, while advancements in on-chip wireless technology are reducing size and increasing speed. The introduction of godspeed wireless transceivers on chip, alongside near high-speed DRAM, is poised to directly facilitate memory requests. This integration suggests the potential for eliminating traditional memory hierarchies, offering a new paradigm in computing efficiency and speed. These developments indicate a near-future where computing systems are significantly more responsive and powerful, leveraging direct, high-speed memory access mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06536v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nitesh Narayana GS, Abhijit Das</dc:creator>
    </item>
    <item>
      <title>Global and Local Attention-based Inception U-Net for Static IR Drop Estimation</title>
      <link>https://arxiv.org/abs/2406.06541</link>
      <description>arXiv:2406.06541v1 Announce Type: new 
Abstract: Static IR drop analysis is a fundamental and critical task in chip design since the IR drop will significantly affect the design's functionality, performance, and reliability. However, the process of IR drop analysis can be time-consuming, potentially taking several hours. Furthermore, in the process of fixing violations, it is frequently imperative to do IR drop analysis iteratively, hence exacerbating the computational burden associated with the analysis. Therefore, a fast and accurate IR drop prediction is paramount for reducing the overall time invested in chip design. In this paper, we propose a global and local attention-based Inception U-Net for static IR drop estimation. Our U-Net incorporates components from the Transformer, CBAM, and Inception architectures to enhance its feature capture capability at different scales and improve the accuracy of predicted IR drop. Experimental results demonstrate that our proposed algorithm can achieve the best results among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06541v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilu Chen, Zhijie Cai, Min Wei, Zhifeng Lin, Jianli Chen</dc:creator>
    </item>
    <item>
      <title>vMCU: Coordinated Memory Management and Kernel Optimization for DNN Inference on MCUs</title>
      <link>https://arxiv.org/abs/2406.06542</link>
      <description>arXiv:2406.06542v1 Announce Type: new 
Abstract: IoT devices based on microcontroller units (MCU) provide ultra-low power consumption and ubiquitous computation for near-sensor deep learning models (DNN). However, the memory of MCU is usually 2-3 orders of magnitude smaller than mobile devices, which makes it challenging to map DNNs onto MCUs. Previous work separates memory management and kernel implementation for MCU and relies on coarse-grained memory management techniques such as inplace update to reduce memory consumption.
  In this paper, we propose to coordinate memory management and kernel optimization for DNN inference on MCUs to enable fine-grained memory management. The key idea is to virtualize the limited memory of MCU as a large memory pool. Each kernel divides the memory pool into kernel-specific segments and handles segment load and store while computing DNN layers. Memory consumption can be reduced because using the fine-grained segment-level memory control, we can overlap the memory footprint of different tensors without the need to materialize them at the same time. Following this idea, we implement \ours{} for DNN inference on MCU. Evaluation for single layers on ARM Cortex-M4 and Cortex-M7 processors shows that \ours{} can reduce from $12.0\%$ to $49.5\%$ RAM usage and from $20.6\%$ to $53.0\%$ energy consumption compared to state-of-the-art work. For full DNN evaluation, \ours{} can reduce the memory bottleneck by $61.5\%$, enabling more models to be deployed on low-end MCUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06542v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Size Zheng, Renze Chen, Meng Li, Zihao Ye, Luis Ceze, Yun Liang</dc:creator>
    </item>
    <item>
      <title>SparrowSNN: A Hardware/software Co-design for Energy Efficient ECG Classification</title>
      <link>https://arxiv.org/abs/2406.06543</link>
      <description>arXiv:2406.06543v1 Announce Type: new 
Abstract: Heart disease is one of the leading causes of death worldwide. Given its high risk and often asymptomatic nature, real-time continuous monitoring is essential. Unlike traditional artificial neural networks (ANNs), spiking neural networks (SNNs) are well-known for their energy efficiency, making them ideal for wearable devices and energy-constrained edge computing platforms. However, current energy measurement of SNN implementations for detecting heart diseases typically rely on empirical values, often overlooking hardware overhead. Additionally, the integer and fire activations in SNNs require multiple memory accesses and repeated computations, which can further compromise energy efficiency. In this paper, we propose sparrowSNN, a redesign of the standard SNN workflow from a hardware perspective, and present a dedicated ASIC design for SNNs, optimized for ultra-low power wearable devices used in heartbeat classification. Using the MIT-BIH dataset, our SNN achieves a state-of-the-art accuracy of 98.29% for SNNs, with energy consumption of 31.39nJ per inference and power usage of 6.1uW, making sparrowSNN the highest accuracy with the lowest energy use among comparable systems. We also compare the energy-to-accuracy trade-offs between SNNs and quantized ANNs, offering recommendations on insights on how best to use SNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06543v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanglu Yan, Zhenyu Bai, Tulika Mitra, Weng-Fai Wong</dc:creator>
    </item>
    <item>
      <title>TSB: Tiny Shared Block for Efficient DNN Deployment on NVCIM Accelerators</title>
      <link>https://arxiv.org/abs/2406.06544</link>
      <description>arXiv:2406.06544v1 Announce Type: new 
Abstract: Compute-in-memory (CIM) accelerators using non-volatile memory (NVM) devices offer promising solutions for energy-efficient and low-latency Deep Neural Network (DNN) inference execution. However, practical deployment is often hindered by the challenge of dealing with the massive amount of model weight parameters impacted by the inherent device variations within non-volatile computing-in-memory (NVCIM) accelerators. This issue significantly offsets their advantages by increasing training overhead, the time needed for mapping weights to device states, energy consumption, and diminishing inference accuracy. To mitigate these challenges, we propose the "Tiny Shared Block (TSB)" method, which integrates a small shared 1x1 convolution block into the DNN architecture. This block is designed to stabilize feature processing across the network, effectively reducing the impact of device variation. Extensive experimental results show that TSB achieves over 20x inference accuracy gap improvement, over 5x training speedup, and weights-to-device mapping cost reduction while requiring less than 0.4% of the original weights to be write-verified during programming, when compared with state-of-the-art baseline solutions. Our approach provides a practical and efficient solution for deploying robust DNN models on NVCIM accelerators, making it a valuable contribution to the field of energy-efficient AI hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06544v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Qin, Zheyu Yan, Zixuan Pan, Wujie Wen, Xiaobo Sharon Hu, Yiyu Shi</dc:creator>
    </item>
    <item>
      <title>SentryCore: A RISC-V Co-Processor System for Safe, Real-Time Control Applications</title>
      <link>https://arxiv.org/abs/2406.06546</link>
      <description>arXiv:2406.06546v1 Announce Type: new 
Abstract: In the last decade, we have witnessed exponential growth in the complexity of control systems for safety-critical applications (automotive, robots, industrial automation) and their transition to heterogeneous mixed-criticality systems (MCSs). The growth of the RISC-V ecosystem is creating a major opportunity to develop open-source, vendor-neutral reference platforms for safety-critical computing. We present SentryCore, a reliable, real-time, self-contained, open-source mega-IP for advanced control functions that can be seamlessly integrated into Systems-on-Chip, e.g., for automotive applications, through industry-standard Advanced eXtensible Interface 4 (AXI4). SentryCore features three embedded RISC-V processor cores in lockstep with error-correcting code (ECC) protected data memory for reliable execution of any safety-critical application. Context switching is accelerated to under 110 clock cycles via a RISC-V core-local interrupt controller (CLIC) and dedicated hardware extensions, while a timer-based direct memory access (DMA) engine streamlines sensor data readout during periodic control loops. SentryCore was implemented in Intel's 16nm process node and tested with FreeRTOS, ThreadX, and RTIC software support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06546v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Michael Rogenmoser, Alessandro Ottaviano, Thomas Benz, Robert Balas, Matteo Perotti, Angelo Garofalo, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Large Language Model (LLM) for Standard Cell Layout Design Optimization</title>
      <link>https://arxiv.org/abs/2406.06549</link>
      <description>arXiv:2406.06549v1 Announce Type: new 
Abstract: Standard cells are essential components of modern digital circuit designs. With process technologies advancing toward 2nm, more routability issues have arisen due to the decreasing number of routing tracks, increasing number and complexity of design rules, and strict patterning rules. The state-of-the-art standard cell design automation framework is able to automatically design standard cell layouts in advanced nodes, but it is still struggling to generate highly competitive Performance-Power-Area (PPA) and routable cell layouts for complex sequential cell designs. Consequently, a novel and efficient methodology incorporating the expertise of experienced human designers to incrementally optimize the PPA of cell layouts is highly necessary and essential. High-quality device clustering, with consideration of netlist topology, diffusion sharing/break and routability in the layouts, can reduce complexity and assist in finding highly competitive PPA, and routable layouts faster. In this paper, we leverage the natural language and reasoning ability of Large Language Model (LLM) to generate high-quality cluster constraints incrementally to optimize the cell layout PPA and debug the routability with ReAct prompting. On a benchmark of sequential standard cells in 2nm, we demonstrate that the proposed method not only achieves up to 19.4% smaller cell area, but also generates 23.5% more LVS/DRC clean cell layouts than previous work. In summary, the proposed method not only successfully reduces cell area by 4.65% on average, but also is able to fix routability in the cell layout designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06549v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chia-Tung Ho, Haoxing Ren</dc:creator>
    </item>
    <item>
      <title>ChiBench: a Benchmark Suite for Testing Electronic Design Automation Tools</title>
      <link>https://arxiv.org/abs/2406.06550</link>
      <description>arXiv:2406.06550v1 Announce Type: new 
Abstract: Electronic Design Automation (EDA) tools are software applications used by engineers in the design, development, simulation, and verification of electronic systems and integrated circuits. These tools typically process specifications written in a Hardware Description Language (HDL), such as Verilog, SystemVerilog or VHDL. Thus, effective testing of these tools requires benchmark suites written in these languages. However, while there exist some open benchmark suites for these languages, they tend to consist of only a handful of specifications. This paper, in contrast, presents ChiBench, a comprehensive suite comprising 50 thousand Verilog programs. These programs were sourced from GitHub repositories and curated using Verible's syntactic analyzer and Jasper(TM)'s HDL semantic analyzer. Since its inception, ChiBench has already revealed bugs in public tools like Verible's obfuscator and parser. In addition to explaining some of these case studies, this paper demonstrates how ChiBench can be used to evaluate the asymptotic complexity and code coverage of typical electronic design automation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06550v1</guid>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael Sumitani, Jo\~ao Victor Amorim, Augusto Mafra, Mirlaine Crepalde, Fernando Magno Quint\~ao Pereira</dc:creator>
    </item>
    <item>
      <title>Instruction Block Movement with Coupled High-Level Program Sequencing</title>
      <link>https://arxiv.org/abs/2406.06738</link>
      <description>arXiv:2406.06738v1 Announce Type: new 
Abstract: Efficiency in instruction fetching is critical to performance, and this requires the primary structures -- L1 instruction caches (L1i), branch target buffers (BTB) and instruction TLBs (iTLB) -- to have the requisite information when needed. This paper proposes a high-level program sequencing mechanism and a coupled technique for block movement, instruction presending, where instruction cache blocks, BTB entries, and iTLB entries are autonomously moved (or sent) from the secondary to the primary structures in a "just in time" fashion so that they are available when needed.
  Empirical results are presented to demonstrate the efficacy of the high-level sequencing mechanism and block movement. Presending is especially effective for benchmarks with a high base MPKI, where the movement of instruction blocks (and BTB/iTLB entries) from secondary to primary structures is frequent. Presending reduces the number of misses in primary structures by an order of magnitude as compared to state-of-the-art instruction prefetching schemes, in many cases, while allowing the processor to operate with small-sized primary BTBs. This reduction in misses results in performance improvements in cases where front-end efficiency is important.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06738v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shyam Murthy, Gurindar S. Sohi</dc:creator>
    </item>
    <item>
      <title>Differentiable Combinatorial Scheduling at Scale</title>
      <link>https://arxiv.org/abs/2406.06593</link>
      <description>arXiv:2406.06593v1 Announce Type: cross 
Abstract: This paper addresses the complex issue of resource-constrained scheduling, an NP-hard problem that spans critical areas including chip design and high-performance computing. Traditional scheduling methods often stumble over scalability and applicability challenges. We propose a novel approach using a differentiable combinatorial scheduling framework, utilizing Gumbel-Softmax differentiable sampling technique. This new technical allows for a fully differentiable formulation of linear programming (LP) based scheduling, extending its application to a broader range of LP formulations. To encode inequality constraints for scheduling tasks, we introduce \textit{constrained Gumbel Trick}, which adeptly encodes arbitrary inequality constraints. Consequently, our method facilitates an efficient and scalable scheduling via gradient descent without the need for training data. Comparative evaluations on both synthetic and real-world benchmarks highlight our capability to significantly improve the optimization efficiency of scheduling, surpassing state-of-the-art solutions offered by commercial and open-source solvers such as CPLEX, Gurobi, and CP-SAT in the majority of the designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06593v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingju Liu, Yingjie Li, Jiaqi Yin, Zhiru Zhang, Cunxi Yu</dc:creator>
    </item>
    <item>
      <title>FAULT+PROBE: A Generic Rowhammer-based Bit Recovery Attack</title>
      <link>https://arxiv.org/abs/2406.06943</link>
      <description>arXiv:2406.06943v1 Announce Type: cross 
Abstract: Rowhammer is a security vulnerability that allows unauthorized attackers to induce errors within DRAM cells. To prevent fault injections from escalating to successful attacks, a widely accepted mitigation is implementing fault checks on instructions and data.
  We challenge the validity of this assumption by examining the impact of the fault on the victim's functionality. Specifically, we illustrate that an attacker can construct a profile of the victim's memory based on the directional patterns of bit flips. This profile is then utilized to identify the most susceptible bit locations within DRAM rows. These locations are then subsequently leveraged during an online attack phase with side information observed from the change in the victim's behavior to deduce sensitive bit values. Consequently, the primary objective of this study is to utilize Rowhammer as a probe, shifting the emphasis away from the victim's memory integrity and toward statistical fault analysis (SFA) based on the victim's operational behavior.
  We show FAULT+PROBE may be used to circumvent the verify-after-sign fault check mechanism, which is designed to prevent the generation of erroneous signatures that leak sensitive information. It does so by injecting directional faults into key positions identified during a memory profiling stage. The attacker observes the signature generation rate and decodes the secret bit value accordingly. This circumvention is enabled by an observable channel in the victim. FAULT+PROBE is not limited to signing victims and can be used to probe secret bits on arbitrary systems where an observable channel is present that leaks the result of the fault injection attempt. To demonstrate the attack, we target the fault-protected ECDSA in wolfSSL's implementation of the TLS 1.3 handshake. We recover 256-bit session keys with an average recovery rate of 22 key bits/hour and a 100% success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06943v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kemal Derya, M. Caner Tol, Berk Sunar</dc:creator>
    </item>
    <item>
      <title>Embedded Graph Convolutional Networks for Real-Time Event Data Processing on SoC FPGAs</title>
      <link>https://arxiv.org/abs/2406.07318</link>
      <description>arXiv:2406.07318v1 Announce Type: cross 
Abstract: The utilisation of event cameras represents an important and swiftly evolving trend aimed at addressing the constraints of traditional video systems. Particularly within the automotive domain, these cameras find significant relevance for their integration into embedded real-time systems due to lower latency and energy consumption. One effective approach to ensure the necessary throughput and latency for event processing systems is through the utilisation of graph convolutional networks (GCNs). In this study, we introduce a series of hardware-aware optimisations tailored for PointNet++, a GCN architecture designed for point cloud processing. The proposed techniques result in more than a 100-fold reduction in model size compared to Asynchronous Event-based GNN (AEGNN), one of the most recent works in the field, with a relatively small decrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars classification), thus following the TinyML trend. Based on software research, we designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional Network) and we implemented it on ZCU104 SoC FPGA platform, achieving a throughput of 13.3 million events per second (MEPS) and real-time partially asynchronous processing with a latency of 4.47 ms. We also address the scalability of the proposed hardware model to improve the obtained accuracy score. To the best of our knowledge, this study marks the first endeavour in accelerating PointNet++ networks on SoC FPGAs, as well as the first hardware architecture exploration of graph convolutional networks implementation for real-time continuous event data processing. We publish both software and hardware source code in an open repository: https://github.com/vision-agh/*** (will be published upon acceptance).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07318v1</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamil Jeziorek, Piotr Wzorek, Krzysztof Blachut, Andrea Pinna, Tomasz Kryjak</dc:creator>
    </item>
    <item>
      <title>Novel Optimized Designs of Modulo $2n+1$ Adder for Quantum Computing</title>
      <link>https://arxiv.org/abs/2406.07486</link>
      <description>arXiv:2406.07486v1 Announce Type: cross 
Abstract: Quantum modular adders are one of the most fundamental yet versatile quantum computation operations. They help implement functions of higher complexity, such as subtraction and multiplication, which are used in applications such as quantum cryptanalysis, quantum image processing, and securing communication. To the best of our knowledge, there is no existing design of quantum modulo $(2n+1)$ adder. In this work, we propose four quantum adders targeted specifically for modulo $(2n+1)$ addition. These adders can provide both regular and modulo $(2n+1)$ sum concurrently, enhancing their application in residue number system based arithmetic. Our first design, QMA1, is a novel quantum modulo $(2n+1)$ adder. The second proposed adder, QMA2, optimizes the utilization of quantum gates within the QMA1, resulting in 37.5% reduced CNOT gate count, 46.15% reduced CNOT depth, and 26.5% decrease in both Toffoli gates and depth. We propose a third adder QMA3 that uses zero resets, a dynamic circuits based feature that reuses qubits, leading to 25% savings in qubit count. Our fourth design, QMA4, demonstrates the benefit of incorporating additional zero resets to achieve a purer zero state, reducing quantum state preparation errors. Notably, we conducted experiments using 5-qubit configurations of the proposed modulo $(2n+1)$ adders on the IBM Washington, a 127-qubit quantum computer based on the Eagle R1 architecture, to demonstrate a 28.8% reduction in QMA1's error of which: (i) 18.63% error reduction happens due to gate and depth reduction in QMA2, and (ii) 2.53% drop in error due to qubit reduction in QMA3, and (iii) 7.64% error decreased due to application of additional zero resets in QMA4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07486v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhaskar Gaur, Himanshu Thapliyal</dc:creator>
    </item>
    <item>
      <title>A Micro Architectural Events Aware Real-Time Embedded System Fault Injector</title>
      <link>https://arxiv.org/abs/2401.08397</link>
      <description>arXiv:2401.08397v2 Announce Type: replace 
Abstract: In contemporary times, the increasing complexity of the system poses significant challenges to the reliability, trustworthiness, and security of the SACRES. Key issues include the susceptibility to phenomena such as instantaneous voltage spikes, electromagnetic interference, neutron strikes, and out-of-range temperatures. These factors can induce switch state changes in transistors, resulting in bit-flipping, soft errors, and transient corruption of stored data in memory. The occurrence of soft errors, in turn, may lead to system faults that can propel the system into a hazardous state. Particularly in critical sectors like automotive, avionics, or aerospace, such malfunctions can have real-world implications, potentially causing harm to individuals.
  This paper introduces a novel fault injector designed to facilitate the monitoring, aggregation, and examination of micro-architectural events. This is achieved by harnessing the microprocessor's PMU and the debugging interface, specifically focusing on ensuring the repeatability of fault injections. The fault injection methodology targets bit-flipping within the memory system, affecting CPU registers and RAM. The outcomes of these fault injections enable a thorough analysis of the impact of soft errors and establish a robust correlation between the identified faults and the essential timing predictability demanded by SACRES.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08397v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Enrico Magliano, Alessio Carpegna, Alessadro Savino, Stefano Di Carlo</dc:creator>
    </item>
    <item>
      <title>Mexican Computers: A Brief Technical and Historical Overview</title>
      <link>https://arxiv.org/abs/2406.04912</link>
      <description>arXiv:2406.04912v2 Announce Type: replace 
Abstract: The emergence of the microprocessor in the early 1970s allowed the design of computers that did not require the substantial economic resources of large computer companies of that era. Shortly after this event, a variety of computers based on microprocessors appeared in the United States and other developed countries. Unlike in those countries, where small and large companies developed most personal computers, in Mexico, the first microprocessor-based computers were designed within academic institutions. It is little known that Mexican computers of that era included a variety of systems ranging from purpose-specific research and teaching-oriented computers to high-performance personal computers. The goal of this article is to describe in detail some of these Mexican computers designed between the late 1970s and mid-1980s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04912v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Ortiz-Arroyo</dc:creator>
    </item>
    <item>
      <title>MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems</title>
      <link>https://arxiv.org/abs/2310.02784</link>
      <description>arXiv:2310.02784v3 Announce Type: replace-cross 
Abstract: Training and deploying large-scale machine learning models is time-consuming, requires significant distributed computing infrastructures, and incurs high operational costs. Our analysis, grounded in real-world large model training on datacenter-scale infrastructures, reveals that 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize this outstanding communication latency and other inherent at-scale inefficiencies, we introduce an agile performance modeling framework, MAD-Max. This framework is designed to optimize parallelization strategies and facilitate hardware-software co-design opportunities. Through the application of MAD-Max to a suite of real-world large-scale ML models on state-of-the-art GPU clusters, we showcase potential throughput enhancements of up to 2.24x for pre-training and up to 5.2x for inference scenarios, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02784v3</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Hsia, Alicia Golden, Bilge Acun, Newsha Ardalani, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu</dc:creator>
    </item>
    <item>
      <title>Multi-qubit Lattice Surgery Scheduling</title>
      <link>https://arxiv.org/abs/2405.17688</link>
      <description>arXiv:2405.17688v2 Announce Type: replace-cross 
Abstract: Fault-tolerant quantum computation using two-dimensional topological quantum error correcting codes can benefit from multi-qubit long-range operations. By using simple commutation rules, a quantum circuit can be transpiled into a sequence of solely non-Clifford multi-qubit gates. Prior work on fault-tolerant compilation avoids optimal scheduling of such gates since they reduce the parallelizability of the circuit. We observe that the reduced parallelization potential is outweighed by the significant reduction in the number of gates. We therefore devise a method for scheduling multi-qubit lattice surgery using an earliest-available-first policy, solving the associated forest packing problem using a representation of the multi-qubit gates as Steiner trees. Our extensive testing on random and application-inspired circuits demonstrates the method's scalability and performance. We show that the transpilation significantly reduces the circuit length on the set of circuits tested, and that the resulting circuit of multi-qubit gates has a further reduction in the expected circuit execution time compared to serial execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17688v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allyson Silva, Xiangyi Zhang, Zak Webb, Mia Kramer, Chan Woo Yang, Xiao Liu, Jessica Lemieux, Ka-Wai Chen, Artur Scherer, Pooya Ronagh</dc:creator>
    </item>
  </channel>
</rss>
