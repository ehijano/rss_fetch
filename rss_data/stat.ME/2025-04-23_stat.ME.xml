<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust Causal Inference for EHR-based Studies of Point Exposures with Missingness in Eligibility Criteria</title>
      <link>https://arxiv.org/abs/2504.16230</link>
      <description>arXiv:2504.16230v1 Announce Type: new 
Abstract: Missingness in variables that define study eligibility criteria is a seldom addressed challenge in electronic health record (EHR)-based settings. It is typically the case that patients with incomplete eligibility information are excluded from analysis without consideration of (implicit) assumptions that are being made, leaving study conclusions subject to potential selection bias. In an effort to ascertain eligibility for more patients, researchers may look back further in time prior to study baseline, and in using outdated values of eligibility-defining covariates may inappropriately be including individuals who, unbeknownst to the researcher, fail to meet eligibility at baseline. To the best of our knowledge, however, very little work has been done to mitigate these concerns. We propose a robust and efficient estimator of the causal average treatment effect on the treated, defined in the study eligible population, in cohort studies where eligibility-defining covariates are missing at random. The approach facilitates the use of flexible machine-learning strategies for component nuisance functions while maintaining appropriate convergence rates for valid asymptotic inference. EHR data from Kaiser Permanente are used as motivation as well as a basis for extensive simulations that verify robustness properties under various degrees of model misspecification. The data are also used to demonstrate the use of the method to analyze differences between two common bariatric surgical interventions for long-term weight and glycemic outcomes among a cohort of severely obese patients with type II diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16230v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Heidi Fischer, Catherine Lee, Susan M. Shortreed, Sebastien Haneuse, Alexander W. Levis</dc:creator>
    </item>
    <item>
      <title>Accounting for spillover when using the augmented synthetic control method: estimating the effect of localized COVID-19 lockdowns in Chile</title>
      <link>https://arxiv.org/abs/2504.16244</link>
      <description>arXiv:2504.16244v1 Announce Type: new 
Abstract: The implementation of public health policies, particularly in a single or small set of units (e.g., regions), can create complex dynamics with effects extending beyond the directly treated areas. This paper examines the direct effect of COVID-19 lockdowns in Chile on the comunas where they were enacted and the spillover effect from neighboring comunas. To draw inference about these effects, the Augmented Synthetic Control Method (ASCM) is extended to account for interference between neighboring units by introducing a stratified control framework. Specifically, Ridge ASCM with stratified controls (ASCM-SC) is proposed to partition control units based on treatment exposure. By leveraging control units that are untreated, or treated but outside the treated unit's neighborhood, this method estimates both the direct and spillover effects of intervention on treated and neighboring units. Simulations demonstrate improved bias reduction under various data-generating processes. ASCM-SC is applied to estimate the direct and total (direct + indirect) effects of COVID-19 lockdowns in Chile at the start of the COVID-19 pandemic. This method provides a more flexible approach for estimating the effects of public health interventions in settings with interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16244v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taylor Krajewski, Michael Hudgens</dc:creator>
    </item>
    <item>
      <title>Pure Error REML for Analyzing Data from Multi-Stratum Designs</title>
      <link>https://arxiv.org/abs/2504.16531</link>
      <description>arXiv:2504.16531v1 Announce Type: new 
Abstract: Since the dawn of response surface methodology, it has been recommended that designs include replicate points, so that pure error estimates of variance can be obtained and used to provide unbiased estimated standard errors of the effects of factors. In designs with more than one stratum, such as split-plot and split-split-plot designs, it is less obvious how pure error estimates of the variance components should be obtained, and no pure error estimates are given by the popular residual maximum likelihood (REML) method of estimation. We propose a method of pure error REML estimation of the variance components, using the full treatment model, obtained by treating each combination of factor levels as a discrete treatment. Our method is easy to implement using standard software and improved estimated standard errors of the fixed effects estimates can be obtained by applying the Kenward-Roger correction based on the pure error REML estimates. We illustrate the new method using several data sets and compare the performance of pure error REML with the standard REML method. The results are comparable when the assumed response surface model is correct, but the new method is considerably more robust in the case of model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16531v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven G Gilmour, Peter Goos, Heiko Grossmann</dc:creator>
    </item>
    <item>
      <title>Censored lifespans in a double-truncated sample: Maximum likelihood inference for the exponential distribution</title>
      <link>https://arxiv.org/abs/2504.16623</link>
      <description>arXiv:2504.16623v1 Announce Type: new 
Abstract: The analysis of a truncated sample can be hindered by censoring. Survival information may be lost to follow-up or the birthdate may be missing. The data can still be modeled as a truncated point process and it is close to a Poisson process, in the Hellinger distance, as long as the sample is small relative to the population. We assume an exponential distribution for the lifespan, derive the likelihood and profile out the unobservable sample size. Identification of the exponential parameter is shown, together with consistency and asymptotic normality of its M-estimator. Even though the estimator sequence is indexed in the sample size, both the point estimator and the standard error are observable. Enterprise lifespans in Germany constitute our example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16623v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fiete Sieg, Anne-Marie Toparkus, Rafael Weissbach</dc:creator>
    </item>
    <item>
      <title>Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations</title>
      <link>https://arxiv.org/abs/2504.16864</link>
      <description>arXiv:2504.16864v1 Announce Type: new 
Abstract: In science and social science, we often wish to explain why an outcome is different in two populations. For instance, if a jobs program benefits members of one city more than another, is that due to differences in program participants (particular covariates) or the local labor markets (outcomes given covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool in econometrics that explains the difference in the mean outcome across two populations. However, the KOB decomposition assumes a linear relationship between covariates and outcomes, while the true relationship may be meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear functional decompositions for the relationship between outcomes and covariates in one population. It seems natural to extend the KOB decomposition using these functional decompositions. We observe that a successful extension should not attribute the differences to covariates -- or, respectively, to outcomes given covariates -- if those are the same in the two populations. Unfortunately, we demonstrate that, even in simple examples, two common decompositions -- functional ANOVA and Accumulated Local Effects -- can attribute differences to outcomes given covariates, even when they are identical in two populations. We provide a characterization of when functional ANOVA misattributes, as well as a general property that any discrete decomposition must satisfy to avoid misattribution. We show that if the decomposition is independent of its input distribution, it does not misattribute. We further conjecture that misattribution arises in any reasonable additive decomposition that depends on the distribution of the covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16864v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Quintero, William T. Stephenson, Advik Shreekumar, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Definition, Identification, and Estimation of the Direct and Indirect Number Needed to Treat</title>
      <link>https://arxiv.org/abs/2504.16912</link>
      <description>arXiv:2504.16912v1 Announce Type: new 
Abstract: The number needed to treat (NNT) is an efficacy and effect size measure commonly used in epidemiological studies and meta-analyses. The NNT was originally defined as the average number of patients needed to be treated to observe one less adverse effect. In this study, we introduce the novel direct and indirect number needed to treat (DNNT and INNT, respectively). The DNNT and the INNT are efficacy measures defined as the average number of patients that needed to be treated to benefit from the treatment's direct and indirect effects, respectively. We start by formally defining these measures using nested potential outcomes. Next, we formulate the conditions for the identification of the DNNT and INNT, as well as for the direct and indirect number needed to expose (DNNE and INNE, respectively) and the direct and indirect exposure impact number (DEIN and IEIN, respectively) in observational studies. Next, we present an estimation method with two analytical examples. A corresponding simulation study follows the examples. The simulation study illustrates that the estimators of the novel indices are consistent, and their analytical confidence intervals meet the nominal coverage rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16912v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Vancak, Arvid Sj\"olander</dc:creator>
    </item>
    <item>
      <title>Compositionality in algorithms for smoothing</title>
      <link>https://arxiv.org/abs/2303.13865</link>
      <description>arXiv:2303.13865v3 Announce Type: cross 
Abstract: Backward Filtering Forward Guiding (BFFG) is a bidirectional algorithm proposed in Mider et al. [2021] and studied more in depth in a general setting in Van der Meulen and Schauer [2022]. In category theory, optics have been proposed for modelling systems with bidirectional data flow. We connect BFFG with optics by demonstrating that the forward and backwards map together define a functor from a category of Markov kernels into a category of optics, which can furthermore be lax monoidal under further assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13865v3</guid>
      <category>math.CT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Schauer, Frank van der Meulen, Andi Q. Wang</dc:creator>
    </item>
    <item>
      <title>Analogy making as the basis of statistical inference</title>
      <link>https://arxiv.org/abs/2504.16186</link>
      <description>arXiv:2504.16186v1 Announce Type: cross 
Abstract: Standard statistical theory has arguably proved to be unsuitable as a basis for constructing a satisfactory completely general framework for performing statistical inference. For example, frequentist theory has never come close to providing such a general inferential framework, which is not only attributable to the question surrounding the soundness of this theory, but also to its focus on attempting to address the problem of how to perform statistical inference only in certain special cases. Also, theories of inference that are grounded in the idea of deducing sample-based inferences about populations of interest from a given set of universally acceptable axioms, e.g. many theories that aim to justify Bayesian inference and theories of imprecise probability, suffer from the difficulty of finding such axioms that are weak enough to be widely acceptable, but strong enough to lead to methods of inference that can be regarded as being efficient. These observations justify the need to look for an alternative means by which statistical inference may be performed, and in particular, to explore the one that is offered by analogy making. What is presented here goes down this path. To be clear, this is done in a way that does not simply endorse the common use of analogy making as a supplementary means of understanding how statistical methods work, but formally develops analogy making as the foundation of a general framework for performing statistical inference. In the latter part of the paper, the use of this framework is illustrated by applying some of the most important analogies contained within it to a relatively simple but arguably still unresolved problem of statistical inference, which naturally leads to an original way being put forward of addressing issues that relate to Bartlett's and Lindley's paradoxes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16186v1</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell J. Bowater</dc:creator>
    </item>
    <item>
      <title>Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees</title>
      <link>https://arxiv.org/abs/2504.16356</link>
      <description>arXiv:2504.16356v1 Announce Type: cross 
Abstract: Graphical models are widely used in diverse application domains to model the conditional dependencies amongst a collection of random variables. In this paper, we consider settings where the graph structure is covariate-dependent, and investigate a deep neural network-based approach to estimate it. The method allows for flexible functional dependency on the covariate, and fits the data reasonably well in the absence of a Gaussianity assumption. Theoretical results with PAC guarantees are established for the method, under assumptions commonly used in an Empirical Risk Minimization framework. The performance of the proposed method is evaluated on several synthetic data settings and benchmarked against existing approaches. The method is further illustrated on real datasets involving data from neuroscience and finance, respectively, and produces interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16356v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahe Lin, Yikai Zhang, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Linear Regression Using Hilbert-Space-Valued Covariates with Unknown Reproducing Kernel</title>
      <link>https://arxiv.org/abs/2504.16780</link>
      <description>arXiv:2504.16780v1 Announce Type: cross 
Abstract: We present a new method of linear regression based on principal components using Hilbert-space-valued covariates with unknown reproducing kernels. We develop a computationally efficient approach to estimation and derive asymptotic theory for the regression parameter estimates under mild assumptions. We demonstrate the approach in simulation studies as well as in data analysis using two-dimensional brain images as predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16780v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Li, Margaret Hoch, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>The matryoshka doll prior: principled multiplicity correction in Bayesian comparison</title>
      <link>https://arxiv.org/abs/1511.04745</link>
      <description>arXiv:1511.04745v3 Announce Type: replace 
Abstract: This paper introduces a general and principled construction of model space priors with a focus on regression problems. The proposed formulation regards each model as a `local` null hypothesis whose alternatives are the set of models that nest it. Assuming constant odds between any `local` null and its alternatives provides a natural isomorphism of model spaces (like a matryoshka doll), constituting an intuitive way to correct for test multiplicity. This isomorphism yields the Poisson distribution as the unique limiting distribution over model dimension under mild assumptions. We compare this model space prior theoretically and in simulations to widely adopted Beta-Binomial constructions. We show that the proposed prior yields a `just-right` multiplicity correction that induces a desirable complexity penalization profile.</description>
      <guid isPermaLink="false">oai:arXiv.org:1511.04745v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew J Womack, Daniel Taylor-Rodriguez, Claudio Fuentes</dc:creator>
    </item>
    <item>
      <title>Synthetic Controls for Experimental Design</title>
      <link>https://arxiv.org/abs/2108.02196</link>
      <description>arXiv:2108.02196v5 Announce Type: replace 
Abstract: This article studies experimental design in settings where the experimental units are large aggregate entities (e.g., markets), and only one or a small number of units can be exposed to the treatment. In such settings, randomization of the treatment may result in treated and control groups with very different characteristics at baseline, inducing biases. We propose a variety of experimental non-randomized synthetic control designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) that select the units to be treated, as well as the untreated units to be used as a control group. Average potential outcomes are estimated as weighted averages of the outcomes of treated units for potential outcomes with treatment, and weighted averages the outcomes of control units for potential outcomes without treatment. We analyze the properties of estimators based on synthetic control designs and propose new inferential techniques. We show that in experimental settings with aggregate units, synthetic control designs can substantially reduce estimation biases in comparison to randomization of the treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.02196v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Abadie, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Auxiliary Data</title>
      <link>https://arxiv.org/abs/2204.10736</link>
      <description>arXiv:2204.10736v2 Announce Type: replace 
Abstract: Evidence-based policy-making requires reliable, spatially disaggregated indicators. The framework of mixed effects random forests leverages the advantages of random forests and hierarchical data in small area estimation. These methods require typically access to auxiliary information on population-level, which is a strong limitation for practitioners. In contrast, our proposed method - for point and uncertainty estimation - abstains from access to unitlevel population data but adaptively incorporates aggregated auxiliary information through calibration-weights. We demonstrate its usage for estimating opportunity cost of care work for Germany from the Socio-Economic Panel and census aggregates. Simulation studies evaluate our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10736v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Krennmair, Nora W\"urz, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>On integral priors for multiple comparison in Bayesian model selection</title>
      <link>https://arxiv.org/abs/2406.14184</link>
      <description>arXiv:2406.14184v3 Announce Type: replace 
Abstract: Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the guarantee of their existence and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. This renders its implementation automatic and generic, both in the nested and in the non-nested cases. We present some examples, including situations where other methodologies need specific adjustments or do not produce a satisfactory answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14184v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Salmer\'on, Juan Antonio Cano, Christian P. Robert</dc:creator>
    </item>
    <item>
      <title>Beyond Arbitrary Replications: A Principled Approach to Simulation Design in Causal Inference</title>
      <link>https://arxiv.org/abs/2409.05161</link>
      <description>arXiv:2409.05161v2 Announce Type: replace 
Abstract: Evaluation of novel treatment effect estimators frequently relies on simulation studies lacking formal statistical comparisons and using arbitrary numbers of replications ($J$). This hinders reproducibility and efficiency. We propose the Test-Informed Simulation Count Algorithm (TISCA) to address these shortcomings. TISCA integrates Welch's t-tests with power analysis, iteratively running simulations until a pre-specified power (e.g., 0.8) is achieved for detecting a user-defined minimum detectable effect size (MDE) at a given significance level ($\alpha$). This yields a statistically justified simulation count ($J$) and rigorous model comparisons. Our bibliometric study confirms the heterogeneity of current practices regarding $J$. A case study revisiting McJames et al. (2024) demonstrates TISCA identifies sufficient simulations ($J=500$ vs. original $J=1000$), saving computational resources while providing statistically sound evidence. TISCA promotes rigorous, efficient, and sustainable simulation practices in causal inference and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05161v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hugo Gobato Souto, Francisco Louzada Neto</dc:creator>
    </item>
    <item>
      <title>Regression-based proximal causal inference for right-censored time-to-event data</title>
      <link>https://arxiv.org/abs/2409.08924</link>
      <description>arXiv:2409.08924v4 Announce Type: replace 
Abstract: Unmeasured confounding is one of the major concerns in causal inference from observational data. Proximal causal inference (PCI) is an emerging methodological framework to detect and potentially account for confounding bias by carefully leveraging a pair of negative control exposure (NCE) and outcome (NCO) variables, also known as treatment and outcome confounding proxies. Although regression-based PCI is well developed for binary and continuous outcomes, analogous PCI regression methods for right-censored time-to-event outcomes are currently lacking. In this paper, we propose a novel two-stage regression PCI approach for right-censored survival data under an additive hazard structural model. We provide theoretical justification for the proposed approach tailored to different types of NCOs, including continuous, count, and right-censored time-to-event variables. We illustrate the approach with an evaluation of the effectiveness of right heart catheterization among critically ill patients using data from the SUPPORT study. Our method is implemented in the open-access R package 'pci2s'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08924v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendrick Li, George C. Linderman, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>On the Selection Stability of Stability Selection and Its Applications</title>
      <link>https://arxiv.org/abs/2411.09097</link>
      <description>arXiv:2411.09097v2 Announce Type: replace 
Abstract: Stability selection is a widely adopted resampling-based framework for high-dimensional variable selection. This paper seeks to broaden the use of an established stability estimator to evaluate the overall stability of the stability selection results, moving beyond single-variable analysis. We suggest that the stability estimator offers two advantages: it can serve as a reference to reflect the robustness of the results obtained, and help identify an optimal regularization value to improve stability. By determining this value, we calibrate key stability selection parameters, namely, the decision threshold and the expected number of falsely selected variables, within established theoretical bounds. The asymptotic distribution of the stability estimator allows us to observe convergence of stability values over successive sub-samples. This approach sheds light on the required number of sub-samples addressing a notable gap in prior studies. Pareto optimality of the proposed regularization value is also discussed. The stabplot R package is developed to facilitate the use of the plots featured in this manuscript, supporting their integration into further statistical analysis and research workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09097v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v3 Announce Type: replace 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For hierarchical multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. Finally, we validate our approach using synthetic data and on CRASH-3, a major clinical trial focused on assessing the effects of tranexamic acid in patients with traumatic brain injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>On the Proportional Principal Stratum Hazards Model</title>
      <link>https://arxiv.org/abs/2503.10481</link>
      <description>arXiv:2503.10481v2 Announce Type: replace 
Abstract: In clinical trials involving both mortality and morbidity, an active treatment can influence the observed risk of the first non-fatal event either directly, through its effect on the underlying non-fatal event process, or indirectly, through its effect on the death process, or both. Discerning the direct effect of treatment on the underlying first non-fatal event process holds clinical interest. However, with the competing risk of death, the Cox proportional hazards model that treats death as non-informative censoring and evaluates treatment effects on time to the first non-fatal event provides an estimate of the cause-specific hazard ratio, which may not correspond to the direct effect. To obtain the direct effect on the underlying first non-fatal event process, within the principal stratification framework, we define the principal stratum hazard and introduce the Proportional Principal Stratum Hazards model. This model estimates the principal stratum hazard ratio, which reflects the direct effect on the underlying first non-fatal event process in the presence of death and simplifies to the hazard ratio in the absence of death. The principal stratum membership is identified probabilistically using the shared frailty model, which assumes independence between the first non-fatal event process and the potential death processes, conditional on per-subject random frailty. Simulation studies are conducted to verify the reliability of our estimators. We illustrate the method using the Carvedilol Prospective Randomized Cumulative Survival trial, which involves heart-failure events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10481v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Thomas D. Cook</dc:creator>
    </item>
    <item>
      <title>Exact Sampling of Gibbs Measures with Estimated Losses</title>
      <link>https://arxiv.org/abs/2404.15649</link>
      <description>arXiv:2404.15649v2 Announce Type: replace-cross 
Abstract: In recent years, the shortcomings of Bayesian posteriors as inferential devices have received increased attention. A popular strategy for fixing them has been to instead target a Gibbs measure based on losses that connect a parameter of interest to observed data. However, existing theory for such inference procedures assumes these losses are analytically available, while in many situations these losses must be stochastically estimated using pseudo-observations. In such cases, we show that when standard Markov Chain Monte Carlo algorithms are used to produce posterior samples, the resulting posterior exhibits strong dependence on the number of pseudo-observations: unless the number of pseudo-observations diverge sufficiently fast the resulting posterior will concentrate very slowly. However, we show that in many situations it is feasible to alleviate this dependence entirely using a modified piecewise deterministic Markov process (PDMP) sampler, and we formally and empirically show that these samplers produce posterior draws that have no dependence on the number of pseudo-observations used to estimate the loss within the Gibbs Measure. We apply our results to three examples that feature intractable likelihoods and model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15649v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Jeremias Knoblauch, Jack Jewson, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v2 Announce Type: replace-cross 
Abstract: Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g. satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then to use its predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is post-outcome, i.e. if variation in the economic outcome causes variation in the RSV. In program evaluation, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition that underlies common practice: the conditional distribution of the RSV given the outcome and treatment is stable across the samples.Based on our identifying formula, we find that the efficient representation of RSVs for causal inference requires three predictions rather than one. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We re-analyze the effect of an anti-poverty program in India using satellite images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Debiasing Functions of Private Statistics in Postprocessing</title>
      <link>https://arxiv.org/abs/2502.13314</link>
      <description>arXiv:2502.13314v3 Announce Type: replace-cross 
Abstract: Given a differentially private unbiased estimate $\tilde{q}=q(D) +\nu$ of a statistic $q(D)$, we wish to obtain unbiased estimates of functions of $q(D)$, such as $1/q(D)$, solely through post-processing of $\tilde{q}$, with no further access to the confidential dataset $D$. To this end, we adapt the deconvolution method used for unbiased estimation in the statistical literature, deriving unbiased estimators for a broad family of twice-differentiable functions when the privacy-preserving noise $\nu$ is drawn from the Laplace distribution (Dwork et al., 2006). We further extend this technique to a more general class of functions, deriving approximately optimal estimators that are unbiased for values in a user-specified interval (possibly extending to $\pm \infty$). We use these results to derive an unbiased estimator for private means when the size $n$ of the dataset is not publicly known. In a numerical application, we find that a mechanism that uses our estimator to return an unbiased sample size and mean outperforms a mechanism that instead uses the previously known unbiased privacy mechanism for such means (Kamath et al., 2023). We also apply our estimators to develop unbiased transformation mechanisms for per-record differential privacy, a privacy concept in which the privacy guarantee is a public function of a record's value (Seeman et al., 2024). Our mechanisms provide stronger privacy guarantees than those in prior work (Finley et al., 2024) by using Laplace, rather than Gaussian, noise. Finally, using a different approach, we go beyond Laplace noise by deriving unbiased estimators for polynomials under the weak condition that the noise distribution has sufficiently many moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13314v3</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Flavio Calmon, Elbert Du, Cynthia Dwork, Brian Finley, Grigory Franguridi</dc:creator>
    </item>
    <item>
      <title>Asymptotic well-calibration of the posterior predictive $p$-value under the modified Kolmogorov-Smirnov test</title>
      <link>https://arxiv.org/abs/2504.14077</link>
      <description>arXiv:2504.14077v2 Announce Type: replace-cross 
Abstract: The posterior predictive $p$-value is a widely used tool for Bayesian model checking. However, under most test statistics, its asymptotic null distribution is more concentrated around 1/2 than uniform. Consequently, its finite-sample behavior is difficult to interpret and tends to lack power, which is a well-known issue among practitioners. A common choice of test statistic is the Kolmogorov-Smirnov test with plug-in estimators. It provides a global measure of model-data discrepancy for real-valued observations and is sensitive to model misspecification. In this work, we establish that under this test statistic, the posterior predictive $p$-value converges in distribution to uniform under the null. We further use numerical experiments to demonstrate that this $p$-value is well-behaved in finite samples and can effectively detect a wide range of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14077v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueming Shen</dc:creator>
    </item>
  </channel>
</rss>
