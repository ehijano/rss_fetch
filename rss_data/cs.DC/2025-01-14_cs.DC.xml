<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 05:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management</title>
      <link>https://arxiv.org/abs/2501.06709</link>
      <description>arXiv:2501.06709v1 Announce Type: new 
Abstract: Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of MELL and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06709v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song</dc:creator>
    </item>
    <item>
      <title>CoCoI: Distributed Coded Inference System for Straggler Mitigation</title>
      <link>https://arxiv.org/abs/2501.06856</link>
      <description>arXiv:2501.06856v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) are widely applied in real-time applications on resource-constrained devices. To accelerate CNN inference, prior works proposed to distribute the inference workload across multiple devices. However, they did not address stragglers and device failures in distributed inference, which is challenging due to the devices' time-varying and possibly unknown computation/communication capacities. To address this, we propose a distributed coded inference system, called CoCoI. It splits the convolutional layers of CNN, considering the data dependency of high-dimensional inputs and outputs, and then adapts coding schemes to generate task redundancy. With CoCoI, the inference results can be determined once a subset of devices complete their subtasks, improving robustness against stragglers and failures. To theoretically analyze the tradeoff between redundancy and subtask workload, we formulate an optimal splitting problem to minimize the expected inference latency. Despite its non-convexity, we determine an approximate strategy with minor errors, and prove that CoCoI outperforms uncoded benchmarks. For performance evaluation, we build a testbed with Raspberry Pi 4Bs. The experimental results show that the approximate strategy closely matches the optimal solution. When compared with uncoded benchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of stragglers and device failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06856v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xing Liu, Chao Huang, Ming Tang</dc:creator>
    </item>
    <item>
      <title>On Optimizing Locality of Graph Transposition on Modern Architectures</title>
      <link>https://arxiv.org/abs/2501.06872</link>
      <description>arXiv:2501.06872v1 Announce Type: new 
Abstract: This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.
  Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.
  We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.
  Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06872v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Koohi Esfahani, Hans Vandierendonck</dc:creator>
    </item>
    <item>
      <title>A Simple Lower Bound for Set Agreement in Dynamic Networks</title>
      <link>https://arxiv.org/abs/2501.07036</link>
      <description>arXiv:2501.07036v1 Announce Type: new 
Abstract: Given a positive integer $k$, $k$-set agreement is the distributed task in which each process $i\in [n]$ in a group of $n$ processing nodes starts with an input value $x_i$ in the set $\{0,\dots,k\}$, and must output a value $y_i$ such that (1) for every $i \in [n]$, $y_i$ is the input value of some process, and (2)$|\{y_i : i\in [n]\}|\leq k$. That is, at most $k$ different values in total must be outputted by the processes. The case $k=1$ correspond to (binary) consensus, arguably the most studied problem in distributed computing. While lower bounds for consensus have been obtained for most of the standard distributed computing models, the design of lower bounds for $k$-set agreement with $k&gt;1$ is notoriously known to be much more difficult, and remains open for many models. The main techniques for designing lower bounds for k-set agreement with $k&gt;1$ use tools from algebraic topology.
  The algebraic topology tools are difficult to manipulate, and require a lot of care for avoiding mistakes. This difficulty increases when the communications are mediated by a network of arbitrary structure. Recently, the KNOWALL model has been specifically designed as a first attempt to understand the LOCAL model through the lens of algebraic topology, and Casta\~neda et al.(2021) have designed lower bounds for $k$-set agreement in the KNOWALL model, with applications to dynamic networks.
  In this work, we re-prove the same lower bound for $k$-set agreement in the KNOWALL model. This new proof stands out in its simplicity, which makes it accessible to a broader audience, and increases confidence in the result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07036v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Fraigniaud, Minh Hang Nguyen, Ami Paz</dc:creator>
    </item>
    <item>
      <title>Generating Data Locality to Accelerate Sparse Matrix-Matrix Multiplication on CPUs</title>
      <link>https://arxiv.org/abs/2501.07056</link>
      <description>arXiv:2501.07056v1 Announce Type: new 
Abstract: Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07056v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini</dc:creator>
    </item>
    <item>
      <title>KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework for Cloud-Assisted Edge Clusters</title>
      <link>https://arxiv.org/abs/2501.07130</link>
      <description>arXiv:2501.07130v1 Announce Type: new 
Abstract: Edge computing has become critical for enabling latency-sensitive applications, especially when paired with cloud resources to form cloud-assisted edge clusters. However, efficient resource management remains challenging due to edge nodes' limited capacity and unreliable connectivity. This paper introduces KubeDSM, a Kubernetes-based dynamic scheduling and migration framework tailored for cloud-assisted edge environments. KubeDSM addresses the challenges of resource fragmentation, dynamic scheduling, and live migration while ensuring Quality of Service (QoS) for latency-sensitive applications. Unlike Kubernetes' default scheduler, KubeDSM adopts batch scheduling to minimize resource fragmentation and incorporates a live migration mechanism to optimize edge resource utilization. Specifically, KubeDSM facilitates three key operations: intra-edge migration to reduce fragmentation, edge-to-cloud migration during resource shortages, and cloud-to-edge migration when resources become available, thereby increasing the number of pods allocated to the edge. Our results demonstrate that KubeDSM consistently achieves a higher average edge ratio and a lower standard deviation in edge ratios, highlighting its ability to provide more effective and stable scheduling across different deployments. We also explore the impact of migration strategies and Quality of Service (QoS) configurations on the edge ratios achieved by KubeDSM. The findings reveal that enabling migrations significantly enhances the edge ratio by reducing fragmentation. Additionally, KubeDSM's adaptability in respecting QoS requirements while maximizing overall edge ratios is confirmed through different QoS scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07130v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirhossein Pashaeehir, Sina Shariati, Shayan Shafaghi, Manni Moghimi, Mahmoud Momtazpour</dc:creator>
    </item>
    <item>
      <title>Big Atomics</title>
      <link>https://arxiv.org/abs/2501.07503</link>
      <description>arXiv:2501.07503v1 Announce Type: new 
Abstract: In this paper, we give theoretically and practically efficient implementations of Big Atomics, i.e., $k$-word linearizable registers that support the load, store, and compare-and-swap (CAS) operations. While modern hardware supports $k = 1$ and sometimes $k = 2$ (e.g., double-width compare-and-swap in x86), our implementations support arbitrary $k$. Big Atomics are useful in many applications, including atomic manipulation of tuples, version lists, and implementing load-linked/store-conditional (LL/SC). We design fast, lock-free implementations of big atomics based on a novel fast-path-slow-path approach we develop. We then use them to develop an efficient concurrent hash table, as evidence of their utility.
  We experimentally validate the approach by comparing a variety of implementations of big atomics under a variety of workloads (thread counts, load/store ratios, contention, oversubscription, and number of atomics). The experiments compare two of our lock-free variants with C++ std::atomic, a lock-based version, a version using sequence locks, and an indirect version. The results show that our approach is close to the fastest under all conditions and far outperforms others under oversubscription. We also compare our big atomics based concurrent hash table to a variety of other state-of-the-art hash tables that support arbitrary length keys and values, including implementations from Intel's TBB, Facebook's Folly, libcuckoo, and a recent release from Boost. The results show that our approach of using big atomics in the design of hash tables is a promising direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07503v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Anderson, Guy E. Blelloch, Siddhartha Jayanti</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient, 2D Parallel Stochastic Gradient Descent for Distributed-Memory Optimization</title>
      <link>https://arxiv.org/abs/2501.07526</link>
      <description>arXiv:2501.07526v1 Announce Type: new 
Abstract: Distributed-memory implementations of numerical optimization algorithm, such as stochastic gradient descent (SGD), require interprocessor communication at every iteration of the algorithm. On modern distributed-memory clusters where communication is more expensive than computation, the scalability and performance of these algorithms are limited by communication cost. This work generalizes prior work on 1D $s$-step SGD and 1D Federated SGD with Averaging (FedAvg) to yield a 2D parallel SGD method (HybridSGD) which attains a continuous performance trade off between the two baseline algorithms. We present theoretical analysis which show the convergence, computation, communication, and memory trade offs between $s$-step SGD, FedAvg, 2D parallel SGD, and other parallel SGD variants. We implement all algorithms in C++ and MPI and evaluate their performance on a Cray EX supercomputing system. Our empirical results show that HybridSGD achieves better convergence than FedAvg at similar processor scales while attaining speedups of $5.3\times$ over $s$-step SGD and speedups up to $121\times$ over FedAvg when used to solve binary classification tasks using the convex, logistic regression model on datasets obtained from the LIBSVM repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07526v1</guid>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Devarakonda, Ramakrishnan Kannan</dc:creator>
    </item>
    <item>
      <title>Intelligent Task Offloading: Advanced MEC Task Offloading and Resource Management in 5G Networks</title>
      <link>https://arxiv.org/abs/2501.06242</link>
      <description>arXiv:2501.06242v1 Announce Type: cross 
Abstract: 5G technology enhances industries with high-speed, reliable, low-latency communication, revolutionizing mobile broadband and supporting massive IoT connectivity. With the increasing complexity of applications on User Equipment (UE), offloading resource-intensive tasks to robust servers is essential for improving latency and speed. The 3GPP's Multi-access Edge Computing (MEC) framework addresses this challenge by processing tasks closer to the user, highlighting the need for an intelligent controller to optimize task offloading and resource allocation. This paper introduces a novel methodology to efficiently allocate both communication and computational resources among individual UEs. Our approach integrates two critical 5G service imperatives: Ultra-Reliable Low Latency Communication (URLLC) and Massive Machine Type Communication (mMTC), embedding them into the decision-making framework. Central to this approach is the utilization of Proximal Policy Optimization, providing a robust and efficient solution to the challenges posed by the evolving landscape of 5G technology. The proposed model is evaluated in a simulated 5G MEC environment. The model significantly reduces processing time by 4% for URLLC users under strict latency constraints and decreases power consumption by 26% for mMTC users, compared to existing baseline models based on the reported simulation results. These improvements showcase the model's adaptability and superior performance in meeting diverse QoS requirements in 5G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06242v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Ebrahimi, Fatemeh Afghah</dc:creator>
    </item>
    <item>
      <title>Stingray: Fast Concurrent Transactions Without Consensus</title>
      <link>https://arxiv.org/abs/2501.06531</link>
      <description>arXiv:2501.06531v1 Announce Type: cross 
Abstract: Recent advances have improved the throughput and latency of blockchains by processing transactions accessing different parts of the state concurrently. However, these systems are unable to concurrently process (a) transactions accessing the same state, even if they are (almost) commutative, e.g., payments much smaller than an account's balance, and (b) multi-party transactions, e.g., asset swaps. Moreover, they are slow to recover from contention, requiring once-in-a-day synchronization. We present Stingray, a novel blockchain architecture that addresses these limitations. The key conceptual contributions are a replicated bounded counter that processes (almost) commutative transactions concurrently, and a FastUnlock protocol that uses a fallback consensus protocol for fast contention recovery. We prove Stingray's security in an asynchronous network with Byzantine faults and demonstrate on a global testbed that Stingray achieves 10,000 times the throughput of prior systems for commutative workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06531v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srivatsan Sridhar, Alberto Sonnino, Lefteris Kokoris-Kogias</dc:creator>
    </item>
    <item>
      <title>Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping</title>
      <link>https://arxiv.org/abs/2501.06589</link>
      <description>arXiv:2501.06589v1 Announce Type: cross 
Abstract: Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 30% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06589v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muru Zhang, Mayank Mishra, Zhongzhu Zhou, William Brandon, Jue Wang, Yoon Kim, Jonathan Ragan-Kelley, Shuaiwen Leon Song, Ben Athiwaratkun, Tri Dao</dc:creator>
    </item>
    <item>
      <title>SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split Learning</title>
      <link>https://arxiv.org/abs/2501.06650</link>
      <description>arXiv:2501.06650v1 Announce Type: cross 
Abstract: Split Learning (SL) is a distributed deep learning approach enabling multiple clients and a server to collaboratively train and infer on a shared deep neural network (DNN) without requiring clients to share their private local data. The DNN is partitioned in SL, with most layers residing on the server and a few initial layers and inputs on the client side. This configuration allows resource-constrained clients to participate in training and inference. However, the distributed architecture exposes SL to backdoor attacks, where malicious clients can manipulate local datasets to alter the DNN's behavior. Existing defenses from other distributed frameworks like Federated Learning are not applicable, and there is a lack of effective backdoor defenses specifically designed for SL.
  We present SafeSplit, the first defense against client-side backdoor attacks in Split Learning (SL). SafeSplit enables the server to detect and filter out malicious client behavior by employing circular backward analysis after a client's training is completed, iteratively reverting to a trained checkpoint where the model under examination is found to be benign. It uses a two-fold analysis to identify client-induced changes and detect poisoned models. First, a static analysis in the frequency domain measures the differences in the layer's parameters at the server. Second, a dynamic analysis introduces a novel rotational distance metric that assesses the orientation shifts of the server's layer parameters during training. Our comprehensive evaluation across various data distributions, client counts, and attack scenarios demonstrates the high efficacy of this dual analysis in mitigating backdoor attacks while preserving model utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06650v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2025.241698</arxiv:DOI>
      <dc:creator>Phillip Rieger, Alessandro Pegoraro, Kavita Kumari, Tigist Abera, Jonathan Knauer, Ahmad-Reza Sadeghi</dc:creator>
    </item>
    <item>
      <title>AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds</title>
      <link>https://arxiv.org/abs/2501.06706</link>
      <description>arXiv:2501.06706v1 Announce Type: cross 
Abstract: AI for IT Operations (AIOps) aims to automate complex operational tasks, such as fault localization and root cause analysis, to reduce human workload and minimize customer impact. While traditional DevOps tools and AIOps algorithms often focus on addressing isolated operational tasks, recent advances in Large Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling end-to-end and multitask automation. This paper envisions a future where AI agents autonomously manage operational tasks throughout the entire incident lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps. Realizing this vision requires a comprehensive framework to guide the design, development, and evaluation of these agents. To this end, we present AIOPSLAB, a framework that not only deploys microservice cloud environments, injects faults, generates workloads, and exports telemetry data but also orchestrates these components and provides interfaces for interacting with and evaluating agents. We discuss the key requirements for such a holistic framework and demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps agents. Through evaluations of state-of-the-art LLM agents within the benchmark created by AIOPSLAB, we provide insights into their capabilities and limitations in handling complex operational tasks in cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06706v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SE</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan</dc:creator>
    </item>
    <item>
      <title>COMPASS: A Compiler Framework for Resource-Constrained Crossbar-Array Based In-Memory Deep Learning Accelerators</title>
      <link>https://arxiv.org/abs/2501.06780</link>
      <description>arXiv:2501.06780v1 Announce Type: cross 
Abstract: Recently, crossbar array based in-memory accelerators have been gaining interest due to their high throughput and energy efficiency. While software and compiler support for the in-memory accelerators has also been introduced, they are currently limited to the case where all weights are assumed to be on-chip. This limitation becomes apparent with the significantly increasing network sizes compared to the in-memory footprint.
  Weight replacement schemes are essential to address this issue. We propose COMPASS, a compiler framework for resource-constrained crossbar-based processing-in-memory (PIM) deep neural network (DNN) accelerators. COMPASS is specially targeted for networks that exceed the capacity of PIM crossbar arrays, necessitating access to external memories. We propose an algorithm to determine the optimal partitioning that divides the layers so that each partition can be accelerated on chip. Our scheme takes into account the data dependence between layers, core utilization, and the number of write instructions to minimize latency, memory accesses, and improve energy efficiency. Simulation results demonstrate that COMPASS can accommodate much more networks using a minimal memory footprint, while improving throughput by 1.78X and providing 1.28X savings in energy-delay product (EDP) over baseline partitioning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06780v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihoon Park, Jeongin Choe, Dohyun Kim, Jae-Joon Kim</dc:creator>
    </item>
    <item>
      <title>Beyond Security-by-design: Securing a compromised system</title>
      <link>https://arxiv.org/abs/2501.07207</link>
      <description>arXiv:2501.07207v1 Announce Type: cross 
Abstract: Digital infrastructures are seeing convergence and connectivity at unprecedented scale. This is true for both current critical national infrastructures and emerging future systems that are highly cyber-physical in nature with complex intersections between humans and technologies, e.g., smart cities, intelligent transportation, high-value manufacturing and Industry 4.0. Diverse legacy and non-legacy software systems underpinned by heterogeneous hardware compose on-the-fly to deliver services to millions of users with varying requirements and unpredictable actions. This complexity is compounded by intricate and complicated supply-chains with many digital assets and services outsourced to third parties. The reality is that, at any particular point in time, there will be untrusted, partially-trusted or compromised elements across the infrastructure. Given this reality, and the societal scale of digital infrastructures, delivering secure and resilient operations is a major challenge. We argue that this requires us to move beyond the paradigm of security-by-design and embrace the challenge of securing-a-compromised-system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07207v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, Neeraj Suri</dc:creator>
    </item>
    <item>
      <title>Union: A Trust-minimized Bridge for Bitcoin</title>
      <link>https://arxiv.org/abs/2501.07435</link>
      <description>arXiv:2501.07435v1 Announce Type: cross 
Abstract: We present Union, a trust-minimized bridge protocol that enables secure transfer of BTC between Bitcoin and a secondary blockchain. The growing ecosystem of blockchain systems built around Bitcoin has created a pressing need for secure and efficient bridges to transfer BTC between networks while preserving Bitcoin's security guarantees. Union employs a multi-party variant of BitVMX, an optimistic proving system on Bitcoin, to create a bridge that operates securely under the assumption that at least one participant remains honest. This 1-of-n honest approach is strikingly different from the conventional honest-majority assumption adopted by practically all federated systems. The protocol introduces several innovations: a packet-based architecture that allows security bonds to be reused for multiple bridge operations, improving capital efficiency; a system of enablers to manage functionaries participation and to enforce penalties; a flexible light client framework adaptable to various blockchain architectures; and an efficient stop watch mechanism to optimize time-lock management. Union is a practical and scalable solution for Bitcoin interoperability that maintains strong security guarantees and minimizes trust assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07435v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramon Amela (Rootstock Labs), Shreemoy Mishra (Rootstock Labs), Sergio Demian Lerner (Rootstock Labs, Fairgate Labs), Javier \'Alvarez Cid-Fuentes (Rootstock Labs)</dc:creator>
    </item>
    <item>
      <title>Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design</title>
      <link>https://arxiv.org/abs/2409.01990</link>
      <description>arXiv:2409.01990v3 Announce Type: replace 
Abstract: This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at \url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01990v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Liu, Yanxuan Yu, Zhixin Lai, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu</dc:creator>
    </item>
    <item>
      <title>Geometric Freeze-Tag Problem</title>
      <link>https://arxiv.org/abs/2412.19706</link>
      <description>arXiv:2412.19706v3 Announce Type: replace 
Abstract: We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02), where the objective is to activate a group of n robots, starting from a single initially active robot. Robots are positioned in $\mathbb{R}^d$, and once activated, they move at a constant speed to wake up others. The goal is to minimize the time required to activate the last robot, known as the makespan. We establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms in $\mathbb{R}^2$ and $\mathbb{R}^3$. Specifically, we improve the previous upper bound for $(\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24) to $5.064r$. For $(\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$, which translates to $22.52r$ for $(\mathbb{R}^3, l_2)$. Here, $r$ denotes the maximum distance of any robot from the initially active robot under the given norm. To our knowledge, these are the first makespan bounds for FTP in $\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots is not necessarily achieved when robots are equally distributed along the boundary in $(\mathbb{R}^2, l_2)$. We further investigate FTP in $(\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a boundary, providing insights into practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19706v3</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sharareh Alipour, Kajal Baghestani, Mahdis Mirzaei, Soroush Sahraei</dc:creator>
    </item>
    <item>
      <title>Reciprocating Locks</title>
      <link>https://arxiv.org/abs/2501.02380</link>
      <description>arXiv:2501.02380v2 Announce Type: replace 
Abstract: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02380v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dave Dice, Alex Kogan</dc:creator>
    </item>
    <item>
      <title>Semitopology: a topological approach to decentralised collaborative action</title>
      <link>https://arxiv.org/abs/2303.09287</link>
      <description>arXiv:2303.09287v5 Announce Type: replace-cross 
Abstract: We introduce semitopology, a generalisation of point-set topology that removes the restriction that intersections of open sets need necessarily be open. The intuition is that points represent participants in a decentralised system, and open sets represent collections of participants that collectively have the authority to collaborate to update their local state; we call this an actionable coalition.
  Examples of actionable coalition include: majority stakes in proof-of-stake blockchains; communicating peers in peer-to-peer networks; and even pedestrians working together to not bump into one another in the street. Where actionable coalitions exist, they have in common that: collaborations are local (updating the states of the participants in the coalition, but not immediately those of the whole system); collaborations are voluntary (up to and including breaking rules); participants may be heterogeneous in their computing power or in their goals (not all pedestrians want to go to the same place); participants can choose with whom to collaborate; and they are not assumed subject to permission or synchronisation by a central authority.
  We develop a topology-flavoured mathematics that goes some way to explaining how and why these complex decentralised systems can exhibit order, and gives us new ways to understand existing practical implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09287v5</guid>
      <category>cs.LO</category>
      <category>cs.DC</category>
      <category>math.GN</category>
      <category>math.LO</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murdoch Gabbay</dc:creator>
    </item>
    <item>
      <title>PraFFL: A Preference-Aware Scheme in Fair Federated Learning</title>
      <link>https://arxiv.org/abs/2404.08973</link>
      <description>arXiv:2404.08973v3 Announce Type: replace-cross 
Abstract: Fairness in federated learning has emerged as a critical concern, aiming to develop an unbiased model among groups (e.g., male or female) of diverse sensitive features. However, there is a trade-off between model performance and fairness, i.e., improving model fairness will decrease model performance. Existing approaches have characterized such a trade-off by introducing hyperparameters to quantify client's preferences for model fairness and model performance. Nevertheless, these approaches are limited to scenarios where each client has only a single pre-defined preference, and fail to work in practical systems where each client generally has multiple preferences. To this end, we propose a Preference-aware scheme in Fair Federated Learning (called PraFFL) to generate preference-specific models in real time. PraFFL can adaptively adjust the model based on each client's preferences to meet their needs. We theoretically prove that PraFFL can offer the optimal model tailored to an arbitrary preference of each client, and show its linear convergence. Experimental results show that our proposed PraFFL outperforms six fair federated learning algorithms in terms of the model's capability of adapting to clients' different preferences. Our implementation is available at https://github.com/rG223/PraFFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08973v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongguang Ye, Wei-Bin Kou, Ming Tang</dc:creator>
    </item>
    <item>
      <title>GraphSnapShot: Caching Local Structure for Fast Graph Learning</title>
      <link>https://arxiv.org/abs/2406.17918</link>
      <description>arXiv:2406.17918v4 Announce Type: replace-cross 
Abstract: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.
  The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17918v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman</dc:creator>
    </item>
    <item>
      <title>Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs</title>
      <link>https://arxiv.org/abs/2407.12117</link>
      <description>arXiv:2407.12117v2 Announce Type: replace-cross 
Abstract: Nowadays, Large Language Models (LLMs) have been trained using extended context lengths to foster more creative applications. However, long context training poses great challenges considering the constraint of GPU memory. It not only leads to substantial activation memory consumption during training, but also incurs considerable memory fragmentation. To facilitate long context training, existing frameworks have adopted strategies such as recomputation and various forms of parallelisms. Nevertheless, these techniques rely on redundant computation or extensive communication, resulting in low Model FLOPS Utilization (MFU). In this paper, we propose MEMO, a novel LLM training framework designed for fine-grained activation memory management. Given the quadratic scaling of computation and linear scaling of memory with sequence lengths when using FlashAttention, we offload memory-consuming activations to CPU memory after each layer's forward pass and fetch them during the backward pass. To maximize the swapping of activations without hindering computation, and to avoid exhausting limited CPU memory, we implement a token-wise activation recomputation and swapping mechanism. Furthermore, we tackle the memory fragmentation issue by employing a bi-level Mixed Integer Programming (MIP) approach, optimizing memory reuse across transformer layers. Empirical results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is attributed to MEMO's ability to minimize memory fragmentation, reduce recomputation and intensive communication, and circumvent the delays associated with the memory reorganization process due to fragmentation. By leveraging fine-grained activation memory management, MEMO facilitates efficient training of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU of 52.30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12117v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3709703</arxiv:DOI>
      <dc:creator>Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui</dc:creator>
    </item>
    <item>
      <title>AgileDART: An Agile and Scalable Edge Stream Processing Engine</title>
      <link>https://arxiv.org/abs/2407.14953</link>
      <description>arXiv:2407.14953v2 Announce Type: replace-cross 
Abstract: Edge applications generate a large influx of sensor data on massive scales, and these massive data streams must be processed shortly to derive actionable intelligence. However, traditional data processing systems are not well-suited for these edge applications as they often do not scale well with a large number of concurrent stream queries, do not support low-latency processing under limited edge computing resources, and do not adapt to the level of heterogeneity and dynamicity commonly present in edge computing environments. As such, we present AgileDart, an agile and scalable edge stream processing engine that enables fast stream processing of many concurrently running low-latency edge applications' queries at scale in dynamic, heterogeneous edge environments. The novelty of our work lies in a dynamic dataflow abstraction that leverages distributed hash table-based peer-to-peer overlay networks to autonomously place, chain, and scale stream operators to reduce query latencies, adapt to workload variations, and recover from failures and a bandit-based path planning model that re-plans the data shuffling paths to adapt to unreliable and heterogeneous edge networks. We show that AgileDart outperforms Storm and EdgeWise on query latency and significantly improves scalability and adaptability when processing many real-world edge stream applications' queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14953v2</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2025.3526143</arxiv:DOI>
      <dc:creator>Cheng-Wei Ching, Xin Chen, Chaeeun Kim, Tongze Wang, Dong Chen, Dilma Da Silva, Liting Hu</dc:creator>
    </item>
    <item>
      <title>On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients</title>
      <link>https://arxiv.org/abs/2411.07959</link>
      <description>arXiv:2411.07959v2 Announce Type: replace-cross 
Abstract: The holy grail of machine learning is to enable Continual Federated Learning (CFL) to enhance the efficiency, privacy, and scalability of AI systems while learning from streaming data. The primary challenge of a CFL system is to overcome global catastrophic forgetting, wherein the accuracy of the global model trained on new tasks declines on the old tasks. In this work, we propose Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel replay-memory based federated strategy consisting of edge-based gradient updates on memory and aggregated gradients on the current data. We provide convergence analysis of the C-FLAG approach which addresses forgetting and bias while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We formulate an optimization sub-problem that minimizes catastrophic forgetting, translating CFL into an iterative algorithm with adaptive learning rates that ensure seamless learning across tasks. We empirically show that C-FLAG outperforms several state-of-the-art baselines on both task and class-incremental settings with respect to metrics such as accuracy and forgetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07959v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satish Kumar Keshri, Nazreen Shah, Ranjitha Prasad</dc:creator>
    </item>
    <item>
      <title>The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications</title>
      <link>https://arxiv.org/abs/2411.19714</link>
      <description>arXiv:2411.19714v2 Announce Type: replace-cross 
Abstract: As urban populations grow, cities are becoming more complex, driving the deployment of interconnected sensing systems to realize the vision of smart cities. These systems aim to improve safety, mobility, and quality of life through applications that integrate diverse sensors with real-time decision-making. Streetscape applications-focusing on challenges like pedestrian safety and adaptive traffic management-depend on managing distributed, heterogeneous sensor data, aligning information across time and space, and enabling real-time processing. These tasks are inherently complex and often difficult to scale. The Streetscape Application Services Stack (SASS) addresses these challenges with three core services: multimodal data synchronization, spatiotemporal data fusion, and distributed edge computing. By structuring these capabilities as clear, composable abstractions with clear semantics, SASS allows developers to scale streetscape applications efficiently while minimizing the complexity of multimodal integration.
  We evaluated SASS in two real-world testbed environments: a controlled parking lot and an urban intersection in a major U.S. city. These testbeds allowed us to test SASS under diverse conditions, demonstrating its practical applicability. The Multimodal Data Synchronization service reduced temporal misalignment errors by 88%, achieving synchronization accuracy within 50 milliseconds. Spatiotemporal Data Fusion service improved detection accuracy for pedestrians and vehicles by over 10%, leveraging multicamera integration. The Distributed Edge Computing service increased system throughput by more than an order of magnitude. Together, these results show how SASS provides the abstractions and performance needed to support real-time, scalable urban applications, bridging the gap between sensing infrastructure and actionable streetscape intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19714v2</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navid Salami Pargoo, Mahshid Ghasemi, Shuren Xia, Mehmet Kerem Turkcan, Taqiya Ehsan, Chengbo Zang, Yuan Sun, Javad Ghaderi, Gil Zussman, Zoran Kostic, Jorge Ortiz</dc:creator>
    </item>
    <item>
      <title>Lightweight Federated Learning with Differential Privacy and Straggler Resilience</title>
      <link>https://arxiv.org/abs/2412.06120</link>
      <description>arXiv:2412.06120v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training through model parameter exchanges instead of raw data. To avoid potential inference attacks from exchanged parameters, differential privacy (DP) offers rigorous guarantee against various attacks. However, conventional methods of ensuring DP by adding local noise alone often result in low training accuracy. Combining secure multi-party computation (SMPC) with DP, while improving the accuracy, incurs high communication and computation overheads as well as straggler vulnerability, in either client-to-server or client-to-client links. In this paper, we propose LightDP-FL, a novel lightweight scheme that ensures provable DP against untrusted peers and server, while maintaining straggler resilience, low overheads and high training accuracy. Our scheme incorporates both individual and pairwise noise into each client's parameter, which can be implemented with minimal overheads. Given the uncertain straggler and colluder sets, we utilize the upper bound on the numbers of stragglers and colluders to prove sufficient noise variance conditions to ensure DP in the worst case. Moreover, we optimize the expected convergence bound to ensure accuracy performance by flexibly controlling the noise variances. Using the CIFAR-10 dataset, our experimental results demonstrate that LightDP-FL achieves faster convergence and stronger straggler resilience compared to baseline methods of the same DP level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06120v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Hong, Xiaojun Lin, Lingjie Duan</dc:creator>
    </item>
    <item>
      <title>Scam Detection for Ethereum Smart Contracts: Leveraging Graph Representation Learning for Secure Blockchain</title>
      <link>https://arxiv.org/abs/2412.12370</link>
      <description>arXiv:2412.12370v3 Announce Type: replace-cross 
Abstract: Due to the increasing abuse of fraudulent activities that result in significant financial and reputational harm, Ethereum smart contracts face a significant problem in detecting fraud. Existing monitoring methods typically rely on lease code analysis or physically extracted features, which suffer from scalability and adaptability limitations. In this study, we use graph representation learning to observe purchase trends and find fraudulent deals. We can achieve powerful categorisation performance by using innovative machine learning versions and transforming Ethereum invoice data into graph structures. Our method addresses label imbalance through SMOTE-ENN techniques and evaluates models like Multi-Layer Perceptron ( MLP ) and Graph Convolutional Networks ( GCN). Experimental results show that the MLP type surpasses the GCN in this environment, with domain-specific assessments closely aligned with real-world assessments. This study provides a scalable and efficient way to improve Ethereum's ecosystem's confidence and security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12370v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Jin, Ze Yang</dc:creator>
    </item>
  </channel>
</rss>
