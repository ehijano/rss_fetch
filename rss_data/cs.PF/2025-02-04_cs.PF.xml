<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 02:48:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>PerfSeer: An Efficient and Accurate Deep Learning Models Performance Predictor</title>
      <link>https://arxiv.org/abs/2502.01206</link>
      <description>arXiv:2502.01206v1 Announce Type: new 
Abstract: Predicting the performance of deep learning (DL) models, such as execution time and resource utilization, is crucial for Neural Architecture Search (NAS), DL cluster schedulers, and other technologies that advance deep learning. The representation of a model is the foundation for its performance prediction. However, existing methods cannot comprehensively represent diverse model configurations, resulting in unsatisfactory accuracy. To address this, we represent a model as a graph that includes the topology, along with the node, edge, and global features, all of which are crucial for effectively capturing the performance of the model. Based on this representation, we propose PerfSeer, a novel predictor that uses a Graph Neural Network (GNN)-based performance prediction model, SeerNet. SeerNet fully leverages the topology and various features, while incorporating optimizations such as Synergistic Max-Mean aggregation (SynMM) and Global-Node Perspective Boost (GNPB) to capture the critical performance information more effectively, enabling it to predict the performance of models accurately. Furthermore, SeerNet can be extended to SeerNet-Multi by using Project Conflicting Gradients (PCGrad), enabling efficient simultaneous prediction of multiple performance metrics without significantly affecting accuracy. We constructed a dataset containing performance metrics for 53k+ model configurations, including execution time, memory usage, and Streaming Multiprocessor (SM) utilization during both training and inference. The evaluation results show that PerfSeer outperforms nn-Meter, Brp-NAS, and DIPPM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01206v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinlong Zhao, Jiande Sun, Jia Zhang, Sujuan Hou, Shuai Li, Tong Liu, Ke Liu</dc:creator>
    </item>
    <item>
      <title>PixelBrax: Learning Continuous Control from Pixels End-to-End on the GPU</title>
      <link>https://arxiv.org/abs/2502.00021</link>
      <description>arXiv:2502.00021v1 Announce Type: cross 
Abstract: We present PixelBrax, a set of continuous control tasks with pixel observations. We combine the Brax physics engine with a pure JAX renderer, allowing reinforcement learning (RL) experiments to run end-to-end on the GPU. PixelBrax can render observations over thousands of parallel environments and can run two orders of magnitude faster than existing benchmarks that rely on CPU-based rendering. Additionally, PixelBrax supports fully reproducible experiments through its explicit handling of any stochasticity within the environments and supports color and video distractors for benchmarking generalization. We open-source PixelBrax alongside JAX implementations of several RL algorithms at github.com/trevormcinroe/pixelbrax.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00021v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trevor McInroe, Samuel Garcin</dc:creator>
    </item>
    <item>
      <title>An Investigation of FP8 Across Accelerators for LLM Inference</title>
      <link>https://arxiv.org/abs/2502.01070</link>
      <description>arXiv:2502.01070v1 Announce Type: cross 
Abstract: The introduction of 8-bit floating-point (FP8) computation units in modern AI accelerators has generated significant interest in FP8-based large language model (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep learning requires a shared scaling factor. Additionally, while E4M3 and E5M2 are well-defined at the individual value level, their scaling and accumulation methods remain unspecified and vary across hardware and software implementations. As a result, FP8 behaves more like a quantization format than a standard numeric representation. In this work, we provide the first comprehensive analysis of FP8 computation and acceleration on two AI accelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that the Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency during LLM inference, offering valuable insights into the practical implications of FP8 adoption for datacenter-scale LLM serving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01070v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwoo Kim, Joonhyung Lee, Gunho Park, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee, Youngjoo Lee</dc:creator>
    </item>
    <item>
      <title>Asymmetries of Service: Interdependence and Synchronicity</title>
      <link>https://arxiv.org/abs/2402.15533</link>
      <description>arXiv:2402.15533v2 Announce Type: replace-cross 
Abstract: On many dimensions, services can be seen to exist along spectra measuring the degree of interaction between customer and agent. For instance, every interaction features some number of contributions by each of those two sides, creating a spectrum of interdependence. Additionally, each interaction is further characterized by the relative pacing of these contributions, implying a spectrum of synchronicity. Where a service falls on such spectra can be a consequence of its design, but it can also be a function of its state. For instance, as broadly evidenced empirically, an agent with several concurrent interactions will be slowed in each individual interaction, altering the service's synchronicity. Here, we study a Hawkes cluster model of the service interaction, which we show captures the interdependence and synchronicity spectra and their resulting customer-agent (a)symmetries. We find insightful connections to behavioral operations, such as proving the occurrence of non-monotonic performance (e.g., inverted-U throughput) from concurrency-driven asynchrony. Hence, we can prescribe the agent's optimal concurrency level. Furthermore, we show how the service design dictates the efficacy of these operational improvements, proving that the concurrency-optimized throughput is itself non-monotonic as a function of the interdependence. Of possible independent interest methodologically, we establish an interpretable temporal decomposition for Hawkes clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15533v2</guid>
      <category>cs.GT</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Daw, Galit B. Yom-Tov</dc:creator>
    </item>
    <item>
      <title>Computational Complexity-Constrained Spectral Efficiency Analysis for 6G Waveforms</title>
      <link>https://arxiv.org/abs/2407.05805</link>
      <description>arXiv:2407.05805v3 Announce Type: replace-cross 
Abstract: In this work, we present a tutorial on how to account for the computational time complexity overhead of signal processing in the spectral efficiency (SE) analysis of wireless waveforms. Our methodology is particularly relevant in scenarios where achieving higher SE entails a penalty in complexity, a common trade-off present in 6G candidate waveforms. We consider that SE derives from the data rate, which is impacted by time-dependent overheads. Thus, neglecting the computational complexity overhead in the SE analysis grants an unfair advantage to more computationally complex waveforms, as they require larger computational resources to meet a signal processing runtime below the symbol period. We demonstrate our points with two case studies. In the first, we refer to IEEE 802.11a-compliant baseband processors from the literature to show that their runtime significantly impacts the SE perceived by upper layers. In the second case study, we show that waveforms considered less efficient in terms of SE can outperform their more computationally expensive counterparts if provided with equivalent high-performance computational resources. Based on these cases, we believe our tutorial can address the comparative SE analysis of waveforms that operate under different computational resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05805v3</guid>
      <category>eess.SP</category>
      <category>cs.CC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saulo Queiroz, Jo\~ao P. Vilela, Benjamin Koon Kei Ng, Chan-Tong Lam, Edmundo Monteiro</dc:creator>
    </item>
  </channel>
</rss>
