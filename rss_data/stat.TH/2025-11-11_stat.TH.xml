<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 13:39:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model</title>
      <link>https://arxiv.org/abs/2511.06040</link>
      <description>arXiv:2511.06040v1 Announce Type: new 
Abstract: We study the computational task of detecting and estimating correlated signals in a pair of spiked Wigner matrices. Our model consists of observations
  $$
  X = \tfrac{\lambda}{\sqrt{n}} xx^{\top} + W \,, \quad Y = \tfrac{\mu}{\sqrt{n}} yy^{\top} + Z \,.
  $$
  where $x,y \in \mathbb R^n$ are signal vectors with norm $\|x\|,\|y\| \approx\sqrt{n}$ and correlation $\langle x,y \rangle \approx \rho\|x\|\|y\|$, while $W,Z$ are independent Gaussian noise matrices. We propose an efficient algorithm that succeeds whenever $F(\lambda,\mu,\rho)&gt;1$, where
  $$
  F(\lambda,\mu,\rho)=\max\Big\{ \lambda,\mu, \frac{ \lambda^2 \rho^2 }{ 1-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ 1-\mu^2+\mu^2 \rho^2 } \Big\} \,.
  $$
  Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be computationally infeasible.
  We complement our algorithmic result with evidence for a matching computational lower bound. In particular, we prove that when $F(\lambda,\mu,\rho)&lt;1$, all algorithms based on {\em low-degree polynomials} fails to distinguish $(X,Y)$ with two independent Wigner matrices. This low-degree analysis strongly suggests that $F(\lambda,\mu,\rho)=1$ is the precise computation threshold for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06040v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>An ordering for the strength of functional dependence</title>
      <link>https://arxiv.org/abs/2511.06498</link>
      <description>arXiv:2511.06498v1 Announce Type: new 
Abstract: We introduce a new dependence order that satisfies eight natural axioms that we propose for a global dependence order. Its minimal and maximal elements characterize independence and perfect dependence. Moreover, it characterizes conditional independence, satisfies information monotonicity, and exhibits several invariance properties. Consequently,it is an ordering for the strength of functional dependence of a random variable Y on a random vector X. As we show, various dependence measures, such as Chatterjee's rank correlation, are increasing in this order. We characterize our ordering by the Schur order and by the concordance order, and we verify it in models such as the additive error model, the multivariate normal distribution, and various copula-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06498v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>Bernstein-von Mises for Adaptively Collected Data</title>
      <link>https://arxiv.org/abs/2511.06639</link>
      <description>arXiv:2511.06639v1 Announce Type: new 
Abstract: Uncertainty quantification (UQ) for adaptively collected data, such as that coming from adaptive experiments, bandits, or reinforcement learning, is necessary for critical elements of data collection such as ensuring safety and conducting after-study inference. The data's adaptivity creates significant challenges for frequentist UQ, yet Bayesian UQ remains the same as if the data were independent and identically distributed (i.i.d.), making it an appealing and commonly used approach. Bayesian UQ requires the (correct) specification of a prior distribution while frequentist UQ does not, but for i.i.d. data the celebrated Bernstein-von Mises theorem shows that as the sample size grows, the prior 'washes out' and Bayesian UQ becomes frequentist-valid, implying that the choice of prior need not be a major impediment to Bayesian UQ as it makes no difference asymptotically. This paper for the first time extends the Bernstein-von Mises theorem to adaptively collected data, proving asymptotic equivalence between Bayesian UQ and Wald-type frequentist UQ in this challenging setting. Our result showing this asymptotic agreement does not require the standard stability condition required by works studying validity of Wald-type frequentist UQ; in cases where stability is satisfied, our results combined with these prior studies of frequentist UQ imply frequentist validity of Bayesian UQ. Counterintuitively however, they also provide a negative result that Bayesian UQ is not asymptotically frequentist valid when stability fails, despite the fact that the prior washes out and Bayesian UQ asymptotically matches standard Wald-type frequentist UQ. We empirically validate our theory (positive and negative) via a range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06639v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Du, Yash Nair, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Experimentation Under Non-stationary Interference</title>
      <link>https://arxiv.org/abs/2511.06685</link>
      <description>arXiv:2511.06685v1 Announce Type: new 
Abstract: We study the estimation of the ATE in randomized controlled trials under a dynamically evolving interference structure. This setting arises in applications such as ride-sharing, where drivers move over time, and social networks, where connections continuously form and dissolve. In particular, we focus on scenarios where outcomes exhibit spatio-temporal interference driven by a sequence of random interference graphs that evolve independently of the treatment assignment. Loosely, our main result states that a truncated Horvitz-Thompson estimator achieves an MSE that vanishes linearly in the number of spatial and time blocks, times a factor that measures the average complexity of the interference graphs. As a key technical contribution that contrasts the static setting we present a fine-grained covariance bound for each pair of space-time points that decays exponentially with the time elapsed since their last ``interaction''. Our results can be applied to many concrete settings and lead to simplified bounds, including where the interference graphs (i) are induced by moving points in a metric space, or (ii) follow a dynamic Erdos-Renyi model, where each edge is created or removed independently in each time period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06685v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Su Jia, Peter Frazier, Nathan Kallus, Christina Lee Yu</dc:creator>
    </item>
    <item>
      <title>Asymptotics of higher criticism via Gaussian approximation</title>
      <link>https://arxiv.org/abs/2511.06789</link>
      <description>arXiv:2511.06789v1 Announce Type: new 
Abstract: Higher criticism is a large-scale testing procedure that can attain the optimal detection boundary for sparse and faint signals. However, there has been a lack of knowledge in most existing works about its asymptotic distribution for more realistic settings other than the independent Gaussian assumption while maintaining the power performance as much as possible. In this paper, we develop a unified framework to analyze the asymptotic distributions of the higher criticism statistic and the more general multi-level thresholding statistic when the individual test statistics are dependent $t$-statistics under a finite ($2+\delta$)-th moment condition, $0&lt;\delta\leq1$. The key idea is to approximate the global test statistic by the supremum of an empirical process indexed by a normalized class of indicator or thresholding functions, respectively. A new Gaussian approximation theorem for suprema of empirical processes with dependent observations is established to derive the explicit asymptotic distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06789v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingkun Qiu</dc:creator>
    </item>
    <item>
      <title>Multiscale Change Point Detection for Functional Time Series</title>
      <link>https://arxiv.org/abs/2511.06870</link>
      <description>arXiv:2511.06870v1 Announce Type: new 
Abstract: We study the problem of detecting and localizing multiple changes in the mean parameter of a Banach space-valued time series. The goal is to construct a collection of narrow confidence intervals, each containing at least one (or exactly one) change, with globally controlled error probability. Our approach relies on a new class of weighted scan statistics, called H\"older-type statistics, which allow a smooth trade-off between efficiency (enabling the detection of closely spaced, small changes) and robustness (against heavier tails and stronger dependence). For Gaussian noise, maximum weighting can be applied, leading to a generalization of optimality results known for scalar, independent data. Even for scalar time series, our approach is advantageous, as it accommodates broad classes of dependency structures and non-stationarity. Its primary advantage, however, lies in its applicability to functional time series, where few methods exist and established procedures impose strong restrictions on the spacing and magnitude of changes. We obtain general results by employing new Gaussian approximations for the partial sum process in H\"older spaces. As an application of our general theory, we consider the detection of distributional changes in a data panel. The finite-sample properties and applications to financial datasets further highlight the merits of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06870v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kutta, Holger Dette, Shixuan Wang</dc:creator>
    </item>
    <item>
      <title>Transformed Fr\'echet Means for Robust Estimation in Hadamard Spaces</title>
      <link>https://arxiv.org/abs/2511.06933</link>
      <description>arXiv:2511.06933v1 Announce Type: new 
Abstract: We establish finite-sample error bounds in expectation for transformed Fr\'echet means in Hadamard spaces under minimal assumptions. Transformed Fr\'echet means provide a unifying framework encompassing classical and robust notions of central tendency in metric spaces. Instead of minimizing squared distances as for the classical 2-Fr\'echet mean, we consider transformations of the distance that are nondecreasing, convex, and have a concave derivative. This class spans a continuum between median and classical mean. It includes the Fr\'echet median, power Fr\'echet means, and the (pseudo-)Huber mean, among others. We obtain the parametric rate of convergence under fewer than two moments and a subclass of estimators exhibits a breakdown point of 1/2. Our results apply in general Hadamard spaces-including infinite-dimensional Hilbert spaces and nonpositively curved geometries-and yield new insights even in Euclidean settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06933v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Sch\"otz</dc:creator>
    </item>
    <item>
      <title>Extrapolation Problem for Multidimensional Stationary Sequences with Missing Observations</title>
      <link>https://arxiv.org/abs/2511.07228</link>
      <description>arXiv:2511.07228v1 Announce Type: new 
Abstract: This paper focuses on the problem of the mean square optimal estimation of linear functionals which depend on the unknown values of a multidimensional stationary stochastic sequence.
  Estimates are based on observations of the sequence with an additive stationary noise sequence.
  The aim of the paper is to develop methods of finding the optimal estimates of the functionals in the case of missing observations.
  The problem is investigated in the case of spectral certainty where the spectral densities of the sequences are exactly known.
  Formulas for calculating the mean-square errors and the spectral characteristics of the optimal linear estimates of functionals are derived under the condition of spectral certainty.
  The minimax (robust) method of estimation is applied in the case of spectral uncertainty, where spectral densities of the sequences are not known exactly while sets of admissible spectral densities are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07228v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.19139/soic.v7i1.527</arxiv:DOI>
      <arxiv:journal_reference>Statistics Opt. Inform. Comput., Vol. 7, March 2019, pp 97-117</arxiv:journal_reference>
      <dc:creator>Oleksandr Masyutka, Mikhail Moklyachuk, Maria Sidei</dc:creator>
    </item>
    <item>
      <title>Interpolation Problem for Multidimensional Stationary Processes with Missing Observations</title>
      <link>https://arxiv.org/abs/2511.07240</link>
      <description>arXiv:2511.07240v1 Announce Type: new 
Abstract: The problem of the mean-square optimal linear estimation of linear functionals which depend on the unknown values of a multidimensional continuous time stationary stochastic process is considered. Estimates are based on observations of the process with an additive stationary stochastic noise process at points which do not belong to some finite intervals of a real line. The problem is investigated in the case of spectral certainty, where the spectral densities of the processes are exactly known. Formulas for calculating the mean-square errors and spectral characteristics of the optimal linear estimates of functionals are proposed under the condition of spectral certainty. The minimax (robust) method of estimation is applied in the case spectral uncertainty, where spectral densities of the processes are not known exactly while some sets of admissible spectral densities of the processes are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible spectral densities</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07240v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.19139/soic.v7i1.430</arxiv:DOI>
      <arxiv:journal_reference>Statistics Opt. Inform. Comput., Vol. 7, March 2019, pp 118-132</arxiv:journal_reference>
      <dc:creator>Oleksandr Masyutka, Mikhail Moklyachuk, Maria Sidei</dc:creator>
    </item>
    <item>
      <title>Robust interpolation of sequences with periodically stationary multiplicative seasonal increments</title>
      <link>https://arxiv.org/abs/2511.07254</link>
      <description>arXiv:2511.07254v1 Announce Type: new 
Abstract: We consider stochastic sequences with periodically stationary generalized multiple increments of fractional order which combines cyclostationary, multi-seasonal, integrated and fractionally integrated patterns. We solve the interpolation problem for linear functionals constructed from unobserved values of a stochastic sequence of this type based on observations of the sequence with a periodically stationary noise sequence. For sequences with known matrices of spectral densities, we obtain formulas for calculating values of the mean square errors and the spectral characteristics of the optimal interpolation of the functionals. Formulas that determine the least favorable spectral densities and the minimax (robust) spectral characteristics of the optimal linear interpolation of the functionals are proposed in the case where spectral densities of the sequences are not exactly known while some sets of admissible spectral densities are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07254v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.15330/cmp.14.1.105-126</arxiv:DOI>
      <arxiv:journal_reference>Carpathian Math. Publ. 2022, 14 (1), 105--126</arxiv:journal_reference>
      <dc:creator>Maksym Luz, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Asymptotics of Differentially Private PCA</title>
      <link>https://arxiv.org/abs/2511.07270</link>
      <description>arXiv:2511.07270v1 Announce Type: new 
Abstract: In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\rightarrow\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07270v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngjoo Yun, Rishabh Dudeja</dc:creator>
    </item>
    <item>
      <title>Wasserstein-Cram\'er-Rao Theory of Unbiased Estimation</title>
      <link>https://arxiv.org/abs/2511.07414</link>
      <description>arXiv:2511.07414v1 Announce Type: new 
Abstract: The quantity of interest in the classical Cram\'er-Rao theory of unbiased estimation (e.g., the Cram\'er-Rao lower bound, its exact attainment for exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the "sensitivity" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cram\'er-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and some concrete results that show that the Wasserstein projection estimator achieves the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing entirely new estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07414v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'as Garc\'ia Trillos, Adam Quinn Jaffe, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning</title>
      <link>https://arxiv.org/abs/2511.05593</link>
      <description>arXiv:2511.05593v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables decentralized model training across multiple clients while optionally preserving data privacy. However, communication efficiency remains a critical bottleneck, particularly for large-scale models. In this work, we introduce two complementary algorithms: ProjFL, designed for unbiased compressors, and ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism. Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead. We establish convergence guarantees for both algorithms under strongly convex, convex, and non-convex settings. Empirical evaluations on standard FL classification benchmarks with deep neural networks show that ProjFL and ProjFL+EF achieve accuracy comparable to existing baselines while substantially reducing communication costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05593v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnaud Descours (UCBL), L\'eonard Deroose, Jan Ramon</dc:creator>
    </item>
    <item>
      <title>Minimum bounding polytropes for estimation of max-linear Bayesian networks</title>
      <link>https://arxiv.org/abs/2511.05962</link>
      <description>arXiv:2511.05962v1 Announce Type: cross 
Abstract: Max-linear Bayesian networks are recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). The identifiability and estimation of max-linear Bayesian networks is an intricate issue as Gissibl, Kl\"uppelberg, and Lauritzen have shown. As such, a max-linear Bayesian network is generally unidentifiable and standard likelihood theory cannot be applied. We can associate tropical polyhedra to max-linear Bayesian networks. Using this, we investigate the minimum-ratio estimator proposed by Gissibl, Kl\"uppelberg, and Lauritzen and give insight on the structure of minimal best-case samples for parameter recovery which we describe in terms of set covers of certain triangulations. We also combine previous work on estimating max-linear models from Tran, Buck, and Kl\"uppelberg to apply our geometric approach to the structural inference of max-linear models. This is tested extensively on simulated data and on real world data set, the NHANES report for 2015--2016 and the upper Danube network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05962v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamillo Ferry</dc:creator>
    </item>
    <item>
      <title>How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy</title>
      <link>https://arxiv.org/abs/2511.06044</link>
      <description>arXiv:2511.06044v1 Announce Type: cross 
Abstract: Attention mechanism is a significant part of Transformer models. It helps extract features from embedded vectors by adding global information and its expressivity has been proved to be powerful. Nevertheless, the quadratic complexity restricts its practicability. Although several researches have provided attention mechanism in sparse form, they are lack of theoretical analysis about the expressivity of their mechanism while reducing complexity. In this paper, we put forward Random Batch Attention (RBA), a linear self-attention mechanism, which has theoretical support of the ability to maintain its expressivity. Random Batch Attention has several significant strengths as follows: (1) Random Batch Attention has linear time complexity. Other than this, it can be implemented in parallel on a new dimension, which contributes to much memory saving. (2) Random Batch Attention mechanism can improve most of the existing models by replacing their attention mechanisms, even many previously improved attention mechanisms. (3) Random Batch Attention mechanism has theoretical explanation in convergence, as it comes from Random Batch Methods on computation mathematics. Experiments on large graphs have proved advantages mentioned above. Also, the theoretical modeling of self-attention mechanism is a new tool for future research on attention-mechanism analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06044v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanwen Liu, Yixuan Ma, Shi Jin, Yuguang Wang</dc:creator>
    </item>
    <item>
      <title>Counterfactual Forecasting For Panel Data</title>
      <link>https://arxiv.org/abs/2511.06189</link>
      <description>arXiv:2511.06189v1 Announce Type: cross 
Abstract: We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06189v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navonil Deb, Raaz Dwivedi, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Sparse Linear Regression is Easy on Random Supports</title>
      <link>https://arxiv.org/abs/2511.06211</link>
      <description>arXiv:2511.06211v1 Announce Type: cross 
Abstract: Sparse linear regression is one of the most basic questions in machine learning and statistics. Here, we are given as input a design matrix $X \in \mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$ where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements. Importantly, we have the additional constraint that the unknown signal vector ${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than the ambient dimension. Our goal is to output a prediction vector $\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^* - {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of measurements: under most natural noise distributions, we can get prediction error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples. Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately, with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap (in the dependence on $d$) between the two and we do not know if it is possible to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices ${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d, 1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for random design matrices from well-behaved families, matrices that have a very low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed sensing), or those with special structural properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06211v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gautam Chandrasekaran, Raghu Meka, Konstantinos Stavropoulos</dc:creator>
    </item>
    <item>
      <title>A generalization bound for exit wave reconstruction via deep unfolding</title>
      <link>https://arxiv.org/abs/2511.06413</link>
      <description>arXiv:2511.06413v1 Announce Type: cross 
Abstract: Transmission Electron Microscopy enables high-resolution imaging of materials, but the resulting images are difficult to interpret directly. One way to address this is exit wave reconstruction, i.e., the recovery of the complex-valued electron wave at the specimen's exit plane from intensity-only measurements. This is an inverse problem with a nonlinear forward model. We consider a simplified forward model, making the problem equivalent to phase retrieval, and propose a discretized regularized variational formulation. To solve the resulting non-convex problem, we employ the proximal gradient algorithm (PGA) and unfold its iterations into a neural network, where each layer corresponds to one PGA step with learnable parameters. This unrolling approach, inspired by LISTA, enables improved reconstruction quality, interpretability, and implicit dictionary learning from data. We analyze the effect of parameter perturbations and show that they can accumulate exponentially with the number of layers $L$. Building on proof techniques of Behboodi et al., originally developed for LISTA, i.e., for a linear forward model, we extend the analysis to our nonlinear setting and establish generalization error bounds of order $\mathcal{O}(\sqrt{L})$. Numerical experiments support the exponential growth of parameter perturbations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06413v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moussa Atwi, Benjamin Berkels</dc:creator>
    </item>
    <item>
      <title>Collapsing Categories for Regression with Mixed Predictors</title>
      <link>https://arxiv.org/abs/2511.06542</link>
      <description>arXiv:2511.06542v1 Announce Type: cross 
Abstract: Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06542v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaegeun Song, Zhong Zheng, Bing Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>A Simple and Effective Random Forest Modelling for Nonlinear Time Series Data</title>
      <link>https://arxiv.org/abs/2511.06544</link>
      <description>arXiv:2511.06544v1 Announce Type: cross 
Abstract: In this paper, we propose Random Forests by Random Weights (RF-RW), a theoretically grounded and practically effective alternative RF modelling for nonlinear time series data, where existing RF-based approaches struggle to adequately capture temporal dependence. RF-RW reconciles the strengths of classic RF with the temporal dependence inherent in time series forecasting. Specifically, it avoids the bootstrap resampling procedure, therefore preserves the serial dependence structure, whilst incorporates independent random weights to reduce correlations among trees. We establish non-asymptotic concentration bounds and asymptotic uniform consistency guarantees, for both fixed- and high-dimensional feature spaces, which extend beyond existing theoretical analyses of RF. Extensive simulation studies demonstrate that RF-RW outperforms existing RF-based approaches and other benchmarks such as SVM and LSTM. It also achieves the lowest error among competitors in our real-data example of predicting UK COVID-19 daily cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06544v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shihao Zhang, Zudi Lu, Chao Zheng</dc:creator>
    </item>
    <item>
      <title>A kernel method for the learning of Wasserstein geometric flows</title>
      <link>https://arxiv.org/abs/2511.06655</link>
      <description>arXiv:2511.06655v1 Announce Type: cross 
Abstract: Wasserstein gradient and Hamiltonian flows have emerged as essential tools for modeling complex dynamics in the natural sciences, with applications ranging from partial differential equations (PDEs) and optimal transport to quantum mechanics and information geometry. Despite their significance, the inverse identification of potential functions and interaction kernels underlying these flows remains relatively unexplored. In this work, we tackle this challenge by addressing the inverse problem of simultaneously recovering the potential function and interaction kernel from discretized observations of the density flow. We formulate the problem as an optimization task that minimizes a loss function specifically designed to enforce the underlying variational structure of Wasserstein flows, ensuring consistency with the geometric properties of the density manifold. Our framework employs a kernel-based operator approach using the associated Reproducing Kernel Hilbert Space (RKHS), which provides a closed-form representation of the unknown components. Furthermore, a comprehensive error analysis is conducted, providing convergence rates under adaptive regularization parameters as the temporal and spatial discretization mesh sizes tend to zero. Finally, a stability analysis is presented to bridge the gap between discrete trajectory data and continuous-time flow dynamics for the Wasserstein Hamiltonian flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06655v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianyu Hu, Juan-Pablo Ortega, Daiying Yin</dc:creator>
    </item>
    <item>
      <title>Integral-Operator-Based Spectral Algorithms for Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2511.06718</link>
      <description>arXiv:2511.06718v1 Announce Type: cross 
Abstract: The widespread adoption of the \emph{maximum mean discrepancy} (MMD) in goodness-of-fit testing has spurred extensive research on its statistical performance. However, recent studies indicate that the inherent structure of MMD may constrain its ability to distinguish between distributions, leaving room for improvement. Regularization techniques have the potential to overcome this limitation by refining the discrepancy measure. In this paper, we introduce a family of regularized kernel-based discrepancy measures constructed via spectral filtering. Our framework can be regarded as a natural generalization of prior studies, removing restrictive assumptions on both kernel functions and filter functions, thereby broadening the methodological scope and the theoretical inclusiveness. We establish non-asymptotic guarantees showing that the resulting tests achieve valid Type~I error control and enhanced power performance. Numerical experiments are conducted to demonstrate the broader generality and competitive performance of the proposed tests compared with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06718v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Sang, Shao-Bo Lin, Xuehu Zhu</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Updates via Harmonic Representations</title>
      <link>https://arxiv.org/abs/2511.06978</link>
      <description>arXiv:2511.06978v1 Announce Type: cross 
Abstract: Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06978v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Zhang</dc:creator>
    </item>
    <item>
      <title>Unlocking the Regression Space</title>
      <link>https://arxiv.org/abs/2511.07183</link>
      <description>arXiv:2511.07183v1 Announce Type: cross 
Abstract: This paper introduces and analyzes a framework that accommodates general heterogeneity in regression modeling. It demonstrates that regression models with fixed or time-varying parameters can be estimated using the OLS and time-varying OLS methods, respectively, across a broad class of regressors and noise processes not covered by existing theory. The proposed setting facilitates the development of asymptotic theory and the estimation of robust standard errors. The robust confidence interval estimators accommodate substantial heterogeneity in both regressors and noise. The resulting robust standard error estimates coincide with White's (1980) heteroskedasticity-consistent estimator but are applicable to a broader range of conditions, including models with missing data. They are computationally simple and perform well in Monte Carlo simulations. Their robustness, generality, and ease of implementation make them highly suitable for empirical applications. Finally, the paper provides a brief empirical illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07183v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liudas Giraitis, George Kapetanios, Yufei Li, Alexia Ventouri</dc:creator>
    </item>
    <item>
      <title>Comparison results for positive supermodular dependent Markov tree distributions</title>
      <link>https://arxiv.org/abs/2404.17441</link>
      <description>arXiv:2404.17441v3 Announce Type: replace 
Abstract: Positive dependencies have been compared in the literature under rather strong assumptions such as equality of conditional distributions, exchangeability, or stationarity. We establish supermodular ordering results for distributions that are Markov with respect to a tree structure. Our comparison results rely on simple stochastic monotonicity conditions and a pointwise ordering of bivariate copulas associated with the edges of the underlying tree. We also study flexibility of the marginal distributions in stochastic and convex order. As a consequence, we obtain first- and second-order stochastic dominance esults for extreme order statistics and sums of positively dependent random variables. As an application, we investigate distributional robustness of the maximum of a perturbed random walk under model uncertainty. Several examples and a detailed discussion of the assumptions demonstrate the generality of our results and reveal deeper insights into non-intuitive positive dependence properties of multidimensional distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17441v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/25-EJS2465</arxiv:DOI>
      <arxiv:journal_reference>Electron. J. Statist. 19(2): 5409-5456 (2025)</arxiv:journal_reference>
      <dc:creator>Jonathan Ansari, Moritz Ritter</dc:creator>
    </item>
    <item>
      <title>Resolution of the Borel-Kolmogorov Paradox via the Maximum Entropy Principle</title>
      <link>https://arxiv.org/abs/2509.24735</link>
      <description>arXiv:2509.24735v3 Announce Type: replace 
Abstract: This paper presents a rigorous resolution of the Borel-Kolmogorov paradox using the Maximum Entropy Principle. We construct a metric-based framework for Bayesian inference that uniquely extends conditional probability to events of null measure. The results unify classical Bayes' rules and provide a robust foundation for Bayesian inference in metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24735v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Tr\'esor, Mykola Lukashchuk</dc:creator>
    </item>
    <item>
      <title>Sequential Change Detection Under A Markov Setup With Unknown Pre-Change and Post-Change Distributions</title>
      <link>https://arxiv.org/abs/2510.26204</link>
      <description>arXiv:2510.26204v2 Announce Type: replace 
Abstract: In this work we extend the results developed in 2022 for a sequential change detection algorithm making use of Page's CUSUM statistic, the empirical distribution as an estimate of the pre-change distribution, and a universal code as a tool for estimating the post-change distribution, from the i.i.d. case to the Markov setup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26204v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Bhoopesh Gulaguli, Shashwat Singh, Rakesh Kumar Bansal</dc:creator>
    </item>
    <item>
      <title>A Generalized Back-Door Criterion for Linear Regression</title>
      <link>https://arxiv.org/abs/2511.04060</link>
      <description>arXiv:2511.04060v2 Announce Type: replace 
Abstract: What assumptions about the data-generating process are required to permit a causal interpretation of partial regression coefficients? To answer this question, this paper generalizes Pearl's single-door and back-door criteria and proposes a new criterion that enables the identification of total or partial causal effects. In addition, this paper elucidates the mechanism of post-treatment bias, showing that a repeated sequence of nodes can be a potential source of this bias. The results apply to linear data-generating processes represented by directed acyclic graphs with distribution-free error terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04060v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masato Shimokawa</dc:creator>
    </item>
    <item>
      <title>Asymptotics for Reinforced Stochastic Processes on Hierarchical Networks</title>
      <link>https://arxiv.org/abs/2511.04562</link>
      <description>arXiv:2511.04562v2 Announce Type: replace 
Abstract: In this paper, we analyze the asymptotic behavior of a system of interacting reinforced stochastic processes $({\bf Z}_n, {\bf N}_n)_n$ on a directed network of $N$ agents. The system is defined by the coupled dynamics ${\bf Z}_{n+1}=(1-r_{n}){\bf Z}_{n}+r_{n}{\bf X}_{n+1}$ and ${\bf N}_{n+1}=(1-\frac{1}{n+1}){\bf N}_n+\frac{1}{n+1}{\bf X}_{n+1}$, where agent actions $\mathbb{P}(X_{n+1,j}=1\mid{\cal F}_n)=\sum_{h} w_{hj}Z_{nh}$ are governed by a column-normalized adjacency matrix ${\bf W}$, and $r_n \sim cn^{-\gamma}$ with $\gamma \in (1/2, 1]$. Existing asymptotic theory has largely been restricted to irreducible and diagonalizable ${\bf W}$. We extend this analysis to the broader and more practical class of reducible and non-diagonalizable matrices ${\bf W}$ possessing a block upper-triangular form, which models hierarchical influence. We first establish synchronization, proving $({\bf Z}^\top_n, {\bf N}^\top_n)^\top \to Z_\infty {\bf 1}$ almost surely, where the distribution of the limit $Z_\infty$ is shown to be determined solely by the internal dynamics of the leading subgroup. Furthermore, we establish a joint central limit theorem for $({\bf Z}_n,{\bf N}_n)_n$, revealing how the spectral properties and Jordan block structure of ${\bf W}$ govern second-order fluctuations. We demonstrate that the convergence rates and the limiting covariance structure exhibit a phase transition dependent on $\gamma$ and the spectral properties of ${\bf W}$. Crucially, we explicitly characterize how the non-diagonalizability of ${\bf W}$ fundamentally alters the asymptotic covariance and introduces new logarithmic scaling factors in the critical case ($\gamma=1$). These results provide a probabilistic foundation for statistical inference on such hierarchical network structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04562v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Yang, Dandan Jiang, Jiang Hu, Zhidong Bai</dc:creator>
    </item>
    <item>
      <title>Selective inference after convex clustering with $\ell_1$ penalization</title>
      <link>https://arxiv.org/abs/2309.01492</link>
      <description>arXiv:2309.01492v2 Announce Type: replace-cross 
Abstract: Classical inference methods notoriously fail when applied to data-driven test hypotheses or inference targets. Instead, dedicated methodologies are required to obtain statistical guarantees for these selective inference problems. Selective inference is particularly relevant post-clustering, typically when testing a difference in mean between two clusters. In this paper, we address convex clustering with $\ell_1$ penalization, by leveraging related selective inference tools for regression, based on Gaussian vectors conditioned to polyhedral sets. In the one-dimensional case, we prove a polyhedral characterization of obtaining given clusters, than enables us to suggest a test procedure with statistical guarantees. This characterization also allows us to provide a computationally efficient regularization path algorithm. Then, we extend the above test procedure and guarantees to multi-dimensional clustering with $\ell_1$ penalization, and also to more general multi-dimensional clusterings that aggregate one-dimensional ones. With various numerical experiments, we validate our statistical guarantees and we demonstrate the power of our methods to detect differences in mean between clusters. Our methods are implemented in the R package poclin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01492v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/ps/2025004</arxiv:DOI>
      <arxiv:journal_reference>ESAIM: PS, 29 (2025) 204-242</arxiv:journal_reference>
      <dc:creator>Fran\c{c}ois Bachoc, Cathy Maugis-Rabusseau, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Diffusion Posterior Sampling is Computationally Intractable</title>
      <link>https://arxiv.org/abs/2402.12727</link>
      <description>arXiv:2402.12727v2 Announce Type: replace-cross 
Abstract: Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.
  In this paper we show that posterior sampling is computationally intractable: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which every algorithm takes superpolynomial time, even though unconditional sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12727v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Ajil Jalal, Aditya Parulekar, Eric Price, Zhiyang Xun</dc:creator>
    </item>
    <item>
      <title>Towards Robust Matched Observational Studies with General Treatment Types: Consistency, Efficiency, and Adaptivity</title>
      <link>https://arxiv.org/abs/2403.14152</link>
      <description>arXiv:2403.14152v3 Announce Type: replace-cross 
Abstract: To ensure reliable causal conclusions from observational (i.e., non-randomized) studies, researchers routinely conduct sensitivity analysis to assess robustness to hidden bias due to unmeasured confounding. In matched observational studies (one of the most widely used observational study designs), two foundational concepts, design sensitivity and Bahadur-Rosenbaum efficiency, are used to quantify the robustness of test statistics and study designs in sensitivity analyses. Unfortunately, these measures of robustness are not developed for non-binary treatments (e.g., continuous or ordinal treatments) and consequently, prevailing recommendations about robust tests may be misleading. In this work, we provide a unified framework to quantify robustness of test statistics and study designs that are agnostic to treatment types. We first present a negative result about a popular, ad-hoc approach based on dichotomizing the treatment variable. Next, we introduce a universal, nearly sufficient sensitivity parameter that is agnostic to the underlying treatment type. We then generalize and derive all-in-one formulas for design sensitivity and Bahadur-Rosenbaum efficiency that can be used for any treatment type. We also propose a general data-adaptive approach to combine candidate test statistics to enhance robustness against unmeasured confounding. Extensive simulation studies and a data application illustrate our proposed framework. For practice, our results yield new, previously undiscovered insights about the robustness of tests and study designs in matched observational studies, especially when investigators are faced with non-binary treatment.sed sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14152v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Elaine K. Chiu, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Optimization via Strategic Law of Large Numbers</title>
      <link>https://arxiv.org/abs/2412.05604</link>
      <description>arXiv:2412.05604v5 Announce Type: replace-cross 
Abstract: This paper proposes a unified framework for the global optimization of a continuous function in a bounded rectangular domain. Specifically, we show that: (1) under the optimal strategy for a two-armed decision model, the sample mean converges to a global optimizer under the Strategic Law of Large Numbers, and (2) a sign-based strategy built upon the solution of a parabolic PDE is asymptotically optimal. Motivated by this result, we propose a class of {\bf S}trategic {\bf M}onte {\bf C}arlo {\bf O}ptimization (SMCO) algorithms, which uses a simple strategy that makes coordinate-wise two-armed decisions based on the signs of the partial gradient of the original function being optimized over (without the need of solving PDEs). While this simple strategy is not generally optimal, we show that it is sufficient for our SMCO algorithm to converge to local optimizer(s) from a single starting point, and to global optimizers under a growing set of starting points. Numerical studies demonstrate the suitability of our SMCO algorithms for global optimization, and illustrate the promise of our theoretical framework and practical approach. For a wide range of test functions with challenging optimization landscapes (including ReLU neural networks with square and hinge loss), our SMCO algorithms converge to the global maximum accurately and robustly, using only a small set of starting points (at most 100 for dimensions up to 1000) and a small maximum number of iterations (200). In fact, our algorithms outperform many state-of-the-art global optimizers, as well as local algorithms augmented with the same set of starting points as ours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05604v5</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Zengjing Chen, Wayne Yuan Gao, Xiaodong Yan, Guodong Zhang</dc:creator>
    </item>
    <item>
      <title>Convergence-divergence models: Generalizations of phylogenetic trees modeling gene flow over time</title>
      <link>https://arxiv.org/abs/2504.07384</link>
      <description>arXiv:2504.07384v3 Announce Type: replace-cross 
Abstract: Phylogenetic trees are simple models of evolutionary processes. They describe conditionally independent divergent evolution of taxa from common ancestors. Phylogenetic trees commonly do not have enough flexibility to adequately model all evolutionary processes. For example, introgressive hybridization, where genes can flow from one taxon to another. Phylogenetic networks model evolution not fully described by a phylogenetic tree. However, many phylogenetic network models assume ancestral taxa merge instantaneously to form ``hybrid'' descendant taxa. In contrast, our convergence-divergence models retain a single underlying ``principal'' tree, but permit gene flow over arbitrary time frames. Alternatively, convergence-divergence models can describe other biological processes leading to taxa becoming more similar over a time frame, such as replicated evolution. Here we present novel maximum likelihood-based algorithms to infer most aspects of $N$-taxon convergence-divergence models, many consistently, using a quartet-based approach. The algorithms can be applied to multiple sequence alignments restricted to genes or genomic windows or to gene presence/absence datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07384v3</guid>
      <category>q-bio.PE</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan D. Mitchell, Barbara R. Holland</dc:creator>
    </item>
    <item>
      <title>An easily verifiable dispersion order for discrete distributions</title>
      <link>https://arxiv.org/abs/2506.23677</link>
      <description>arXiv:2506.23677v2 Announce Type: replace-cross 
Abstract: Dispersion is a fundamental concept in statistics, yet standard approaches - especially via stochastic orders - face limitations in the discrete setting. In particular, the classical dispersive order, well-established for continuous distributions, becomes overly restrictive for discrete random variables due to support inclusion requirements. To address this, we propose a novel weak dispersive order for discrete distributions. This order retains desirable properties while relaxing structural constraints, thereby broadening applicability. We further introduce a class of variability measures based on probability concentration, offering robust and interpretable alternatives that conform to classical axioms. Empirical illustrations highlight the practical relevance of this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23677v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Schr\"odinger Potential Estimation in Unpaired Data Translation</title>
      <link>https://arxiv.org/abs/2508.07392</link>
      <description>arXiv:2508.07392v2 Announce Type: replace-cross 
Abstract: Modern methods of generative modelling and unpaired data translation based on Schr\"odinger bridges and stochastic optimal control theory aim to transform an initial density to a target one in an optimal way. In the present paper, we assume that we only have access to i.i.d. samples from initial and final distributions. This makes our setup suitable for both generative modelling and unpaired data translation. Relying on the stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as the reference one and estimate the corresponding Schr\"odinger potential. Introducing a risk function as the Kullback-Leibler divergence between couplings, we derive tight bounds on generalization ability of an empirical risk minimizer in a class of Schr\"odinger potentials including Gaussian mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we almost achieve fast rates of convergence up to some logarithmic factors in favourable scenarios. We also illustrate performance of the suggested approach with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07392v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Puchkin, Denis Suchkov, Alexey Naumov, Denis Belomestny</dc:creator>
    </item>
    <item>
      <title>Error exponents of quantum state discrimination with composite correlated hypotheses</title>
      <link>https://arxiv.org/abs/2508.12901</link>
      <description>arXiv:2508.12901v2 Announce Type: replace-cross 
Abstract: We study the error exponents in quantum hypothesis testing between two sets of quantum states, extending the analysis beyond the independent and identically distributed case to encompass composite correlated hypotheses. In particular, we introduce and compare two natural extensions of the quantum Hoeffding divergence and anti-divergence to sets of quantum states, establishing their equivalence or quantitative relations. In the error exponent regime, we generalize the quantum Hoeffding bound to stable sequences of convex, compact sets of quantum states, demonstrating that the optimal Type-I error exponent, under an exponential constraint on the Type-II error, is precisely characterized by the regularized quantum Hoeffding divergence between the sets. In the strong converse exponent regime, we provide a general lower bound on the exponent in terms of the regularized quantum Hoeffding anti-divergence and a matching upper bound when the null hypothesis is a singleton. The generality of these results enables applications in various contexts, including (i) refining the generalized quantum Stein's lemma by [Fang, Fawzi &amp; Fawzi, 2024]; (ii) exhibiting counterexamples to the continuity of the regularized Petz R\'enyi divergence and Hoeffding divergence; (iii) obtaining error exponents for adversarial channel discrimination and resource detection problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12901v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Fang, Masahito Hayashi</dc:creator>
    </item>
    <item>
      <title>The Gravitational Aspect of Information: The Physical Reality of Asymmetric "Distance"</title>
      <link>https://arxiv.org/abs/2510.22664</link>
      <description>arXiv:2510.22664v2 Announce Type: replace-cross 
Abstract: We show that when a Brownian bridge is physically constrained to satisfy a canonical condition, its time evolution exactly coincides with an m-geodesic on the statistical manifold of Gaussian distributions. This identification provides a direct physical realization of a geometric concept in information geometry. It implies that purely random processes evolve along informationally straight trajectories, analogous to geodesics in general relativity. Our findings suggest that the asymmetry of informational ``distance" (divergence) plays a fundamental physical role, offering a concrete step toward an equivalence principle for information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22664v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>gr-qc</category>
      <category>hep-ph</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>quant-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoi Koide, Armin van de Venn</dc:creator>
    </item>
    <item>
      <title>Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence</title>
      <link>https://arxiv.org/abs/2510.23434</link>
      <description>arXiv:2510.23434v2 Announce Type: replace-cross 
Abstract: Experiments deliver credible but often localized effects, tied to specific sites, populations, or mechanisms. When such estimates are insufficient to extrapolate effects for broader policy questions, such as external validity and general-equilibrium (GE) effects, researchers combine trials with external evidence from reduced-form or structural observational estimates, or prior experiments. We develop a unified framework for designing experiments in this setting: the researcher selects which parameters (or moments) to identify experimentally from a feasible set (e.g., which treatment arms and/or individuals to include in the experiment), allocates sample size, and specifies how to weight experimental and observational estimators. Because observational inputs may be biased in ways unknown ex ante, we develop a minimax proportional regret objective that evaluates any candidate design relative to an oracle that knows the bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that requires no prespecified bias bound and depends only on information about the precision of the estimators and the estimand's sensitivity to the underlying parameters. We illustrate the framework by (i) designing small-scale cash transfer experiments aimed at estimating GE effects and (ii) optimizing site selection for microfinance interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23434v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aristotelis Epanomeritakis, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v2 Announce Type: replace-cross 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition leverages association structure:
  vertex-level coefficients measuring directional concordance between outcome
  and features, or between feature pairs, define embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
  </channel>
</rss>
