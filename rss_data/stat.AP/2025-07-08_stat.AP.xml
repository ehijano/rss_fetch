<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Large scale study of primary school student performance relative to their LMS activity and socioeconomic demographics using a Bayesian Additive Regression Trees containing random effects</title>
      <link>https://arxiv.org/abs/2507.05262</link>
      <description>arXiv:2507.05262v1 Announce Type: new 
Abstract: Using data collected on almost every 9-12 years old student in Uruguay, we show how to apply Bayesian Additive Regression Trees (BART) with random effects to study performance association with Learning Managment System (LMS) activity and socioeconomic status. Performance data is joined with LMS activity pattern data. BART is chosen because it is possible to include school-level random effects. The model can be used for early identification of at-risk students, and highlights schools that are successful or need intervention. An interesting finding is that high levels of LMS usage show larger positive effects on performance in low socioeconomic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05262v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia da Silva, Bruno Tancredi, Ignacio Alvarez-Castro</dc:creator>
    </item>
    <item>
      <title>The contribution of machine learning to the prevention of burnout among healthcare workers in Morocco</title>
      <link>https://arxiv.org/abs/2507.05264</link>
      <description>arXiv:2507.05264v1 Announce Type: new 
Abstract: In recent years, and particularly during the Covid-19 pandemic, Morocco has experienced significant pressure from user demand, leading to a significant workload in public hospitals. This situation raises major questions regarding the occupational health of healthcare staff. While previous studies have focused on the role of AI in the safety and resilience of military personnel, no research has investigated its role in protecting healthcare personnel from psychosocial risks. This inadequacy leads us to formulate the following central question:What is the contribution of machine learning to the prevention of emotional exhaustion (burnout) among healthcare staff in Morocco? This work is part of a modeling approach aimed at developing a predictive model of the risks of emotional exhaustion (burn-out), the parameters of which will be estimated using supervised learning. From a scientific perspective, this work aims to contribute to the development of systems for preventing psychosocial risks affecting staff in healthcare establishments. From a managerial perspective, this research aims to equip decision-makers in healthcare establishments so that they can anticipate psychosocial disorders linked to emotional exhaustion (burn-out) and implement appropriate preventive measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05264v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.30870.66880</arxiv:DOI>
      <dc:creator>Mohammed Eddaou</dc:creator>
    </item>
    <item>
      <title>Assessing Methodological Variability in Wastewater Surveillance: A Wavelet Decomposition Approach</title>
      <link>https://arxiv.org/abs/2507.05539</link>
      <description>arXiv:2507.05539v1 Announce Type: new 
Abstract: Wastewater surveillance has emerged as a critical public health tool, enabling early detection of infectious disease outbreaks and providing timely, population-level insights into community health trends. However, variability in sample collection and processing, for example between wastewater influent and settled solids, can introduce methodological noise that differentially impacts true epidemiological signals and limits cross-site comparability. To address this challenge, we aimed to discern underlying disease trends from methodological variability in SARS-CoV-2 wastewater data using discrete wavelet transform (DWT), with a focus on comparing influent and solids samples from the same geographic locations. We applied DWT to longitudinal SARS-CoV-2 RNA concentrations in wastewater from five California cities, each with paired influent and solids samples. DWT decomposes each signal into two components: (1) approximation coefficients that capture smoothed long-term trends, and (2) detail coefficients that isolate high-frequency fluctuations and transient variations in the signal. We reconstructed signals by progressively removing the high-frequency components and assessed similarity between sample types using hierarchical clustering. Clustering of raw signals did not yield city-specific groupings, indicating that methodological noise obscured the underlying epidemiological signal. Intermediate reconstructions that retained some high-frequency components continued to show mixed groupings. In contrast, reconstructions based solely on low-frequency approximation coefficients revealed clear, city-specific clustering, with influent and solids samples from the same city aligning closely. These findings support our hypothesis that high-frequency components are primarily driven by sample processing and laboratory noise, while low-frequency components reflect shared epidemiological trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05539v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria L. Daza-Torres, J. Cricelio Montesinos-Lopez, Rachel Olson, C. Winston Bess, Colleen C. Naughton, Heather N. Bischel, Miriam Nuno</dc:creator>
    </item>
    <item>
      <title>A Short-Term Integrated Wind Speed Prediction System Based on Fuzzy Set Feature Extraction</title>
      <link>https://arxiv.org/abs/2507.05761</link>
      <description>arXiv:2507.05761v1 Announce Type: new 
Abstract: Wind energy has significant potential owing to the continuous growth of wind power and advancements in technology. However, the evolution of wind speed is influenced by the complex interaction of multiple factors, making it highly variable. The nonlinear and nonstationary nature of wind speed evolution can have a considerable impact on the overall power system. To address this challenge, we propose an integrated multiframe wind speed prediction system based on fuzzy feature extraction. This system employs a convex subset partitioning approach using a triangular affiliation function for fuzzy feature extraction. By applying soft clustering to the subsets, constructing an affiliation matrix, and identifying clustering centers, the system introduces the concepts of inner and boundary domains. It subsequently calculates the distances from data points to the clustering centers by measuring both interclass and intraclass distances. This method updates the cluster centers using the membership matrix, generating optimal feature values. Building on this foundation, we use multiple machine learning methods to input the fuzzy features into the prediction model and integrate learning techniques to predict feature values. Because different datasets require different modeling approaches, the integrated weight-updating module was used to dynamically adjust model weights by setting a dual objective function to ensure the accuracy and stability of the prediction. The effectiveness of the proposed model in terms of prediction performance and generalization ability is demonstrated through an empirical analysis of data from the Penglai wind farm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05761v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Geng, Jianzhou Wang, Jinze Li, Zhiwu Li</dc:creator>
    </item>
    <item>
      <title>Modeling of Vertical Distribution of Suspended Sediment Concentration in Open Channel Turbulent Flows Using Fractional Differential Entropy</title>
      <link>https://arxiv.org/abs/2507.05986</link>
      <description>arXiv:2507.05986v1 Announce Type: new 
Abstract: Suspended sediment concentration and sediment transport heavily correlates to fluid behavior, thus proving it to be a lucrative field for exploration. Most of the existing deterministic and probabilistic methods proved to be complex with high computation cost. In this paper, we proposed a simpler yet accurate and cost effective concentration model using fractional entropy due to Ubriaco for continuous domain, termed as fractional differential entropy (FDE). We estimated the type I distribution of suspended sediment concentration along the vertical direction in open channels considering the dimensionless normalized concentration as a random variable and constructing an optimization problem using the FDE. The surface concentration is assumed to be zero throughout the study. We further validate our FDE based concentration distribution model through regression and error analysis using some selected experimental and field data. The results are compared with the existing concentration models, which show the superiority of the proposed model with respect to the aspects considered under this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05986v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poulami Paul, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>A Bayesian circular mixed-effects model for explaining variability in directional movement in American football</title>
      <link>https://arxiv.org/abs/2507.06122</link>
      <description>arXiv:2507.06122v1 Announce Type: new 
Abstract: Change of direction is a key element of player movement in American football, yet there remains a lack of objective approaches for in-game performance evaluation of this athletic trait. Using tracking data, we propose a Bayesian mixed-effects model with heterogeneous variances for assessing a player's ability to make variable directional adjustments while moving on the field. We model the turn angle (i.e., angle between successive displacement vectors) for NFL ball carriers on both passing and rushing plays, focusing on receivers after the catch and running backs after the handoff. In particular, we consider a von Mises distribution for the frame-level turn angle and explicitly model both the mean and concentration parameters with relevant spatiotemporal and contextual covariates. Of primary interest, we include player random effects that allow the turn angle concentration to vary by ball carrier nested within position groups. This offers practical insight into player evaluation, as it reveals the shiftiest ball carriers with great variability in turning behavior. We illustrate our approach with results from the first nine weeks of the 2022 NFL regular season and explore player-specific and positional differences in turn angle variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06122v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Nguyen, Ronald Yurko</dc:creator>
    </item>
    <item>
      <title>Bridging Prediction and Intervention Problems in Social Systems</title>
      <link>https://arxiv.org/abs/2507.05216</link>
      <description>arXiv:2507.05216v1 Announce Type: cross 
Abstract: Many automated decision systems (ADS) are designed to solve prediction problems -- where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an interventionist paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05216v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia T. Liu, Inioluwa Deborah Raji, Angela Zhou, Luke Guerdan, Jessica Hullman, Daniel Malinsky, Bryan Wilder, Simone Zhang, Hammaad Adam, Amanda Coston, Ben Laufer, Ezinne Nwankwo, Michael Zanger-Tishler, Eli Ben-Michael, Solon Barocas, Avi Feller, Marissa Gerchick, Talia Gillis, Shion Guha, Daniel Ho, Lily Hu, Kosuke Imai, Sayash Kapoor, Joshua Loftus, Razieh Nabi, Arvind Narayanan, Ben Recht, Juan Carlos Perdomo, Matthew Salganik, Mark Sendak, Alexander Tolbert, Berk Ustun, Suresh Venkatasubramanian, Angelina Wang, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Community Bail Fund Systems: Fluid Limits and Approximations</title>
      <link>https://arxiv.org/abs/2507.05490</link>
      <description>arXiv:2507.05490v1 Announce Type: cross 
Abstract: Community bail funds (CBFs) assist individuals who have been arrested and cannot afford bail, preventing unnecessary pretrial incarceration along with its harmful or sometimes fatal consequences. By posting bail, CBFs allow defendants to stay at home and maintain their livelihoods until trial. This paper introduces new stochastic models that combine queueing theory with classic insurance risk models to capture the dynamics of the remaining funds in a CBF. We first analyze a model where all bail requests are accepted. Although the remaining fund balance can go negative, this model provides insight for CBFs that are not financially constrained. We then apply the Skorokhod map to make sure the CBF balance does not go negative and show that the Skorokhod map produces a model where requests are partially fulfilled. Finally, we analyze a model where bail requests can be blocked if there is not enough money to satisfy the request upon arrival. Although the blocking model prevents the CBF from being negative, the blocking feature gives rise to new analytical challenges for a direct stochastic analysis. Thus, we prove a functional law of large numbers or a fluid limit for the blocking model and show that the fluid limit is a distributed delay equation. We assess the quality of our fluid limit via simulation and show that the fluid limit accurately describes the large-scale stochastic dynamics of the CBF. Finally, we prove stochastic ordering results for the CBF processes we analyze.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05490v1</guid>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yidan Zhang, Jamol Pender</dc:creator>
    </item>
    <item>
      <title>hassediagrams:an R package that generates the Hasse diagram of the layout structure and the restricted layout structure</title>
      <link>https://arxiv.org/abs/2507.05949</link>
      <description>arXiv:2507.05949v1 Announce Type: cross 
Abstract: With the advent of modern statistical software, complex experimental designs are now routinely employed in many areas of research. Failing to correctly identify the structure of the experimental design can lead to incorrect model selection and misleading inferences. This paper describes the hassediagrams package in R that determines the structure of the design, summarised by the layout structure, and generates a Hasse diagram of the layout structure. By considering the randomisation performed, in conjunction with the layout structure, a set of randomisation objects can be defined that form the restricted layout structure. This structure can also be visualised using a generalisation of the Hasse diagram. Objects in the restricted layout structure can be used to identify the terms to include in the statistical model. The use of the procedure thus ensures consistency of model selection due to the systematic approach taken to generate the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05949v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damianos Michaelides, Simon T. Bate, Marion J. Chatfield</dc:creator>
    </item>
    <item>
      <title>FDR controlling procedures with dimension reduction and their application to GWAS with linkage disequilibrium score</title>
      <link>https://arxiv.org/abs/2507.06049</link>
      <description>arXiv:2507.06049v1 Announce Type: cross 
Abstract: Genome-wide association studies (GWAS) have led to the discovery of numerous single nucleotide polymorphisms (SNPs) associated with various phenotypes and complex diseases. However, the identified genetic variants do not fully explain the heritability of complex traits, known as the missing heritability problem. To address this challenge and accurately control false positives while maximizing true associations, we propose two approaches involving linkage disequilibrium (LD) scores as covariates. We apply principal component analysis (PCA), one of the dimensionality reduction techniques, to control the False Discovery Rate (FDR) in the presence of high-dimensional covariates. This method not only provides a convenient interpretation of how multiple covariates in high dimensions affect the control of FDR but also offers higher statistical power compared to cases where covariates are not used. Furthermore, we aim to investigate how covariates contribute to increasing the statistical power through various simulation experiments, comparing the results with real data examples to derive better interpretations. Using real-world datasets, including GWAS with Body Mass Index (BMI) as the phenotype, we evaluate the performance of our proposed approaches. By incorporating LD scores as covariates in FDR-controlled GWAS analyzes, we demonstrate their effectiveness in selecting informative LD scores and improving the identification of significant SNPs. Our methods alleviate computational burden and enhance interpretability while retaining essential information from LD scores. In general, our study contributes to the advancement of statistical methods in GWAS and provides practical guidance for researchers looking to improve the precision of genetic association analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dayeon Jung, Yewon Kim, Junyong Park</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. Subject-level mediators are aggregated from cell-level data to perform mediation analysis assessing causal pathways linking gene expression to clinical outcomes. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>A Quantile Nelson-Siegel model</title>
      <link>https://arxiv.org/abs/2401.09874</link>
      <description>arXiv:2401.09874v2 Announce Type: replace 
Abstract: We propose a novel framework for modeling the yield curve from a quantile perspective. Building on the dynamic Nelson-Siegel model of Diebold et al. (2006), we extend its traditional mean-based approach to a quantile regression setting, enabling the estimation of yield curve factors - level, slope, and curvature - at specific quantiles of the conditional distribution. A key advantage of our framework is its ability to characterize the entire conditional distribution of the yield curve across maturities and over time. In an empirical analysis of the U.S. term structure of interest rates, our method demonstrates superior out-of-sample forecasting performance, particularly in capturing the tails of the yield distribution - an aspect increasingly emphasized in the recent literature on distributional forecasting. In addition to its forecasting advantages, our approach reveals rich distributional features beyond the mean. In particular, we find that the dynamic changes in these distributional features differ markedly between the Great Recession and the COVID-19 pandemic period, highlighting a fundamental shift in how interest rate markets respond to distinct economic shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09874v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Aubrey Poon, Luca Rossini, Dan Zhu</dc:creator>
    </item>
    <item>
      <title>Forecasting the U.S. Renewable-Energy Mix with an ALR-BDARMA Compositional Time-Series Framework</title>
      <link>https://arxiv.org/abs/2507.04087</link>
      <description>arXiv:2507.04087v2 Announce Type: replace 
Abstract: Accurate forecasts of the US renewable-generation mix are critical for planning transmission upgrades, sizing storage, and setting balancing-market rules. We present a Bayesian Dirichlet ARMA (BDARMA) model for monthly shares of hydro, geothermal, solar, wind, wood, municipal waste, and biofuels from January 2010 to January 2025. The mean vector follows a parsimonious VAR(2) in additive-log-ratio space, while the Dirichlet concentration parameter combines an intercept with ten Fourier harmonics, letting predictive dispersion expand or contract with the seasons.
  A 61-split rolling-origin study generates twelve-month density forecasts from January 2019 to January 2024. Relative to three benchmarks, a Gaussian VAR(2) in transform space, a seasonal naive copy of last year's proportions, and a drift-free additive-log-ratio random walk, BDARMA lowers the mean continuous ranked probability score by fifteen to sixty percent, achieves component-wise ninety percent interval coverage close to nominal, and matches Gaussian VAR point accuracy through eight months with a maximum loss of 0.02 Aitchison units thereafter. BDARMA therefore delivers sharp, well-calibrated probabilistic forecasts of multivariate renewable-energy shares without sacrificing point precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04087v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Longitudinal Data under Unknown Interference</title>
      <link>https://arxiv.org/abs/2106.15074</link>
      <description>arXiv:2106.15074v4 Announce Type: replace-cross 
Abstract: In longitudinal studies where units are embedded in space or a social network, interference may arise, meaning that a unit's outcome can depend on treatment histories of others. The presence of interference poses significant challenges for causal inference, particularly when the interference structure -- how a unit's outcome responds to others' influences -- is complex, heterogeneous, and unknown to researchers. This paper develops a general framework for identifying and estimating both direct and spillover effects of treatment histories under minimal assumptions about the interference structure. We define a class of policy-relevant causal estimands and show that they can be represented by a modified marginal structural model (MSM). Under the standard assumption of sequential exchangeability, these estimands are identifiable and can be estimated using inverse probability weighting (IPW). We derive conditions for consistency and asymptotic normality of the estimators and provide procedures for constructing Wald-type confidence intervals with valid coverage in large samples. The method's utility is demonstrated through applications in both social science and biomedical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15074v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Michael Jetsupphasuk</dc:creator>
    </item>
    <item>
      <title>Exact phylodynamic likelihood via structured Markov genealogy processes</title>
      <link>https://arxiv.org/abs/2405.17032</link>
      <description>arXiv:2405.17032v3 Announce Type: replace-cross 
Abstract: We show that each member of a broad class of Markovian population models induces a unique stochastic process on the space of genealogies. We construct this genealogy process and derive exact expressions for the likelihood of an observed genealogy in terms of a filter equation, the structure of which is completely determined by the population model. We show that existing phylodynamic methods based on either the coalescent or the linear birth-death processes are special cases. We derive some properties of filter equations and describe a class of algorithms that can be used to numerically solve them. Our results open the door to statistically efficient likelihood-based phylodynamic inference for a much wider class of models than is currently possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17032v3</guid>
      <category>q-bio.QM</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron A. King, Qianying Lin, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>A Bayesian workflow for securitizing casualty insurance risk</title>
      <link>https://arxiv.org/abs/2407.14666</link>
      <description>arXiv:2407.14666v3 Announce Type: replace-cross 
Abstract: Casualty insurance-linked securities (ILS) are appealing to investors because the underlying insurance claims, which are directly related to resulting security performance, are uncorrelated with most other asset classes. Conversely, casualty ILS are appealing to insurers as an efficient capital management tool. However, securitizing casualty insurance risk is non-trivial, as it requires forecasting loss ratios for pools of insurance policies that have not yet been written, in addition to estimating how the underlying losses will develop over time within future accident years. In this paper, we lay out a Bayesian workflow that tackles these complexities by using: (1) theoretically informed time-series and state-space models to capture how loss ratios develop and change over time; (2) historic industry data to inform prior distributions of models fit to individual programs; (3) stacking to combine loss ratio predictions from candidate models, and (4) both prior predictive simulations and simulation-based calibration to aid model specification. Using historic Schedule P filings, we then show how our proposed Bayesian workflow can be used to assess and compare models across a variety of key model performance metrics evaluated on future accident year losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14666v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Haines, Conor Goold, J. Mark Shoun</dc:creator>
    </item>
    <item>
      <title>Incorporating Memory into Continuous-Time Spatial Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2408.17278</link>
      <description>arXiv:2408.17278v2 Announce Type: replace-cross 
Abstract: Obtaining reliable and precise estimates of wildlife species abundance and distribution is essential for the conservation and management of animal populations and natural reserves. Spatial capture-recapture (SCR) models provide estimates of population size and spatial density from data collected from remote sensors such as camera traps. Such data contain spatial correlation between observations of the same individual, which SCR models partly account for through a latent individual-specific activity centre, a location near which the individual is more likely detected. However, SCR models assume that the observations of an individual are independent over time and space, conditional on its activity centre, so that observed sightings at a given time and location do not influence the probability of being seen at future times and/or locations. This assumption is ecologically unrealistic given the smooth movement of animals over space through time. We propose a new continuous-time modelling framework that incorporates both an individual's (latent) activity centre and its (known) previous location and time of detection. By formulating the detections of an individual as an inhomogeneous temporal Poisson process, we develop a model drawing inspiration from the Ornstein-Uhlenbeck process, which is commonly used to model animal movement. Applying our model to a camera-trap survey of American martens, we observe a substantial improvement in model fit and notable differences in the estimated spatial distribution of activity centres. A simulation study shows that standard SCR models can produce substantially biased population estimates when spatio-temporal dependence is ignored, while the memory-based model remains robust. These findings highlight the importance of accounting for memory of previous detections in SCR models to improve ecological interpretation and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17278v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Panchaud, Ruth King, David Borchers, Hannah Worthington, Ian Durbach, Paul Van Dam-Bates</dc:creator>
    </item>
    <item>
      <title>HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics</title>
      <link>https://arxiv.org/abs/2505.05602</link>
      <description>arXiv:2505.05602v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly estimating their capabilities from inherently stochastic outputs while systematically quantifying uncertainty in these estimates becomes increasingly important. Further, advanced AI evaluations often have a nested hierarchical structure, exhibit high levels of complexity, and come with high costs in testing the most advanced AI systems. To address these challenges, we introduce HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI Evaluation Statistics. HiBayES supports robust inferences in classical question-answer benchmarks and advanced agentic evaluations, particularly in low-data scenarios (e.g., &lt; 20 data points per evaluation). Built on Generalized Linear Models (GLMs), Bayesian data analysis, and formal model comparison, HiBayES provides principled uncertainty quantification and robust parameter estimation. This paper offers a comprehensive introduction to HiBayES, including illustrative examples, comparisons to conventional statistical methods, and practical guidance for implementing multilevel Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta version) for out-of-the-box implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05602v2</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lennart Luettgau, Harry Coppock, Magda Dubois, Christopher Summerfield, Cozmin Ududec</dc:creator>
    </item>
    <item>
      <title>The Zeta Tail Distribution: A Novel Event-Count Model</title>
      <link>https://arxiv.org/abs/2506.17496</link>
      <description>arXiv:2506.17496v2 Announce Type: replace-cross 
Abstract: We introduce the Zeta Tail(a) probability distribution as a new model for random damage-event counts in risk analysis. Although readily motivated as an analogue of the Geometric(p) distribution, Zeta Tail(a) has received little attention in the scholarly literature. In the present work, we begin by deriving various fundamental properties of this novel distribution. We then assess its usefulness as an alternative to Geometric(p), both theoretically and through application to a set of meteorological data. Lastly, we discuss conceptual differences between employing the Zeta Tail(a) model conditionally (i.e., given observed data with certain known characteristics) and unconditionally (i.e., for arbitrary, as yet unobserved data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17496v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers</dc:creator>
    </item>
  </channel>
</rss>
