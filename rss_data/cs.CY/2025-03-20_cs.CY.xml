<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rendering Transparency to Ranking in Educational Assessment via Bayesian Comparative Judgement</title>
      <link>https://arxiv.org/abs/2503.15549</link>
      <description>arXiv:2503.15549v1 Announce Type: new 
Abstract: Ensuring transparency in educational assessment is increasingly critical, particularly post-pandemic, as demand grows for fairer and more reliable evaluation methods. Comparative Judgement (CJ) offers a promising alternative to traditional assessments, yet concerns remain about its perceived opacity. This paper examines how Bayesian Comparative Judgement (BCJ) enhances transparency by integrating prior information into the judgement process, providing a structured, data-driven approach that improves interpretability and accountability.
  BCJ assigns probabilities to judgement outcomes, offering quantifiable measures of uncertainty and deeper insights into decision confidence. By systematically tracking how prior data and successive judgements inform final rankings, BCJ clarifies the assessment process and helps identify assessor disagreements. Multi-criteria BCJ extends this by evaluating multiple learning outcomes (LOs) independently, preserving the richness of CJ while producing transparent, granular rankings aligned with specific assessment goals. It also enables a holistic ranking derived from individual LOs, ensuring comprehensive evaluations without compromising detailed feedback.
  Using a real higher education dataset with professional markers in the UK, we demonstrate BCJ's quantitative rigour and ability to clarify ranking rationales. Through qualitative analysis and discussions with experienced CJ practitioners, we explore its effectiveness in contexts where transparency is crucial, such as high-stakes national assessments. We highlight the benefits and limitations of BCJ, offering insights into its real-world application across various educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15549v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andy Gray, Alma Rahat, Stephen Lindsay, Jen Pearson, Tom Crick</dc:creator>
    </item>
    <item>
      <title>Transfeminist AI Governance</title>
      <link>https://arxiv.org/abs/2503.15682</link>
      <description>arXiv:2503.15682v1 Announce Type: new 
Abstract: This article re-imagines the governance of artificial intelligence (AI) through a transfeminist lens, focusing on challenges of power, participation, and injustice, and on opportunities for advancing equity, community-based resistance, and transformative change. AI governance is a field of research and practice seeking to maximize benefits and minimize harms caused by AI systems. Unfortunately, AI governance practices are frequently ineffective at preventing AI systems from harming people and the environment, with historically marginalized groups such as trans people being particularly vulnerable to harm. Building upon trans and feminist theories of ethics, I introduce an approach to transfeminist AI governance. Applying a transfeminist lens in combination with a critical self-reflexivity methodology, I retroactively reinterpret findings from three empirical studies of AI governance practices in Canada and globally. In three reflections on my findings, I show that large-scale AI governance systems structurally prioritize the needs of industry over marginalized communities. As a result, AI governance is limited by power imbalances and exclusionary norms. This research shows that re-grounding AI governance in transfeminist ethical principles can support AI governance researchers, practitioners, and organizers in addressing those limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15682v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Blair Attard-Frost</dc:creator>
    </item>
    <item>
      <title>Student's Use of Generative AI as a Support Tool in an Advanced Web Development Course</title>
      <link>https://arxiv.org/abs/2503.15684</link>
      <description>arXiv:2503.15684v1 Announce Type: new 
Abstract: Various studies have studied the impact of Generative AI on Computing Education. However, they have focused on the implications for novice programmers. In this experience report, we analyze the use of GenAI as a support tool for learning, creativity, and productivity in a web development course for undergraduate students with extensive programming experience. We collected diverse data (assignments, reflections, logs, and a survey) and found that students used GenAI on different tasks (code generation, idea generation, etc.) with a reported increase in learning and productivity. However, they are concerned about over-reliance and incorrect solutions and want more training in prompting strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15684v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Isaac Alpizar-Chacon, Hieke Keuning</dc:creator>
    </item>
    <item>
      <title>WeirdFlows: Anomaly Detection in Financial Transaction Flows</title>
      <link>https://arxiv.org/abs/2503.15896</link>
      <description>arXiv:2503.15896v1 Announce Type: new 
Abstract: In recent years, the digitization and automation of anti-financial crime (AFC) investigative processes have faced significant challenges, particularly the need for interpretability of AI model results and the lack of labeled data for training. Network analysis has emerged as a valuable approach in this context.
  In this paper, we present WeirdFlows, a top-down search pipeline for detecting potentially fraudulent transactions and non-compliant agents. In a transaction network, fraud attempts are often based on complex transaction patterns that change over time to avoid detection. The WeirdFlows pipeline requires neither an a priori set of patterns nor a training set. In addition, by providing elements to explain the anomalies found, it facilitates and supports the work of an AFC analyst.
  We evaluate WeirdFlows on a dataset from Intesa Sanpaolo (ISP) bank, comprising 80 million cross-country transactions over 15 months, benchmarking our implementation of the algorithm. The results, corroborated by ISP AFC experts, highlight its effectiveness in identifying suspicious transactions and actors, particularly in the context of the economic sanctions imposed in the EU after February 2022. This demonstrates \textit{WeirdFlows}' capability to handle large datasets, detect complex transaction patterns, and provide the necessary interpretability for formal AFC investigations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15896v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Capozzi, Salvatore Vilella, Dario Moncalvo, Marco Fornasiero, Valeria Ricci, Silvia Ronchiadin, Giancarlo Ruffo</dc:creator>
    </item>
    <item>
      <title>Autonomous AI imitators increase diversity in homogeneous information ecosystems</title>
      <link>https://arxiv.org/abs/2503.16021</link>
      <description>arXiv:2503.16021v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) have facilitated autonomous AI agents capable of imitating human-generated content. This technological advancement raises fundamental questions about AI's potential impact on the diversity and democratic value of information ecosystems. Here, we introduce a large-scale simulation framework to examine AI-based imitation in news, a context critically influential for public discourse. By systematically testing two distinct imitation strategies across a range of information environments varying in initial diversity, we demonstrate that AI-generated articles do not uniformly homogenize content. Instead, AI's influence is strongly context-dependent: AI-generated articles can introduce valuable diversity in originally homogeneous news environments, while potentially diminishing diversity in contexts that initially display high heterogeneity. These results illustrate that the baseline diversity of an information space critically shapes AI's impact, challenging assumptions that AI-driven imitation uniformly threatens information diversity. Instead, when information is initially homogeneous, AI-driven imitation can expand perspectives, styles, and topics. This is especially important in news contexts, where information diversity fosters richer public debate by exposing citizens to alternative viewpoints, challenging biases, and preventing narrative monopolies, which is essential for a resilient democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16021v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emil Bakkensen Johansen, Oliver Baumann</dc:creator>
    </item>
    <item>
      <title>Doing More With Less: Mismatch-Based Risk-Limiting Audits</title>
      <link>https://arxiv.org/abs/2503.16104</link>
      <description>arXiv:2503.16104v1 Announce Type: new 
Abstract: One approach to risk-limiting audits (RLAs) compares randomly selected cast vote records (CVRs) to votes read by human auditors from the corresponding ballot cards. Historically, such methods reduce audit sample sizes by considering how each sampled CVR differs from the corresponding true vote, not merely whether they differ. Here we investigate the latter approach, auditing by testing whether the total number of mismatches in the full set of CVRs exceeds the minimum number of CVR errors required for the reported outcome to be wrong (the "CVR margin"). This strategy makes it possible to audit more social choice functions and simplifies RLAs conceptually, which makes it easier to explain than some other RLA approaches. The cost is larger sample sizes. "Mismatch-based RLAs" only require a lower bound on the CVR margin, which for some social choice functions is easier to calculate than the effect of particular errors. When the population rate of mismatches is low and the lower bound on the CVR margin is close to the true CVR margin, the increase in sample size is small. However, the increase may be very large when errors include errors that, if corrected, would widen the CVR margin rather than narrow it; errors affect the margin between candidates other than the reported winner with the fewest votes and the reported loser with the most votes; or errors that affect different margins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16104v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Michelle Blom, Philip B. Stark, Peter J. Stuckey, Vanessa J. Teague, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>Towards Non-linear Cultural Production and systems of machinic agency: in the case of TikTok value generation</title>
      <link>https://arxiv.org/abs/2503.16137</link>
      <description>arXiv:2503.16137v1 Announce Type: new 
Abstract: The rise of TikTok has brought forth novel ways to create and consume media content, accelerated by technologies such as hyper-individualised algorithms and easy-to-use video production tools. Despite its popularity, scholars and politicians alike have raised many concerns on the legitimacy and ethics of TikTok regarding its services, and its collected data. However, much of these discussions take the premise of user-generated content for granted, attributing them to human expression without critically evaluating how the making of on-platform content production have changed. With a grounded theory approach, in conjunction with a platform-aware walkthrough that pays special attention to the material and immaterial premises of platform value generation, my findings suggest that the intensification of datafication have proliferated from consumption behaviours to the process of content production, whereas content production no longer solely produce media content. As platforms become the active recruiter, mobiliser and co-producer of media production, I argue that it is no longer feasible to distinguish human and machine contribution in the ways they are consumed to facilitate platform valorisation. I propose that the technical arrangements of TikTok, in relation to its users has fostered a non-linear mode of platform cultural production capable of generating economic value through a system of machinic agency that incorporates human and machines in an indistinguishable manner. As content, the premises of platform valorisation has become an inseparable effort of human-machines, I urge that the relationship between technology and humans be reassessed as a system of machinic agency that mutually shapes our mediated reality, rather than singular, differentiable actors that contribute to platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16137v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hongrui Jin</dc:creator>
    </item>
    <item>
      <title>Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2503.16148</link>
      <description>arXiv:2503.16148v1 Announce Type: new 
Abstract: Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16148v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mats Faulborn, Indira Sen, Max Pellert, Andreas Spitz, David Garcia</dc:creator>
    </item>
    <item>
      <title>Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1</title>
      <link>https://arxiv.org/abs/2503.16304</link>
      <description>arXiv:2503.16304v1 Announce Type: new 
Abstract: In recent years, the development of Large Language Models (LLMs) has made significant breakthroughs in the field of natural language processing and has gradually been applied to the field of humanities and social sciences research. LLMs have a wide range of application value in the field of humanities and social sciences because of its strong text understanding, generation and reasoning capabilities. In humanities and social sciences research, LLMs can analyze large-scale text data and make inferences.
  This article analyzes the large language model DeepSeek-R1 from seven aspects: low-resource language translation, educational question-answering, student writing improvement in higher education, logical reasoning, educational measurement and psychometrics, public health policy analysis, and art education.Then we compare the answers given by DeepSeek-R1 in the seven aspects with the answers given by o1-preview. DeepSeek-R1 performs well in the humanities and social sciences, answering most questions correctly and logically, and can give reasonable analysis processes and explanations. Compared with o1-preview, it can automatically generate reasoning processes and provide more detailed explanations, which is suitable for beginners or people who need to have a detailed understanding of this knowledge, while o1-preview is more suitable for quick reading.
  Through analysis, it is found that LLM has broad application potential in the field of humanities and social sciences, and shows great advantages in improving text analysis efficiency, language communication and other fields. LLM's powerful language understanding and generation capabilities enable it to deeply explore complex problems in the field of humanities and social sciences, and provide innovative tools for academic research and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16304v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiran Gu, Fuhao Duan, Wenhao Li, Bochen Xu, Ying Cai, Teng Yao, Chenxun Zhuo, Tianming Liu, Bao Ge</dc:creator>
    </item>
    <item>
      <title>Is Negative Representation More Engaging? The Influence of News Title Framing of Older Adults on Viewer Behavior</title>
      <link>https://arxiv.org/abs/2503.15493</link>
      <description>arXiv:2503.15493v1 Announce Type: cross 
Abstract: Grounded in framing theory, this study examines how news titles about older adults shape user engagement on a Chinese video-sharing platform. We analyzed 2,017 video news titles from 2016 to 2021, identifying nine frames. Negative frames produced higher views and shares, suggesting that negative portrayals garner attention and encourage further distribution. In contrast, positive frames led to more collections and rewards, reflecting viewer preference and financial support for favorable depictions. These findings underscore how framing aligns with ageism concerns and highlight the need for more balanced media portrayals of older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15493v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhilong Zhao (School of Journalism and Communication, South China University of Technology), Jiaxin Xia (Department of Communication, Faculty of Social Sciences, University of Macau)</dc:creator>
    </item>
    <item>
      <title>AI-Powered Assistive Technologies for Visual Impairment</title>
      <link>https://arxiv.org/abs/2503.15494</link>
      <description>arXiv:2503.15494v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is revolutionizing assistive technologies. It offers innovative solutions to enhance the quality of life for individuals with visual impairments. This review examines the development, applications, and impact of AI-powered tools in key domains, such as computer vision, natural language processing (NLP), and wearable devices. Specific advancements include object recognition for identifying everyday items, scene description for understanding surroundings, and NLP-driven text-to-speech systems for accessing digital information. Assistive technologies like smart glasses, smartphone applications, and AI-enabled navigation aids are discussed, demonstrating their ability to support independent travel, facilitate social interaction, and increase access to education and employment opportunities.
  The integration of deep learning models, multimodal interfaces, and real-time data processing has transformed the functionality and usability of these tools, fostering inclusivity and empowerment. This article also addresses critical challenges, including ethical considerations, affordability, and adaptability in diverse environments. Future directions highlight the need for interdisciplinary collaboration to refine these technologies, ensuring equitable access and sustainable innovation. By providing a comprehensive overview, this review underscores AI's transformative potential in promoting independence, enhancing accessibility, and fostering social inclusion for visually impaired individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15494v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prudhvi Naayini, Praveen Kumar Myakala, Chiranjeevi Bura, Anil Kumar Jonnalagadda, Srikanth Kamatala</dc:creator>
    </item>
    <item>
      <title>The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study</title>
      <link>https://arxiv.org/abs/2503.15497</link>
      <description>arXiv:2503.15497v1 Announce Type: cross 
Abstract: This study investigates how the Big Five personality traits influence decision-making processes in AI agents within public spaces. Using AgentVerse framework and GPT-3.5-turbo, we simulated interactions among 10 AI agents, each embodying different dimensions of the Big Five personality traits, in a classroom environment responding to misinformation. The experiment assessed both public expressions ([Speak]) and private thoughts ([Think]) of agents, revealing significant correlations between personality traits and decision-making patterns. Results demonstrate that Openness to Experience had the strongest impact on information acceptance, with curious agents showing high acceptance rates and cautious agents displaying strong skepticism. Extraversion and Conscientiousness also showed notable influence on decision-making, while Neuroticism and Agreeableness exhibited more balanced responses. Additionally, we observed significant discrepancies between public expressions and private thoughts, particularly in agents with friendly and extroverted personalities, suggesting that social context influences decision-making behavior. Our findings contribute to understanding how personality traits shape AI agent behavior in social settings and have implications for developing more nuanced and context-aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15497v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingjun Ren, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems</title>
      <link>https://arxiv.org/abs/2503.15511</link>
      <description>arXiv:2503.15511v1 Announce Type: cross 
Abstract: The proliferation of powerful AI capabilities and systems necessitates a commensurate focus on user trust. We introduce the Trust Calibration Maturity Model (TCMM) to capture and communicate the maturity of AI system trustworthiness. The TCMM scores maturity along 5 dimensions that drive user trust: Performance Characterization, Bias &amp; Robustness Quantification, Transparency, Safety &amp; Security, and Usability. Information captured in the TCMM can be presented along with system performance information to help a user to appropriately calibrate trust, to compare requirements with current states of development, and to clarify trustworthiness needs. We present the TCMM and demonstrate its use on two AI system-target task pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15511v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott T Steinmetz, Asmeret Naugle, Paul Schutte, Matt Sweitzer, Alex Washburne, Lisa Linville, Daniel Krofcheck, Michal Kucer, Samuel Myren</dc:creator>
    </item>
    <item>
      <title>Superhuman AI Disclosure: Impacts on Toxicity, Fairness, and Trust Vary by Expertise and Persona Attributes</title>
      <link>https://arxiv.org/abs/2503.15514</link>
      <description>arXiv:2503.15514v1 Announce Type: cross 
Abstract: As artificial intelligence demonstrates surpassing human performance across real-world tasks, disclosing superhuman capabilities poses challenges for fairness, accountability, and trust. To investigate how transparency impacts attitudes and perceptions, we introduce a grounded and validated set of synthetic personas reflecting diverse fairness concerns and technology acceptance levels. Then we evaluate responses in two contrasting domains: (1) a competitive player in StarCraft II, where strategy and high-skill gameplay often elicit toxic interactions, and (2) a cooperative personal-assistant in providing information. Across numerous interactions spanning persona profiles, we test non-disclosure versus explicit superhuman labelling under controlled game outcomes and usage contexts. Our findings reveal sharp domain-specific effects: in StarCraft II, explicitly labelling AI as superhuman, novice personas who learned of it reported lower toxicity and higher fairness-attributing defeat to advanced skill rather than hidden cheating-whereas expert personas found the disclosure statements irksome but still less deceptive than non-disclosure. Conversely, in the LLM as personal-assistant setting, disclosure of superhuman capabilities improved perceived trustworthiness, though it risked AI overreliance among certain persona segments. We release Dataset X-containing persona cards-including profile attributes, disclosure prompts, and detailed interaction logs, accompanied by reproducible protocols and disclaimers for adapting them to diverse tasks. Our results demonstrate that transparency is not a cure-all: while it reduces suspicion and enhances trust in cooperative contexts, it may inflame resistance or disappointment in competitive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15514v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaymari Chua, Chen Wang, Lina Yao</dc:creator>
    </item>
    <item>
      <title>From Divergence to Consensus: Evaluating the Role of Large Language Models in Facilitating Agreement through Adaptive Strategies</title>
      <link>https://arxiv.org/abs/2503.15521</link>
      <description>arXiv:2503.15521v1 Announce Type: cross 
Abstract: Achieving consensus in group decision-making often involves overcoming significant challenges, particularly in reconciling diverse perspectives and mitigating biases that hinder agreement. Traditional methods relying on human facilitators are often constrained by scalability and efficiency, especially in large-scale, fast-paced discussions. To address these challenges, this study proposes a novel framework employing large language models (LLMs) as automated facilitators within a custom-built multi-user chat system. Leveraging cosine similarity as a core metric, this approach evaluates the ability of three state-of-the-art LLMs- ChatGPT 4.0, Mistral Large 2, and AI21 Jamba Instruct- to synthesize consensus proposals that align with participants' viewpoints. Unlike conventional techniques, the system integrates adaptive facilitation strategies, including clarifying misunderstandings, summarizing discussions, and proposing compromises, enabling the LLMs to iteratively refine consensus proposals based on user feedback. Experimental results demonstrate the superiority of ChatGPT 4.0, which achieves higher alignment with participant opinions, requiring fewer iterations to reach consensus compared to its counterparts. Moreover, analysis reveals the nuanced performance of the models across various sustainability-focused discussion topics, such as climate action, quality education, good health and well-being, and access to clean water and sanitation. These findings highlight the transformative potential of LLM-driven facilitation for improving collective decision-making processes and underscore the importance of advancing evaluation metrics and cross-cultural adaptability in future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15521v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loukas Triantafyllopoulos, Dimitris Kalles</dc:creator>
    </item>
    <item>
      <title>The Use of Artificial Intelligence Tools in Assessing Content Validity: A Comparative Study with Human Experts</title>
      <link>https://arxiv.org/abs/2503.15525</link>
      <description>arXiv:2503.15525v1 Announce Type: cross 
Abstract: In this study, it was investigated whether AI evaluators assess the content validity of B1-level English reading comprehension test items in a manner similar to human evaluators. A 25-item multiple-choice test was developed, and these test items were evaluated by four human and four AI evaluators. No statistically significant difference was found between the scores given by human and AI evaluators, with similar evaluation trends observed. The Content Validity Ratio (CVR) and the Item Content Validity Index (I-CVI) were calculated and analyzed using the Wilcoxon Signed-Rank Test, with no statistically significant difference. The findings revealed that in some cases, AI evaluators could replace human evaluators. However, differences in specific items were thought to arise from varying interpretations of the evaluation criteria. Ensuring linguistic clarity and clearly defining criteria could contribute to more consistent evaluations. In this regard, the development of hybrid evaluation systems, in which AI technologies are used alongside human experts, is recommended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15525v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hatice Gurdil, Hatice Ozlem Anadol, Yesim Beril Soguksu</dc:creator>
    </item>
    <item>
      <title>Reflections on the Use of Dashboards in the Covid-19 Pandemic</title>
      <link>https://arxiv.org/abs/2503.15529</link>
      <description>arXiv:2503.15529v1 Announce Type: cross 
Abstract: Dashboards have arguably been the most used visualizations during the COVID-19 pandemic. They were used to communicate its evolution to national governments for disaster mitigation, to the public domain to inform about its status, and to epidemiologists to comprehend and predict the evolution of the disease. Each design had to be tailored for different tasks and to varying audiences - in many cases set up in a very short time due to the urgent need. In this paper, we collect notable examples of dashboards and reflect on their use and design during the pandemic from a user-oriented perspective: we interview a group of researchers with varying visualization expertise who actively used dashboards during the pandemic as part of their daily workflow. We discuss our findings and compile a list of lessons learned to support future visualization researchers and dashboard designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15529v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessio Arleo, Rita Borgo, J\"orn Kohlhammer, Roy Ruddle, Holger Scharlach, Xiaoru Yuan</dc:creator>
    </item>
    <item>
      <title>Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for Online Transactions</title>
      <link>https://arxiv.org/abs/2503.15546</link>
      <description>arXiv:2503.15546v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into autonomous robotic agents for conducting online transactions poses significant cybersecurity challenges. This study aims to enforce robust cybersecurity constraints to mitigate the risks associated with data breaches, transaction fraud, and system manipulation. The background focuses on the rise of LLM-driven robotic systems in e-commerce, finance, and service industries, alongside the vulnerabilities they introduce. A novel security architecture combining blockchain technology with multi-factor authentication (MFA) and real-time anomaly detection was implemented to safeguard transactions. Key performance metrics such as transaction integrity, response time, and breach detection accuracy were evaluated, showing improved security and system performance. The results highlight that the proposed architecture reduced fraudulent transactions by 90%, improved breach detection accuracy to 98%, and ensured secure transaction validation within a latency of 0.05 seconds. These findings emphasize the importance of cybersecurity in the deployment of LLM-driven robotic systems and suggest a framework adaptable to various online platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15546v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shraddha Pradipbhai Shah, Aditya Vilas Deshpande</dc:creator>
    </item>
    <item>
      <title>Homogeneous Algorithms Can Reduce Competition in Personalized Pricing</title>
      <link>https://arxiv.org/abs/2503.15634</link>
      <description>arXiv:2503.15634v1 Announce Type: cross 
Abstract: Firms' algorithm development practices are often homogeneous. Whether firms train algorithms on similar data, aim at similar benchmarks, or rely on similar pre-trained models, the result is correlated predictions. We model the impact of correlated algorithms on competition in the context of personalized pricing. Our analysis reveals that (1) higher correlation diminishes consumer welfare and (2) as consumers become more price sensitive, firms are increasingly incentivized to compromise on the accuracy of their predictions in exchange for coordination. We demonstrate our theoretical results in a stylized empirical study where two firms compete using personalized pricing algorithms. Our results underscore the ease with which algorithms facilitate price correlation without overt communication, which raises concerns about a new frontier of anti-competitive behavior. We analyze the implications of our results on the application and interpretation of US antitrust law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15634v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathanael Jo, Kathleen Creel, Ashia Wilson, Manish Raghavan</dc:creator>
    </item>
    <item>
      <title>A two-stage model leveraging friendship network for community evolution prediction in interactive networks</title>
      <link>https://arxiv.org/abs/2503.15788</link>
      <description>arXiv:2503.15788v1 Announce Type: cross 
Abstract: Interactive networks representing user participation and interactions in specific "events" are highly dynamic, with communities reflecting collective behaviors that evolve over time. Predicting these community evolutions is crucial for forecasting the trajectory of the related "event". Some models for community evolution prediction have been witnessed, but they primarily focused on coarse-grained evolution types (e.g., expand, dissolve, merge, split), often neglecting fine-grained evolution extents (e.g., the extent of community expansion). Furthermore, these models typically utilize only one network data (here is interactive network data) for dynamic community featurization, overlooking the more stable friendship network that represents the friendships between people to enrich community representations. To address these limitations, we propose a two-stage model that predicts both the type and extent of community evolution. Our model unifies multi-class classification for evolution type and regression for evolution extent within a single framework and fuses data from both interactive and friendship networks for a comprehensive community featurization. We also introduce a hybrid strategy to differentiate between evolution types that are difficult to distinguish. Experimental results on three datasets show the significant superiority of the proposed model over other models, confirming its efficacy in predicting community evolution in interactive networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15788v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Hu, Yihang Wu, Biao Cai</dc:creator>
    </item>
    <item>
      <title>ChatGPT and U(X): A Rapid Review on Measuring the User Experience</title>
      <link>https://arxiv.org/abs/2503.15808</link>
      <description>arXiv:2503.15808v1 Announce Type: cross 
Abstract: ChatGPT, powered by a large language model (LLM), has revolutionized everyday human-computer interaction (HCI) since its 2022 release. While now used by millions around the world, a coherent pathway for evaluating the user experience (UX) ChatGPT offers remains missing. In this rapid review (N = 58), I explored how ChatGPT UX has been approached quantitatively so far. I focused on the independent variables (IVs) manipulated, the dependent variables (DVs) measured, and the methods used for measurement. Findings reveal trends, gaps, and emerging consensus in UX assessments. This work offers a first step towards synthesizing existing approaches to measuring ChatGPT UX, urgent trajectories to advance standardization and breadth, and two preliminary frameworks aimed at guiding future research and tool development. I seek to elevate the field of ChatGPT UX by empowering researchers and practitioners in optimizing user interactions with ChatGPT and similar LLM-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15808v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Katie Seaborn</dc:creator>
    </item>
    <item>
      <title>Affective Polarization Amongst Swedish Politicians</title>
      <link>https://arxiv.org/abs/2503.16193</link>
      <description>arXiv:2503.16193v1 Announce Type: cross 
Abstract: This study investigates affective polarization among Swedish politicians on Twitter from 2021 to 2023, including the September 2022 parliamentary election. Analyzing over 25,000 tweets and employing large language models (LLMs) for sentiment and political classification, we distinguish between positive partisanship (support of allies) and negative partisanship (criticism of opponents).
  Our findings are contingent on the definition of the in-group. When political in-groups are defined at the ideological bloc level, negative and positive partisanship occur at similar rates. However, when the in-group is defined at the party level, negative partisanship becomes significantly more dominant and is 1.51 times more likely (1.45, 1.58). This effect is even stronger among extreme politicians, who engage in negativity more than their moderate counterparts. Negative partisanship also proves to be a strategic choice for online visibility, attracting 3.18 more likes and 1.69 more retweets on average.
  By adapting methods developed for two-party systems and leveraging LLMs for Swedish-language analysis, we provide novel insights into how multiparty politics shapes polarizing discourse. Our results underscore both the strategic appeal of negativity in digital spaces and the growing potential of LLMs for large-scale, non-English political research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16193v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois t'Serstevens, Roberto Cerina, Gustav Pepper</dc:creator>
    </item>
    <item>
      <title>Investigating The Implications of Cyberattacks Against Precision Agricultural Equipment</title>
      <link>https://arxiv.org/abs/2503.16283</link>
      <description>arXiv:2503.16283v1 Announce Type: cross 
Abstract: As various technologies are integrated and implemented into the food and agricultural industry, it is increasingly important for stakeholders throughout the sector to identify and reduce cybersecurity vulnerabilities and risks associated with these technologies. However, numerous industry and government reports suggest that many farmers and agricultural equipment manufacturers do not fully understand the cyber threats posed by modern agricultural technologies, including CAN bus-driven farming equipment. This paper addresses this knowledge gap by attempting to quantify the cybersecurity risks associated with cyberattacks on farming equipment that utilize CAN bus technology. The contribution of this paper is twofold. First, it presents a hypothetical case study, using real-world data, to illustrate the specific and wider impacts of a cyberattack on a CAN bus-driven fertilizer applicator employed in row-crop farming. Second, it establishes a foundation for future research on quantifying cybersecurity risks related to agricultural machinery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16283v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Freyhof, George Grispos, Santosh K. Pitla, William Mahoney</dc:creator>
    </item>
    <item>
      <title>Cultivating Cybersecurity: Designing a Cybersecurity Curriculum for the Food and Agriculture Sector</title>
      <link>https://arxiv.org/abs/2503.16292</link>
      <description>arXiv:2503.16292v1 Announce Type: cross 
Abstract: As technology increasingly integrates into farm settings, the food and agriculture sector has become vulnerable to cyberattacks. However, previous research has indicated that many farmers and food producers lack the cybersecurity education they require to identify and mitigate the growing number of threats and risks impacting the industry. This paper presents an ongoing research effort describing a cybersecurity initiative to educate various populations in the farming and agriculture community. The initiative proposes the development and delivery of a ten-module cybersecurity course, to create a more secure workforce, focusing on individuals who, in the past, have received minimal exposure to cybersecurity education initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16292v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Grispos, Logan Mears, Larry Loucks, William Mahoney</dc:creator>
    </item>
    <item>
      <title>Software development projects as a way for multidisciplinary soft and future skills education</title>
      <link>https://arxiv.org/abs/2502.21114</link>
      <description>arXiv:2502.21114v2 Announce Type: replace 
Abstract: Soft and future skills are in high demand in the modern job market. These skills are required for both technical and non-technical people. It is difficult to teach these competencies in a classical academic environment.
  The paper presents a possible approach to teaching in soft and future skills in a short, intensive joint project. In our case, it is a project within the Erasmus+ framework, but it can be organized in many different frameworks.
  In the project we use problem based learning, active learning and group-work teaching methodologies. Moreover, the approach put high emphasizes diversity. We arrange a set of multidisciplinary students in groups. Each group is working on software development tasks. This type of projects demand diversity, and only a part of the team needs technical skills. In our case less than half of participants had computer science background. Additionally, software development projects are usually interesting for non-technical students.
  The multicultural, multidisciplinary and international aspects are very important in a modern global working environment. On the other hand, short time of the project and its intensity allow to simulate stressful situations in a real word tasks. The effects of the project on the required competencies are measured using the KYSS method.
  The results prove that the presented method increased participants soft skills in communication, cooperation, digital skills and self reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21114v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krzysztof Podlaski, Michal Beczkowski, Katharina Simbeck, Katrin Dziergwa, Derek O'Reilly, Shane Dowdall, Joao Monteiro, Catarina Oliveira Lucas, Johanna Hautamaki, Heikki Ahonen, Hiram Bollaert, Philippe Possemiers, Zofia Stawska</dc:creator>
    </item>
    <item>
      <title>Multi-Output Distributional Fairness via Post-Processing</title>
      <link>https://arxiv.org/abs/2409.00553</link>
      <description>arXiv:2409.00553v2 Announce Type: replace-cross 
Abstract: The post-processing approaches are becoming prominent techniques to enhance machine learning models' fairness because of their intuitiveness, low computational cost, and excellent scalability. However, most existing post-processing methods are designed for task-specific fairness measures and are limited to single-output models. In this paper, we introduce a post-processing method for multi-output models, such as the ones used for multi-task/multi-class classification and representation learning, to enhance a model's distributional parity, a task-agnostic fairness measure. Existing methods for achieving distributional parity rely on the (inverse) cumulative density function of a model's output, restricting their applicability to single-output models. Extending previous works, we propose to employ optimal transport mappings to move a model's outputs across different groups towards their empirical Wasserstein barycenter. An approximation technique is applied to reduce the complexity of computing the exact barycenter and a kernel regression method is proposed to extend this process to out-of-sample data. Our empirical studies evaluate the proposed approach against various baselines on multi-task/multi-class classification and representation learning tasks, demonstrating the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00553v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gang Li, Qihang Lin, Ayush Ghosh, Tianbao Yang</dc:creator>
    </item>
  </channel>
</rss>
