<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 04:01:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotic analysis of high-dimensional uniformity tests under heavy-tailed alternatives</title>
      <link>https://arxiv.org/abs/2506.00393</link>
      <description>arXiv:2506.00393v1 Announce Type: new 
Abstract: We study the high-dimensional uniformity testing problem, which involves testing whether the underlying distribution is the uniform distribution, given $n$ data points on the $p$-dimensional unit hypersphere. While this problem has been extensively studied in scenarios with fixed $p$, only three testing procedures are known in high-dimensional settings: the Rayleigh test \cite{Cutting-P-V}, the Bingham test \cite{Cutting-P-V2}, and the packing test \cite{Jiang13}. Most existing research focuses on the former two tests, and the consistency of the packing test remains open. We show that under certain classes of alternatives involving projections of heavy-tailed distributions, the Rayleigh test is asymptotically blind, and the Bingham test has asymptotic power equivalent to random guessing. In contrast, we show theoretically that the packing test is powerful against such alternatives, and empirically that its size suffers from severe distortion due to the slow convergence nature of extreme-value statistics. By exploiting the asymptotic independence of these three tests, we then propose a new test based on Fisher's combination technique that combines their strengths. The new test is shown to enjoy all the optimality properties of each individual test, and unlike the packing test, it maintains excellent type-I error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00393v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tiefeng Jiang, Tuan Pham</dc:creator>
    </item>
    <item>
      <title>$L_2$-norm posterior contraction in Gaussian models with unknown variance</title>
      <link>https://arxiv.org/abs/2506.00401</link>
      <description>arXiv:2506.00401v1 Announce Type: new 
Abstract: The testing-based approach is a fundamental tool for establishing posterior contraction rates. Although the Hellinger metric is attractive owing to the existence of a desirable test function, it is not directly applicable in Gaussian models, because translating the Hellinger metric into more intuitive metrics typically requires strong boundedness conditions. When the variance is known, this issue can be addressed by directly constructing a test function relative to the $L_2$-metric using the likelihood ratio test. However, when the variance is unknown, existing results are limited and rely on restrictive assumptions. To overcome this limitation, we derive a test function tailored to an unknown variance setting with respect to the $L_2$-metric and provide sufficient conditions for posterior contraction based on the testing-based approach. We apply this result to analyze high-dimensional regression and nonparametric regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00401v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seonghyun Jeong</dc:creator>
    </item>
    <item>
      <title>Detecting non-uniform patterns on high-dimensional hyperspheres</title>
      <link>https://arxiv.org/abs/2506.00444</link>
      <description>arXiv:2506.00444v1 Announce Type: new 
Abstract: We propose a new probabilistic characterization of the uniform distribution on hyperspheres in terms of its inner product, extending the ideas of \cite{cuesta2009projection,cuesta2007sharp} in a data-driven manner. Using this characterization, we define a new distance that quantifies the deviation of an arbitrary distribution from uniformity. As an application, we construct a novel nonparametric test for the uniformity testing problem: determining whether a set of \(n\) i.i.d. random points on the \(p\)-dimensional hypersphere is approximately uniformly distributed. The proposed test is based on a degenerate U-process and is universally consistent in fixed-dimensional settings. Furthermore, in high-dimensional settings, it stands apart from existing tests with its simple implementation and asymptotic theory, while also possessing a model-free consistency property. Specifically, it can detect any alternative outside a ball of radius \(n^{-1/2}\) with respect to the proposed distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00444v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tiefeng Jiang, Tuan Pham</dc:creator>
    </item>
    <item>
      <title>Estimating a regression function under possible heteroscedastic and heavy-tailed errors. Application to shape-restricted regression</title>
      <link>https://arxiv.org/abs/2506.00852</link>
      <description>arXiv:2506.00852v1 Announce Type: new 
Abstract: We consider a regression framework where the design points are deterministic and the errors possibly non-i.i.d. and heavy-tailed (with a moment of order $p$ in $[1,2]$). Given a class of candidate regression functions, we propose a surrogate for the classical least squares estimator (LSE). For this new estimator, we establish a nonasymptotic risk bound with respect to the absolute loss which takes the form of an oracle type inequality. This inequality shows that our estimator possesses natural adaptation properties with respect to some elements of the class. When this class consists of monotone functions or convex functions on an interval, these adaptation properties are similar to those established in the literature for the LSE. However, unlike the LSE, we prove that our estimator remains stable with respect to a possible heteroscedasticity of the errors and may even converge at a parametric rate (up to a logarithmic factor) when the LSE is not even consistent. We illustrate the performance of this new estimator over classes of regression functions that satisfy a shape constraint: piecewise monotone, piecewise convex/concave, among other examples. The paper also contains some approximation results by splines with degrees in $\{0,1\}$ and VC bounds for the dimensions of classes of level sets. These results may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00852v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannick Baraud, Guillaume Maillard</dc:creator>
    </item>
    <item>
      <title>A Quantized Order Estimator</title>
      <link>https://arxiv.org/abs/2506.00984</link>
      <description>arXiv:2506.00984v1 Announce Type: new 
Abstract: This paper considers the order estimation problem of stochastic autoregressive exogenous input (ARX) systems by using quantized data. Based on the least squares algorithm and inspired by the control systems information criterion (CIC), a new kind of criterion aimed at addressing the inaccuracy of quantized data is proposed for ARX systems with quantized data. When the upper bounds of the system orders are known and the persistent excitation condition is satisfied, the system order estimates are shown to be consistent for small quantization step. Furthermore, a concrete method is given for choosing quantization parameters to ensure that the system order estimates are consistent. A numerical example is given to demonstrate the effectiveness of the theoretical results of the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00984v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lida Jing</dc:creator>
    </item>
    <item>
      <title>Discriminating Tail Behavior Using Halfspace Depths: Population and Empirical Perspectives</title>
      <link>https://arxiv.org/abs/2506.01126</link>
      <description>arXiv:2506.01126v1 Announce Type: new 
Abstract: We study the empirical version of halfspace depths with the objective of establishing a connection between the rates of convergence and the tail behaviour of the corresponding underlying distributions. The intricate interplay between the sample size and the parameter driving the tail behaviour forms one of the main results of this analysis. The chosen approach is mainly based on weighted empirical processes indexed by sets by Alexander (1987), which leads to relatively direct and elegant proofs, regardless of the nature of the tail. This method is further enriched by our findings on the population version, which also enable us to distinguish between light and heavy tails. These results lay the foundation for our subsequent analysis of the empirical versions. Building on these theoretical insights, we propose a methodology to assess the tail behaviour of the underlying multivariate distribution of a sample, which we illustrate on simulated data. The study concludes with an application to a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01126v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sibsankar Singha, Marie Kratz, Sreekar Vadlamani</dc:creator>
    </item>
    <item>
      <title>Exact degrees of freedom via a projector-rank partition theorem</title>
      <link>https://arxiv.org/abs/2506.01619</link>
      <description>arXiv:2506.01619v1 Announce Type: new 
Abstract: Degrees of freedom ($\mathrm{df}$) allocation becomes opaque once blocking, nesting, fractionation, or unequal replication disturb the balanced one-stratum classical ANOVA. We resolve this ambiguity with a $\mathrm{df}$ partition theorem. For $N$ observations and a set $\mathcal{S}$ that indexes all randomization strata in the experiment, let $\mathbf{P}_s$ denote the idempotent projector that averages observations within randomization stratum $s \in \mathcal{S}$ and $\mathcal C_{\overline E}$ the factorial-contrast subspace attached to the possibly aliased effect $\overline{E}$. We prove the rank identity $N-1 = \sum_{s}\sum_{\overline E}\dim\left(\mathcal C_{\overline E} \cap \operatorname{Im}\mathbf{P}_s\right)$, which simultaneously diagonalizes all strata and all effects, yielding exact integer $\mathrm{df}$ for any fixed-random mixture, block structure, or replication pattern. It results in closed-form $\mathrm{df}$ tables for complex unbalanced designs$-$including many that traditionally relied on numerical approximations. Aliasing measures such as the $\mathrm{df}$-retention ratio $\rho(\overline E)$ and the corresponding deficiency $\delta(\overline{E})$ that extend Box-Hunter's resolution concept to multi-stratum and incomplete designs, and supply immediate diagnostics of information loss, are introduced. Classical results such as Cochran's identity when $\lvert \mathcal{S} \rvert =1$ and balanced fractions, Yates' $\mathrm{df}$ for balanced split-plots, and resolution-$R$ when $\rho(\overline{E}) = 0$ emerge as corollaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01619v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nagananda K G</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Latent Variable Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2506.01893</link>
      <description>arXiv:2506.01893v1 Announce Type: new 
Abstract: Variational inference (VI) is a popular method for approximating intractable posterior distributions in Bayesian inference and probabilistic machine learning. In this paper, we introduce a general framework for quantifying the statistical accuracy of mean-field variational inference (MFVI) for posterior approximation in Bayesian latent variable models with categorical local latent variables. Utilizing our general framework, we capture the exact asymptotic regime where MFVI `works' for the celebrated latent Dirichlet allocation (LDA) model. Focusing on the mixed membership stochastic blockmodel (MMSB), we show that the vanilla fully factorized MFVI, often used in the literature, is suboptimal. We propose a partially grouped VI algorithm for this model and show that it works, and derive its exact asymptotic performance. We further illustrate that our bounds are tight for both the above models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01893v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Zhong, Sumit Mukherjee, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Multi-sample rank tests for location against Lehmann-type alternatives</title>
      <link>https://arxiv.org/abs/2506.01914</link>
      <description>arXiv:2506.01914v1 Announce Type: new 
Abstract: This paper deals with testing the equality of $k$ ($k\ge 2$) distribution functions against possible stochastic ordering among them. Two classes of rank tests are proposed for this testing problem. The statistics of the tests under study are based on precedence and exceedance statistics and are natural extension of corresponding statistics for the two-sample testing problem. Furthermore, as an extension of the Lehmann alternative for the two-sample location problem, we propose a new subclass of the general alternative for the stochastic order of multiple samples. We show that under the new Lehmann-type alternative any rank test statistics is distribution free. The power functions of the two new families of rank tests are compared to the power performance of the Jonckheere-Terpstra rank test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01914v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikolay I. Nikolov, Eugenia Stoimenova</dc:creator>
    </item>
    <item>
      <title>Overfitting has a limitation: a model-independent generalization error bound based on R\'enyi entropy</title>
      <link>https://arxiv.org/abs/2506.00182</link>
      <description>arXiv:2506.00182v1 Announce Type: cross 
Abstract: Will further scaling up of machine learning models continue to bring success? A significant challenge in answering this question lies in understanding generalization error, which is the impact of overfitting. Understanding generalization error behavior of increasingly large-scale machine learning models remains a significant area of investigation, as conventional analyses often link error bounds to model complexity, failing to fully explain the success of extremely large architectures. This research introduces a novel perspective by establishing a model-independent upper bound for generalization error applicable to algorithms whose outputs are determined solely by the data's histogram, such as empirical risk minimization or gradient-based methods. Crucially, this bound is shown to depend only on the R\'enyi entropy of the data-generating distribution, suggesting that a small generalization error can be maintained even with arbitrarily large models, provided the data quantity is sufficient relative to this entropy. This framework offers a direct explanation for the phenomenon where generalization performance degrades significantly upon injecting random noise into data, where the performance degrade is attributed to the consequent increase in the data distribution's R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be data-distribution-dependent, demonstrating that an amount of data corresponding to the R\'enyi entropy is indeed essential for successful learning, thereby highlighting the tightness of our proposed generalization bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00182v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Suzuki</dc:creator>
    </item>
    <item>
      <title>Riemannian Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2506.00226</link>
      <description>arXiv:2506.00226v1 Announce Type: cross 
Abstract: This paper proposes an innovative extension of Principal Component Analysis (PCA) that transcends the traditional assumption of data lying in Euclidean space, enabling its application to data on Riemannian manifolds. The primary challenge addressed is the lack of vector space operations on such manifolds. Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA) as a geometric approach to analyze data on Riemannian manifolds, particularly effective for structured datasets like medical images, where the manifold's intrinsic structure is apparent. However, PGA's applicability is limited when dealing with general datasets that lack an implicit local distance notion. In this work, we introduce a generalized framework, termed {\em Riemannian Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with a local distance structure. Specifically, we adapt the PCA methodology to Riemannian manifolds by equipping data tables with local metrics, enabling the incorporation of manifold geometry. This framework provides a unified approach for dimensionality reduction and statistical analysis directly on manifolds, opening new possibilities for datasets with region-specific or part-specific distance notions, ensuring respect for their intrinsic geometric properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00226v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Oldemar Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Symmetrization for high dimensional dependent random variables</title>
      <link>https://arxiv.org/abs/2506.00547</link>
      <description>arXiv:2506.00547v1 Announce Type: cross 
Abstract: We establish a generic symmetrization property for dependent random variables $\{x_{t}\}_{t=1}^{n}$ on $\mathbb{R}^{p}$, where $p$ $&gt;&gt;$ $n$ is allowed. We link $\mathbb{E}\psi (\max_{1\leq i\leq p}|1/n\sum_{t=1}^{n}(x_{i,t}$ $-$ $\mathbb{E}x_{i,t})|)$ to $\mathbb{E}\psi (\max_{1\leq i\leq p}|1/n$ $\sum_{t=1}^{n}\eta _{t}(x_{i,t}$ $-$ $\mathbb{E}% x_{i,t})|)$ for non-decreasing convex $\psi $ $:$ $[0,\infty )$ $\rightarrow $ $\mathbb{R}$, where $\{\eta _{t}\}_{t=1}^{n}$ are block-wise independent random variables, with a remainder term based on high dimensional Gaussian approximations that need not hold at a high level. Conventional usage of $% \eta _{t}(x_{i,t}$ $-$ $\tilde{x}_{i,t})$ with $\{\tilde{x}% _{i,t}\}_{t=1}^{n} $ an independent copy of $\{x_{i,t}\}_{t=1}^{n}$, and Rademacher $\eta _{t}$, is not required in a generic environment, although we may trivially replace $\mathbb{E}x_{i,t}$ with $\tilde{x}_{i,t}$. In the latter case with Rademacher $\eta _{t}$ our result reduces to classic symmetrization under independence. We bound and therefore verify the Gaussian approximations in mixing and physical dependence settings, thus bounding $\mathbb{E}\psi (\max_{1\leq i\leq p}|1/n\sum_{t=1}^{n}(x_{i,t}$ $-$ $\mathbb{E}x_{i,t})|)$; and apply the main result to a generic % Nemirovski (2000)-like $\mathcal{L}_{q}$-maximal moment bound for $\mathbb{E}\max_{1\leq i\leq p}|1/n\sum_{t=1}^{n}(x_{i,t}$ $-$ $\mathbb{E}x_{i,t})|^{q}$, $q$ $\geq $ $1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00547v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan B. Hill</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification of synchrosqueezing transform under complicated nonstationary noise</title>
      <link>https://arxiv.org/abs/2506.00779</link>
      <description>arXiv:2506.00779v1 Announce Type: cross 
Abstract: We propose a bootstrapping algorithm to quantify the uncertainty of the time-frequency representation (TFR) generated by the short-time Fourier transform (STFT)-based synchrosqueezing transform (SST) when the input signal is oscillatory with time-varying amplitude and frequency and contaminated by complex nonstationary noise. To this end, we leverage a recently developed high-dimensional Gaussian approximation technique to establish a sequential Gaussian approximation for nonstationary random processes under mild assumptions. This result is of independent interest and enables us to quantify the approximate Gaussianity of the random field over the time-frequency domain induced by the STFT. Building on this foundation, we establish the robustness of SST-based signal decomposition in the presence of nonstationary noise. Furthermore, under the assumption that the noise is locally stationary, we develop a Gaussian auto-regressive bootstrap framework for uncertainty quantification of the TFR obtained via SST and provide a theoretical justification. We validate the proposed method through simulated examples and demonstrate its utility by analyzing spindle activity in electroencephalogram recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00779v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hau-Tieng Wu, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Estimating Unobservable States in Stochastic Epidemic Models with Partial Information</title>
      <link>https://arxiv.org/abs/2506.00906</link>
      <description>arXiv:2506.00906v1 Announce Type: cross 
Abstract: This article investigates stochastic epidemic models with partial information and addresses the estimation of current values of not directly observable states. The latter is also called nowcasting and related to the so-called "dark figure" problem, which concerns, for example, the estimation of unknown numbers of asymptomatic and undetected infections. The study is based on Ouabo Kamkumo et al. (2025), which provides detailed information about stochastic multi-compartment epidemic models with partial information and various examples. Starting point is a description of the state dynamics by a system of nonlinear stochastic recursions resulting from a time-discretization of a diffusion approximation of the underlying counting processes. The state vector is decomposed into an observable and an unobservable component. The latter is estimated from the observations using the extended Kalman filter approach in order to take into account the nonlinearity of the state dynamics. Numerical simulations for a Covid-19 model with partial information are presented to verify the performance and accuracy of the estimation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00906v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Ouabo Kamkumo, Ibrahim Mbouandi Njiasse, Ralf Wunderlich</dc:creator>
    </item>
    <item>
      <title>Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation</title>
      <link>https://arxiv.org/abs/2506.01267</link>
      <description>arXiv:2506.01267v1 Announce Type: cross 
Abstract: Despite tremendous advancements of machine learning models and algorithms in various application domains, they are known to be vulnerable to subtle, natural or intentionally crafted perturbations in future input data, known as adversarial attacks. While numerous adversarial learning methods have been proposed, fundamental questions about their statistical optimality in robust loss remain largely unanswered. In particular, the minimax rate of convergence and the construction of rate-optimal estimators under future $X$-attacks are yet to be worked out.
  In this paper, we address this issue in the context of nonparametric regression, under suitable assumptions on the smoothness of the regression function and the geometric structure of the input perturbation set. We first establish the minimax rate of convergence under adversarial $L_q$-risks with $1 \leq q \leq \infty$ and propose a piecewise local polynomial estimator that achieves the minimax optimality. The established minimax rate elucidates how the smoothness level and perturbation magnitude affect the fundamental limit of adversarial learning under future $X$-attacks. Furthermore, we construct a data-driven adaptive estimator that is shown to achieve, within a logarithmic factor, the optimal rate across a broad scale of nonparametric and adversarial classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01267v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingfu Peng, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Characterization based Goodness-of-Fit for Generalized Pareto Distribution: A Blend of Stein's Identity and Dynamic Survival Extropy</title>
      <link>https://arxiv.org/abs/2506.01473</link>
      <description>arXiv:2506.01473v1 Announce Type: cross 
Abstract: This paper proposes a goodness of fit test for the generalized Pareto distribution (GPD). Firstly, we provide two characterizations of GPD based on Stein's identity and dynamic survival extropy. These characterizations are used to test GPD separately for the positive and negative shape parameter cases. A Monte Carlo simulation is conducted to provide the critical values and power of the proposed test against a good number of alternatives. Our test is simple to use and it has asymptotic normality and relatively high power, which strengthened the purpose of proposing it. Considering the case of right censored data, we provide the procedure to handle censored case too. A few real-life applications are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01473v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Kandpal, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Examining marginal properness in the external validation of survival models with squared and logarithmic losses</title>
      <link>https://arxiv.org/abs/2212.05260</link>
      <description>arXiv:2212.05260v3 Announce Type: replace 
Abstract: Scoring rules promote rational and honest decision-making, which is important for model evaluation and becoming increasingly important for automated procedures such as `AutoML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis, with a focus on their theoretical and empirical properness. We introduce a marginal definition of properness and show that both the Integrated Survival Brier Score (ISBS) and the Right-Censored Log-Likelihood (RCLL) are theoretically improper under this definition. We also investigate a new class of losses that may inform future survival scoring rules. Simulation experiments reveal that both the ISBS and RCLL behave as proper scoring rules in practice. The RCLL showed no violations across all settings, while ISBS exhibited only minor, negligible violations at extremely small sample sizes, suggesting one can trust results from historical experiments. As such we advocate for both the RCLL and ISBS in external validation of models, including in automated procedures. However, we note practical challenges in estimating these losses including estimation of censoring distributions and densities; as such further research is required to advance development of robust and honest evaluation in survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05260v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Sonabend, John Zobolas, Riccardo Be Bin, Philipp Kopper, Lukas Burk, Andreas Bender</dc:creator>
    </item>
    <item>
      <title>Negative Moment Bounds for Sample Autocovariance Matrices of Stationary Processes Driven by Conditional Heteroscedastic Errors and Their Applications</title>
      <link>https://arxiv.org/abs/2301.07476</link>
      <description>arXiv:2301.07476v5 Announce Type: replace 
Abstract: We establish a negative moment bound for the sample autocovariance matrix of a stationary process driven by conditional heteroscedastic errors. This moment bound enables us to asymptotically express the mean squared prediction error (MSPE) of the least squares predictor as the sum of three terms related to model complexity, model misspecification, and conditional heteroscedasticity. A direct application of this expression is the development of a model selection criterion that can asymptotically identify the best (in the sense of MSPE) subset AR model in the presence of misspecification and conditional heteroscedasticity. Finally, numerical simulations are conducted to confirm our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07476v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsueh-Han Huang, Shu-Hui Yu, Ching-Kang Ing</dc:creator>
    </item>
    <item>
      <title>Persistence Diagram Estimation of Multivariate Piecewise H\"older-continuous Signals</title>
      <link>https://arxiv.org/abs/2403.19396</link>
      <description>arXiv:2403.19396v4 Announce Type: replace 
Abstract: To our knowledge, the analysis of convergence rates for persistence diagrams estimation from noisy signals has predominantly relied on lifting signal estimation results through sup-norm (or other functional norm) stability theorems. We believe that moving forward from this approach can lead to considerable gains. We illustrate it in the setting of nonparametric regression. From a minimax perspective, we examine the inference of persistence diagrams (for the sublevel sets filtration). We show that for piecewise H\"older-continuous functions, with control over the reach of the set of discontinuities, taking the persistence diagram coming from a simple histogram estimator of the signal permits achieving the minimax rates known for H\"older-continuous functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19396v4</guid>
      <category>math.ST</category>
      <category>math.AT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Henneuse</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v2 Announce Type: replace 
Abstract: This paper examines the construction of confidence sets for parameters defined as linear functionals of a function of W and X whose conditional mean given Z and X equals the conditional mean of another variable Y given Z and X. Many estimands of interest in causal inference can be expressed in this form, including the average treatment effect in proximal causal inference and treatment effect contrasts in instrumental variable models. We derive a necessary condition for a confidence set to be uniformly valid over a model that allows for the dependence between W and Z given X to be arbitrarily weak. Specifically, we show that for any such confidence set, there must exist some laws in the model under which, with high probability, the confidence set has a diameter greater than or equal to the diameter of the parameter's range. In particular, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid over the aforementioned model. Furthermore, we argue that inverting the score test, a successful approach in that literature, generally fails for the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables, but possibly Y, are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Bayesian Geostatistics Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2304.12414</link>
      <description>arXiv:2304.12414v3 Announce Type: replace-cross 
Abstract: We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12414v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Wenpin Tang, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Kernel $\epsilon$-Greedy for Multi-Armed Bandits with Covariates</title>
      <link>https://arxiv.org/abs/2306.17329</link>
      <description>arXiv:2306.17329v2 Announce Type: replace-cross 
Abstract: We consider the $\epsilon$-greedy strategy for the multi-arm bandit with covariates (MABC) problem, where the mean reward functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). We propose to estimate the unknown mean reward functions using an online weighted kernel ridge regression estimator, and show the resultant estimator to be consistent under appropriate decay rates of the exploration probability sequence, $\{\epsilon_t\}_t$, and regularization parameter, $\{\lambda_t\}_t$. Moreover, we show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17329v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Perturbative partial moment matching and gradient-flow adaptive importance sampling transformations for Bayesian leave one out cross-validation</title>
      <link>https://arxiv.org/abs/2402.08151</link>
      <description>arXiv:2402.08151v3 Announce Type: replace-cross 
Abstract: Importance sampling (IS) allows one to approximate leave one out (LOO) cross-validation for a Bayesian model, without refitting, by inverting the Bayesian update equation to subtract a given data point from a model posterior. For each data point, one computes expectations under the corresponding LOO posterior by weighted averaging over the full data posterior. This task sometimes requires weight stabilization in the form of adapting the posterior distribution via transformation. So long as one is successful in finding a suitable transformation, one avoids refitting. To this end, we motivate the use of bijective perturbative transformations of the form $T(\boldsymbol{\theta})=\boldsymbol{\theta} + h Q(\boldsymbol{\theta}),$ for $0&lt;h\ll 1,$ and introduce two classes of such transformations: 1) partial moment matching and 2) gradient flow evolution. The former extends prior literature on moment-matching under the recognition that adaptation for LOO is a small perturbation on the full data posterior. The latter class of methods define transformations based on relaxing various statistical objectives: in our case the variance of the IS estimator and the KL divergence between the transformed distribution and the statistics of the LOO fold. Being model-specific, the gradient flow transformations require evaluating Jacobian determinants. While these quantities are generally readily available through auto-differentiation, we derive closed-form expressions in the case of logistic regression and shallow ReLU activated neural networks. We tested the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08151v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow</dc:creator>
    </item>
    <item>
      <title>Kernel-based Optimally Weighted Conformal Prediction Intervals</title>
      <link>https://arxiv.org/abs/2405.16828</link>
      <description>arXiv:2405.16828v2 Announce Type: replace-cross 
Abstract: In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real and synthetic time-series data against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16828v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonghyeok Lee, Chen Xu, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Probability for Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2407.01171</link>
      <description>arXiv:2407.01171v2 Announce Type: replace-cross 
Abstract: We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of com- plex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01171v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems (NeurIPS) 2024</arxiv:journal_reference>
      <dc:creator>Vladimir R. Kostic, Karim Lounici, Gregoire Pacreau, Pietro Novelli, Giacomo Turri, Massimiliano Pontil</dc:creator>
    </item>
    <item>
      <title>Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees</title>
      <link>https://arxiv.org/abs/2410.08934</link>
      <description>arXiv:2410.08934v4 Announce Type: replace-cross 
Abstract: Personalized federated learning (PFL) offers a flexible framework for aggregating information across distributed clients with heterogeneous data. This work considers a personalized federated learning setting that simultaneously learns global and local models. While purely local training has no communication cost, collaborative learning among the clients can leverage shared knowledge to improve statistical accuracy, presenting an accuracy-communication trade-off in personalized federated learning. However, the theoretical analysis of how personalization quantitatively influences sample and algorithmic efficiency and their inherent trade-off is largely unexplored. This paper makes a contribution towards filling this gap, by providing a quantitative characterization of the personalization degree on the tradeoff. The results further offers theoretical insights for choosing the personalization degree. As a side contribution, we establish the minimax optimality in terms of statistical accuracy for a widely studied PFL formulation. The theoretical result is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08934v4</guid>
      <category>stat.ML</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li</dc:creator>
    </item>
    <item>
      <title>Ergodic Network Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.17779</link>
      <description>arXiv:2412.17779v3 Announce Type: replace-cross 
Abstract: We propose a novel framework for Network Stochastic Differential Equations (N-SDE), where each node in a network is governed by an SDE influenced by interactions with its neighbors. The evolution of each node is driven by the interplay of three key components: the node's intrinsic dynamics (\emph{momentum effect}), feedback from neighboring nodes (\emph{network effect}), and a \emph{stochastic volatility} term modeled by Brownian motion. Our primary objective is to estimate the parameters of the N-SDE system from high-frequency discrete-time observations. The motivation behind this model lies in its ability to analyze very high-dimensional time series by leveraging the inherent sparsity of the underlying network graph. We consider two distinct scenarios: \textit{i) known network structure}: the graph is fully specified, and we establish conditions under which the parameters can be identified, considering the linear growth of the parameter space with the number of edges. \textit{ii) unknown network structure}: the graph must be inferred from the data. For this, we develop an iterative procedure using adaptive Lasso, tailored to a specific subclass of N-SDE models. In this work, we assume the network graph is oriented, paving the way for novel applications of SDEs in causal inference, enabling the study of cause-effect relationships in dynamic systems. Through extensive simulation studies, we demonstrate the performance of our estimators across various graph topologies in high-dimensional settings. We also showcase the framework's applicability to real-world datasets, highlighting its potential for advancing the analysis of complex networked systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17779v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francesco Iafrate, Stefano Iacus</dc:creator>
    </item>
    <item>
      <title>Martingale Approach to Gambler's Ruin Problem for Correlated Random Walks</title>
      <link>https://arxiv.org/abs/2501.10302</link>
      <description>arXiv:2501.10302v2 Announce Type: replace-cross 
Abstract: The gambler's ruin problem for correlated random walks (CRW), both with and without delays, is addressed using the Optional Stopping Theorem for martingales. We derive closed-form expressions for the ruin probabilities and the expected game duration for CRW with increments $\{1,-1\}$ and for symmetric CRW with increments $\{1,0,-1\}$ (CRW with delays). Additionally, a martingale technique is developed for general CRW with delays. The gambler's ruin probability for a game involving bets on two arbitrary patterns is also examined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10302v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Pozdnyakov</dc:creator>
    </item>
    <item>
      <title>Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</title>
      <link>https://arxiv.org/abs/2502.06051</link>
      <description>arXiv:2502.06051v2 Announce Type: replace-cross 
Abstract: Although many popular reinforcement learning algorithms are underpinned by $f$-divergence regularization, their sample complexity with respect to the \emph{regularized objective} still lacks a tight characterization. In this paper, we analyze $f$-divergence-regularized offline policy learning. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we give the first $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for contextual bandits, surpassing existing $\tilde{O}(\epsilon^{-1})$ bound under all-policy concentrability and $\tilde{O}(\epsilon^{-2})$ bound under single-policy concentrability. Our analysis for general function approximation leverages the principle of pessimism in the face of uncertainty to refine a mean-value-type argument to its extreme. This in turn leads to a novel moment-based technique, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. We further propose a lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the strong convexity of reverse KL. In addition, for $f$-divergences with strongly convex $f$, to which reverse KL \emph{does not} belong, we show that the sharp sample complexity $\tilde{\Theta}(\epsilon^{-1})$ is achievable even without single-policy concentrability. In this case, the algorithm design can get rid of pessimistic estimators. We further extend our analysis to dueling bandits, and we believe these results take a significant step toward a comprehensive understanding of $f$-divergence-regularized policy learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06051v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingyue Zhao, Kaixuan Ji, Heyang Zhao, Tong Zhang, Quanquan Gu</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Allocations for Binary Responses: Insights from Considering Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v4 Announce Type: replace-cross 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a fixed variance level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
  </channel>
</rss>
