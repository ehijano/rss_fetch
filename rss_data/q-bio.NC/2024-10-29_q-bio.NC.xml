<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:04:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Linton Stereo Illusion: Response on Johnston (1991)</title>
      <link>https://arxiv.org/abs/2410.19740</link>
      <description>arXiv:2410.19740v1 Announce Type: new 
Abstract: In (Linton, 2024) I present a new illusion (the 'Linton Stereo Illusion') that challenges our understanding of stereo vision. A vision scientist has shared their own analysis of the 'Linton Stereo Illusion' (titled: 'There is no challenge to our understanding of stereo vision: Response to Linton and Kriegeskorte (ECVP 2024 and ArXiv:2408.00770)') claiming that the 'Linton Stereo Illusion' is fully explained by Johnston (1991). I regard Johnston (1991) as one of the most important stereo vision papers in our young (&lt; 200-year-old) field, and so this challenge requires a response. In this paper I explain why Johnston (1991) cannot explain the 'Linton Stereo Illusion'. Indeed, Johnston (1991) makes predictions that are the exact opposite of those observed in the 'Linton Stereo Illusion'. I also highlight a key concern with Johnston (1991)'s account that has so far been overlooked. Johnston (1991)'s account predicts that vergence eye movements will cause massive stereo distortions, leading to a world of unstable stereo perception. But this simply does not reflect our visual experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19740v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Linton</dc:creator>
    </item>
    <item>
      <title>The Geometry of Concepts: Sparse Autoencoder Feature Structure</title>
      <link>https://arxiv.org/abs/2410.19750</link>
      <description>arXiv:2410.19750v1 Announce Type: new 
Abstract: Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The "atomic" small-scale structure contains "crystals" whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man-woman-king-queen). We find that the quality of such parallelograms and associated function vectors improves greatly when projecting out global distractor directions such as word length, which is efficiently done with linear discriminant analysis. 2) The "brain" intermediate-scale structure has significant spatial modularity; for example, math and code features form a "lobe" akin to functional lobes seen in neural fMRI images. We quantify the spatial locality of these lobes with multiple metrics and find that clusters of co-occurring features, at coarse enough scale, also cluster together spatially far more than one would expect if feature geometry were random. 3) The "galaxy" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We also quantify how the clustering entropy depends on the layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19750v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxiao Li, Eric J. Michaud, David D. Baek, Joshua Engels, Xiaoqing Sun, Max Tegmark</dc:creator>
    </item>
    <item>
      <title>EEG Based Decoding of the Perception and Regulation of Taboo Words</title>
      <link>https://arxiv.org/abs/2410.19953</link>
      <description>arXiv:2410.19953v1 Announce Type: new 
Abstract: In daily interactions, emotions are frequently conveyed and triggered through verbal exchanges. Sometimes, we must modulate our emotional reactions to align with societal norms. Among the emotional words, taboo words represent a specific category that has been poorly studied. One intriguing question is whether these word categories can be predicted from EEG responses with the use of machine learning methods. To address this question, Support Vector Machine (SVM) was applied to decode the word categories from Event Related Potential (ERP) in 40 native Italian speakers. 240 neutral, negative and taboo words were used to this aim. Results indicate that the SVM classifier successfully distinguished between the three-word categories, with significant differences in neural activity ascribed to the late positive potential mainly detected in the central-parietal-occipital and anterior right scalp areas in the time windows of 450-649 ms and 650-850 ms. These findings were in line with the established distribution pattern of the late positive potential. Intriguingly, the study also revealed that word categories were still detectable in the regulate condition. This study extends previous results on the domain of the cortical responses of taboo words, and how machine learning methods can be used to predict word categories from EEG responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19953v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parisa Ahmadi Ghomroudi, Michele Scaltritti, Bianca Monachesi, Peera Wongupparaj, Remo Job, Alessandro Grecucci</dc:creator>
    </item>
    <item>
      <title>Roles of LLMs in the Overall Mental Architecture</title>
      <link>https://arxiv.org/abs/2410.20037</link>
      <description>arXiv:2410.20037v1 Announce Type: new 
Abstract: To better understand existing LLMs, we may examine the human mental (cognitive/psychological) architecture, and its components and structures. Based on psychological, philosophical, and cognitive science literatures, it is argued that, within the human mental architecture, existing LLMs correspond well with implicit mental processes (intuition, instinct, and so on). However, beyond such implicit processes, explicit processes (with better symbolic capabilities) are also present within the human mental architecture, judging from psychological, philosophical, and cognitive science literatures. Various theoretical and empirical issues and questions in this regard are explored. Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs. The results are synergistic combinations (in several different senses simultaneously).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20037v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ron Sun</dc:creator>
    </item>
    <item>
      <title>LinBridge: A Learnable Framework for Interpreting Nonlinear Neural Encoding Models</title>
      <link>https://arxiv.org/abs/2410.20053</link>
      <description>arXiv:2410.20053v1 Announce Type: new 
Abstract: Neural encoding of artificial neural networks (ANNs) links their computational representations to brain responses, offering insights into how the brain processes information. Current studies mostly use linear encoding models for clarity, even though brain responses are often nonlinear. This has sparked interest in developing nonlinear encoding models that are still interpretable. To address this problem, we propose LinBridge, a learnable and flexible framework based on Jacobian analysis for interpreting nonlinear encoding models. LinBridge posits that the nonlinear mapping between ANN representations and neural responses can be factorized into a linear inherent component that approximates the complex nonlinear relationship, and a mapping bias that captures sample-selective nonlinearity. The Jacobian matrix, which reflects output change rates relative to input, enables the analysis of sample-selective mapping in nonlinear models. LinBridge employs a self-supervised learning strategy to extract both the linear inherent component and nonlinear mapping biases from the Jacobian matrices of the test set, allowing it to adapt effectively to various nonlinear encoding models. We validate the LinBridge framework in the scenario of neural visual encoding, using computational visual representations from CLIP-ViT to predict brain activity recorded via functional magnetic resonance imaging (fMRI). Our experimental results demonstrate that: 1) the linear inherent component extracted by LinBridge accurately reflects the complex mappings of nonlinear neural encoding models; 2) the sample-selective mapping bias elucidates the variability of nonlinearity across different levels of the visual processing hierarchy. This study presents a novel tool for interpreting nonlinear neural encoding models and offers fresh evidence about hierarchical nonlinearity distribution in the visual cortex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20053v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohui Gao, Yue Cheng, Peiyang Li, Yijie Niu, Yifan Ren, Yiheng Liu, Haiyang Sun, Zhuoyi Li, Weiwei Xing, Xintao Hu</dc:creator>
    </item>
    <item>
      <title>Conformal models for hypercolumns in the primary visual cortex V1</title>
      <link>https://arxiv.org/abs/2410.20184</link>
      <description>arXiv:2410.20184v1 Announce Type: new 
Abstract: We propose a differential geometric model of hypercolumns in the primary visual cortex V1 that combines features of the symplectic model of the primary visual cortex by A. Sarti, G. Citti and J. Petitot and of the spherical model of hypercolumns by P. Bressloff and J. Cowan. The model is based on classical results in Conformal Geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20184v1</guid>
      <category>q-bio.NC</category>
      <category>math.DG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. V. Alekseevsky, A. Spiro</dc:creator>
    </item>
    <item>
      <title>Murine AI excels at cats and cheese: Structural differences between human and mouse neurons and their implementation in generative AIs</title>
      <link>https://arxiv.org/abs/2410.20735</link>
      <description>arXiv:2410.20735v1 Announce Type: new 
Abstract: Mouse and human brains have different functions that depend on their neuronal networks. In this study, we analyzed nanometer-scale three-dimensional structures of brain tissues of the mouse medial prefrontal cortex and compared them with structures of the human anterior cingulate cortex. The obtained results indicated that mouse neuronal somata are smaller and neurites are thinner than those of human neurons. These structural features allow mouse neurons to be integrated in the limited space of the brain, though thin neurites should suppress distal connections according to cable theory. We implemented this mouse-mimetic constraint in convolutional layers of a generative adversarial network (GAN) and a denoising diffusion implicit model (DDIM), which were then subjected to image generation tasks using photo datasets of cat faces, cheese, human faces, and birds. The mouse-mimetic GAN outperformed a standard GAN in the image generation task using the cat faces and cheese photo datasets, but underperformed for human faces and birds. The mouse-mimetic DDIM gave similar results, suggesting that the nature of the datasets affected the results. Analyses of the four datasets indicated differences in their image entropy, which should influence the number of parameters required for image generation. The preferences of the mouse-mimetic AIs coincided with the impressions commonly associated with mice. The relationship between the neuronal network and brain function should be investigated by implementing other biological findings in artificial neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20735v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rino Saiga, Kaede Shiga, Yo Maruta, Chie Inomoto, Hiroshi Kajiwara, Naoya Nakamura, Yu Kakimoto, Yoshiro Yamamoto, Masahiro Yasutake, Masayuki Uesugi, Akihisa Takeuchi, Kentaro Uesugi, Yasuko Terada, Yoshio Suzuki, Viktor Nikitin, Vincent De Andrade, Francesco De Carlo, Yuichi Yamashita, Masanari Itokawa, Soichiro Ide, Kazutaka Ikeda, Ryuta Mizutani</dc:creator>
    </item>
    <item>
      <title>Peripheral brain interfacing: Reading high-frequency brain signals from the output of the nervous system</title>
      <link>https://arxiv.org/abs/2410.20872</link>
      <description>arXiv:2410.20872v1 Announce Type: new 
Abstract: Accurate and robust recording and decoding from the central nervous system (CNS) is essential for advances in human-machine interfacing. However, technologies used to directly measure CNS activity are limited by their resolution, sensitivity to interferences, and invasiveness. Advances in muscle recordings and deep learning allow us to decode the spiking activity of spinal motor neurons (MNs) in real time and with high accuracy. MNs represent the motor output layer of the CNS, receiving and sampling signals originating in different regions in the nervous system, and generating the neural commands that control muscles. The input signals to MNs can be estimated from the MN outputs. Here we argue that peripheral neural interfaces using muscle sensors represent a promising, non-invasive approach to estimate some neural activity from the CNS that reaches the MNs but does not directly modulate force production. We also discuss the evidence supporting this concept, and the necessary advances to consolidate and test MN-based CNS interfaces in controlled and real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20872v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaime Ib\'a\~nez, Blanka Zicher, Etienne Burdet, Stuart N. Baker, Carsten Mehring, Dario Farina</dc:creator>
    </item>
    <item>
      <title>BSD: a Bayesian framework for parametric models of neural spectra</title>
      <link>https://arxiv.org/abs/2410.20896</link>
      <description>arXiv:2410.20896v1 Announce Type: new 
Abstract: The analysis of neural power spectra plays a crucial role in understanding brain function and dysfunction. While recent efforts have led to the development of methods for decomposing spectral data, challenges remain in performing statistical analysis and group-level comparisons. Here, we introduce Bayesian Spectral Decomposition (BSD), a Bayesian framework for analysing neural spectral power. BSD allows for the specification, inversion, comparison, and analysis of parametric models of neural spectra, addressing limitations of existing methods. We first establish the face validity of BSD on simulated data and show how it outperforms an established method (\fooof{}) for peak detection on artificial spectral data. We then demonstrate the efficacy of BSD on a group-level study of EEG spectra in 204 healthy subjects from the LEMON dataset. Our results not only highlight the effectiveness of BSD in model selection and parameter estimation, but also illustrate how BSD enables straightforward group-level regression of the effect of continuous covariates such as age. By using Bayesian inference techniques, BSD provides a robust framework for studying neural spectral data and their relationship to brain function and dysfunction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20896v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Nicholas A. Alexander, Robert A. Seymour, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>The Effect of Acute Stress on the Interpretability and Generalization of Schizophrenia Predictive Machine Learning Models</title>
      <link>https://arxiv.org/abs/2410.19739</link>
      <description>arXiv:2410.19739v1 Announce Type: cross 
Abstract: Introduction Schizophrenia is a severe mental disorder, and early diagnosis is key to improving outcomes. Its complexity makes predicting onset and progression challenging. EEG has emerged as a valuable tool for studying schizophrenia, with machine learning increasingly applied for diagnosis. This paper assesses the accuracy of ML models for predicting schizophrenia and examines the impact of stress during EEG recording on model performance. We integrate acute stress prediction into the analysis, showing that overlapping conditions like stress during recording can negatively affect model accuracy.
  Methods Four XGBoost models were built: one for stress prediction, two to classify schizophrenia (at rest and task), and a model to predict schizophrenia for both conditions. XAI techniques were applied to analyze results. Experiments tested the generalization of schizophrenia models using their datasets' healthy controls and independent health-screened controls. The stress model identified high-stress subjects, who were excluded from further analysis. A novel method was used to adjust EEG frequency band power to remove stress artifacts, improving predictive model performance.
  Results Our results show that acute stress vary across EEG sessions, affecting model performance and accuracy. Generalization improved once these varying stress levels were considered and compensated for during model training. Our findings highlight the importance of thorough health screening and management of the patient's condition during the process. Stress induced during or by the EEG recording can adversely affect model generalization. This may require further preprocessing of data by treating stress as an additional physiological artifact. Our proposed approach to compensate for stress artifacts in EEG data used for training models showed a significant improvement in predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19739v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gideon Vos, Maryam Ebrahimpour, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi</dc:creator>
    </item>
    <item>
      <title>Single-word Auditory Attention Decoding Using Deep Learning Model</title>
      <link>https://arxiv.org/abs/2410.19793</link>
      <description>arXiv:2410.19793v1 Announce Type: cross 
Abstract: Identifying auditory attention by comparing auditory stimuli and corresponding brain responses, is known as auditory attention decoding (AAD). The majority of AAD algorithms utilize the so-called envelope entrainment mechanism, whereby auditory attention is identified by how the envelope of the auditory stream drives variation in the electroencephalography (EEG) signal. However, neural processing can also be decoded based on endogenous cognitive responses, in this case, neural responses evoked by attention to specific words in a speech stream. This approach is largely unexplored in the field of AAD but leads to a single-word auditory attention decoding problem in which an epoch of an EEG signal timed to a specific word is labeled as attended or unattended. This paper presents a deep learning approach, based on EEGNet, to address this challenge. We conducted a subject-independent evaluation on an event-based AAD dataset with three different paradigms: word category oddball, word category with competing speakers, and competing speech streams with targets. The results demonstrate that the adapted model is capable of exploiting cognitive-related spatiotemporal EEG features and achieving at least 58% accuracy on the most realistic competing paradigm for the unseen subjects. To our knowledge, this is the first study dealing with this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19793v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nhan Duc Thanh Nguyen, Huy Phan, Kaare Mikkelsen, Preben Kidmose</dc:creator>
    </item>
    <item>
      <title>Training Compute-Optimal Vision Transformers for Brain Encoding</title>
      <link>https://arxiv.org/abs/2410.19810</link>
      <description>arXiv:2410.19810v1 Announce Type: cross 
Abstract: The optimal training of a vision transformer for brain encoding depends on three factors: model size, data size, and computational resources. This study investigates these three pillars, focusing on the effects of data scaling, model scaling, and high-performance computing on brain encoding results. Using VideoGPT to extract efficient spatiotemporal features from videos and training a Ridge model to predict brain activity based on these features, we conducted benchmark experiments with varying data sizes (10k, 100k, 1M, 6M) and different model configurations of GPT-2, including hidden layer dimensions, number of layers, and number of attention heads. We also evaluated the effects of training models with 32-bit vs 16-bit floating point representations. Our results demonstrate that increasing the hidden layer dimensions significantly improves brain encoding performance, as evidenced by higher Pearson correlation coefficients across all subjects. In contrast, the number of attention heads does not have a significant effect on the encoding results. Additionally, increasing the number of layers shows some improvement in brain encoding correlations, but the trend is not as consistent as that observed with hidden layer dimensions. The data scaling results show that larger training datasets lead to improved brain encoding performance, with the highest Pearson correlation coefficients observed for the largest dataset size (6M). These findings highlight that the effects of data scaling are more significant compared to model scaling in enhancing brain encoding performance. Furthermore, we explored the impact of floating-point precision by comparing 32-bit and 16-bit representations. Training with 16-bit precision yielded the same brain encoding accuracy as 32-bit, while reducing training time by 1.17 times, demonstrating its efficiency for high-performance computing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19810v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sana Ahmadi, Francois Paugam, Tristan Glatard, Pierre Lune Bellec</dc:creator>
    </item>
    <item>
      <title>Resolving Domain Shift For Representations Of Speech In Non-Invasive Brain Recordings</title>
      <link>https://arxiv.org/abs/2410.19986</link>
      <description>arXiv:2410.19986v1 Announce Type: cross 
Abstract: Machine learning techniques have enabled researchers to leverage neuroimaging data to decode speech from brain activity, with some amazing recent successes achieved by applications built using invasive devices. However, research requiring surgical implants has a number of practical limitations. Non-invasive neuroimaging techniques provide an alternative but come with their own set of challenges, the limited scale of individual studies being among them. Without the ability to pool the recordings from different non-invasive studies, data on the order of magnitude needed to leverage deep learning techniques to their full potential remains out of reach. In this work, we focus on non-invasive data collected using magnetoencephalography (MEG). We leverage two different, leading speech decoding models to investigate how an adversarial domain adaptation framework augments their ability to generalize across datasets. We successfully improve the performance of both models when training across multiple datasets. To the best of our knowledge, this study is the first ever application of feature-level, deep learning based harmonization for MEG neuroimaging data. Our analysis additionally offers further evidence of the impact of demographic features on neuroimaging data, demonstrating that participant age strongly affects how machine learning models solve speech decoding tasks using MEG data. Lastly, in the course of this study we produce a new open-source implementation of one of these models to the benefit of the broader scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19986v1</guid>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremiah Ridge, Oiwi Parker Jones</dc:creator>
    </item>
    <item>
      <title>The functional architecture of the early vision and neurogeometric models</title>
      <link>https://arxiv.org/abs/2202.10157</link>
      <description>arXiv:2202.10157v3 Announce Type: replace 
Abstract: The initial sections of the paper give a concise presentation, specially designed for a mathematically oriented audience, of some of the most basic facts on the functional architecture of early vision. Such information is usually scattered in a variety of papers and books, which are not easily accessible by non-specialists. Our goal is thus to offer a handy and short introduction to this topics, which might be helpful for researchers willing to enter the area of the applications of modern Differential Geometry in studies on the visual systems, baptized neurogeometry by J. Petitot. We then offer a survey of three of the most important neurogeometric models: Petitot's contact model of the primary visual cortex, its extension to A. Sarti, G. Citti and J. Petitot's symplectic model, and P. C. Bressloff and J. D. Cowan's spherical model of hypercolumns. We finally discuss the main points of the so-called ``conformal model'' for hypercolumns (a model that was briefly presented in [D. V. Alekseevsky, Conformal model of hypercolumns in V1 cortex and the Moebius group, in ``Geometric science of information'', pp. 65--72, Springer, 2021] and given in detail in [D. V. Alekseevsky and A. Spiro, Conformal models for hypercolumns in the primary visual cortex V1, arXiv 2024]), which can be considered as a synthesis of the symplectic and the spherical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10157v3</guid>
      <category>q-bio.NC</category>
      <category>math.DG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitri V. Alekseevsky, Andrea Spiro</dc:creator>
    </item>
    <item>
      <title>Hierarchical Event Descriptor library schema for EEG data annotation</title>
      <link>https://arxiv.org/abs/2310.15173</link>
      <description>arXiv:2310.15173v2 Announce Type: replace 
Abstract: Standardizing terminology to annotate electrophysiological events can improve both computational research and clinical care. Sharing data enriched with standard terms can facilitate data exploration, from case studies to mega-analyses. The machine readability of such electrophysiological event annotations is essential for performing analyses efficiently across software tools and packages. Hierarchical Event Descriptors (HED) provide a framework for describing events in neuroscience experiments. HED library schemas extend the standard HED schema vocabulary to include specialized vocabularies, such as standardized clinical terms for electrophysiological events. The Standardized Computer-based Organized Reporting of EEG (SCORE) defines terms for annotating EEG events, including artifacts. This study developed a HED library schema for SCORE, making the terms machine-readable. We demonstrate that the HED-SCORE library schema can be used to annotate events in EEG data stored in the Brain Imaging Data Structure (BIDS). Clinicians and researchers worldwide can now use the HED-SCORE library schema to annotate and compute on electrophysiological data obtained from the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15173v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dora Hermes, Tal Pal Attia, S\'andor Beniczky, Jorge Bosch-Bayard, Arnaud Delorme, Brian Nils Lundstrom, Christine Rogers, Stefan Rampp, Seyed Yahya Shirazi, Dung Truong, Pedro Valdes-Sosa, Greg Worrell, Scott Makeig, Kay Robbins</dc:creator>
    </item>
    <item>
      <title>Learning to combine top-down context and feed-forward representations under ambiguity with apical and basal dendrites</title>
      <link>https://arxiv.org/abs/2312.05484</link>
      <description>arXiv:2312.05484v3 Announce Type: replace 
Abstract: One of the hallmark features of neocortical anatomy is the presence of extensive top-down projections into primary sensory areas, with many impinging on the distal apical dendrites of pyramidal neurons. While it is known that they exert a modulatory effect, altering the gain of responses, their functional role remains an active area of research. It is hypothesized that these top-down projections carry contextual information that can help animals to resolve ambiguities in sensory data. One proposed mechanism of contextual integration is a non-linear integration of distinct input streams at apical and basal dendrites of pyramidal neurons. Computationally, however, it is yet to be demonstrated how such an architecture could leverage distinct compartments for flexible contextual integration and sensory processing when both sensory and context signals can be unreliable. Here, we implement an augmented deep neural network with distinct apical and basal compartments that integrates a) contextual information from top-down projections to apical compartments, and b) sensory representations driven by bottom-up projections to basal compartments, via a biophysically inspired rule. In addition, we develop a new multi-scenario contextual integration task using a generative image modeling approach. In addition to generalizing previous contextual integration tasks, it better captures the diversity of scenarios where neither contextual nor sensory information are fully reliable. To solve this task, this model successfully learns to select among integration strategies. We find that our model outperforms those without the "apical prior" when contextual information contradicts sensory input. Altogether, this suggests that the apical prior and biophysically inspired integration rule could be key components necessary for handling the ambiguities that animals encounter in the diverse contexts of the real world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05484v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nizar Islah, Guillaume Etter, Mashbayar Tugsbayar, Tugce Gurbuz, Blake Richards, Eilif Muller</dc:creator>
    </item>
    <item>
      <title>NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes</title>
      <link>https://arxiv.org/abs/2409.17510</link>
      <description>arXiv:2409.17510v3 Announce Type: replace 
Abstract: Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\'e of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including HCP and UK Biobank under supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17510v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziquan Wei, Tingting Dan, Jiaqi Ding, Guorong Wu</dc:creator>
    </item>
    <item>
      <title>Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data</title>
      <link>https://arxiv.org/abs/2311.03520</link>
      <description>arXiv:2311.03520v3 Announce Type: replace-cross 
Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the Adolescent Brain Cognitive Development Dataset, and demonstrated its effectiveness in predicting individual differences in intelligence. Our model achieved lower mean squared errors and higher correlation scores than existing relevant graph architectures and other traditional machine learning models for all of the intelligence prediction tasks. The middle frontal gyrus exhibited a significant contribution to both fluid and crystallized intelligence, suggesting their pivotal role in these cognitive processes. Total composite scores identified a diverse set of brain regions to be relevant which underscores the complex nature of total intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03520v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu</dc:creator>
    </item>
    <item>
      <title>Self-Attention-Based Contextual Modulation Improves Neural System Identification</title>
      <link>https://arxiv.org/abs/2406.07843</link>
      <description>arXiv:2406.07843v2 Announce Type: replace-cross 
Abstract: Convolutional neural networks (CNNs) have been shown to be state-of-the-art models for visual cortical neurons. Cortical neurons in the primary visual cortex are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs integrate global contextual information to model contextual modulation via two mechanisms: successive convolutions and a fully connected readout layer. In this paper, we find that self-attention (SA), an implementation of non-local network mechanisms, can improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and peak tuning. We introduce peak tuning as a metric to evaluate a model's ability to capture a neuron's feature preference. We factorize networks to assess each context mechanism, revealing that information in the local receptive field is most important for modeling overall tuning, but surround information is critically necessary for characterizing the tuning peak. We find that self-attention can replace posterior spatial-integration convolutions when learned incrementally, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary. Finally, we find that decomposing receptive field learning and contextual modulation learning in an incremental manner may be an effective and robust mechanism for learning surround-center interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07843v2</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Lin, Tianye Wang, Shang Gao, Shiming Tang, Tai Sing Lee</dc:creator>
    </item>
  </channel>
</rss>
