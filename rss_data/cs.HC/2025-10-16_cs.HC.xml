<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 01:49:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Changing Oneself by Teaching Others? Exploring the Prot\'eg\'e Effect in Digital Stress Self-Regulation</title>
      <link>https://arxiv.org/abs/2510.12944</link>
      <description>arXiv:2510.12944v1 Announce Type: new 
Abstract: The prot\'eg\'ee effect suggests that individuals learn more effectively when they teach a subject. While this has shown potential for acquiring knowledge and skills, can it also support acquiring a new behaviour? This study evaluated a prot\'eg\'e-based intervention designed to manage digital stress. Over three weeks, 137 participants with moderate to high digital stress were assigned to four groups. Two were prot\'eg\'ee-based: a passive group, given material to teach, and an active group, received headlines and had to search for and prepare teaching content. Both groups completed three sessions, each focused on one digital stress component: availability demand stress, approval anxiety, and fear of missing out. A digital literacy group received similar content and quizzes, and a control group. Outcomes measured included digital stress, problematic social media use, word-of-mouth about its management, and issue involvement. Findings highlight the challenge of translating cognitive engagement into behavioural change, especially amid persistent digital habits and socially reinforced stressors. Results offer insights into the limitations of interventions based on the prot\'eg\'ee effect when applied to behaviour change, particularly in the context of reflective digital wellbeing strategies. Future research could explore interactive formats, such as peer engagement or self-regulatory elements, to enhance motivation and impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12944v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sameha Alshakhsi, Ala Yankouskaya, Dena Al-Thani, Raian Ali</dc:creator>
    </item>
    <item>
      <title>TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution</title>
      <link>https://arxiv.org/abs/2510.12972</link>
      <description>arXiv:2510.12972v1 Announce Type: new 
Abstract: Accessibility checkers are tools in support of accessible app development and their use is encouraged by accessibility best practices. However, most current checkers evaluate static or mechanically-generated contexts, failing to capture common accessibility errors impacting mobile app functionality. We present TaskAudit, an accessibility evaluation system that focuses on detecting functiona11ity errors through simulated interactions. TaskAudit comprises three components: a Task Generator that constructs interactive tasks from app screens, a Task Executor that uses agents with a screen reader proxy to perform these tasks, and an Accessibility Analyzer that detects and reports accessibility errors by examining interaction traces. Evaluation on real-world apps shows that our strategy detects 48 functiona11ity errors from 54 app screens, compared to between 4 and 20 with existing checkers. Our analysis demonstrates common error patterns that TaskAudit can detect in addition to prior work, including label-functionality mismatch, cluttered navigation, and inappropriate feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12972v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyuan Zhong, Xia Chen, Davin Win Kyi, Chen Li, James Fogarty, Jacob O. Wobbrock</dc:creator>
    </item>
    <item>
      <title>Behavioral Biometrics for Automatic Detection of User Familiarity in VR</title>
      <link>https://arxiv.org/abs/2510.12988</link>
      <description>arXiv:2510.12988v1 Announce Type: new 
Abstract: As virtual reality (VR) devices become increasingly integrated into everyday settings, a growing number of users without prior experience will engage with VR systems. Automatically detecting a user's familiarity with VR as an interaction medium enables real-time, adaptive training and interface adjustments, minimizing user frustration and improving task performance. In this study, we explore the automatic detection of VR familiarity by analyzing hand movement patterns during a passcode-based door-opening task, which is a well-known interaction in collaborative virtual environments such as meeting rooms, offices, and healthcare spaces. While novice users may lack prior VR experience, they are likely to be familiar with analogous real-world tasks involving keypad entry. We conducted a pilot study with 26 participants, evenly split between experienced and inexperienced VR users, who performed tasks using both controller-based and hand-tracking interactions. Our approach uses state-of-the-art deep classifiers for automatic VR familiarity detection, achieving the highest accuracies of 92.05% and 83.42% for hand-tracking and controller-based interactions, respectively. In the cross-device evaluation, where classifiers trained on controller data were tested using hand-tracking data, the model achieved an accuracy of 78.89%. The integration of both modalities in the mixed-device evaluation obtained an accuracy of 94.19%. Our results underline the promise of using hand movement biometrics for the real-time detection of user familiarity in critical VR applications, paving the way for personalized and adaptive VR experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12988v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Numan Zafar, Priyo Ranjan Kundu Prosun, Shafique Ahmad Chaudhry</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Visual Fatigue Detection Using Eye Gaze Patterns in VR</title>
      <link>https://arxiv.org/abs/2510.12994</link>
      <description>arXiv:2510.12994v1 Announce Type: new 
Abstract: Prolonged exposure to virtual reality (VR) systems leads to visual fatigue, impairs user comfort, performance, and safety, particularly in high-stakes or long-duration applications. Existing fatigue detection approaches rely on subjective questionnaires or intrusive physiological signals, such as EEG, heart rate, or eye-blink count, which limit their scalability and real-time applicability. This paper introduces a deep learning-based study for detecting visual fatigue using continuous eye-gaze trajectories recorded in VR. We use the GazeBaseVR dataset comprising binocular eye-tracking data from 407 participants across five immersive tasks, extract cyclopean eye-gaze angles, and evaluate six deep classifiers. Our results demonstrate that EKYT achieves up to 94% accuracy, particularly in tasks demanding high visual attention, such as video viewing and text reading. We further analyze gaze variance and subjective fatigue measures, indicating significant behavioral differences between fatigued and non-fatigued conditions. These findings establish eye-gaze dynamics as a reliable and nonintrusive modality for continuous fatigue detection in immersive VR, offering practical implications for adaptive human-computer interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12994v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Numan Zafar, Johnathan Locke, Shafique Ahmad Chaudhry</dc:creator>
    </item>
    <item>
      <title>Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale</title>
      <link>https://arxiv.org/abs/2510.13009</link>
      <description>arXiv:2510.13009v1 Announce Type: new 
Abstract: As the use of large language models (LLMs) becomes increasingly global, understanding public attitudes toward these systems requires tools that are adapted to local contexts and languages. In the Arab world, LLM adoption has grown rapidly with both globally dominant platforms and regional ones like Fanar and Jais offering Arabic-specific solutions. This highlights the need for culturally and linguistically relevant scales to accurately measure attitudes toward LLMs in the region. Tools assessing attitudes toward artificial intelligence (AI) can provide a base for measuring attitudes specific to LLMs. The 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which measures two dimensions, the AI Fear and the AI Acceptance, has been recently adopted and adapted to develop new instruments in English using a sample from the UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward Primary LLM (AT-PLLM) scales. In this paper, we translate the two scales, AT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking adults. The results show that the scale, translated into Arabic, is a reliable and valid tool that can be used for the Arab population and language. Psychometric analyses confirmed a two-factor structure, strong measurement invariance across genders, and good internal reliability. The scales also demonstrated strong convergent and discriminant validity. Our scales will support research in a non-Western context, a much-needed effort to help draw a global picture of LLM perceptions, and will also facilitate localized research and policy-making in the Arab region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13009v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Basad Barajeeh, Ala Yankouskaya, Sameha AlShakhsi, Chun Sing Maxwell Ho, Guandong Xu, Raian Ali</dc:creator>
    </item>
    <item>
      <title>Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments</title>
      <link>https://arxiv.org/abs/2510.13011</link>
      <description>arXiv:2510.13011v1 Announce Type: new 
Abstract: Social and behavioral scientists increasingly aim to study how humans interact, collaborate, and make decisions alongside artificial intelligence. However, the experimental infrastructure for such work remains underdeveloped: (1) few platforms support real-time, multi-party studies at scale; (2) most deployments require bespoke engineering, limiting replicability and accessibility, and (3) existing tools do not treat AI agents as first-class participants. We present Deliberate Lab, an open-source platform for large-scale, real-time behavioral experiments that supports both human participants and large language model (LLM)-based agents. We report on a 12-month public deployment of the platform (N=88 experimenters, N=9195 experiment participants), analyzing usage patterns and workflows. Case studies and usage scenarios are aggregated from platform users, complemented by in-depth interviews with select experimenters. By lowering technical barriers and standardizing support for hybrid human-AI experimentation, Deliberate Lab expands the methodological repertoire for studying collective decision-making and human-centered AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13011v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Crystal Qian, Vivian Tsai, Michael Behr, Nada Hussein, L\'eo Laugier, Nithum Thain, Lucas Dixon</dc:creator>
    </item>
    <item>
      <title>Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents</title>
      <link>https://arxiv.org/abs/2510.13091</link>
      <description>arXiv:2510.13091v2 Announce Type: new 
Abstract: Online freelance marketplaces, a rapidly growing part of the global labor market, are creating a fair environment where professional skills are the main factor for hiring. While these platforms can reduce bias from traditional hiring, the personal information in user profiles raises concerns about ongoing discrimination. Past studies on this topic have mostly used existing data, which makes it hard to control for other factors and clearly see the effect of things like gender or race. To solve these problems, this paper presents a new method that uses Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) to create realistic, artificial freelancer profiles for controlled experiments. This approach effectively separates individual factors, enabling a clearer statistical analysis of how different variables influence the freelancer project process. In addition to analyzing extracted data with traditional statistical methods for post-project stage analysis, our research utilizes a dataset with highly controlled variables, generated by an RAG-LLM, to conduct a simulated hiring experiment for pre-project stage analysis. The results of our experiments show that, regarding gender, while no significant preference emerged in initial hiring decisions, female freelancers are substantially more likely to receive imperfect ratings post-project stage. Regarding regional bias, a strong and consistent preference favoring US-based freelancers shows that people are more likely to be selected in the simulated experiments, perceived as more leader-like, and receive higher ratings on the live platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13091v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wugeng Zheng, Guohou Shan</dc:creator>
    </item>
    <item>
      <title>Adapting to the User: A Systematic Review of Personalized Interaction in VR</title>
      <link>https://arxiv.org/abs/2510.13123</link>
      <description>arXiv:2510.13123v1 Announce Type: new 
Abstract: As virtual reality (VR) systems become increasingly more advanced, they are likewise expected to respond intelligently and adapt to individual user states, abilities, and preferences. Recent work has explored how VR can be adapted and tailored to individual users. However, existing reviews tend to address either user-state sensing or adaptive interaction design in isolation, limiting our understanding of their combined implementation in VR. Therefore, in this paper, we examine the growing research on personalized interaction in VR, with a particular focus on utilizing participants' immersion information and adaptation mechanisms to modify virtual environments and enhance engagement, performance, or a specific goal. We synthesize findings from studies that employ adaptive techniques across diverse application domains and summarize a five-stage conceptual framework that unifies adaptive mechanisms across domains. Our analysis reveals emerging trends, including the integration of multimodal sensors, an increasing reliance on user state inference, and the challenge of balancing responsiveness with transparency. We conclude by proposing future directions for developing more user-centered VR systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13123v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tangyao Li, Yitong Zhu, Hai-Ning Liang, Yuyang Wang</dc:creator>
    </item>
    <item>
      <title>Smart UX-design for Rescue Operations Wearable - A Knowledge Graph Informed Visualization Approach for Information Retrieval in Emergency Situations</title>
      <link>https://arxiv.org/abs/2510.13539</link>
      <description>arXiv:2510.13539v1 Announce Type: new 
Abstract: This paper presents a knowledge graph-informed smart UX-design approach for supporting information retrieval for a wearable, providing treatment recommendations during emergency situations to health professionals. This paper describes requirements that are unique to knowledge graph-based solutions, as well as the direct requirements of health professionals. The resulting implementation is provided for the project, which main goal is to improve first-aid rescue operations by supporting artificial intelligence in situation detection and knowledge graph representation via a contextual-based recommendation for treatment assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13539v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/eIT57321.2023.10187320</arxiv:DOI>
      <dc:creator>Mubaris Nadeem, Johannes Zenkert, Christian Weber, Madjid Fathi, Muhammad Hamza</dc:creator>
    </item>
    <item>
      <title>Speculating a Tactile Grammar: Toward Task-Aligned Chart Design for Non-Visual Perception</title>
      <link>https://arxiv.org/abs/2510.13731</link>
      <description>arXiv:2510.13731v1 Announce Type: new 
Abstract: Tactile graphics are often adapted from visual chart designs, yet many of these encodings do not translate effectively to non-visual exploration. Blind and low-vision (BLV) people employ a variety of physical strategies such as measuring lengths with fingers or scanning for texture differences to interpret tactile charts. These observations suggest an opportunity to move beyond direct visual translation and toward a tactile-first design approach. We outline a speculative tactile design framework that explores how data analysis tasks may align with tactile strategies and encoding choices. While this framework is not yet validated, it offers a lens for generating tactile-first chart designs and sets the stage for future empirical exploration. We present speculative mockups to illustrate how the Tactile Perceptual Grammar might guide the design of an accessible COVID-19 dashboard. This scenario illustrates how the grammar can guide encoding choices that better support comparison, trend detection, and proportion estimation in tactile formats. We conclude with design implications and a discussion of future validation through co-design and task-based evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13731v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Areen Khalaila, Dylan Cashman</dc:creator>
    </item>
    <item>
      <title>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</title>
      <link>https://arxiv.org/abs/2510.13068</link>
      <description>arXiv:2510.13068v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) captures neural activity across multiple temporal and spectral scales, yielding signals that are rich but complex for representation learning. Recently, EEG foundation models trained to predict masked signal-tokens have shown promise for learning generalizable representations. However, their performance is hindered by their signal tokenization modules. Existing neural tokenizers fail to preserve high-frequency dynamics, limiting their ability to reconstruct EEG signals with high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM) centered on a codebook-based tokenizer. Our tokenizer integrates: (i) multi-scale feature extraction modules that capture the full frequency neural spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware loss function for efficient training. This design enables efficient EEG compression while supporting accurate reconstruction across all frequency bands, leading to robust generative masked modeling. Our empirical results demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ tokenizer establishes a strong prior for codebook-based general-purpose brainwave models, enabling advances in neural decoding, generative modeling and multimodal biosignal integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13068v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou</dc:creator>
    </item>
    <item>
      <title>DIGITWISE: Digital Twin-based Modeling of Adaptive Video Streaming Engagement</title>
      <link>https://arxiv.org/abs/2510.13267</link>
      <description>arXiv:2510.13267v1 Announce Type: cross 
Abstract: As the popularity of video streaming entertainment continues to grow, understanding how users engage with the content and react to its changes becomes a critical success factor for every stakeholder. User engagement, i.e., the percentage of video the user watches before quitting, is central to customer loyalty, content personalization, ad relevance, and A/B testing. This paper presents DIGITWISE, a digital twin-based approach for modeling adaptive video streaming engagement. Traditional adaptive bitrate (ABR) algorithms assume that all users react similarly to video streaming artifacts and network issues, neglecting individual user sensitivities. DIGITWISE leverages the concept of a digital twin, a digital replica of a physical entity, to model user engagement based on past viewing sessions. The digital twin receives input about streaming events and utilizes supervised machine learning to predict user engagement for a given session. The system model consists of a data processing pipeline, machine learning models acting as digital twins, and a unified model to predict engagement. DIGITWISE employs the XGBoost model in both digital twins and unified models. The proposed architecture demonstrates the importance of personal user sensitivities, reducing user engagement prediction error by up to 5.8% compared to non-user-aware models. Furthermore, DIGITWISE can optimize content provisioning and delivery by identifying the features that maximize engagement, providing an average engagement increase of up to 8.6%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13267v1</guid>
      <category>eess.IV</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3625468.3647613</arxiv:DOI>
      <dc:creator>Emanuele Artioli, Farzad Tashtarian, Christian Timmerer</dc:creator>
    </item>
    <item>
      <title>A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health</title>
      <link>https://arxiv.org/abs/2411.02594</link>
      <description>arXiv:2411.02594v3 Announce Type: replace 
Abstract: Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02594v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757486</arxiv:DOI>
      <dc:creator>Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura M. Schwab Reese, Munmun De Choudhury</dc:creator>
    </item>
    <item>
      <title>The Value of AI Advice: Personalized and Value-Maximizing AI Advisors Are Necessary to Reliably Benefit Experts and Organizations</title>
      <link>https://arxiv.org/abs/2412.19530</link>
      <description>arXiv:2412.19530v2 Announce Type: replace 
Abstract: Despite advances in AI's performance and interpretability, AI advisors can undermine experts' decisions and increase the time and effort experts must invest to make decisions. Consequently, AI systems deployed in high-stakes settings often fail to consistently add value across experts and organizations and can even diminish the value that experts alone provide. Beyond harm in specific domains, such outcomes impede progress in research and practice, underscoring the need to understand when and why different AI advisors add or diminish value. To bridge this gap, we stress the importance of assessing the value AI advice brings to real-world contexts when designing and evaluating AI advisors. Building on this perspective, we characterize key pillars -- pathways through which AI advice impacts value -- and develop a framework that incorporates these pillars to create reliable, personalized, and value-adding advisors. Our results highlight the need for value-driven development of AI advisors that advise selectively, are tailored to experts' unique behaviors, and are optimized for context-specific trade-offs between decision improvements and advising costs. They also reveal how the lack of inclusion of these pillars in the design of AI advising systems may be contributing to the failures observed in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19530v2</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Wolczynski, Maytal Saar-Tsechansky, Tong Wang</dc:creator>
    </item>
    <item>
      <title>ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound Diffusion Field</title>
      <link>https://arxiv.org/abs/2412.20142</link>
      <description>arXiv:2412.20142v2 Announce Type: replace 
Abstract: Passive human speed estimation plays a critical role in acoustic sensing. Despite extensive study, existing systems, however, suffer from various limitations: First, the channel measurement rate proves inadequate to estimate high moving speeds. Second, previous acoustic speed estimation exploits Doppler Frequency Shifts (DFS) created by moving targets and relies on microphone arrays, making them only capable of sensing the radial speed within a constrained distance. To overcome these issues, we present ASE, an accurate and robust Acoustic Speed Estimation system on a single commodity microphone. We propose a novel Orthogonal Time-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a high rate that was previously infeasible, making it possible to estimate high speeds. We then model the sound propagation from a unique perspective of the acoustic diffusion field, and infer the speed from the acoustic spatial distribution, a completely different way of thinking about speed estimation beyond prior DFS-based approaches. We further develop novel techniques for motion detection and signal enhancement to deliver a robust and practical system. We implement and evaluate ASE through extensive real-world experiments. Our results show that ASE reliably tracks walking speed, independently of target location and direction, with a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection rate of 97.4% for large coverage, e.g., free walking in a 4m x 4m room. We believe ASE pushes acoustic speed estimation beyond the conventional DFS-based paradigm and inspires exciting research in acoustic sensing. Code is available at https://github.com/aiot-lab/ASE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20142v2</guid>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3749475</arxiv:DOI>
      <arxiv:journal_reference>Sheng Lyu and Chenshu Wu. 2025. ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound Diffusion Field. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 9, 3, Article 115 (September 2025), 26 pages</arxiv:journal_reference>
      <dc:creator>Sheng Lyu, Chenshu Wu</dc:creator>
    </item>
    <item>
      <title>Group Decision-Making System with Sentiment Analysis of Discussion Chat and Fuzzy Consensus Modeling</title>
      <link>https://arxiv.org/abs/2503.18765</link>
      <description>arXiv:2503.18765v2 Announce Type: replace 
Abstract: Group Decision-Making (GDM) plays a crucial role in various real-life scenarios where individuals express their opinions in natural language rather than structured numerical values. Traditional GDM approaches often overlook the subjectivity and ambiguity present in human discussions, making it challenging to achieve a fair and consensus-driven decision. This paper proposes a fuzzy consensus-based group decision-making system that integrates sentiment and emotion analysis to extract preference values from textual inputs. The proposed framework combines explicit voting preferences with sentiment scores derived from chat discussions, which are then processed using a Fuzzy Inference System (FIS) to compute a total preference score for each alternative and determine the top-ranked option. To ensure fairness in group decision-making, we introduce a fuzzy logic-based consensus measurement model that evaluates participants' agreement and confidence levels to assess overall feedback. To illustrate the effectiveness of our approach, we apply the methodology to a restaurant selection scenario, where a group of individuals must decide on a dining option based on brief chat discussions. The results demonstrate that the fuzzy consensus mechanism successfully aggregates individual preferences and ensures a balanced outcome that accurately reflects group sentiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18765v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FUZZ62266.2025.11197676</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Conference on Fuzzy Systems (FUZZ)</arxiv:journal_reference>
      <dc:creator>Adilet Yerkin, Pakizar Shamoi</dc:creator>
    </item>
    <item>
      <title>AI Realtor: Towards Grounded Persuasive Language Generation for Automated Copywriting</title>
      <link>https://arxiv.org/abs/2502.16810</link>
      <description>arXiv:2502.16810v4 Announce Type: replace-cross 
Abstract: This paper develops an agentic framework that employs large language models (LLMs) for grounded persuasive language generation in automated copywriting, with real estate marketing as a focal application. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy. Our findings suggest a promising agentic approach to automate large-scale targeted copywriting while ensuring factuality of content generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16810v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jibang Wu, Chenghao Yang, Yi Wu, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition</title>
      <link>https://arxiv.org/abs/2509.19330</link>
      <description>arXiv:2509.19330v2 Announce Type: replace-cross 
Abstract: EEG-based multimodal emotion recognition(EMER) has gained significant attention and witnessed notable advancements, the inherent complexity of human neural systems has motivated substantial efforts toward multimodal approaches. However, this field currently suffers from three critical limitations: (i) the absence of open-source implementations. (ii) the lack of standardized and transparent benchmarks for fair performance analysis. (iii) in-depth discussion regarding main challenges and promising research directions is a notable scarcity. To address these challenges, we introduce LibEMER, a unified evaluation framework that provides fully reproducible PyTorch implementations of curated deep learning methods alongside standardized protocols for data preprocessing, model realization, and experimental setups. This framework enables unbiased performance assessment on three widely-used public datasets across two learning tasks. The open-source library is publicly accessible at: https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19330v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zejun Liu, Yunshan Chen, Chengxi Xie, Yugui Xie, Huan Liu</dc:creator>
    </item>
    <item>
      <title>(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm</title>
      <link>https://arxiv.org/abs/2510.12364</link>
      <description>arXiv:2510.12364v2 Announce Type: replace-cross 
Abstract: Recent advancements in generative artificial intelligence (GenAI), particularly large language models, have introduced new possibilities for software development practices. In our paper we investigate the emerging Vibe Coding (VC) paradigm that emphasizes intuitive, affect-driven, and improvisational interactions between developers and AI systems. Building upon the discourse of End-User Development (EUD), we explore how VC diverges from conventional programming approaches such as those supported by tools like GitHub Copilot. Through five semi-structured interview sessions with ten experienced software practitioners, we identify five thematic dimensions: creativity, sustainability, the future of programming, collaboration, and criticism. Our analysis conceptualizes VC within the metaphor of co-drifting, contrasting it with the prevalent co-piloting perspective of AI-assisted development. We argue that VC reconfigures the developers role, blurring boundaries between professional and non-developers. While VC enables novel forms of expression and rapid prototyping, it also introduces challenges regarding reproducibility, scalability, and inclusivity. We propose that VC represents a meaningful shift in programming culture, warranting further investigation within human-computer interaction (HCI) and software engineering research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12364v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Krings, Nino S. Bohn, Thomas Ludwig</dc:creator>
    </item>
  </channel>
</rss>
