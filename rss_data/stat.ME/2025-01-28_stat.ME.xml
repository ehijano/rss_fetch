<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sequential Methods for Error Correction of Probabilistic Wind Power Forecasts</title>
      <link>https://arxiv.org/abs/2501.14805</link>
      <description>arXiv:2501.14805v1 Announce Type: new 
Abstract: Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method we achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14805v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensen, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Bayesian Method for Instrumental Variable Analysis with Partly Interval-Censored Time-to-Event Outcome</title>
      <link>https://arxiv.org/abs/2501.14837</link>
      <description>arXiv:2501.14837v1 Announce Type: new 
Abstract: This paper develops a semiparametric Bayesian instrumental variable analysis method for estimating the causal effect of an endogenous variable when dealing with unobserved confounders and measurement errors with partly interval-censored time-to-event data, where event times are observed exactly for some subjects but left-censored, right-censored, or interval-censored for others. Our method is based on a two-stage Dirichlet process mixture instrumental variable (DPMIV) model which simultaneously models the first-stage random error term for the exposure variable and the second-stage random error term for the time-to-event outcome using a bivariate Gaussian mixture of the Dirichlet process (DPM) model. The DPM model can be broadly understood as a mixture model with an unspecified number of Gaussian components, which relaxes the normal error assumptions and allows the number of mixture components to be determined by the data. We develop an MCMC algorithm for the DPMIV model tailored for partly interval-censored data and conduct extensive simulations to assess the performance of our DPMIV method in comparison with some competing methods. Our simulations revealed that our proposed method is robust under different error distributions and can have superior performance over its parametric counterpart under various scenarios. We further demonstrate the effectiveness of our approach on an UK Biobank data to investigate the causal effect of systolic blood pressure on time-to-development of cardiovascular disease from the onset of diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14837v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Xuyang Lu, Jin Zhou, Hua Zhou, Gang Li</dc:creator>
    </item>
    <item>
      <title>Assessing Skew Normality in Marks Distribution, a Comparative Analysis of Shapiro Wilk Tests</title>
      <link>https://arxiv.org/abs/2501.14845</link>
      <description>arXiv:2501.14845v1 Announce Type: new 
Abstract: This paper investigates the distribution of marks obtained by students across multiple courses to explore whether the data conforms to a skew-normal distribution. Traditional methods for assessing normality, such as the Shapiro Wilk test, often reject normality in datasets with evident skewness. To address this, we apply a modified Shapiro Wilk test tailored for skew-normal distributions, as described in the literature, to evaluate the suitability of skew-normal models for these datasets. The analysis includes both classical and modified tests, complemented by visualizations such as histograms and Q-Q plots of transformed data. Our findings highlight the relevance of using specialized statistical methods for skew normality, offering valuable insights into the characteristics of academic performance data. This study provides a framework for robust statistical analysis in educational research, emphasizing the need to account for distributional properties when analyzing student performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14845v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Himadri Mukherjee, Pratham Bhonge</dc:creator>
    </item>
    <item>
      <title>Clustering of functional data prone to complex heteroscedastic measurement error</title>
      <link>https://arxiv.org/abs/2501.14919</link>
      <description>arXiv:2501.14919v1 Announce Type: new 
Abstract: Several factors make clustering of functional data challenging, including the infinite-dimensional space to which observations belong and the lack of a defined probability density function for the functional random variable. To overcome these barriers, researchers either assume that observations belong to a finite-dimensional space spanned by basis functions or apply nonparametric smoothing methods to the functions prior to clustering. Although extensive literature describes clustering methods for functional data, few studies have explored the clustering of measurement error--prone function-valued data. In this work, we consider clustering methods for functional data prone to complex, heteroscedastic measurement errors. Two stage-based methods using mixed-effects models are first applied to adjust for measurement error bias, followed by cluster analysis of the measurement error--adjusted curves. Through simulations, we investigate how varying sample size, the magnitude of measurement error, and the presence of complex heteroscedastic measurement errors influence the cluster analysis of functional data. Our results indicate that failing to account for measurement errors and the correlation structures associated with frequently collected functional data reduces the accuracy of identifying the true latent groups or clusters. The method consistently produces better results regardless of the initial clustering values used. Moreover, it is flexible and can be applied to various clustering approaches, based on the specific distribution of the data. The developed methods are applied to two data sets: a school-based study of energy expenditure among elementary school-aged children in Texas and data from the National Health and Nutrition Examination Survey on participants' physical activity monitored by wearable devices at frequent intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14919v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andi Mai, Carmen Tekwe, Roger Zoh, Lan Xue</dc:creator>
    </item>
    <item>
      <title>Generalized Reduced-Rank Regression with Homogeneity Pursuit</title>
      <link>https://arxiv.org/abs/2501.15016</link>
      <description>arXiv:2501.15016v1 Announce Type: new 
Abstract: Homogeneity, low rank, and sparsity are three widely adopted assumptions in multi-response regression models to address the curse of dimensionality and improve estimation accuracy. However, there is limited literature that examines these assumptions within a unified framework. In this paper, we investigate the homogeneity, low rank, and sparsity assumptions under the generalized linear model with high-dimensional responses and covariates, encompassing a wide range of practical applications. Our work establishes a comprehensive benchmark for comparing the effects of these three assumptions and introduces a regularized maximum likelihood estimation method to fit the corresponding models. Under mild conditions,we prove the statistical consistency of our estimator. Theoretical results provide insights into the role of homogeneity and offer a quantitative analysis of scenarios where homogeneity improves estimation accuracy. The proposed method's effectiveness is demonstrated through numerical simulations and an empirical analysis of tree species data from Barro Colorado Island.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15016v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruipeng Dong, Ganggang Xu, Yongtao Guan</dc:creator>
    </item>
    <item>
      <title>Salvaging Forbidden Treasure in Medical Data: Utilizing Surrogate Outcomes and Single Records for Rare Event Modeling</title>
      <link>https://arxiv.org/abs/2501.15079</link>
      <description>arXiv:2501.15079v1 Announce Type: new 
Abstract: The vast repositories of Electronic Health Records (EHR) and medical claims hold untapped potential for studying rare but critical events, such as suicide attempt. Conventional setups often model suicide attempt as a univariate outcome and also exclude any ``single-record'' patients with a single documented encounter due to a lack of historical information. However, patients who were diagnosed with suicide attempts at the only encounter could, to some surprise, represent a substantial proportion of all attempt cases in the data, as high as 70--80%. We innovate a hybrid and integrative learning framework to leverage concurrent outcomes as surrogates and harness the forbidden yet precious information from single-record data. Our approach employs a supervised learning component to learn the latent variables that connect primary (e.g., suicide) and surrogate outcomes (e.g., mental disorders) to historical information. It simultaneously employs an unsupervised learning component to utilize the single-record data, through the shared latent variables. As such, our approach offers a general strategy for information integration that is crucial to modeling rare conditions and events. With hospital inpatient data from Connecticut, we demonstrate that single-record data and concurrent diagnoses indeed carry valuable information, and utilizing them can substantially improve suicide risk modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15079v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohui Yin, Shane Sacco, Robert H. Aseltine, Fei Wang, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Versatile Differentially Private Learning for General Loss Functions</title>
      <link>https://arxiv.org/abs/2501.15127</link>
      <description>arXiv:2501.15127v1 Announce Type: new 
Abstract: This paper aims to provide a versatile privacy-preserving release mechanism along with a unified approach for subsequent parameter estimation and statistical inference. We propose the ZIL privacy mechanism based on zero-inflated symmetric multivariate Laplace noise, which requires no prior specification of subsequent analysis tasks, allows for general loss functions under minimal conditions, imposes no limit on the number of analyses, and is adaptable to the increasing data volume in online scenarios. We derive the trade-off function for the proposed ZIL mechanism that characterizes its privacy protection level. Within the M-estimation framework, we propose a novel doubly random corrected loss (DRCL) for the ZIL mechanism, which provides consistent and asymptotic normal M-estimates for the parameters of the target population under differential privacy constraints. The proposed approach is easy to compute without numerical integration and differentiation for noisy data. It is applicable for a general class of loss functions, including non-smooth loss functions like check loss and hinge loss. Simulation studies, including logistic regression and quantile regression, are conducted to evaluate the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15127v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qilong Lu, Songxi Chen, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Beyond Fixed Restriction Time: Adaptive Restricted Mean Survival Time Methods in Clinical Trials</title>
      <link>https://arxiv.org/abs/2501.15284</link>
      <description>arXiv:2501.15284v1 Announce Type: new 
Abstract: Restricted mean survival time (RMST) offers a compelling nonparametric alternative to hazard ratios for right-censored time-to-event data, particularly when the proportional hazards assumption is violated. By capturing the total event-free time over a specified horizon, RMST provides an intuitive and clinically meaningful measure of absolute treatment benefit. Nonetheless, selecting the restriction time $L$ poses challenges: choosing a small $L$ can overlook late-emerging benefits, whereas a large $L$, often underestimated in its impact, may inflate variance and undermine power. We propose a novel data-driven, adaptive procedure that identifies the optimal restriction time $L^*$ from a continuous range by maximizing a criterion balancing effect size and estimation precision. Consequently, our procedure is particularly useful when the pattern of the treatment effect is unknown at the design stage. We provide a rigorous theoretical foundation that accounts for variability introduced by adaptively choosing $L^*$. To address nonregular estimation under the null, we develop two complementary strategies: a convex-hull-based estimator, and a penalized approach that further enhances power. Additionally, when restriction time candidates are defined on a discrete grid, we propose a procedure that surprisingly incurs no asymptotic penalty for selection, thus achieving oracle performance. Extensive simulations across realistic survival scenarios demonstrate that our method outperforms traditional RMST analyses and the log-rank test, achieving superior power while maintaining nominal Type I error rates. In a phase III pancreatic cancer trial with transient treatment effects, our procedure uncovers clinically meaningful benefits that standard methods overlook. Our methods are implemented in the R package AdaRMST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15284v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghao Sun, Douglas E. Schaubel, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Stochastic Volatility under Informative Missingness</title>
      <link>https://arxiv.org/abs/2501.15297</link>
      <description>arXiv:2501.15297v1 Announce Type: new 
Abstract: Stochastic volatility models that treat the variance of a time series as a stochastic process have proven to be important tools for analyzing dynamic variability. Current methods for fitting and conducting inference on stochastic volatility models are limited by the assumption that any missing data are missing at random. With a recent explosion in technology to facilitate the collection of dynamic self-response data for which mechanisms underlying missing data are inherently scientifically informative, this limitation in statistical methodology also limits scientific advancement. The goal of this article is to develop the first statistical methodology for modeling, fitting, and conducting inference on stochastic volatility with data that are missing not at random. The approach is based upon a novel imputation method derived using Tukey's representation, which utilizes the Markovian nature of stochastic volatility models to overcome unidentifiable components often faced when modeling informative missingness in other settings. This imputation method is combined with a new conditional particle filtering with ancestor sampling procedure that accounts for variability in imputation to formulate a complete particle Gibbs sampling scheme. The use of the method is illustrated through the analysis of mobile phone self-reported mood from individuals being monitored after unsuccessful suicide attempts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15297v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gehui Zhang, Gong Tang, Lori Scott, Robert T Krafty</dc:creator>
    </item>
    <item>
      <title>A general, flexible and harmonious framework to construct interpretable functions in regression analysis</title>
      <link>https://arxiv.org/abs/2501.15526</link>
      <description>arXiv:2501.15526v1 Announce Type: new 
Abstract: An interpretable model or method has several appealing features, such as reliability to adversarial examples, transparency of decision-making, and communication facilitator. However, interpretability is a subjective concept, and even its definition can be diverse. The same model may be deemed as interpretable by a study team, but regarded as a black-box algorithm by another squad. Simplicity, accuracy and generalizability are some additional important aspects of evaluating interpretability. In this work, we present a general, flexible and harmonious framework to construct interpretable functions in regression analysis with a focus on continuous outcomes. We formulate a functional skeleton in light of users' expectations of interpretability. A new measure based on Mallows's $C_p$-statistic is proposed for model selection to balance approximation, generalizability, and interpretability. We apply this approach to derive a sample size formula in adaptive clinical trial designs to demonstrate the general workflow, and to explain operating characteristics in a Bayesian Go/No-Go paradigm to show the potential advantages of using meaningful intermediate variables. Generalization to categorical outcomes is illustrated in an example of hypothesis testing based on Fisher's exact test. A real data analysis of NHANES (National Health and Nutrition Examination Survey) is conducted to investigate relationships between some important laboratory measurements. We also discuss some extensions of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15526v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhan, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Finding network effect of randomized treatment under weak assumptions for any outcome and any effect heterogeneity</title>
      <link>https://arxiv.org/abs/2501.15814</link>
      <description>arXiv:2501.15814v1 Announce Type: new 
Abstract: In estimating the effects of a treatment/policy with a network, an unit is subject to two types of treatment: one is the direct treatment on the unit itself, and the other is the indirect treatment (i.e., network/spillover influence) through the treated units among the friends/neighbors of the unit. In the literature, linear models are widely used where either the number of the treated neighbors or the proportion of them among the neighbors represents the intensity of the indirect treatment. In this paper, we obtain a nonparametric network-based "causal reduced form (CRF)" that allows any outcome variable (binary, count, continuous, ...) and any effect heterogeneity. Then we assess those popular linear models through the lens of the CRF. This reveals what kind of restrictive assumptions are embedded in those models, and how the restrictions can result in biases. With the CRF, we conduct almost model-free estimation and inference for network effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15814v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Myoung-jae Lee</dc:creator>
    </item>
    <item>
      <title>In the Shadow of Silence: Modelling Missing Data in the Dark Networks of Crime and Terrorists</title>
      <link>https://arxiv.org/abs/2501.15825</link>
      <description>arXiv:2501.15825v1 Announce Type: new 
Abstract: The clandestine nature of covert networks makes reliable data difficult to obtain and leads to concerns with missing data. We explore the use of network models to represent missingness mechanisms. Exponential random graph models provide a flexible way of parameterising departures from conventional missingness assumptions and data management practices. We demonstrate the effects of model specification, true network structure, and different not-at-random missingness mechanisms across six empirical covert networks. Our framework for modelling realistic missingness mechanisms investigates potential inferential pitfalls, evaluates decisions in collecting data, and offers the opportunity to incorporate non-random missingness into the estimation of network generating mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15825v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Januar, H Colin Gallagher, Johan Koskinen</dc:creator>
    </item>
    <item>
      <title>Isotropic randomization for one-sample testing in metric spaces</title>
      <link>https://arxiv.org/abs/2501.15945</link>
      <description>arXiv:2501.15945v1 Announce Type: new 
Abstract: We address the problem of testing hypotheses about a specific value of the Fr\'echet mean in metric spaces, extending classical mean testing from Euclidean spaces to more general settings. We extend an Euclidean testing procedure progresively, starting with test construction in Riemannian manifolds, leveraging their natural geometric structure through exponential and logarithm maps, and then extend to general metric spaces through the introduction of admissible randomization techniques. This approach preserves essential geometric properties required for valid statistical inference while maintaining broad applicability. We establish theoretical guarantees for our testing procedure and demonstrate its effectiveness through numerical experiments across different metric spaces and distributional settings. The practical utility of our method is further illustrated through an application to wind data in western Denmark, showcasing its relevance for real-world statistical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15945v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthieu Bult\'e, Helle S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v1 Announce Type: new 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently-used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid or coherent and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Our approach ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>Gaussian credible intervals in Bayesian nonparametric estimation of the unseen</title>
      <link>https://arxiv.org/abs/2501.16008</link>
      <description>arXiv:2501.16008v1 Announce Type: new 
Abstract: The unseen-species problem assumes $n\geq1$ samples from a population of individuals belonging to different species, possibly infinite, and calls for estimating the number $K_{n,m}$ of hitherto unseen species that would be observed if $m\geq1$ new samples were collected from the same population. This is a long-standing problem in statistics, which has gained renewed relevance in biological and physical sciences, particularly in settings with large values of $n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the unseen-species problem under the Pitman-Yor prior, and propose a novel methodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$, for any $n\geq1$. By leveraging a Gaussian central limit theorem for the posterior distribution of $K_{n,m}$, our method improves upon competitors in two key aspects: firstly, it enables the full parameterization of the Pitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need of Monte Carlo sampling, enhancing computational efficiency. We validate the proposed method on synthetic and real data, demonstrating that it improves the empirical performance of competitors by significantly narrowing the gap between asymptotic and exact credible intervals for any $m\geq1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16008v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Contardi, Emanuele Dolera, Stefano Favaro</dc:creator>
    </item>
    <item>
      <title>On spatial point processes with composition-valued marks</title>
      <link>https://arxiv.org/abs/2501.16049</link>
      <description>arXiv:2501.16049v1 Announce Type: new 
Abstract: Methods for marked spatial point processes with scalar marks have seen extensive development in recent years. While the impressive progress in data collection and storage capacities has yielded an immense increase in spatial point process data with highly challenging non-scalar marks, methods for their analysis are not equally well developed. In particular, there are no methods for composition-valued marks, i.e. vector-valued marks with a sum-to-constant constrain (typically 1 or 100). Prompted by the need for a suitable methodological framework, we extend existing methods to spatial point processes with composition-valued marks and adapt common mark characteristics to this context. The proposed methods are applied to analyse spatial correlations in data on tree crown-to-base and business sector compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Mari Myllym\"aki, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>A Standardization Procedure to Incorporate Variance Partitioning Based Priors in Latent Gaussian Models</title>
      <link>https://arxiv.org/abs/2501.16057</link>
      <description>arXiv:2501.16057v1 Announce Type: new 
Abstract: Latent Gaussian Models (LGMs) are a subset of Bayesian Hierarchical models where Gaussian priors, conditional on variance parameters, are assigned to all effects in the model. LGMs are employed in many fields for their flexibility and computational efficiency. However, practitioners find prior elicitation on the variance parameters challenging because of a lack of intuitive interpretation for them. Recently, several papers have tackled this issue by rethinking the model in terms of variance partitioning (VP) and assigning priors to parameters reflecting the relative contribution of each effect to the total variance. So far, the class of priors based on VP has been mainly deployed for random effects and fixed effects separately. This work presents a novel standardization procedure that expands the applicability of VP priors to a broader class of LGMs, including both fixed and random effects. We describe the steps required for standardization through various examples, with a particular focus on the popular class of intrinsic Gaussian Markov random fields (IGMRFs). The practical advantages of standardization are demonstrated with simulated data and a real dataset on survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16057v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luisa Ferrari, Massimo Ventrucci</dc:creator>
    </item>
    <item>
      <title>Moving toward best practice when using propensity score weighting in survey observational studies</title>
      <link>https://arxiv.org/abs/2501.16156</link>
      <description>arXiv:2501.16156v1 Announce Type: new 
Abstract: Propensity score weighting is a common method for estimating treatment effects with survey data. The method is applied to minimize confounding using measured covariates that are often different between individuals in treatment and control. However, existing literature does not reach a consensus on the optimal use of survey weights for population-level inference in the propensity score weighting analysis. Under the balancing weights framework, we provided a unified solution for incorporating survey weights in both the propensity score of estimation and the outcome regression model. We derived estimators for different target populations, including the combined, treated, controlled, and overlap populations. We provide a unified expression of the sandwich variance estimator and demonstrate that the survey-weighted estimator is asymptotically normal, as established through the theory of M-estimators. Through an extensive series of simulation studies, we examined the performance of our derived estimators and compared the results to those of alternative methods. We further carried out two case studies to illustrate the application of the different methods of propensity score analysis with complex survey data. We concluded with a discussion of our findings and provided practical guidelines for propensity score weighting analysis of observational data from complex surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16156v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukang Zeng, Fan Li, Guangyu Tong</dc:creator>
    </item>
    <item>
      <title>The typicality principle and its implications for statistics and data science</title>
      <link>https://arxiv.org/abs/2501.14860</link>
      <description>arXiv:2501.14860v1 Announce Type: cross 
Abstract: A central focus of data science is the transformation of empirical evidence into knowledge. As such, the key insights and scientific attitudes of deep thinkers like Fisher, Popper, and Tukey are expected to inspire exciting new advances in machine learning and artificial intelligence in years to come. Along these lines, the present paper advances a novel {\em typicality principle} which states, roughly, that if the observed data is sufficiently ``atypical'' in a certain sense relative to a posited theory, then that theory is unwarranted. This emphasis on typicality brings familiar but often overlooked background notions like model-checking to the inferential foreground. One instantiation of the typicality principle is in the context of parameter estimation, where we propose a new typicality-based regularization strategy that leans heavily on goodness-of-fit testing. The effectiveness of this new regularization strategy is illustrated in three non-trivial examples where ordinary maximum likelihood estimation fails miserably. We also demonstrate how the typicality principle fits within a bigger picture of reliable and efficient uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14860v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Zeyu Zhang, Ryan Martin, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Private Minimum Hellinger Distance Estimation via Hellinger Distance Differential Privacy</title>
      <link>https://arxiv.org/abs/2501.14974</link>
      <description>arXiv:2501.14974v1 Announce Type: cross 
Abstract: Hellinger distance has been widely used to derive objective functions that are alternatives to maximum likelihood methods. Motivated by recent regulatory privacy requirements, estimators satisfying differential privacy constraints are being derived. In this paper, we describe different notions of privacy using divergences and establish that Hellinger distance minimizes the added variance within the class of power divergences for an additive Gaussian mechanism. We demonstrate that a new definition of privacy, namely Hellinger differential privacy, shares several features of the standard notion of differential privacy while allowing for sharper inference. Using these properties, we develop private versions of gradient descent and Newton-Raphson algorithms for obtaining private minimum Hellinger distance estimators, which are robust and first-order efficient. Using numerical experiments, we illustrate the finite sample performance and verify that they retain their robustness properties under gross-error contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14974v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengnan Deng, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>A General Approach to Relaxing Unconfoundedness</title>
      <link>https://arxiv.org/abs/2501.15400</link>
      <description>arXiv:2501.15400v1 Announce Type: cross 
Abstract: This paper defines a general class of relaxations of the unconfoundedness assumption. This class includes several previous approaches as special cases, including the marginal sensitivity model of Tan (2006). This class therefore allows us to precisely compare and contrast these previously disparate relaxations. We use this class to derive a variety of new identification results which can be used to assess sensitivity to unconfoundedness. In particular, the prior literature focuses on average parameters, like the average treatment effect (ATE). We move beyond averages by providing sharp bounds for a large class of parameters, including both the quantile treatment effect (QTE) and the distribution of treatment effects (DTE), results which were previously unknown even for the marginal sensitivity model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15400v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew A. Masten, Alexandre Poirier, Muyang Ren</dc:creator>
    </item>
    <item>
      <title>Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport</title>
      <link>https://arxiv.org/abs/2501.15549</link>
      <description>arXiv:2501.15549v1 Announce Type: cross 
Abstract: Recently, optimal transport-based approaches have gained attention for deriving counterfactuals, e.g., to quantify algorithmic discrimination. However, in the general multivariate setting, these methods are often opaque and difficult to interpret. To address this, alternative methodologies have been proposed, using causal graphs combined with iterative quantile regressions (Ple\v{c}ko and Meinshausen (2020)) or sequential transport (Fernandes Machado et al. (2025)) to examine fairness at the individual level, often referred to as ``counterfactual fairness.'' Despite these advancements, transporting categorical variables remains a significant challenge in practical applications with real datasets. In this paper, we propose a novel approach to address this issue. Our method involves (1) converting categorical variables into compositional data and (2) transporting these compositions within the probabilistic simplex of $\mathbb{R}^d$. We demonstrate the applicability and effectiveness of this approach through an illustration on real-world data, and discuss limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15549v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic</dc:creator>
    </item>
    <item>
      <title>BoTier: Multi-Objective Bayesian Optimization with Tiered Composite Objectives</title>
      <link>https://arxiv.org/abs/2501.15554</link>
      <description>arXiv:2501.15554v1 Announce Type: cross 
Abstract: Scientific optimization problems are usually concerned with balancing multiple competing objectives, which come as preferences over both the outcomes of an experiment (e.g. maximize the reaction yield) and the corresponding input parameters (e.g. minimize the use of an expensive reagent). Typically, practical and economic considerations define a hierarchy over these objectives, which must be reflected in algorithms for sample-efficient experiment planning. Herein, we introduce BoTier, a composite objective that can flexibly represent a hierarchy of preferences over both experiment outcomes and input parameters. We provide systematic benchmarks on synthetic and real-life surfaces, demonstrating the robust applicability of BoTier across a number of use cases. Importantly, BoTier is implemented in an auto-differentiable fashion, enabling seamless integration with the BoTorch library, thereby facilitating adoption by the scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15554v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Haddadnia, Leonie Grashoff, Felix Strieth-Kalthoff</dc:creator>
    </item>
    <item>
      <title>I-trustworthy Models. A framework for trustworthiness evaluation of probabilistic classifiers</title>
      <link>https://arxiv.org/abs/2501.15617</link>
      <description>arXiv:2501.15617v1 Announce Type: cross 
Abstract: As probabilistic models continue to permeate various facets of our society and contribute to scientific advancements, it becomes a necessity to go beyond traditional metrics such as predictive accuracy and error rates and assess their trustworthiness. Grounded in the competence-based theory of trust, this work formalizes I-trustworthy framework -- a novel framework for assessing the trustworthiness of probabilistic classifiers for inference tasks by linking local calibration to trustworthiness. To assess I-trustworthiness, we use the local calibration error (LCE) and develop a method of hypothesis-testing. This method utilizes a kernel-based test statistic, Kernel Local Calibration Error (KLCE), to test local calibration of a probabilistic classifier. This study provides theoretical guarantees by offering convergence bounds for an unbiased estimator of KLCE. Additionally, we present a diagnostic tool designed to identify and measure biases in cases of miscalibration. The effectiveness of the proposed test statistic is demonstrated through its application to both simulated and real-world datasets. Finally, LCE of related recalibration methods is studied, and we provide evidence of insufficiency of existing methods to achieve I-trustworthiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15617v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritwik Vashistha, Arya Farahi</dc:creator>
    </item>
    <item>
      <title>Advancing Portfolio Optimization: Adaptive Minimum-Variance Portfolios and Minimum Risk Rate Frameworks</title>
      <link>https://arxiv.org/abs/2501.15793</link>
      <description>arXiv:2501.15793v1 Announce Type: cross 
Abstract: This study presents the Adaptive Minimum-Variance Portfolio (AMVP) framework and the Adaptive Minimum-Risk Rate (AMRR) metric, innovative tools designed to optimize portfolios dynamically in volatile and nonstationary financial markets. Unlike traditional minimum-variance approaches, the AMVP framework incorporates real-time adaptability through advanced econometric models, including ARFIMA-FIGARCH processes and non-Gaussian innovations. Empirical applications on cryptocurrency and equity markets demonstrate the proposed framework's superior performance in risk reduction and portfolio stability, particularly during periods of structural market breaks and heightened volatility. The findings highlight the practical implications of using the AMVP and AMRR methodologies to address modern investment challenges, offering actionable insights for portfolio managers navigating uncertain and rapidly changing market conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15793v1</guid>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Jha, Abootaleb Shirvani, Ali Jaffri, Svetlozar T. Rachev, Frank J. Fabozzi</dc:creator>
    </item>
    <item>
      <title>A mirror descent approach to maximum likelihood estimation in latent variable models</title>
      <link>https://arxiv.org/abs/2501.15896</link>
      <description>arXiv:2501.15896v1 Announce Type: cross 
Abstract: We introduce an approach based on mirror descent and sequential Monte Carlo (SMC) to perform joint parameter inference and posterior estimation in latent variable models. This approach is based on minimisation of a functional over the parameter space and the space of probability distributions and, contrary to other popular approaches, can be implemented when the latent variable takes values in discrete spaces. We provide a detailed theoretical analysis of both the mirror descent algorithm and its approximation via SMC. We experimentally show that the proposed algorithm outperforms standard expectation maximisation algorithms and is competitive with other popular methods for real-valued latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15896v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Low-Rank Tensor Models</title>
      <link>https://arxiv.org/abs/2501.16223</link>
      <description>arXiv:2501.16223v1 Announce Type: cross 
Abstract: Statistical inference for tensors has emerged as a critical challenge in analyzing high-dimensional data in modern data science. This paper introduces a unified framework for inferring general and low-Tucker-rank linear functionals of low-Tucker-rank signal tensors for several low-rank tensor models. Our methodology tackles two primary goals: achieving asymptotic normality and constructing minimax-optimal confidence intervals. By leveraging a debiasing strategy and projecting onto the tangent space of the low-Tucker-rank manifold, we enable inference for general and structured linear functionals, extending far beyond the scope of traditional entrywise inference. Specifically, in the low-Tucker-rank tensor regression or PCA model, we establish the computational and statistical efficiency of our approach, achieving near-optimal sample size requirements (in regression model) and signal-to-noise ratio (SNR) conditions (in PCA model) for general linear functionals without requiring sparsity in the loading tensor. Our framework also attains both computationally and statistically optimal sample size and SNR thresholds for low-Tucker-rank linear functionals. Numerical experiments validate our theoretical results, showcasing the framework's utility in diverse applications. This work addresses significant methodological gaps in statistical inference, advancing tensor analysis for complex and high-dimensional data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16223v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ke Xu, Elynn Chen, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Bayesian Spatial Predictive Synthesis</title>
      <link>https://arxiv.org/abs/2203.05197</link>
      <description>arXiv:2203.05197v4 Announce Type: replace 
Abstract: Due to spatial dependence -- often characterized as complex and non-linear -- model misspecification is a prevalent and critical issue in spatial data analysis and prediction. As the data, and thus model performance, is heterogeneous, typical model selection and ensemble methods that assume homogeneity are not suitable. We address the issue of model uncertainty for spatial data by proposing a novel Bayesian ensemble methodology that captures spatially-varying model uncertainty and performance heterogeneity of multiple spatial predictions, and synthesizes them for improved predictions, which we call Bayesian spatial predictive synthesis. Our proposal is defined by specifying a latent factor spatially-varying coefficient model as the synthesis function, which enables spatial characteristics of each model to be learned and ensemble coefficients to vary over regions to achieve flexible predictions. We derive our method from the theoretically best approximation of the data generating process, and show that it provides a finite sample theoretical guarantee for its predictive performance, specifically that the predictions are exact minimax. Two MCMC strategies are implemented for full uncertainty quantification, as well as a variational inference strategy for fast point inference. We also extend the estimation strategy for general responses. Through simulation examples and two real data applications in real estate and ecology, our proposed Bayesian spatial predictive synthesis outperforms standard spatial models and ensemble methods, and advanced machine learning methods, in terms of predictive accuracy and uncertainty quantification, while maintaining interpretability of the prediction mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05197v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danielle Cabel, Shonosuke Sugasawa, Masahiro Kato, Kosaku Takanashi, Kenichiro McAlinn</dc:creator>
    </item>
    <item>
      <title>Multivariate Dynamic Mediation Analysis under a Reinforcement Learning Framework</title>
      <link>https://arxiv.org/abs/2310.16203</link>
      <description>arXiv:2310.16203v2 Announce Type: replace 
Abstract: Mediation analysis is an important analytic tool commonly used in a broad range of scientific applications. In this article, we study the problem of mediation analysis when there are multivariate and conditionally dependent mediators, and when the variables are observed over multiple time points. The problem is challenging, because the effect of a mediator involves not only the path from the treatment to this mediator itself at the current time point, but also all possible paths pointed to this mediator from its upstream mediators, as well as the carryover effects from all previous time points. We propose a novel multivariate dynamic mediation analysis approach. Drawing inspiration from the Markov decision process model that is frequently employed in reinforcement learning, we introduce a Markov mediation process paired with a system of time-varying linear structural equation models to formulate the problem. We then formally define the individual mediation effect, built upon the idea of simultaneous interventions and intervention calculus. We next derive the closed-form expression and propose an iterative estimation procedure under the Markov mediation process model. We study both the asymptotic property and the empirical performance of the proposed estimator, and further illustrate our method with a mobile health application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16203v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lan Luo, Chengchun Shi, Jitao Wang, Zhenke Wu, Lexin Li</dc:creator>
    </item>
    <item>
      <title>On the estimation of the number of components in multivariate functional principal component analysis</title>
      <link>https://arxiv.org/abs/2311.04540</link>
      <description>arXiv:2311.04540v3 Announce Type: replace 
Abstract: Happ and Greven (2018) developed a methodology for principal components analysis of multivariate functional data observed on different dimensional domains. Their approach relies on an estimation of univariate functional principal components for each univariate functional feature. In this paper, we present extensive simulations to investigate choosing the number of principal components to retain. We show empirically that the conventional approach of using a percentage of variance explained threshold for each univariate functional feature may be unreliable when aiming to explain an overall percentage of variance in the multivariate functional data, and thus we advise practitioners to exercise caution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04540v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/03610918.2025.2459862</arxiv:DOI>
      <dc:creator>Steven Golovkine, Edward Gunning, Andrew J. Simpkin, Norma Bargary</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v2 Announce Type: replace 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Statistical analysis and method to quantify the impact of measurement uncertainty on dynamic mode decomposition</title>
      <link>https://arxiv.org/abs/2403.17318</link>
      <description>arXiv:2403.17318v2 Announce Type: replace 
Abstract: We apply random matrix theory to study the impact of measurement uncertainty on dynamic mode decomposition. Specifically, when the measurements follow a normal probability density function, we show how the moments of that density propagate through the dynamic mode decomposition. While we focus on the first and second moments, the analytical expressions we derive are general and can be extended to higher-order moments. Furthermore, the proposed numerical method for propagating uncertainty is agnostic of specific dynamic mode decomposition formulations. Of particular relevance, the estimated second moments provide confidence bounds that may be used as a metric of trustworthiness, that is, how much one can rely on a finite-dimensional linear operator to represent an underlying dynamical system. We perform numerical experiments on two canonical systems and verify the estimated confidence levels by comparing the moments with those obtained from Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17318v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. Algikar, P. Sharma, M. Netto, L. Mili</dc:creator>
    </item>
    <item>
      <title>A New Fit Assessment Framework for Common Factor Models Using Generalized Residuals</title>
      <link>https://arxiv.org/abs/2405.15204</link>
      <description>arXiv:2405.15204v2 Announce Type: replace 
Abstract: Assessing fit in common factor models solely through the lens of mean and covariance structures, as is commonly done with conventional goodness-of-fit (GOF) assessments, may overlook critical aspects of misfit, potentially leading to misleading conclusions. To achieve more flexible fit assessment, we extend the theory of generalized residuals (Haberman &amp; Sinharay, 2013), originally developed for models with categorical data, to encompass more general measurement models. Within this extended framework, we propose several fit test statistics designed to evaluate various parametric assumptions involved in common factor models. The examples include assessing the distributional assumptions of latent variables and functional form assumptions of individual manifest variables. The performance of the proposed statistics is examined through simulation studies and an empirical data analysis. Our findings suggest that generalized residuals are promising tools for detecting misfit in measurement models, often masked when assessed by conventional GOF testing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15204v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youjin Sung, Youngjin Han, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Eliciting prior information from clinical trials via calibrated Bayes factor</title>
      <link>https://arxiv.org/abs/2406.19346</link>
      <description>arXiv:2406.19346v2 Announce Type: replace 
Abstract: In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19346v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i Demartino, Leonardo Egidi, Nicola Torelli, Ioannis Ntzoufras</dc:creator>
    </item>
    <item>
      <title>Cellwise outlier detection in heterogeneous populations</title>
      <link>https://arxiv.org/abs/2409.07881</link>
      <description>arXiv:2409.07881v2 Announce Type: replace 
Abstract: Real-world applications may be affected by outlying values. In the model-based clustering literature, several methodologies have been proposed to detect units that deviate from the majority of the data (rowwise outliers) and trim them from the parameter estimates. However, the discarded observations can encompass valuable information in some observed features. Following the more recent cellwise contamination paradigm, we introduce a Gaussian mixture model for cellwise outlier detection. The proposal is estimated via an Expectation-Maximization (EM) algorithm with an additional step for flagging the contaminated cells of a data matrix and then imputing - instead of discarding - them before the parameter estimation. This procedure adheres to the spirit of the EM algorithm by treating the contaminated cells as missing values. We analyze the performance of the proposed model in comparison with other existing methodologies through a simulation study with different scenarios and illustrate its potential use for clustering, outlier detection, and imputation on three real data sets. Additional applications include socio-economic studies, environmental analysis, healthcare, and any domain where the aim is to cluster data affected by missing information and outlying values within features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07881v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Zaccaria, Luis A. Garc\'ia-Escudero, Francesca Greselin, Agust\'in Mayo-\'Iscar</dc:creator>
    </item>
    <item>
      <title>Batch Predictive Inference</title>
      <link>https://arxiv.org/abs/2409.13990</link>
      <description>arXiv:2409.13990v4 Announce Type: replace 
Abstract: Constructing prediction sets with coverage guarantees for unobserved outcomes is a core problem in modern statistics. Methods for predictive inference have been developed for a wide range of settings, but usually only consider test data points one at a time. Here we study the problem of distribution-free predictive inference for a batch of multiple test points, aiming to construct prediction sets for functions -- such as the mean or median -- of any number of unobserved test datapoints. This setting includes constructing simultaneous prediction sets with a high probability of coverage, and selecting datapoints satisfying a specified condition while controlling the number of false claims.
  For the general task of predictive inference on a function of a batch of test points, we introduce a methodology called batch predictive inference (batch PI), and provide a distribution-free coverage guarantee under exchangeability of the calibration and test data. Batch PI requires the quantiles of a rank ordering function defined on certain subsets of ranks. While computing these quantiles is NP-hard in general, we show that it can be done efficiently in many cases of interest, most notably for batch score functions with a compositional structure -- which includes examples of interest such as the mean -- via a dynamic programming algorithm that we develop. Batch PI has advantages over naive approaches (such as partitioning the calibration data or directly extending conformal prediction) in many settings, as it can deliver informative prediction sets even using small calibration sample sizes. We illustrate that our procedures provide informative inference across the use cases mentioned above, through experiments on both simulated data and a drug-target interaction dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13990v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based goodness-of-fit test for parametric families of conditional distributions</title>
      <link>https://arxiv.org/abs/2409.20262</link>
      <description>arXiv:2409.20262v4 Announce Type: replace 
Abstract: In this paper, we introduce a consistent goodness-of-fit test for distributional regression. The test statistic is based on a process that traces the difference between a nonparametric and a semi-parametric estimate of the marginal distribution function of Y. As its asymptotic null distribution is not distribution-free, a parametric bootstrap method is used to determine critical values. Empirical results suggest that, in certain scenarios, the test outperforms existing specification tests by achieving a higher power and thereby offering greater sensitivity to deviations from the assumed parametric distribution family. Notably, the proposed test does not involve any hyperparameters and can easily be applied to individual datatsets using the gofreg-package in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20262v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gitte Kremling, Gerhard Dikta</dc:creator>
    </item>
    <item>
      <title>Testing Mutual Independence in Metric Spaces Using Distance Profiles</title>
      <link>https://arxiv.org/abs/2412.06766</link>
      <description>arXiv:2412.06766v2 Announce Type: replace 
Abstract: This paper introduces a novel unified framework for testing mutual independence among a vector of random objects that may reside in different metric spaces, including some existing methodologies as special cases. The backbone of the proposed tests is the notion of joint distance profiles, which uniquely characterize the joint law of random objects under a mild condition on the joint law or on the metric spaces. Our test statistics measure the difference of the joint distance profiles of each data point with respect to the joint law and the product of marginal laws of the vector of random objects, where flexible data-adaptive weight profiles are incorporated for power enhancement. We derive the limiting distribution of the test statistics under the null hypothesis of mutual independence and show that the proposed tests with specific weight profiles are asymptotically distribution-free if the marginal distance profiles are continuous. We also establish the consistency of the tests under sequences of alternative hypotheses converging to the null. Furthermore, since the asymptotic tests with non-trivial weight profiles require the knowledge of the underlying data distribution, we adopt a permutation scheme to approximate the $p$-values and provide theoretical guarantees that the permutation-based tests control the type I error rate under the null and are consistent under the alternatives. We demonstrate the power of the proposed tests across various types of data objects through simulations and real data applications, where our tests are shown to have superior performance compared with popular existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06766v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaqing Chen, Paromita Dubey</dc:creator>
    </item>
    <item>
      <title>Determining The Number of Factors in Two-Way Factor Model of High-Dimensional Matrix-Variate Time Series: A White-Noise based Method for Serial Correlation Models</title>
      <link>https://arxiv.org/abs/2501.13614</link>
      <description>arXiv:2501.13614v2 Announce Type: replace 
Abstract: In this paper, we study a new two-way factor model for high-dimensional matrix-variate time series. To estimate the number of factors in this two-way factor model, we decompose the series into two parts: one being a non-weakly correlated series and the other being a weakly correlated noise. By comparing the difference between two series, we can construct white-noise based signal statistics to determine the number of factors in row loading matrix (column loading matrix). Furthermore, to mitigate the negative impact on the accuracy of the estimation, which is caused by the interaction between the row loading matrix and the column loading matrix, we propose a transformation so that the transformed model only contains the row loading matrix (column loading matrix). We define sequences of ratios of two test statistics as signal statistics to determine the number of factors and derive the consistence of the estimation. We implement the numerical studies to examine the performance of the new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13614v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Xia</dc:creator>
    </item>
    <item>
      <title>Identification of Treatment Effects under Limited Exogenous Variation</title>
      <link>https://arxiv.org/abs/1811.09837</link>
      <description>arXiv:1811.09837v2 Announce Type: replace-cross 
Abstract: Multidimensional heterogeneity and endogeneity are important features of a wide class of econometric models. With control variables to correct for endogeneity, nonparametric identification of treatment effects requires strong support conditions. To alleviate this requirement, we consider varying coefficients specifications for the conditional expectation function of the outcome given a treatment and control variables. This function is expressed as a linear combination of either known functions of the treatment, with unknown coefficients varying with the controls, or known functions of the controls, with unknown coefficients varying with the treatment. We use this modeling approach to give necessary and sufficient conditions for identification of average treatment effects. A sufficient condition for identification is conditional nonsingularity, that the second moment matrix of the known functions given the variable in the varying coefficients is nonsingular with probability one. For known treatment functions with sufficient variation, we find that triangular models with discrete instrument cannot identify average treatment effects when the number of support points for the instrument is less than the number of coefficients. For known functions of the controls, we find that average treatment effects can be identified in general nonseparable triangular models with binary or discrete instruments. We extend our analysis to flexible models of increasing dimension and relate conditional nonsingularity to the full support condition of Imbens and Newey (2009), thereby embedding semi- and non-parametric identification into a common framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:1811.09837v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Whitney K. Newey, Sami Stouli</dc:creator>
    </item>
    <item>
      <title>Cardinality-Regularized Hawkes-Granger Model</title>
      <link>https://arxiv.org/abs/2208.10671</link>
      <description>arXiv:2208.10671v2 Announce Type: replace-cross 
Abstract: We propose a new sparse Granger-causal learning framework for temporal event data. We focus on a specific class of point processes called the Hawkes process. We begin by pointing out that most of the existing sparse causal learning algorithms for the Hawkes process suffer from a singularity in maximum likelihood estimation. As a result, their sparse solutions can appear only as numerical artifacts. In this paper, we propose a mathematically well-defined sparse causal learning framework based on a cardinality-regularized Hawkes process, which remedies the pathological issues of existing approaches. We leverage the proposed algorithm for the task of instance-wise causal event analysis, where sparsity plays a critical role. We validate the proposed framework with two real use-cases, one from the power grid and the other from the cloud data center management domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.10671v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Tsuyoshi Id\'e, Georgios Kollias, Dzung T. Phan, Naoki Abe, "Cardinality-Regularized Hawkes-Granger Model," Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pp.2682-2694</arxiv:journal_reference>
      <dc:creator>Tsuyoshi Id\'e, Georgios Kollias, Dzung T. Phan, Naoki Abe</dc:creator>
    </item>
    <item>
      <title>Community detection in bipartite signed networks is highly dependent on parameter choice</title>
      <link>https://arxiv.org/abs/2405.08203</link>
      <description>arXiv:2405.08203v2 Announce Type: replace-cross 
Abstract: Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on projected bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious user communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08203v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Candellone, Erik-Jan van Kesteren, Sofia Chelmi, Javier Garcia-Bernardo</dc:creator>
    </item>
    <item>
      <title>Generalized Neyman Allocation for Locally Minimax Optimal Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2405.19317</link>
      <description>arXiv:2405.19317v3 Announce Type: replace-cross 
Abstract: This study investigates an asymptotically locally minimax optimal algorithm for fixed-budget best-arm identification (BAI). We propose the Generalized Neyman Allocation (GNA) algorithm and demonstrate that its worst-case upper bound on the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our lower and upper bounds are tight, matching exactly including constant terms within the small-gap regime. The GNA algorithm generalizes the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and refines existing BAI algorithms, such as those proposed by Glynn &amp; Juneja (2004). By proposing an asymptotically minimax optimal algorithm, we address the longstanding open issue in BAI (Kaufmann, 2020) and treatment choice (Kasy &amp; Sautmann, 202) by restricting a class of distributions to the small-gap regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19317v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Causally-Aware Unsupervised Feature Selection Learning</title>
      <link>https://arxiv.org/abs/2410.12224</link>
      <description>arXiv:2410.12224v2 Announce Type: replace-cross 
Abstract: Unsupervised feature selection (UFS) has recently gained attention for its effectiveness in processing unlabeled high-dimensional data. However, existing methods overlook the intrinsic causal mechanisms within the data, resulting in the selection of irrelevant features and poor interpretability. Additionally, previous graph-based methods fail to account for the differing impacts of non-causal and causal features in constructing the similarity graph, which leads to false links in the generated graph. To address these issues, a novel UFS method, called Causally-Aware UnSupErvised Feature Selection learning (CAUSE-FS), is proposed. CAUSE-FS introduces a novel causal regularizer that reweights samples to balance the confounding distribution of each treatment feature. This regularizer is subsequently integrated into a generalized unsupervised spectral regression model to mitigate spurious associations between features and clustering labels, thus achieving causal feature selection. Furthermore, CAUSE-FS employs causality-guided hierarchical clustering to partition features with varying causal contributions into multiple granularities. By integrating similarity graphs learned adaptively at different granularities, CAUSE-FS increases the importance of causal features when constructing the fused similarity graph to capture the reliable local structure of data. Extensive experimental results demonstrate the superiority of CAUSE-FS over state-of-the-art methods, with its interpretability further validated through feature visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12224v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongxin Shen, Yanyong Huang, Dongjie Wang, Minbo Ma, Fengmao Lv, Tianrui Li</dc:creator>
    </item>
    <item>
      <title>A Note on Doubly Robust Estimator in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2411.07978</link>
      <description>arXiv:2411.07978v4 Announce Type: replace-cross 
Abstract: This note introduces a doubly robust (DR) estimator for regression discontinuity (RD) designs. RD designs provide a quasi-experimental framework for estimating treatment effects, where treatment assignment depends on whether a running variable surpasses a predefined cutoff. A common approach in RD estimation is the use of nonparametric regression methods, such as local linear regression. However, the validity of these methods still relies on the consistency of the nonparametric estimators. In this study, we propose the DR-RD estimator, which combines two distinct estimators for the conditional expected outcomes. The primary advantage of the DR-RD estimator lies in its ability to ensure the consistency of the treatment effect estimation as long as at least one of the two estimators is consistent. Consequently, our DR-RD estimator enhances robustness of treatment effect estimators in RD designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07978v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v2 Announce Type: replace-cross 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under a fixed normal chart centered at the true parameter, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings-including spheres, the Stiefel manifold, fixed-rank matrices manifolds, and rank-one tensor manifolds-and, for Euclidean submanifolds, introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
