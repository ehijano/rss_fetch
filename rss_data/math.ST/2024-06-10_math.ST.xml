<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Jun 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comments on B. Hansen's Reply to "A Comment on: `A Modern Gauss-Markov Theorem'", and Some Related Discussion</title>
      <link>https://arxiv.org/abs/2406.03971</link>
      <description>arXiv:2406.03971v1 Announce Type: cross 
Abstract: In P\"otscher and Preinerstorfer (2022) and in the abridged version P\"otscher and Preinerstorfer (2024, published in Econometrica) we have tried to clear up the confusion introduced in Hansen (2022a) and in the earlier versions Hansen (2021a,b). Unfortunatelly, Hansen's (2024) reply to P\"otscher and Preinerstorfer (2024) further adds to the confusion. While we are already somewhat tired of the matter, for the sake of the econometrics community we feel compelled to provide clarification. We also add a comment on Portnoy (2023), a "correction" to Portnoy (2022), as well as on Lei and Wooldridge (2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03971v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt M. P\"otscher</dc:creator>
    </item>
    <item>
      <title>On Regularization via Early Stopping for Least Squares Regression</title>
      <link>https://arxiv.org/abs/2406.04425</link>
      <description>arXiv:2406.04425v1 Announce Type: cross 
Abstract: A fundamental problem in machine learning is understanding the effect of early stopping on the parameters obtained and the generalization capabilities of the model. Even for linear models, the effect is not fully understood for arbitrary learning rates and data. In this paper, we analyze the dynamics of discrete full batch gradient descent for linear regression. With minimal assumptions, we characterize the trajectory of the parameters and the expected excess risk. Using this characterization, we show that when training with a learning rate schedule $\eta_k$, and a finite time horizon $T$, the early stopped solution $\beta_T$ is equivalent to the minimum norm solution for a generalized ridge regularized problem. We also prove that early stopping is beneficial for generic data with arbitrary spectrum and for a wide variety of learning rate schedules. We provide an estimate for the optimal stopping time and empirically demonstrate the accuracy of our estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04425v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Sonthalia, Jackie Lok, Elizaveta Rebrova</dc:creator>
    </item>
    <item>
      <title>Crafting Heavy-Tails in Weight Matrix Spectrum without Gradient Noise</title>
      <link>https://arxiv.org/abs/2406.04657</link>
      <description>arXiv:2406.04657v1 Announce Type: cross 
Abstract: Modern training strategies of deep neural networks (NNs) tend to induce a heavy-tailed (HT) spectra of layer weights. Extensive efforts to study this phenomenon have found that NNs with HT weight spectra tend to generalize well. A prevailing notion for the occurrence of such HT spectra attributes gradient noise during training as a key contributing factor. Our work shows that gradient noise is unnecessary for generating HT weight spectra: two-layer NNs trained with full-batch Gradient Descent/Adam can exhibit HT spectra in their weights after finite training steps. To this end, we first identify the scale of the learning rate at which one step of full-batch Adam can lead to feature learning in the shallow NN, particularly when learning a single index teacher model. Next, we show that multiple optimizer steps with such (sufficiently) large learning rates can transition the bulk of the weight's spectra into an HT distribution. To understand this behavior, we present a novel perspective based on the singular vectors of the weight matrices and optimizer updates. We show that the HT weight spectrum originates from the `spike', which is generated from feature learning and interacts with the main bulk to generate an HT spectrum. Finally, we analyze the correlations between the HT weight spectra and generalization after multiple optimizer updates with varying learning rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04657v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vignesh Kothapalli, Tianyu Pang, Shenyang Deng, Zongmin Liu, Yaoqing Yang</dc:creator>
    </item>
    <item>
      <title>Fractionally integrated curve time series with cointegration</title>
      <link>https://arxiv.org/abs/2212.04071</link>
      <description>arXiv:2212.04071v2 Announce Type: replace 
Abstract: We introduce methods and theory for fractionally cointegrated curve time series. We develop a variance-ratio test to determine the dimensions associated with the nonstationary and stationary subspaces. For each subspace, we apply a local Whittle estimator to estimate the long-memory parameter and establish its consistency. A Monte Carlo study of finite-sample performance is included, along with two empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04071v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won-Ki Seo, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Linear estimators for Gaussian random variables in Hilbert spaces</title>
      <link>https://arxiv.org/abs/2305.11083</link>
      <description>arXiv:2305.11083v2 Announce Type: replace 
Abstract: We study a statistical model for infinite dimensional Gaussian random variables with unknown parameters. For this model we derive linear estimators for the mean and the variance of the Gaussian distribution. Furthermore, we construct confidence intervals and perform hypothesis testing. A linear regression problem in infinite dimensions and some perspectives to statistical and machine learning are presented as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11083v2</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Tappe</dc:creator>
    </item>
    <item>
      <title>The Laplace approximation accuracy in high dimensions: a refined analysis and new skew adjustment</title>
      <link>https://arxiv.org/abs/2306.07262</link>
      <description>arXiv:2306.07262v3 Announce Type: replace 
Abstract: In Bayesian inference, making deductions about a parameter of interest requires one to sample from or compute an integral against a posterior distribution. A popular method to make these computations cheaper in high-dimensional settings is to replace the posterior with its Laplace approximation (LA), a Gaussian distribution. In this work, we derive a leading order decomposition of the LA error, a powerful technique to analyze the accuracy of the approximation more precisely than was possible before. It allows us to derive the first ever skew correction to the LA which provably improves its accuracy by an order of magnitude in the high-dimensional regime. Our approach also enables us to prove both tighter upper bounds on the standard LA and the first ever lower bounds in high dimensions. In particular, we prove that $d^2\ll n$ is in general necessary for accuracy of the LA, where $d$ is dimension and $n$ is sample size. Finally, we apply our theory in two example models: a Dirichlet posterior arising from a multinomial observation, and logistic regression with Gaussian design. In the latter setting, we prove high probability bounds on the accuracy of the LA and skew-corrected LA in powers of $d/\sqrt n$ alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07262v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anya Katsevich</dc:creator>
    </item>
    <item>
      <title>Estimation of several parameters in discretely-observed Stochastic Differential Equations with additive fractional noise</title>
      <link>https://arxiv.org/abs/2306.16272</link>
      <description>arXiv:2306.16272v3 Announce Type: replace 
Abstract: We investigate the problem of joint statistical estimation of several parameters for a stochastic differential equation driven by an additive fractional Brownian motion. Based on discrete-time observations of the model, we construct an estimator of the Hurst parameter, the diffusion parameter and the drift, which lies in a parametrised family of coercive drift coefficients. Our procedure is based on the assumption that the stationary distribution of the SDE and of its increments permits to identify the parameters of the model. Under this assumption, we prove consistency results and derive a rate of convergence for the estimator. Finally, we show that the identifiability assumption is satisfied in the case of a family of fractional Ornstein-Uhlenbeck processes and illustrate our results with some numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16272v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>El Mehdi Haress, Alexandre Richard</dc:creator>
    </item>
    <item>
      <title>Quasi-likelihood analysis for adaptive estimation of a degenerate diffusion process</title>
      <link>https://arxiv.org/abs/2402.15256</link>
      <description>arXiv:2402.15256v2 Announce Type: replace 
Abstract: The adaptive quasi-likelihood analysis is developed for a degenerate diffusion process. Asymptotic normality and moment convergence are proved for the quasi-maximum likelihood estimators and quasi-Bayesian estimators, in the adaptive scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15256v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnaud Gloter, Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>What Is a Good Imputation Under MAR Missingness?</title>
      <link>https://arxiv.org/abs/2403.19196</link>
      <description>arXiv:2403.19196v2 Announce Type: replace 
Abstract: Missing values pose a persistent challenge in modern data science. Consequently, there is an ever-growing number of publications introducing new imputation methods in various fields. The present paper attempts to take a step back and provide a more systematic analysis. Starting from an in-depth discussion of the Missing at Random (MAR) condition for nonparametric imputation, we first develop an identification result, showing that the widely used Multiple Imputation by Chained Equations (MICE) approach indeed identifies the right conditional distributions. Building on this analysis,  we propose three essential properties a successful imputation method should meet, thus enabling a more principled evaluation of existing methods and more targeted development of new methods. In particular, we introduce a new imputation method, denoted mice-DRF, that meets two out of the three criteria. We then discuss and refine ways to rank imputation methods, developing a powerful, easy-to-use scoring algorithm to rank missing value imputations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19196v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af (PREMEDICAL), Erwan Scornet (PREMEDICAL), Julie Josse (PREMEDICAL)</dc:creator>
    </item>
  </channel>
</rss>
