<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Solution of Advection Equation with Discontinuous Initial and Boundary Conditions via Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2601.20978</link>
      <description>arXiv:2601.20978v1 Announce Type: new 
Abstract: In this paper, we investigate several techniques for modeling the one-dimensional advection equation for a specific class of problems with discontinuous initial and boundary conditions using physics-informed neural networks (PINNs). To mitigate the spectral bias phenomenon, we employ a Fourier feature mapping layer as the input representation, adopt a two-stage training strategy in which the Fourier feature parameters and the neural network weights are optimized sequentially, and incorporate adaptive loss weighting. To further enhance the approximation accuracy, a median filter is applied to the spatial data, and the predicted solution is constrained through a bounded linear mapping. Moreover, for certain nonlinear problems, we introduce a modified loss function inspired by the upwind numerical scheme to alleviate the excessive smoothing of discontinuous solutions typically observed in neural network approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20978v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omid Khosravi, Mehdi Tatari</dc:creator>
    </item>
    <item>
      <title>Identification of space-dependent coefficients in two competing terms of a nonlinear subdiffusion equation</title>
      <link>https://arxiv.org/abs/2601.21018</link>
      <description>arXiv:2601.21018v1 Announce Type: new 
Abstract: We consider a (sub)diffusion equation with a nonlinearity of the form $pf(u)-qu$, where $p$ and $q$ are space dependent functions. Prominent examples are the Fisher-KPP, the Frank-Kamenetskii-Zeldovich and the Allen-Cahn equations. We devise a fixed point scheme for reconstructing the spatially varying coefficients from interior observations a) at final time under two different excitations b) at two different time instances under a single excitation. Convergence of the scheme as well as local uniqueness of these coefficients is proven. Numerical experiments illustrate the performance of the reconstruction scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21018v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Kaltenbacher, William Rundell</dc:creator>
    </item>
    <item>
      <title>Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity</title>
      <link>https://arxiv.org/abs/2601.21080</link>
      <description>arXiv:2601.21080v1 Announce Type: new 
Abstract: We propose a parametric hyperbolic conservation law (SymCLaw) for learning hyperbolic systems directly from data while ensuring conservation, entropy stability, and hyperbolicity by design. Unlike existing approaches that typically enforce only conservation or rely on prior knowledge of the governing equations, our method parameterizes the flux functions in a form that guarantees real eigenvalues and complete eigenvectors of the flux Jacobian, thereby preserving hyperbolicity. At the same time, we embed entropy-stable design principles by jointly learning a convex entropy function and its associated flux potential, ensuring entropy dissipation and the selection of physically admissible weak solutions. A corresponding entropy-stable numerical flux scheme provides compatibility with standard discretizations, allowing seamless integration into classical solvers. Numerical experiments on benchmark problems, including Burgers, shallow water, Euler, and KPP equations, demonstrate that SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions, highlighting its potential as a principled foundation for data-driven modeling of hyperbolic conservation laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21080v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lizuo Liu, Lu Zhang, Anne Gelb</dc:creator>
    </item>
    <item>
      <title>An efficient implicit scheme for the multimaterial Euler equations in Lagrangian coordinates</title>
      <link>https://arxiv.org/abs/2601.21241</link>
      <description>arXiv:2601.21241v1 Announce Type: new 
Abstract: Stratified fluids composed of a sequence of alternate layers show interesting macroscopic properties, which may be quite different from those of the individual constituent fluids. On a macroscopic scale, such systems can be considered a sort of fluid metamaterial. In many cases each fluid layer can be described by Euler equations following the stiffened gas equation of state. The computation of detailed numerical solutions of such stratified material poses several challenges, first and foremost the issue of artificial smearing of material parameters across interface boundaries. Lagrangian schemes completely eliminate this issue, but at the cost of rather stringent time step restrictions. In this work we introduce an implicit numerical method for the multimaterial Euler equations in Lagrangian coordinates. The implicit discretization is aimed at bypassing the prohibitive time step restrictions present in flows with stratified media, where one of the materials is particularly dense, or rigid (or both). This is the case for flows of water-air mixtures, air-granular media, or similar high density ratio systems. We will present the novel discretisation approach, which makes extensive use of the remarkable structure of the governing equations in Lagrangian coordinates to find the solution by means of a single implicit discrete wave equation for the pressure field, yielding a symmetric positive definite structure and thus a particularly efficient algorithm. Additionally, we will introduce simple filtering strategies for counteracting the emergence of pressure or density oscillations typically encountered in multimaterial flows, and will present results concerning the robustness, accuracy, and performance of the proposed method, including applications to stratified media with high density and stiffness ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21241v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2025.114086</arxiv:DOI>
      <arxiv:journal_reference>J. Comput. Phys. 537 (2025) 11408</arxiv:journal_reference>
      <dc:creator>Simone Chiocchetti, Giovanni Russo</dc:creator>
    </item>
    <item>
      <title>Natural superconvergence points for splines</title>
      <link>https://arxiv.org/abs/2601.21368</link>
      <description>arXiv:2601.21368v1 Announce Type: new 
Abstract: This paper develops a unified theory of natural superconvergence points for polynomial spline approximations to second-order elliptic problems. Beginning with the one-dimensional case, we establish that when a point $x_0$ is a local symmetric center of the partition, the numerical error $(u-u_h)^{(s)}(x_0)$ exhibits superconvergence whenever the polynomial degree $k$ and the derivative order $s$ share the same parity. In particular, for the smoothest spline (B-spline) solution, the abundance of superconvergence points allows us to construct asymptotic expansion of the error within the element that fully characterize all superconvergence points, for both function values and derivatives. The theoretical framework is then extended to higher-dimensional settings on simplicial and tensor-product meshes, and the essential conclusions are preserved, with one-dimensional derivatives generalized to mixed derivatives. Numerical experiments demonstrate that superconvergence persists even in extremely localized symmetric regions, revealing that superconvergence points are both readily attainable and follow systematic distribution patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21368v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Yang, Zhimin Zhang</dc:creator>
    </item>
    <item>
      <title>Higher-Order Finite Difference Methods for the Tempered Fractional Laplacian</title>
      <link>https://arxiv.org/abs/2601.21388</link>
      <description>arXiv:2601.21388v1 Announce Type: new 
Abstract: This paper presents a general framework of high-order finite difference (HFD) schemes for the tempered fractional Laplacian (TFL) based on new generating functions obtained from the discrete symbols. Specifically, for sufficiently smooth functions, the resulting discretizations achieve high-order convergence with orders $p=4, 6, 8$. The discrete operators lead to Toeplitz stiffness matrices, allowing efficient matrix-vector multiplications via fast algorithms. Building on these approximations, HFD methods are formulated for solving TFL equations, and their stability and convergence are rigorously analyzed. Numerical simulations confirm the effectiveness of the proposed methods, showing excellent agreement with the theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21388v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyi Wang, Dongling Wang</dc:creator>
    </item>
    <item>
      <title>Numerical Methods for Dynamical Low-Rank Approximations of Stochastic Differential Equations -- Part I: Time discretization</title>
      <link>https://arxiv.org/abs/2601.21428</link>
      <description>arXiv:2601.21428v1 Announce Type: new 
Abstract: In this work (Part I), we study three time-discretization procedures of the Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations (SDEs). Specifically, we consider the Dynamically Orthogonal (DO) method for DLRA proposed and analyzed in arXiv:2308.11581v4, which consists of a linear combination of products between deterministic orthonormal modes and stochastic modes, both time-dependent. The first strategy we consider for numerical time-integration is very standard, consisting in a forward discretization in time of both deterministic and stochastic components. Its convergence is proven subject to a time-step restriction dependent on the smallest singular value of the Gram matrix associated to the stochastic modes. Under the same condition on the time-step, this smallest singular value is shown to be always positive, provided that the SDE under study is driven by a non-degenerate noise. The second and the third algorithms, on the other hand, are staggered ones, in which we alternately update the deterministic and the stochastic modes in half steps. These approaches are shown to be more stable than the first one and allow us to obtain convergence results without the aforementioned restriction on the time-step. Computational experiments support theoretical results. In this work we do not consider the discretization in probability, which will be the topic of Part II.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21428v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshihito Kazashi, Fabio Nobile, Fabio Zoccolan</dc:creator>
    </item>
    <item>
      <title>Numerical analysis of a locking-free primal hybrid method for linear elasticity with $H(\mathrm{div})$-conforming stress recovery</title>
      <link>https://arxiv.org/abs/2601.21635</link>
      <description>arXiv:2601.21635v1 Announce Type: new 
Abstract: In this work, we study a primal hybrid finite element method for the approximation of linear elasticity problems, posed in terms of displacement, an auxiliary pressure field, and a Lagrange multiplier related to the traction. We develop a general analysis for the existence and uniqueness of the solution for the discrete problem, which is applied to the construction of stable approximation spaces on triangular and quadrilateral meshes. The use of these spaces lead to optimal convergence orders, resulting in a locking-free method capable of providing robust approximations for nearly incompressible problems. Finally, we propose a strategy for recovering the stress field from the hybrid solution by solving element-wise sub-problems. The resulting stress approximation is $H(\mathrm{div})$-conforming, locally equilibrated, weakly symmetric, and robust to locking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21635v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Taraschi, Maicon Ribeiro Correa</dc:creator>
    </item>
    <item>
      <title>A Hybrid semi-Lagrangian Flow Mapping Approach for Vlasov Systems: Combining Iterative and Compositional Flow Maps</title>
      <link>https://arxiv.org/abs/2601.21668</link>
      <description>arXiv:2601.21668v1 Announce Type: new 
Abstract: We propose a hybrid semi-Lagrangian scheme for the Vlasov--Poisson equation that combines the Numerical Flow Iteration (NuFI) method with the Characteristic Mapping Method (CMM). Both approaches exploit the semi-group property of the underlying diffeomorphic flow, enabling the reconstruction of solutions through flow maps that trace characteristics back to their initial positions. NuFI builds this flow map iteratively, preserving symplectic structure and conserving invariants, but its computational cost scales quadratically with time. Its advantage lies in a compact, low-dimensional representation depending only on the electric field. In contrast, CMM achieves low computational costs when remapping by composing the global flow map from explicitly stored submaps. The proposed hybrid method merges these strengths: NuFi is employed for accurate and conservative local time stepping, while CMM efficiently propagates the solution through submap composition. This approach reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate the effectiveness of the scheme and highlight the trade-offs between memory usage and computational cost. We benchmark against a semi-Lagrangian predictor-corrector scheme used in modern gyrokinetic codes, evaluating accuracy and conservation properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21668v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <category>physics.flu-dyn</category>
      <category>physics.plasm-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Krah, Zetao Lin, R. -Paul Wilhelm, Fabio Bacchini, Jean-Christophe Nave, Virginie Grandgirard, Kai Schneider</dc:creator>
    </item>
    <item>
      <title>Adaptive Kernel Methods</title>
      <link>https://arxiv.org/abs/2601.21707</link>
      <description>arXiv:2601.21707v1 Announce Type: new 
Abstract: Kernel methods approximate nonlinear maps in a data-driven manner by projecting the target map onto a finite-dimensional Hilbert space called the solution space. Traditionally, this space is a subspace of a fixed ambient reproducing kernel Hilbert space (RKHS), determined solely by the chosen kernel and the dataset, whose elements identify the basis elements. Consequently, the projection operator underlying the kernel method depends on the loss function, the dataset, and the choice of ambient RKHS. In this study, we consider kernel methods whose solution spaces also depend on learnable parameters that are independent of the dataset. The resulting methods can be viewed as variable projection operators that depend on the loss function, the dataset, and the new learnable parameters instead of a fixed RKHS. This work has two main contributions. First, we propose an efficient approximation of kernels associated with infinite-dimensional RKHSs, commonly used to reduce the solution-space dimension for large datasets. Second, we construct fixed-dimensional, parameter-dependent solution spaces that enable highly efficient kernel models suitable for large-scale problems without the need to approximate kernels of infinite-dimensional RKHSs. Our novel family of adaptive kernel methods generalizes earlier approaches, including Random Fourier Features, and we demonstrate their effectiveness through several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21707v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tam\'as D\'ozsa, Andrea Angino, Zolt\'an Szab\'o, J\'ozsef Bokor, Matthias Voigt</dc:creator>
    </item>
    <item>
      <title>A reduced basis method for parabolic PDEs based on a space-time least squares formulation</title>
      <link>https://arxiv.org/abs/2601.21736</link>
      <description>arXiv:2601.21736v1 Announce Type: new 
Abstract: In this work, we present a POD-greedy reduced basis method for parabolic partial differential equations (PDEs), based on the least squares space-time formulation proposed in [Hinze, Kahle, Stahl, A least-squares space-time approach for parabolic equations, 2023, arXiv:2305.03402] that assumes only minimal regularity. We extend this approach to the parameter-dependent case. The corresponding variational formulation then is based on a parameter-dependent, symmetric, uniformly coercive, and continuous bilinear form. We apply the reduced basis method to this formulation, following the well-developed techniques for parameterized coercive problems, as seen e.g. in reduced basis methods for parameterized elliptic PDEs. We present an offline-online decomposition and provide certification with absolute and relative error bounds. The performance of the method is demonstrated using selected numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21736v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Hinze, Christian Kahle, Michael Stahl</dc:creator>
    </item>
    <item>
      <title>Solving Hamilton-Jacobi equations by minimizing residuals of monotone discretizations</title>
      <link>https://arxiv.org/abs/2601.21764</link>
      <description>arXiv:2601.21764v1 Announce Type: new 
Abstract: We derive sufficient conditions under which residual minimization yields well-posed discrete solutions for nonlinear equations defined by monotone finite--difference discretizations. Our analysis is motivated by the challenge of solving fully nonlinear Hamilton--Jacobi (HJ) equations in high dimensions by means of a Neural Network, which is trained by minimizing residuals arising from monotone discretizations of the Hamiltonian. While classical theory ensures that consistency and monotonicity imply convergence to the viscosity solution, treating these discrete systems as optimization problems introduces new analytical hurdles: solvability and the uniqueness of local minima do not follow from monotonicity alone.
  By establishing the well--posedness of these optimization--based solvers, our framework enables the adaptation of Level Set Methods to high--dimensional settings, unlocking new capabilities in applications such as high--dimensional segmentation and interface tracking. Finally, we observe that these arguments extend almost directly to degenerate elliptic or parabolic PDEs on graphs equipped with monotone graph Laplacians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21764v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Bokanowski, Carlos Esteve-Yag\"ue, Richard Tsai</dc:creator>
    </item>
    <item>
      <title>A novel Krylov subspace method for approximating Fr\'echet derivatives of large-scale matrix functions</title>
      <link>https://arxiv.org/abs/2601.21799</link>
      <description>arXiv:2601.21799v1 Announce Type: new 
Abstract: We present a novel Krylov subspace method for approximating $L_f(A, E) \vc{b}$, the matrix-vector product of the Fr\'echet derivative $L_f(A, E)$ of a large-scale matrix function $f(A)$ in direction $E$, a task that arises naturally in the sensitivity analysis of quantities involving matrix functions, such as centrality measures for networks. It also arises in the context of gradient-based methods for optimization problems that feature matrix functions, e.g., when fitting an evolution equation to an observed solution trajectory. In principle, the well-known identity \[
  f\left( \begin{bmatrix}
  A &amp; E \\ 0 &amp; A
  \end{bmatrix} \right) \begin{bmatrix}
  0 \\ \vc{b}
  \end{bmatrix} = \begin{bmatrix}
  L_f(A, E) \vc{b} \\ f(A) \vc{b}
  \end{bmatrix}, \] allows one to directly apply any standard Krylov subspace method, such as the Arnoldi algorithm, to address this task. However, this comes with the major disadvantage that the involved block triangular matrix has unfavorable spectral properties, which impede the convergence analysis and, to a certain extent, also the observed convergence. To avoid these difficulties, we propose a novel modification of the Arnoldi algorithm that aims at better preserving the block triangular structure. In turn, this allows one to bound the convergence of the modified method by the best polynomial approximation of the derivative $f^\prime$ on the numerical range of $A$. Several numerical experiments illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21799v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kressner, Peter Oehme</dc:creator>
    </item>
    <item>
      <title>Quotient geometry of tensor ring decomposition</title>
      <link>https://arxiv.org/abs/2601.21874</link>
      <description>arXiv:2601.21874v1 Announce Type: new 
Abstract: Differential geometries derived from tensor decompositions have been extensively studied and provided the foundations for a variety of efficient numerical methods. Despite the practical success of the tensor ring (TR) decomposition, its intrinsic geometry remains less understood, primarily due to the underlying ring structure and the resulting nontrivial gauge invariance. We establish the quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of the core tensors and capturing the gauge invariance. Additionally, the results can be extended to the uniform TR decomposition, where all core tensors are identical. Numerical experiments validate the developed geometries via tensor ring completion tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21874v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Gao, Renfeng Peng, Ya-xiang Yuan</dc:creator>
    </item>
    <item>
      <title>Towards regularized learning from functional data with covariate shift</title>
      <link>https://arxiv.org/abs/2601.21019</link>
      <description>arXiv:2601.21019v1 Announce Type: cross 
Abstract: This paper investigates a general regularization framework for unsupervised domain adaptation in vector-valued regression under the covariate shift assumption, utilizing vector-valued reproducing kernel Hilbert spaces (vRKHS). Covariate shift occurs when the input distributions of the training and test data differ, introducing significant challenges for reliable learning. By restricting the hypothesis space, we develop a practical operator learning algorithm capable of handling functional outputs. We establish optimal convergence rates for the proposed framework under a general source condition, providing a theoretical foundation for regularized learning in this setting. We also propose an aggregation-based approach that forms a linear combination of estimators corresponding to different regularization parameters and different kernels. The proposed approach addresses the challenge of selecting appropriate tuning parameters, which is crucial for constructing a good estimator, and we provide a theoretical justification for its effectiveness. Furthermore, we illustrate the proposed method on a real-world face image dataset, demonstrating robustness and effectiveness in mitigating distributional discrepancies under covariate shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21019v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Holzleitner, Sergiy Pereverzyev Jr., Sergei V. Pereverzyev, Vaibhav Silmana, S. Sivananthan</dc:creator>
    </item>
    <item>
      <title>Solving the Offline and Online Min-Max Problem of Non-smooth Submodular-Concave Functions: A Zeroth-Order Approach</title>
      <link>https://arxiv.org/abs/2601.21243</link>
      <description>arXiv:2601.21243v1 Announce Type: cross 
Abstract: We consider max-min and min-max problems with objective functions that are possibly non-smooth, submodular with respect to the minimiser and concave with respect to the maximiser. We investigate the performance of a zeroth-order method applied to this problem. The method is based on the subgradient of the Lov\'asz extension of the objective function with respect to the minimiser and based on Gaussian smoothing to estimate the smoothed function gradient with respect to the maximiser. In expectation sense, we prove the convergence of the algorithm to an $\epsilon$-saddle point in the offline case. Moreover, we show that, in the expectation sense, in the online setting, the algorithm achieves $O(\sqrt{N\bar{P}_N})$ online duality gap, where $N$ is the number of iterations and $\bar{P}_N$ is the path length of the sequence of optimal decisions. The complexity analysis and hyperparameter selection are presented for all the cases. The theoretical results are illustrated via numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21243v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Ali Farzin, Yuen-Man Pun, Philipp Braun, Tyler Summers, Iman Shames</dc:creator>
    </item>
    <item>
      <title>LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models</title>
      <link>https://arxiv.org/abs/2601.21623</link>
      <description>arXiv:2601.21623v1 Announce Type: cross 
Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21623v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav Budzinskiy, Marian Gloser, Tolunay Yilmaz, Ying Hong Tham, Yuanyi Lin, Wenyi Fang, Fan Wu, Philipp Petersen</dc:creator>
    </item>
    <item>
      <title>Rapid estimation of global sea surface temperatures from sparse streaming in situ observations</title>
      <link>https://arxiv.org/abs/2601.21913</link>
      <description>arXiv:2601.21913v1 Announce Type: cross 
Abstract: Reconstructing high-resolution sea surface temperatures (SST) from staggered SST measurements is essential for weather forecasting and climate projections. However, when SST measurements are sparse, the resulting inferred SST fields are rather inaccurate. Here, we demonstrate the ability of Sparse Discrete Empirical Interpolation Method (S-DEIM) to reconstruct the high-resolution SST field from sparse in situ observations, without using a model. The S-DEIM estimate consists of two terms, one computed from instantaneous in situ observations using empirical interpolation, and the other learned from the historical time series of observations using recurrent neural networks (RNNs). We train the RNNs using the National Oceanic and Atmospheric Administration's weekly high-resolution SST dataset spanning the years 1989-2021 which constitutes the training data. Subsequently, we examine the performance of S-DEIM on the test data, comprising January 2022 to January 2023. For this test data, S-DEIM infers the high-resolution SST from 100 in situ observations, constituting only 0.2% of the high-resolution spatial grid. We show that the resulting S-DEIM reconstructions are about 40% more accurate than earlier empirical interpolation methods, such as DEIM and Q-DEIM. Furthermore, 91% of S-DEIM estimates fall within $\pm 1^\circ$C of the true SST. We also demonstrate that S-DEIM is robust with respect to sensor placement: even when the sensors are distributed randomly, S-DEIM reconstruction error deteriorates only by 1-2%. S-DEIM is also computationally efficient. Training the RNN, which is performed only once offline, takes approximately one minute. Once trained, the S-DEIM reconstructions are computed in less than a second. As such, S-DEIM can be used for rapid SST reconstruction from sparse streaming observational data in real time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21913v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cassidy All, Kevin Ho, Maya Magnuski, Christopher Nicolaides, Louisa B. Ebby, Mohammad Farazmand</dc:creator>
    </item>
    <item>
      <title>On Approximate Computation of Critical Points</title>
      <link>https://arxiv.org/abs/2601.21917</link>
      <description>arXiv:2601.21917v1 Announce Type: cross 
Abstract: We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in $n$ variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most $2^n$ whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex optimization, approximate computation of critical points is a tractable task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21917v1</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Ali Ahmadi, Georgina Hall</dc:creator>
    </item>
    <item>
      <title>PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</title>
      <link>https://arxiv.org/abs/2601.22137</link>
      <description>arXiv:2601.22137v1 Announce Type: cross 
Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22137v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</dc:creator>
    </item>
    <item>
      <title>A Second-Order Nonlocal Approximation to Manifold Poisson Models with Neumann Boundary</title>
      <link>https://arxiv.org/abs/2403.05888</link>
      <description>arXiv:2403.05888v5 Announce Type: replace 
Abstract: In this paper, we propose a class of nonlocal models to approximate the Poisson model on manifolds with homogeneous Neumann boundary condition, where the manifolds are assumed to be embedded in high dimensional Euclid spaces.
  In comparison to the existing nonlocal approximation of Poisson models with Neumann boundary, we optimize the truncation error of model by adding an augmented function involving the second order normal derivative along the $2\delta$ layer of boundary, with $2\delta$ be the nonlocal interaction horizon. The 2nd normal derivative is expressed as the difference between the interior Laplacian and the boundary Laplacian.
  The concentration of our paper is on the construction of nonlocal model, the well-posedness of model, and its second-order convergence rate to its local counterpart. The localization rate of our nonlocal model is currently optimal among all related works even for the case of high dimensional Euclid spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05888v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajie Zhang, Yanzun Meng, Zuoqiang Shi</dc:creator>
    </item>
    <item>
      <title>The jump filter in the discontinuous Galerkin method for hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2407.19169</link>
      <description>arXiv:2407.19169v2 Announce Type: replace 
Abstract: When simulating hyperbolic conservation laws with discontinuous solutions, high-order linear numerical schemes often produce undesirable spurious oscillations. In this paper, we propose a jump filter within the discontinuous Galerkin (DG) method to mitigate these oscillations. This filter operates locally based on jump information at cell interfaces, targeting high-order polynomial modes within each cell. Besides its localized nature, our proposed filter preserves key attributes of the DG method, including conservation, $L^2$ stability, and high-order accuracy. We also explore its compatibility with other damping techniques, and demonstrate its seamless integration into a hybrid limiter. In scenarios featuring strong shock waves, this hybrid approach, incorporating this jump filter as the low-order limiter, effectively suppresses numerical oscillations while exhibiting low numerical dissipation. Additionally, the proposed jump filter maintains the compactness of the DG scheme, which greatly aids in efficient parallel computing. Moreover, it boasts an impressively low computational cost, given that no characteristic decomposition is required and all computations are confined to physical space. Numerical experiments validate the effectiveness and performance of our proposed scheme, confirming its accuracy and shock-capturing capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19169v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Wei, Lingling Zhou, Yinhua Xia</dc:creator>
    </item>
    <item>
      <title>On the data-sparsity of the solution of Riccati equations with applications to feedback control</title>
      <link>https://arxiv.org/abs/2408.16569</link>
      <description>arXiv:2408.16569v2 Announce Type: replace 
Abstract: Solving large-scale continuous-time algebraic Riccati equations is a significant challenge in various control theory applications. This work demonstrates that when the matrix coefficients of the equation are quasiseparable, the solution also exhibits numerical quasiseparability. This property enables us to develop two efficient Riccati solvers. The first solver is applicable to the general quasiseparable case, while the second is tailored to the particular case of banded coefficients. Numerical experiments confirm the effectiveness of the proposed algorithms on both synthetic examples and case studies from the control of partial differential equations and agent-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16569v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Massei, Luca Saluzzi</dc:creator>
    </item>
    <item>
      <title>A Posteriori Error Estimation Improved by a Reconstruction Operator for the Stokes Optimal Control Problem</title>
      <link>https://arxiv.org/abs/2502.16482</link>
      <description>arXiv:2502.16482v2 Announce Type: replace 
Abstract: This paper focuses on a posteriori error estimates for a pressure-robust finite element method, which incorporates a divergence-free reconstruction operator, within the context of the distributed optimal control problem constrained by the Stokes equations. We develop an enhanced residual-based a posteriori error estimator that is independent of pressure and establish its global reliability and efficiency. The proposed a posteriori error estimator enables the separation of velocity and pressure errors in a posteriori error estimation, ensuring velocity-related estimates are free of pressure influence. Numerical experiments confirm our conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16482v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingshi Li, Jiachuan Zhang</dc:creator>
    </item>
    <item>
      <title>On the Compressibility of Integral Operators in Anisotropic Wavelet Coordinates</title>
      <link>https://arxiv.org/abs/2504.06938</link>
      <description>arXiv:2504.06938v2 Announce Type: replace 
Abstract: The present article is concerned with the s*-compressibility of classical boundary integral operators in anisotropic wavelet coordinates. Having the s*-compressibility at hand, one can design adaptive wavelet algorithms which are asymptotically optimal, meaning that any target accuracy can be achieved at a computational expense that stays proportional to the number of degrees of freedom (within the setting determined by an underlying wavelet basis) that would ideally be necessary for realising that target accuracy if full knowledge about the unknown solution were given. As we consider anisotropic wavelet bases, we can achieve higher convergence rates compared to the standard, isotropic setting. Especially, edge singularities of anisotropic nature can be resolved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06938v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helmut Harbrecht, Remo von Rickenbach</dc:creator>
    </item>
    <item>
      <title>On Symmetric Lanczos Quadrature for Stochastic Trace Estimation</title>
      <link>https://arxiv.org/abs/2504.18913</link>
      <description>arXiv:2504.18913v2 Announce Type: replace 
Abstract: A common approach to approximating quadratic forms of matrix functions is to use a quadrature rule derived from the Lanczos process, known as a Lanczos quadrature. Although symmetric quadrature rules are computationally favorable, it has remained unclear whether a symmetric Lanczos quadrature is practically feasible. In this work, we resolve this ambiguity by establishing necessary and sufficient conditions for the existence of symmetric Lanczos quadratures. We show that the sufficient condition can be met for a class of Jordan-Wielandt matrices by carefully constructing initial vectors with specific distributions for the Lanczos algorithm. Applying such a symmetric Lanczos quadrature to compute the Estrada index of bipartite or directed graphs ensures that the resulting stochastic trace estimators are unbiased. Furthermore, we observe that the variance of the quadratic form estimator based on the symmetric Lanczos quadrature is lower than that of the standard estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18913v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Li, Shengxin Zhu</dc:creator>
    </item>
    <item>
      <title>High-Order Hermite Optimization: Fast and Exact Gradient Computation in Open-Loop Quantum Optimal Control using a Discrete Adjoint Approach</title>
      <link>https://arxiv.org/abs/2505.09857</link>
      <description>arXiv:2505.09857v4 Announce Type: replace 
Abstract: This work introduces the High-Order Hermite Optimization (HOHO) method, an open-loop discrete adjoint method for quantum optimal control. Our method is the first of its kind to efficiently compute exact (discrete) gradients when using continuous, parameterized control pulses while solving the forward equations (e.g. Schrodinger's equation or the Linblad master equation) with an arbitrarily high-order Hermite Runge-Kutta method. The HOHO method is implemented in QuantumGateDesign$.$jl (https://github.com/leespen1/QuantumGateDesign.jl), an open-source software package for the Julia programming language, which we use to perform numerical experiments comparing the method to Juqbox$.$jl (https://github.com/LLNL/Juqbox.jl). For realistic model problems we observe speedups up to 775x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09857v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>quant-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2026.114697</arxiv:DOI>
      <dc:creator>Spencer Lee, Daniel Appelo</dc:creator>
    </item>
    <item>
      <title>Projected Sobolev Natural Gradient Descent for Efficient Neural Network Solution of the Gross-Pitaevskii Equation</title>
      <link>https://arxiv.org/abs/2512.11339</link>
      <description>arXiv:2512.11339v3 Announce Type: replace 
Abstract: This paper introduces a projected Sobolev natural gradient descent (NGD) method for computing ground states of the Gross-Pitaevskii equation. By projecting a continuous Riemannian Sobolev gradient flow onto the normalized neural network tangent space, we derive a discrete NGD algorithm that preserves the normalization constraint. The numerical implementation employs variational Monte Carlo with a hybrid sampling strategy to accurately account for the normalization constant arising from nonlinear interaction terms. To enhance computational efficiency, a matrix-free Nystr\"om-preconditioned conjugate gradient solver is adopted to approximate the NGD operator without explicit matrix assembly. Numerical experiments demonstrate that the proposed method converges significantly faster than physics-informed neural network approaches and exhibits linear scalability with respect to spatial dimensions. Moreover, the resulting neural-network solutions provide high-quality initial guesses that substantially accelerate subsequent refinement by traditional high-precision solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11339v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenglong Bao, Chen Cui, Kai Jiang, Shi Shu</dc:creator>
    </item>
    <item>
      <title>Applying the Spectral Method for Modeling Linear Filters: Butterworth, Linkwitz-Riley, and Chebyshev filters</title>
      <link>https://arxiv.org/abs/2508.07206</link>
      <description>arXiv:2508.07206v2 Announce Type: replace-cross 
Abstract: This paper proposes a new technique for computer modeling linear filters based on the spectral form of mathematical description of linear systems. It assumes the representation of input and output signals of the filter as orthogonal expansions, while filters themselves are described by two-dimensional non-stationary transfer functions. This technique allows one to model the output signal in continuous time, and it is successfully tested on the Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07206v2</guid>
      <category>eess.SP</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fraope.2026.100508</arxiv:DOI>
      <arxiv:journal_reference>Franklin Open 2026, 14, 100508</arxiv:journal_reference>
      <dc:creator>Konstantin A. Rybakov, Egor D. Shermatov</dc:creator>
    </item>
    <item>
      <title>Comparison of Extended Lubrication Theories for Stokes Flow</title>
      <link>https://arxiv.org/abs/2510.02595</link>
      <description>arXiv:2510.02595v2 Announce Type: replace-cross 
Abstract: Lubrication theory makes use of the assumptions of a long and thin fluid domain and a small scaled Reynolds number to formulate a linearized approximation to the Navier-Stokes equations. Extended lubrication theory aims to improve the model accuracy by relaxing these assumptions and including additional terms in the formulation. However, such models are sensitive to large surface gradients which lead the assumptions of the model to break down. In this paper, we present a formulation of extended lubrication theory, and compare our model with several existing models, along with the numerical solution to the Stokes equations. The error in pressure and velocity is characterized for a variety of fluid domain geometries. Our results indicate that the new solution is suitable for a wide range of geometries. The magnitude of surface variation and the length scale ratio are both important factors influencing the accuracy of the extended lubrication theory models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02595v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Dennis, Thomas G. Fai</dc:creator>
    </item>
    <item>
      <title>Moving Least Squares without Quasi-Uniformity: A Stochastic Approach</title>
      <link>https://arxiv.org/abs/2601.13782</link>
      <description>arXiv:2601.13782v3 Announce Type: replace-cross 
Abstract: Local Polynomial Regression (LPR) and Moving Least Squares (MLS) are closely related nonparametric estimation methods, developed independently in statistics and approximation theory. While statistical LPR analysis focuses on overcoming sampling noise under probabilistic assumptions, the deterministic MLS theory studies smoothness properties and convergence rates with respect to the \textit{fill-distance} (a resolution parameter). Despite this similarity, the deterministic assumptions underlying MLS fail to hold under random sampling. We begin by quantifying the probabilistic behavior of the fill-distance $h_n$ and \textit{separation} $\delta_n$ of an i.i.d. random sample. That is, for a distribution satisfying a mild regularity condition, $h_n\propto n^{-1/d}\log^{1/d} (n)$ and $\delta_n \propto n^{-2/d}$. We then prove that, for MLS of degree $k\!-\!1$, the approximation error associated with a differential operator $Q$ of order $|m|\le k-1$ decays as $h_n^{\,k-|m|}$ up to logarithmic factors, establishing stochastic analogues of the classical MLS estimates. Additionally, We show that the MLS approximant is smooth with high probability. Finally, we apply the stochastic MLS theory to manifold estimation. Assuming that the sampled Manifold is $k$-times smooth, we show that the Hausdorff distance between the true manifold and its MLS reconstruction decays as $h_n^k$, extending the deterministic Manifold-MLS guarantees to random samples. This work provides the first unified stochastic analysis of MLS, demonstrating that -- despite the failure of deterministic sampling assumptions -- the classical convergence and smoothness properties persist under natural probabilistic models</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13782v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shir Tapiro-Moshe, Yariv Aizenbud, Barak Sober</dc:creator>
    </item>
  </channel>
</rss>
