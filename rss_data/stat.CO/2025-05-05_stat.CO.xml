<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>BreakLoops: A New Feature for the Multi-Gene, Multi-Cancer Family History-Based Model, Fam3Pro</title>
      <link>https://arxiv.org/abs/2505.01466</link>
      <description>arXiv:2505.01466v1 Announce Type: new 
Abstract: Previously, we presented PanelPRO, now known as Fam3PRO, an open-source R package for multi-gene, multi-cancer risk modeling with pedigree data. The initial release could not handle pedigrees that contained cyclic structures called loops, which occur when relatives mate. Here, we present a graph-based function called breakloops that can detect and break loops in any pedigree. The core algorithm identifies the optimal set of loop breakers when individuals in a loop have exactly one parental mating, and extends to handle cases where individuals have multiple parental matings. The algorithm transforms complex pedigrees by strategically creating clones of key individuals to disrupt cycles while minimizing computational complexity. Our extensive testing demonstrates that this new feature can handle a wide variety of pedigree structures. The breakloops function is available in Fam3Pro version 2.0.0. This advancement enables Fam3Pro to assess cancer risk in a wider range of family structures, enhancing its applicability in clinical settings</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01466v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Kubista, Ryan Hernandez-Cancela, Jianfeng Ke, Romain Berquet, Gavin Lee, Giovanni Parmigiani, Danielle Braun</dc:creator>
    </item>
    <item>
      <title>sae4health: An R Shiny Application for Small Area Estimation in Low- and Middle-Income Countries</title>
      <link>https://arxiv.org/abs/2505.01467</link>
      <description>arXiv:2505.01467v1 Announce Type: new 
Abstract: Accurate subnational estimation of health indicators is critical for public health planning, especially in low- and middle-income countries (LMICs), where data and tools are often limited. The sae4health R shiny app, built on the surveyPrev package, provides a user-friendly tool for prevalence mapping using small area estimation (SAE) methods. Both area- and unit-level models with spatial random effects are available, with fast Bayesian inference performed using Integrated Nested Laplace Approximation (INLA). Currently, the app supports analysis of over 150 indicators from Demographic and Health Surveys (DHS) across multiple administrative levels. sae4health simplifies the use of complex prevalence mapping models to support data-driven decision-making. The app provides interactive visualization, summary, and report generation functionalities for a wide range of use cases. This paper outlines the app's statistical framework and demonstrates the workflow through a case study of child stunting in Nigeria. Additional documentation is available on the supporting website (https://sae4health.stat.uw.edu).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01467v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunhan Wu, Qianyu Dong, Jieyi Xu, Zehang Richard Li, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Accelerating Posterior sampling for Scalable Gaussian Process model</title>
      <link>https://arxiv.org/abs/2505.02000</link>
      <description>arXiv:2505.02000v1 Announce Type: new 
Abstract: This Paper conducts a thorough simulation study to assess the effectiveness of various acceleration techniques designed to enhance the conjugate gradient algorithm, which is used for solving large linear systems to accelerate Bayesian computation in spatial analysis. The focus is on the application of symbolic decomposition and preconditioners, which are essential for the computational efficiency of conjugate gradient. The findings reveal notable differences in the effectiveness of these acceleration methods. Specific preconditioners, such as the Diagonal Preconditioner, consistently delivered improvements in computational speed. However, in settings involving high-dimensional matrices, traditional solvers were less effective, underscoring the importance of specialized acceleration techniques like the diagonal preconditioner and cgsparse. These methods demonstrated robust performance across a variety of scenarios. The results of this study not only enhance our understanding of the algorithmic dynamics within spatial statistics but also offer valuable guidance for practitioners in choosing the most appropriate computational techniques for their specific needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02000v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Zhou</dc:creator>
    </item>
    <item>
      <title>Fast Likelihood-Free Parameter Estimation for L\'evy Processes</title>
      <link>https://arxiv.org/abs/2505.01639</link>
      <description>arXiv:2505.01639v1 Announce Type: cross 
Abstract: L\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. Through extensive simulations across several L\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling rapid bootstrap-based uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01639v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Coloma, William Kleiber</dc:creator>
    </item>
    <item>
      <title>Applications of higher order Markov models and Pressure Index to strategize controlled run chases in Twenty20 cricket</title>
      <link>https://arxiv.org/abs/2505.01849</link>
      <description>arXiv:2505.01849v1 Announce Type: cross 
Abstract: In limited overs cricket, the team batting first posts a target score for the team batting second to achieve in order to win the match. The team batting second is constrained by decreasing resources in terms of number of balls left and number of wickets in hand in the process of reaching the target as the second innings progresses. The Pressure Index, a measure created by researchers in the past, serves as a tool for quantifying the level of pressure that a team batting second encounters in limited overs cricket. Through a ball-by-ball analysis of the second innings, it reveals how effectively the team batting second in a limited-over game proceeds towards their target. This research employs higher order Markov chains to examine the strategies employed by successful teams during run chases in Twenty20 matches. By studying the trends in successful run chases spanning over 16 years and utilizing a significant dataset of 3378 Twenty20 matches, specific strategies are identified. Consequently, an efficient approach to successful run chases in Twenty20 cricket is formulated, effectively limiting the Pressure Index to [0.5, 3.5] or even further down under 0.5 as early as possible. The innovative methodology adopted in this research offers valuable insights for cricket teams looking to enhance their performance in run chases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01849v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rhitankar Bandyopadhyay, Dibyojyoti Bhattacharjee</dc:creator>
    </item>
    <item>
      <title>Bayesian learning of the optimal action-value function in a Markov decision process</title>
      <link>https://arxiv.org/abs/2505.01859</link>
      <description>arXiv:2505.01859v1 Announce Type: cross 
Abstract: The Markov Decision Process (MDP) is a popular framework for sequential decision-making problems, and uncertainty quantification is an essential component of it to learn optimal decision-making strategies. In particular, a Bayesian framework is used to maintain beliefs about the optimal decisions and the unknown ingredients of the model, which are also to be learned from the data, such as the rewards and state dynamics. However, many existing Bayesian approaches for learning the optimal decision-making strategy are based on unrealistic modelling assumptions and utilise approximate inference techniques. This raises doubts whether the benefits of Bayesian uncertainty quantification are fully realised or can be relied upon.
  We focus on infinite-horizon and undiscounted MDPs, with finite state and action spaces, and a terminal state. We provide a full Bayesian framework, from modelling to inference to decision-making. For modelling, we introduce a likelihood function with minimal assumptions for learning the optimal action-value function based on Bellman's optimality equations, analyse its properties, and clarify connections to existing works. For deterministic rewards, the likelihood is degenerate and we introduce artificial observation noise to relax it, in a controlled manner, to facilitate more efficient Monte Carlo-based inference. For inference, we propose an adaptive sequential Monte Carlo algorithm to both sample from and adjust the sequence of relaxed posterior distributions. For decision-making, we choose actions using samples from the posterior distribution over the optimal strategies. While commonly done, we provide new insight that clearly shows that it is a generalisation of Thompson sampling from multi-arm bandit problems. Finally, we evaluate our framework on the Deep Sea benchmark problem and demonstrate the exploration benefits of posterior sampling in MDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01859v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Guo, Chon Wai Ho, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2505.01995</link>
      <description>arXiv:2505.01995v1 Announce Type: cross 
Abstract: Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size $n$ at a rate of $O(n^{\zeta})$ for some $0 \leq \zeta&lt;1$, while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $0\leq \zeta &lt; \frac{1}{2}$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01995v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sehwan Kim, Faming Liang</dc:creator>
    </item>
    <item>
      <title>On the Equivalence of Gaussian Graphical Models Defined on Complete Bipartite Graphs</title>
      <link>https://arxiv.org/abs/2505.02384</link>
      <description>arXiv:2505.02384v1 Announce Type: cross 
Abstract: This paper introduces two Gaussian graphical models defined on complete bipartite graphs. We show that the determinants of the precision matrices associated with the models are equal up to scale, where the scale factor only depends on model parameters. In this context, we will introduce a notion of ``equivalence" between the two Gaussian graphical models. This equivalence has two key applications: first, it can significantly reduce the complexity of computing the exact value of the determinant, and second, it enables the derivation of closed-form expressions for the determinants in certain special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02384v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehdi Molkaraie</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Learning with Multilevel Trace-class Neural Networks</title>
      <link>https://arxiv.org/abs/2203.12961</link>
      <description>arXiv:2203.12961v5 Announce Type: replace 
Abstract: In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors which can be preferable to traditional DNNs as (a) they are identifiable and (b) they possess desirable convergence properties. TNN priors are defined on functions with infinitely many hidden units, and have strongly convergent Karhunen-Loeve-type approximations with finitely many hidden units. A practical hurdle is that the Bayesian solution is computationally demanding, requiring simulation methods, so approaches to drive down the complexity are needed. In this paper, we leverage the strong convergence of TNN in order to apply Multilevel Monte Carlo (MLMC) to these models. In particular, an MLMC method that was introduced is used to approximate posterior expectations of Bayesian TNN models with optimal computational complexity, and this is mathematically proved. The results are verified with several numerical experiments on model problems arising in machine learning, including regression, classification, and reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.12961v5</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Ajay Jasra, Kody J. H. Law, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>Towards Adaptive Self-Normalized Importance Samplers</title>
      <link>https://arxiv.org/abs/2505.00372</link>
      <description>arXiv:2505.00372v2 Announce Type: replace 
Abstract: The self-normalized importance sampling (SNIS) estimator is a Monte Carlo estimator widely used to approximate expectations in statistical signal processing and machine learning.
  The efficiency of SNIS depends on the choice of proposal, but selecting a good proposal is typically unfeasible. In particular, most of the existing adaptive IS (AIS) literature overlooks the optimal SNIS proposal.
  In this paper, we introduce an AIS framework that uses MCMC to approximate the optimal SNIS proposal within an iterative scheme. This is, to the best of our knowledge, the first AIS framework targeting specifically the SNIS optimal proposal. We find a close connection with adaptive schemes used in ratio importance sampling (RIS), which also brings a new perspective and paves the way for combining techniques from AIS and adaptive RIS. We outline possible extensions, connections with existing MCMC-driven AIS algorithms, theoretical directions, and demonstrate performance in numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00372v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Branchini, V\'ictor Elvira</dc:creator>
    </item>
    <item>
      <title>Fluctuation without dissipation: Microcanonical Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2303.18221</link>
      <description>arXiv:2303.18221v3 Announce Type: replace-cross 
Abstract: Stochastic sampling algorithms such as Langevin Monte Carlo are inspired by physical systems in a heat bath. Their equilibrium distribution is the canonical ensemble given by a prescribed target distribution, so they must balance fluctuation and dissipation as dictated by the fluctuation-dissipation theorem. In contrast to the common belief, we show that the fluctuation-dissipation theorem is not required because only the configuration space distribution, and not the full phase space distribution, needs to be canonical. We propose a continuous-time Microcanonical Langevin Monte Carlo (MCLMC) as a dissipation-free system of stochastic differential equations (SDE). We derive the corresponding Fokker-Planck equation and show that the stationary distribution is the microcanonical ensemble with the desired canonical distribution on configuration space. We prove that MCLMC is ergodic for any nonzero amount of stochasticity, and for smooth, convex potentials, the expectation values converge exponentially fast. Furthermore, the deterministic drift and the stochastic diffusion separately preserve the stationary distribution. This uncommon property is attractive for practical implementations as it implies that the drift-diffusion discretization schemes are bias-free, so the only source of bias is the discretization of the deterministic dynamics. We applied MCLMC on a lattice $\phi^4$ model, where Hamiltonian Monte Carlo (HMC) is currently the state-of-the-art integrator. For the same accuracy, MCLMC converges 12 times faster than HMC on an $8\times8$ lattice. On a $64\times64$ lattice, it is already 32 times faster. The trend is expected to persist to larger lattices, which are of particular interest, for example, in lattice quantum chromodynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.18221v3</guid>
      <category>hep-lat</category>
      <category>cond-mat.stat-mech</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Published in Proceedings of Machine Learning Research (PMLR), 253 (2024) 111--126</arxiv:journal_reference>
      <dc:creator>Jakob Robnik, Uro\v{s} Seljak</dc:creator>
    </item>
    <item>
      <title>Quantile Least Squares: A Flexible Approach for Robust Estimation and Validation of Location-Scale Families</title>
      <link>https://arxiv.org/abs/2402.07837</link>
      <description>arXiv:2402.07837v2 Announce Type: replace-cross 
Abstract: In this paper, the problem of robust estimation and validation of location-scale families is revisited. The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters. These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant. The influence functions of the QLS estimators are specified and plotted for several location-scale families. They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified). The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations. Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9. For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data. In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07837v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Adjieteh, Vytaras Brazauskas</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v4 Announce Type: replace-cross 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance (BTBA): Examining Its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v3 Announce Type: replace-cross 
Abstract: We introduce Bhirkuti's Test of Bias Acceptance (BTBA), a standardized framework for evaluating estimator bias in Monte Carlo simulation studies. BTBA uses a simulation-specific standardized score (Z*) and a decision matrix to assess bias acceptability based on the mean and variance of Z* distributions. Under ideal conditions, Z* values should approximate a standard normal distribution (Z-distribution) with a mean near zero and variance near one in the context of simulation research. Systematic deviations from these patterns such as shifted means or inflated variances indicate bias or estimator instability in simulation-based research. BTBA visualizes these patterns using ridgeline density plots, which reveal distributional features such as central tendency, spread, skewness, and outliers. Demonstrated in a latent growth modeling context, BTBA offers a reproducible and interpretable method for diagnosing bias across varying simulation conditions. By addressing key limitations of traditional relative bias (RB) metrics, BTBA provides a theoretically grounded, distribution-aware, transparent, and replicable alternative for evaluating estimator quality, particularly in psychometric modeling, structural equation modeling, and missing data research. Through this framework, we aim to enhance methodological decision-making by integrating statistical reasoning with comprehensive visualization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
  </channel>
</rss>
