<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Feb 2026 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access</title>
      <link>https://arxiv.org/abs/2602.11357</link>
      <description>arXiv:2602.11357v1 Announce Type: new 
Abstract: Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11357v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoling Yi, Ryan Antonio, Yunhao Deng, Fanchen Kong, Joren Dumoulin, Jun Yin, Marian Verhelst</dc:creator>
    </item>
    <item>
      <title>EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement &amp; Routing for RF Circuits</title>
      <link>https://arxiv.org/abs/2602.11461</link>
      <description>arXiv:2602.11461v1 Announce Type: new 
Abstract: This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometries with frequency sweeps from 1-100 GHz, generating 7.5 million training samples, that predicts inductor Q-factor with less than 2% error and enables fast gradient-based layout optimization with a 93.77% success rate in producing high-Q layouts; (2) an intelligent P-Cell optimizer that reduces layout area while maintaining design-rule-check (DRC) compliance; and (3) a complete placement and routing engine with frequency-dependent EM spacing rules and DRC-aware synthesis. The neural inductor model demonstrates superior accuracy across 1-100 GHz, enabling EM-accurate component synthesis with real-time inference. The framework successfully generates DRC-aware GDSII layouts for RF circuits, representing a significant step toward automated RF physical design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11461v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilun Huang, Asal Mehradfar, Salman Avestimehr, Hamidreza Aghasi</dc:creator>
    </item>
    <item>
      <title>PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System</title>
      <link>https://arxiv.org/abs/2602.11521</link>
      <description>arXiv:2602.11521v1 Announce Type: new 
Abstract: The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.
  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11521v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lian Liu, Shixin Zhao, Yutian Zhou, Yintao He, Mengdi Wang, Yinhe Han, Ying Wang</dc:creator>
    </item>
    <item>
      <title>Benchmarking for Single Feature Attribution with Microarchitecture Cliffs</title>
      <link>https://arxiv.org/abs/2602.11580</link>
      <description>arXiv:2602.11580v1 Announce Type: new 
Abstract: Architectural simulators play a critical role in early microarchitectural exploration due to their flexibility and high productivity. However, their effectiveness is often constrained by fidelity: simulators may deviate from the behavior of the final RTL, leading to unreliable performance estimates. Consequently, model calibration, which aligns simulator behavior with the RTL as the ground-truth microarchitecture, becomes essential for achieving accurate performance modeling.
  To facilitate model calibration accuracy, we propose Microarchitecture Cliffs, a benchmark generation methodology designed to expose mismatches in microarchitectural behavior between the simulator and RTL. After identifying the key architectural components that require calibration, the Cliff methodology enables precise attribution of microarchitectural differences to a single microarchitectural feature through a set of benchmarks. In addition, we develop a set of automated tools to improve the efficiency of the Cliff workflow.
  We apply the Cliff methodology to calibrate the XiangShan version of gem5 (XS-GEM5) against the XiangShan open-source CPU (XS-RTL). We reduce the performance error of XS-GEM5 from 59.2% to just 1.4% on the Cliff benchmarks. Meanwhile, the calibration guided by Cliffs effectively reduces the relative error of a representative tightly coupled microarchitectural feature by 48.03%. It also substantially lowers the absolute performance error, with reductions of 15.1% and 21.0% on SPECint2017 and SPECfp2017, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11580v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhen, Qingxuan Kang, Yungang Bao, Trevor E. Carlson</dc:creator>
    </item>
    <item>
      <title>Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories</title>
      <link>https://arxiv.org/abs/2602.11614</link>
      <description>arXiv:2602.11614v1 Announce Type: new 
Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11614v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yousuf Choudhary, Tosiron Adegbija</dc:creator>
    </item>
    <item>
      <title>MING: An Automated CNN-to-Edge MLIR HLS framework</title>
      <link>https://arxiv.org/abs/2602.11966</link>
      <description>arXiv:2602.11966v1 Announce Type: new 
Abstract: Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11966v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahong Bi, Lars Sch\"utze, Jeronimo Castrillon</dc:creator>
    </item>
    <item>
      <title>Lightweight Error-Correction Code Encoders in Superconducting Electronic Systems</title>
      <link>https://arxiv.org/abs/2509.00962</link>
      <description>arXiv:2509.00962v1 Announce Type: cross 
Abstract: Data transmission from superconducting electronic circuits, such as single flux quantum (SFQ) logic, to room-temperature electronics is susceptible to bit errors, which may result from flux trapping, fabrication defects, and process parameter variations (PPV). Due to the cooling power budget at 4.2 K and constraints on the chip area, the size of the error-correction code encoders is limited. In this work, three lightweight error-correction code encoders are proposed that are based on Hamming(7,4), Hamming(8,4), and Reed-Muller(1,3) codes and implemented with SFQ logic. The performance of these encoders is analyzed in the presence of PPV. The trade-offs between the theoretical complexity and physical size of error-correction code encoders are identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00962v1</guid>
      <category>eess.SP</category>
      <category>cond-mat.supr-con</category>
      <category>cs.AR</category>
      <category>quant-ph</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SOCC66126.2025.11235429</arxiv:DOI>
      <dc:creator>Yerzhan Mustafa, Berker Pek\"oz, Sel\c{c}uk K\"ose</dc:creator>
    </item>
    <item>
      <title>HiFloat4 Format for Language Model Inference</title>
      <link>https://arxiv.org/abs/2602.11287</link>
      <description>arXiv:2602.11287v1 Announce Type: cross 
Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11287v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanyong Luo, Jing Huang, Yu Cheng, Ziwei Yu, Kaihua Zhang, Kehong Hong, Xinda Ma, Xin Wang, Anping Tong, Guipeng Hu, Yun Xu, Mehran Taghian, Peng Wu, Guanglin Li, Yunke Peng, Tianchi Hu, Minqi Chen, Michael Bi Mi, Hu Liu, Xiping Zhou, Junsong Wang, Qiang Lin, Heng Liao</dc:creator>
    </item>
    <item>
      <title>Metastable Dynamical Computing with Energy Landscapes: A Primer</title>
      <link>https://arxiv.org/abs/2602.11390</link>
      <description>arXiv:2602.11390v1 Announce Type: cross 
Abstract: Smartphones, laptops, and data centers are CMOS-based technologies that ushered our world into the information age of the 21st century. Despite their advantages for scalable computing, their implementations come with surprisingly large energetic costs. This challenge has revitalized scientific and engineering interest in energy-efficient information-processing designs. One current paradigm -- dynamical computing -- controls the location and shape of minima in potential energy landscapes that are connected to a thermal environment. The landscape supports distinguishable metastable energy minima that serve as a system's mesoscopic memory states. Information is represented by microstate distributions. Dynamically manipulating the memory states then corresponds to information processing. This framing provides a natural description of the associated thermodynamic transformations and required resources. Appealing to bifurcation theory, a computational protocol in the metastable regime can be analyzed by tracking the evolution of fixed points in the state space. We illustrate the paradigm's capabilities by performing 1-bit and 2-bit computations with double-well and quadruple-well potentials, respectively. These illustrate how dynamical computing can serve as a basis for designing universal logic gates and investigating their out-of-equilibrium thermodynamic performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11390v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.supr-con</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Z. Pratt, Kyle J. Ray, James P. Crutchfield</dc:creator>
    </item>
    <item>
      <title>RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis</title>
      <link>https://arxiv.org/abs/2602.11506</link>
      <description>arXiv:2602.11506v1 Announce Type: cross 
Abstract: The transition toward localized intelligence through Small Language Models (SLMs) has intensified the need for rigorous performance characterization on resource-constrained edge hardware. However, objectively measuring the theoretical performance ceilings of diverse architectures across heterogeneous platforms remains a formidable challenge. In this work, we propose a systematic framework based on the Roofline model that unifies architectural primitives and hardware constraints through the lens of operational intensity (OI). By defining an inference-potential region, we introduce the Relative Inference Potential as a novel metric to compare efficiency differences between Large Language Models (LLMs) on the same hardware substrate. Extensive empirical analysis across diverse compute tiers reveals that variations in performance and OI are significantly influenced by sequence length. We further identify a critical regression in OI as model depth increases. Additionally, our findings highlight an efficiency trap induced by hardware heterogeneity and demonstrate how structural refinements, such as Multi-head Latent Attention (M LA), can effectively unlock latent inference potential across various hardware substrates. These insights provide actionable directions for hardware-software co-design to align neural structures with physical constraints in on-device intelligence. The released code is available in the Appendix C.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11506v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Bi, Xueshu Chen, Luoyang Sun, Yuhang Yao, Qing Shen, Jungang Lou, Cheng Deng</dc:creator>
    </item>
    <item>
      <title>PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models</title>
      <link>https://arxiv.org/abs/2602.11530</link>
      <description>arXiv:2602.11530v1 Announce Type: cross 
Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11530v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eunyeong Cho, Jehyeon Bang, Ranggi Hwang, Minsoo Rhu</dc:creator>
    </item>
    <item>
      <title>Secure Scattered Memory: Rethinking Secure Enclave Memory with Secret Sharing</title>
      <link>https://arxiv.org/abs/2402.15824</link>
      <description>arXiv:2402.15824v2 Announce Type: replace-cross 
Abstract: The rise of cloud computing demands secure memory systems that ensure data confidentiality, integrity, and freshness against replay attacks. Existing schemes such as AES-XTS, AES-GCM, and AES-CTR each trade performance for security, with only AES-CTR plus Message Authentication Codes (MAC) and Merkle Trees (MT) providing full protection - at the cost of substantial counter and MT overhead. This paper introduces Secure Scattered Memory (SSM), a novel scheme that replaces counter-based encryption with polynomial-based secret sharing. Each data block is encoded into multiple cryptographically independent shares distributed across memory, inherently preventing information leakage while ensuring integrity and freshness through mathematical reconstruction properties. Implemented and synthesized in a 28 nm commercial PDK, SSM occupies 0.27 mm^2 and consumes 284.53 mW. Experiments show only 10% and 8% performance overhead over AES-XTS and AES-GCM, respectively, while outperforming Morphable Counter (MICRO 2018) by up to 40%, achieving 12% better performance than EMCC/RMCC (MICRO 2022), and exceeding COSMOS (MICRO 2025) by 3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15824v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Geng, Yuezhi Che, Dazhao Chen, Michael Niemier, Xiaobo Sharon Hu</dc:creator>
    </item>
  </channel>
</rss>
