<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 01:40:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Supervised brain node and network construction under voxel-level functional imaging</title>
      <link>https://arxiv.org/abs/2407.21242</link>
      <description>arXiv:2407.21242v1 Announce Type: new 
Abstract: Recent advancements in understanding the brain's functional organization related to behavior have been pivotal, particularly in the development of predictive models based on brain connectivity. Traditional methods in this domain often involve a two-step process by first constructing a connectivity matrix from predefined brain regions, and then linking these connections to behaviors or clinical outcomes. However, these approaches with unsupervised node partitions predict outcomes inefficiently with independently established connectivity. In this paper, we introduce the Supervised Brain Parcellation (SBP), a brain node parcellation scheme informed by the downstream predictive task. With voxel-level functional time courses generated under resting-state or cognitive tasks as input, our approach clusters voxels into nodes in a manner that maximizes the correlation between inter-node connections and the behavioral outcome, while also accommodating intra-node homogeneity. We rigorously evaluate the SBP approach using resting-state and task-based fMRI data from both the Adolescent Brain Cognitive Development (ABCD) study and the Human Connectome Project (HCP). Our analyses show that SBP significantly improves out-of-sample connectome-based predictive performance compared to conventional step-wise methods under various brain atlases. This advancement holds promise for enhancing our understanding of brain functional architectures with behavior and establishing more informative network neuromarkers for clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21242v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanwan Xu, Selena Wang, Chichun Tan, Xilin Shen, Wenjing Luo, Todd Constable, Tianxi Li, Yize Zhao</dc:creator>
    </item>
    <item>
      <title>Can expected error costs justify testing a hypothesis at multiple alpha levels rather than searching for an elusive optimal alpha?</title>
      <link>https://arxiv.org/abs/2407.21303</link>
      <description>arXiv:2407.21303v1 Announce Type: new 
Abstract: Simultaneous testing of one hypothesis at multiple alpha levels can be performed within a conventional Neyman-Pearson framework. This is achieved by treating the hypothesis as a family of hypotheses, each member of which explicitly concerns test level as well as effect size. Such testing encourages researchers to think about error rates and strength of evidence in both the statistical design and reporting stages of a study. Here, we show that these multi-alpha level tests can deliver acceptable expected total error costs. We first present formulas for expected error costs from single alpha and multiple alpha level tests, given prior probabilities of effect sizes that have either dichotomous or continuous distributions. Error costs are tied to decisions, with different decisions assumed for each of the potential outcomes in the multi-alpha level case. Expected total costs for tests at single and multiple alpha levels are then compared with optimal costs. This comparison highlights how sensitive optimization is to estimated error costs and to assumptions about prevalence. Testing at multiple default thresholds removes the need to formally identify decisions, or to model costs and prevalence as required in optimization approaches. Although total expected error costs with this approach will not be optimal, our results suggest they may be lower, on average, than when so-called optimal test levels are based on mis-specified models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21303v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janet Aisbett</dc:creator>
    </item>
    <item>
      <title>Socio-cognitive Networks between Researchers</title>
      <link>https://arxiv.org/abs/2407.21067</link>
      <description>arXiv:2407.21067v1 Announce Type: cross 
Abstract: Understanding why researchers cite each other has been a longstanding conjecture in studying scientific networks. Prior research suggests relevance, group cohesion, or honest source crediting as possible factors. However, the dual nature of cognitive and social dimensions underlying citation is often overlooked by not considering the intermediary steps leading up to a citation. For one work to be cited by another, it must first be published by a set of authors. Therefore, we investigate the reasons behind researchers' citations, explicitly examining the interplay of socio-cognitive ties through the interdependence of coauthorship and citation networks. We assess our claims in an empirical analysis by employing the Author-Oriented Relational HyperEvent Model (AuthRHEM) to study Chilean astronomers' citation and collaboration behavior between 2013 and 2015 in a joint framework. We find evidence that when deciding which work to cite, authors prefer other work with novelty and cognitive ties, such as work-to-work relations. At the same time, coherent groups are relevant because coauthors are cocited more frequently in subsequent publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21067v1</guid>
      <category>cs.SI</category>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alejandro Espinosa-Rada, J\"urgen Lerner, Cornelius Fritz</dc:creator>
    </item>
    <item>
      <title>Large-scale Epidemiological modeling: Scanning for Mosquito-Borne Diseases Spatio-temporal Patterns in Brazil</title>
      <link>https://arxiv.org/abs/2407.21286</link>
      <description>arXiv:2407.21286v1 Announce Type: cross 
Abstract: The influence of climate on mosquito-borne diseases like dengue and chikungunya is well-established, but comprehensively tracking long-term spatial and temporal trends across large areas has been hindered by fragmented data and limited analysis tools. This study presents an unprecedented analysis, in terms of breadth, estimating the SIR transmission parameters from incidence data in all 5,570 municipalities in Brazil over 14 years (2010-2023) for both dengue and chikungunya. We describe the Episcanner computational pipeline, developed to estimate these parameters, producing a reusable dataset describing all dengue and chikungunya epidemics that have taken place in this period, in Brazil. The analysis reveals new insights into the climate-epidemic nexus: We identify distinct geographical and temporal patterns of arbovirus disease incidence across Brazil, highlighting how climatic factors like temperature and precipitation influence the timing and intensity of dengue and chikungunya epidemics. The innovative Episcanner tool empowers researchers and public health officials to explore these patterns in detail, facilitating targeted interventions and risk assessments. This research offers a new perspective on the long-term dynamics of climate-driven mosquito-borne diseases and their geographical specificities linked to the effects of global temperature fluctuations such as those captured by the ENSO index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21286v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo C. Araujo, Claudia T. Code\c{c}o, Sandro Loch, Lu\~a B. Vacaro, La\'is P. Freitas, Raquel M. Lana, Leonardo S. Bastos, Iasmim F. de Almeida, Fernanda Valente, Luiz M. Carvalho, Fl\'avio C. Coelho</dc:creator>
    </item>
    <item>
      <title>Modeling Urban Transport Choices: Incorporating Sociocultural Aspects</title>
      <link>https://arxiv.org/abs/2407.21307</link>
      <description>arXiv:2407.21307v1 Announce Type: cross 
Abstract: This paper introduces an agent-based simulation model aimed at understanding urban commuters mode choices and evaluating the impacts of transport policies to promote sustainable mobility. Crafted for developing countries, where utilitarian travel heavily relies on motorcycles, the model integrates sociocultural factors that influence transport behavior. Multinomial models and inferential statistics applied to survey data from Cali, Colombia, inform the model, revealing significant influences of sociodemographic factors and travel attributes on mode choice. Findings highlight the importance of cost, time, safety, comfort, and personal security, with disparities across socioeconomic groups. Policy simulations demonstrate positive responses to interventions like free public transportation, increased bus frequency, and enhanced security, yet with modest shifts in mode choice. Multifaceted policy approaches are deemed more effective, addressing diverse user preferences. Outputs can be extended to cities with similar sociocultural characteristics and transport dynamics. The methodology applied in this work can be replicated for other territories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21307v1</guid>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kathleen Salazar-Serna, Lorena Cadavid, Carlos J. Franco</dc:creator>
    </item>
    <item>
      <title>A Bayesian Bootstrap Approach for Dynamic Borrowing for Minimizing Mean Squared Error</title>
      <link>https://arxiv.org/abs/2407.21588</link>
      <description>arXiv:2407.21588v1 Announce Type: cross 
Abstract: For dynamic borrowing to leverage external data to augment the control arm of small RCTs, the key step is determining the amount of borrowing based on the similarity of the outcomes in the controls from the trial and the external data sources. A simple approach for this task uses the empirical Bayesian approach, which maximizes the marginal likelihood (maxML) of the amount of borrowing, while a likelihood-independent alternative minimizes the mean squared error (minMSE). We consider two minMSE approaches that differ from each other in the way of estimating the parameters in the minMSE rule. The classical one adjusts for bias due to sample variance, which in some situations is equivalent to the maxML rule. We propose a simplified alternative without the variance adjustment, which has asymptotic properties partially similar to the maxML rule, leading to no borrowing if means of control outcomes from the two data sources are different and may have less bias than that of the maxML rule. In contrast, the maxML rule may lead to full borrowing even when two datasets are moderately different, which may not be a desirable property. For inference, we propose a Bayesian bootstrap (BB) based approach taking the uncertainty of the estimated amount of borrowing and that of pre-adjustment into account. The approach can also be used with a pre-adjustment on the external controls for population difference between the two data sources using, e.g., inverse probability weighting. The proposed approach is computationally efficient and is implemented via a simple algorithm. We conducted a simulation study to examine properties of the proposed approach, including the coverage of 95 CI based on the Bayesian bootstrapped posterior samples, or asymptotic normality. The approach is illustrated by an example of borrowing controls for an AML trial from another study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21588v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixian Wang, Ram Tiwari</dc:creator>
    </item>
    <item>
      <title>Remarks on the Poisson additive process</title>
      <link>https://arxiv.org/abs/2407.21651</link>
      <description>arXiv:2407.21651v1 Announce Type: cross 
Abstract: The Poisson additive process is a binary conditionally additive process such that the first is the Poisson process provided the second is given. We prove the existence and uniqueness of predictable increasing mean intensity for the Poisson additive process. Besides, we establish a likelihood ratio formula for the Poisson additive process. It directly implies there doesn't exist an anticipative Poisson additive process which is absolutely continuous with respect to the standard Poisson process, which confirms a conjecture proposed by P. Br\'emaud in his PhD thesis in 1972. When applied to the Hawkes process, it concludes that the self-exciting function is constant. Similar results are also obtained for the Wiener additive process and Markov additive process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21651v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoming Wang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Risk Measurement by SV-GARCH-EVT Model</title>
      <link>https://arxiv.org/abs/2201.09434</link>
      <description>arXiv:2201.09434v5 Announce Type: replace 
Abstract: This paper aims to more effectively manage and mitigate stock market risks by accurately characterizing financial market returns and volatility. We enhance the Stochastic Volatility (SV) model by incorporating fat-tailed distributions and leverage effects, estimating model parameters using Markov Chain Monte Carlo (MCMC) methods. By integrating extreme value theory (EVT) to fit the tail distribution of standard residuals, we develop the SV-EVT-VaR-based dynamic model. Our empirical analysis, using daily S\&amp;P 500 index data and simulated returns, shows that SV-EVT-based models outperform others in backtesting. These models effectively capture the fat-tailed properties of financial returns and the leverage effect, proving superior for out-of-sample data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.09434v5</guid>
      <category>stat.AP</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Bo, Minheng Xiao</dc:creator>
    </item>
    <item>
      <title>Endogenous Coalition Formation in Policy Debates</title>
      <link>https://arxiv.org/abs/1904.05327</link>
      <description>arXiv:1904.05327v2 Announce Type: replace-cross 
Abstract: Political actors form coalitions around their joint normative beliefs in order to influence the policy process on contentious issues such as climate change or population ageing. Policy process theory maintains that learning within and across coalitions is a central predictor of coalition formation and policy change but has yet to explain how policy learning works. The present article explains the formation and maintenance of coalitions by focusing on the ways actors adopt policy beliefs from other actors in policy debates. A policy debate is a complex social system in which temporal network dependence guides how actors contribute ideological statements to the debate. Belief adoption matters in three complementary ways: bonding, which exploits cues within coalitions; bridging, which explores new beliefs outside one's perimeter in the debate; and repulsion, which reinforces polarization between coalitions and cements their belief systems. We formalize this theory of endogenous coalition formation in policy debates and test it on a micro-level empirical dataset using statistical network analysis and event history analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.05327v2</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philip Leifeld, Laurence Brandenberger</dc:creator>
    </item>
    <item>
      <title>Non-Adaptive Multi-Stage Algorithm for Group Testing with Prior Statistics</title>
      <link>https://arxiv.org/abs/2402.10018</link>
      <description>arXiv:2402.10018v2 Announce Type: replace-cross 
Abstract: In this paper, we propose an efficient multi-stage algorithm for non-adaptive Group Testing (GT) with general correlated prior statistics. The proposed solution can be applied to any correlated statistical prior represented in trellis, e.g., finite state machines and Markov processes. We introduce a variation of List Viterbi Algorithm (LVA) to enable accurate recovery using much fewer tests than objectives, which efficiently gains from the correlated prior statistics structure. Our numerical results demonstrate that the proposed Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori (MAP) performance with feasible complexity in practical regimes, such as with COVID-19 and sparse signal recovery applications, and reduce in the scenarios tested the number of pooled tests by at least $25\%$ compared to existing classical low complexity GT algorithms. Moreover, we analytically characterize the complexity of the proposed MSGT algorithm that guarantees its efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10018v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayelet C. Portnoy, Alejandro Cohen</dc:creator>
    </item>
  </channel>
</rss>
