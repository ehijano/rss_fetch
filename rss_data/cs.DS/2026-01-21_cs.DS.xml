<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Jan 2026 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exact Computation of the Catalan Number $C(2,050,572,903)$</title>
      <link>https://arxiv.org/abs/2601.11621</link>
      <description>arXiv:2601.11621v1 Announce Type: new 
Abstract: This paper presents a two-phase algorithm for computing exact Catalan numbers at an unprecedented scale. The method is demonstrated by computing $C(n)$ for $n = 2,050,572,903$ yielding a result with a targeted $1,234,567,890$ decimal digits. To circumvent the memory limitations associated with evaluating large factorials, the algorithm operates exclusively in the prime-exponent domain. Phase 1 employs a parallel segmented sieve to enumerate primes up to $2n$ and applies Legendre's formula to determine the precise prime factorization of $C(n)$. The primes are grouped by exponent and serialized to disk. Phase 2 reconstructs the final integer using a memory-efficient balanced product tree with chunking. The algorithm runs on a time complexity of $\Theta(n(\log n)^2)$ bit-operations and a space complexity of $\Theta(n \log n)$ bits. This result represents the largest exact Catalan number computed to date. Performance statistics for a single-machine execution are reported, and verification strategies -- including modular checks and SHA-256 hash validation -- are discussed. The source code and factorization data are provided to ensure reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11621v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahesh Ramani</dc:creator>
    </item>
    <item>
      <title>Bicriteria Algorithms for Submodular Cover with Partition and Fairness Constraints</title>
      <link>https://arxiv.org/abs/2601.11755</link>
      <description>arXiv:2601.11755v1 Announce Type: new 
Abstract: In many submodular optimization applications, datasets are naturally partitioned into disjoint subsets. These scenarios give rise to submodular optimization problems with partition-based constraints, where the desired solution set should be in some sense balanced, fair, or resource-constrained across these partitions. While existing work on submodular cover largely overlooks this structure, we initiate a comprehensive study of the problem of Submodular Cover with Partition Constraints (SCP) and its key variants. Our main contributions are the development and analysis of scalable bicriteria approximation algorithms for these NP-hard optimization problems for both monotone and nonmonotone objectives. Notably, the algorithms proposed for the monotone case achieve optimal approximation guarantees while significantly reducing query complexity compared to existing methods. Finally, empirical evaluations on real-world and synthetic datasets further validate the efficiency and effectiveness of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11755v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjing Chen, Yixin Chen, Victoria G. Crawford</dc:creator>
    </item>
    <item>
      <title>Sum Estimation via Vector Similarity Search</title>
      <link>https://arxiv.org/abs/2601.11765</link>
      <description>arXiv:2601.11765v1 Announce Type: new 
Abstract: Semantic embeddings to represent objects such as image, text and audio are widely used in machine learning and have spurred the development of vector similarity search methods for retrieving semantically related objects. In this work, we study the sibling task of estimating a sum over all objects in a set, such as the kernel density estimate (KDE) and the normalizing constant for softmax distributions. While existing solutions provably reduce the sum estimation task to acquiring $\mathcal{O}(\sqrt{n})$ most similar vectors, where $n$ is the number of objects, we introduce a novel algorithm that only requires $\mathcal{O}(\log(n))$ most similar vectors. Our approach randomly assigns objects to levels with exponentially-decaying probabilities and constructs a vector similarity search data structure for each level. With the top-$k$ objects from each level, we propose an unbiased estimate of the sum and prove a high-probability relative error bound. We run experiments on OpenImages and Amazon Reviews with a vector similar search implementation to show that our method can achieve lower error using less computational time than existing reductions. We show results on applications in estimating densities, computing softmax denominators, and counting the number of vectors within a ball.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11765v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Mussmann, Mehul Smriti Raje, Kavya Tumkur, Oumayma Messoussi, Cyprien Hachem, Seby Jacob</dc:creator>
    </item>
    <item>
      <title>Analysis of a Random Local Search Algorithm for Dominating Set</title>
      <link>https://arxiv.org/abs/2601.11841</link>
      <description>arXiv:2601.11841v1 Announce Type: new 
Abstract: Dominating Set is a well-known combinatorial optimization problem which finds application in computational biology or mobile communication. Because of its $\mathrm{NP}$-hardness, one often turns to heuristics for good solutions. Many such heuristics have been empirically tested and perform rather well. However, it is not well understood why their results are so good or even what guarantees they can offer regarding their runtime or the quality of their results. For this, a strong theoretical foundation has to be established. We contribute to this by rigorously analyzing a Random Local Search (RLS) algorithm that aims to find a minimum dominating set on a graph. We consider its performance on cycle graphs with $n$ vertices. We prove an upper bound for the expected runtime until an optimum is found of $\mathcal{O}\left(n^4\log^2(n)\right)$. In doing so, we introduce several models to represent dominating sets on cycles that help us understand how RLS explores the search space to find an optimum. For our proof we use techniques which are already quite popular for the analysis of randomized algorithms. We further apply a special method to analyze a reversible Markov Chain, which arises as a result of our modeling. This method has not yet found wide application in this kind of runtime analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11841v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik Higl</dc:creator>
    </item>
    <item>
      <title>Parameterized Complexity of Scheduling Problems in Robotic Process Automation</title>
      <link>https://arxiv.org/abs/2601.11984</link>
      <description>arXiv:2601.11984v1 Announce Type: new 
Abstract: This paper studies the growing domain of Robotic Process Automation (RPA) problems. Motivated by scheduling problems arising in RPA, we study the parameterized complexity of the single-machine problem $1|\text{prec},r_j,d_j|*$. We focus on parameters naturally linked to RPA systems, including chain-like precedences, the number of distinct processing times, and the structure of the time windows. We show that the problem is W[2]-hard parameterized by the number of chains, even with only two prescribed processing times and two distinct time-window lengths. This hardness remains even for distinct processing times and time windows under prec-consistent time windows. On the positive side, we obtain polynomial-time algorithm when all jobs share a single time-window length and FPT when the processing times, release times and deadlines are chain-uniform. We also show that the problem lies in XP when parameterized by the width of the precedence relation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11984v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Dvo\v{r}\'ak, Anton\'in Nov\'ak, P\v{r}emysl \v{S}\r{u}cha, Du\v{s}an Knop, Claire Hanen</dc:creator>
    </item>
    <item>
      <title>Computing Maximal Repeating Subsequences in a String</title>
      <link>https://arxiv.org/abs/2601.12200</link>
      <description>arXiv:2601.12200v1 Announce Type: new 
Abstract: In this paper we initiate the study of computing a maximal (not necessarily maximum) repeating pattern in a single input string, where the corresponding problems have been studied (e.g., a maximal common subsequence) only in two or more input strings by Hirota and Sakai starting 2019. Given an input string $S$ of length $n$, we can compute a maximal square subsequence of $S$ in $O(n\log n)$ time, greatly improving the $O(n^2)$ bound for computing the longest square subsequence of $S$. For a maximal $k$-repeating subsequence, our bound is $O(f(k)n\log n)$, where \(f(k)\) is a computable function such that $f(k) &lt; k\cdot 4^k$. This greatly improves the $O(n^{2k-1})$ bound for computing a longest $k$-repeating subsequence of $S$, for $k\geq 3$. Both results hold for the constrained case, i.e., when the solution must contain a subsequence $X$ of $S$, though with higher running times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12200v1</guid>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyang Gong, Adiesha Liyanage, Braeden Sopp, Binhai Zhu</dc:creator>
    </item>
    <item>
      <title>Analyzing Collection Strategies: A Computational Perspective on the Coupon Collector Problem</title>
      <link>https://arxiv.org/abs/2601.12351</link>
      <description>arXiv:2601.12351v1 Announce Type: new 
Abstract: The Coupon Collector Problem (CCP) is a well-known combinatorial problem that seeks to estimate the number of random draws required to complete a collection of $n$ distinct coupon types. Various generalizations of this problem have been applied in numerous engineering domains. However, practical applications are often hindered by the computational challenges associated with deriving numerical results for moments and distributions. In this work, we present three algorithms for solving the most general form of the CCP, where coupons are collected under any arbitrary drawing probability, with the objective of obtaining $t$ copies of a subset of $k$ coupons from a total of $n$. The First algorithm provides the base model to compute the expectation, variance, and the second moment of the collection process. The second algorithm utilizes the construction of the base model and computes the same values in polynomial time with respect to $n$ under the uniform drawing distribution, and the third algorithm extends to any general drawing distribution. All algorithms leverage Markov models specifically designed to address computational challenges, ensuring exact computation of the expectation and variance of the collection process. Their implementation uses a dynamic programming approach that follows from the Markov models framework, and their time complexity is analyzed accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12351v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadas Abraham, Ido Feldman, Eitan Yaakobi</dc:creator>
    </item>
    <item>
      <title>Approximation Schemes for Sequential Hiring Problems</title>
      <link>https://arxiv.org/abs/2601.12750</link>
      <description>arXiv:2601.12750v1 Announce Type: new 
Abstract: The main contribution of this paper resides in providing novel algorithmic advances and analytical insights for the sequential hiring problem, a recently introduced dynamic optimization model where a firm adaptively fills a limited number of positions from a pool of applicants with known values and acceptance probabilities. While earlier research established a strong foundation -- notably an LP-based $(1 - \frac{e^{-k}k^k}{k!})$-approximation by Epstein and Ma (Operations Research, 2024) -- the attainability of superior approximation guarantees has remained a central open question.
  Our work addresses this challenge by establishing the first polynomial-time approximation scheme for sequential hiring, proposing an $O(n^{O(1)} \cdot T^{2^{\tilde{O}(1/\epsilon^{2})}})$-time construction of semi-adaptive policies whose expected reward is within factor $1 - \epsilon$ of optimal. To overcome the constant-factor optimality loss inherent to earlier literature, and to circumvent intrinsic representational barriers of adaptive policies, our approach is driven by the following innovations:
  -- The block-responsive paradigm: We introduce block-responsive policies, a new class of decision-making strategies, selecting ordered sets (blocks) of applicants rather than single individuals, while still allowing for internal reactivity.
  -- Adaptivity and efficiency: We prove that these policies can nearly match the performance of general adaptive policies while utilizing polynomially-sized decision trees.
  -- Efficient construction: By developing a recursive enumeration-based framework, we resolve the problematic ``few-positions'' regime, bypassing a fundamental hurdle that hindered previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12750v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danny Segev, Uri Stein</dc:creator>
    </item>
    <item>
      <title>Kd-tree Based Wasserstein Distance Approximation for High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2601.12975</link>
      <description>arXiv:2601.12975v1 Announce Type: new 
Abstract: The Wasserstein distance is a discrepancy measure between probability distributions, defined by an optimal transport problem. It has been used for various tasks such as retrieving similar items in high-dimensional images or text data. In retrieval applications, however, the Wasserstein distance is calculated repeatedly, and its cubic time complexity with respect to input size renders it unsuitable for large-scale datasets. Recently, tree-based approximation methods have been proposed to address this bottleneck. For example, the Flowtree algorithm computes transport on a quadtree and evaluates cost using the ground metric, and clustering-tree approaches have been reported to achieve high accuracy. However, these existing trees often incur significant construction time for preprocessing, and crucially, standard quadtrees cannot grow deep enough in high-dimensional spaces, resulting in poor approximation accuracy. In this paper, we propose kd-Flowtree, a kd-tree-based Wasserstein distance approximation method that uses a kd-tree for data embedding. Since kd-trees can grow sufficiently deep and adaptively even in high-dimensional cases, kd-Flowtree is capable of maintaining good approximation accuracy for such cases. In addition, kd-trees can be constructed quickly than quadtrees, which contributes to reducing the computation time required for nearest neighbor search, including preprocessing. We provide a probabilistic upper bound on the nearest-neighbor search accuracy of kd-Flowtree, and show that this bound is independent of the dataset size. In the numerical experiments, we demonstrated that kd-Flowtree outperformed the existing Wasserstein distance approximation methods for retrieval tasks with real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12975v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanata Teshigawara, Keisho Oh, Ken Kobayashi, Kazuhide Nakata</dc:creator>
    </item>
    <item>
      <title>The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage</title>
      <link>https://arxiv.org/abs/2601.13220</link>
      <description>arXiv:2601.13220v1 Announce Type: new 
Abstract: Retrieving data from large-scale source code archives is vital for AI training, neural-based software analysis, and information retrieval, to cite a few. This paper studies and experiments with the design of a compressed key-value store for the indexing of large-scale source code datasets, evaluating its trade-off among three primary computational resources: (compressed) space occupancy, time, and energy efficiency. Extensive experiments on a national high-performance computing infrastructure demonstrate that different compression configurations yield distinct trade-offs, with high compression ratios and order-of-magnitude gains in retrieval throughput and energy efficiency. We also study data parallelism and show that, while it significantly improves speed, scaling energy efficiency is more difficult, reflecting the known non-energy-proportionality of modern hardware and challenging the assumption of a direct time-energy correlation. This work streamlines automation in energy-aware configuration tuning and standardized green benchmarking deployable in CI/CD pipelines, thus empowering system architects with a spectrum of Pareto-optimal energy-compression-throughput trade-offs and actionable guidelines for building sustainable, efficient storage backends for massive open-source code archival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13220v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Ferragina, Francesco Tosoni</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Online TRP on a Line</title>
      <link>https://arxiv.org/abs/2601.13494</link>
      <description>arXiv:2601.13494v1 Announce Type: new 
Abstract: We study the online traveling repairperson problem on a line within the recently proposed learning-augmented framework, which provides predictions on the requests to be served via machine learning. In the original model (with no predictions), there is a stream of requests released over time along the line. The goal is to minimize the sum (or average) of the completion times of the requests. In the original model, the state-of-the-art competitive ratio lower bound is $1+\sqrt{2} &gt; 2.414$ for any deterministic algorithm and the state-of-the-art competitive ratio upper bound is 4 for a deterministic algorithm. Our prediction model involves predicted positions, possibly error-prone, of each request in the stream known a priori but the arrival times of requests are not known until their arrival. We first establish a 3-competitive lower bound which extends to the original model. We then design a deterministic algorithm that is $(2+\sqrt{3})\approx 3.732$-competitive when predictions are perfect. With imperfect predictions (maximum error $\delta &gt; 0$), we show that our deterministic algorithm becomes $\min\{3.732+4\delta,4\}$-competitive, knowing $\delta$. To the best of our knowledge, these are the first results for online traveling repairperson problem in the learning-augmented framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13494v1</guid>
      <category>cs.DS</category>
      <category>cs.RO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnil Guragain, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Zero-free regions and concentration inequalities for hypergraph colorings in the local lemma regime</title>
      <link>https://arxiv.org/abs/2601.13796</link>
      <description>arXiv:2601.13796v1 Announce Type: new 
Abstract: We show that for $q$-colorings in $k$-uniform hypergraphs with maximum degree $\Delta$, if $k\ge 50$ and $q\ge 700\Delta^{\frac{5}{k-10}}$, there is a "Lee-Yang" zero-free strip around the interval $[0,1]$ of the partition function, which includes the special case of uniform enumeration of hypergraph colorings. As an immediate consequence, we obtain Berry-Esseen type inequalities for hypergraph $q$-colorings under such conditions, demonstrating the asymptotic normality for the size of any color class in a uniformly random coloring. Our framework also extends to the study of "Fisher zeros", leading to deterministic algorithms for approximating the partition function in the zero-free region.
  Our approach is based on extending the recent work of [Liu, Wang, Yin, Yu, STOC 2025] to general constraint satisfaction problems (CSP). We focus on partition functions defined for CSPs by introducing external fields to the variables. A key component in our approach is a projection-lifting scheme, which enables us to essentially lift information percolation type analysis for Markov chains from the real line to the complex plane. Last but not least, we also show a Chebyshev-type inequality under the sampling LLL condition for atomic CSPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13796v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.PR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingcheng Liu, Yixiao Yu</dc:creator>
    </item>
    <item>
      <title>Efficient Parallel $(\Delta+1)$-Edge-Coloring</title>
      <link>https://arxiv.org/abs/2601.13822</link>
      <description>arXiv:2601.13822v1 Announce Type: new 
Abstract: We study the $(\Delta+1)$-edge-coloring problem in the parallel $\left(\mathrm{PRAM}\right)$ model of computation. The celebrated Vizing's theorem [Viz64] states that every simple graph $G = (V,E)$ can be properly $(\Delta+1)$-edge-colored. In a seminal paper, Karloff and Shmoys [KS87] devised a parallel algorithm with time $O\left(\Delta^5\cdot\log n\cdot\left(\log^3 n+\Delta^2\right)\right)$ and $O(m\cdot\Delta)$ processors. This result was improved by Liang et al. [LSH96] to time $O\left(\Delta^{4.5}\cdot \log^3\Delta\cdot \log n + \Delta^4 \cdot\log^4 n\right)$ and $O\left(n\cdot\Delta^{3} +n^2\right)$ processors. [LSH96] claimed $O\left(\Delta^{3.5} \cdot\log^3\Delta\cdot \log n + \Delta^3\cdot \log^4 n\right)$ time, but we point out a flaw in their analysis, which once corrected, results in the above bound. We devise a faster parallel algorithm for this fundamental problem. Specifically, our algorithm uses $O\left(\Delta^4\cdot \log^4 n\right)$ time and $O(m\cdot \Delta)$ processors. Another variant of our algorithm requires $O\left(\Delta^{4+o(1)}\cdot\log^2 n\right)$ time, and $O\left(m\cdot\Delta\cdot\log n\cdot\log^{\delta}\Delta\right)$ processors, for an arbitrarily small $\delta&gt;0$. We also devise a few other tradeoffs between the time and the number of processors, and devise an improved algorithm for graphs with small arboricity. On the way to these results, we also provide a very fast parallel algorithm for updating $(\Delta+1)$-edge-coloring. Our algorithm for this problem is dramatically faster and simpler than the previous state-of-the-art algorithm (due to [LSH96]) for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13822v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Elkin, Ariel Khuzman</dc:creator>
    </item>
    <item>
      <title>Nemesis, an Escape Game in Graphs</title>
      <link>https://arxiv.org/abs/2601.13841</link>
      <description>arXiv:2601.13841v1 Announce Type: new 
Abstract: We define a new escape game in graphs that we call Nemesis. The game is played on a graph having a subset of vertices labeled as exits and the goal of one of the two players, called the fugitive, is to reach one of these exit vertices. The second player, i.e. the fugitive adversary, is called the Nemesis. Her goal is to trap the fugitive in a connected component which does not contain any exit. At each round of the game, the fugitive moves from one vertex to an adjacent vertex. Then the Nemesis deletes one edge anywhere in the graph. The game ends when either the fugitive reached an exit or when he is in a connected component that does not contain any exit. In trees and graphs of maximum degree bounded by 3, Nemesis can be solved in linear time. We also show that a variant of the game called Blizzard where only edges adjacent to the position of the fugitive can be deleted also admits a linear time solution. For arbitrary graphs, we show that Nemesis is PSPACE-complete, and that it is NP-hard on planar multigraphs. We extend our results to the related Cat Herding problem, proving its PSPACE-completeness. We also prove that finding a strategy based on a full binary escape tree whose leaves are exists is NP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13841v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pierre Berg\'e, Antoine Dailly, Yan Gerard</dc:creator>
    </item>
    <item>
      <title>Approximating splits for decision trees quickly in sparse data streams</title>
      <link>https://arxiv.org/abs/2601.12525</link>
      <description>arXiv:2601.12525v1 Announce Type: cross 
Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + \alpha)$ approximation when using conditional entropy in amortized $O(\alpha^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + \alpha)$ approximation in amortized $O(\alpha^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12525v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978520.69</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 2025 SIAM International Conference on Data Mining (SDM) (pp. 647-655) 2025</arxiv:journal_reference>
      <dc:creator>Nikolaj Tatti</dc:creator>
    </item>
    <item>
      <title>Explicit Almost-Optimal $\varepsilon$-Balanced Codes via Free Expander Walks</title>
      <link>https://arxiv.org/abs/2601.12606</link>
      <description>arXiv:2601.12606v1 Announce Type: cross 
Abstract: We study the problem of constructing explicit codes whose rate and distance match the Gilbert-Varshamov bound in the low-rate, high-distance regime. In 2017, Ta-Shma gave an explicit family of codes where every pair of codewords has relative distance $\frac{1-\varepsilon}{2}$, with rate $\Omega(\varepsilon^{2+o(1)})$, matching the Gilbert-Varshamov bound up to a factor of $\varepsilon^{o(1)}$. Ta-Shma's construction was based on starting with a good code and amplifying its bias with walks arising from the $s$-wide-replacement product.
  In this work, we give an arguably simpler almost-optimal construction, based on what we call free expander walks: ordinary expander walks where each step is taken on a distinct expander from a carefully chosen sequence. This sequence of expanders is derived from the construction of near-$X$-Ramanujan graphs due to O'Donnell and Wu.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12606v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-Ting Hsieh, Sidhanth Mohanty, Rachel Yun Zhang</dc:creator>
    </item>
    <item>
      <title>Quantum Data Structure for Range Minimum Query</title>
      <link>https://arxiv.org/abs/2601.13195</link>
      <description>arXiv:2601.13195v1 Announce Type: cross 
Abstract: Given an array $a[1..n]$, the Range Minimum Query (RMQ) problem is to maintain a data structure that supports RMQ queries: given a range $[l, r]$, find the index of the minimum element among $a[l..r]$, i.e., $\operatorname{argmin}_{i \in [l, r]} a[i]$. In this paper, we propose a quantum data structure that supports RMQ queries and range updates, with an optimal time complexity $\widetilde \Theta(\sqrt{nq})$ for performing $q = O(n)$ operations without preprocessing, compared to the classical $\widetilde\Theta(n+q)$. As an application, we obtain a time-efficient quantum algorithm for $k$-minimum finding without the use of quantum random access memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13195v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcss.2026.103756</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computer and System Sciences, 103756, 2026</arxiv:journal_reference>
      <dc:creator>Qisheng Wang, Zhean Xu, Zhicheng Zhang</dc:creator>
    </item>
    <item>
      <title>The Query Complexity of Local Search in Rounds on General Graphs</title>
      <link>https://arxiv.org/abs/2601.13266</link>
      <description>arXiv:2601.13266v1 Announce Type: cross 
Abstract: We analyze the query complexity of finding a local minimum in $t$ rounds on general graphs. More precisely, given a graph $G = (V,E)$ and oracle access to an unknown function $f : V \to \mathbb{R}$, the goal is to find a local minimum--a vertex $v$ such that $f(v) \leq f(u)$ for all $(u,v) \in E$--using at most $t$ rounds of interaction with the oracle. The query complexity is well understood on grids, but much less is known beyond. This abstract problem captures many optimization tasks, such as finding a local minimum of a loss function during neural network training.
  For each graph with $n$ vertices, we prove a deterministic upper bound of $O(t n^{1/t} (s\Delta)^{1-1/t})$, where $s$ is the separation number and $\Delta$ is the maximum degree of the graph. We complement this result with a randomized lower bound of $\Omega(t n^{1/t}-t)$ that holds for any connected graph. We also find that parallel steepest descent with a warm start provides improved bounds for graphs with high separation number and bounded degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13266v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simina Br\^anzei, Ioannis Panageas, Dimitris Paparas</dc:creator>
    </item>
    <item>
      <title>A uniformity principle for spatial matching</title>
      <link>https://arxiv.org/abs/2601.13426</link>
      <description>arXiv:2601.13426v1 Announce Type: cross 
Abstract: Platforms matching spatially distributed supply to demand face a fundamental design choice: given a fixed total budget of service range, how should it be allocated across supply nodes ex ante, i.e. before supply and demand locations are realized, to maximize fulfilled demand? We model this problem using bipartite random geometric graphs where $n$ supply and $m$ demand nodes are uniformly distributed on $[0,1]^k$ ($k \ge 1$), and edges form when demand falls within a supply node's service region, the volume of which is determined by its service range. Since each supply node serves at most one demand, platform performance is determined by the expected size of a maximum matching. We establish a uniformity principle: whenever one service range allocation is more uniform than the other, the more uniform allocation yields a larger expected matching. This principle emerges from diminishing marginal returns to range expanding service range, and limited interference between supply nodes due to bounded ranges naturally fragmenting the graph. For $k=1$, we further characterize the expected matching size through a Markov chain embedding and derive closed-form expressions for special cases. Our results provide theoretical guidance for optimizing service range allocation and designing incentive structures in ride-hailing, on-demand labor markets, and drone delivery networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13426v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ameen, Flore Sentenac, Sophie H. Yu</dc:creator>
    </item>
    <item>
      <title>A Minimax Perspective on Almost-Stable Matchings</title>
      <link>https://arxiv.org/abs/2601.14195</link>
      <description>arXiv:2601.14195v1 Announce Type: cross 
Abstract: Stability is crucial in matching markets, yet in many real-world settings - from hospital residency allocations to roommate assignments - full stability is either impossible to achieve or can come at the cost of leaving many agents unmatched. When stability cannot be achieved, algorithmicists and market designers face a critical question: how should instability be measured and distributed among participants? Existing approaches to "almost-stable" matchings focus on aggregate measures, minimising either the total number of blocking pairs or the count of agents involved in blocking pairs. However, such aggregate objectives can result in concentrated instability on a few individual agents, raising concerns about fairness and incentives to deviate. We introduce a fairness-oriented approach to approximate stability based on the minimax principle: we seek matchings that minimise the maximum number of blocking pairs any agent is in. Equivalently, we minimise the maximum number of agents that anyone has justified envy towards. This distributional objective protects the worst-off agents from a disproportionate amount of instability. We characterise the computational complexity of this notion across fundamental matching settings. Surprisingly, even very modest guarantees prove computationally intractable: we show that it is NP-complete to decide whether a matching exists in which no agent is in more than one blocking pair, even when preference lists have constant-bounded length. This hardness applies to both Stable Roommates and maximum-cardinality Stable Marriage. On the positive side, we provide polynomial-time algorithms when agents rank at most two others, and present approximation algorithms and integer programs. Our results map the algorithmic landscape and reveal fundamental trade-offs between distributional guarantees and computational feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14195v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Glitzner, David Manlove</dc:creator>
    </item>
    <item>
      <title>Provably Fast and Space-Efficient Parallel Biconnectivity</title>
      <link>https://arxiv.org/abs/2301.01356</link>
      <description>arXiv:2301.01356v2 Announce Type: replace 
Abstract: Biconnectivity is one of the most fundamental graph problems. The canonical parallel biconnectivity algorithm is the Tarjan-Vishkin algorithm, which has $O(n+m)$ optimal work (number of operations) and polylogarithmic span (longest dependent operations) on a graph with $n$ vertices and $m$ edges. However, Tarjan-Vishkin is not widely used in practice. We believe the reason is the space-inefficiency (it generates an auxiliary graph with $O(m)$ edges). In practice, existing parallel implementations are based on breath-first search (BFS). Since BFS has span proportional to the diameter of the graph, existing parallel BCC implementations suffer from poor performance on large-diameter graphs and can be even slower than the sequential algorithm on many real-world graphs.
  We propose the first parallel biconnectivity algorithm (FAST-BCC) that has optimal work, polylogarithmic span, and is space-efficient. Our algorithm first generates a skeleton graph based on any spanning tree of the input graph. Then we use the connectivity information of the skeleton to compute the biconnectivity of the original input. All the steps in our algorithm are highly-parallel. We carefully analyze the correctness of our algorithm, which is highly non-trivial.
  We implemented FAST-BCC and compared it with existing implementations, including GBBS, Slota and Madduri's algorithm, and the sequential Hopcroft-Tarjan algorithm. We ran them on a 96-core machine on 27 graphs, including social, web, road, $k$-NN, and synthetic graphs, with significantly varying sizes and edge distributions. FAST-BCC is the fastest on all 27 graphs. On average (geometric means), FAST-BCC is 5.1$\times$ faster than GBBS, and 3.1$\times$ faster than the best existing baseline on each graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01356v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3572848.3577483</arxiv:DOI>
      <dc:creator>Xiaojun Dong, Letong Wang, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Improved All-Pairs Approximate Shortest Paths in Congested Clique</title>
      <link>https://arxiv.org/abs/2405.02695</link>
      <description>arXiv:2405.02695v2 Announce Type: replace 
Abstract: In this paper, we present a new randomized $O(1)$-approximation algorithm for the All-Pairs Shortest Paths (APSP) problem in weighted undirected graphs that runs in just $O(\log \log \log n)$ rounds in the Congested-Clique model.
  Before our work, the fastest algorithms achieving an $O(1)$-approximation for APSP in weighted undirected graphs required $\operatorname{poly}(\log n)$ rounds, as shown by Censor-Hillel, Dory, Korhonen, and Leitersdorf (PODC 2019 &amp; Distributed Computing 2021). In the unweighted undirected setting, Dory and Parter (PODC 2020 &amp; Journal of the ACM 2022) obtained $O(1)$-approximation in $\operatorname{poly}(\log \log n)$ rounds.
  By terminating our algorithm early, for any given parameter $t \geq 1$, we obtain an $O(t)$-round algorithm that guarantees an $O\left(\log^{1/2^t} n\right)$ approximation in weighted undirected graphs. This tradeoff between round complexity and approximation factor offers flexibility, allowing the algorithm to adapt to different requirements. In particular, for any constant $\varepsilon &gt; 0$, an $O\left(\log^\varepsilon n\right)$-approximation can be obtained in $O(1)$ rounds. Previously, $O(1)$-round algorithms were only known for $O(\log n)$-approximation, as shown by Chechik and Zhang (PODC 2022).
  A key ingredient in our algorithm is a lemma that, under certain conditions, allows us to improve an $a$-approximation for APSP to an $O(\sqrt{a})$-approximation in $O(1)$ rounds. To prove this lemma, we develop several new techniques, including an $O(1)$-round algorithm for computing the $k$-nearest nodes, as well as new types of hopsets and skeleton graphs based on the notion of $k$-nearest nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02695v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong Duc Bui, Shashwat Chandra, Yi-Jun Chang, Michal Dory, Dean Leitersdorf</dc:creator>
    </item>
    <item>
      <title>Protrusion Decompositions Revisited: Uniform Lossy Kernels for Reducing Treewidth and Linear Kernels for Hitting Disconnected Minors</title>
      <link>https://arxiv.org/abs/2601.08424</link>
      <description>arXiv:2601.08424v2 Announce Type: replace 
Abstract: Let F be a finite family of graphs. In the F-Deletion problem, one is given a graph G and an integer k, and the goal is to find k vertices whose deletion results in a graph with no minor from the family F. This may be regarded as a far-reaching generalization of Vertex Cover and Feedback vertex Set. In their seminal work, Fomin, Lokshtanov, Misra &amp; Saurabh [FOCS 2012] gave a polynomial kernel for this problem when the family F contains a planar graph. As the size of their kernel is g(F) * k^{f(F)}, a natural follow-up question was whether the dependence on F in the exponent of k can be avoided. The answer turned out to be negative: Giannapoulou, Jansen, Lokshtanov &amp; Saurabh [TALG 2017] proved that this is already inevitable for the special case of the Treewidth-d-Deletion problem.
  In this work, we show that this non-uniformity can be avoided at the expense of a small loss. First, we present a simple 2-approximate kernelization algorithm for Treewidth-d-Deletion with kernel size g(d) * k^5. Next, we show that the approximation factor can be made arbitrarily close to 1, if we settle for a kernelization protocol with O(1) calls to an oracle that solves instances of size bounded by a uniform polynomial in k.
  We also obtain linear kernels on sparse graph classes when F contains a planar graph, whereas the previously known theorems required all graphs in F to be connected. Specifically, we generalize the kernelization algorithm by Kim, Langer, Paul, Reidl, Rossmanith, Sau &amp; Sikdar [TALG 2015] on graph classes that exclude a topological minor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08424v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.STACS.2026.31</arxiv:DOI>
      <dc:creator>Roohani Sharma, Micha{\l} W{\l}odarczyk</dc:creator>
    </item>
    <item>
      <title>A Control-Theoretic Perspective on Optimal High-Order Optimization</title>
      <link>https://arxiv.org/abs/1912.07168</link>
      <description>arXiv:1912.07168v5 Announce Type: replace-cross 
Abstract: We provide a control-theoretic perspective on optimal tensor algorithms for minimizing a convex function in a finite-dimensional Euclidean space. Given a function $\Phi: \mathbb{R}^d \rightarrow \mathbb{R}$ that is convex and twice continuously differentiable, we study a closed-loop control system that is governed by the operators $\nabla \Phi$ and $\nabla^2 \Phi$ together with a feedback control law $\lambda(\cdot)$ satisfying the algebraic equation $(\lambda(t))^p\|\nabla\Phi(x(t))\|^{p-1} = \theta$ for some $\theta \in (0, 1)$. Our first contribution is to prove the existence and uniqueness of a local solution to this system via the Banach fixed-point theorem. We present a simple yet nontrivial Lyapunov function that allows us to establish the existence and uniqueness of a global solution under certain regularity conditions and analyze the convergence properties of trajectories. The rate of convergence is $O(1/t^{(3p+1)/2})$ in terms of objective function gap and $O(1/t^{3p})$ in terms of squared gradient norm. Our second contribution is to provide two algorithmic frameworks obtained from discretization of our continuous-time system, one of which generalizes the large-step A-HPE framework and the other of which leads to a new optimal $p$-th order tensor algorithm. While our discrete-time analysis can be seen as a simplification and generalization of~\citet{Monteiro-2013-Accelerated}, it is largely motivated by the aforementioned continuous-time analysis, demonstrating the fundamental role that the feedback control plays in optimal acceleration and the clear advantage that the continuous-time perspective brings to algorithmic design. A highlight of our analysis is that we show that all of the $p$-th order optimal tensor algorithms that we discuss minimize the squared gradient norm at a rate of $O(k^{-3p})$, which complements the recent analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:1912.07168v5</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Lin, Michael. I. Jordan</dc:creator>
    </item>
    <item>
      <title>A closer look at TDFA</title>
      <link>https://arxiv.org/abs/2206.01398</link>
      <description>arXiv:2206.01398v3 Announce Type: replace-cross 
Abstract: We present an algorithm for regular expression parsing and submatch extraction based on tagged deterministic finite automata. The algorithm works with different disambiguation policies. We give detailed pseudocode for the algorithm, covering important practical optimizations. All transformations from a regular expression to an optimized automaton are explained on a step-by-step example. We consider both ahead-of-time and just-in-time determinization and describe variants of the algorithm suited to each setting. We provide benchmarks showing that the algorithm is very fast in practice. Our research is based on two independent implementations: an open-source lexer generator RE2C and an experimental Java library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01398v3</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Angelo Borsotti, Ulya Trafimovich</dc:creator>
    </item>
    <item>
      <title>High-Temperature Fermionic Gibbs States are Mixtures of Gaussian States</title>
      <link>https://arxiv.org/abs/2505.09730</link>
      <description>arXiv:2505.09730v2 Announce Type: replace-cross 
Abstract: Efficient simulation of a quantum system generally relies on structural properties of the quantum state. Motivated by the recent results by Bakshi et al. on the sudden death of entanglement in high-temperature Gibbs states of quantum spin systems, we study the high-temperature Gibbs states of bounded-degree local fermionic Hamiltonians, which include the special case of geometrically local fermionic systems. We prove that at a sufficiently high temperature that is independent of the system size, the Gibbs state is a probabilistic mixture of fermionic Gaussian states. This forms the basis of an efficient classical algorithm to prepare the Gibbs state by sampling from a distribution of fermionic Gaussian states. As a contrasting example, we show that high-temperature Gibbs states of the Sachdev-Ye-Kitaev (SYK) model are not convex mixtures of Gaussian states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09730v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshar Ramkumar, Yiyi Cai, Yu Tong, Jiaqing Jiang</dc:creator>
    </item>
    <item>
      <title>Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2509.10077</link>
      <description>arXiv:2509.10077v2 Announce Type: replace-cross 
Abstract: Efficient planning and sequence selection are central to intelligence, yet current approaches remain largely incompatible with biological computation. Classical graph algorithms like Dijkstra's or A* require global state and biologically implausible operations such as backtracing, while reinforcement learning methods rely on slow gradient-based policy updates that appear inconsistent with rapid behavioral adaptation observed in natural systems.
  We propose a biologically plausible algorithm for shortest-path computation that operates through local spike-based message-passing with realistic processing delays. The algorithm exploits spike-timing coincidences to identify nodes on optimal paths: Neurons that receive inhibitory-excitatory message pairs earlier than predicted reduce their response delays, creating a temporal compression that propagates backwards from target to source. Through analytical proof and simulations on random spatial networks, we demonstrate that the algorithm converges and discovers all shortest paths using purely timing-based mechanisms. By showing how short-term timing dynamics alone can compute shortest paths, this work provides new insights into how biological networks might solve complex computational problems through purely local computation and relative spike-time prediction. These findings open new directions for understanding distributed computation in biological and artificial systems, with possible implications for computational neuroscience, AI, reinforcement learning, and neuromorphic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10077v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simen Storesund, Kristian Valset Aars, Robin Dietrich, Nicolai Waniek</dc:creator>
    </item>
    <item>
      <title>Perspectives on Stochastic Localization</title>
      <link>https://arxiv.org/abs/2510.04460</link>
      <description>arXiv:2510.04460v2 Announce Type: replace-cross 
Abstract: We survey different perspectives on the stochastic localization process of Eldan, a powerful construction that has had many exciting recent applications in high-dimensional probability and algorithm design. Unlike prior surveys on this topic, our focus is on giving a self-contained presentation of all known alternative constructions of Eldan's stochastic localization, with an emphasis on connections between different constructions. Our hope is that by collecting these perspectives, some of which had primarily arisen within a particular community (e.g., probability theory, theoretical computer science, information theory, or machine learning), we can broaden the accessibility of stochastic localization, and ease its future use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04460v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bobby Shi, Kevin Tian, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2510.21414</link>
      <description>arXiv:2510.21414v3 Announce Type: replace-cross 
Abstract: Maximum-likelihood (ML) decoding for arbitrary block codes remains fundamentally hard, with worst-case time complexity-measured by the total number of multiplications-being no better than straightforward exhaustive search, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper introduces a simple, code-agnostic framework that reduces the worst-case complexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable reduction in practice. The result holds for both linear and nonlinear block codes over general memoryless channels and under both hard-decision and soft-decision decoding. It naturally extends to intersymbol-interference (ISI) channels and ML list decoding with only a negligible increase in complexity. Our core insight is that, upon receipt of each sequence at the receiver, the conditional probability of that sequence for each codeword in the codebook (i.e., the \emph{likelihood}) can be expressed as the inner product of two carefully constructed vectors -- the first depending on the received sequence, and the second on that codeword itself. As a result, evaluating the likelihoods for all codewords in the codebook reduces to a single vector-matrix multiplication, and ML decoding (MLD) becomes the simple task of picking the maximum entry in the resulting vector. The only non-trivial cost lies in the vector-matrix product. However, our matrix construction allows the use of the Mailman algorithm to reduce this cost. This time reduction is achieved at the cost of high space complexity, requiring $\mathcal{O}(q^{k+1} n)$ space to store the pre-computed codebook matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21414v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoang Ly, Emina Soljanin, Michael Schleppy</dc:creator>
    </item>
  </channel>
</rss>
