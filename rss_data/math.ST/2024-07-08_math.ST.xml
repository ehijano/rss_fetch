<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Wide stable neural networks: Sample regularity, functional convergence and Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2407.03909</link>
      <description>arXiv:2407.03909v1 Announce Type: new 
Abstract: We study the large-width asymptotics of random fully connected neural networks with weights drawn from $\alpha$-stable distributions, a family of heavy-tailed distributions arising as the limiting distributions in the Gnedenko-Kolmogorov heavy-tailed central limit theorem. We show that in an arbitrary bounded Euclidean domain $\mathcal{U}$ with smooth boundary, the random field at the infinite-width limit, characterized in previous literature in terms of finite-dimensional distributions, has sample functions in the fractional Sobolev-Slobodeckij-type quasi-Banach function space $W^{s,p}(\mathcal{U})$ for integrability indices $p &lt; \alpha$ and suitable smoothness indices $s$ depending on the activation function of the neural network, and establish the functional convergence of the processes in $\mathcal{P}(W^{s,p}(\mathcal{U}))$. This convergence result is leveraged in the study of functional posteriors for edge-preserving Bayesian inverse problems with stable neural network priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03909v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tom\'as Soto</dc:creator>
    </item>
    <item>
      <title>A simplified directional KeRF algorithm</title>
      <link>https://arxiv.org/abs/2407.04042</link>
      <description>arXiv:2407.04042v1 Announce Type: new 
Abstract: Random forest methods belong to the class of non-parametric machine learning algorithms. They were first introduced in 2001 by Breiman and they perform with accuracy in high dimensional settings. In this article, we consider, a simplified kernel-based random forest algorithm called simplified directional KeRF (Kernel Random Forest). We establish the asymptotic equivalence between simplified directional KeRF and centered KeRF, with additional numerical experiments supporting our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04042v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iakovidis Isidoros, Nicola Arcozzi</dc:creator>
    </item>
    <item>
      <title>Using Synthetic Data to Regularize Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2407.04194</link>
      <description>arXiv:2407.04194v1 Announce Type: new 
Abstract: To overcome challenges in fitting complex models with small samples, catalytic priors have recently been proposed to stabilize the inference by supplementing observed data with synthetic data generated from simpler models. Based on a catalytic prior, the Maximum A Posteriori (MAP) estimator is a regularized estimator that maximizes the weighted likelihood of the combined data. This estimator is straightforward to compute, and its numerical performance is superior or comparable to other likelihood-based estimators. In this paper, we study several theoretical aspects regarding the MAP estimator in generalized linear models, with a particular focus on logistic regression. We first prove that under mild conditions, the MAP estimator exists and is stable against the randomness in synthetic data. We then establish the consistency of the MAP estimator when the dimension of covariates diverges slower than the sample size. Furthermore, we utilize the convex Gaussian min-max theorem to characterize the asymptotic behavior of the MAP estimator as the dimension grows linearly with the sample size. These theoretical results clarify the role of the tuning parameters in a catalytic prior, and provide insights in practical applications. We provide numerical studies to confirm the effective approximation of our asymptotic theory in finite samples and to illustrate adjusting inference based on the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04194v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihao Li, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>An Axiomatic Definition of Hierarchical Clustering</title>
      <link>https://arxiv.org/abs/2407.03574</link>
      <description>arXiv:2407.03574v1 Announce Type: cross 
Abstract: In this paper, we take an axiomatic approach to defining a population hierarchical clustering for piecewise constant densities, and in a similar manner to Lebesgue integration, extend this definition to more general densities. When the density satisfies some mild conditions, e.g., when it has connected support, is continuous, and vanishes only at infinity, or when the connected components of the density satisfy these conditions, our axiomatic definition results in Hartigan's definition of cluster tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03574v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ery Arias-Castro, Elizabeth Coda</dc:creator>
    </item>
    <item>
      <title>When can weak latent factors be statistically inferred?</title>
      <link>https://arxiv.org/abs/2407.03616</link>
      <description>arXiv:2407.03616v1 Announce Type: cross 
Abstract: This article establishes a new and comprehensive estimation and inference theory for principal component analysis (PCA) under the weak factor model that allow for cross-sectional dependent idiosyncratic components under nearly minimal the factor strength relative to the noise level or signal-to-noise ratio. Our theory is applicable regardless of the relative growth rate between the cross-sectional dimension $N$ and temporal dimension $T$. This more realistic assumption and noticeable result requires completely new technical device, as the commonly-used leave-one-out trick is no longer applicable to the case with cross-sectional dependence. Another notable advancement of our theory is on PCA inference $ - $ for example, under the regime where $N\asymp T$, we show that the asymptotic normality for the PCA-based estimator holds as long as the signal-to-noise ratio (SNR) grows faster than a polynomial rate of $\log N$. This finding significantly surpasses prior work that required a polynomial rate of $N$. Our theory is entirely non-asymptotic, offering finite-sample characterizations for both the estimation error and the uncertainty level of statistical inference. A notable technical innovation is our closed-form first-order approximation of PCA-based estimator, which paves the way for various statistical tests. Furthermore, we apply our theories to design easy-to-implement statistics for validating whether given factors fall in the linear spans of unknown latent factors, testing structural breaks in the factor loadings for an individual unit, checking whether two units have the same risk exposures, and constructing confidence intervals for systematic risks. Our empirical studies uncover insightful correlations between our test results and economic cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03616v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yuling Yan, Yuheng Zheng</dc:creator>
    </item>
    <item>
      <title>Asymmetric Iterated Prisoner's Dilemma on BA Scale-Free Network</title>
      <link>https://arxiv.org/abs/2407.03904</link>
      <description>arXiv:2407.03904v1 Announce Type: cross 
Abstract: In real-world scenarios, individuals often cooperate for mutual benefit. However, differences in wealth can lead to varying outcomes for similar actions. In complex social networks, individuals' choices are also influenced by their neighbors. To explore the evolution of strategies in realistic settings, we conducted repeated asymmetric prisoners dilemma experiments on a weighted BA scale-free network. Our analysis highlighted how the four components of memory-one strategies affect win rates, found two special strategies in the evolutionary process, and increased the cooperation levels among individuals. These findings offer practical insights for addressing real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03904v1</guid>
      <category>physics.soc-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Ding, Chunyan Zhang, Jianlei Zhang</dc:creator>
    </item>
    <item>
      <title>Statistics for Phylogenetic Trees in the Presence of Stickiness</title>
      <link>https://arxiv.org/abs/2407.03977</link>
      <description>arXiv:2407.03977v1 Announce Type: cross 
Abstract: Samples of phylogenetic trees arise in a variety of evolutionary and biomedical applications, and the Fr\'echet mean in Billera-Holmes-Vogtmann tree space is a summary tree shown to have advantages over other mean or consensus trees. However, use of the Fr\'echet mean raises computational and statistical issues which we explore in this paper. The Fr\'echet sample mean is known often to contain fewer internal edges than the trees in the sample, and in this circumstance calculating the mean by iterative schemes can be problematic due to slow convergence. We present new methods for identifying edges which must lie in the Fr\'echet sample mean and apply these to a data set of gene trees relating organisms from the apicomplexa which cause a variety of parasitic infections. When a sample of trees contains a significant level of heterogeneity in the branching patterns, or topologies, displayed by the trees then the Fr\'echet mean is often a star tree, lacking any internal edges. Not only in this situation, the population Fr\'echet mean is affected by a non-Euclidean phenomenon called stickness which impacts upon asymptotics, and we examine two data sets for which the mean tree is a star tree. The first consists of trees representing the physical shape of artery structures in a sample of medical images of human brains in which the branching patterns are very diverse. The second consists of gene trees from a population of baboons in which there is evidence of substantial hybridization. We develop hypothesis tests which work in the presence of stickiness. The first is a test for the presence of a given edge in the Fr\'echet population mean; the second is a two-sample test for differences in two distributions which share the same sticky population mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03977v1</guid>
      <category>q-bio.PE</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars Lammers, Tom M. W. Nye, Stephan F. Huckemann</dc:creator>
    </item>
    <item>
      <title>Meta-Learning and representation learner: A short theoretical note</title>
      <link>https://arxiv.org/abs/2407.04189</link>
      <description>arXiv:2407.04189v1 Announce Type: cross 
Abstract: Meta-learning, or "learning to learn," is a subfield of machine learning where the goal is to develop models and algorithms that can learn from various tasks and improve their learning process over time. Unlike traditional machine learning methods focusing on learning a specific task, meta-learning aims to leverage experience from previous tasks to enhance future learning. This approach is particularly beneficial in scenarios where the available data for a new task is limited, but there exists abundant data from related tasks. By extracting and utilizing the underlying structure and patterns across these tasks, meta-learning algorithms can achieve faster convergence and better performance with fewer data. The following notes are mainly inspired from \cite{vanschoren2018meta}, \cite{baxter2019learning}, and \cite{maurer2005algorithmic}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04189v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mouad El Bouchattaoui</dc:creator>
    </item>
    <item>
      <title>Linear causal disentanglement via higher-order cumulants</title>
      <link>https://arxiv.org/abs/2407.04605</link>
      <description>arXiv:2407.04605v1 Announce Type: cross 
Abstract: Linear causal disentanglement is a recent method in causal representation learning to describe a collection of observed variables via latent variables with causal dependencies between them. It can be viewed as a generalization of both independent component analysis and linear structural equation models. We study the identifiability of linear causal disentanglement, assuming access to data under multiple contexts, each given by an intervention on a latent variable. We show that one perfect intervention on each latent variable is sufficient and in the worst case necessary to recover parameters under perfect interventions, generalizing previous work to allow more latent than observed variables. We give a constructive proof that computes parameters via a coupled tensor decomposition. For soft interventions, we find the equivalence class of latent graphs and parameters that are consistent with observed data, via the study of a system of polynomial equations. Our results hold assuming the existence of non-zero higher-order cumulants, which implies non-Gaussianity of variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04605v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paula Leyes Carreno, Chiara Meroni, Anna Seigal</dc:creator>
    </item>
    <item>
      <title>High-dimensional variable clustering based on maxima of a weakly dependent random process</title>
      <link>https://arxiv.org/abs/2302.00934</link>
      <description>arXiv:2302.00934v3 Announce Type: replace 
Abstract: We propose a new class of models for variable clustering called Asymptotic Independent block (AI-block) models, which defines population-level clusters based on the independence of the maxima of a multivariate stationary mixing random process among clusters. This class of models is identifiable, meaning that there exists a maximal element with a partial order between partitions, allowing for statistical inference. We also present an algorithm depending on a tuning parameter that recovers the clusters of variables without specifying the number of clusters \emph{a priori}. Our work provides some theoretical insights into the consistency of our algorithm, demonstrating that under certain conditions it can effectively identify clusters in the data with a computational complexity that is polynomial in the dimension. A data-driven selection method for the tuning parameter is also proposed. To further illustrate the significance of our work, we applied our method to neuroscience and environmental real-datasets. These applications highlight the potential and versatility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00934v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexis Boulin, Elena Di Bernardino, Thomas Lalo\"e, Gwladys Toulemonde</dc:creator>
    </item>
    <item>
      <title>Parametric Estimation of Tempered Stable Laws</title>
      <link>https://arxiv.org/abs/2303.07060</link>
      <description>arXiv:2303.07060v4 Announce Type: replace 
Abstract: Tempered stable distributions are frequently used in financial applications (e.g., for option pricing) in which the tails of stable distributions would be too heavy. Given the non-explicit form of the probability density function, estimation relies on numerical algorithms which typically are time-consuming. We compare several parametric estimation methods such as the maximum likelihood method and different generalized method of moment approaches. We study large sample properties and derive consistency, asymptotic normality, and asymptotic efficiency results for our estimators. Additionally, we conduct simulation studies to analyze finite sample properties measured by the empirical bias, precision, and asymptotic confidence interval coverage rates and compare computational costs. We cover relevant subclasses of tempered stable distributions such as the classical tempered stable distribution and the tempered stable subordinator. Moreover, we discuss the normal tempered stable distribution which arises by subordinating a Brownian motion with a tempered stable subordinator. Our financial applications to log returns of asset indices and to energy spot prices illustrate the benefits of tempered stable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07060v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Till Massing</dc:creator>
    </item>
    <item>
      <title>Bootstrap-Assisted Inference for Generalized Grenander-type Estimators</title>
      <link>https://arxiv.org/abs/2303.13598</link>
      <description>arXiv:2303.13598v3 Announce Type: replace 
Abstract: Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust'' in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13598v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Michael Jansson, Kenichi Nagasawa</dc:creator>
    </item>
    <item>
      <title>Optimal transport and Wasserstein distances for causal models</title>
      <link>https://arxiv.org/abs/2303.14085</link>
      <description>arXiv:2303.14085v2 Announce Type: replace 
Abstract: In this paper, we introduce a variant of optimal transport adapted to the causal structure given by an underlying directed graph $G$. Different graph structures lead to different specifications of the optimal transport problem. For instance, a fully connected graph yields standard optimal transport, a linear graph structure corresponds to causal optimal transport between the distributions of two discrete-time stochastic processes, and an empty graph leads to a notion of optimal transport related to CO-OT, Gromov-Wasserstein distances and factored OT. We derive different characterizations of $G$-causal transport plans and introduce Wasserstein distances between causal models that respect the underlying graph structure. We show that average treatment effects are continuous with respect to $G$-causal Wasserstein distances and small perturbations of structural causal models lead to small deviations in $G$-causal Wasserstein distance. We also introduce an interpolation between causal models based on $G$-causal Wasserstein distance and compare it to standard Wasserstein interpolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14085v2</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Cheridito, Stephan Eckstein</dc:creator>
    </item>
    <item>
      <title>Estimating Max-Stable Random Vectors with Discrete Spectral Measure using Model-Based Clustering</title>
      <link>https://arxiv.org/abs/2402.01609</link>
      <description>arXiv:2402.01609v3 Announce Type: replace 
Abstract: This study introduces a novel estimation method for the entries and structure of a matrix $A$ in the linear factor model $\textbf{X} = A\textbf{Z} + \textbf{E}$. This is applied to an observable vector $\textbf{X} \in \mathbb{R}^d$ with $\textbf{Z} \in \mathbb{R}^K$, a vector composed of independently regularly varying random variables, and independent lighter tail noise $\textbf{E} \in \mathbb{R}^d$. This leads to max-linear models treated in classical multivariate extreme value theory. The spectral of the limit distribution is subsequently discrete and completely characterised by the matrix $A$. Every max-stable random vector with discrete spectral measure can be written as a max-linear model. Each row of the matrix $A$ is supposed to be both scaled and sparse. Additionally, the value of $K$ is not known a priori. The problem of identifying the matrix $A$ from its matrix of pairwise extremal correlation is addressed. In the presence of pure variables, which are elements of $\textbf{X}$ linked, through $A$, to a single latent factor, the matrix $A$ can be reconstructed from the extremal correlation matrix. Our proofs of identifiability are constructive and pave the way for our innovative estimation for determining the number of factors $K$ and the matrix $A$ from $n$ weakly dependent observations on $\textbf{X}$. We apply the suggested method to weekly maxima rainfall and wildfires to illustrate its applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01609v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexis Boulin</dc:creator>
    </item>
    <item>
      <title>Sampling from the Mean-Field Stationary Distribution</title>
      <link>https://arxiv.org/abs/2402.07355</link>
      <description>arXiv:2402.07355v4 Announce Type: replace 
Abstract: We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term. Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime. A key technical contribution is to establish a new uniform-in-$N$ log-Sobolev inequality for the stationary distribution of the mean-field Langevin dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07355v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunbum Kook, Matthew S. Zhang, Sinho Chewi, Murat A. Erdogdu, Mufan Bill Li</dc:creator>
    </item>
    <item>
      <title>Cyclical Long Memory: Decoupling, Modulation, and Modeling</title>
      <link>https://arxiv.org/abs/2403.07170</link>
      <description>arXiv:2403.07170v3 Announce Type: replace 
Abstract: A new model for general cyclical long memory is introduced, by means of random modulation of certain bivariate long memory time series. This construction essentially decouples the two key features of cyclical long memory: quasi-periodicity and long-term persistence. It further allows for a general cyclical phase in cyclical long memory time series. Several choices for suitable bivariate long memory series are discussed, including a parametric fractionally integrated vector ARMA model. The parametric models introduced in this work have explicit autocovariance functions that can be used readily in simulation, estimation, and other tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07170v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefanos Kechagias, Vladas Pipiras, Pavlos Zoubouloglou</dc:creator>
    </item>
    <item>
      <title>High-probability minimax lower bounds</title>
      <link>https://arxiv.org/abs/2406.13447</link>
      <description>arXiv:2406.13447v2 Announce Type: replace 
Abstract: The minimax risk is often considered as a gold standard against which we can compare specific statistical procedures. Nevertheless, as has been observed recently in robust and heavy-tailed estimation problems, the inherent reduction of the (random) loss to its expectation may entail a significant loss of information regarding its tail behaviour. In an attempt to avoid such a loss, we introduce the notion of a minimax quantile, and seek to articulate its dependence on the quantile level. To this end, we develop high-probability variants of the classical Le Cam and Fano methods, as well as a technique to convert local minimax risk lower bounds to lower bounds on minimax quantiles. To illustrate the power of our framework, we deploy our techniques on several examples, recovering recent results in robust mean estimation and stochastic convex optimisation, as well as obtaining several new results in covariance matrix estimation, sparse linear regression, nonparametric density estimation and isotonic regression. Our overall goal is to argue that minimax quantiles can provide a finer-grained understanding of the difficulty of statistical problems, and that, in wide generality, lower bounds on these quantities can be obtained via user-friendly tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13447v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Kabir A. Verchand, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test</title>
      <link>https://arxiv.org/abs/2406.18397</link>
      <description>arXiv:2406.18397v2 Announce Type: replace 
Abstract: In this article, we introduce the novel concept of the second maximum of a Gaussian random field on a Riemannian submanifold. This second maximum serves as a powerful tool for characterizing the distribution of the maximum. By utilizing an ad-hoc Kac Rice formula, we derive the explicit form of the maximum's distribution, conditioned on the second maximum and some regressed component of the Riemannian Hessian. This approach results in an exact test, based on the evaluation of spacing between these maxima, which we refer to as the spacing test.
  We investigate the applicability of this test in detecting sparse alternatives within Gaussian symmetric tensors, continuous sparse deconvolution, and two-layered neural networks with smooth rectifiers. Our theoretical results are supported by numerical experiments, which illustrate the calibration and power of the proposed tests. More generally, this test can be applied to any Gaussian random field on a Riemannian manifold, and we provide a general framework for the application of the spacing test in continuous sparse kernel regression.
  Furthermore, when the variance-covariance function of the Gaussian random field is known up to a scaling factor, we derive an exact Studentized version of our test, coined the $t$-spacing test. This test is perfectly calibrated under the null hypothesis and has high power for detecting sparse alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18397v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jean-Marc Aza\"is, Federico Dalmao, Yohann De Castro</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Riesz Representers: Generalization, Mis-specification, and the Counterfactual Effective Dimension</title>
      <link>https://arxiv.org/abs/2102.11076</link>
      <description>arXiv:2102.11076v4 Announce Type: replace-cross 
Abstract: Kernel balancing weights provide confidence intervals for average treatment effects, based on the idea of balancing covariates for the treated group and untreated group in feature space, often with ridge regularization. Previous works on the classical kernel ridge balancing weights have certain limitations: (i) not articulating generalization error for the balancing weights, (ii) typically requiring correct specification of features, and (iii) justifying Gaussian approximation for only average effects.
  I interpret kernel balancing weights as kernel ridge Riesz representers (KRRR) and address these limitations via a new characterization of the counterfactual effective dimension. KRRR is an exact generalization of kernel ridge regression and kernel ridge balancing weights. I prove strong properties similar to kernel ridge regression: population $L_2$ rates controlling generalization error, and a standalone closed form solution that can interpolate. The framework relaxes the stringent assumption that the underlying regression model is correctly specified by the features. It extends Gaussian approximation beyond average effects to heterogeneous effects, justifying confidence sets for causal functions. I use KRRR to quantify uncertainty for heterogeneous treatment effects, by age, of 401(k) eligibility on assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.11076v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh</dc:creator>
    </item>
    <item>
      <title>The $s$-value: evaluating stability with respect to distributional shifts</title>
      <link>https://arxiv.org/abs/2105.03067</link>
      <description>arXiv:2105.03067v4 Announce Type: replace-cross 
Abstract: Common statistical measures of uncertainty such as $p$-values and confidence intervals quantify the uncertainty due to sampling, that is, the uncertainty due to not observing the full population. However, sampling is not the only source of uncertainty. In practice, distributions change between locations and across time. This makes it difficult to gather knowledge that transfers across data sets. We propose a measure of instability that quantifies the distributional instability of a statistical parameter with respect to Kullback-Leibler divergence, that is, the sensitivity of the parameter under general distributional perturbations within a Kullback-Leibler divergence ball. In addition, we quantify the instability of parameters with respect to directional or variable-specific shifts. Measuring instability with respect to directional shifts can be used to detect the type of shifts a parameter is sensitive to. We discuss how such knowledge can inform data collection for improved estimation of statistical parameters under shifted distributions. We evaluate the performance of the proposed measure on real data and show that it can elucidate the distributional instability of a parameter with respect to certain shifts and can be used to improve estimation accuracy under shifted distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03067v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suyash Gupta, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Channel State Acquisition in Uplink NOMA for Cellular-Connected UAV: Exploitation of Doppler and Modulation Diversities</title>
      <link>https://arxiv.org/abs/2108.06713</link>
      <description>arXiv:2108.06713v3 Announce Type: replace-cross 
Abstract: Integration of unmanned aerial vehicles (UAVs) for surveillance or monitoring applications into fifth generation (5G) New Radio (NR) cellular networks is an intriguing problem that has recently tackled a lot of interest in both academia and industry. For an efficient spectrum usage, we consider a recently-proposed sky-ground nonorthogonal multiple access (NOMA) scheme, where a cellular-connected UAV acting as aerial user (AU) and a static terrestrial user (TU) are paired to simultaneously transmit their uplink signals to a base station (BS) in the same time-frequency resource blocks. In such a case, due to the highly dynamic nature of the UAV, the signal transmitted by the AU experiences both time dispersion due to multipath propagation effects and frequency dispersion caused by Doppler shifts. On the other hand, for a static ground network, frequency dispersion of the signal transmitted by the TU is negligible and only multipath effects have to be taken into account. To decode the superposed signals at the BS through successive interference cancellation, accurate estimates of both the AU and TU channels are needed. In this paper, we propose channel estimation procedures that suitably exploit the different circular/noncircular modulation formats (modulation diversity) and the different almost-cyclostationarity features (Doppler diversity) of the AU and TU by means of widely-linear time-varying processing. Our estimation approach is semi-blind since Doppler shifts and time delays of the AU are estimated based on the received data only, whereas the remaining relevant parameters of the AU and TU channels are acquired relying also on the available training symbols, which are transmitted by the AU and TU in a nonorthogonal manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.06713v3</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donatella Darsena, Ivan Iudice, Francesco Verde</dc:creator>
    </item>
    <item>
      <title>Robust Sparse Mean Estimation via Sum of Squares</title>
      <link>https://arxiv.org/abs/2206.03441</link>
      <description>arXiv:2206.03441v2 Announce Type: replace-cross 
Abstract: We study the problem of high-dimensional sparse mean estimation in the presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\mathbb R^d$ with "certifiably bounded" $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m = (k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$ with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our algorithms follow the Sum-of-Squares based, proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively the best possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03441v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering</title>
      <link>https://arxiv.org/abs/2206.05245</link>
      <description>arXiv:2206.05245v2 Announce Type: replace-cross 
Abstract: We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\alpha \in (0, 1/2)$, we are given $m$ points in $\mathbb{R}^n$, $\lfloor \alpha m \rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\widehat \mu$ such that $\| \widehat \mu - \mu \|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with "certifiably bounded" $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\alpha)^{O(1/t)}$ with sample complexity $m = (k\log(n))^{O(t)}/\alpha$ and running time $\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee of $\Theta (\sqrt{\log(1/\alpha)})$ with quasi-polynomial sample and computational complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05245v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>Quantitative limit theorems and bootstrap approximations for empirical spectral projectors</title>
      <link>https://arxiv.org/abs/2208.12871</link>
      <description>arXiv:2208.12871v3 Announce Type: replace-cross 
Abstract: Given finite i.i.d.~samples in a Hilbert space with zero mean and trace-class covariance operator $\Sigma$, the problem of recovering the spectral projectors of $\Sigma$ naturally arises in many applications. In this paper, we consider the problem of finding distributional approximations of the spectral projectors of the empirical covariance operator $\hat \Sigma$, and offer a dimension-free framework where the complexity is characterized by the so-called relative rank of $\Sigma$. In this setting, novel quantitative limit theorems and bootstrap approximations are presented subject only to mild conditions in terms of moments and spectral decay. In many cases, these even improve upon existing results in a Gaussian setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12871v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Jirak, Martin Wahl</dc:creator>
    </item>
    <item>
      <title>Convergence of flow-based generative models via proximal gradient descent in Wasserstein space</title>
      <link>https://arxiv.org/abs/2310.17582</link>
      <description>arXiv:2310.17582v3 Announce Type: replace-cross 
Abstract: Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condition. The assumption on data density is merely a finite second moment, and the theory extends to data distributions without density and when there are inversion errors in the reverse process where we obtain KL-$W_2$ mixed error guarantees. The non-asymptotic convergence rate of the JKO-type $W_2$-proximal GD is proved for a general class of convex objective functionals that includes the KL divergence as a special case, which can be of independent interest. The analysis framework can extend to other first-order Wasserstein optimization schemes applied to flow-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17582v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiuyuan Cheng, Jianfeng Lu, Yixin Tan, Yao Xie</dc:creator>
    </item>
    <item>
      <title>A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models</title>
      <link>https://arxiv.org/abs/2401.07187</link>
      <description>arXiv:2401.07187v2 Announce Type: replace-cross 
Abstract: In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. In the last part, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs). The former two models are known to be the main pillars of the modern generative AI era, while ICL is a strong capability of LLMs in learning from a few examples in the context. Finally, we conclude the paper by suggesting several promising directions for deep learning theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07187v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Namjoon Suh, Guang Cheng</dc:creator>
    </item>
    <item>
      <title>Rates of Convergence of the Magnetization in the Tensor Curie-Weiss Potts Model</title>
      <link>https://arxiv.org/abs/2406.15907</link>
      <description>arXiv:2406.15907v2 Announce Type: replace-cross 
Abstract: In this paper, we derive distributional convergence rates for the magnetization vector and the maximum pseudolikelihood estimator of the inverse temperature parameter in the tensor Curie-Weiss Potts model. Limit theorems for the magnetization vector have been derived recently in Bhowal and Mukherjee (2023), where several phase transition phenomena in terms of the scaling of the (centered) magnetization and its asymptotic distribution were established, depending upon the position of the true parameters in the parameter space. In the current work, we establish Berry-Esseen type results for the magnetization vector, specifying its rate of convergence at these different phases. At most points in the parameter space, this rate is $N^{-1/2}$ ($N$ being the size of the Curie-Weiss network), while at some "special" points, the rate is either $N^{-1/4}$ or $N^{-1/6}$, depending upon the behavior of the fourth derivative of a certain "negative free energy function" at these special points. These results are then used to derive Berry-Esseen type bounds for the maximum pseudolikelihood estimator of the inverse temperature parameter whenever it lies above a certain criticality threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15907v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanchayan Bhowal, Somabha Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
