<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Social Cost of Greenhouse Gases -- OPTiMEM and the Heat Conjecture(s)</title>
      <link>https://arxiv.org/abs/2601.06085</link>
      <description>arXiv:2601.06085v1 Announce Type: new 
Abstract: Despite well-meaning scenarios that propose global CO2 emissions will decline presented in every IPCC report since 1988, the trend of global CO2 increase continues without significant change. Even if any individual nation manages to flatten its emissions, what matters is the trajectory of the globe. Together the gulf between climate science and climate economics, plus the urgent need for alternative methods of estimation, provided the incentives for development of our Ocean-Heat-Content (OHC) Physics and Time Macro Economic Model (OPTiMEM) system.
  To link NOAA damages to climate required creating a carbon consumption model to drive a physics model of climate. How fast could carbon be burned and how much coal, oil and natural gas was reasonably available? A carbon model driving climate meant burning the carbon, and modelling how the earth heated up. We developed this using the most recent best greenhouse gas equations and production models for CO2, CH4, N2O, and halogenated gases. This developed an ocean heat content model for the globe. Each step is validated against Known carbon consumption, CO2, temperature, and ocean heat content. This allows a physics founded model of climate costs to be projected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06085v1</guid>
      <category>econ.TH</category>
      <category>physics.ao-ph</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brian P. Hanley (Butterfly Sciences), Pieter Tans (Institute of Arctic and Alpine Research University of Colorado Boulder), Edward A. G. Schuur (Center for Ecosystem Science and Society Northern Arizona University), Geoffrey Gardiner (London Institute of Banking and Finance), Adam Smith (Climate Central)</dc:creator>
    </item>
    <item>
      <title>The Replicator-Optimization Mechanism: A Scale-Relative Formalism for Persistence-Conditioned Dynamics with Application to Consent-Based Metaethics</title>
      <link>https://arxiv.org/abs/2601.06363</link>
      <description>arXiv:2601.06363v1 Announce Type: new 
Abstract: This paper formalizes a widely used dynamical class--replicator-mutator dynamics and Price-style selection-and-transmission--and makes explicit the modeling choices (scale, atomic unit, interaction topology, transmission kernel) that determine how this class instantiates across domains. The backbone is known; we do not claim to have discovered selection. The novel contributions are threefold: (i) a scale-relative kernel parameterization where atomic units are themselves parameters, enabling systematic instantiation across physics, biology, economics, cognition, and social organization; (ii) a consent-friction instantiation for political philosophy, where friction is the primitive, legitimacy functions as survival probability, and belief-transfer functions as mutation kernel; and (iii) a derivation path from social contract theory rather than from biology or physics, arriving at the same formal structure via an independent route.
  We provide a bridge principle connecting descriptive dynamics to instrumental normativity: if agents prefer lower expected friction, then "ought" claims are shorthand for policies that reduce expected friction under the specified dynamics. This conditional structure avoids the is-ought fallacy while grounding normative discourse in empirically tractable dynamics. We address pathological cases (authoritarian stability, suppressed friction) through explicit modeling of latent versus observed friction. The framework generates testable predictions through operationalization of friction, legitimacy, and belief-transfer dynamics, and is falsifiable at the level of measurement apparatus rather than formal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06363v1</guid>
      <category>econ.TH</category>
      <category>cs.MA</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murad Farzulla</dc:creator>
    </item>
    <item>
      <title>Bounded Rationality with Subjective Evaluations in Enlivened but Truncated Decision Trees</title>
      <link>https://arxiv.org/abs/2601.06405</link>
      <description>arXiv:2601.06405v1 Announce Type: new 
Abstract: In normative models a decision-maker is usually assumed to be Bayesian rational, and so to maximize subjective expected utility, within a complete and correctly specified decision model. Following the discussion in Hammond (2007) of Schumpeter's (1911, 1934) concept of entrepreneurship, as well as Shackle's (1953) concept of potential surprise, we consider enlivened decision trees whose growth over time cannot be accurately modelled in full detail. An enlivened decision tree involves more severe limitations than a mis-specified model, unforeseen contingencies, or unawareness, all of which are typically modelled with reference to a universal state space large enough to encompass any decision model that an agent may consider. We consider a motivating example based on Homer's classic tale of Odysseus and the Sirens. Though our novel framework transcends standard notions of risk or uncertainty, for finite decision trees that may be truncated because of bounded rationality, an extended and refined form of Bayesian rationality is still possible, with real-valued subjective evaluations instead of consequences attached to terminal nodes where truncations occur. Moreover, these subjective evaluations underlie, for example, the kind of Monte Carlo tree search algorithm used by recent chess-playing software packages. They may also help rationalize the contentious precautionary principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06405v1</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Peter J. Hammond</dc:creator>
    </item>
    <item>
      <title>A Note on 'The Limits of Price Discrimination' by Bergemann, Brooks, and Morris</title>
      <link>https://arxiv.org/abs/2601.07452</link>
      <description>arXiv:2601.07452v1 Announce Type: new 
Abstract: This note revisits the analysis of third-degree price discrimination developed by Bergemann et al. (2015), which characterizes the set of consumer-producer surplus pairs that can be achieved through market segmentation. This was proved by means of market segmentation with random prices, but it was claimed that any segmentation with possibly random pricing has a corresponding direct segmentation, where a deterministic price is charged in each market segment. However, the latter claim is not correct under the definition of market segmentation given in the paper, and we provide counterexamples. We then propose an alternative definition to resolve this issue and examine the implications of the difference between the two definitions in terms of the main result of their paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07452v1</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keita Kuwahara</dc:creator>
    </item>
    <item>
      <title>A Model of Artificial Jagged Intelligence</title>
      <link>https://arxiv.org/abs/2601.07573</link>
      <description>arXiv:2601.07573v1 Announce Type: new 
Abstract: Generative AI systems often display highly uneven performance across tasks that appear ``nearby'': they can be excellent on one prompt and confidently wrong on another with only small changes in wording or context. We call this phenomenon Artificial Jagged Intelligence (AJI). This paper develops a tractable economic model of AJI that treats adoption as an information problem: users care about \emph{local} reliability, but typically observe only coarse, global quality signals. In a baseline one-dimensional landscape, truth is a rough Brownian process, and the model ``knows'' scattered points drawn from a Poisson process. The model interpolates optimally, and the local error is measured by posterior variance. We derive an adoption threshold for a blind user, show that experienced errors are amplified by the inspection paradox, and interpret scaling laws as denser coverage that improves average quality without eliminating jaggedness. We then study mastery and calibration: a calibrated user who can condition on local uncertainty enjoys positive expected value even in domains that fail the blind adoption test. Modelling mastery as learning a reliability map via Gaussian process regression yields a learning-rate bound driven by information gain, clarifying when discovering ``where the model works'' is slow. Finally, we study how scaling interacts with discoverability: when calibrated signals and user mastery accelerate the harvesting of scale improvements, and when opacity can make gains from scaling effectively invisible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07573v1</guid>
      <category>econ.TH</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Gans</dc:creator>
    </item>
    <item>
      <title>Coalition Tactics: Bribery and Control in Parliamentary Elections</title>
      <link>https://arxiv.org/abs/2601.07279</link>
      <description>arXiv:2601.07279v1 Announce Type: cross 
Abstract: Strategic manipulation of elections is typically studied in the context of promoting individual candidates.
  In parliamentary elections, however, the focus shifts: voters may care more about the overall governing coalition than the individual parties' seat counts.
  This paper studies this new problem: manipulating parliamentary elections with the goal of promoting the collective seat count of a coalition of parties.
  We focus on proportional representation elections, and consider two variants of the problem; one in which the sole goal is to maximize the total number of seats held by the desired coalition, and the other with a dual objective of both promoting the coalition and promoting the relative power of some favorite party within the coalition.
  We examine two types of strategic manipulations:
  \emph{bribery}, which allows modifying voters' preferences, and \emph{control}, which allows
  changing the sets of voters and parties.
  We consider multiple bribery types, presenting polynomial-time algorithms for some, while proving NP-hardness for others.
  For control, we provide polynomial-time algorithms for control by adding and deleting voters. In contrast, control by adding and deleting parties, we show, is either impossible (i.e., the problem is immune to control) or computationally hard, in particular, W[1]-hard when parameterized by the number of parties that can be added or deleted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07279v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hodaya Barr, Eden Hartman, Yonatan Aumann, Sarit Kraus</dc:creator>
    </item>
    <item>
      <title>Condorcet's Paradox as Non-Orientability</title>
      <link>https://arxiv.org/abs/2601.07283</link>
      <description>arXiv:2601.07283v1 Announce Type: cross 
Abstract: Preference cycles are prevalent in problems of decision-making, and are contradictory when preferences are assumed to be transitive. This contradiction underlies Condorcet's Paradox, a pioneering result of Social Choice Theory, wherein intuitive and seemingly desirable constraints on decision-making necessarily lead to contradictory preference cycles. Topological methods have since broadened Social Choice Theory and elucidated existing results. However, characterisations of preference cycles in Topological Social Choice Theory are lacking. In this paper, we address this gap by introducing a framework for topologically modelling preference cycles that generalises Baryshnikov's existing topological model of strict, ordinal preferences on 3 alternatives. In our framework, the contradiction underlying Condorcet's Paradox topologically corresponds to the non-orientability of a surface homeomorphic to either the Klein Bottle or Real Projective Plane, depending on how preference cycles are represented. These findings allow us to reduce Arrow's Impossibility Theorem to a statement about the orientability of a surface. Furthermore, these results contribute to existing wide-ranging interest in the relationship between non-orientability, impossibility phenomena in Economics, and logical paradoxes more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07283v1</guid>
      <category>math.AT</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ori Livson, Siddharth Pritam, Mikhail Prokopenko</dc:creator>
    </item>
    <item>
      <title>Does Ideological Polarization Lead to Policy Polarization?</title>
      <link>https://arxiv.org/abs/2502.14712</link>
      <description>arXiv:2502.14712v5 Announce Type: replace 
Abstract: I study an election between two ideologically polarized parties that are both office- and policy-motivated. The parties compete by proposing policies on a single issue. The analysis uncovers a non-monotonic relationship between ideological and policy polarization. When ideological polarization is low, an increase leads to policy moderation; when it is high, the opposite occurs, and policies become more extreme. Moreover, incorporating ideological polarization refines our understanding of the role of valence: both high- and low-valence candidates may adopt more extreme positions, depending on the electorate's degree of ideological polarization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14712v5</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Denter</dc:creator>
    </item>
  </channel>
</rss>
