<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 01:43:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revealing the temporal dynamics of antibiotic anomalies in the infant gut microbiome with neural jump ODEs</title>
      <link>https://arxiv.org/abs/2510.00087</link>
      <description>arXiv:2510.00087v1 Announce Type: new 
Abstract: Detecting anomalies in irregularly sampled multi-variate time-series is challenging, especially in data-scarce settings. Here we introduce an anomaly detection framework for irregularly sampled time-series that leverages neural jump ordinary differential equations (NJODEs). The method infers conditional mean and variance trajectories in a fully path dependent way and computes anomaly scores. On synthetic data containing jump, drift, diffusion, and noise anomalies, the framework accurately identifies diverse deviations. Applied to infant gut microbiome trajectories, it delineates the magnitude and persistence of antibiotic-induced disruptions: revealing prolonged anomalies after second antibiotic courses, extended duration treatments, and exposures during the second year of life. We further demonstrate the predictive capabilities of the inferred anomaly scores in accurately predicting antibiotic events and outperforming diversity-based baselines. Our approach accommodates unevenly spaced longitudinal observations, adjusts for static and dynamic covariates, and provides a foundation for inferring microbial anomalies induced by perturbations, offering a translational opportunity to optimize intervention regimens by minimizing microbial disruptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00087v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Adamov, Markus Chardonnet, Florian Krach, Jakob Heiss, Josef Teichmann, Nicholas A. Bokulich</dc:creator>
    </item>
    <item>
      <title>Bayesian Cox model with graph-structured variable selection priors for multi-omics biomarker identification</title>
      <link>https://arxiv.org/abs/2503.13078</link>
      <description>arXiv:2503.13078v1 Announce Type: cross 
Abstract: An important goal in cancer research is the survival prognosis of a patient based on a minimal panel of genomic and molecular markers such as genes or proteins. Purely data-driven models without any biological knowledge can produce non-interpretable results. We propose a penalized semiparametric Bayesian Cox model with graph-structured selection priors for sparse identification of multi-omics features by making use of a biologically meaningful graph via a Markov random field (MRF) prior to capturing known relationships between multi-omics features. Since the fixed graph in the MRF prior is for the prior probability distribution, it is not a hard constraint to determine variable selection, so the proposed model can verify known information and has the potential to identify new and novel biomarkers for drawing new biological knowledge. Our simulation results show that the proposed Bayesian Cox model with graph-based prior knowledge results in more trustable and stable variable selection and non-inferior survival prediction, compared to methods modeling the covariates independently without any prior knowledge. The results also indicate that the performance of the proposed model is robust to a partially correct graph in the MRF prior, meaning that in a real setting where not all the true network information between covariates is known, the graph can still be useful. The proposed model is applied to the primary invasive breast cancer patients data in The Cancer Genome Atlas project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13078v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias {\O}stmo Hermansen, Manuela Zucknick, Zhi Zhao</dc:creator>
    </item>
    <item>
      <title>Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment</title>
      <link>https://arxiv.org/abs/2510.00048</link>
      <description>arXiv:2510.00048v1 Announce Type: cross 
Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00048v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fahad Mostafa, Kannon Hossain, Hafiz Khan</dc:creator>
    </item>
    <item>
      <title>Proximity-based cities emit less mobility-driven CO$_2$</title>
      <link>https://arxiv.org/abs/2510.00094</link>
      <description>arXiv:2510.00094v1 Announce Type: cross 
Abstract: In the quest for more environmentally sustainable urban areas, the concept of the 15-minute city has been proposed to encourage active mobility, primarily through walking and cycling. An urban area is considered a ``15-minute city" if every resident can access essential services within a 15-minute walk or bike ride from their home. However, there is an ongoing debate about the effectiveness of this model in reducing car usage and carbon emissions. In this study, we conduct a large-scale data-driven analysis to evaluate the impact of service proximity to homes on CO$_2$ emissions. By examining nearly 400 cities worldwide, we discover that, within the same city, areas with services located closer to residents produce less CO$_2$ emissions per capita from transportation. We establish a clear relationship between the proximity of services and CO$_2$ emissions for each city. Additionally, we quantify the potential reduction in emissions for 30 cities if they optimise the location of their services. This optimisation maintains each city's total number of services while redistributing them to ensure equal accessibility throughout the entire urban area. Our findings indicate that improving the proximity of services can significantly reduce expected urban emissions related to transportation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00094v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Marzolla, Matteo Bruno, Hygor P. M. Melo, Vittorio Loreto</dc:creator>
    </item>
    <item>
      <title>Structural Refinement of Bayesian Networks for Efficient Model Parameterisation</title>
      <link>https://arxiv.org/abs/2510.00334</link>
      <description>arXiv:2510.00334v1 Announce Type: cross 
Abstract: Many Bayesian network modelling applications suffer from the issue of data scarcity. Hence the use of expert judgement often becomes necessary to determine the parameters of the conditional probability tables (CPTs) throughout the network. There are usually a prohibitively large number of these parameters to determine, even when complementing any available data with expert judgements. To address this challenge, a number of CPT approximation methods have been developed that reduce the quantity and complexity of parameters needing to be determined to fully parameterise a Bayesian network. This paper provides a review of a variety of structural refinement methods that can be used in practice to efficiently approximate a CPT within a Bayesian network. We not only introduce and discuss the intrinsic properties and requirements of each method, but we evaluate each method through a worked example on a Bayesian network model of cardiovascular risk assessment. We conclude with practical guidance to help Bayesian network practitioners choose an alternative approach when direct parameterisation of a CPT is infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00334v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Drury, Martine J. Barons, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>The Multivariate SEM-PGS Model: Using Polygenic Scores to Investigate Cross-Trait Genetic Nurture and Assortative Mating</title>
      <link>https://arxiv.org/abs/2510.00353</link>
      <description>arXiv:2510.00353v1 Announce Type: cross 
Abstract: Genetic nurture effects and assortative mating (AM) occur across many human behaviors and can bias estimates from traditional genetic models. These influences are typically studied univariately, within the same trait. However, estimation of cross-trait genetic nurture effects and cross-trait AM remains underexplored due to the absence of suitable approaches. To address this, we developed a multivariate extension of the SEM-PGS model for datasets with genotyped and phenotyped parents and offspring, enabling joint estimation of within-trait and cross-trait genetic and environmental influences. By integrating haplotypic polygenic scores (PGS) into a structural equation modeling framework, the model simultaneously estimates same-trait and cross-trait direct effects, genetic nurture, vertical transmission, and assortative mating. We also provide the first formal description of how copaths can be used to model multivariate assortative mating and derive the corresponding parameter expectations in matrix form. Forward-time Monte Carlo simulations under varying conditions of r^2_PGS and N_trio demonstrate that the model yields unbiased estimates of both within-trait and cross-trait effects when assumptions are met. The precision of estimates was adequate with large sample sizes (N_trio &gt; 16k) and improved as PGS predictive power increased. In addition, our simulation results show that failing to model cross-trait effects biases within-trait estimates, underscoring the importance of incorporating cross-trait effects. The multivariate SEM-PGS model offers a powerful and flexible tool for disentangling gene-environment interplay and advancing the understanding of familial influences on human traits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00353v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuanyu Lyu, Jared Balbona, Tong Chen, Matthew C. Keller</dc:creator>
    </item>
    <item>
      <title>Rapid Scaling of Compositional Uncertainty from Sample to Population Levels</title>
      <link>https://arxiv.org/abs/2510.00980</link>
      <description>arXiv:2510.00980v1 Announce Type: cross 
Abstract: Understanding population composition is essential across ecological, evolutionary, conservation, and resource management contexts. Modern methods such as genetic stock identification (GSI) estimate the proportion of individuals from each subpopulation using genetic data. Ideally, these estimates are obtained through mixture analysis, which captures both sampling and genetic uncertainty. However, historical datasets often rely on individual assignment methods that only account for sample-level uncertainty, limiting the validity of population-level inferences. To address this, we propose a reverse Dirichlet-multinomial model and derive multiple variance estimators to propagate uncertainty from the sample to the population level. We extend this framework to genetic mark-recapture studies, assess performance via simulation, and apply our method to estimate the escapement of Sockeye Salmon (Oncorhynchus nerka) in the Taku River.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Wang, Martin Lysy, Audrey B\'eliveau</dc:creator>
    </item>
    <item>
      <title>The causal structure of galactic astrophysics</title>
      <link>https://arxiv.org/abs/2510.01112</link>
      <description>arXiv:2510.01112v1 Announce Type: cross 
Abstract: Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for astrophysical datasets and illustrate it on $\sim$5$\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01112v1</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.CO</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Desmond, Joseph Ramsey</dc:creator>
    </item>
    <item>
      <title>The Rhythm of Aging: Stability and Drift in Human Senescence</title>
      <link>https://arxiv.org/abs/2504.04143</link>
      <description>arXiv:2504.04143v3 Announce Type: replace 
Abstract: Human aging is marked by a steady rise in the risk of dying with age--a process demographers call senescence. Over the past century, life expectancy has risen dramatically, but is this because we are aging slower, or simply starting it later? Vaupel hypothesizes that the pace at which individuals age may be constant, with gains in longevity coming from the delayed onset of senescence rather than its slowing down. We test this idea using a new framework that decomposes the pace of senescence into three components: a biological baseline, a long-term trend, and the cumulative impact of period shocks. Applying this to cohort mortality data from 12 countries, we find that once period shocks are accounted for, there is no statistical evidence of a long-term trend, consistent Vaupel's hypothesis. Rather than indicating a change in the process that drives senescence, these variations are consistent with echoes of shared historical events. These results suggest that while longevity has shifted, the rhythm of human aging may be conserved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04143v3</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio Cabral Patricio</dc:creator>
    </item>
    <item>
      <title>Automated flood detection from Sentinel-1 GRD time series using Bayesian analysis for change point problems</title>
      <link>https://arxiv.org/abs/2504.19526</link>
      <description>arXiv:2504.19526v3 Announce Type: replace 
Abstract: Current Synthetic Aperture Radar (SAR)-based flood detection methods face critical limitations that hinder operational deployment. Supervised learning approaches require extensive labeled training data, exhibit poor geographical transferability, and may fail to adapt to new regions without additional training examples. Existing approaches do not fully exploit the rich temporal information available in SAR time series, instead relying on simple change detection between pre- and post-flood images or supplementary datasets that often introduce error propagation. These limitations prevent effective automated flood monitoring in data-scarce regions where disaster response is most needed. To address these limitations, we develop a novel training-free approach by adapting Bayesian analysis for change point problems, specifically for automated flood detection from Sentinel-1 Ground Range Detected time series data. Our method statistically models the temporal behavior of SAR backscatter intensity over a one-year baseline period, then computes the posterior probability of change points at flood observation dates. This approach eliminates supervised learning dependencies by using Bayesian inference to identify when backscatter deviations exceed expected normal variations, leveraging inherent statistical properties of time series data. Validation across three diverse geographical contexts using the UrbanSARFloods benchmark dataset demonstrates superior performance compared to conventional thresholding and deep learning approaches, achieving F1 scores up to 0.76. This enables immediate deployment to any region with SAR coverage, providing critical advantages for disaster response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19526v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Tomohiro Tanaka, Nifat Sultana</dc:creator>
    </item>
    <item>
      <title>Transporting Predictions via Double Machine Learning: Predicting Partially Unobserved Students' Outcomes</title>
      <link>https://arxiv.org/abs/2509.12533</link>
      <description>arXiv:2509.12533v2 Announce Type: replace 
Abstract: Educational policymakers often lack data on student outcomes in regions where standardized tests were not administered. Machine learning techniques can be used to predict unobserved outcomes in target populations by training models on data from a source population. However, differences between the source and target populations, particularly in covariate distributions, can reduce the transportability of these models, potentially reducing predictive accuracy and introducing bias. We propose using double machine learning for a covariate-shift weighted model. First, we estimate the overlap score-namely, the probability that an observation belongs to the source dataset given its covariates. Second, balancing weights, defined as the density ratio of target-to-source membership probabilities, are used to reweight the individual observations' contribution to the loss or likelihood function in the target outcome prediction model. This approach downweights source observations that are less similar to the target population, allowing predictions to rely more heavily on observations with greater overlap. As a result, predictions become more generalizable under covariate shift. We illustrate this framework in the context of uncertain data on students' standardized financial literacy scores (FLS). Using Bayesian Additive Regression Trees (BART), we predict missing FLS. We find minimal differences in predictive performance between the weighted and unweighted models, suggesting limited covariate shift in our empirical setting. Nonetheless, the proposed approach provides a principled framework for addressing covariate shift and is broadly applicable to predictive modeling in the social and health sciences, where differences between source and target populations are common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12533v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Falco J. Bargagli-Stoffi, Emma Landry, Kevin P. Josey, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte</dc:creator>
    </item>
    <item>
      <title>A Penalized Distributed Lag Non-Linear Lee-Carter Framework for Regional Weekly Mortality Forecasting</title>
      <link>https://arxiv.org/abs/2509.24087</link>
      <description>arXiv:2509.24087v2 Announce Type: replace 
Abstract: Accurate forecasts of weekly mortality are essential for public health and the insurance industry. We develop a forecasting framework that extends the Lee-Carter model with age- and region-specific seasonal effects and penalized distributed lag non-linear components that capture the delayed and non-linear effects of heat, cold, and influenza on mortality. The model accommodates overdispersed mortality rates via a negative binomial distribution. We model the temporal dynamics of the latent factors in the model using SARIMAX processes and capture cross-regional dependencies through a copula-based approach. Using regional French mortality data (1990-2019), we demonstrate that the proposed framework yields well-calibrated forecast distributions and improves predictive accuracy relative to benchmark models. The results further show substantial heterogeneity in temperature- and influenza-related relative risks between ages and regions. These findings underscore the importance of incorporating exogenous drivers and dependence structures into a weekly mortality forecasting framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24087v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Robben, Karim Barigou</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v4 Announce Type: replace-cross 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v2 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khellaf R\'emi, Bellet Aur\'elien, Josse Julie</dc:creator>
    </item>
    <item>
      <title>Simultaneous estimation of the effective reproduction number and the time series of daily infections: Application to Covid-19</title>
      <link>https://arxiv.org/abs/2506.21027</link>
      <description>arXiv:2506.21027v2 Announce Type: replace-cross 
Abstract: The time varying effective reproduction number is an important parameter for communication and policy decisions during an epidemic. In this paper, we present new statistical methods for estimating the reproduction number based on the popular model of \citet{cori2013new} which defines the effective reproduction number based on self-exciting dynamics of new infections. Such a model is conceptually simple and less susceptible to misspecifications than more complicated multi-compartment models. However, statistical inference is challenging, and the previous literature has either relied on proxy data and/or a two-step approach in which the number of infections are first estimated. In contrast, we present a coherent Bayesian method that approximates the joint posterior of daily new infections and reproduction numbers using a novel Markov chain Monte Carlo (MCMC) algorithm. Comparing our method to the state-of-the-art three-step estimation procedure of \citet{huisman2022estimation}, both using daily confirmed cases from Switzerland in the Covid-19 epidemic and simulated data, we find that our method is more accurate in terms of point estimates and uncertainty quantification, especially near the beginning and end of an observation period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21027v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hans R. K\"unsch, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence</title>
      <link>https://arxiv.org/abs/2509.16599</link>
      <description>arXiv:2509.16599v2 Announce Type: replace-cross 
Abstract: Background: Evidence synthesis facilitates evidence-based medicine. This task becomes increasingly difficult to accomplished with applying computational solutions, since the medical literature grows at astonishing rates. Objective: This study evaluates an information retrieval-driven workflow, CASMA, to enhance the efficiency, transparency, and reproducibility of systematic reviews. Endometriosis recurrence serves as the ideal case due to its complex and ambiguous literature. Methods: The hybrid approach integrates PRISMA guidelines with fuzzy matching and regular expression (regex) to facilitate semi-automated deduplication and filtered records before manual screening. The workflow synthesised evidence from randomised controlled trials on the efficacy of a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified splitting method addressed unit-of-analysis errors in multi-arm trials. Results: The workflow sharply reduced the screening workload, taking only 11 days to fetch and filter 33,444 records. Seven eligible RCTs were synthesized (841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of $0.64$ ($95\%$ CI $0.48$ to $0.86$), demonstrating a $36\%$ reduction in recurrence, with non-significant heterogeneity ($I^2=0.00\%$, $\tau^2=0.00$). The findings were robust and stable, as they were backed by sensitivity analyses. Conclusion: This study demonstrates an application of an information-retrieval-driven workflow for medical evidence synthesis. The approach yields valuable clinical results and a generalisable framework to scale up the evidence synthesis, bridging the gap between clinical research and computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16599v2</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandro Tsang</dc:creator>
    </item>
  </channel>
</rss>
