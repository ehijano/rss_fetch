<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Missing data in non-stationary multivariate time series from digital studies in Psychiatry</title>
      <link>https://arxiv.org/abs/2506.14946</link>
      <description>arXiv:2506.14946v1 Announce Type: new 
Abstract: Mobile technology (e.g., mobile phones and wearable devices) provides scalable methods for collecting physiological and behavioral biomarkers in patients' naturalistic settings, as well as opportunities for therapeutic advancements and scientific discoveries regarding the etiology of psychiatric illness. Continuous data collection through mobile devices generates highly complex data: entangled multivariate time series of outcomes, exposures, and covariates. Missing data is a pervasive problem in biomedical and social science research, and Ecological Momentary Assessment (EMA) data in psychiatric research is no exception. However, the complex data structure of multivariate time series and their non-stationary nature make missing data a major challenge for proper inference. Additional historical information included in time series analyses exacerbates the issue of missing data and also introduces problems for confounding adjustment. The majority of existing imputation methods are either designed for stationary time series or for longitudinal data with limited follow-up periods. The limited work on non-stationary time series either focuses on missing exogenous information or ignores the complex temporal dependence among outcomes, exposures, and covariates. We propose a Monte Carlo Expectation Maximization algorithm for the state space model (MCEM-SSM) to effectively handle missing data in non-stationary entangled multivariate time series. We demonstrate the method's advantages over other widely used missing data imputation strategies through simulations of both stationary and non-stationary time series, subject to various missing mechanisms. Finally, we apply the MCEM-SSM to a multi-year smartphone observational study of bipolar and schizophrenia patients to investigate the association between digital social connectivity and negative mood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14946v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiaoxuan Cai, Charlotte R. Fowler, Li Zeng, Habiballah Rahimi Eichi, Dost Ongur, Lisa Dixon, Justin T. Baker, Jukka-Pekka Onnela, Linda Valeri</dc:creator>
    </item>
    <item>
      <title>Functional Change Point Detection via Adjacent Deviation Subspace</title>
      <link>https://arxiv.org/abs/2506.15143</link>
      <description>arXiv:2506.15143v1 Announce Type: new 
Abstract: This paper develops the concept of the Adjacent Deviation Subspace (ADS), a novel framework for reducing infinite-dimensional functional data into finite-dimensional vector or scalar representations while preserving critical information of functional change points. To identify this functional subspace, we propose an efficient dimension reduction operator that overcomes the critical limitation of information loss inherent in traditional functional principal component analysis (FPCA). Building upon this foundation, we first construct a test statistic based on the dimension-reducing target operator to test the existence of change points. Second, we present the MPULSE criterion to estimate change point locations in lower-dimensional representations. This approach not only reduces computational complexity and mitigates false positives but also provides intuitive graphical visualization of change point locations. Lastly, we establish a unified analytical framework that seamlessly integrates dimension reduction with precise change point detection. Extensive simulation studies and real-world data applications validate the robustness and efficacy of our method, consistently demonstrating superior performance over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15143v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luoyao Yu, Long Feng, Xuehu Zhu</dc:creator>
    </item>
    <item>
      <title>A penalized least squares estimator for extreme-value mixture models</title>
      <link>https://arxiv.org/abs/2506.15272</link>
      <description>arXiv:2506.15272v1 Announce Type: new 
Abstract: Estimating the parameters of max-stable parametric models poses significant challenges, particularly when some parameters lie on the boundary of the parameter space. This situation arises when a subset of variables exhibits extreme values simultaneously, while the remaining variables do not -- a phenomenon referred to as an extreme direction in the literature. In this paper, we propose a novel estimator for the parameters of a general parametric mixture model, incorporating a penalization approach based on a pseudo-norm. This penalization plays a crucial role in accurately identifying parameters at the boundary of the parameter space. Additionally, our estimator comes with a data-driven algorithm to detect groups of variables corresponding to extreme directions. We assess the performance of our estimator in terms of both parameter estimation and the identification of extreme directions through extensive simulation studies. Finally, we apply our methods to data on river discharges and financial portfolio losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15272v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anas Mourahib, Anna Kiriliouk, Johan Segers</dc:creator>
    </item>
    <item>
      <title>Multivariate and Multiple Contrast Testing in General Covariate-adjusted Factorial Designs</title>
      <link>https://arxiv.org/abs/2506.15292</link>
      <description>arXiv:2506.15292v1 Announce Type: new 
Abstract: Evaluating intervention effects on multiple outcomes is a central research goal in a wide range of quantitative sciences. It is thereby common to compare interventions among each other and with a control across several, potentially highly correlated, outcome variables. In this context, researchers are interested in identifying effects at both, the global level (across all outcome variables) and the local level (for specific variables). At the same time, potential confounding must be accounted for. This leads to the need for powerful multiple contrast testing procedures (MCTPs) capable of handling multivariate outcomes and covariates. Given this background, we propose an extension of MCTPs within a semiparametric MANCOVA framework that allows applicability beyond multivariate normality, homoscedasticity, or non-singular covariance structures. We illustrate our approach by analysing multivariate psychological intervention data, evaluating joint physiological and psychological constructs such as heart rate variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15292v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marl\'ene Baumeister, Konstantin Emil Thiel, Lynn Matits, Georg Zimmermann, Markus Pauly, Paavo Sattler</dc:creator>
    </item>
    <item>
      <title>Causal inference amid missingness-specific independencies and mechanism shifts</title>
      <link>https://arxiv.org/abs/2506.15441</link>
      <description>arXiv:2506.15441v1 Announce Type: new 
Abstract: The recovery of causal effects in structural models with missing data often relies on $m$-graphs, which assume that missingness mechanisms do not directly influence substantive variables. Yet, in many real-world settings, missing data can alter decision-making processes, as the absence of key information may affect downstream actions and states. To overcome this limitation, we introduce $lm$-SCMs and $lm$-graphs, which extend $m$-graphs by integrating a label set that represents relevant context-specific independencies (CSI), accounting for mechanism shifts induced by missingness. We define two causal effects within these systems: the Full Average Treatment Effect (FATE), which reflects the effect in a hypothetical scenario had no data been missing, and the Natural Average Treatment Effect (NATE), which captures the effect under the unaltered CSIs in the system. We propose recovery criteria for these queries and present doubly-robust estimators for a graphical model inspired by a real-world application. Simulations highlight key differences between these estimands and estimation methods. Findings from the application case suggest a small effect of ADHD treatment upon test achievement among Norwegian children, with a slight effect shift due to missing pre-tests scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15441v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Leonard Henckel, Johan Pensar, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Towards Robust Causal Effect Identification Beyond Markov Equivalence</title>
      <link>https://arxiv.org/abs/2506.15561</link>
      <description>arXiv:2506.15561v1 Announce Type: new 
Abstract: Causal effect identification typically requires a fully specified causal graph, which can be difficult to obtain in practice. We provide a sufficient criterion for identifying causal effects from a candidate set of Markov equivalence classes with added background knowledge, which represents cases where determining the causal graph up to a single Markov equivalence class is challenging. Such cases can happen, for example, when the untestable assumptions (e.g. faithfulness) that underlie causal discovery algorithms do not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15561v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Z. Teh, Kayvan Sadeghi, Terry Soo</dc:creator>
    </item>
    <item>
      <title>Independent increments and group sequential tests</title>
      <link>https://arxiv.org/abs/2506.15599</link>
      <description>arXiv:2506.15599v1 Announce Type: new 
Abstract: Widely used methods and software for group
  sequential tests of a null hypothesis of no treatment difference
  that allow for early stopping of a clinical trial depend primarily
  on the fact that sequentially-computed test statistics have the
  independent increments property. However, there are many practical
  situations where the sequentially-computed test statistics do not
  possess this property. Key examples are in trials where the primary
  outcome is a time to an event but where the assumption of
  proportional hazards is likely violated, motivating consideration of
  treatment effects such as the difference in restricted mean survival
  time or the use of approaches that are alternatives to the familiar
  logrank test, in which case the associated test statistics may not
  possess independent increments. We show that, regardless of the
  covariance structure of sequentially-computed test statistics, one
  can always derive linear combinations of these test statistics
  sequentially that do have the independent increments property. We
  also describe how to best choose these linear combinations to target
  specific alternative hypotheses, such as proportional or
  non-proportional hazards or log odds alternatives. We thus derive
  new, sequentially-computed test statistics that not only have the
  independent increments property, supporting straightforward use of
  existing methods and software, but that also have greater power
  against target alternative hypotheses than do procedures based on
  the original test statistics, regardless of whether or not the original
  statistics have the independent increments property. We illustrate
  with two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15599v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios A. Tsiatis, Marie Davidian</dc:creator>
    </item>
    <item>
      <title>On independence testing using the (partial) distance correlation</title>
      <link>https://arxiv.org/abs/2506.15659</link>
      <description>arXiv:2506.15659v1 Announce Type: new 
Abstract: Distance correlation is a measure of dependence between two paired random vectors or matrices of arbitrary, not necessarily equal, dimensions. Unlike Pearson correlation, the population distance correlation coefficient is zero if and only if the random vectors are independent. Thus, distance correlation measures both linear and non-linear association between two univariate and or multivariate random variables. Partial distance correlation expands to the case of conditional independence. To test for (conditional) independence, the p-value may be computed either via permutations or asymptotically via the $\chi^2$ distribution. In this paper we perform an intra-comparison of both approaches for (conditional) independence and an inter-comparison to the classical Pearson correlation where for the latter we compute the asymptotic p-value. The results are rather surprising, especially for the case of conditional independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15659v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kontemeniotis Nikolaos, Vargiakakis Rafail, Tsagris Michail</dc:creator>
    </item>
    <item>
      <title>How many federal employees are not satisfied? Using response times to estimate population proportions under the survey variable cause model</title>
      <link>https://arxiv.org/abs/2506.14915</link>
      <description>arXiv:2506.14915v1 Announce Type: cross 
Abstract: We propose a statistical model to estimate population proportions under the survey variable cause model (Groves 2006), the setting in which the characteristic measured by the survey has a direct causal effect on survey participation. For example, we estimate employee satisfaction from a survey in which the decision of an employee to participate depends on their satisfaction. We model the time at which a respondent 'arrives' to take the survey, leveraging results from the counting processes literature that has been developed to analyze similar problems with survival data. Our approach is particularly useful for nonresponse bias analysis because it relies on different assumptions than traditional adjustments such as poststratification, which assumes the common cause model, the setting in which external factors explain the characteristic measured by the survey and participation. Our motivation is the Federal Employee Viewpoint Survey, which asks federal employees whether they are satisfied with their work organization. Our model suggests that the sample proportion overestimates the proportion of federal employees that are not satisfied with their work organization even after adjustment by poststratification. Employees that are not satisfied likely select into the survey, and this selection cannot be explained by personal characteristics like race, gender, and occupation or work-place characteristics like agency, unit, and location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14915v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond</title>
      <link>https://arxiv.org/abs/2506.14950</link>
      <description>arXiv:2506.14950v1 Announce Type: cross 
Abstract: Solving conditional moment restrictions (CMRs) is a key problem considered in statistics, causal inference, and econometrics, where the aim is to solve for a function of interest that satisfies some conditional moment equalities. Specifically, many techniques for causal inference, such as instrumental variable (IV) regression and proximal causal learning (PCL), are CMR problems. Most CMR estimators use a two-stage approach, where the first-stage estimation is directly plugged into the second stage to estimate the function of interest. However, naively plugging in the first-stage estimator can cause heavy bias in the second stage. This is particularly the case for recently proposed CMR estimators that use deep neural network (DNN) estimators for both stages, where regularisation and overfitting bias is present. We propose DML-CMR, a two-stage CMR estimator that provides an unbiased estimate with fast convergence rate guarantees. We derive a novel learning objective to reduce bias and develop the DML-CMR algorithm following the double/debiased machine learning (DML) framework. We show that our DML-CMR estimator can achieve the minimax optimal convergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity conditions, where $N$ is the sample size. We apply DML-CMR to a range of problems using DNN estimators, including IV regression and proximal causal learning on real-world datasets, demonstrating state-of-the-art performance against existing CMR estimators and algorithms tailored to those problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14950v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska</dc:creator>
    </item>
    <item>
      <title>Image Processing Techniques to Identify and Quantify Spatiotemporal Carbon Cycle Extremes</title>
      <link>https://arxiv.org/abs/2506.15555</link>
      <description>arXiv:2506.15555v1 Announce Type: cross 
Abstract: Rising atmospheric carbon dioxide due to human activities through fossil fuel emissions and land use changes have increased climate extremes such as heat waves and droughts that have led to and are expected to increase the occurrence of carbon cycle extremes. Carbon cycle extremes represent large anomalies in the carbon cycle that are associated with gains or losses in carbon uptake. Carbon cycle extremes could be continuous in space and time and cross political boundaries. Here, we present a methodology to identify large spatiotemporal extremes (STEs) in the terrestrial carbon cycle using image processing tools for feature detection. We characterized the STE events based on neighborhood structures that are three-dimensional adjacency matrices for the detection of spatiotemporal manifolds of carbon cycle extremes. We found that the area affected and carbon loss during negative carbon cycle extremes were consistent with continuous neighborhood structures. In the gross primary production data we used, 100 carbon cycle STEs accounted for more than 75\% of all the negative carbon cycle extremes. This paper presents a comparative analysis of the magnitude of carbon cycle STEs and attribution of those STEs to climate drivers as a function of neighborhood structures for two observational datasets and an Earth system model simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15555v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bharat Sharma, Jitendra Kumar, Auroop R. Ganguly, Forrest M. Hoffman</dc:creator>
    </item>
    <item>
      <title>Fast computation of exact confidence intervals for randomized experiments with binary outcomes</title>
      <link>https://arxiv.org/abs/2305.09906</link>
      <description>arXiv:2305.09906v2 Announce Type: replace 
Abstract: Given a randomized experiment with binary outcomes, exact confidence intervals for the average causal effect of the treatment can be computed through a series of permutation tests. This approach requires minimal assumptions and is valid for all sample sizes, as it does not rely on large-sample approximations such as those implied by the central limit theorem. We show that these confidence intervals can be found in $O(n \log n)$ permutation tests in the case of balanced designs, where the treatment and control groups have equal sizes, and $O(n^2)$ permutation tests in the general case. Prior to this work, the most efficient known constructions required $O(n^2)$ such tests in the balanced case [Li and Ding, 2016], and $O(n^4)$ tests in the general case [Rigdon and Hudgens, 2015]. Our results thus facilitate exact inference as a viable option for randomized experiments far larger than those accessible by previous methods. We also generalize our construction to produce confidence intervals for other causal estimands, including the relative risk ratio and odds ratio, yielding similar computational gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09906v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. M. Aronow, Haoge Chang, Patrick Lopatto</dc:creator>
    </item>
    <item>
      <title>Identifiable and interpretable nonparametric factor analysis</title>
      <link>https://arxiv.org/abs/2311.08254</link>
      <description>arXiv:2311.08254v2 Announce Type: replace 
Abstract: Factor models are widely used to reduce dimensionality in modeling high-dimensional data. However, there remains a need for models that can be reliably fit in modest sample sizes and are identifiable, interpretable, and flexible. To address this gap, we propose a NIFTY model that uses a linear factor structure with Gaussian residuals, but with a novel latent variable modeling structure. In particular, we model each latent variable as a one-dimensional nonlinear mapping of a uniform latent location. A key innovation is allowing different latent variables to be transformations of the same latent locations, accommodating intrinsic lower-dimensional nonlinear structures. Leveraging on pre-trained data obtained by diffusion maps and post-processing of MCMC samples, we obtain model identifiability. In addition, we softly constrain the empirical distribution of the latent locations to be close to uniform to address a latent posterior shift problem, which is common in factor models and can lead to substantial bias in parameter inferences, predictions, and generative modeling. We show good performance in density estimation and data visualization in simulations, and apply NIFTY to bird song data in an environmental monitoring application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08254v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maoran Xu, Steven Winter, Amy H. Herring, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A New Fit Assessment Framework for Common Factor Models Using Generalized Residuals</title>
      <link>https://arxiv.org/abs/2405.15204</link>
      <description>arXiv:2405.15204v3 Announce Type: replace 
Abstract: Assessing fit in common factor models solely through the lens of mean and covariance structures, as is commonly done with conventional goodness-of-fit (GOF) assessments, may overlook critical aspects of misfit, potentially leading to misleading conclusions. To achieve more flexible fit assessment, we extend the theory of generalized residuals (Haberman &amp; Sinharay, 2013), originally developed for models with categorical data, to encompass more general measurement models. Within this extended framework, we propose several fit test statistics designed to evaluate various parametric assumptions involved in common factor models. The examples include assessing the distributional assumptions of latent variables and functional form assumptions of individual manifest variables. The performance of the proposed statistics is examined through simulation studies and an empirical data analysis. Our findings suggest that generalized residuals are promising tools for detecting misfit in measurement models, often masked when assessed by conventional GOF testing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15204v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youjin Sung, Youngjin Han, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Hidden Markov modelling of spatio-temporal dynamics of measles in 1750-1850 Finland</title>
      <link>https://arxiv.org/abs/2405.16885</link>
      <description>arXiv:2405.16885v2 Announce Type: replace 
Abstract: Real world spatio-temporal datasets, and phenomena related to them, are often challenging to visualise or gain a general overview of. In order to summarise information encompassed in such data, we combine two well known statistical modelling methods. To account for the spatial dimension, we use the intrinsic modification of the conditional autoregression, and incorporate it with the hidden Markov model, allowing the spatial patterns to vary over time. We apply our method into parish register data considering deaths caused by measles in Finland in 1750-1850, and gain novel insight of previously undiscovered infection dynamics. Five distinctive, reoccurring states describing spatially and temporally differing infection burden and potential routes of spread are identified. We also find that there is a change in the occurrences of the most typical spatial patterns circa 1812, possibly due to changes in communication routes after major administrative transformations in Finland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16885v2</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiia-Maria Pasanen, Jouni Helske, Tarmo Ketola</dc:creator>
    </item>
    <item>
      <title>Quantile Treatment Effects in High Dimensional Panel Data</title>
      <link>https://arxiv.org/abs/2504.00785</link>
      <description>arXiv:2504.00785v2 Announce Type: replace 
Abstract: We introduce novel estimators for quantile causal effects with high dimensional panel data (large $N$ and $T$), where only one or a few units are affected by the intervention or policy. Our method extends the generalized synthetic control method \citep{xu_2017} from average treatment effects on the treated to quantile treatment effects on the treated, allowing the underlying factor structure to change across the quantile of the interested outcome distribution. Our method involves estimating the quantile-dependent factors using the control group, followed by a quantile regression to estimate the quantile treatment effect using the treated units. We establish the asymptotic properties of our estimators and propose a bootstrap procedure for statistical inference, supported by simulation studies. An empirical application of the 2008 China Stimulus Program is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00785v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Li Zheng</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis of Priors in the Bayesian Dirichlet Auto-Regressive Moving Average Model</title>
      <link>https://arxiv.org/abs/2506.13973</link>
      <description>arXiv:2506.13973v2 Announce Type: replace 
Abstract: Prior choice can strongly influence Bayesian Dirichlet ARMA (B-DARMA) inference for compositional time-series. Using simulations with (i) correct lag order, (ii) overfitting, and (iii) underfitting, we assess five priors: weakly-informative, horseshoe, Laplace, mixture-of-normals, and hierarchical. With the true lag order, all priors achieve comparable RMSE, though horseshoe and hierarchical slightly reduce bias. Under overfitting, aggressive shrinkage-especially the horseshoe-suppresses noise and improves forecasts, yet no prior rescues a model that omits essential VAR or VMA terms.
  We then fit B-DARMA to daily SP 500 sector weights using an intentionally large lag structure. Shrinkage priors curb spurious dynamics, whereas weakly-informative priors magnify errors in volatile sectors. Two lessons emerge: (1) match shrinkage strength to the degree of overparameterization, and (2) prioritize correct lag selection, because no prior repairs structural misspecification. These insights guide prior selection and model complexity management in high-dimensional compositional time-series applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13973v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Liz Medina, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Optimal designs for discrete choice models via graph Laplacians</title>
      <link>https://arxiv.org/abs/2208.08926</link>
      <description>arXiv:2208.08926v2 Announce Type: replace-cross 
Abstract: In discrete choice experiments, the information matrix depends on the model parameters. Therefore designing optimally informative experiments for arbitrary initial parameters often yields highly nonlinear optimization problems and makes optimal design infeasible. To overcome such challenges, we connect design theory for discrete choice experiments with Laplacian matrices of undirected graphs, resulting in complexity reduction and feasibility of optimal design. We rewrite the $D$-optimality criterion in terms of Laplacians via Kirchhoff's matrix tree theorem, and show that its dual has a simple description via the Cayley-Menger determinant of the Farris transform of the Laplacian matrix. This results in a drastic reduction of complexity and allows us to implement a gradient descent algorithm to find locally $D$-optimal designs. For the subclass of Bradley-Terry paired comparison models, we find a direct link to maximum likelihood estimation for Laplacian-constrained Gaussian graphical models. Finally, we study the performance of our algorithm and demonstrate its application to real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08926v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank R\"ottger, Thomas Kahle, Rainer Schwabe</dc:creator>
    </item>
    <item>
      <title>Efficient estimation with incomplete data via generalised ANOVA decompositions</title>
      <link>https://arxiv.org/abs/2409.05729</link>
      <description>arXiv:2409.05729v2 Announce Type: replace-cross 
Abstract: We study the semiparametric efficient estimation of a class of linear functionals in settings where a complete multivariate dataset is supplemented by additional datasets recording subsets of the variables of interest. These datasets are allowed to have a general, in particular non-monotonic, structure. Our main contribution is to characterise the asymptotic minimal mean squared error for these problems and to introduce an estimator whose risk approximately matches this lower bound. We show that the efficient rescaled variance can be expressed as the minimal value of a quadratic optimisation problem over a function space, thus establishing a fundamental link between these estimation problems and the theory of generalised ANOVA decompositions. Our estimation procedure uses iterated nonparametric regression to mimic an approximate influence function derived through gradient descent. We prove that this estimator is approximately normally distributed, provide an estimator of its variance and thus develop confidence intervals of asymptotically minimal width. Finally we present extensions of our theory demonstrating that the framework can be adapted to include various types of sampling bias and non-linear functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05729v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas B. Berrett</dc:creator>
    </item>
    <item>
      <title>KANITE: Kolmogorov-Arnold Networks for ITE estimation</title>
      <link>https://arxiv.org/abs/2503.13912</link>
      <description>arXiv:2503.13912v2 Announce Type: replace-cross 
Abstract: We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs) for Individual Treatment Effect (ITE) estimation under multiple treatments setting in causal inference. By utilizing KAN's unique abilities to learn univariate activation functions as opposed to learning linear weights by Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE framework comprises two key architectures: 1.Integral Probability Metric (IPM) architecture: This employs an IPM loss in a specialized manner to effectively align towards ITE estimation across multiple treatments. 2. Entropy Balancing (EB) architecture: This uses weights for samples that are learned by optimizing entropy subject to balancing the covariates across treatment groups. Extensive evaluations on benchmark datasets demonstrate that KANITE outperforms state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and $\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of KANITE in achieving improved causal estimates, emphasizing the potential of KANs to advance causal inference methodologies across diverse application areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13912v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eshan Mehendale, Abhinav Thorat, Ravi Kolla, Niranjan Pedanekar</dc:creator>
    </item>
    <item>
      <title>From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care</title>
      <link>https://arxiv.org/abs/2506.13584</link>
      <description>arXiv:2506.13584v2 Announce Type: replace-cross 
Abstract: In this work, we reflect on the data-driven modeling paradigm that is gaining ground in AI-driven automation of patient care. We argue that the repurposing of existing real-world patient datasets for machine learning may not always represent an optimal approach to model development as it could lead to undesirable outcomes in patient care. We reflect on the history of data analysis to explain how the data-driven paradigm rose to popularity, and we envision ways in which systems thinking and clinical domain theory could complement the existing model development approaches in reaching human-centric outcomes. We call for a purpose-driven machine learning paradigm that is grounded in clinical theory and the sociotechnical realities of real-world operational contexts. We argue that understanding the utility of existing patient datasets requires looking in two directions: upstream towards the data generation, and downstream towards the automation objectives. This purpose-driven perspective to AI system development opens up new methodological opportunities and holds promise for AI automation of patient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13584v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Anadria, Roel Dobbe, Anastasia Giachanou, Ruurd Kuiper, Richard Bartels, Wouter van Amsterdam, \'I\~nigo Mart\'inez de Rituerto de Troya, Carmen Z\"urcher, Daniel Oberski</dc:creator>
    </item>
  </channel>
</rss>
