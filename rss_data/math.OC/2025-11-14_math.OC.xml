<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints</title>
      <link>https://arxiv.org/abs/2511.09845</link>
      <description>arXiv:2511.09845v1 Announce Type: new 
Abstract: This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(\delta, \epsilon)$-Goldstein stationary points using $\Theta(\delta^{-1}\epsilon^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09845v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cac Phan</dc:creator>
    </item>
    <item>
      <title>Benders Decomposition for Passenger-Oriented Train Timetabling with Hybrid Periodicity</title>
      <link>https://arxiv.org/abs/2511.09892</link>
      <description>arXiv:2511.09892v1 Announce Type: new 
Abstract: Periodic timetables are widely adopted in passenger railway operations due to their regular service patterns and well-coordinated train connections. However, fluctuations in passenger demand require varying train services across different periods, necessitating adjustments to the periodic timetable. This study addresses a hybrid periodic train timetabling problem, which enhances the flexibility and demand responsiveness of a given periodic timetable through schedule adjustments and aperiodic train insertions, taking into account the rolling stock circulation. Since timetable modifications may affect initial passenger routes, passenger routing is incorporated into the problem to guide planning decisions towards a passenger-oriented objective. Using a time-space network representation, the problem is formulated as a dynamic railway service network design model with resource constraints. To handle the complexity of real-world instances, we propose a decomposition-based algorithm integrating Benders decomposition and column generation, enhanced with multiple preprocessing and accelerating techniques. Numerical experiments demonstrate the effectiveness of the algorithm and highlight the advantage of hybrid periodic timetables in reducing passenger travel costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09892v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyuan Yao, Anita Sch\"obel, Lei Nie, Sven J\"ager</dc:creator>
    </item>
    <item>
      <title>Global Convergence of Four-Layer Matrix Factorization under Random Initialization</title>
      <link>https://arxiv.org/abs/2511.09925</link>
      <description>arXiv:2511.09925v1 Announce Type: new 
Abstract: Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09925v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minrui Luo, Weihang Xu, Xiang Gao, Maryam Fazel, Simon Shaolei Du</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of inexact MBA method for constrained upper-$\mathcal{C}^2$ optimization problems</title>
      <link>https://arxiv.org/abs/2511.09940</link>
      <description>arXiv:2511.09940v1 Announce Type: new 
Abstract: This paper concerns a class of constrained optimization problems in which, the objective and constraint functions are both upper-$\mathcal{C}^2$. For such nonconvex and nonsmooth optimization problems, we develop an inexact moving balls approximation (MBA) method by a workable inexactness criterion for the solving of subproblems. By leveraging a global error bound for the strongly convex program associated with parametric optimization problems, we establish the full convergence of the iterate sequence under the partial bounded multiplier property (BMP) and the Kurdyka-{\L}ojasiewicz (KL) property of the constructed potential function, and achieve the local convergence rate of the iterate and objective value sequences if the potential function satisfies the KL property of exponent $q\in[1/2,1)$. A verifiable condition is also provided to check whether the potential function satisfies the KL property of exponent $q\in[1/2,1)$ at the given critical point. To the best of our knowledge, this is the first implementable inexact MBA method with a full convergence certificate for the constrained nonconvex and nonsmooth optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09940v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyu Liu, Shaohua Pan</dc:creator>
    </item>
    <item>
      <title>The Age-Structured Chemostat with Substrate Dynamics as a Control System</title>
      <link>https://arxiv.org/abs/2511.09963</link>
      <description>arXiv:2511.09963v1 Announce Type: new 
Abstract: In this work we study an age-structured chemostat model with a renewal boundary condition and a coupled substrate equation. The model is nonlinear and consists of a hyperbolic partial differential equation and an ordinary differential equation with nonlinear, nonlocal terms appearing both in the ordinary differential equation and the boundary condition. Both differential equations contain a non-negative control input, while the states of the model are required to be positive. Under an appropriate weak solution framework, we determine the state space and the input space for this model. We prove global existence and uniqueness of solutions for all admissible initial conditions and all allowable control inputs. To this purpose we employ a combination of Banach's fixed-point theorem with implicit solution formulas and useful solution estimates. Finally, we show that the age-structured chemostat model gives a well-defined control system on a metric space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09963v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Iasson Karafyllis, Dionysis Theodosis, Miroslav Krstic</dc:creator>
    </item>
    <item>
      <title>An inexact semismooth Newton-Krylov method for semilinear elliptic optimal control problem</title>
      <link>https://arxiv.org/abs/2511.10058</link>
      <description>arXiv:2511.10058v1 Announce Type: new 
Abstract: An inexact semismooth Newton method has been proposed for solving semi-linear elliptic optimal control problems in this paper. This method incorporates the generalized minimal residual (GMRES) method, a type of Krylov subspace method, to solve the Newton equations and utilizes nonmonotonic line search to adjust the iteration step size. The original problem is reformulated into a nonlinear equation through variational inequality principles and discretized using a second-order finite difference scheme. By leveraging slanting differentiability, the algorithm constructs semismooth Newton directions and employs GMRES method to inexactly solve the Newton equations, significantly reducing computational overhead. A dynamic nonmonotonic line search strategy is introduced to adjust stepsizes adaptively, ensuring global convergence while overcoming local stagnation. Theoretical analysis demonstrates that the algorithm achieves superlinear convergence near optimal solutions when the residual control parameter $\eta_k$ approaches to 0. Numerical experiments validate the method's accuracy and efficiency in solving semilinear elliptic optimal control problems, corroborating theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10058v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiqi Chen, Xuesong Chen</dc:creator>
    </item>
    <item>
      <title>dHPR: A Distributed Halpern Peaceman--Rachford Method for Non-smooth Distributed Optimization Problems</title>
      <link>https://arxiv.org/abs/2511.10069</link>
      <description>arXiv:2511.10069v1 Announce Type: new 
Abstract: This paper introduces the distributed Halpern Peaceman--Rachford (dHPR) method, an efficient algorithm for solving distributed convex composite optimization problems with non-smooth objectives, which achieves a non-ergodic $O(1/k)$ iteration complexity regarding Karush--Kuhn--Tucker residual. By leveraging the symmetric Gauss--Seidel decomposition, the dHPR effectively decouples the linear operators in the objective functions and consensus constraints while maintaining parallelizability and avoiding additional large proximal terms, leading to a decentralized implementation with provably fast convergence. The superior performance of dHPR is demonstrated through comprehensive numerical experiments on distributed LASSO, group LASSO, and $L_1$-regularized logistic regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10069v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangcheng Feng, Defeng Sun, Yancheng Yuan, Guojun Zhang</dc:creator>
    </item>
    <item>
      <title>S-D-RSM: Stochastic Distributed Regularized Splitting Method for Large-Scale Convex Optimization Problems</title>
      <link>https://arxiv.org/abs/2511.10133</link>
      <description>arXiv:2511.10133v1 Announce Type: new 
Abstract: This paper investigates the problems large-scale distributed composite convex optimization, with motivations from a broad range of applications, including multi-agent systems, federated learning, smart grids, wireless sensor networks, compressed sensing, and so on. Stochastic gradient descent (SGD) and its variants are commonly employed to solve such problems. However, existing algorithms often rely on vanishing step sizes, strong convexity assumptions, or entail substantial computational overhead to ensure convergence or obtain favorable complexity. To bridge the gap between theory and practice, we integrate consensus optimization and operator splitting techniques (see Problem Reformulation) to develop a novel stochastic splitting algorithm, termed the \emph{stochastic distributed regularized splitting method} (S-D-RSM). In practice, S-D-RSM performs parallel updates of proximal mappings and gradient information for only a randomly selected subset of agents at each iteration. By introducing regularization terms, it effectively mitigates consensus discrepancies among distributed nodes. In contrast to conventional stochastic methods, our theoretical analysis establishes that S-D-RSM achieves global convergence without requiring diminishing step sizes or strong convexity assumptions. Furthermore, it achieves an iteration complexity of $\mathcal{O}(1/\epsilon)$ with respect to both the objective function value and the consensus error. Numerical experiments show that S-D-RSM achieves up to 2--3$\times$ speedup compared to state-of-the-art baselines, while maintaining comparable or better accuracy. These results not only validate the algorithm's theoretical guarantees but also demonstrate its effectiveness in practical tasks such as compressed sensing and empirical risk minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10133v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maoran Wang, Xingju Cai, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Locally Linear Convergence for Nonsmooth Convex Optimization via Coupled Smoothing and Momentum</title>
      <link>https://arxiv.org/abs/2511.10239</link>
      <description>arXiv:2511.10239v1 Announce Type: new 
Abstract: We propose an adaptive accelerated smoothing technique for a nonsmooth convex optimization problem where the smoothing update rule is coupled with the momentum parameter. We also extend the setting to the case where the objective function is the sum of two nonsmooth functions. With regard to convergence rate, we provide the global (optimal) sublinear convergence guarantees of O(1/k), which is known to be provably optimal for the studied class of functions, along with a local linear rate if the nonsmooth term fulfills a so-call locally strong convexity condition. We validate the performance of our algorithm on several problem classes, including regression with the l1-norm (the Lasso problem), sparse semidefinite programming (the MaxCut problem), Nuclear norm minimization with application in model free fault diagnosis, and l_1-regularized model predictive control to showcase the benefits of the coupling. An interesting observation is that although our global convergence result guarantees O(1/k) convergence, we consistently observe a practical transient convergence rate of O(1/k^2), followed by asymptotic linear convergence as anticipated by the theoretical result. This two-phase behavior can also be explained in view of the proposed smoothing rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10239v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reza Rahimi Baghbadorani, Sergio Grammatico, Peyman Mohajerin Esfahani</dc:creator>
    </item>
    <item>
      <title>Halpern Acceleration of the Inexact Proximal Point Method of Rockafellar</title>
      <link>https://arxiv.org/abs/2511.10372</link>
      <description>arXiv:2511.10372v1 Announce Type: new 
Abstract: This paper investigates a Halpern acceleration of the inexact proximal point method for solving maximal monotone inclusion problems in Hilbert spaces. The proposed Halpern inexact proximal point method (HiPPM) is shown to be globally convergent, and a unified framework is developed to analyze its worst-case convergence rate. Under mild summability conditions on the inexactness tolerances, HiPPM achieves an $\mathcal{O}(1/k^{2})$ rate in terms of the squared fixed-point residual. Furthermore, under additional mild condition, the method retains a fast linear convergence rate. Building upon this framework, we further extend the acceleration technique to constrained convex optimization through the augmented Lagrangian formulation. In analogy to Rockafellar's classical results, the resulting accelerated inexact augmented Lagrangian method inherits the convergence rate and complexity guarantees of HiPPM. The analysis thus provides a unified theoretical foundation for accelerated inexact proximal algorithms and their augmented Lagrangian extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10372v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liwei Zhang, Fanli Zhuang, Ning Zhang</dc:creator>
    </item>
    <item>
      <title>Minimizing smooth Kurdyka-{\L}ojasiewicz functions via generalized descent methods: Convergence rate and complexity</title>
      <link>https://arxiv.org/abs/2511.10414</link>
      <description>arXiv:2511.10414v1 Announce Type: new 
Abstract: This paper addresses the generalized descent algorithm (DEAL) for minimizing smooth functions, which is analyzed under the Kurdyka-{\L}ojasiewicz (KL) inequality. In particular, the suggested algorithm guarantees a sufficient decrease by adapting to the cost function's geometry. We leverage the KL property to establish the global convergence, convergence rates, and complexity. A particular focus is placed on the linear convergence of generalized descent methods. We show that the constant step-size and Armijo line search strategies along a generalized descent direction satisfy our generalized descent condition. Additionally, for nonsmooth functions by leveraging the smoothing techniques such as forward-backward and high-order Moreau envelopes, we show that the boosted proximal gradient method (BPGA) and the boosted high-order proximal-point (BPPA) methods are also specific cases of DEAL, respectively. It is notable that if the order of the high-order proximal term is chosen in a certain way (depending on the KL exponent), then the sequence generated by BPPA converges linearly for an arbitrary KL exponent. Our preliminary numerical experiments on inverse problems and LASSO demonstrate the efficiency of the proposed methods, validating our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10414v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Ahookhosh, Susan Ghaderi, Alireza Kabgani, Morteza Rahimi</dc:creator>
    </item>
    <item>
      <title>On fundamental properties of high-order forward-backward envelope</title>
      <link>https://arxiv.org/abs/2511.10421</link>
      <description>arXiv:2511.10421v1 Announce Type: new 
Abstract: This paper studies the fundamental properties of the high-order forward-backward splitting mapping (HiFBS) and its associated forward-backward envelope (HiFBE) through the lens of high-order regularization for nonconvex composite functions. Specifically, we (i) establish the boundedness and uniform boundedness of HiFBS, along with the H\"older and Lipschitz continuity of HiFBE; (ii) derive an explicit form for the subdifferentials of HiFBE; and (iii) investigate necessary and sufficient conditions for the differentiability and weak smoothness of HiFBE under suitable assumptions. By leveraging the prox-regularity of $g$ and the concept of $p$-calmness, we further demonstrate the local single-valuedness and continuity of HiFBS, which in turn guarantee the differentiability of HiFBE in neighborhoods of calm points. This paves the way for the development of gradient-based algorithms tailored to nonconvex composite optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10421v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Kabgani, Masoud Ahookhosh</dc:creator>
    </item>
    <item>
      <title>(Adaptive) Scaled gradient methods beyond locally Holder smoothness: Lyapunov analysis, convergence rate and complexity</title>
      <link>https://arxiv.org/abs/2511.10425</link>
      <description>arXiv:2511.10425v1 Announce Type: new 
Abstract: This paper addresses the unconstrained minimization of smooth convex functions whose gradients are locally Holder continuous. Building on these results, we analyze the Scaled Gradient Algorithm (SGA) under local smoothness assumptions, proving its global convergence and iteration complexity. Furthermore, under local strong convexity and the Kurdyka-Lojasiewicz (KL) inequality, we establish linear convergence rates and provide explicit complexity bounds. In particular, we show that when the gradient is locally Lipschitz continuous, SGA attains linear convergence for any KL exponent. We then introduce and analyze an adaptive variant of SGA (AdaSGA), which automatically adjusts the scaling and step-size parameters. For this method, we show global convergence, and derive local linear rates under strong convexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10425v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Ghaderi, Morteza Rahimi, Yves Moreau, Masoud Ahookhosh</dc:creator>
    </item>
    <item>
      <title>Riccati-ZORO: An efficient algorithm for heuristic online optimization of internal feedback laws in robust and stochastic model predictive control</title>
      <link>https://arxiv.org/abs/2511.10473</link>
      <description>arXiv:2511.10473v1 Announce Type: new 
Abstract: We present Riccati-ZORO, an algorithm for tube-based optimal control problems (OCP). Tube OCPs predict a tube of trajectories in order to capture predictive uncertainty. The tube induces a constraint tightening via additional backoff terms. This backoff can significantly affect the performance, and thus implicitly defines a cost of uncertainty. Optimizing the feedback law used to predict the tube can significantly reduce the backoffs, but its online computation is challenging.
  Riccati-ZORO jointly optimizes the nominal trajectory and uncertainty tube based on a heuristic uncertainty cost design. The algorithm alternates between two subproblems: (i) a nominal OCP with fixed backoffs, (ii) an unconstrained tube OCP, which optimizes the feedback gains for a fixed nominal trajectory. For the tube optimization, we propose a cost function informed by the proximity of the nominal trajectory to constraints, prioritizing reduction of the corresponding backoffs. These ideas are developed in detail for ellipsoidal tubes under linear state feedback. In this case, the decomposition into the two subproblems yields a substantial reduction of the computational complexity with respect to the state dimension from $\mathcal{O}(n_x^6)$ to $\mathcal{O}(n_x^3)$, i.e., the complexity of a nominal OCP.
  We investigate the algorithm in numerical experiments, and provide two open-source implementations: a prototyping version in CasADi and a high-performance implementation integrated into the acados OCP solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10473v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Messerer, Yunfan Gao, Jonathan Frey, Moritz Diehl</dc:creator>
    </item>
    <item>
      <title>Measuring dissimilarity between convex cones by means of max-min angles</title>
      <link>https://arxiv.org/abs/2511.10483</link>
      <description>arXiv:2511.10483v1 Announce Type: new 
Abstract: This work introduces a novel dissimilarity measure between two convex cones, based on the max-min angle between them. We demonstrate that this measure is closely related to the Pompeiu-Hausdorff distance, a well-established metric for comparing compact sets. Furthermore, we examine cone configurations where the measure admits simplified or analytic forms. For the specific case of polyhedral cones, a nonconvex cutting-plane method is deployed to compute, at least approximately, the measure between them. Our approach builds on a tailored version of Kelley's cutting-plane algorithm, which involves solving a challenging master program per iteration. When this master program is solved locally, our method yields an angle that satisfies certain necessary optimality conditions of the underlying nonconvex optimization problem yielding the dissimilarity measure between the cones. As an application of the proposed mathematical and algorithmic framework, we address the image-set classification task under limited data conditions, a task that falls within the scope of the \emph{Few-Shot Learning} paradigm. In this context, image sets belonging to the same class are modeled as polyhedral cones, and our dissimilarity measure proves useful for understanding whether two image sets belong to the same class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10483v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Welington de Oliveira, Valentina Sessa, David Sossa</dc:creator>
    </item>
    <item>
      <title>Low-Discrepancy Set Post-Processing via Gradient Descent</title>
      <link>https://arxiv.org/abs/2511.10496</link>
      <description>arXiv:2511.10496v1 Announce Type: new 
Abstract: The construction of low-discrepancy sets, used for uniform sampling and numerical integration, has recently seen great improvements based on optimization and machine learning techniques. However, these methods are computationally expensive, often requiring days of computation or access to GPU clusters. We show that simple gradient descent-based techniques allow for comparable results when starting with a reasonably uniform point set. Not only is this method much more efficient and accessible, but it can be applied as post-processing to any low-discrepancy set generation method for a variety of standard discrepancy measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10496v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Linhang Huang, Woorim Lee, Cole Smidt, Braeden Sodt, Xuan Zhang</dc:creator>
    </item>
    <item>
      <title>Time-periodic branched transport</title>
      <link>https://arxiv.org/abs/2511.10498</link>
      <description>arXiv:2511.10498v1 Announce Type: new 
Abstract: We develop a new framework for branched transport between probability measures which are allowed to vary in time. This framework can be used to model problems where the underlying transportation network displays a branched structure, but the source and target mass distributions can change cyclically over time, such as road networks or circulatory systems. We introduce the notion of time-dependent transport paths along with associated energies and distances, and prove existence of transport paths whose energy achieves the distance. We also show the time-dependent transport yields a metric structure on subsets of appropriately defined measure-valued Sobolev spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10498v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Kitagawa, Cecilia Mikat</dc:creator>
    </item>
    <item>
      <title>Verification of Sequential Convex Programming for Parametric Non-convex Optimization</title>
      <link>https://arxiv.org/abs/2511.10622</link>
      <description>arXiv:2511.10622v1 Announce Type: new 
Abstract: We introduce a verification framework to exactly verify the worst-case performance of sequential convex programming (SCP) algorithms for parametric non-convex optimization. The verification problem is formulated as an optimization problem that maximizes a performance metric (e.g., the suboptimality after a given number of iterations) over parameters constrained to be in a parameter set and iterate sequences consistent with the SCP update rules. Our framework is general, extending the notion of SCP to include both conventional variants such as trust-region, convex-concave, and prox-linear methods, and algorithms that combine convex subproblems with rounding steps, as in relaxing and rounding schemes. Unlike existing analyses that may only provide local guarantees under limited conditions, our framework delivers global worst-case guarantees--quantifying how well an SCP algorithm performs across all problem instances in the specified family. Applications in control, signal processing, and operations research demonstrate that our framework provides, for the first time, global worst-case guarantees for SCP algorithms in the parametric setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10622v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rajiv Sambharya, Nikolai Matni, George Pappas</dc:creator>
    </item>
    <item>
      <title>Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity</title>
      <link>https://arxiv.org/abs/2511.10626</link>
      <description>arXiv:2511.10626v1 Announce Type: new 
Abstract: Constrained non-convex optimization is fundamentally challenging, as global solutions are generally intractable and constraint qualifications may not hold. However, in many applications, including safe policy optimization in control and reinforcement learning, such problems possess hidden convexity, meaning they can be reformulated as convex programs via a nonlinear invertible transformation. Typically such transformations are implicit or unknown, making the direct link with the convex program impossible. On the other hand, (sub-)gradients with respect to the original variables are often accessible or can be easily estimated, which motivates algorithms that operate directly in the original (non-convex) problem space using standard (sub-)gradient oracles. In this work, we develop the first algorithms to provably solve such non-convex problems to global minima. First, using a modified inexact proximal point method, we establish global last-iterate convergence guarantees with $\widetilde{\mathcal{O}}(\varepsilon^{-3})$ oracle complexity in non-smooth setting. For smooth problems, we propose a new bundle-level type method based on linearly constrained quadratic subproblems, improving the oracle complexity to $\widetilde{\mathcal{O}}(\varepsilon^{-1})$. Surprisingly, despite non-convexity, our methodology does not require any constraint qualifications, can handle hidden convex equality constraints, and achieves complexities matching those for solving unconstrained hidden convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10626v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilyas Fatkhullin, Niao He, Guanghui Lan, Florian Wolf</dc:creator>
    </item>
    <item>
      <title>Optimal control of Volterra integral diffusions and application to contract theory</title>
      <link>https://arxiv.org/abs/2511.09701</link>
      <description>arXiv:2511.09701v1 Announce Type: cross 
Abstract: This paper focuses on the optimal control of a class of stochastic Volterra integral equations. Here the coefficients are regular and not assumed to be of convolution type. We show that, under mild regularity assumptions, these equations can be lifted in a Sobolev space, whose Hilbertian structure allows us to attack the problem through a dynamic programming approach. We are then able to use the theory of viscosity solutions on Hilbert spaces to characterise the value function of the control problem as the unique solution of a parabolic equation on Sobolev space. We provide applications and examples to illustrate the usefulness of our theory, in particular for a certain class of time inconsistent principal agent problems. As a byproduct of our analysis, we introduce a new Markovian approximation for Volterra type dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09701v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Dylan Possama\"i, Mehdi Talbi</dc:creator>
    </item>
    <item>
      <title>Generalized infinite dimensional Alpha-Procrustes based geometries</title>
      <link>https://arxiv.org/abs/2511.09801</link>
      <description>arXiv:2511.09801v1 Announce Type: cross 
Abstract: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09801v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salvish Goomanee, Andi Han, Pratik Jawanpuria, Bamdev Mishra</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Quadratically Regularized Optimal Transport</title>
      <link>https://arxiv.org/abs/2511.09807</link>
      <description>arXiv:2511.09807v1 Announce Type: cross 
Abstract: It is well known that optimal transport suffers from the curse of dimensionality: when the prescribed marginals are approximated by i.i.d. samples, the convergence of the empirical optimal transport problem to the population counterpart slows exponentially with increasing dimension. Entropically regularized optimal transport (EOT) has become the standard bearer in many statistical applications as it avoids this curse. Indeed, EOT has parametric sample complexity, as has been shown in a series of works based on the smoothness of the EOT potentials or the strong concavity of the dual EOT problem. However, EOT produces full-support approximations to the (sparse) OT problem, leading to overspreading in applications, and is computationally unstable for small regularization parameters. The most popular alternative is quadratically regularized optimal transport (QOT), which penalizes couplings by $L^2$ norm instead of relative entropy. QOT produces sparse approximations of OT and is computationally stable. However, its potentials are not smooth (do not belong to a Donsker class) and its dual problem is not strongly concave, hence QOT is often assumed to suffer from the curse of dimensionality. In this paper, we show that QOT nevertheless has parametric sample complexity. More precisely, we establish central limit theorems for its dual potentials, optimal couplings, and optimal costs. Our analysis is based on novel arguments that focus on the regularity of the support of the optimal QOT coupling. Specifically, we establish a Lipschitz property of its sections and leverage VC theory to bound its statistical complexity. Our analysis also leads to gradient estimates of independent interest, including $C^{1,1}$ regularity of the population potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09807v1</guid>
      <category>math.ST</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Eustasio del Barrio, Marcel Nutz</dc:creator>
    </item>
    <item>
      <title>On the Convergence of Overparameterized Problems: Inherent Properties of the Compositional Structure of Neural Networks</title>
      <link>https://arxiv.org/abs/2511.09810</link>
      <description>arXiv:2511.09810v1 Announce Type: cross 
Abstract: This paper investigates how the compositional structure of neural networks shapes their optimization landscape and training dynamics. We analyze the gradient flow associated with overparameterized optimization problems, which can be interpreted as training a neural network with linear activations. Remarkably, we show that the global convergence properties can be derived for any cost function that is proper and real analytic. We then specialize the analysis to scalar-valued cost functions, where the geometry of the landscape can be fully characterized. In this setting, we demonstrate that key structural features -- such as the location and stability of saddle points -- are universal across all admissible costs, depending solely on the overparameterized representation rather than on problem-specific details. Moreover, we show that convergence can be arbitrarily accelerated depending on the initialization, as measured by an imbalance metric introduced in this work. Finally, we discuss how these insights may generalize to neural networks with sigmoidal activations, showing through a simple example which geometric and dynamical properties persist beyond the linear case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09810v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arthur Castello Branco de Oliveira, Dhruv Jatkar, Eduardo Sontag</dc:creator>
    </item>
    <item>
      <title>Implicit Multiple Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2511.09916</link>
      <description>arXiv:2511.09916v1 Announce Type: cross 
Abstract: Recently, triple decomposition has attracted increasing attention for decomposing third-order tensors into three factor tensors. However, this approach is limited to third-order tensors and enforces uniformity in the lower dimensions across all factor tensors, which restricts its flexibility and applicability. To address these issues, we propose the Multiple decomposition, a novel framework that generalizes triple decomposition to arbitrary order tensors and allows the short dimensions of the factor tensors to differ. We establish its connections with other classical tensor decompositions. Furthermore, implicit neural representation (INR) is employed to continuously represent the factor tensors in Multiple decomposition, enabling the method to generalize to non-grid data. We refer to this INR-based Multiple decomposition as Implicit Multiple Tensor Decomposition (IMTD). Then, the Proximal Alternating Least Squares (PALS) algorithm is utilized to solve the IMTD-based tensor reconstruction models. Since the objective function in IMTD-based models often lacks the Kurdyka-Lojasiewicz (KL) property, we establish a KL-free convergence analysis for the algorithm. Finally, extensive numerical experiments further validate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09916v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunjing Yang, Libin Zheng, Minru Bai</dc:creator>
    </item>
    <item>
      <title>Theoretical Analysis of Resource-Induced Phase Transitions in Estimation Strategies</title>
      <link>https://arxiv.org/abs/2511.10184</link>
      <description>arXiv:2511.10184v1 Announce Type: cross 
Abstract: Organisms adapt to volatile environments by integrating sensory information with internal memory, yet their information processing is constrained by resource limitations. Such limitations can fundamentally alter optimal estimation strategies in biological systems. For example, recent experiments suggest that organisms exhibit nonmonotonic phase transitions between memoryless and memory-based estimation strategies depending on sensory reliability. However, an analytical understanding of these resource-induced phase transitions is still missing. This Letter presents an analytical characterization of resource-induced phase transitions in optimal estimation strategies. Our result identifies the conditions under which resource limitations alter estimation strategies and analytically reveals the mechanism underlying the emergence of discontinuous, nonmonotonic, and scaling behaviors. These results provide a theoretical foundation for understanding how limited resources shape information processing in biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10184v1</guid>
      <category>physics.bio-ph</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <category>q-bio.SC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takehiro Tottori, Tetsuya J. Kobayashi</dc:creator>
    </item>
    <item>
      <title>Zeroes and Extrema of Functions via Random Measures</title>
      <link>https://arxiv.org/abs/2511.10293</link>
      <description>arXiv:2511.10293v1 Announce Type: cross 
Abstract: We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10293v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios Christou Micheas</dc:creator>
    </item>
    <item>
      <title>Operator Models for Continuous-Time Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.10383</link>
      <description>arXiv:2511.10383v1 Announce Type: cross 
Abstract: Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10383v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Hoischen, Petar Bevanda, Max Beier, Stefan Sosnowski, Boris Houska, Sandra Hirche</dc:creator>
    </item>
    <item>
      <title>On topological properties of closed attractors</title>
      <link>https://arxiv.org/abs/2511.10429</link>
      <description>arXiv:2511.10429v1 Announce Type: cross 
Abstract: The notion of an attractor has various definitions in the theory of dynamical systems. Under compactness assumptions, several of those definitions coincide and the theory is rather complete. However, without compactness, the picture becomes blurry. To improve our understanding, we characterize in this work when a closed, not necessarily compact, asymptotically stable attractor on a locally compact metric space is homotopy equivalent to its domain of attraction. This enables a further structural study of the corresponding feedback stabilization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10429v1</guid>
      <category>math.DS</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter Jongeneel</dc:creator>
    </item>
    <item>
      <title>Learning parameter-dependent shear viscosity from data, with application to sea and land ice</title>
      <link>https://arxiv.org/abs/2511.10452</link>
      <description>arXiv:2511.10452v1 Announce Type: cross 
Abstract: Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10452v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.flu-dyn</category>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo G. de Diego, Georg Stadler</dc:creator>
    </item>
    <item>
      <title>Formal Verification of Control Lyapunov-Barrier Functions for Safe Stabilization with Bounded Controls</title>
      <link>https://arxiv.org/abs/2511.10510</link>
      <description>arXiv:2511.10510v1 Announce Type: cross 
Abstract: We present verifiable conditions for synthesizing a single smooth Lyapunov function that certifies both asymptotic stability and safety under bounded controls. These sufficient conditions ensure the strict compatibility of a control barrier function (CBF) and a control Lyapunov function (CLF) on the exact safe set certified by the barrier. An explicit smooth control Lyapunov-barrier function (CLBF) is then constructed via a patching formula that is provably correct by design. Two examples illustrate the computational procedure, showing that the proposed approach is less conservative than sum-of-squares (SOS)-based compatible CBF-CLF designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10510v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Liu</dc:creator>
    </item>
    <item>
      <title>Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming</title>
      <link>https://arxiv.org/abs/2511.10639</link>
      <description>arXiv:2511.10639v1 Announce Type: cross 
Abstract: We propose a joint estimation method for the Direction-of-Arrival (DoA) and the Noise Covariance Matrix (NCM) tailored for beamforming applications. Building upon an existing NCM framework, our approach simplifies the estimation procedure by deriving an quasi-linear solution, instead of the traditional exhaustive search. Additionally, we introduce a novel DoA estimation technique that operates across all frequency bins, improving robustness in reverberant environments. Simulation results demonstrate that our method outperforms classical techniques, such as MUSIC, in mid- to high-angle scenarios, achieving lower angular errors and superior signal enhancement through beamforming. The proposed framework was also fared against other techniques for signal enhancement, having better noise rejection and interference canceling capabilities. These improvements are validated using both theoretical and empirical performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10639v1</guid>
      <category>eess.AS</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vitor Gelsleichter Probst Curtarelli</dc:creator>
    </item>
    <item>
      <title>A robust BFGS algorithm for unconstrained nonlinear optimization problems</title>
      <link>https://arxiv.org/abs/1212.5929</link>
      <description>arXiv:1212.5929v2 Announce Type: replace 
Abstract: In this paper, a modified BFGS algorithm is proposed. The modified BFGS matrix estimates a modified Hessian matrix which is a convex combination of an identity matrix for the steepest descent algorithm and a Hessian matrix for the Newton algorithm. The coefficient of the convex combination in the modified BFGS algorithm is dynamically chosen in every iteration. It is proved that, for any twice differentiable nonlinear function (convex or non-convex), the algorithm is globally convergent to a stationary point. If the stationary point is a local optimizer where the Hessian is strongly positive definite in a neighborhood of the optimizer, the iterates will eventually enter and stay in the neighborhood, and the modified BFGS algorithm reduces to the BFGS algorithm in this neighborhood. Therefore, the modified BFGS algorithm is super-linearly convergent. Moreover, the computational cost of the modified BFGS in each iteration is almost the same as the cost of the BFGS. Numerical test on the CUTE test set is reported. The performance of the modified BFGS algorithm implemented in our MATLAB function is compared to the BFGS algorithm implemented in the MATLAB Optimization Toolbox function, a limited memory BFGS implemented as L-BFGS, a descent conjugate gradient algorithm implemented as CG-Descent 5.3, and a limited memory, descent and conjugate algorithm implemented as L-CG-Descent. This result shows that the modified BFGS algorithm may be very effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:1212.5929v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/02331934.2022.2124869</arxiv:DOI>
      <arxiv:journal_reference>Optimization, 73(3), 851-873, 2024</arxiv:journal_reference>
      <dc:creator>Yaguang Yang</dc:creator>
    </item>
    <item>
      <title>Quantum computing and the stable set problem</title>
      <link>https://arxiv.org/abs/2405.12845</link>
      <description>arXiv:2405.12845v4 Announce Type: replace 
Abstract: Given an undirected graph, the stable set problem asks to determine the cardinality of the largest subset of pairwise non-adjacent vertices. This value is called the stability number of the graph, and its computation is an NP-hard problem. In this paper, we solve the stable set problem using the D-Wave quantum annealer. By formulating the problem as a quadratic unconstrained binary optimization problem with the penalty method, we show its optimal value equals the graph's stability number for specific penalty values. However, D-Wave's quantum annealer is a heuristic, so the solutions may be far from the optimum and may not represent stable sets. To address these, we introduce a post-processing procedure that identifies samples that could lead to improved solutions. Additionally, we propose a partitioning method to handle larger instances that cannot be embedded on D-Wave's quantum processing unit. Finally, we investigate how different penalty parameter values affect the solutions' quality. Extensive computational results show that the post-processing procedure significantly improves the solution quality, while the partitioning method successfully extends our approach to medium-size instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12845v4</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10556788.2025.2490639</arxiv:DOI>
      <dc:creator>Alja\v{z} Krpan, Janez Povh, Dunja Pucher</dc:creator>
    </item>
    <item>
      <title>Mean Field Game with Reflected Jump Diffusion Dynamics: A Linear Programming Approach</title>
      <link>https://arxiv.org/abs/2508.20388</link>
      <description>arXiv:2508.20388v2 Announce Type: replace 
Abstract: This paper develops a linear programming approach for mean field games with reflected jump-diffusion dynamics. We first prove the equivalence between the mean field equilibria in the linear programming formulation and those in the weak relaxed control formulation under some measurability and growth conditions on model coefficients. Building upon the characterization of the occupation measure in the equivalence result, we further establish the existence of linear programming mean field equilibria under fairly general conditions on model coefficients. Finally, a numerical example is presented to illustrate the computation of a mean field equilibrium using the linear programming formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20388v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongxia Liang, Xiang Yu, Keyu Zhang</dc:creator>
    </item>
    <item>
      <title>Differential Dynamic Programming for the Optimal Control Problem with an Ellipsoidal Target Set and Its Statistical Inference</title>
      <link>https://arxiv.org/abs/2509.07546</link>
      <description>arXiv:2509.07546v2 Announce Type: replace 
Abstract: This work addresses an extended class of optimal control problems where a target for a system state has the form of an ellipsoid rather than a fixed, single point. As a computationally affordable method for resolving the extended problem, we present a revised version of the differential dynamic programming (DDP), termed the differential dynamic programming with ellipsoidal target set (ETS-DDP). To this end, the problem with an ellipsoidal target set is reformulated into an equivalent form with the orthogonal projection operator, yielding that the resulting cost functions turn out to be discontinuous at some points. As the DDP usually requires the differentiability of cost functions, in the ETS-DDP formulation we locally approximate the (nonsmooth) cost functions to smoothed ones near the path generated at the previous iteration, by utilizing the orthogonal projection operator. Moreover, a statistical inference method is also presented for designing the ellipsoidal target set, based on data on admissible target points collected by expert demonstrations. Via a simulation on autonomous parking of a vehicle, it is seen that the proposed ETS-DDP efficiently derives an admissible state trajectory while running much faster than the point-targeted DDP, at the expense of optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07546v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungjun Eom, Gyunghoon Park</dc:creator>
    </item>
    <item>
      <title>On the Moreau envelope properties of weakly convex functions</title>
      <link>https://arxiv.org/abs/2509.13960</link>
      <description>arXiv:2509.13960v2 Announce Type: replace 
Abstract: In this document, we present the main properties satisfied by the Moreau envelope of weakly convex functions. The Moreau envelope has been introduced in convex optimization to regularize convex functionals while preserving their global minimizers. However, the Moreau envelope is also defined for the more general class of weakly convex function and can be a useful tool for optimization in this context. The main properties of the Moreau envelope have been demonstrated for convex functions and are generalized to weakly convex function in various works. This document summarizes the vast literature on the properties of the Moreau envelope and provides the associated proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13960v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marien Renaud, Arthur Leclaire, Nicolas Papadakis</dc:creator>
    </item>
    <item>
      <title>Automated algorithm design via Nevanlinna-Pick interpolation</title>
      <link>https://arxiv.org/abs/2509.21416</link>
      <description>arXiv:2509.21416v2 Announce Type: replace 
Abstract: The synthesis of optimization algorithms typically follows a design-first-analyze-later approach, which often obscures fundamental performance limitations and hinders the systematic design of algorithms operating at the achievable theoretical boundaries. Recently, a framework based on frequency-domain techniques from robust control theory has emerged as a powerful tool for automating algorithm synthesis. By integrating the design and analysis stages, this framework enables the identification of fundamental performance limits. In this paper, we build on this framework and extend it to address algorithms for solving strongly convex problems with equality constraints. As a result, we obtain a new class of algorithms that offers sharp trade-off between number of matrix multiplication per iteration and convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21416v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim K. Ozaslan, Tryphon T. Georgiou, Mihailo R. Jovanovic</dc:creator>
    </item>
    <item>
      <title>Optimal Control of a Bioeconomic Crop-Energy System with Energy Reinvestment</title>
      <link>https://arxiv.org/abs/2510.11381</link>
      <description>arXiv:2510.11381v2 Announce Type: replace 
Abstract: We develop an optimal control model for allocating agricultural crop residues between bioenergy production and soil fertility restoration. The system captures a novel circular feedback: a fraction of cumulative energy output is reinvested into soil productivity, linking energy use with ecological regeneration. The dynamics are governed by a nonlinear three-state system describing soil fertility, residue biomass, and accumulated energy, with a single control representing the proportion of biomass diverted to energy. The objective is to maximize a discounted net benefit that accounts for energy revenue, soil value, and operational costs. We apply the Pontryagin Maximum Principle in current-value form to derive necessary optimality conditions and characterize the structure of optimal controls. Numerical simulations based on direct optimization reveal interior and switching regimes, and show how planning horizon and reinvestment efficiency influence optimal strategies. The results highlight the strategic role of energy reinvestment in achieving sustainable residue management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11381v2</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.22124/jmm.2025.31369.2815</arxiv:DOI>
      <dc:creator>Othman Cherkaoui Dekkaki</dc:creator>
    </item>
    <item>
      <title>Point Convergence Analysis of the Accelerated Gradient Method for Multiobjective Optimization: Continuous and Discrete</title>
      <link>https://arxiv.org/abs/2510.26382</link>
      <description>arXiv:2510.26382v3 Announce Type: replace 
Abstract: This paper investigates the point convergence of accelerated gradient methods for multiobjective optimization, in both continuous and discrete settings. We address the open problems of whether the solution trajectory of the multiobjective inertial gradient-like dynamical system (MAVD) with asymptotic vanishing damping converges when $\alpha = 3$, and whether the sequence generated by the multiobjective Nesterov accelerated method (MAG) converges to a weakly Pareto optimal solution. For the continuous system (MAVD) with $\alpha = 3$, we prove that the trajectory $x(t)$ converges to a weakly Pareto optimal solution. For the discrete case, we propose a multiobjective accelerated gradient method with a generalized momentum factor (MAG-GM), and prove that the generated sequence $\{x_k\}$ converges to a weakly Pareto optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26382v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingdong Yin</dc:creator>
    </item>
    <item>
      <title>History-Aware Adaptive High-Order Tensor Regularization</title>
      <link>https://arxiv.org/abs/2511.05788</link>
      <description>arXiv:2511.05788v2 Announce Type: replace 
Abstract: In this paper, we develop a new adaptive regularization method for minimizing a composite function, which is the sum of a $p$th-order ($p \ge 1$) Lipschitz continuous function and a simple, convex, and possibly nonsmooth function. We use a history of local Lipschitz estimates to adaptively select the current regularization parameter, an approach we shall term the {\it history-aware adaptive regularization method}. We explore how the selection of an appropriate volume of historical information affects both the theoretical and practical performance. By using all the historical information, our method matches the complexity guarantees of the standard $p$th-order tensor methods that require a known Lipschitz constant, for both convex and nonconvex objectives. In the nonconvex case, the number of iterations required to find an $(\epsilon_g,\epsilon_H)$-approximate second-order stationary point is bounded by $\mathcal{O}(\max\{\epsilon_g^{-(p+1)/p}, \epsilon_H^{-(p+1)/(p-1)}\})$. For convex functions, we establish an $\mathcal{O}(\epsilon^{-1/p})$ iteration complexity for finding an $\epsilon$-approximate optimal point and further propose an accelerated variant attaining an iteration complexity of $\mathcal{O}(\epsilon^{-1/(p+1)})$. For practical consideration, we propose several variants of this method with only part of historical information. We introduce cyclic and sliding-window strategies for choosing historical Lipschitz estimates, which mitigate the limitation of overly conservative updates. As long as a rough upper bound of the Lipschitz constant is known, these two variants achieve the same iteration complexity guarantees in terms of the input accuracy as the method using full historical information. Finally, extensive numerical experiments are conducted to demonstrate the effectiveness of our adaptive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05788v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang He, Bo Jiang, Yuntian Jiang, Chuwen Zhang, Shuzhong Zhang</dc:creator>
    </item>
    <item>
      <title>Equivalence of entropy solutions and gradient flows for pressureless 1D Euler systems</title>
      <link>https://arxiv.org/abs/2312.04932</link>
      <description>arXiv:2312.04932v3 Announce Type: replace-cross 
Abstract: We study distributional solutions of pressureless Euler systems on the line. In particular we show that Lagrangian solutions, introduced by Brenier, Gangbo, Savar\'{e} and Westdickenberg, and entropy solutions, studied by Nguyen and Tudorascu for the Euler--Poisson system, are equivalent. For the Euler--Poisson system this can be seen as a generalization to second-order systems of the equivalence between $L^2$-gradient flows and entropy solutions for a first-order aggregation equation proved by Bonaschi, Carrillo, Di Francesco and Peletier. The key observation is an equivalence between Ole\u{\i}nik's E-condition for conservation laws and a characterization due to Natile and Savar\'{e} of the normal cone for $L^2$-gradient flows. This new equivalence allows us to define unique solutions after blow-up for classical solutions of the Euler--Poisson system with quadratic confinement due to Carrillo, Choi and Zatorska, as well as to describe their asymptotic behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04932v3</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Antonio Carrillo, Sondre Tesdal Galtung</dc:creator>
    </item>
    <item>
      <title>Kernel Modelling of Fading Memory Systems</title>
      <link>https://arxiv.org/abs/2403.11945</link>
      <description>arXiv:2403.11945v4 Announce Type: replace-cross 
Abstract: The paper is a follow-up of the recently introduced kernel-based framework to identify nonlinear input-output systems regularized by desirable input-output incremental properties. Assuming that the system has fading memory, we propose to learn the functional that maps the past input to the present output rather than the operator mapping input trajectories to output trajectories. While retaining the benefits of the previously proposed framework, this modification simplifies the selection of the kernel, enforces causality, and enables temporal simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11945v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongkang Huo, Thomas Chaffey, Rodolphe Sepulchre</dc:creator>
    </item>
    <item>
      <title>The Maximum Theoretical Ground Speed of the Wheeled Vehicle</title>
      <link>https://arxiv.org/abs/2502.15341</link>
      <description>arXiv:2502.15341v2 Announce Type: replace-cross 
Abstract: In this paper, we propose one possible theoretical limit on the maximum ground speed of wheeled vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15341v2</guid>
      <category>physics.class-ph</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Altay Zhakatayev, Mukatai Nemerebayev</dc:creator>
    </item>
    <item>
      <title>Hessian stability and convergence rates for entropic and Sinkhorn potentials via semiconcavity</title>
      <link>https://arxiv.org/abs/2504.11133</link>
      <description>arXiv:2504.11133v2 Announce Type: replace-cross 
Abstract: In this paper we determine quantitative stability bounds for the Hessian of entropic potentials, \ie, the dual solution to the entropic optimal transport problem. To the authors' knowledge this is the first work addressing this second-order quantitative stability estimate in general unbounded settings. Our proof strategy relies on semiconcavity properties of entropic potentials and on the representation of entropic transport plans as laws of forward and backward diffusion processes, known as Schr\"odinger bridges. Moreover, our approach allows to deduce a stochastic proof of quantitative stability estimates for entropic transport plans and for gradients of entropic potentials as well. Finally, as a direct consequence of these stability bounds, we deduce exponential convergence rates for gradient and Hessian of Sinkhorn iterates along Sinkhorn's algorithm, a problem that was still open in unbounded settings. Our rates have a polynomial dependence on the regularization parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11133v2</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giacomo Greco, Luca Tamanini</dc:creator>
    </item>
    <item>
      <title>Optimizing the ground state energy of the three-dimensional magnetic Dirichlet Laplacian with constant magnetic field</title>
      <link>https://arxiv.org/abs/2504.21597</link>
      <description>arXiv:2504.21597v2 Announce Type: replace-cross 
Abstract: This paper concerns the shape optimization problem of minimizing the ground state energy of the magnetic Dirichlet Laplacian with constant magnetic field among three-dimensional domains of fixed volume. In contrast to the two-dimensional case, a generalized ``magnetic'' Faber-Krahn inequality does not hold and the minimizers are not expected to be balls when the magnetic field is turned on. An analysis of the problem among cylindrical domains reveals geometric constraints for general minimizers. In particular, minimizers must elongate with a certain rate along the direction of the magnetic field as the field strength increases. In addition to the theoretical analysis, we present numerical minimizers which confirm this prediction and give rise to further conjectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21597v2</guid>
      <category>math-ph</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Baur</dc:creator>
    </item>
    <item>
      <title>A localized consensus-based sampling algorithm</title>
      <link>https://arxiv.org/abs/2505.24861</link>
      <description>arXiv:2505.24861v3 Announce Type: replace-cross 
Abstract: We propose a localized consensus-based method for sampling from non-Gaussian distributions. This method arises from an alternative derivation of consensus-based sampling (CBS). Starting from ensemble-preconditioned Langevin dynamics, we approximate the potential with a Moreau envelope, replace the gradient in the Langevin equation with a proximal operator, and finally approximate this operator by a weighted mean. Under Gaussian initial and target distributions, this procedure recovers the standard CBS dynamics. In addition, when we retain only the approximations valid beyond the Gaussian case, we retrieve a refined variant of polarized CBS. The resulting algorithm, which we call localized consensus-based sampling, is affine-invariant, exact for Gaussian targets in the mean-field limit, and demonstrates improved robustness over polarized CBS in numerical experiments. Like other consensus-based methods, localized CBS is fully gradient-free and easily parallelizable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24861v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arne Bouillon, Alexander Bodard, Panagiotis Patrinos, Dirk Nuyens, Giovanni Samaey</dc:creator>
    </item>
    <item>
      <title>A Novel Sliced Fused Gromov-Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2508.02364</link>
      <description>arXiv:2508.02364v2 Announce Type: replace-cross 
Abstract: The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are powerful tools for comparing heterogeneous data. Their computation is, however, challenging since both distances are based on non-convex, quadratic optimal transport (OT) problems. Leveraging 1D OT, a sliced version of GW has been proposed to lower the computational burden. Unfortunately, this sliced version is restricted to Euclidean geometry and loses invariance to isometries, strongly limiting its application in practice. To overcome these issues, we propose a novel slicing technique for GW as well as for FGW that is based on an appropriate lower bound, hierarchical OT, and suitable quadrature rules for the underlying 1D OT problems. Our novel sliced FGW significantly reduces the numerical effort while remaining invariant to isometric transformations and allowing the comparison of arbitrary geometries. We show that our new distance actually defines a pseudo-metric for structured spaces that bounds FGW from below and study its interpolation properties between sliced Wasserstein and GW. Since we avoid the underlying quadratic program, our sliced distance is numerically more robust and reliable than the original GW and FGW distance; especially in the context of shape retrieval and graph isomorphism testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02364v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Piening, Robert Beinert</dc:creator>
    </item>
    <item>
      <title>Minimal Regret Walras Equilibria for Combinatorial Markets via Duality, Integrality, and Sensitivity Gaps</title>
      <link>https://arxiv.org/abs/2511.09021</link>
      <description>arXiv:2511.09021v2 Announce Type: replace-cross 
Abstract: We consider combinatorial multi-item markets and propose the notion of a $\Delta$-regret Walras equilibrium, which is an allocation of items to players and a set of item prices that achieve the following goals: prices clear the market, the allocation is capacity-feasible, and the players' strategies lead to a total regret of $\Delta$. The regret is defined as the sum of individual player regrets measured by the utility gap with respect to the optimal item bundle given the prices. We derive necessary and sufficient conditions for the existence of $\Delta$-regret equilibria, where we establish a connection to the duality gap and the integrality gap of the social welfare problem. For the special case of monotone valuations, the derived necessary and sufficient optimality conditions coincide and lead to a complete characterization of achievable $\Delta$-regret equilibria. For general valuations, we establish an interesting connection to the area of sensitivity theory in linear optimization. We show that the sensitivity gap of the optimal-value function of two (configuration) linear programs with changed right-hand side can be used to establish a bound on the achievable regret. Finally, we use these general structural results to translate known approximation algorithms for the social welfare optimization problem into algorithms computing low-regret Walras equilibria. We also demonstrate how to derive strong lower bounds based on integrality and duality gaps but also based on NP-complexity theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09021v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alo\"is Duguet, Tobias Harks, Martin Schmidt, Julian Schwarz</dc:creator>
    </item>
  </channel>
</rss>
