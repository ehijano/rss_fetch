<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 01:37:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A systematic machine learning approach to measure and assess biases in mobile phone population data</title>
      <link>https://arxiv.org/abs/2509.02603</link>
      <description>arXiv:2509.02603v1 Announce Type: new 
Abstract: Traditional sources of population data, such as censuses and surveys, are costly, infrequent, and often unavailable in crisis-affected regions. Mobile phone application data offer near real-time, high-resolution insights into population distribution, but their utility is undermined by unequal access to and use of digital technologies, creating biases that threaten representativeness. Despite growing recognition of these issues, there is still no standard framework to measure and explain such biases, limiting the reliability of digital traces for research and policy. We develop and implement a systematic, replicable framework to quantify coverage bias in aggregated mobile phone application data without requiring individual-level demographic attributes. The approach combines a transparent indicator of population coverage with explainable machine learning to identify contextual drivers of spatial bias. Using four datasets for the United Kingdom benchmarked against the 2021 census, we show that mobile phone data consistently achieve higher population coverage than major national surveys, but substantial biases persist across data sources and subnational areas. Coverage bias is strongly associated with demographic, socioeconomic, and geographic features, often in complex nonlinear ways. Contrary to common assumptions, multi-application datasets do not necessarily reduce bias compared to single-app sources. Our findings establish a foundation for bias assessment standards in mobile phone data, offering practical tools for researchers, statistical agencies, and policymakers to harness these datasets responsibly and equitably.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02603v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carmen Cabrera, Francisco Rowe</dc:creator>
    </item>
    <item>
      <title>Use ADAS Data to Predict Near-Miss Events: A Group-Based Zero-Inflated Poisson Approach</title>
      <link>https://arxiv.org/abs/2509.02614</link>
      <description>arXiv:2509.02614v1 Announce Type: new 
Abstract: Driving behavior big data leverages multi-sensor telematics to understand how people drive and powers applications such as risk evaluation, insurance pricing, and targeted intervention. Usage-based insurance (UBI) built on these data has become mainstream. Telematics-captured near-miss events (NMEs) provide a timely alternative to claim-based risk, but weekly NMEs are sparse, highly zero-inflated, and behaviorally heterogeneous even after exposure normalization. Analyzing multi-sensor telematics and ADAS warnings, we show that the traditional statistical models underfit the dataset. We address these challenges by proposing a set of zero-inflated Poisson (ZIP) frameworks that learn latent behavior groups and fit offset-based count models via EM to yield calibrated, interpretable weekly risk predictions. Using a naturalistic dataset from a fleet of 354 commercial drivers over a year, during which the drivers completed 287,511 trips and logged 8,142,896 km in total, our results show consistent improvements over baselines and prior telematics models, with lower AIC/BIC values in-sample and better calibration out-of-sample. We also conducted sensitivity analyses on the EM-based grouping for the number of clusters, finding that the gains were robust and interpretable. Practically, this supports context-aware ratemaking on a weekly basis and fairer premiums by recognizing heterogeneous driving styles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02614v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinbo Zhang, Montserrat Guillen, Lishuai Li, Xin Li, Youhua Frank Chen</dc:creator>
    </item>
    <item>
      <title>Detecting distinctive structural changes in economic data</title>
      <link>https://arxiv.org/abs/2509.02712</link>
      <description>arXiv:2509.02712v1 Announce Type: new 
Abstract: This article introduces a novel method for detecting distinctive structural changes in economic data, particularly within frequency distribution tables. The approach identifies significant shifts in the distribution of a variable over time or across populations, capturing changes in category shares, enabling a deeper understanding of the underlying dynamics and trends. The method is applicable to both categorical and numerical data and is especially useful in fields such as industrial economics, demography, social science and market analysis, where comparative analysis is essential. Selected numerical examples illustrate its effectiveness in tracking market structure evolution, where shifts in firm-level market shares may signal changing competitive dynamics. The results offer interpretable insights into structural transformations in economic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02712v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joanna D\k{e}bicka, Edyta Mazurek</dc:creator>
    </item>
    <item>
      <title>A Composite-Loss Graph Neural Network for the Multivariate Post-Processing of Ensemble Weather Forecasts</title>
      <link>https://arxiv.org/abs/2509.02784</link>
      <description>arXiv:2509.02784v1 Announce Type: new 
Abstract: Ensemble forecasting systems have advanced meteorology by providing probabilistic estimates of future states, supporting applications from renewable energy production to transportation safety. Nonetheless, systematic biases often persist, making statistical post-processing essential. Traditional parametric post-processing techniques and machine learning-based methods can produce calibrated predictive distributions at specific locations and lead times, yet often struggle to capture dependencies across forecast dimensions. To address this, multivariate post-processing methods-such as ensemble copula coupling and the Schaake shuffle-are widely applied in a second step to restore realistic inter-variable or spatio-temporal dependencies. The aim of this study is the multivariate post-processing of ensemble forecasts using a graph neural network (dualGNN) trained with a composite loss function that combines the energy score (ES) and the variogram score (VS). The method is evaluated on two datasets: WRF-based solar irradiance forecasts over northern Chile and ECMWF visibility forecasts for Central Europe. The dualGNN consistently outperforms all empirical copula-based post-processed forecasts and shows significant improvements compared to graph neural networks trained solely on either the continuous ranked probability score (CRPS) or the ES, according to the evaluated multivariate verification metrics. Furthermore, for the WRF forecasts, the rank-order structure of the dualGNN forecasts captures valuable dependency information, enabling a more effective restoration of spatial relationships than either the raw numerical weather prediction ensemble or historical observational rank structures. By contrast, for the visibility forecasts, the GNNs trained on CRPS, ES, or the ES-VS combination outperform the calibrated reference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02784v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'aria Lakatos</dc:creator>
    </item>
    <item>
      <title>Learning from geometry-aware near-misses to real-time COR: A spatiotemporal grouped random GEV framework</title>
      <link>https://arxiv.org/abs/2509.02871</link>
      <description>arXiv:2509.02871v1 Announce Type: new 
Abstract: Real-time prediction of corridor-level crash occurrence risk (COR) remains challenging, as existing near-miss based extreme value models oversimplify collision geometry, exclude vehicle-infrastructure (V-I) interactions, and inadequately capture spatial heterogeneity in vehicle dynamics. This study introduces a geometry-aware two-dimensional time-to-collision (2D-TTC) indicator within a Hierarchical Bayesian spatiotemporal grouped random parameter (HBSGRP) framework using a non-stationary univariate generalized extreme value (UGEV) model to estimate short-term COR in urban corridors. High-resolution trajectories from the Argoverse-2 dataset, covering 28 locations along Miami's Biscayne Boulevard, were analyzed to extract extreme V-V and V-I near misses. The model incorporates dynamic variables and roadway features as covariates, with partial pooling across locations to address unobserved heterogeneity. Results show that the HBSGRP-UGEV framework outperforms fixed-parameter alternatives, reducing DIC by up to 7.5% for V-V and 3.1% for V-I near-misses. Predictive validation using ROC-AUC confirms strong performance: 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections. Model interpretation reveals that relative speed and distance dominate V-V risks at intersections and segments, with deceleration critical in segments, while V-I risks are driven by speed, boundary proximity, and steering/heading adjustments. These findings highlight the value of a statistically rigorous, geometry-sensitive, and spatially adaptive modeling approach for proactive corridor-level safety management, supporting real-time interventions and long-term design strategies aligned with Vision Zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02871v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Prediction of Maximum Temperature record in Spain 1960-2023 by the means of ERA5 atmospheric geopotentials</title>
      <link>https://arxiv.org/abs/2509.03459</link>
      <description>arXiv:2509.03459v1 Announce Type: new 
Abstract: The increasing frequency of extreme temperature events, such as daily maximum temperature ($T_x$) records, underscores the need for robust tools to understand their drivers and predict their occurrence. Previous studies have identified increasing and non-stationary trends in $T_x$ records across the Iberian Peninsula, particularly during summer, the literature directly exploring their connection with upper-level atmospheric covariates remains limited. This work develops and applies an innovative methodological framework to model the occurrence of $T_x$ records and their relationship with geopotential height fields. We used daily $T_x$ data from 36 Spanish stations (1960-2023) provided by ECA&amp;D and geopotential height data at 300, 500, and 700 hPa from ERA5. Exploratory analysis revealed a non-stationary trend in records, a higher frequency in the interior of the peninsula, and decreasing spatial co-occurrence with distance. We designed a hierarchical spatio-temporal logistic regression algorithm prioritizing interpretability and high-dimensionality reduction. The approach involves: (1) fitting local models per station; (2) applying a spatial consensus filter based on statistical significance to reduce the initial 1620 covariates to 17 in a base model (M1); and (3) a controlled incorporation of interaction terms. Among the tested models, a global model (M2) that enhances M1 with geodetic interactions was selected for its optimal balance between predictive performance (AUC) and complexity. Model M2 demonstrates high predictive accuracy at interior stations and good performance at coastal stations. It also adequately reproduces key observed properties, including the persistence of record streaks and patterns of spatial co-occurrence. This study provides a novel tool for predicting upcoming record events with high accuracy while maintaining a concise and interpretable structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03459v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elsa Barrio-Torres, Jes\'us Abaurrea, Jes\'us As\'in, Jorge Castillo-Mateo, Ana Carmen Cebri\'an, Zeus Gracia</dc:creator>
    </item>
    <item>
      <title>Robust Survival Estimation under Interval Censoring: Expectation-Maximization and Bayesian Accelerated Failure Time Assessment via Simulation and Application</title>
      <link>https://arxiv.org/abs/2509.02634</link>
      <description>arXiv:2509.02634v1 Announce Type: cross 
Abstract: Interval censoring occurs when event times are only known to fall between scheduled assessments, a common design in clinical trials, epidemiology, and reliability studies. Standard right-censoring methods, such as Kaplan-Meier and Cox regression, are not directly applicable and can produce biased results. This study compares three complementary approaches for interval-censored survival data. First, the Turnbull nonparametric maximum likelihood estimator (NPMLE) via the EM algorithm recovers the survival distribution without strong assumptions. Second, Weibull and log-normal accelerated failure time (AFT) models with interval likelihoods provide smooth, covariate-adjusted survival curves and interpretable time-ratio effects. Third, Bayesian AFT models extend these tools by quantifying posterior uncertainty, incorporating prior information, and enabling interval-aware model comparisons via PSIS-LOO cross-validation. Simulations across generating distributions, censoring intensities, sample sizes, and covariate structures evaluated the integrated squared error (ISE) for curve recovery, integrated Brier score (IBS) for prediction, and coverage for uncertainty calibration. Results show that the EM achieves the lowest ISE for distribution recovery, AFT models improve predictive performance when families are correctly specified, and Bayesian AFT offers calibrated uncertainty and principled model selection. An application to the ovarian cancer dataset, restructured into interval-censored form, demonstrates the workflow in practice: the EM algorithm reveals the baseline shape, parametric AFT provides covariate-adjusted predictions, and Bayesian AFT validates model adequacy through posterior predictive checks. Together, these methods form a tiered strategy: EM for shape discovery, AFT for covariate-driven prediction, and Bayesian AFT for complete uncertainty quantification and model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02634v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J. T. Korley</dc:creator>
    </item>
    <item>
      <title>Optimizing Prognostic Biomarker Discovery in Pancreatic Cancer Through Hybrid Ensemble Feature Selection and Multi-Omics Data</title>
      <link>https://arxiv.org/abs/2509.02648</link>
      <description>arXiv:2509.02648v1 Announce Type: cross 
Abstract: Prediction of patient survival using high-dimensional multi-omics data requires systematic feature selection methods that ensure predictive performance, sparsity, and reliability for prognostic biomarker discovery. We developed a hybrid ensemble feature selection (hEFS) approach that combines data subsampling with multiple prognostic models, integrating both embedded and wrapper-based strategies for survival prediction. Omics features are ranked using a voting-theory-inspired aggregation mechanism across models and subsamples, while the optimal number of features is selected via a Pareto front, balancing predictive accuracy and model sparsity without any user-defined thresholds. When applied to multi-omics datasets from three pancreatic cancer cohorts, hEFS identifies significantly fewer and more stable biomarkers compared to the conventional, late-fusion CoxLasso models, while maintaining comparable discrimination performance. Implemented within the open-source mlr3fselect R package, hEFS offers a robust, interpretable, and clinically valuable tool for prognostic modelling and biomarker discovery in high-dimensional survival settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02648v1</guid>
      <category>q-bio.GN</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Zobolas, Anne-Marie George, Alberto L\'opez, Sebastian Fischer, Marc Becker, Tero Aittokallio</dc:creator>
    </item>
    <item>
      <title>Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction</title>
      <link>https://arxiv.org/abs/2509.02826</link>
      <description>arXiv:2509.02826v1 Announce Type: cross 
Abstract: Obesity is a critical global health issue driven by dietary, physiological, and environmental factors, and is strongly associated with chronic diseases such as diabetes, cardiovascular disorders, and cancer. Machine learning has emerged as a promising approach for early obesity risk prediction, yet a comparative evaluation of ensemble techniques -- particularly hybrid majority voting and ensemble stacking -- remains limited. This study aims to compare hybrid majority voting and ensemble stacking methods for obesity risk prediction, identifying which approach delivers higher accuracy and efficiency. The analysis seeks to highlight the complementary strengths of these ensemble techniques in guiding better predictive model selection for healthcare applications. Two datasets were utilized to evaluate three ensemble models: Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer Perceptron as meta-classifier). A pool of nine Machine Learning (ML) algorithms, evaluated across a total of 50 hyperparameter configurations, was analyzed to identify the top three models to serve as base learners for the ensemble methods. Preprocessing steps involved dataset balancing, and outlier detection, and model performance was evaluated using Accuracy and F1-Score. On Dataset-1, weighted hard voting and stacking achieved nearly identical performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard voting. On Dataset-2, stacking demonstrated superior results (Accuracy: 0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707, F1: 0.981675) and weighted hard voting, which showed the lowest performance. The findings confirm that ensemble stacking provides stronger predictive capability, particularly for complex data distributions, while hybrid majority voting remains a robust alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02826v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Towhidul Islam, Md Sumon Ali</dc:creator>
    </item>
    <item>
      <title>Bayesian Network Propensity Score to Evaluate Treatment Effects in Observational Studies</title>
      <link>https://arxiv.org/abs/2509.03194</link>
      <description>arXiv:2509.03194v1 Announce Type: cross 
Abstract: This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel approach for estimating treatment effects in observational studies characterized by unknown (and likely unbalanced) designs and complex dependency structures among covariates. Traditional methods, such as logistic regression, often impose rigid parametric assumptions that may lead to misspecification errors, compromising causal inference. Recent classical and machine learning alternatives, such as boosted CART, random forests, and Stable Balancing Weights, seem to be attractive in a predictive perspective, but they typically lack asymptotic properties, such as consistency, efficiency, and valid variance estimation. In contrast, the recently proposed BNPS to estimate propensity scores uses Bayesian Networks to flexibly model conditional dependencies while preserving essential statistical properties such as consistency, asymptotic normality and asymptotic efficiency. Combined with the H\'ajek estimator, BNPS enables robust estimation of the Average Treatment Effect (ATE) in scenarios with strong covariate interactions and unknown data-generating mechanisms. Through extensive simulations across fifteen realistic scenarios and varying sample sizes, BNPS consistently outperforms benchmark methods in both empirical rejection rates and coverage accuracy. Finally, an application to a real-world dataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan, Italy) demonstrates BNPS's practical value in assessing the impact of pelvic lymph node dissection on hospitalization duration and biochemical recurrence. The findings support BNPS as a statistically robust, interpretable and transparent alternative for causal inference in complex observational settings, enhancing the reliability of evidence from real-world biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03194v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clelia Di Serio, Federica Cugnata, Pier Luigi Conti, Alberto Briganti, Fulvia Mecatti, Paola Vicard, Paola Maria Vittoria Rancoita</dc:creator>
    </item>
    <item>
      <title>Exponentially weighted moving average chart using zero-inflated negative binomial distribution</title>
      <link>https://arxiv.org/abs/2509.03304</link>
      <description>arXiv:2509.03304v1 Announce Type: cross 
Abstract: Zero-inflated models are frequently used to deal with data having many zeros. A commonly used model for over-dispersed data containing zeros is known as the zero-inflated Poisson model. However, to account for the heterogeneity of counts that leads to excess variance besides inflation of zeros in the data using a more flexible model than the zero-inflated Poisson model, a zero-inflated negative binomial (ZINB) is suggested. In the present study, Shewhart and exponentially weighted moving average (EWMA) control charts are suggested to monitor the ZINB data. The charts are compared using the average run length and standard deviation of run length by using extensive Monte Carlo simulations. Besides a comprehensive simulation study assuming different settings of parameters of ZINB, a real data set is used to show the practicality of the proposed charts. The results indicate that the EWMA chart is better than the Shewhart chart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03304v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00949655.2024.2385014</arxiv:DOI>
      <dc:creator>Ali Abbas, Sajid Ali, Ismail Shah</dc:creator>
    </item>
    <item>
      <title>Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</title>
      <link>https://arxiv.org/abs/2509.03515</link>
      <description>arXiv:2509.03515v1 Announce Type: cross 
Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03515v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour</dc:creator>
    </item>
    <item>
      <title>Unveiling the Impact of Social and Environmental Determinants of Health on Lung Function Decline in Cystic Fibrosis through Data Integration using the US Registry</title>
      <link>https://arxiv.org/abs/2506.08731</link>
      <description>arXiv:2506.08731v3 Announce Type: replace 
Abstract: Integrating diverse data sources offers a comprehensive view of patient health and holds potential for improving clinical decision-making. In Cystic Fibrosis (CF), which is a genetic disorder primarily affecting the lungs, biomarkers that track lung function decline such as FEV1 serve as important predictors for assessing disease progression. Prior research has shown that incorporating social and environmental determinants of health improves prognostic accuracy. To investigate the lung function decline among individuals with CF, we integrate data from the U.S. Cystic Fibrosis Foundation Patient Registry with social and environmental health information. Our analysis focuses on the relationship between lung function and the deprivation index, a composite measure of socioeconomic status. We used advanced multivariate mixed-effects models, which allow for the joint modelling of multiple longitudinal outcomes with flexible functional forms. This methodology provides an understanding of interrelationships among outcomes, addressing the complexities of dynamic health data. We examine whether this relationship varies with patients' exposure duration to high-deprivation areas, analyzing data across time and within individual US states. Results show a strong relation between lung function and the area under the deprivation index curve across all states. These results underscore the importance of integrating social and environmental determinants of health into clinical models of disease progression. By accounting for broader contextual factors, healthcare providers can gain deeper insights into disease trajectories and design more targeted intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08731v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleni-Rosalina Andrinopoulou, Emrah Gecili, Rhonda D Szczesniak</dc:creator>
    </item>
    <item>
      <title>Persistence diagrams as morphological signatures of cells: A method to measure and compare cells within a population</title>
      <link>https://arxiv.org/abs/2310.20644</link>
      <description>arXiv:2310.20644v2 Announce Type: replace-cross 
Abstract: Cell biologists study in parallel the morphology of cells with the regulation mechanisms that modify this morphology. Such studies are complicated by the inherent heterogeneity present in the cell population. It remains difficult to define the morphology of a cell with parameters that can quantify this heterogeneity, leaving the cell biologist to rely on manual inspection of cell images. We propose an alternative to this manual inspection that is based on topological data analysis. We characterise the shape of a cell by its contour and nucleus. We build a filtering of the edges defining the contour using a radial distance function initiated from the nucleus. This filtering is then used to construct a persistence diagram that serves as a signature of the cell shape. Two cells can then be compared by computing the Wasserstein distance between their persistence diagrams. Given a cell population, we then compute a distance matrix that includes all pairwise distances between its members. We analyse this distance matrix using hierarchical clustering with different linkage schemes and define a purity score that quantifies consistency between those different schemes, which can then be used to assess homogeneity within the cell population. We illustrate and validate our approach to identify sub-populations in human mesenchymal stem cell populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20644v2</guid>
      <category>q-bio.QM</category>
      <category>math.AT</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yossi Bokor Bleile, Pooja Yadav, Patrice Koehl, Florian Rehfeldt</dc:creator>
    </item>
    <item>
      <title>Multilevel Primary Aim Analyses of Clustered SMARTs: With Applications in Health Policy</title>
      <link>https://arxiv.org/abs/2503.08987</link>
      <description>arXiv:2503.08987v2 Announce Type: replace-cross 
Abstract: In many health policy settings, adaptive interventions target a population of clusters (e.g., schools), with the ultimate intent of impacting outcomes at the level of individuals within the clusters. Health policy researchers can use clustered, sequential, multiple assignment, randomized trials (SMARTs) to answer important scientific questions concerning clustered adaptive interventions. A common primary aim is to compare the mean of a nested, end-of-study outcome between two clustered adaptive interventions. However, existing methods are not suitable when the primary outcome in a clustered SMART is nested and longitudinal (e.g., repeated outcome measures nested within mental healthcare providers, and mental healthcare providers nested within schools). This manuscript proposes a three-level marginal mean modeling and estimation approach for comparing adaptive interventions in a clustered SMART. The proposed method enables policy analysts to answer a wider array of scientific questions in the marginal comparison of clustered adaptive interventions. Further, relative to using an existing two-level method with a nested end-of-study outcome, the proposed method benefits from improved statistical efficiency. With this approach, we examine longitudinal comparisons of adaptive interventions for improving school-based mental healthcare and contrast its performance with existing approaches for studying static end-of-study outcomes. Methods were motivated by the Adaptive School-Based Implementation of CBT (ASIC) study, a clustered SMART designed to construct an adaptive health policy to improve the adoption of evidence-based CBT by mental healthcare professionals in high schools across Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08987v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Durham, Anil Battalahalli, Amy Kilbourne, Andrew Quanbeck, Wenchu Pan, Tim Lycurgus, Daniel Almirall</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data</title>
      <link>https://arxiv.org/abs/2503.15745</link>
      <description>arXiv:2503.15745v2 Announce Type: replace-cross 
Abstract: The heterogeneous treatment effect plays a crucial role in precision medicine.There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus bias function to characterize the effect of biases caused by unmeasured confounders, censoring, and outcome heterogeneity, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the bias function. We further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology outperforms the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15745v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangcai Mao, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>An integrated method for clustering and association network inference</title>
      <link>https://arxiv.org/abs/2503.22467</link>
      <description>arXiv:2503.22467v2 Announce Type: replace-cross 
Abstract: High dimensional Gaussian graphical models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. The Normal-Block model is introduced, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. The approach builds on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies. A zero-inflated version of the model is also proposed to account for real-world data properties. For the inference procedure, two approaches are introduced, a straightforward method based on state-of-the-art approaches and an original, more rigorous method that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called \textbf{normalblockr}, is available on github\footnote{https://github.com/jeannetous/normalblockr}. The results of the models in terms of clustering and network inference are presented, using both simulated data and various types of real-world data (proteomics and words occurrences on webpages).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22467v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanne Tous, Julien Chiquet</dc:creator>
    </item>
    <item>
      <title>Population-Scale Network Embeddings Expose Educational Divides in Network Structure Related to Right-Wing Populist Voting</title>
      <link>https://arxiv.org/abs/2508.21236</link>
      <description>arXiv:2508.21236v2 Announce Type: replace-cross 
Abstract: Administrative registry data can be used to construct population-scale networks whose ties reflect shared social contexts between persons. With machine learning, such networks can be encoded into numerical representations -- embeddings -- that automatically capture individuals' position within the network. We created embeddings for all persons in the Dutch population from a population-scale network that represents five shared contexts: neighborhood, work, family, household, and school. To assess the informativeness of these embeddings, we used them to predict right-wing populist voting. Embeddings alone predicted right-wing populist voting above chance-level but performed worse than individual characteristics. Combining the best subset of embeddings with individual characteristics only slightly improved predictions. After transforming the embeddings to make their dimensions more sparse and orthogonal, we found that one embedding dimension was strongly associated with the outcome. Mapping this dimension back to the population network revealed differences in network structure related to right-wing populist voting between different school ties and achieved education levels. Our study contributes methodologically by demonstrating how population-scale network embeddings can be made interpretable, and substantively by linking structural network differences in education to right-wing populist voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21236v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte L\"uken, Javier Garcia-Bernardo, Sreeparna Deb, Flavio Hafner, Megha Khosla</dc:creator>
    </item>
  </channel>
</rss>
