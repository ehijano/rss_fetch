<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Efficient Reactive Synthesis</title>
      <link>https://arxiv.org/abs/2404.17834</link>
      <description>arXiv:2404.17834v1 Announce Type: new 
Abstract: Our main result is a polynomial time algorithm for deciding realizability for the GXU sublogic of linear temporal logic. This logic is particularly suitable for the specification of embedded control systems, and it is more expressive than GR(1). Reactive control programs for GXU specifications are represented as Mealy machines, which are extended by the monitoring of input events. Now, realizability for GXU specifications is shown to be equivalent to solving a certain subclass of 2QBF satisfiability problems. These logical problems can be solved in cubic time in the size of GXU specifications. For unrealizable GXU specifications, stronger environment assumptions are mined from failed consistency checks based on Padoa's characterization of definability and Craig interpolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17834v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Ye, Harald Ruess</dc:creator>
    </item>
    <item>
      <title>The Theory and Practice of Deductive Verification of OCaml Programs</title>
      <link>https://arxiv.org/abs/2404.17901</link>
      <description>arXiv:2404.17901v1 Announce Type: new 
Abstract: Despite all the tremendous recent success of deductive verification, it is rarely the case that verification tools are applied to programs written in functional languages. When compared to the imperative world, there are only a handful of verification tools that can deal with functional programs. We believe the lack of pedagogical, problem-oriented documentation on how to use such tools might be one of the reasons behind this apparent mismatch between deductive verification and real-world functional software. In this paper, our goal is to drift away from this tendency. We chose the OCaml language as our working environment and provide a comprehensive, hands-on tutorial on how to apply different verification tools to OCaml-written programs. Our presentation takes an incremental approach: we first focus on purely functional programs; then on imperative programs, yet avoid pointers; finally, we use Separation Logic to reason about pointer-manipulating programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17901v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M\'ario Pereira</dc:creator>
    </item>
    <item>
      <title>A Formal Model to Prove Instantiation Termination for E-matching-Based Axiomatisations (Extended Version)</title>
      <link>https://arxiv.org/abs/2404.18007</link>
      <description>arXiv:2404.18007v1 Announce Type: new 
Abstract: SMT-based program analysis and verification often involve reasoning about program features that have been specified using quantifiers; incorporating quantifiers into SMT-based reasoning is, however, known to be challenging. If quantifier instantiation is not carefully controlled, then runtime and outcomes can be brittle and hard to predict. In particular, uncontrolled quantifier instantiation can lead to unexpected incompleteness and even non-termination. E-matching is the most widely-used approach for controlling quantifier instantiation, but when axiomatisations are complex, even experts cannot tell if their use of E-matching guarantees completeness or termination.
  This paper presents a new formal model that facilitates the proof, once and for all, that giving a complex E-matching-based axiomatisation to an SMT solver, such as Z3 or cvc5, will not cause non-termination. Key to our technique is an operational semantics for solver behaviour that models how the E-matching rules common to most solvers are used to determine when quantifier instantiations are enabled, but abstracts over irrelevant details of individual solvers. We demonstrate the effectiveness of our technique by presenting a termination proof for a set theory axiomatisation adapted from those used in the Dafny and Viper verifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18007v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Ge, Ronald Garcia, Alexander J. Summers</dc:creator>
    </item>
    <item>
      <title>Type Inference for Isabelle2Cpp</title>
      <link>https://arxiv.org/abs/2404.18067</link>
      <description>arXiv:2404.18067v1 Announce Type: new 
Abstract: Isabelle2Cpp is a code generation framework that supports automatic generation of C++ code from Isabelle/HOL specifications. However, if some type information of Isabelle/HOL specification is missing, Isabelle2Cpp may not complete the code generation automatically. In order to solve this problem, this paper provides a type system for Isabelle2Cpp, which is used to perform type inference and type unification for expressions of the intermediate representation in Isabelle2Cpp. The system introduces new type inference rules and unification algorithms to enhance the Isabelle2Cpp framework. By incorporating the type system, the Isabelle2Cpp framework can provide more comprehensive type information for expression generation, which leads to more complete and accurate C++ code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18067v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongchen Jiang, Chenxi Fu</dc:creator>
    </item>
    <item>
      <title>Parameterized Dynamic Logic -- Towards A Cyclic Logical Framework for Program Verification via Operational Semantics</title>
      <link>https://arxiv.org/abs/2404.18098</link>
      <description>arXiv:2404.18098v1 Announce Type: new 
Abstract: Dynamic logic and its variations, because of their good expressive forms capturing program specifications clearly by isolating programs from logical formulas, have been used as a formalism in program reasoning for decades and have many applications in different areas. The program models of traditional dynamic logics are in explicit forms. With a clearly-defined syntactic structure, compositional verification is made possible, in which a deduction step transfers proving a program into proving its sub-programs. This structure-based reasoning forms the basis of many dynamic logics and popular Hoare-style logics. However, structural rules induce a major drawback that for different target programs, different rules have to be proposed to adapt different program structures. Moreover, there exist programs that does not support (or not entirely support) a structure-based reasoning. In this paper, we propose a parameterized `dynamic-logic-like' logic called DLp with general forms of program models and formulas, and propose a cyclic proof system for this logic. Program reasoning in DLp is directly based on symbolic executions of programs according to their operational semantics. This reduces the burden of designing a large set of rules when specializing a logic theory to a specific domain, and facilitates verifying programs without a suitable structure for direct reasoning. Without reasoning by dissolving program structures, DLp can cause an infinite proof structure. To solve this, we build a cyclic preproof structure for the proof system of DLp and prove its soundness. Case studies are analyzed to show how DLp works for reasoning about different types of programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18098v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanrui Zhang</dc:creator>
    </item>
    <item>
      <title>Decidability of Graph Neural Networks via Logical Characterizations</title>
      <link>https://arxiv.org/abs/2404.18151</link>
      <description>arXiv:2404.18151v1 Announce Type: new 
Abstract: We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving "Presburger quantifiers". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18151v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Benedikt, Chia-Hsuan Lu, Boris Motik, Tony Tan</dc:creator>
    </item>
    <item>
      <title>LEGO-like Small-Model Constructions for {\AA}qvist's Logics</title>
      <link>https://arxiv.org/abs/2404.18205</link>
      <description>arXiv:2404.18205v1 Announce Type: new 
Abstract: {\AA}qvist's logics (E, F, F+(CM), and G) are among the best-known systems in the long tradition of preference-based approaches for modeling conditional obligation. While the general semantics of preference models align well with philosophical intuitions, more constructive characterizations are needed to assess computational complexity and facilitate automated deduction. Existing small model constructions from conditional logics (due to Friedman and Halpern) are applicable only to F+(CM) and G, while recently developed proof-theoretic characterizations leave unresolved the exact complexity of theoremhood in logic F. In this paper, we introduce alternative small model constructions, obtained uniformly for all four {\AA}qvist's logics. Our constructions propose alternative semantical characterizations and imply co-NP-completeness of theoremhood. Furthermore, they can be naturally encoded in classical propositional logic for automated deduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18205v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Rozplokhas</dc:creator>
    </item>
    <item>
      <title>Tracy, Traces, and Transducers: Computable Counterexamples and Explanations for HyperLTL Model-Checking</title>
      <link>https://arxiv.org/abs/2404.18280</link>
      <description>arXiv:2404.18280v1 Announce Type: new 
Abstract: HyperLTL model-checking enables the automated verification of information-flow properties for security-critical systems. However, it only provides a binary answer. Here, we introduce two paradigms to compute counterexamples and explanations for HyperLTL model-checking, thereby considerably increasing its usefulness. Both paradigms are based on the maxim ``counterexamples/explanations are Skolem functions for the existentially quantified trace variables''.
  Our first paradigm is complete (everything can be explained), but restricted to ultimately periodic system traces. The second paradigm works with (Turing machine) computable Skolem functions and is therefore much more general, but also shown incomplete (not everything can computably be explained). Finally, we prove that it is decidable whether a given finite transition system and a formula have computable Skolem functions witnessing that the system satisfies the formula. Our algorithm also computes transducers implementing computable Skolem functions, if they exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18280v1</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Winter, Martin Zimmermann</dc:creator>
    </item>
    <item>
      <title>The Essense of Useful Evaluation Through Quantitative Types (Extended Version)</title>
      <link>https://arxiv.org/abs/2404.18874</link>
      <description>arXiv:2404.18874v1 Announce Type: new 
Abstract: Several evaluation notions for lambda calculus qualify as reasonable cost models according to Slot and van Emde Boas' Invariance Thesis. A notable result achieved by Accattoli and Dal Lago is that leftmost-outermost reduction is reasonable, where the term representation uses sharing and the steps are useful. These results, initially studied in call-by-name, have also been extended to call-by-value. However, the existing formulations of usefulness lack inductive structure, making it challenging in particular to define and reason about type systems on top of the untyped syntax. Additionally, no type-based quantitative interpretations exist for useful evaluation. In this work, we establish the first inductive definition of useful evaluation for open weak call-by-value. This new useful strategy connects to a previous implementation of usefulness through a low-level abstract machine, incurring only in linear time overhead, thus providing a reasonable cost model for open call-by-value implementation. We also propose a semantic interpretation of useful call-by-value using a non-idempotent intersection type system equipped with a notion of tightness. The resulting interpretation is quantitative, i.e. provides exact step-count information for program evaluation. This turns out to be the first semantical interpretation in the literature for a notion of useful evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18874v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Barenbaum, Delia Kesner, Mariana Milicich</dc:creator>
    </item>
    <item>
      <title>Middle Architecture Criteria</title>
      <link>https://arxiv.org/abs/2404.17757</link>
      <description>arXiv:2404.17757v1 Announce Type: cross 
Abstract: Mid-level ontologies are used to integrate terminologies and data across disparate domains. There are, however, no clear, defensible criteria for determining whether a given ontology should count as mid-level, because we lack a rigorous characterization of what the middle level of generality is supposed to contain. Attempts to provide such a characterization have failed, we believe, because they have focused on the goal of specifying what is characteristic of those single ontologies that have been advanced as mid-level ontologies. Unfortunately, single ontologies of this sort are generally a mixture of top- and mid-level, and sometimes even of domain-level terms. To gain clarity, we aim to specify the necessary and sufficient conditions for a collection of one or more ontologies to inhabit what we call a mid-level architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17757v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Beverley, Giacomo De Colle, Mark Jensen, Carter Benson, Barry Smith</dc:creator>
    </item>
    <item>
      <title>The Common Core Ontologies</title>
      <link>https://arxiv.org/abs/2404.17758</link>
      <description>arXiv:2404.17758v1 Announce Type: cross 
Abstract: The Common Core Ontologies (CCO) are designed as a mid-level ontology suite that extends the Basic Formal Ontology. CCO has since been increasingly adopted by a broad group of users and applications and is proposed as the first standard mid-level ontology. Despite these successes, documentation of the contents and design patterns of the CCO has been comparatively minimal. This paper is a step toward providing enhanced documentation for the mid-level ontology suite through a discussion of the contents of the eleven ontologies that collectively comprise the Common Core Ontology suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17758v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Jensen, Giacomo De Colle, Sean Kindya, Cameron More, Alexander P. Cox, John Beverley</dc:creator>
    </item>
    <item>
      <title>Scalable, Interpretable Distributed Protocol Verification by Inductive Proof Slicing</title>
      <link>https://arxiv.org/abs/2404.18048</link>
      <description>arXiv:2404.18048v1 Announce Type: cross 
Abstract: Many techniques for automated inference of inductive invariants for distributed protocols have been developed over the past several years, but their performance can still be unpredictable and their failure modes opaque for large-scale verification tasks. In this paper, we present inductive proof slicing, a new automated, compositional technique for inductive invariant inference that scales effectively to large distributed protocol verification tasks. Our technique is built on a core, novel data structure, the inductive proof graph, which explicitly represents the lemma and action dependencies of an inductive invariant and is built incrementally during the inference procedure, backwards from a target safety property. We present an invariant inference algorithm that integrates localized syntax-guided lemma synthesis routines at nodes of this graph, which are accelerated by computation of localized grammar and state variable slices. Additionally, in the case of failure to produce a complete inductive invariant, maintenance of this proof graph structure allows failures to be localized to small sub-components of this graph, enabling fine-grained failure diagnosis and repair by a user. We evaluate our technique on several complex distributed and concurrent protocols, including a large scale specification of the Raft consensus protocol, which is beyond the capabilities of modern distributed protocol verification tools, and also demonstrate how its interpretability features allow effective diagnosis and repair in cases of initial failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18048v1</guid>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Schultz, Edward Ashton, Heidi Howard, Stavros Tripakis</dc:creator>
    </item>
    <item>
      <title>Domain Reasoning in TopKAT</title>
      <link>https://arxiv.org/abs/2404.18417</link>
      <description>arXiv:2404.18417v1 Announce Type: cross 
Abstract: TopKAT is the algebraic theory of Kleene algebra with tests (KAT) extended with a top element. Compared to KAT, one pleasant feature of TopKAT is that, in relational models, the top element allows us to express the domain and codomain of a relation. This enables several applications in program logics, such as proving under-approximate specifications or reachability properties of imperative programs. However, while TopKAT inherits many pleasant features of KATs, such as having a decidable equational theory, it is incomplete with respect to relational models. In other words, there are properties that hold true of all relational TopKATs but cannot be proved with the axioms of TopKAT. This issue is potentially worrisome for program-logic applications, in which relational models play a key role.
  In this paper, we further investigate the completeness properties of TopKAT with respect to relational models. We show that TopKAT is complete with respect to (co)domain comparison of KAT terms, but incomplete when comparing the (co)domain of arbitrary TopKAT terms. Since the encoding of under-approximate specifications in TopKAT hinges on this type of formula, the aforementioned incompleteness results have a limited impact when using TopKAT to reason about such specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18417v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ICALP.2024.133</arxiv:DOI>
      <dc:creator>Cheng Zhang, Arthur Azevedo de Amorim, Marco Gaboardi</dc:creator>
    </item>
    <item>
      <title>When Lawvere meets Peirce: an equational presentation of boolean hyperdoctrines</title>
      <link>https://arxiv.org/abs/2404.18795</link>
      <description>arXiv:2404.18795v1 Announce Type: cross 
Abstract: Fo-bicategories are a categorification of Peirce's calculus of relations. Notably, their laws provide a proof system for first-order logic that is both purely equational and complete. This paper illustrates a correspondence between fo-bicategories and Lawvere's hyperdoctrines. To streamline our proof, we introduce peircean bicategories, which offer a more succinct characterization of fo-bicategories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18795v1</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Bonchi, Alessandro Di Giorgio, Davide Trotta</dc:creator>
    </item>
    <item>
      <title>Safe Reach Set Computation via Neural Barrier Certificates</title>
      <link>https://arxiv.org/abs/2404.18813</link>
      <description>arXiv:2404.18813v1 Announce Type: cross 
Abstract: We present a novel technique for online safety verification of autonomous systems, which performs reachability analysis efficiently for both bounded and unbounded horizons by employing neural barrier certificates. Our approach uses barrier certificates given by parameterized neural networks that depend on a given initial set, unsafe sets, and time horizon. Such networks are trained efficiently offline using system simulations sampled from regions of the state space. We then employ a meta-neural network to generalize the barrier certificates to state space regions that are outside the training set. These certificates are generated and validated online as sound over-approximations of the reachable states, thus either ensuring system safety or activating appropriate alternative actions in unsafe scenarios. We demonstrate our technique on case studies from linear models to nonlinear control-dependent models for online autonomous driving scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18813v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Abate, Sergiy Bogomolov, Alec Edwards, Kostiantyn Potomkin, Sadegh Soudjani, Paolo Zuliani</dc:creator>
    </item>
    <item>
      <title>On classes of bounded tree rank, their interpretations, and efficient sparsification</title>
      <link>https://arxiv.org/abs/2404.18904</link>
      <description>arXiv:2404.18904v1 Announce Type: cross 
Abstract: Graph classes of bounded tree rank were introduced recently in the context of the model checking problem for first-order logic of graphs. These graph classes are a common generalization of graph classes of bounded degree and bounded treedepth, and they are a special case of graph classes of bounded expansion. We introduce a notion of decomposition for these classes and show that these decompositions can be efficiently computed. Also, a natural extension of our decomposition leads to a new characterization and decomposition for graph classes of bounded expansion (and an efficient algorithm computing this decomposition).
  We then focus on interpretations of graph classes of bounded tree rank. We give a characterization of graph classes interpretable in graph classes of tree rank $2$. Importantly, our characterization leads to an efficient sparsification procedure: For any graph class $C$ interpretable in a efficiently bounded graph class of tree rank at most $2$, there is a polynomial time algorithm that to any $G \in C$ computes a (sparse) graph $H$ from a fixed graph class of tree rank at most $2$ such that $G = I(H)$ for a fixed interpretation $I$. To the best of our knowledge, this is the first efficient "interpretation reversal" result that generalizes the result of Gajarsk\'y et al. [LICS 2016], who showed an analogous result for graph classes interpretable in classes of graphs of bounded degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18904v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Gajarsk\'y, Rose McCarty</dc:creator>
    </item>
    <item>
      <title>Towards declarative comparabilities: application to functional dependencies</title>
      <link>https://arxiv.org/abs/1909.12656</link>
      <description>arXiv:1909.12656v4 Announce Type: replace 
Abstract: In real life, data are often of poor quality as a result, for instance, of uncertainty, mismeasurements, missing values or bad inputs. This issue hampers an implicit yet crucial operation of every database management system: equality testing. Indeed, equality is, in the end, a context-dependent operation with a plethora of interpretations. In practice, the treatment of different types of equality is left to programmers, who have to struggle with those interpretations in their code. We propose a new lattice-based declarative framework to address this problem. It allows specification of numerous semantics for equality at a high level of abstraction. To go beyond tuple equality, we study functional dependencies (FDs) in the light of our framework. First, we define abstract FDs, generalizing classical FDs. These lead to the consideration of particular interpretations of equality: realities. Building upon realities and possible/certain answers, we introduce possible/certain FDs and give some related complexity results.</description>
      <guid isPermaLink="false">oai:arXiv.org:1909.12656v4</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lhouari Nourine, Jean Marc Petit, Simon Vilmin</dc:creator>
    </item>
    <item>
      <title>Automata Linear Dynamic Logic on Finite Traces</title>
      <link>https://arxiv.org/abs/2108.12003</link>
      <description>arXiv:2108.12003v2 Announce Type: replace 
Abstract: Temporal logics are widely used by the Formal Methods and AI communities. Linear Temporal Logic is a popular temporal logic and is valued for its ease of use as well as its balance between expressiveness and complexity. LTL is equivalent in expressiveness to Monadic First-Order Logic and satisfiability for LTL is PSPACE-complete. Linear Dynamic Logic (LDL), another temporal logic, is equivalent to Monadic Second-Order Logic, but its method of satisfiability checking cannot be applied to a nontrivial subset of LDL formulas.
  Here we introduce Automata Linear Dynamic Logic on Finite Traces (ALDL_f) and show that satisfiability for ALDL_f formulas is in PSPACE. A variant of Linear Dynamic Logic on Finite Traces (LDL_f), ALDL_f combines propositional logic with nondeterministic finite automata (NFA) to express temporal constraints. ALDL$_f$ is equivalent in expressiveness to Monadic Second-Order Logic. This is a gain in expressiveness over LTL at no cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.12003v2</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kevin W. Smith, Moshe Y. Vardi</dc:creator>
    </item>
    <item>
      <title>Quantitative Equality in Substructural Logic via Lipschitz Doctrines</title>
      <link>https://arxiv.org/abs/2110.05388</link>
      <description>arXiv:2110.05388v3 Announce Type: replace 
Abstract: Substructural logics naturally support a quantitative interpretation of formulas, as they are seen as consumable resources. Distances are the quantitative counterpart of equivalence relations: they measure how much two objects are similar, rather than just saying whether they are equivalent or not. Hence, they provide the natural choice for modelling equality in a substructural setting. In this paper, we develop this idea, using the categorical language of Lawvere's doctrines. We work in a minimal fragment of Linear Logic enriched by graded modalities, which are needed to write a resource sensitive substitution rule for equality, enabling its quantitative interpretation as a distance. We introduce both a deductive calculus and the notion of Lipschitz doctrine to give it a sound and complete categorical semantics. The study of 2-categorical properties of Lipschitz doctrines provides us with a universal construction, which generates examples based for instance on metric spaces and quantitative realisability. Finally, we show how to smoothly extend our results to richer substructural logics, up to full Linear Logic with quantifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.05388v3</guid>
      <category>cs.LO</category>
      <category>math.CT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Dagnino, Fabio Pasquali</dc:creator>
    </item>
    <item>
      <title>Solving Quantified Modal Logic Problems by Translation to Classical Logics</title>
      <link>https://arxiv.org/abs/2212.09570</link>
      <description>arXiv:2212.09570v3 Announce Type: replace 
Abstract: This article describes an evaluation of Automated Theorem Proving (ATP) systems on problems taken from the QMLTP library of first-order modal logic problems. Principally, the problems are translated to both typed first-order and higher-order logic in the TPTP language using an embedding approach, and solved using first-order resp. higher-order logic ATP systems and model finders. Additionally, the results from native modal logic ATP systems are considered, and compared with the results from the embedding approach. The findings are that the embedding process is reliable and successful when state-of-the-art ATP systems are used as backend reasoners, The first-order and higher-order embeddings perform similarly, native modal logic ATP systems have comparable performance to classical systems using the embedding for proving theorems, native modal logic ATP systems are outperformed by the embedding approach for disproving conjectures, and the embedding approach can cope with a wider range of modal logics than the native modal systems considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09570v3</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Steen, Geoff Sutcliffe, Christoph Benzm\"uller</dc:creator>
    </item>
    <item>
      <title>Robust Probabilistic Temporal Logics</title>
      <link>https://arxiv.org/abs/2306.05806</link>
      <description>arXiv:2306.05806v2 Announce Type: replace 
Abstract: We robustify PCTL and PCTL*, the most important specification languages for probabilistic systems, and show that robustness does not increase the complexity of the model-checking problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05806v2</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Martin Zimmermann</dc:creator>
    </item>
    <item>
      <title>The Complexity of Second-order HyperLTL</title>
      <link>https://arxiv.org/abs/2311.15675</link>
      <description>arXiv:2311.15675v2 Announce Type: replace 
Abstract: We determine the complexity of second-order HyperLTL satisfiability, finite-state satisfiability, and model-checking: All three are as hard as truth in third-order arithmetic.
  We also consider two fragments of second-order HyperLTL that have been introduced with the aim to facilitate effective model-checking by restricting the sets one can quantify over. The first one restricts second-order quantification to smallest/largest sets that satisfy a guard while the second one restricts second-order quantification further to least fixed points of (first-order) HyperLTL definable functions.
  The first fragment is still as hard as truth in third-order arithmetic while satisfiability for the second one is $\Sigma_1^1$-complete, i.e., only as hard as (first-order) HyperLTL and therefore much less complex. Finally, finite-state satisfiability and model-checking are in $\Sigma_2^2$ and $\Sigma_1^1$-hard, and thus also less complex than model-checking full second-order HyperLTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15675v2</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadar Frenkel, Martin Zimmermann</dc:creator>
    </item>
    <item>
      <title>Synthesizing Strongly Equivalent Logic Programs: Beth Definability for Answer Set Programs via Craig Interpolation in First-Order Logic</title>
      <link>https://arxiv.org/abs/2402.07696</link>
      <description>arXiv:2402.07696v3 Announce Type: replace 
Abstract: We show a projective Beth definability theorem for logic programs under the stable model semantics: For given programs $P$ and $Q$ and vocabulary $V$ (set of predicates) the existence of a program $R$ in $V$ such that $P \cup R$ and $P \cup Q$ are strongly equivalent can be expressed as a first-order entailment. Moreover, our result is effective: A program $R$ can be constructed from a Craig interpolant for this entailment, using a known first-order encoding for testing strong equivalence, which we apply in reverse to extract programs from formulas. As a further perspective, this allows transforming logic programs via transforming their first-order encodings. In a prototypical implementation, the Craig interpolation is performed by first-order provers based on clausal tableaux or resolution calculi. Our work shows how definability and interpolation, which underlie modern logic-based approaches to advanced tasks in knowledge representation, transfer to answer set programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07696v3</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Heuer, Christoph Wernhard</dc:creator>
    </item>
    <item>
      <title>Mechanised uniform interpolation for modal logics K, GL, and iSL</title>
      <link>https://arxiv.org/abs/2402.10494</link>
      <description>arXiv:2402.10494v2 Announce Type: replace 
Abstract: The uniform interpolation property in a given logic can be understood as the definability of propositional quantifiers. We mechanise the computation of these quantifiers and prove correctness in the Coq proof assistant for three modal logics, namely: (1) the modal logic K, for which a pen-and-paper proof exists; (2) G\"odel-L\"ob logic GL, for which our formalisation clarifies an important point in an existing, but incomplete, sequent-style proof; and (3) intuitionistic strong L\"ob logic iSL, for which this is the first proof-theoretic construction of uniform interpolants. Our work also yields verified programs that allow one to compute the propositional quantifiers on any formula in this logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10494v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hugo F\'er\'ee, Iris van der Giessen, Sam van Gool, Ian Shillito</dc:creator>
    </item>
    <item>
      <title>MCSat-based Finite Field Reasoning in the Yices2 SMT Solver</title>
      <link>https://arxiv.org/abs/2402.17927</link>
      <description>arXiv:2402.17927v2 Announce Type: replace 
Abstract: This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields. Our reasoning approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields. As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our reasoning approach as a new plugin for finite fields and extended Yices2's frontend to parse finite field problems, making our implementation the first MCSat-based reasoning engine for finite fields. We present its evaluation on finite field benchmarks, comparing it against cvc5. Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further reasoning techniques for this theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17927v2</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Hader, Daniela Kaufmann, Ahmed Irfan, St\'ephane Graham-Lengrand, Laura Kov\'acs</dc:creator>
    </item>
    <item>
      <title>One is all you need: Second-order Unification without First-order Variables</title>
      <link>https://arxiv.org/abs/2404.10616</link>
      <description>arXiv:2404.10616v3 Announce Type: replace 
Abstract: We consider the fragment of Second-Order unification, referred to as \emph{Second-Order Ground Unification (SOGU)}, with the following properties: (i) only one second-order variable allowed, (ii) first-order variables do not occur. We show that Hilbert's 10$^{th}$ problem is reducible to a \emph{necessary condition} for SOGU unifiability if the signature contains a binary function symbol and two constants, thus proving undecidability. This generalizes known undecidability results, as either first-order variable occurrences or multiple second-order variables were required for the reductions. Furthermore, we show that adding the following restriction:(i) the second-order variable has arity 1, (ii) the signature is finite, and (iii) the problem has \emph{bounded congruence}, results in a decidable fragment. The latter fragment is related to \emph{bounded second-order unification} in the sense that the number of bound variable occurrences is a function of the problem structure. We conclude with a discussion concerning the removal of the \emph{bounded congruence} restriction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10616v3</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David M. Cerna, Julian Parsert</dc:creator>
    </item>
    <item>
      <title>Generalized Optimization Modulo Theories</title>
      <link>https://arxiv.org/abs/2404.16122</link>
      <description>arXiv:2404.16122v2 Announce Type: replace 
Abstract: Optimization Modulo Theories (OMT) has emerged as an important extension of the highly successful Satisfiability Modulo Theories (SMT) paradigm. The OMT problem requires solving an SMT problem with the restriction that the solution must be optimal with respect to a given objective function. We introduce a generalization of the OMT problem where, in particular, objective functions can range over partially ordered sets. We provide a formalization of and an abstract calculus for the generalized OMT problem and prove their key correctness properties. Generalized OMT extends previous work on OMT in several ways. First, in contrast to many current OMT solvers, our calculus is theory-agnostic, enabling the optimization of queries over any theories or combinations thereof. Second, our formalization unifies both single- and multi-objective optimization problems, allowing us to study them both in a single framework and facilitating the use of objective functions that are not supported by existing OMT approaches. Finally, our calculus is sufficiently general to fully capture a wide variety of current OMT approaches (each of which can be realized as a specific strategy for rule application in the calculus) and to support the exploration of new search strategies. Much like the original abstract DPLL(T) calculus for SMT, our Generalized OMT calculus is designed to establish a theoretical foundation for understanding and research and to serve as a framework for studying variations of and extensions to existing OMT methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16122v2</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nestan Tsiskaridze, Clark Barrett, Cesare Tinelli</dc:creator>
    </item>
    <item>
      <title>Automata-Theoretic Characterisations of Branching-Time Temporal Logics</title>
      <link>https://arxiv.org/abs/2404.17421</link>
      <description>arXiv:2404.17421v2 Announce Type: replace 
Abstract: Characterisations theorems serve as important tools in model theory and can be used to assess and compare the expressive power of temporal languages used for the specification and verification of properties in formal methods. While complete connections have been established for the linear-time case between temporal logics, predicate logics, algebraic models, and automata, the situation in the branching-time case remains considerably more fragmented. In this work, we provide an automata-theoretic characterisation of some important branching-time temporal logics, namely CTL* and ECTL* interpreted on arbitrary-branching trees, by identifying two variants of Hesitant Tree Automata that are proved equivalent to those logics. The characterisations also apply to Monadic Path Logic and the bisimulation-invariant fragment of Monadic Chain Logic, again interpreted over trees. These results widen the characterisation landscape of the branching-time case and solve a forty-year-old open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17421v2</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Massimo Benerecetti, Laura Bozzelli, Fabio Mogavero, Adriano Peron</dc:creator>
    </item>
    <item>
      <title>The Big-O Problem for Max-Plus Automata is Decidable (PSPACE-Complete)</title>
      <link>https://arxiv.org/abs/2304.05229</link>
      <description>arXiv:2304.05229v2 Announce Type: replace-cross 
Abstract: We show that the big-O problem for max-plus automata is decidable and PSPACE-complete. The big-O (or affine domination) problem asks whether, given two max-plus automata computing functions f and g, there exists a constant c such that f &lt; cg+ c. This is a relaxation of the containment problem asking whether f &lt; g, which is undecidable. Our decidability result uses Simon's forest factorisation theorem, and relies on detecting specific elements, that we call witnesses, in a finite semigroup closed under two special operations: stabilisation and flattening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.05229v2</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laure Daviaud, David Purser, Marie Tcheng</dc:creator>
    </item>
    <item>
      <title>Lifted Inference beyond First-Order Logic</title>
      <link>https://arxiv.org/abs/2308.11738</link>
      <description>arXiv:2308.11738v2 Announce Type: replace-cross 
Abstract: Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results rely on a novel and general methodology of "counting by splitting". Besides their application to probabilistic inference, our results provide a general framework for counting combinatorial structures. We expand a vast array of previous results in discrete mathematics literature on directed acyclic graphs, phylogenetic networks, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11738v2</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sagar Malhotra, Davide Bizzaro, Luciano Serafini</dc:creator>
    </item>
    <item>
      <title>IntervalMDP.jl: Accelerated Value Iteration for Interval Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2401.04068</link>
      <description>arXiv:2401.04068v2 Announce Type: replace-cross 
Abstract: In this paper, we present IntervalMDP.jl, a Julia package for probabilistic analysis of interval Markov Decision Processes (IMDPs). IntervalMDP.jl facilitates the synthesis of optimal strategies and verification of IMDPs against reachability specifications and discounted reward properties. The library supports sparse matrices and is compatible with data formats from common tools for the analysis of probabilistic models, such as PRISM. A key feature of IntervalMDP.jl is that it presents both a multi-threaded CPU and a GPU-accelerated implementation of value iteration algorithms for IMDPs. In particular, IntervalMDP.jl takes advantage of the Julia type system and the inherently parallelizable nature of value iteration to improve the efficiency of performing analysis of IMDPs. On a set of examples, we show that IntervalMDP.jl substantially outperforms existing tools for verification and strategy synthesis for IMDPs in both computation time and memory consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04068v2</guid>
      <category>eess.SY</category>
      <category>cs.LO</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik Baymler Mathiesen, Morteza Lahijanian, Luca Laurenti</dc:creator>
    </item>
    <item>
      <title>An Incremental MaxSAT-based Model to Learn Interpretable and Balanced Classification Rules</title>
      <link>https://arxiv.org/abs/2403.16418</link>
      <description>arXiv:2403.16418v2 Announce Type: replace-cross 
Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset. Finally, IMLIB and IMLI are compared using diverse databases. IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16418v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-45368-7_15</arxiv:DOI>
      <arxiv:journal_reference>Intelligent Systems (2023), LNCS, vol 14195 (pp. 227-242), Springer Nature</arxiv:journal_reference>
      <dc:creator>Ant\^onio Carlos Souza Ferreira J\'unior, Thiago Alves Rocha</dc:creator>
    </item>
  </channel>
</rss>
