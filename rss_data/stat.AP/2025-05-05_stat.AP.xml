<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generalizable simulation framework for the request-to-order process in the procurement of onboard vessel requisitions</title>
      <link>https://arxiv.org/abs/2505.01806</link>
      <description>arXiv:2505.01806v1 Announce Type: new 
Abstract: Procurement in maritime logistics faces challenges due to uncertainties in demand and fluctuating market conditions. To address these complexities, we introduce a flexible discrete-event simulation framework that models the request-to-order process. This framework captures critical stages, including the generation of onboard vessel requisitions, requisition handling, and order allocation. Through numerical analysis, we compare two order allocation policies: a naive practice, which relies heavily on contracts, and a dynamic supplier selection approach that explores cost opportunities in the spot market. Our findings reveal trade-offs between cost efficiency and contract compliance, particularly in meeting volume commitments to contracted suppliers. Excessive reliance on spot market opportunities can yield significant savings but at the expense of contract compliance. Additionally, when spot rates are highly sensitive to order quantities, both policies tend to overutilize contracts, highlighting the need for larger volume commitments in such cases. These results offer actionable insights for improving procurement practices, while the framework's adaptability makes it a powerful decision-support tool across diverse procurement contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01806v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Vassos, Richard Lusby, Pierre Pinson</dc:creator>
    </item>
    <item>
      <title>Applications of higher order Markov models and Pressure Index to strategize controlled run chases in Twenty20 cricket</title>
      <link>https://arxiv.org/abs/2505.01849</link>
      <description>arXiv:2505.01849v1 Announce Type: new 
Abstract: In limited overs cricket, the team batting first posts a target score for the team batting second to achieve in order to win the match. The team batting second is constrained by decreasing resources in terms of number of balls left and number of wickets in hand in the process of reaching the target as the second innings progresses. The Pressure Index, a measure created by researchers in the past, serves as a tool for quantifying the level of pressure that a team batting second encounters in limited overs cricket. Through a ball-by-ball analysis of the second innings, it reveals how effectively the team batting second in a limited-over game proceeds towards their target. This research employs higher order Markov chains to examine the strategies employed by successful teams during run chases in Twenty20 matches. By studying the trends in successful run chases spanning over 16 years and utilizing a significant dataset of 3378 Twenty20 matches, specific strategies are identified. Consequently, an efficient approach to successful run chases in Twenty20 cricket is formulated, effectively limiting the Pressure Index to [0.5, 3.5] or even further down under 0.5 as early as possible. The innovative methodology adopted in this research offers valuable insights for cricket teams looking to enhance their performance in run chases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01849v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rhitankar Bandyopadhyay, Dibyojyoti Bhattacharjee</dc:creator>
    </item>
    <item>
      <title>What do we mean when we say we are clustering multimorbidity?</title>
      <link>https://arxiv.org/abs/2505.02111</link>
      <description>arXiv:2505.02111v1 Announce Type: new 
Abstract: Clustering multimorbidity has been a global research priority in recent years. Existing studies usually identify these clusters using one of several popular clustering methods and then explore various characteristics of these clusters, e.g., their genetic underpinning or their sociodemographic drivers, as downstream analysis. These studies make several choices during clustering that are often not explicitly acknowledged in the literature, e.g., whether they are clustering conditions or clustering individuals, and thus, they lead to different clustering solutions. We observe that, in general, clustering multimorbidity might mean different things in different studies, and argue that making these choices more explicit and, more importantly, letting the downstream analysis, or the purpose of identifying multimorbidity clusters, guide these choices, might lead to more transparent and operationalizable multimorbidity clusters. In this study, we discuss various purposes of identifying multimorbidity clusters and build a case for how different purposes can justify the different choices in data and methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02111v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sohan Seth, Nazir Lone, Niels Peek, Bruce Guthrie</dc:creator>
    </item>
    <item>
      <title>Deriving Duration Time from Occupancy Data -- A case study in the length of stay in Intensive Care Units for COVID-19 patients</title>
      <link>https://arxiv.org/abs/2505.02587</link>
      <description>arXiv:2505.02587v1 Announce Type: new 
Abstract: This paper focuses on drawing information on underlying processes, which are not directly observed in the data. In particular, we work with data in which only the total count of units in a system at a given time point is observed, but the underlying process of inflows, length of stay and outflows is not. The particular data example looked at in this paper is the occupancy of intensive care units (ICU) during the COVID-19 pandemic, where the aggregated numbers of occupied beds in ICUs on the district level (`Landkreis') are recorded, but not the number of incoming and outgoing patients. The Skellam distribution allows us to infer the number of incoming and outgoing patients from the occupancy in the ICUs. This paper goes a step beyond and approaches the question of whether we can also estimate the average length of stay of ICU patients. Hence, the task is to derive not only the number of incoming and outgoing units from a total net count but also to gain information on the duration time of patients on ICUs. We make use of a stochastic Expectation-Maximisation algorithm and additionally include exogenous information which are assumed to explain the intensity of inflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02587v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martje Rave, G\"oran Kauermann</dc:creator>
    </item>
    <item>
      <title>Expectiles as basis risk-optimal payment schemes in parametric insurance</title>
      <link>https://arxiv.org/abs/2505.02607</link>
      <description>arXiv:2505.02607v1 Announce Type: new 
Abstract: Payments in parametric insurance solutions are linked to an index and thus decoupled from policyholders' true losses. While this principle has appealing operational benefits compared to traditional indemnity coverage, i.e. is very efficient and cost effective, a downside is the discrepancy between payouts and actual damage, called basis risk. We show that in an asymmetrically weighted mean square error framework, the basis risk-minimizing payment schemes for pure parametric and parametric index insurance contracts can be expressed as conditional expectiles of policyholders' true loss given a compensation-triggering incident. We provide connections to stochastic orderings and demonstrate that regression approaches allow easy implementation in practice. Our results are visualized in parametric coverage for cyber risks and agricultural insurance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02607v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Johannes Maier, Matthias Scherer</dc:creator>
    </item>
    <item>
      <title>Optimising Kernel-based Multivariate Statistical Process Control</title>
      <link>https://arxiv.org/abs/2505.01556</link>
      <description>arXiv:2505.01556v1 Announce Type: cross 
Abstract: Multivariate Statistical Process Control (MSPC) is a framework for monitoring and diagnosing complex processes by analysing the relationships between multiple process variables simultaneously. Kernel MSPC extends the methodology by leveraging kernel functions to capture non-linear relationships between the data, enhancing the process monitoring capabilities. However, optimising the kernel MSPC parameters, such as the kernel type and kernel parameters, is often done in literature in time-consuming and non-procedural manners such as cross-validation or grid search. In the present paper, we propose optimising the kernel MSPC parameters with Kernel Flows (KF), a recent kernel learning methodology introduced for Gaussian Process Regression (GPR). Apart from the optimisation technique, the novelty of the study resides also in the utilisation of kernel combinations for learning the optimal kernel type, and introduces individual kernel parameters for each variable. The proposed methodology is evaluated with multiple cases from the benchmark Tennessee Eastman Process. The faults are detected for all evaluated cases, including the ones not detected in the original study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01556v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zina-Sabrina Duma, Victoria Jorry, Tuomas Sihvonen, Satu-Pia Reinikainen, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Fast Likelihood-Free Parameter Estimation for L\'evy Processes</title>
      <link>https://arxiv.org/abs/2505.01639</link>
      <description>arXiv:2505.01639v1 Announce Type: cross 
Abstract: L\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. Through extensive simulations across several L\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling rapid bootstrap-based uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01639v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Coloma, William Kleiber</dc:creator>
    </item>
    <item>
      <title>Unified and Simple Sample Size Calculations for Individual or Cluster Randomized Trials with Skewed or Ordinal Outcomes</title>
      <link>https://arxiv.org/abs/2505.01640</link>
      <description>arXiv:2505.01640v1 Announce Type: cross 
Abstract: Sample size calculations can be challenging with skewed continuous outcomes in randomized controlled trials (RCTs). Standard t-test-based calculations may require data transformation, which may be difficult before data collection. Calculations based on individual and clustered Wilcoxon rank-sum tests have been proposed as alternatives, but these calculations assume no ties in continuous outcomes, and clustered Wilcoxon rank-sum tests perform poorly with heterogeneous cluster sizes. Recent work has shown that continuous outcomes can be analyzed in a robust manner using ordinal cumulative probability models. Analogously, sample size calculations for ordinal outcomes can be applied as a robust design strategy for continuous outcomes. We show that Whitehead's sample size calculations for independent ordinal outcomes can be easily extended to continuous outcomes. We extend these calculations to cluster RCTs using a design effect incorporating the rank intraclass correlation coefficient. Therefore, we provide a unifying and simple approach for designing individual and cluster RCTs that makes minimal assumptions on the distribution of the still-to-be-collected outcome. We conduct simulations to evaluate our approach's performance and illustrate its application in multiple RCTs: an individual RCT with skewed continuous outcomes, a cluster RCT with skewed continuous outcomes, and a non-inferiority cluster RCT with an irregularly distributed count outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01640v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengxin Tu, Chun Li, Caroline De Schacht, Carolyn M. Audet, Aminu Taura Abdullahi, Edwin Trevathan, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>Integrated optimization of operations and capacity planning under uncertainty for drayage procurement in container logistics</title>
      <link>https://arxiv.org/abs/2505.01808</link>
      <description>arXiv:2505.01808v1 Announce Type: cross 
Abstract: We present an integrated framework for truckload procurement in container logistics, bridging strategic and operational aspects that are often treated independently in existing research. Drayage, the short-haul trucking of containers, plays a critical role in intermodal container logistics. Using dynamic programming, we identify optimal operational policies for allocating drayage volumes among capacitated carriers under uncertain container flows and spot rates. The computational complexity of optimization under uncertainty is mitigated through sample average approximation. These optimal policies serve as the basis for evaluating specific capacity arrangements. To optimize capacity reservations with strategic and spot carriers, we employ an efficient quasi-Newton method. Numerical experiments demonstrate significant cost-efficiency improvements, including a 21.2% cost reduction in a four-period scenario. Monte Carlo simulations further highlight the strong generalization capabilities of the proposed joint optimization method across out-of-sample scenarios. These findings underscore the importance of integrating strategic and operational decisions to enhance cost efficiency in truckload procurement under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01808v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Vassos, Richard Lusby, Pierre Pinson</dc:creator>
    </item>
    <item>
      <title>General Form Moment-based Estimator of Weibull, Gamma, and Log-normal Distributions</title>
      <link>https://arxiv.org/abs/2505.01911</link>
      <description>arXiv:2505.01911v1 Announce Type: cross 
Abstract: This paper presents a unified and novel estimation framework for the Weibull, Gamma, and Log-normal distributions based on arbitrary-order moment pairs. Traditional estimation techniques, such as Maximum Likelihood Estimation (MLE) and the classical Method of Moments (MoM), are often restricted to fixed-order moment inputs and may require specific distributional assumptions or optimization procedures. In contrast, our general-form moment-based estimator allows the use of any two empirical moments, such as mean and variance, or higher-order combinations, to compute the underlying distribution parameters. For each distribution, we develop provably convergent numerical algorithms that guarantee unique solutions within a bounded parameter space and provide estimates within a user-defined error tolerance. The proposed framework generalizes existing estimation methods and offers greater flexibility and robustness for statistical modeling in diverse application domains. This is, to our knowledge, the first work that formalizes such a general estimation structure and provides theoretical guarantees across these three foundational distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01911v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kang Liu</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2505.02257</link>
      <description>arXiv:2505.02257v1 Announce Type: cross 
Abstract: In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02257v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Revolutions as Structural Breaks: The Long-Term Economic and Institutional Consequences of the 1979 Iranian Revolution</title>
      <link>https://arxiv.org/abs/2505.02425</link>
      <description>arXiv:2505.02425v1 Announce Type: cross 
Abstract: This paper examines whether major political institutional disruptions produce temporary shocks or structural breaks in long-term development. Using the 1979 Iranian Revolution as a natural experiment, we apply the synthetic control method to estimate its causal effect on economic growth and institutional quality. Drawing on a panel of 66 countries from 1950 to 2015, we construct counterfactual trajectories for Iran in the absence of revolutionary change. Our results show a persistent and statistically significant divergence in per capita GDP, institutional quality, and legal constraints on executive power. We perform in-space and in-time placebo tests to rule out confounding events, such as the Iran-Iraq War and international sanctions, and propose confidence interval estimation to address uncertainty in treatment effects. The findings identify the Iranian Revolution as a structural institutional rupture, with implications for the classification of institutional change more broadly. We contribute a generalizable empirical framework for distinguishing between temporary and structural institutional shocks in long-run development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02425v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuno Garoupa, Rok Spruk</dc:creator>
    </item>
    <item>
      <title>Ethnic Conflicts, Civil War and Economic Growth: Region-Level Evidence from former Yugoslavia</title>
      <link>https://arxiv.org/abs/2505.02431</link>
      <description>arXiv:2505.02431v1 Announce Type: cross 
Abstract: We investigate the long-term impact of civil war on subnational economic growth across 78 regions in five former Yugoslav republics from 1950 to 2015. Leveraging the outbreak of ethnic tensions and the onset of conflict, we construct counterfactual growth trajectories using a robust region-level donor pool from 28 conflict-free countries. Applying a hybrid synthetic control and difference-in-differences approach, we find that the war in former Yugoslavia inflicted unprecedented regional per capita GDP losses estimated at 38 percent, with substantial regional heterogeneity. The most war-affected regions suffered prolonged and permanent economic declines, while capital cities experienced more transitory effects. Our results are robust to extensive variety of specification tests, placebo analyses, and falsification exercises. Notably, ethnic tensions between Serbs and Croats explain up to 40 percent of the observed variation in economic losses, underscoring the deep and lasting influence of ethnic divisions on economic impacts of the armed conflicts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02431v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandar Keseljevic, Stefan Nikolic, Rok Spruk</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman filter for uncertainty in human language comprehension</title>
      <link>https://arxiv.org/abs/2505.02590</link>
      <description>arXiv:2505.02590v1 Announce Type: cross 
Abstract: Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02590v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diksha Bhandari, Alessandro Lopopolo, Milena Rabovsky, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview</title>
      <link>https://arxiv.org/abs/2505.02609</link>
      <description>arXiv:2505.02609v1 Announce Type: cross 
Abstract: Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02609v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyu Wang, Ang\'elique Saillet, Philom\`ene Le Gall, Alain Lacroux, Christelle Martin-Lacroux, Vincent Brault</dc:creator>
    </item>
    <item>
      <title>Attractor-Based Coevolving Dot Product Random Graph Model</title>
      <link>https://arxiv.org/abs/2505.02675</link>
      <description>arXiv:2505.02675v1 Announce Type: cross 
Abstract: We introduce the attractor-based coevolving dot product random graph model (ABCDPRGM) to analyze time-series network data manifesting polarizing or flocking behavior. Graphs are generated based on latent positions under the random dot product graph regime. We assign group membership to each node. When evolving through time, the latent position of each node will change based on its current position and two attractors, which are defined to be the centers of the latent positions of all of its neighbors who share its group membership or who have different group membership than it. Parameters are assigned to the attractors to quantify the amount of influence that the attractors have on the trajectory of the latent position of each node. We developed estimators for the parameters, demonstrated their consistency, and established convergence rates under specific assumptions. Through the ABCDPRGM, we provided a novel framework for quantifying and understanding the underlying forces influencing the polarizing or flocking behaviors in dynamic network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02675v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwen Yang, Daniel L. Sussman</dc:creator>
    </item>
    <item>
      <title>A Sloppy approach to QSP: XAI enabling fit-for-purpose models</title>
      <link>https://arxiv.org/abs/2505.02750</link>
      <description>arXiv:2505.02750v1 Announce Type: cross 
Abstract: Quantitative Systems Pharmacology (QSP) promises to accelerate drug development, enable personalized medicine, and improve the predictability of clinical outcomes. Realizing its full potential depends on effectively managing the complexity of the underlying mathematical models and biological systems. Here, we present and validate a novel QSP workflow grounded in the principles of sloppy modeling, offering a practical and principled strategy for building and deploying models in a QSP pipeline. Our approach begins with a literature-derived model, constructed to be as comprehensive and unbiased as possible by drawing from the collective knowledge of prior research. At the core of the workflow is the Manifold Boundary Approximation Method (MBAM), which simplifies models while preserving their predictive capacity and mechanistic interpretability. Applying MBAM as a context-specific model reduction strategy, we link the simplified representation directly to the downstream predictions of interest. The resulting reduced models are computationally efficient and well-suited to key QSP tasks, including virtual population generation, experimental design, and target discovery. We demonstrate the utility of this workflow through case studies involving the coagulation cascade and SHIV infection. Our analysis suggests several promising next steps for improving the efficacy of bNAb therapies in HIV infected patients within the context of a general-purpose QSP modeling workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02750v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.MN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah DeTal, Christian N. K. Anderson, Mark K. Transtrum</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v4 Announce Type: replace 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Low-Rank Expectile Representations of a Data Matrix, with Application to Diurnal Heart Rates</title>
      <link>https://arxiv.org/abs/2412.04765</link>
      <description>arXiv:2412.04765v4 Announce Type: replace 
Abstract: Low-rank matrix factorization is a powerful tool for understanding the structure of 2-way data, and is usually accomplished by minimizing a sum of squares criterion. Expectile analysis generalizes squared-error loss by introducing asymmetry, allowing tail behavior to be elicited. Here we present a framework for low-rank expectile analysis of a data matrix that incorporates both additive and multiplicative effects, utilizing expectile loss, and accommodating arbitrary patterns of missing data. The representation can be fit with gradient-descent. Simulation studies demonstrate the accuracy of the structure recovery. Using diurnal heart rate data indexed by person-days versus minutes within a day, we find divergent behavior for lower versus upper expectiles, with the lower expectiles being much more stable within subjects across days, while the upper expectiles are much more variable, even within subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04765v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuge Ouyang, Yunxuan Tang, Benjamin Osafo Agyare</dc:creator>
    </item>
    <item>
      <title>Assessing the Role of Volumetric Brain Information in Multiple Sclerosis Progression</title>
      <link>https://arxiv.org/abs/2412.09497</link>
      <description>arXiv:2412.09497v2 Announce Type: replace 
Abstract: Multiple sclerosis is a chronic autoimmune disease that affects the central nervous system. Understanding multiple sclerosis progression and identifying the implicated brain structures is crucial for personalized treatment decisions. Deformation-based morphometry utilizes anatomical magnetic resonance imaging to quantitatively assess volumetric brain changes at the voxel level, providing insight into how each brain region contributes to clinical progression with regards to neurodegeneration. Utilizing such voxel-level data from a relapsing multiple sclerosis clinical trial, we extend a model-agnostic feature importance metric to identify a robust and predictive feature set that corresponds to clinical progression. These features correspond to brain regions that are clinically meaningful in MS disease research, demonstrating their scientific relevance. When used to predict progression using classical survival models and 3D convolutional neural networks, the identified regions led to the best-performing models, demonstrating their prognostic strength. We also find that these features generalize well to other definitions of clinical progression and can compensate for the omission of highly prognostic clinical features, underscoring the predictive power and clinical relevance of deformation-based morphometry as a regional identification tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09497v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy A. Shen, Aidan McLoughlin, Zoe Vernon, Jonathan Lin, Richard A. D. Carano, Peter J. Bickel, Zhuang Song, Haiyan Huang</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v3 Announce Type: replace-cross 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay, Kalyan Das, Sumitra Purkayastha</dc:creator>
    </item>
    <item>
      <title>A Bayesian workflow for securitizing casualty insurance risk</title>
      <link>https://arxiv.org/abs/2407.14666</link>
      <description>arXiv:2407.14666v2 Announce Type: replace-cross 
Abstract: Casualty insurance-linked securities (ILS) are appealing to investors because the underlying insurance claims, which are directly related to resulting security performance, are uncorrelated with most other asset classes. Conversely, casualty ILS are appealing to insurers as an efficient capital management tool. However, securitizing casualty insurance risk is non-trivial, as it requires forecasting loss ratios for pools of insurance policies that have not yet been written, in addition to estimating how the underlying losses will develop over time within future accident years. In this paper, we lay out a Bayesian workflow that tackles these complexities by using: (1) theoretically informed time-series and state-space models to capture how loss ratios develop and change over time; (2) historic industry data to inform prior distributions of models fit to individual programs; (3) stacking to combine loss ratio predictions from candidate models, and (4) both prior predictive simulations and simulation-based calibration to aid model specification. Using historic Schedule P filings, we then show how our proposed Bayesian workflow can be used to assess and compare models across a variety of key model performance metrics evaluated on future accident year losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14666v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Haines, Conor Goold, J. Mark Shoun</dc:creator>
    </item>
    <item>
      <title>Nonparametric Monitoring of Spatial Dependence</title>
      <link>https://arxiv.org/abs/2408.17022</link>
      <description>arXiv:2408.17022v2 Announce Type: replace-cross 
Abstract: In process monitoring, it is common for measurements to be taken regularly or randomly from different spatial locations in two or three dimensions. While there are nonparametric methods for process monitoring with such spatial data to detect changes in the mean, there is a gap in the literature for nonparametric control charting methods developed to monitor spatial dependence. This study considers streams of regular, rectangular data sets using spatial ordinal patterns (SOPs) as a nonparametric method to detect spatial dependencies. We propose novel SOP control charts, which are distribution-free and do not require prior Phase-I analysis. To uncover higher-order dependencies, we develop a new class of statistics that combines SOPs with the Box-Pierce approach. An extensive simulation study demonstrates the superiority and effectiveness of our proposed charts over traditional parametric approaches, particularly when the spatial dependence is nonlinear or bilateral or when the spatial data contains outliers. The proposed SOP control charts are illustrated using real-world datasets to detect (i) heavy rainfall in Germany, (ii) war-related fires in (eastern) Ukraine, and (iii) manufacturing defects in textile production. This wide range of applications and findings demonstrates the broad utility of the proposed nonparametric control charts. In addition, all methods in this study are provided as a publicly available \texttt{Julia} package on \href{https://github.com/AdaemmerP/OrdinalPatterns.jl}{GitHub} for further implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17022v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Ad\"ammer, Philipp Wittenberg, Christian H. Wei{\ss}, Murat Caner Testik</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance (BTBA): Examining Its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v3 Announce Type: replace-cross 
Abstract: We introduce Bhirkuti's Test of Bias Acceptance (BTBA), a standardized framework for evaluating estimator bias in Monte Carlo simulation studies. BTBA uses a simulation-specific standardized score (Z*) and a decision matrix to assess bias acceptability based on the mean and variance of Z* distributions. Under ideal conditions, Z* values should approximate a standard normal distribution (Z-distribution) with a mean near zero and variance near one in the context of simulation research. Systematic deviations from these patterns such as shifted means or inflated variances indicate bias or estimator instability in simulation-based research. BTBA visualizes these patterns using ridgeline density plots, which reveal distributional features such as central tendency, spread, skewness, and outliers. Demonstrated in a latent growth modeling context, BTBA offers a reproducible and interpretable method for diagnosing bias across varying simulation conditions. By addressing key limitations of traditional relative bias (RB) metrics, BTBA provides a theoretically grounded, distribution-aware, transparent, and replicable alternative for evaluating estimator quality, particularly in psychometric modeling, structural equation modeling, and missing data research. Through this framework, we aim to enhance methodological decision-making by integrating statistical reasoning with comprehensive visualization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Improving exponential-family random graph models for bipartite networks</title>
      <link>https://arxiv.org/abs/2502.01892</link>
      <description>arXiv:2502.01892v2 Announce Type: replace-cross 
Abstract: Bipartite graphs, representing two-mode networks, arise in many research fields. These networks have two disjoint node sets representing distinct entity types, for example persons and groups, with edges representing associations between the two entity types. In bipartite graphs, the smallest possible cycle is a cycle of length four, and hence four-cycles are the smallest structure to model closure in such networks. Exponential-family random graph models (ERGMs) are a widely used model for social, and other, networks, including specifically bipartite networks. Existing ERGM terms to model four-cycles in bipartite networks, however, are relatively rarely used. In this work we demonstrate some problems with these existing terms to model four-cycles, and define new ERGM terms to help overcome these problems. The position of the new terms in the ERGM dependence hierarchy, and their interpretation, is discussed. The new terms are demonstrated in simulation experiments, and their application illustrated on a canonical example of an empirical two-mode network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stivala, Peng Wang, Alessandro Lomi</dc:creator>
    </item>
    <item>
      <title>A Theoretical Model for Grit in Pursuing Ambitious Ends</title>
      <link>https://arxiv.org/abs/2503.02952</link>
      <description>arXiv:2503.02952v2 Announce Type: replace-cross 
Abstract: Ambition and risk-taking have been heralded as important ways for marginalized communities to get out of cycles of poverty. As a result, educational messaging often encourages individuals to strengthen their personal resolve and develop characteristics such as discipline and grit to succeed in ambitious ends. However, recent work in philosophy and sociology highlights that this messaging often does more harm than good for students in these situations. We study similar questions using a different epistemic approach and in simple theoretical models -- we provide a quantitative model of decision-making between stable and risky choices in the improving multi-armed bandits framework. We use this model to first study how individuals' "strategies" are affected by their level of grittiness and how this affects their accrued rewards. Then, we study the impact of various interventions, such as increasing grit or providing a financial safety net. Our investigation of rational decision making involves two different formal models of rationality, the competitive ratio between the accrued reward and the optimal reward and Bayesian quantification of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02952v2</guid>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avrim Blum, Emily Diana, Kavya Ravichandran, Alexander Williams Tolbert</dc:creator>
    </item>
    <item>
      <title>Optimal Change Point Detection and Inference in the Spectral Density of General Time Series Models</title>
      <link>https://arxiv.org/abs/2503.23211</link>
      <description>arXiv:2503.23211v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23211v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai</dc:creator>
    </item>
    <item>
      <title>Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios</title>
      <link>https://arxiv.org/abs/2504.15268</link>
      <description>arXiv:2504.15268v2 Announce Type: replace-cross 
Abstract: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, thus enabling flexible, granular, and realistic scenarios. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15268v2</guid>
      <category>q-fin.RM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JD Opdyke</dc:creator>
    </item>
    <item>
      <title>Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model</title>
      <link>https://arxiv.org/abs/2504.21795</link>
      <description>arXiv:2504.21795v2 Announce Type: replace-cross 
Abstract: The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21795v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuankang Zhao, Matthew Engelhard</dc:creator>
    </item>
  </channel>
</rss>
