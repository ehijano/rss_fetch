<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neural empirical interpolation method for nonlinear model reduction</title>
      <link>https://arxiv.org/abs/2406.03562</link>
      <description>arXiv:2406.03562v1 Announce Type: new 
Abstract: In this paper, we introduce the neural empirical interpolation method (NEIM), a neural network-based alternative to the discrete empirical interpolation method for reducing the time complexity of computing the nonlinear term in a reduced order model (ROM) for a parameterized nonlinear partial differential equation. NEIM is a greedy algorithm which accomplishes this reduction by approximating an affine decomposition of the nonlinear term of the ROM, where the vector terms of the expansion are given by neural networks depending on the ROM solution, and the coefficients are given by an interpolation of some "optimal" coefficients. Because NEIM is based on a greedy strategy, we are able to provide a basic error analysis to investigate its performance. NEIM has the advantages of being easy to implement in models with automatic differentiation, of being a nonlinear projection of the ROM nonlinearity, of being efficient for both nonlocal and local nonlinearities, and of relying solely on data and not the explicit form of the ROM nonlinearity. We demonstrate the effectiveness of the methodology on solution-dependent and solution-independent nonlinearities, a nonlinear elliptic problem, and a nonlinear parabolic model of liquid crystals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03562v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Hirsch, Federico Pichi, Jan S. Hesthaven</dc:creator>
    </item>
    <item>
      <title>GFN: A graph feedforward network for resolution-invariant reduced operator learning in multifidelity applications</title>
      <link>https://arxiv.org/abs/2406.03569</link>
      <description>arXiv:2406.03569v1 Announce Type: new 
Abstract: This work presents a novel resolution-invariant model order reduction strategy for multifidelity applications. We base our architecture on a novel neural network layer developed in this work, the graph feedforward network, which extends the concept of feedforward networks to graph-structured data by creating a direct link between the weights of a neural network and the nodes of a mesh, enhancing the interpretability of the network. We exploit the method's capability of training and testing on different mesh sizes in an autoencoder-based reduction strategy for parametrised partial differential equations. We show that this extension comes with provable guarantees on the performance via error bounds. The capabilities of the proposed methodology are tested on three challenging benchmarks, including advection-dominated phenomena and problems with a high-dimensional parameter space. The method results in a more lightweight and highly flexible strategy when compared to state-of-the-art models, while showing excellent generalisation performance in both single fidelity and multifidelity scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03569v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ois\'in M. Morrison, Federico Pichi, Jan S. Hesthaven</dc:creator>
    </item>
    <item>
      <title>Approximating partial differential equations without boundary conditions</title>
      <link>https://arxiv.org/abs/2406.03634</link>
      <description>arXiv:2406.03634v1 Announce Type: new 
Abstract: We consider the problem of numerically approximating the solutions to an elliptic partial differential equation (PDE) for which the boundary conditions are lacking. To alleviate this missing information, we assume to be given measurement functionals of the solution. In this context, a near optimal recovery algorithm based on the approximation of the Riesz representers of these functionals in some intermediate Hilbert spaces is proposed and analyzed in [Binev et al. 2024]. Inherent to this algorithm is the computation of $H^s$, $s&gt;1/2$, inner products on the boundary of the computational domain. We take advantage of techniques borrowed from the analysis of fractional diffusion problems to design and analyze a fully practical near optimal algorithm not relying on the challenging computation of $H^s$ inner products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03634v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Bonito, Diane Guignard</dc:creator>
    </item>
    <item>
      <title>A second-order accurate, original energy dissipative numerical scheme for chemotaxis and its convergence analysis</title>
      <link>https://arxiv.org/abs/2406.03761</link>
      <description>arXiv:2406.03761v1 Announce Type: new 
Abstract: This paper proposes a second-order accurate numerical scheme for the Patlak-Keller-Segel system with various mobilities for the description of chemotaxis. Formulated in a variational structure, the entropy part is novelly discretized by a modified Crank-Nicolson approach so that the solution to the proposed nonlinear scheme corresponds to a minimizer of a convex functional. A careful theoretical analysis reveals that the unique solvability and positivity-preserving property could be theoretically justified. More importantly, such a second order numerical scheme is able to preserve the dissipative property of the original energy functional, instead of a modified one. To the best of our knowledge, the proposed scheme is the first second-order accurate one in literature that could achieve both the numerical positivity and original energy dissipation. In addition, an optimal rate convergence estimate is provided for the proposed scheme, in which rough and refined error estimate techniques have to be included to accomplish such an analysis. Ample numerical results are presented to demonstrate robust performance of the proposed scheme in preserving positivity and original energy dissipation in blowup simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03761v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Ding, Cheng Wang, Shenggao Zhou</dc:creator>
    </item>
    <item>
      <title>Convergence of a Riemannian gradient method for the Gross-Pitaevskii energy functional in a rotating frame</title>
      <link>https://arxiv.org/abs/2406.03885</link>
      <description>arXiv:2406.03885v1 Announce Type: new 
Abstract: This paper investigates the numerical approximation of ground states of rotating Bose-Einstein condensates. This problem requires the minimization of the Gross-Pitaevskii energy $E$ on a Riemannian manifold $\mathbb{S}$. To find a corresponding minimizer $u$, we use a generalized Riemannian gradient method that is based on the concept of Sobolev gradients in combination with an adaptively changing metric on the manifold. By a suitable choice of the metric, global energy dissipation for the arising gradient method can be proved. The energy dissipation property in turn implies global convergence to the density $|u|^2$ of a critical point $u$ of $E$ on $\mathbb{S}$. Furthermore, we present a precise characterization of the local convergence rates in a neighborhood of each ground state $u$ and how these rates depend on the first spectral gap of $E^{\prime\prime}(u)$ restricted to the $L^2$-orthogonal complement of $u$. With this we establish the first convergence results for a Riemannian gradient method to minimize the Gross-Pitaevskii energy functional in a rotating frame. At the same, we refine previous results obtained in the case without rotation. The major complication in our new analysis is the missing isolation of minimizers, which are at most unique up to complex phase shifts. For that, we introduce an auxiliary iteration in the tangent space $T_{\mathrm{i} u} \mathbb{S}$ and apply the Ostrowski theorem to characterize the asymptotic convergence rates through a weighted eigenvalue problem. Afterwards, we link the auxiliary iteration to the original Riemannian gradient method and bound the spectrum of the weighted eigenvalue problem to obtain quantitative convergence rates. Our findings are validated in numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03885v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Henning, Mahima Yadav</dc:creator>
    </item>
    <item>
      <title>Operator learning based on sparse high-dimensional approximation</title>
      <link>https://arxiv.org/abs/2406.03973</link>
      <description>arXiv:2406.03973v1 Announce Type: new 
Abstract: We present a dimension-incremental method for function approximation in bounded orthonormal product bases to learn the solutions of various differential equations. Therefore, we deconstruct the source function of the differential equation into parameters like Fourier or Spline coefficients and treat the solution of the differential equation as a high-dimensional function w.r.t. the spatial variables, these parameters and also further possible parameters from the differential equation itself. Finally, we learn this function in the sense of sparse approximation in a suitable function space by detecting coefficients of the basis expansion with largest absolute value. Investigating the corresponding indices of the basis coefficients yields further insights on the structure of the solution as well as its dependency on the parameters and their interactions and allows for a reasonable generalization to even higher dimensions and therefore better resolutions of the deconstructed source function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03973v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Potts, Fabian Taubert</dc:creator>
    </item>
    <item>
      <title>Quadrature error estimates on non-matching grids in a fictitious domain framework for fluid-structure interaction problems</title>
      <link>https://arxiv.org/abs/2406.03981</link>
      <description>arXiv:2406.03981v1 Announce Type: new 
Abstract: We consider a fictitious domain formulation for fluid-structure interaction problems based on a distributed Lagrange multiplier to couple the fluid and solid behaviors. How to deal with the coupling term is crucial since the construction of the associated finite element matrix requires the integration of functions defined over non-matching grids: the exact computation can be performed by intersecting the involved meshes, whereas an approximate coupling matrix can be evaluated on the original meshes by introducing a quadrature error. The purpose of this paper is twofold: we prove that the discrete problem is well-posed also when the coupling term is constructed in approximate way and we discuss quadrature error estimates over non-matching grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03981v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Boffi, Fabio Credali, Lucia Gastaldi</dc:creator>
    </item>
    <item>
      <title>Generalized Wedderburn Rank Reduction</title>
      <link>https://arxiv.org/abs/2406.03992</link>
      <description>arXiv:2406.03992v1 Announce Type: new 
Abstract: We generalize the Wedderburn rank reduction formula by replacing the inverse with the Moore--Penrose pseudoinverse. In particular, this allows one to remove the non--singularity of a certain matrix from assumptions. The results implies in a straightforward way Nystroem, CUR decompositions, meta-factorization, and a result of Ameli, Shadden. We investigate which properties of the matrix are inherited by the generalized Wedderburn reduction. Reductions leading to the best low-rank approximation are explicitly described in terms of singular vectors. We give a self--contained calculation of the range and the nullspace of the projection $A(BA)^+B$ and prove that any projection can be expressed in this way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03992v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar K\k{e}dzierski</dc:creator>
    </item>
    <item>
      <title>Symplectic Methods in Deep Learning</title>
      <link>https://arxiv.org/abs/2406.04104</link>
      <description>arXiv:2406.04104v1 Announce Type: new 
Abstract: Deep learning is widely used in tasks including image recognition and generation, in learning dynamical systems from data and many more. It is important to construct learning architectures with theoretical guarantees to permit safety in the applications. There has been considerable progress in this direction lately. In particular, symplectic networks were shown to have the non vanishing gradient property, essential for numerical stability. On the other hand, architectures based on higher order numerical methods were shown to be efficient in many tasks where the learned function has an underlying dynamical structure. In this work we construct symplectic networks based on higher order explicit methods with non vanishing gradient property and test their efficiency on various examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04104v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sofya Maslovskaya, Sina Ober-Bl\"obaum</dc:creator>
    </item>
    <item>
      <title>Parametric Intrusive Reduced Order Models enhanced with Machine Learning Correction Terms</title>
      <link>https://arxiv.org/abs/2406.04169</link>
      <description>arXiv:2406.04169v1 Announce Type: new 
Abstract: In this paper, we propose an equation-based parametric Reduced Order Model (ROM), whose accuracy is improved with data-driven terms added into the reduced equations. These additions have the aim of reintroducing contributions that in standard ROMs are not taken into account. In particular, in this work we consider two types of contributions: the turbulence modeling, added through a reduced-order approximation of the eddy viscosity field, and the correction model, aimed to re-introduce the contribution of the discarded modes. Both approaches have been investigated in previous works and the goal of this paper is to extend the model to a parametric setting making use of ad-hoc machine learning procedures. More in detail, we investigate different neural networks' architectures, from simple dense feed-forward to Long-Short Term Memory neural networks, in order to find the most suitable model for the re-introduced contributions. We tested the methods on two test cases with different behaviors: the periodic turbulent flow past a circular cylinder and the unsteady turbulent flow in a channel-driven cavity. In both cases, the parameter considered is the Reynolds number and the machine learning-enhanced ROM considerably improved the pressure and velocity accuracy with respect to the standard ROM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04169v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Ivagnes, Giovanni Stabile, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision</title>
      <link>https://arxiv.org/abs/2404.10771</link>
      <description>arXiv:2404.10771v2 Announce Type: cross 
Abstract: Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the $\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving $\textit{machine precision}$ in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10771v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuo Chen, Jacob McCarran, Esteban Vizcaino, Marin Solja\v{c}i\'c, Di Luo</dc:creator>
    </item>
    <item>
      <title>Estimated electric conductivities of thermal plasma for air-fuel combustion and oxy-fuel combustion with potassium or cesium seeding</title>
      <link>https://arxiv.org/abs/2406.03499</link>
      <description>arXiv:2406.03499v1 Announce Type: cross 
Abstract: A complete model for estimating the electric conductivity of combustion product gases, with added cesium (Cs) or potassium (K) vapor for ionization, is presented. Neutral carrier gases serve as the bulk fluid that carries the seed material, as well as the electrons generated by the partial thermal (equilibrium) ionization of the seed alkali metal. The model accounts for electron-neutral scattering, as well as electron-ion and electron-electron scattering. The model is tested through comparison with published data. The model is aimed at being utilized for the plasma within magnetohydrodynamic (MHD) channels, where direct power extraction from passing electrically conducting plasma gas enables electric power generation. The thermal ionization model is then used to estimate the electric conductivity of seeded combustion gases under complete combustion of three selected fuels, namely: hydrogen (H2), methane (CH4), and carbon (C). For each of these three fuels, two options for the oxidizer were applied, namely: air (21 % molecular oxygen, 79 % molecular nitrogen by mole), and pure oxygen (oxy-combustion). Two types of seeds (with 1 % mole fraction, based on the composition before ionization) were also applied for each of the six combinations of (fuel-oxidizer), leading to a total of 12 different MHD plasma cases. For each of these cases, the electric conductivity was computed for a range of temperatures from 2000 K to 3000 K. The smallest estimated electric conductivity was 0.35 S/m for oxy-hydrogen combustion at 2000 K, with potassium seeding. The largest estimated electric conductivity was 180.30 S/m for oxy-carbon combustion at 3000 K, with cesium seeding. At 2000 K, replacing potassium with cesium causes a gain in the electric conductivity by a multiplicative gain factor of about 3.6 regardless of the fuel and oxidizer. This gain factor declines to between 1.77 and 2.07 at 3000 K.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03499v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.heliyon.2024.e31697</arxiv:DOI>
      <arxiv:journal_reference>Heliyon, volume 10, issues 11, article number e31697, 2024</arxiv:journal_reference>
      <dc:creator>Osama A. Marzouk</dc:creator>
    </item>
    <item>
      <title>Strong convergence rates for full-discrete approximations of the stochastic Allen-Cahn equations on 2D torus</title>
      <link>https://arxiv.org/abs/2406.03715</link>
      <description>arXiv:2406.03715v1 Announce Type: cross 
Abstract: In this paper we construct space-time full discretizations of stochastic Allen-Cahn equations driven by space-time white noise on 2D torus. The approximations are implemented by tamed exponential Euler discretization in time and spectral Galerkin method in space. We finally obtain the convergence rates with the spatial order of $\alpha-\delta$ and the temporal order of ${\alpha}/{6}-\delta$ in $\mathcal C^{-\alpha}$ for $\alpha\in(0,1/3)$ and $\delta&gt;0$ arbitrarily small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03715v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Ma, Lifei Wang, Huanyu Yang</dc:creator>
    </item>
    <item>
      <title>Recognizing weighted means in geodesic spaces</title>
      <link>https://arxiv.org/abs/2406.03913</link>
      <description>arXiv:2406.03913v1 Announce Type: cross 
Abstract: Geodesic metric spaces support a variety of averaging constructions for given finite sets. Computing such averages has generated extensive interest in diverse disciplines. Here we consider the inverse problem of recognizing computationally whether or not a given point is such an average, exactly or approximately. In nonpositively curved spaces, several averaging notions, including the usual weighted barycenter, produce the same "mean set". In such spaces, at points where the tangent cone is a Euclidean space, the recognition problem reduces to Euclidean projection onto a polytope. Hadamard manifolds comprise one example. Another consists of CAT(0) cubical complexes, at relative-interior points: the recognition problem is harder for general points, but we present an efficient semidefinite-programming-based algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03913v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Goodwin, Adrian S. Lewis, Genaro Lopez-Acedo, Adriana Nicolae</dc:creator>
    </item>
    <item>
      <title>Latent Neural Operator for Solving Forward and Inverse PDE Problems</title>
      <link>https://arxiv.org/abs/2406.03923</link>
      <description>arXiv:2406.03923v1 Announce Type: cross 
Abstract: Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values. Most existed works build the model in the original geometric space, leading to high computational costs when the number of sample points is large. We present the Latent Neural Operator (LNO) solving PDEs in the latent space. In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map. Our model retains flexibility that can decode values in any position not limited to locations defined in training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems. Moreover, the proposed LNO improves in both prediction accuracy and computational efficiency. Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03923v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tian Wang, Chuang Wang</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis using Physics-informed neural networks</title>
      <link>https://arxiv.org/abs/2301.02428</link>
      <description>arXiv:2301.02428v3 Announce Type: replace 
Abstract: The goal of this paper is to provide a simple approach to perform local sensitivity analysis using Physics-informed neural networks (PINN). The main idea lies in adding a new term in the loss function that regularizes the solution in a small neighborhood near the nominal value of the parameter of interest. The added term represents the derivative of the loss function with respect to the parameter of interest. The result of this modification is a solution to the problem along with the derivative of the solution with respect to the parameter of interest (the sensitivity). We call the new technique SA-PNN which stands for sensitivity analysis in PINN. The effectiveness of the technique is shown using four examples: the first one is a simple one-dimensional advection-diffusion problem to show the methodology, the second is a two-dimensional Poisson's problem with nine parameters of interest, and the third and fourth examples are one and two-dimensional transient two-phase flow in porous media problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02428v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John M. Hanna, Jos\'e V. Aguado, Sebastien Comas-Cardona, Ramzi Askri, Domenico Borzacchiello</dc:creator>
    </item>
    <item>
      <title>Approximating the closest structured singular matrix polynomial</title>
      <link>https://arxiv.org/abs/2301.06335</link>
      <description>arXiv:2301.06335v2 Announce Type: replace 
Abstract: Consider a matrix polynomial $P \left( \lambda \right)= A_0 + \lambda A_1 + \ldots + \lambda^d A_d$, with $A_0,\ldots, A_d$ complex (or real) matrices with a certain structure. In this paper we discuss an iterative method to numerically approximate the closest structured singular matrix polynomial $\widetilde P\left( \lambda \right)$, using the distance induced by the Frobenius norm. An important peculiarity of the approach we propose is the possibility to include different types of structural constraints. The method also allows us to limit the perturbations to just a few matrices and also to include additional structures, such as the preservation of the sparsity pattern of one or more matrices $A_i$, and also collective-like properties, like a palindromic structure. The iterative method is based on the numerical integration of the gradient system associated with a suitable functional which quantifies the distance to singularity of a matrix polynomial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06335v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miryam Gnazzo, Nicola Guglielmi</dc:creator>
    </item>
    <item>
      <title>Asymptotic behaviour of the semidiscrete FE approximations to weakly damped wave equations with minimal smoothness on initial data</title>
      <link>https://arxiv.org/abs/2302.12476</link>
      <description>arXiv:2302.12476v2 Announce Type: replace 
Abstract: Exponential decay estimates of a general linear weakly damped wave equation are studied with decay rate lying in a range. Based on the $C^0$-conforming finite element method to discretize spatial variables keeping temporal variable continuous, a semidiscrete system is analysed, and uniform decay estimates are derived with precisely the same decay rate as in the continuous case. Optimal error estimates with minimal smoothness assumptions on the initial data are established, which preserve exponential decay rate, and for a 2D problem, the maximum error bound is also proved. The present analysis is then generalized to include the problems with non-homogeneous forcing function, space-dependent damping, and problems with compensator. It is observed that decay rates are improved with large viscous damping and compensator. Finally, some numerical experiments are performed to validate the theoretical results established in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12476v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>P. Danumjaya, Anil Kumar, Amiya K. Pani</dc:creator>
    </item>
    <item>
      <title>Smoothed Circulant Embedding with Applications to Multilevel Monte Carlo Methods for PDEs with Random Coefficients</title>
      <link>https://arxiv.org/abs/2306.13493</link>
      <description>arXiv:2306.13493v2 Announce Type: replace 
Abstract: We consider the computational efficiency of Monte Carlo (MC) and Multilevel Monte Carlo (MLMC) methods applied to partial differential equations with random coefficients. These arise, for example, in groundwater flow modelling, where a commonly used model for the unknown parameter is a random field. We make use of the circulant embedding procedure for sampling from the aforementioned coefficient. To improve the computational complexity of the MLMC estimator in the case of highly oscillatory random fields, we devise and implement a smoothing technique integrated into the circulant embedding method. This allows to choose the coarsest mesh on the first level of MLMC independently of the correlation length of the covariance function of the random field, leading to considerable savings in computational cost. We illustrate this with numerical experiments, where we see a saving of factor 5-10 in computational cost for accuracies of practical interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13493v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasia Istratuca, Aretha Teckentrup</dc:creator>
    </item>
    <item>
      <title>A direct sampling method based on the Green's function for time-dependent inverse scattering problems</title>
      <link>https://arxiv.org/abs/2308.06020</link>
      <description>arXiv:2308.06020v2 Announce Type: replace 
Abstract: This paper concerns the numerical simulation of time domain inverse acoustic scattering problems with a point-like scatterer, multiple point-like scatterers or normal size scatterers. Based on the Green's function and the application of the time convolution, direct sampling methods are proposed to reconstruct the location of the scatterer. The proposed methods involve only integral calculus without solving any equations and are easy to implement. Numerical experiments are provided to show the effectiveness and robustness of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06020v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingqing Yu, Bo Chen, Jiaru Wang, Yao Sun</dc:creator>
    </item>
    <item>
      <title>Robust Blockwise Random Pivoting: Fast and Accurate Adaptive Interpolative Decomposition</title>
      <link>https://arxiv.org/abs/2309.16002</link>
      <description>arXiv:2309.16002v3 Announce Type: replace 
Abstract: The interpolative decomposition (ID) aims to construct a low-rank approximation formed by a basis consisting of row/column skeletons in the original matrix and a corresponding interpolation matrix. This work explores fast and accurate ID algorithms from five essential perspectives for empirical performance: (a) skeleton complexity that measures the minimum possible ID rank for a given low-rank approximation error, (b) asymptotic complexity in FLOPs, (c) parallelizability of the computational bottleneck as matrix-matrix multiplications, (d) error-revealing property that enables automatic rank detection for given error tolerances without prior knowledge of target ranks, (e) ID-revealing property that ensures efficient construction of the optimal interpolation matrix after selecting the skeletons. While a broad spectrum of algorithms have been developed to optimize parts of the aforementioned perspectives, practical ID algorithms proficient in all perspectives remain absent. To fill in the gap, we introduce robust blockwise random pivoting (RBRP) that is parallelizable, error-revealing, and exactly ID-revealing, with comparable skeleton and asymptotic complexities to the best existing ID algorithms in practice. Through extensive numerical experiments on various synthetic and natural datasets, we demonstrate the appealing empirical performance of RBRP from the five perspectives above, as well as the robustness of RBRP to adversarial inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16002v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Dong, Chao Chen, Per-Gunnar Martinsson, Katherine Pearce</dc:creator>
    </item>
    <item>
      <title>Nonlinear Methods for Shape Optimization Problems in Liquid Crystal Tactoids</title>
      <link>https://arxiv.org/abs/2310.04022</link>
      <description>arXiv:2310.04022v2 Announce Type: replace 
Abstract: Anisotropic fluids, such as nematic liquid crystals, can form non-spherical equilibrium shapes known as tactoids. Predicting the shape of these structures as a function of material parameters is challenging and paradigmatic of a broader class of problems that combine shape and order. Here, we consider a discrete shape optimization approach with finite elements to find the configuration of two-dimensional and three-dimensional tactoids using the Landau de Gennes framework and a Q-tensor representation. Efficient solution of the resulting constrained energy minimization problem is achieved using a quasi-Newton and nested iteration algorithm. Numerical validation is performed with benchmark solutions and compared against experimental data and earlier work. We explore physically motivated subproblems, whereby the shape and order are separately held fixed, respectively, to explore the role of both and examine material parameter dependence of the convergence. Nested iteration significantly improves both the computational cost and convergence of numerical solutions of these highly deformable materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04022v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James H. Adler, Anca S. Andrei, Timothy J. Atherton</dc:creator>
    </item>
    <item>
      <title>A Meshless Solver for Blood Flow Simulations in Elastic Vessels Using Physics-Informed Neural Network</title>
      <link>https://arxiv.org/abs/2312.05601</link>
      <description>arXiv:2312.05601v4 Announce Type: replace 
Abstract: Investigating blood flow in the cardiovascular system is crucial for assessing cardiovascular health. Computational approaches offer some non-invasive alternatives to measure blood flow dynamics. Numerical simulations based on traditional methods such as finite-element and other numerical discretizations have been extensively studied and have yielded excellent results. However, adapting these methods to real-life simulations remains a complex task. In this paper, we propose a method that offers flexibility and can efficiently handle real-life simulations. We suggest utilizing the physics-informed neural network (PINN) to solve the Navier-Stokes equation in a deformable domain, specifically addressing the simulation of blood flow in elastic vessels. Our approach models blood flow using an incompressible, viscous Navier-Stokes equation in an Arbitrary Lagrangian-Eulerian form. The mechanical model for the vessel wall structure is formulated by an equation of Newton's second law of momentum and linear elasticity to the force exerted by the fluid flow. Our method is a mesh-free approach that eliminates the need for discretization and meshing of the computational domain. This makes it highly efficient in solving simulations involving complex geometries. Additionally, with the availability of well-developed open-source machine learning framework packages and parallel modules, our method can easily be accelerated through GPU computing and parallel computing. To evaluate our approach, we conducted experiments on regular cylinder vessels as well as vessels with plaque on their walls. We compared our results to a solution calculated by Finite Element Methods using a dense grid and small time steps, which we considered as the ground truth solution. We report the relative error and the time consumed to solve the problem, highlighting the advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05601v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Zhang, Raymond Chan, Xue-Cheng Tai</dc:creator>
    </item>
    <item>
      <title>Multifidelity domain decomposition-based physics-informed neural networks and operators for time-dependent problems</title>
      <link>https://arxiv.org/abs/2401.07888</link>
      <description>arXiv:2401.07888v2 Announce Type: replace 
Abstract: Multiscale problems are challenging for neural network-based discretizations of differential equations, such as physics-informed neural networks (PINNs). This can be (partly) attributed to the so-called spectral bias of neural networks. To improve the performance of PINNs for time-dependent problems, a combination of multifidelity stacking PINNs and domain decomposition-based finite basis PINNs is employed. In particular, to learn the high-fidelity part of the multifidelity model, a domain decomposition in time is employed. The performance is investigated for a pendulum and a two-frequency problem as well as the Allen-Cahn equation. It can be observed that the domain decomposition approach clearly improves the PINN and stacking PINN approaches. Finally, it is demonstrated that the FBPINN approach can be extended to multifidelity physics-informed deep operator networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07888v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Heinlein, Amanda A. Howard, Damien Beecroft, Panos Stinis</dc:creator>
    </item>
    <item>
      <title>Sharp Lower Bounds on the Manifold Widths of Sobolev and Besov Spaces</title>
      <link>https://arxiv.org/abs/2402.04407</link>
      <description>arXiv:2402.04407v3 Announce Type: replace 
Abstract: We consider the problem of determining the manifold $n$-widths of Sobolev and Besov spaces with error measured in the $L_p$-norm. The manifold widths control how efficiently these spaces can be approximated by general non-linear parametric methods with the restriction that the parameter selection and parameterization maps must be continuous. Existing upper and lower bounds only match when the Sobolev or Besov smoothness index $q$ satisfies $q\leq p$ or $1 \leq p \leq 2$. We close this gap and obtain sharp lower bounds for all $1 \leq p,q \leq \infty$ for which a compact embedding holds. A key part of our analysis is to determine the exact value of the manifold widths of finite dimensional $\ell^M_q$-balls in the $\ell_p$-norm when $p\leq q$. Although this result is not new, we provide a new proof and apply it to lower bounding the manifold widths of Sobolev and Besov spaces. Our results show that the Bernstein widths, which are typically used to lower bound the manifold widths, decay asymptotically faster than the manifold widths in many cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04407v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan W. Siegel</dc:creator>
    </item>
    <item>
      <title>Early Stopping of Untrained Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2402.04610</link>
      <description>arXiv:2402.04610v2 Announce Type: replace 
Abstract: In recent years, new regularization methods based on (deep) neural networks have shown very promising empirical performance for the numerical solution of ill-posed problems, e.g., in medical imaging and imaging science. Due to the nonlinearity of neural networks, these methods often lack satisfactory theoretical justification. In this work, we rigorously discuss the convergence of a successful unsupervised approach that utilizes untrained convolutional neural networks to represent solutions to linear ill-posed problems. Untrained neural networks are particularly appealing for many applications because they do not require paired training data. The regularization property of the approach relies solely on the architecture of the neural network instead. Due to the vast over-parameterization of the employed neural network, suitable early stopping is essential for the success of the method. We establish that the classical discrepancy principle is an adequate method for early stopping of two-layer untrained convolutional neural networks learned by gradient descent, and furthermore, it yields an approximation with minimax optimal convergence rates. Numerical results are also presented to illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04610v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Jahn, Bangti Jin</dc:creator>
    </item>
    <item>
      <title>A randomized lattice rule without component-by-component construction</title>
      <link>https://arxiv.org/abs/2403.02660</link>
      <description>arXiv:2403.02660v2 Announce Type: replace 
Abstract: We study the multivariate integration problem for periodic functions from the weighted Korobov space in the randomized setting. We introduce a new randomized rank-1 lattice rule with a randomly chosen number of points, which avoids the need for component-by-component construction in the search for good generating vectors while still achieving nearly the optimal rate of the randomized error. Our idea is to exploit the fact that at least half of the possible generating vectors yield nearly the optimal rate of the worst-case error in the deterministic setting. By randomly choosing generating vectors $r$ times and comparing their corresponding worst-case errors, one can find one generating vector with a desired worst-case error bound with a very high probability, and the (small) failure probability can be controlled by increasing $r$ logarithmically as a function of the number of points. Numerical experiments are conducted to support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02660v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Goda</dc:creator>
    </item>
    <item>
      <title>Computing large deviation rate functions of entropy production for diffusion processes by an interacting particle method</title>
      <link>https://arxiv.org/abs/2403.19223</link>
      <description>arXiv:2403.19223v3 Announce Type: replace 
Abstract: We study an interacting particle method (IPM) for computing the large deviation rate function of entropy production for diffusion processes, with emphasis on the vanishing-noise limit and high dimensions. The crucial ingredient to obtain the rate function is the computation of the principal eigenvalue $\lambda$ of elliptic, non-self-adjoint operators. We show that this principal eigenvalue can be approximated in terms of the spectral radius of a discretized evolution operator obtained from an operator splitting scheme and an Euler--Maruyama scheme with a small time step size, and we show that this spectral radius can be accessed through a large number of iterations of this discretized semigroup, suitable for the IPM. The IPM applies naturally to problems in unbounded domains, scales easily to high dimensions, and adapts to singular behaviors in the vanishing-noise limit. We show numerical examples in dimensions up to 16. The numerical results show that our numerical approximation of $\lambda$ converges to the analytical vanishing-noise limit within visual tolerance with a fixed number of particles and a fixed time step size. Our paper appears to be the first one to obtain numerical results of principal eigenvalue problems for non-self-adjoint operators in such high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19223v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhizhang Wu, Renaud Raqu\'epas, Jack Xin, Zhiwen Zhang</dc:creator>
    </item>
    <item>
      <title>A bound preserving cut discontinuous Galerkin method for one dimensional hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2404.13936</link>
      <description>arXiv:2404.13936v2 Announce Type: replace 
Abstract: In this paper we present a family of high order cut finite element methods with bound preserving properties for hyperbolic conservation laws in one space dimension. The methods are based on the discontinuous Galerkin framework and use a regular background mesh, where interior boundaries are allowed to cut through the mesh arbitrarily. Our methods include ghost penalty stabilization to handle small cut elements and a new reconstruction of the approximation on macro-elements, which are local patches consisting of cut and un-cut neighboring elements that are connected by stabilization. We show that the reconstructed solution retains conservation and order of convergence.
  Our lowest-order scheme results in a piecewise constant solution that satisfies a maximum principle for scalar hyperbolic conservation laws.
  When the lowest order scheme is applied to the Euler equations, the scheme is positivity preserving in the sense that positivity of pressure and density are retained. For the high-order schemes, suitable bound preserving limiters are applied to the reconstructed solution on macro-elements. In the scalar case, a maximum principle limiter is applied, which ensures that the limited approximation satisfies the maximum principle. Correspondingly, we use a positivity preserving limiter for the Euler equations and show that our scheme is positivity preserving. In the presence of shocks, additional limiting is needed to avoid oscillations, hence we apply a standard TVB limiter to the reconstructed solution. The time step restrictions are of the same order as for the corresponding discontinuous Galerkin methods on the background mesh. Numerical computations illustrate accuracy, bound preservation, and shock capturing capabilities of the proposed schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13936v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Fu, Gunilla Kreiss, Sara Zahedi</dc:creator>
    </item>
    <item>
      <title>Cons-training tensor networks</title>
      <link>https://arxiv.org/abs/2405.09005</link>
      <description>arXiv:2405.09005v2 Announce Type: replace 
Abstract: In this study, we introduce a novel family of tensor networks, termed \textit{constrained matrix product states} (MPS), designed to incorporate exactly arbitrary discrete linear constraints, including inequalities, into sparse block structures. These tensor networks are particularly tailored for modeling distributions with support strictly over the feasible space, offering benefits such as reducing the search space in optimization problems, alleviating overfitting, improving training efficiency, and decreasing model size. Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in U(1) symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario. We further develop a novel canonical form for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules and permit optimal truncation schemes. Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary objective functions subject to discrete linear constraints. Our method's efficacy is demonstrated by solving the quadratic knapsack problem, achieving superior performance compared to a leading nonlinear integer programming solver. Additionally, we analyze the complexity and scalability of our approach, demonstrating its potential in addressing complex constrained combinatorial optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09005v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Lopez-Piqueres, Jing Chen</dc:creator>
    </item>
    <item>
      <title>PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces</title>
      <link>https://arxiv.org/abs/2305.11915</link>
      <description>arXiv:2305.11915v3 Announce Type: replace-cross 
Abstract: In the paper, we describe in operator form classes of PDEs that admit PINN's error estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert type lemma that is a tool for PINN's residuals bounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11915v3</guid>
      <category>math.FA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiexing Gao, Yurii Zakharian</dc:creator>
    </item>
    <item>
      <title>Optimal Embedding Dimension for Sparse Subspace Embeddings</title>
      <link>https://arxiv.org/abs/2311.10680</link>
      <description>arXiv:2311.10680v2 Announce Type: replace-cross 
Abstract: A random $m\times n$ matrix $S$ is an oblivious subspace embedding (OSE) with parameters $\epsilon&gt;0$, $\delta\in(0,1/3)$ and $d\leq m\leq n$, if for any $d$-dimensional subspace $W\subseteq R^n$,
  $P\big(\,\forall_{x\in W}\ (1+\epsilon)^{-1}\|x\|\leq\|Sx\|\leq (1+\epsilon)\|x\|\,\big)\geq 1-\delta.$
  It is known that the embedding dimension of an OSE must satisfy $m\geq d$, and for any $\theta &gt; 0$, a Gaussian embedding matrix with $m\geq (1+\theta) d$ is an OSE with $\epsilon = O_\theta(1)$. However, such optimal embedding dimension is not known for other embeddings. Of particular interest are sparse OSEs, having $s\ll m$ non-zeros per column, with applications to problems such as least squares regression and low-rank approximation.
  We show that, given any $\theta &gt; 0$, an $m\times n$ random matrix $S$ with $m\geq (1+\theta)d$ consisting of randomly sparsified $\pm1/\sqrt s$ entries and having $s= O(\log^4(d))$ non-zeros per column, is an oblivious subspace embedding with $\epsilon = O_{\theta}(1)$. Our result addresses the main open question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse OSEs can achieve $m=O(d)$ embedding dimension, and it improves on $m=O(d\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first oblivious subspace embedding with $O(d)$ embedding dimension that can be applied faster than current matrix multiplication time, and to obtain an optimal single-pass algorithm for least squares regression. We further extend our results to Leverage Score Sparsification (LESS), which is a recently introduced non-oblivious embedding technique. We use LESS to construct the first subspace embedding with low distortion $\epsilon=o(1)$ and optimal embedding dimension $m=O(d/\epsilon^2)$ that can be applied in current matrix multiplication time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10680v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabarish Chenakkod, Micha{\l} Derezi\'nski, Xiaoyu Dong, Mark Rudelson</dc:creator>
    </item>
    <item>
      <title>Subhomogeneous Deep Equilibrium Models</title>
      <link>https://arxiv.org/abs/2403.00720</link>
      <description>arXiv:2403.00720v2 Announce Type: replace-cross 
Abstract: Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feedforward, convolutional, and graph neural network examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00720v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietro Sittoni, Francesco Tudisco</dc:creator>
    </item>
  </channel>
</rss>
