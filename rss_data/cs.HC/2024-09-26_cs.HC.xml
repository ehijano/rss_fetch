<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 01:58:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Beyond Following: Mixing Active Initiative into Computational Creativity</title>
      <link>https://arxiv.org/abs/2409.16291</link>
      <description>arXiv:2409.16291v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (AI) encounters limitations in efficiency and fairness within the realm of Procedural Content Generation (PCG) when human creators solely drive and bear responsibility for the generative process. Alternative setups, such as Mixed-Initiative Co-Creative (MI-CC) systems, exhibited their promise. Still, the potential of an active mixed initiative, where AI takes a role beyond following, is understudied. This work investigates the influence of the adaptive ability of an active and learning AI agent on creators' expectancy of creative responsibilities in an MI-CC setting. We built and studied a system that employs reinforcement learning (RL) methods to learn the creative responsibility preferences of a human user during online interactions. Situated in story co-creation, we develop a Multi-armed-bandit agent that learns from the human creator, updates its collaborative decision-making belief, and switches between its capabilities during an MI-CC experience. With 39 participants joining a human subject study, Our developed system's learning capabilities are well recognized compared to the non-learning ablation, corresponding to a significant increase in overall satisfaction with the MI-CC experience. These findings indicate a robust association between effective MI-CC collaborative interactions, particularly the implementation of proactive AI initiatives, and deepened understanding among all participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16291v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyu Lin, Upol Ehsan, Rohan Agarwal, Samihan Dani, Vidushi Vashishth, Mark Riedl</dc:creator>
    </item>
    <item>
      <title>Interaction Techniques for User-friendly Interfaces for Gate-based Quantum Computing</title>
      <link>https://arxiv.org/abs/2409.16475</link>
      <description>arXiv:2409.16475v1 Announce Type: new 
Abstract: Quantum computers offer promising approaches to various fields. To use current noisy quantum computers, developers need to examine the compilation of a logical circuit, the status of available hardware, and noises in results. As those tasks are less common in classical computing, quantum developers may not be familiar with performing them. Therefore, easier and more intuitive interfaces are necessary to make quantum computers more approachable. While existing notebook-based toolkits like Qiskit offer application programming interfaces and visualization techniques, it is still difficult to navigate the vast space of quantum program design and hardware status.
  Inspired by human-computer interaction (HCI) work in data science and visualization, our work introduces four user interaction techniques that can augment existing notebook-based toolkits for gate-based quantum computing: (1) a circuit writer that lets users provide high-level information about a circuit and generates a code snippet to build it; (2) a machine explorer that provides detailed properties and configurations of a hardware with a code to load selected information; (3) a circuit viewer that allows for comparing logical circuit, compiled circuit, and hardware configurations; and (4) a visualization for adjusting measurement outcomes with hardware error rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16475v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyeok Kim, Kaitlin N. Smith</dc:creator>
    </item>
    <item>
      <title>NoTeeline: Supporting Real-Time Notetaking from Keypoints with Large Language Models</title>
      <link>https://arxiv.org/abs/2409.16493</link>
      <description>arXiv:2409.16493v1 Announce Type: new 
Abstract: Video has become a popular media form for information sharing and consumption. However, taking notes while watching a video requires significant time and effort. To address this, we propose a novel interactive system, NoTeeline, for taking real-time, personalized notes. NoTeeline lets users quickly jot down keypoints (micronotes), which are automatically expanded into full-fledged notes that capture the content of the user's micronotes and are consistent with the user's writing style. In a within-subjects study (N=12), we found that NoTeeline helps users create high-quality notes that capture the essence of their micronotes with a higher factual correctness (93.2%) while accurately reflecting their writing style. While using NoTeeline, participants experienced significantly reduced mental effort, captured satisfactory notes while writing 47% less text, and completed notetaking with 43.9% less time compared to a manual notetaking baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16493v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faria Huq, Abdus Samee, David Chuan-en Lin, Xiaodi Alice Tang, Jeffrey P. Bigham</dc:creator>
    </item>
    <item>
      <title>Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories</title>
      <link>https://arxiv.org/abs/2409.16561</link>
      <description>arXiv:2409.16561v1 Announce Type: new 
Abstract: An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process. To facilitate this, we introduce MOCHA, an interactive machine learning tool informed by two theories of human concept learning and cognition. First, it utilizes a neuro-symbolic pipeline to support Variation Theory-based counterfactual data generation. By asking users to annotate counterexamples that are syntactically and semantically similar to already-annotated data but predicted to have different labels, the system can learn more effectively while helping users understand the model and reflect on their own label definitions. Second, MOCHA uses Structural Alignment Theory to present groups of counterexamples, helping users comprehend alignable differences between data items and annotate them in batch. We validated MOCHA's effectiveness and usability through a lab study with 18 participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16561v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simret Araya Gebreegziabher, Yukun Yang, Elena L. Glassman, Toby Jia-Jun Li</dc:creator>
    </item>
    <item>
      <title>AI Makes You Smarter, But None The Wiser: The Disconnect Between Performance and Metacognition</title>
      <link>https://arxiv.org/abs/2409.16708</link>
      <description>arXiv:2409.16708v1 Announce Type: new 
Abstract: Optimizing human-AI interaction requires users to reflect on their own performance critically. Our study examines whether people using AI to complete tasks can accurately monitor how well they perform. Participants (N = 246) used AI to solve 20 logical problems from the Law School Admission Test. While their task performance improved by three points compared to a norm population, participants overestimated their performance by four points. Interestingly, higher AI literacy was linked to less accurate self-assessment. Participants with more technical knowledge of AI were more confident but less precise in judging their own performance. Using a computational model, we explored individual differences in metacognitive accuracy and found that the Dunning-Kruger effect, usually observed in this task, ceased to exist with AI use. We discuss how AI levels our cognitive and metacognitive performance and consider the consequences of performance overestimation for designing interactive AI systems that enhance cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16708v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniela Fernandes, Steeven Villa, Salla Nicholls, Otso Haavisto, Daniel Buschek, Albrecht Schmidt, Thomas Kosch, Chenxinran Shen, Robin Welsch</dc:creator>
    </item>
    <item>
      <title>"It Explains What I am Currently Going Through Perfectly to a Tee": Understanding User Perceptions on LLM-Enhanced Narrative Interventions</title>
      <link>https://arxiv.org/abs/2409.16732</link>
      <description>arXiv:2409.16732v1 Announce Type: new 
Abstract: Stories about overcoming personal struggles can effectively illustrate the application of psychological theories in real life, yet they may fail to resonate with individuals' experiences. In this work, we employ large language models (LLMs) to create tailored narratives that acknowledge and address unique challenging thoughts and situations faced by individuals. Our study, involving 346 young adults across two settings, demonstrates that LLM-enhanced stories were perceived to be better than human-written ones in conveying key takeaways, promoting reflection, and reducing belief in negative thoughts. These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles. The findings of this work provide crucial design considerations for future narrative-based digital mental health interventions, such as the need to maintain relatability without veering into implausibility and refining the wording and tone of AI-enhanced content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16732v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Bhattacharjee, Sarah Yi Xu, Pranav Rao, Yuchen Zeng, Jonah Meyerhoff, Syed Ishtiaque Ahmed, David C Mohr, Michael Liut, Alex Mariakakis, Rachel Kornfield, Joseph Jay Williams</dc:creator>
    </item>
    <item>
      <title>Modeling the Modqueue: Towards Understanding and Improving Report Resolution on Reddit</title>
      <link>https://arxiv.org/abs/2409.16840</link>
      <description>arXiv:2409.16840v1 Announce Type: new 
Abstract: There are three common stages in the moderation process employed by platforms like Reddit: rule creation, reporting/triaging, and report resolution. While the first two stages are well-studied in HCI, the third stage remains under-explored. Directly observing report resolution is challenging, since it requires using invasive tracking tools that moderators may feel uncomfortable with. However, evaluating the current state of this stage is crucial to improve moderation outcomes, especially as online communities continue to grow. In this paper, we present a non-invasive methodology to study report resolution via modeling and simulations. Using agent-based modeling, we analyze the performance of report resolution on Reddit using theory-driven measures and use our results to motivate interventions. We then highlight potential improvements that can be gained by adopting these interventions. We conclude by discussing how modeling and simulations can be used to navigate processes like report resolution and inform the design of new moderation interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16840v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanvi Bajpai, Eshwar Chandrasekharan</dc:creator>
    </item>
    <item>
      <title>Quantifying Visual Properties of GAM Shape Plots: Impact on Perceived Cognitive Load and Interpretability</title>
      <link>https://arxiv.org/abs/2409.16870</link>
      <description>arXiv:2409.16870v1 Announce Type: new 
Abstract: Generalized Additive Models (GAMs) offer a balance between performance and interpretability in machine learning. The interpretability aspect of GAMs is expressed through shape plots, representing the model's decision-making process. However, the visual properties of these plots, e.g. number of kinks (number of local maxima and minima), can impact their complexity and the cognitive load imposed on the viewer, compromising interpretability. Our study, including 57 participants, investigates the relationship between the visual properties of GAM shape plots and cognitive load they induce. We quantify various visual properties of shape plots and evaluate their alignment with participants' perceived cognitive load, based on 144 plots. Our results indicate that the number of kinks metric is the most effective, explaining 86.4% of the variance in users' ratings. We develop a simple model based on number of kinks that provides a practical tool for predicting cognitive load, enabling the assessment of one aspect of GAM interpretability without direct user involvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16870v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sven Kruschel, Lasse Bohlen, Julian Rosenberger, Patrick Zschech, Mathias Kraus</dc:creator>
    </item>
    <item>
      <title>Wrapped in Anansi's Web: Unweaving the Impacts of Generative-AI Personalization and VR Immersion in Oral Storytelling</title>
      <link>https://arxiv.org/abs/2409.16894</link>
      <description>arXiv:2409.16894v1 Announce Type: new 
Abstract: Oral traditions, vital to cultural identity, are losing relevance among youth due to the dominance of modern media. This study addresses the revitalization of these traditions by reconnecting young people with folklore. We introduce Anansi the Spider VR, a novel virtual space that combines first-person virtual reality (VR) with generative artificial intelligence (Gen-AI)-driven narrative personalization. This space immerses users in the Anansi Spider story, empowering them to influence the narrative as they envision themselves as the `protagonists,' thereby enhancing personal reflection. In a 2 by 2 between-subjects study with 48 participants, we employed a mixed-method approach to measure user engagement and changes in interest, complemented by semi-structured interviews providing qualitative insights into personalization and immersion. Our results indicate that personalization in VR significantly boosts engagement and cultural learning interest. We recommend that future studies using VR and Gen-AI to revitalize oral storytelling prioritize respecting cultural integrity and honoring original storytellers and communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16894v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ka Hei Carrie Lau, Bhada Yun, Samuel Saruba, Efe Bozkir, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Sense of Agency in Closed-loop Muscle Stimulation</title>
      <link>https://arxiv.org/abs/2409.16896</link>
      <description>arXiv:2409.16896v1 Announce Type: new 
Abstract: To maintain a user's sense of agency (SoA) when working with a physical motor augmentation device, the actuation must align with the user's intentions. In experiments, this is often achieved using stimulus-response paradigms where the motor augmentation can be optimally timed. However, in the everyday world users primarily act at their own volition. We designed a closed-loop system for motor augmentation using an EEG-based brain-computer interface (BCI) to cue users' volitional finger tapping. Relying on the readiness potentials, the system autonomously cued the finger movement at the time of the intent to interact via electrical muscle stimulation (EMS). The prototype discriminated pre-movement from idle EEG segments with an average F1 score of 0.7. However, we found only weak evidence for a maintained SoA. Still, participants reported a higher level of control when working with the system instead of being passively moved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16896v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lukas Gehrke, Leonie Terfurth, Klaus Gramann</dc:creator>
    </item>
    <item>
      <title>Tactile Perception of Electroadhesion: Effect of DC versus AC Stimulation and Finger Moisture</title>
      <link>https://arxiv.org/abs/2409.16936</link>
      <description>arXiv:2409.16936v1 Announce Type: new 
Abstract: Electroadhesion has emerged as a viable technique for displaying tactile feedback on touch surfaces, particularly capacitive touchscreens found in smartphones and tablets. This involves applying a voltage signal to the conductive layer of the touchscreen to generate tactile sensations on the fingerpads of users. In our investigation, we explore the tactile perception of electroadhesion under DC and AC stimulations. Our tactile perception experiments with 10 participants demonstrate a significantly lower voltage detection threshold for AC signals compared to their DC counterparts. This discrepancy is elucidated by the underlying electro-mechanical interactions between the finger and the voltage-induced touchscreen and considering the response of mechanoreceptors in the fingerpad to electrostatic forces generated by electroadhesion. Additionally, our study highlights the impact of moisture on electroadhesive tactile perception. Participants with moist fingers exhibited markedly higher threshold levels. Our electrical impedance measurements show a substantial reduction in impedance magnitude when sweat is present at the finger-touchscreen interface, indicating increased conductivity. These findings not only contribute to our understanding of tactile perception under electroadhesion but also shed light on the underlying physics. In this regard, the results of this study extend beyond mobile devices to encompass other applications of this technology, including robotics, automation, space missions, and textiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16936v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TOH.2024.3441670</arxiv:DOI>
      <dc:creator>Easa AliAbbasi, Muhammad Muzammil, Omer Sirin, Philippe Lef\`evre, {\O}rjan Gr{\o}ttem Martinsen, Cagatay Basdogan</dc:creator>
    </item>
    <item>
      <title>Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI</title>
      <link>https://arxiv.org/abs/2409.16978</link>
      <description>arXiv:2409.16978v1 Announce Type: new 
Abstract: While Explainable AI (XAI) aims to make AI understandable and useful to humans, it has been criticised for relying too much on formalism and solutionism, focusing more on mathematical soundness than user needs. We propose an alternative to this bottom-up approach inspired by design thinking: the XAI research community should adopt a top-down, user-focused perspective to ensure user relevance. We illustrate this with a relatively young subfield of XAI, Training Data Attribution (TDA). With the surge in TDA research and growing competition, the field risks repeating the same patterns of solutionism. We conducted a needfinding study with a diverse group of AI practitioners to identify potential user needs related to TDA. Through interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks that are currently largely overlooked. We invite the TDA and XAI communities to consider these novel tasks and improve the user relevance of their research outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16978v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisa Nguyen, Johannes Bertram, Evgenii Kortukov, Jean Y. Song, Seong Joon Oh</dc:creator>
    </item>
    <item>
      <title>Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing</title>
      <link>https://arxiv.org/abs/2409.17088</link>
      <description>arXiv:2409.17088v1 Announce Type: new 
Abstract: We explore how interactions inspired by drawing software can help edit text. Making an analogy between visual and text editing, we consider words as pixels, sentences as regions, and tones as colours. For instance, direct manipulations move, shorten, expand, and reorder text; tools change number, tense, and grammar; colours map to tones explored along three dimensions in a tone picker; and layers help organize and version text. This analogy also leads to new workflows, such as boolean operations on text fragments to construct more elaborated text. A study shows participants were more successful at editing text and preferred using the proposed interface over existing solutions. Broadly, our work highlights the potential of interaction analogies to rethink existing workflows, while capitalizing on familiar features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17088v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damien Masson, Young-Ho Kim, Fanny Chevalier</dc:creator>
    </item>
    <item>
      <title>Identify As A Human Does: A Pathfinder of Next-Generation Anti-Cheat Framework for First-Person Shooter Games</title>
      <link>https://arxiv.org/abs/2409.14830</link>
      <description>arXiv:2409.14830v1 Announce Type: cross 
Abstract: The gaming industry has experienced substantial growth, but cheating in online games poses a significant threat to the integrity of the gaming experience. Cheating, particularly in first-person shooter (FPS) games, can lead to substantial losses for the game industry. Existing anti-cheat solutions have limitations, such as client-side hardware constraints, security risks, server-side unreliable methods, and both-sides suffer from a lack of comprehensive real-world datasets. To address these limitations, the paper proposes HAWK, a server-side FPS anti-cheat framework for the popular game CS:GO. HAWK utilizes machine learning techniques to mimic human experts' identification process, leverages novel multi-view features, and it is equipped with a well-defined workflow. The authors evaluate HAWK with the first large and real-world datasets containing multiple cheat types and cheating sophistication, and it exhibits promising efficiency and acceptable overheads, shorter ban times compared to the in-use anti-cheat, a significant reduction in manual labor, and the ability to capture cheaters who evaded official inspections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14830v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Zhang, Chenxin Sun, Yue Gu, Qingyu Zhang, Jiayi Lin, Xiaojiang Du, Chenxiong Qian</dc:creator>
    </item>
    <item>
      <title>New Insights into Global Warming: End-to-End Visual Analysis and Prediction of Temperature Variations</title>
      <link>https://arxiv.org/abs/2409.16311</link>
      <description>arXiv:2409.16311v1 Announce Type: cross 
Abstract: Global warming presents an unprecedented challenge to our planet however comprehensive understanding remains hindered by geographical biases temporal limitations and lack of standardization in existing research. An end to end visual analysis of global warming using three distinct temperature datasets is presented. A baseline adjusted from the Paris Agreements one point five degrees Celsius benchmark based on data analysis is employed. A closed loop design from visualization to prediction and clustering is created using classic models tailored to the characteristics of the data. This approach reduces complexity and eliminates the need for advanced feature engineering. A lightweight convolutional neural network and long short term memory model specifically designed for global temperature change is proposed achieving exceptional accuracy in long term forecasting with a mean squared error of three times ten to the power of negative six and an R squared value of zero point nine nine nine nine. Dynamic time warping and KMeans clustering elucidate national level temperature anomalies and carbon emission patterns. This comprehensive method reveals intricate spatiotemporal characteristics of global temperature variations and provides warming trend attribution. The findings offer new insights into climate change dynamics demonstrating that simplicity and precision can coexist in environmental analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16311v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meihua Zhou, Nan Wan, Tianlong Zheng, Hanwen Xu, Li Yang, Tingting Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial Intelligence for Education Using Topic Modeling</title>
      <link>https://arxiv.org/abs/2409.16376</link>
      <description>arXiv:2409.16376v1 Announce Type: cross 
Abstract: Generative artificial intelligence (GenAI) can reshape education and learning. While large language models (LLMs) like ChatGPT dominate current educational research, multimodal capabilities, such as text-to-speech and text-to-image, are less explored. This study uses topic modeling to map the research landscape of multimodal and generative AI in education. An extensive literature search using Dimensions.ai yielded 4175 articles. Employing a topic modeling approach, latent topics were extracted, resulting in 38 interpretable topics organized into 14 thematic areas. Findings indicate a predominant focus on text-to-text models in educational contexts, with other modalities underexplored, overlooking the broader potential of multimodal approaches. The results suggest a research gap, stressing the importance of more balanced attention across different AI modalities and educational levels. In summary, this research provides an overview of current trends in generative AI for education, underlining opportunities for future exploration of multimodal technologies to fully realize the transformative potential of artificial intelligence in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16376v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ville Heilala, Roberto Araya, Raija H\"am\"al\"ainen</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions</title>
      <link>https://arxiv.org/abs/2409.16430</link>
      <description>arXiv:2409.16430v1 Announce Type: cross 
Abstract: Large Language Models(LLMs) have revolutionized various applications in natural language processing (NLP) by providing unprecedented text generation, translation, and comprehension capabilities. However, their widespread deployment has brought to light significant concerns regarding biases embedded within these models. This paper presents a comprehensive survey of biases in LLMs, aiming to provide an extensive review of the types, sources, impacts, and mitigation strategies related to these biases. We systematically categorize biases into several dimensions. Our survey synthesizes current research findings and discusses the implications of biases in real-world applications. Additionally, we critically assess existing bias mitigation techniques and propose future research directions to enhance fairness and equity in LLMs. This survey serves as a foundational resource for researchers, practitioners, and policymakers concerned with addressing and understanding biases in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16430v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh</dc:creator>
    </item>
    <item>
      <title>Spacewalker: Traversing Representation Spaces for Fast Interactive Exploration and Annotation of Unstructured Data</title>
      <link>https://arxiv.org/abs/2409.16793</link>
      <description>arXiv:2409.16793v1 Announce Type: cross 
Abstract: Unstructured data in industries such as healthcare, finance, and manufacturing presents significant challenges for efficient analysis and decision making. Detecting patterns within this data and understanding their impact is critical but complex without the right tools. Traditionally, these tasks relied on the expertise of data analysts or labor-intensive manual reviews. In response, we introduce Spacewalker, an interactive tool designed to explore and annotate data across multiple modalities. Spacewalker allows users to extract data representations and visualize them in low-dimensional spaces, enabling the detection of semantic similarities. Through extensive user studies, we assess Spacewalker's effectiveness in data annotation and integrity verification. Results show that the tool's ability to traverse latent spaces and perform multi-modal queries significantly enhances the user's capacity to quickly identify relevant data. Moreover, Spacewalker allows for annotation speed-ups far superior to conventional methods, making it a promising tool for efficiently navigating unstructured data and improving decision making processes. The code of this work is open-source and can be found at: https://github.com/code-lukas/Spacewalker</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16793v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Heine, Fabian H\"orst, Jana Fragemann, Gijs Luijten, Miriam Balzer, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold</dc:creator>
    </item>
    <item>
      <title>Robotic Backchanneling in Online Conversation Facilitation: A Cross-Generational Study</title>
      <link>https://arxiv.org/abs/2409.16899</link>
      <description>arXiv:2409.16899v1 Announce Type: cross 
Abstract: Japan faces many challenges related to its aging society, including increasing rates of cognitive decline in the population and a shortage of caregivers. Efforts have begun to explore solutions using artificial intelligence (AI), especially socially embodied intelligent agents and robots that can communicate with people. Yet, there has been little research on the compatibility of these agents with older adults in various everyday situations. To this end, we conducted a user study to evaluate a robot that functions as a facilitator for a group conversation protocol designed to prevent cognitive decline. We modified the robot to use backchannelling, a natural human way of speaking, to increase receptiveness of the robot and enjoyment of the group conversation experience. We conducted a cross-generational study with young adults and older adults. Qualitative analyses indicated that younger adults perceived the backchannelling version of the robot as kinder, more trustworthy, and more acceptable than the non-backchannelling robot. Finally, we found that the robot's backchannelling elicited nonverbal backchanneling in older participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16899v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/RO-MAN57019.2023.10309362</arxiv:DOI>
      <dc:creator>Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara, Mihoko Otake-Mastuura</dc:creator>
    </item>
    <item>
      <title>A Roadmap for Embodied and Social Grounding in LLMs</title>
      <link>https://arxiv.org/abs/2409.16900</link>
      <description>arXiv:2409.16900v1 Announce Type: cross 
Abstract: The fusion of Large Language Models (LLMs) and robotic systems has led to a transformative paradigm in the robotic field, offering unparalleled capabilities not only in the communication domain but also in skills like multimodal input handling, high-level reasoning, and plan generation. The grounding of LLMs knowledge into the empirical world has been considered a crucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless, connecting LLMs' representations to the external world with multimodal approaches or with robots' bodies is not enough to let them understand the meaning of the language they are manipulating. Taking inspiration from humans, this work draws attention to three necessary elements for an agent to grasp and experience the world. The roadmap for LLMs grounding is envisaged in an active bodily system as the reference point for experiencing the environment, a temporally structured experience for a coherent, self-related interaction with the external world, and social skills to acquire a common-grounded shared experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16900v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Incao, Carlo Mazzola, Giulia Belgiovine, Alessandra Sciutti</dc:creator>
    </item>
    <item>
      <title>Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models</title>
      <link>https://arxiv.org/abs/2409.16920</link>
      <description>arXiv:2409.16920v1 Announce Type: cross 
Abstract: Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16920v1</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li</dc:creator>
    </item>
    <item>
      <title>AI-assisted Gaze Detection for Proctoring Online Exams</title>
      <link>https://arxiv.org/abs/2409.16923</link>
      <description>arXiv:2409.16923v1 Announce Type: cross 
Abstract: For high-stakes online exams, it is important to detect potential rule violations to ensure the security of the test. In this study, we investigate the task of detecting whether test takers are looking away from the screen, as such behavior could be an indication that the test taker is consulting external resources. For asynchronous proctoring, the exam videos are recorded and reviewed by the proctors. However, when the length of the exam is long, it could be tedious for proctors to watch entire exam videos to determine the exact moments when test takers look away. We present an AI-assisted gaze detection system, which allows proctors to navigate between different video frames and discover video frames where the test taker is looking in similar directions. The system enables proctors to work more effectively to identify suspicious moments in videos. An evaluation framework is proposed to evaluate the system against human-only and ML-only proctoring, and a user study is conducted to gather feedback from proctors, aiming to demonstrate the effectiveness of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16923v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong-Siang Shih, Zach Zhao, Chenhao Niu, Bruce Iberg, James Sharpnack, Mirza Basim Baig</dc:creator>
    </item>
    <item>
      <title>SmartState: An Automated Research Protocol Adherence System</title>
      <link>https://arxiv.org/abs/2305.04411</link>
      <description>arXiv:2305.04411v5 Announce Type: replace 
Abstract: Developing and enforcing study protocols is crucial in medical research, especially as interactions with participants become more intricate. Traditional rules-based systems struggle to provide the automation and flexibility required for real-time, personalized data collection. We introduce SmartState, a state-based system designed to act as a personal agent for each participant, continuously managing and tracking their unique interactions. Unlike traditional reporting systems, SmartState enables real-time, automated data collection with minimal oversight. By integrating large language models to distill conversations into structured data, SmartState reduces errors and safeguards data integrity through built-in protocol and participant auditing. We demonstrate its utility in research trials involving time-dependent participant interactions, addressing the increasing need for reliable automation in complex clinical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04411v5</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel E. Armstrong (University of Kentucky), Mitchell A. Klusty (University of Kentucky), Aaron D. Mullen (University of Kentucky), Jeffery C. Talbert (University of Kentucky), V. K. Cody Bumgardner (University of Kentucky)</dc:creator>
    </item>
    <item>
      <title>Exploring Internet of Things Adoption Challenges in Manufacturing Firms: A Delphi Fuzzy Analytical Hierarchy Process Approach</title>
      <link>https://arxiv.org/abs/2309.12350</link>
      <description>arXiv:2309.12350v4 Announce Type: replace 
Abstract: Innovation is crucial for sustainable success in today's fiercely competitive global manufacturing landscape. Bangladesh's manufacturing sector must embrace transformative technologies like the Internet of Things (IoT) to thrive in this environment. This article addresses the vital task of identifying and evaluating barriers to IoT adoption in Bangladesh's manufacturing industry. Through synthesizing expert insights and carefully reviewing contemporary literature, we explore the intricate landscape of IoT adoption challenges. Our methodology combines the Delphi and Fuzzy Analytical Hierarchy Process, systematically analyzing and prioritizing these challenges. This approach harnesses expert knowledge and uses fuzzy logic to handle uncertainties. Our findings highlight key obstacles, with "Lack of top management commitment to new technology" (B10), "High initial implementation costs" (B9), and "Risks in adopting a new business model" (B7) standing out as significant challenges that demand immediate attention. These insights extend beyond academia, offering practical guidance to industry leaders. With the knowledge gained from this study, managers can develop tailored strategies, set informed priorities, and embark on a transformative journey toward leveraging IoT's potential in Bangladesh's industrial sector. This article provides a comprehensive understanding of IoT adoption challenges and equips industry leaders to navigate them effectively. This strategic navigation, in turn, enhances the competitiveness and sustainability of Bangladesh's manufacturing sector in the IoT era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12350v4</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Shahriar, Md. Saiful Islam, Md Abrar Jahin, Istiyaque Ahmed Ridoy, Raihan Rafi Prottoy, Adiba Abid, M. F. Mridha</dc:creator>
    </item>
    <item>
      <title>Evaluating Usability and Engagement of Large Language Models in Virtual Reality for Traditional Scottish Curling</title>
      <link>https://arxiv.org/abs/2408.09285</link>
      <description>arXiv:2408.09285v2 Announce Type: replace 
Abstract: This paper explores the innovative application of Large Language Models (LLMs) in Virtual Reality (VR) environments to promote heritage education, focusing on traditional Scottish curling presented in the game ``Scottish Bonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with pre-defined scripted chatbots, evaluating key criteria such as usability, user engagement, and learning outcomes. The results show that LLM-based chatbots significantly improve interactivity and engagement, creating a more dynamic and immersive learning environment. This integration helps document and preserve cultural heritage and enhances dissemination processes, which are crucial for safeguarding intangible cultural heritage (ICH) amid environmental changes. Furthermore, the study highlights the potential of novel technologies in education to provide immersive experiences that foster a deeper appreciation of cultural heritage. These findings support the wider application of LLMs and VR in cultural education to address global challenges and promote sustainable practices to preserve and enhance cultural heritage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09285v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ka Hei Carrie Lau, Efe Bozkir, Hong Gao, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Misty: UI Prototyping Through Interactive Conceptual Blending</title>
      <link>https://arxiv.org/abs/2409.13900</link>
      <description>arXiv:2409.13900v2 Announce Type: replace 
Abstract: UI prototyping often involves iterating and blending elements from examples such as screenshots and sketches, but current tools offer limited support for incorporating these examples. Inspired by the cognitive process of conceptual blending, we introduce a novel UI workflow that allows developers to rapidly incorporate diverse aspects from design examples into work-in-progress UIs. We prototyped this workflow as Misty. Through an exploratory first-use study with 14 frontend developers, we assessed Misty's effectiveness and gathered feedback on this workflow. Our findings suggest that Misty's conceptual blending workflow helps developers kickstart creative explorations, flexibly specify intent in different stages of prototyping, and inspires developers through serendipitous UI blends. Misty demonstrates the potential for tools that blur the boundaries between developers and designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13900v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuwen Lu, Alan Leung, Amanda Swearngin, Jeffrey Nichols, Titus Barik</dc:creator>
    </item>
    <item>
      <title>An Alternative to Multi-Factor Authentication with a Triple-Identity Authentication Scheme</title>
      <link>https://arxiv.org/abs/2407.19459</link>
      <description>arXiv:2407.19459v4 Announce Type: replace-cross 
Abstract: The existing authentication system has two entry points (i.e., username and password fields) to interact with the outside, but neither of them has a gatekeeper, making the system vulnerable to cyberattacks. In order to ensure the authentication security, the system sets a third entry point and use an external MFA service to guard it. The crux of the problem is that the system has no internal mechanism to guard its own entry points as no identifiers can be defined for the username and password without using any personal information. To solve this problem, we open the hash algorithm of a dual-password login-authentication system to three login credentials. Therefore, the intermediate elements of the algorithm can be used to define an identifier to verify the user identity at each entry point of the system. As a result of the above setup, a triple-identity authentication is established, the key of which is that the readily available user's login name and password are randomly converted into a matrix of meaningless hash elements which are concealed, incommunicable, inaccessible, and independent of personal information. So the identifiers defined using such elements can be used by the system to verify the identities of the user at all the entry points of the system, thereby ensuring the authentication security without relying on MFA services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19459v4</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyun Borjigin</dc:creator>
    </item>
    <item>
      <title>Can I Pet Your Robot? Incorporating Capacitive Touch Sensing into a Soft Socially Assistive Robot Platform</title>
      <link>https://arxiv.org/abs/2409.12338</link>
      <description>arXiv:2409.12338v2 Announce Type: replace-cross 
Abstract: This work presents a method of incorporating low-cost capacitive tactile sensors on a soft socially assistive robot platform. By embedding conductive thread into the robot's crocheted exterior, we formed a set of low-cost, flexible capacitive tactile sensors that do not disrupt the robot's soft, zoomorphic embodiment. We evaluated the sensors' performance through a user study (N=20) and found that the sensors reliably detected user touch events and localized touch inputs to one of three regions on the robot's exterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12338v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amy O'Connell, Bailey Cislowski, Heather Culbertson, Maja Matari\'c</dc:creator>
    </item>
    <item>
      <title>Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</title>
      <link>https://arxiv.org/abs/2409.14509</link>
      <description>arXiv:2409.14509v3 Announce Type: replace-cross 
Abstract: LLM-based applications are helping people write, and LLM-generated text is making its way into social media, journalism, and our classrooms. However, the differences between LLM-generated and human-written text remain unclear. To explore this, we hired professional writers to edit paragraphs in several creative domains. We first found these writers agree on undesirable idiosyncrasies in LLM-generated text, formalizing it into a seven-category taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP corpus: 1,057 LLM-generated paragraphs edited by professional writers according to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms of writing quality, revealing common limitations across model families. Third, we explored automatic editing methods to improve LLM-generated text. A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14509v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</dc:creator>
    </item>
  </channel>
</rss>
