<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 May 2025 04:03:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation</title>
      <link>https://arxiv.org/abs/2505.13919</link>
      <description>arXiv:2505.13919v1 Announce Type: new 
Abstract: Data-driven methods offer an effective equation-free solution for predicting physical dynamics. However, the same physical system can exhibit significantly different dynamic behaviors in various environments. This causes prediction functions trained for specific environments to fail when transferred to unseen environments. Therefore, cross-environment prediction requires modeling the dynamic functions of different environments. In this work, we propose a model weight generation method, \texttt{EnvAd-Diff}. \texttt{EnvAd-Diff} operates in the weight space of the dynamic function, generating suitable weights from scratch based on environmental condition for zero-shot prediction. Specifically, we first train expert prediction functions on dynamic trajectories from a limited set of visible environments to create a model zoo, thereby constructing sample pairs of prediction function weights and their corresponding environments. Subsequently, we train a latent space diffusion model conditioned on the environment to model the joint distribution of weights and environments. Considering the lack of environmental prior knowledge in real-world scenarios, we propose a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems demonstrate that a 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13919v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruikun Li, Huandong Wang, Jingtao Ding, Yuan Yuan, Qingmin Liao, Yong Li</dc:creator>
    </item>
    <item>
      <title>PLUTUS Open Source -- Breaking Barriers in Algorithmic Trading</title>
      <link>https://arxiv.org/abs/2505.14050</link>
      <description>arXiv:2505.14050v1 Announce Type: new 
Abstract: Algorithmic trading has long been an opaque, fragmented domain, guarded by secrecy and built around proprietary systems. In contrast to the open, collaborative evolution in fields like machine learning or software engineering, the algorithmic trading ecosystem has been slow to adopt reproducibility, standardization, and shared infrastructure. This paper introduces PLUTUS Open Source, an initiative sponsored by ALGOTRADE to reshape this landscape through openness, structure, and collaboration. PLUTUS combines a reproducibility standard, a modular development framework, and a growing suite of community-built reference strategies. The project provides a systematic approach to designing, testing, and documenting trading algorithms, regardless of the user's technical or financial background. We outline the motivation behind the initiative, present its foundational structure, and showcase working examples that adhere to the PLUTUS standard. We also invite the broader research and trading communities to contribute, iterate, and help build a transparent and inclusive future for algorithmic trading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14050v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An-Dan Nguyen, Quang-Khoi Ta, Duy-Anh Vo</dc:creator>
    </item>
    <item>
      <title>Higher-order, mixed-hybrid finite elements for Kirchhoff-Love shells</title>
      <link>https://arxiv.org/abs/2505.14115</link>
      <description>arXiv:2505.14115v1 Announce Type: new 
Abstract: A novel mixed-hybrid method for Kirchhoff-Love shells is proposed that enables the use of classical, possibly higher-order Lagrange elements in numerical analyses. In contrast to purely displacement-based formulations that require higher continuity of shape functions as in IGA, the mixed formulation features displacements and moments as primary unknowns. Thereby the continuity requirements are reduced, allowing equal-order interpolations of the displacements and moments. Hybridization enables an element-wise static condensation of the degrees of freedom related to the moments, at the price of introducing (significantly less) rotational degrees of freedom acting as Lagrange multipliers to weakly enforce the continuity of tangential moments along element edges. The mixed model is formulated coordinate-free based on the Tangential Differential Calculus, making it applicable for explicitly and implicitly defined shell geometries. All mechanically relevant boundary conditions are considered. Numerical results confirm optimal higher-order convergence rates whenever the mechanical setup allows for sufficiently smooth solutions; new benchmark test cases of this type are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14115v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonas Neumeyer, Michael Wolfgang Kaiser, Thomas-Peter Fries</dc:creator>
    </item>
    <item>
      <title>Global Maxwell Tomography Using the Volume-Surface Integral Equation for Improved Estimation of Electrical Properties</title>
      <link>https://arxiv.org/abs/2505.14546</link>
      <description>arXiv:2505.14546v1 Announce Type: new 
Abstract: Objective: Global Maxwell Tomography (GMT) is a noninvasive inverse optimization method for the estimation of electrical properties (EP) from magnetic resonance (MR) measurements. GMT uses the volume integral equation (VIE) in the forward problem and assumes that the sample has negligible effect on the coil currents. Consequently, GMT calculates the coil's incident fields with an initial EP distribution and keeps them constant for all optimization iterations. This can lead to erroneous reconstructions. This work introduces a novel version of GMT that replaces VIE with the volume-surface integral equation (VSIE), which recalculates the coil currents at every iteration based on updated EP estimates before computing the associated fields. Methods: We simulated an 8-channel transceiver coil array for 7 T brain imaging and reconstructed the EP of a realistic head model using VSIE-based GMT. We built the coil, collected experimental MR measurements, and reconstructed EP of a two-compartment phantom. Results: In simulations, VSIE-based GMT outperformed VIE-based GMT by at least 12% for both EP. In experiments, the relative difference with respect to probe-measured EP values in the inner (outer) compartment was 13% (26%) and 17% (33%) for the permittivity and conductivity, respectively. Conclusion: The use of VSIE over VIE enhances GMT's performance by accounting for the effect of the EP on the coil currents. Significance: VSIE-based GMT does not rely on an initial EP estimate, rendering it more suitable for experimental reconstructions compared to the VIE-based GMT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14546v1</guid>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilias Giannakopoulos, Jos\'e E. Cruz Serrall\'es, Jan Pa\v{s}ka, Martijn A. Cloos, Ryan Brown, Riccardo Lattanzi</dc:creator>
    </item>
    <item>
      <title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
      <link>https://arxiv.org/abs/2505.13469</link>
      <description>arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13469v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aayam Bansal, Harsh Vardhan Narsaria</dc:creator>
    </item>
    <item>
      <title>Improved Methods for Model Pruning and Knowledge Distillation</title>
      <link>https://arxiv.org/abs/2505.14052</link>
      <description>arXiv:2505.14052v1 Announce Type: cross 
Abstract: Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14052v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Jiang, Anying Fu, Youling Zhang</dc:creator>
    </item>
    <item>
      <title>Design and Evaluation of a Microservices Cloud Framework for Online Travel Platforms</title>
      <link>https://arxiv.org/abs/2505.14508</link>
      <description>arXiv:2505.14508v1 Announce Type: cross 
Abstract: Handling online travel agents globally requires efficient and flexible software solution architectures. When it needs to handle thousands of agents and billions of clients data globally. Microservices architecture is used to break down a large program into numerous, smaller services which can run individually and perform individual tasks. This paper analyses and integrates a unique Microservices Cloud Framework designed to support Online Travel Platforms (MCF-OTP). MCF-OTPs main goal is to increase the performance, flexibility, and maintenance of online travel platforms via cloud computing and microservice technologies. Large-scale travel apps, including managing numerous data sources, dealing with traffic peaks, and providing fault tolerance, can be addressed by the suggested framework. The framework increases good interpretation between flawless data synchronization, microservices, and dynamic scaling based on demand technology. An organization framework that optimizes service borders and minimizes inter-service dependencies is recommended. Thus, this can result in elevated development adaptability. In this research, the principal goal is to evaluate MCF-OTPs efficiency using the indicators of fault tolerance and response time. It is indicated by the findings that the MCF-OTP structure excels traditional monolithic designs in terms of dependability and scalability, managing traffic spikes seamlessly and decreasing downtime. The cost-effective analysis helps ascertain the net gain attained by the startup fees and the ongoing operational costs. The cloud-based environment is used to reduce the fracture cost which also helps to increase the efficiency of resource allocation, according to the research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14508v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions</title>
      <link>https://arxiv.org/abs/2410.03779</link>
      <description>arXiv:2410.03779v2 Announce Type: replace-cross 
Abstract: Graph neural networks have been a powerful tool for mesh-based physical simulation. To efficiently model large-scale systems, existing methods mainly employ hierarchical graph structures to capture multi-scale node relations. However, these graph hierarchies are typically manually designed and fixed, limiting their ability to adapt to the evolving dynamics of complex physical systems. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. EvoMesh introduces anisotropic message passing, which enables direction-specific aggregation of dynamic features between nodes within each hierarchy, while simultaneously learning node selection probabilities for the next hierarchical level based on physical context. This design creates more flexible message shortcuts and enhances the model's capacity to capture long-range dependencies. Extensive experiments on five benchmark physical simulation datasets show that EvoMesh outperforms recent fixed-hierarchy message passing networks by large margins. Code is available at https://github.com/hbell99/EvoMesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03779v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huayu Deng, Xiangming Zhu, Yunbo Wang, Xiaokang Yang</dc:creator>
    </item>
    <item>
      <title>SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</title>
      <link>https://arxiv.org/abs/2410.19236</link>
      <description>arXiv:2410.19236v3 Announce Type: replace-cross 
Abstract: The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19236v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Darin Tsui, Aryan Musharaf, Yigit Efe Erginbas, Justin Singh Kang, Amirali Aghazadeh</dc:creator>
    </item>
    <item>
      <title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
      <link>https://arxiv.org/abs/2505.07078</link>
      <description>arXiv:2505.07078v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07078v2</guid>
      <category>q-fin.TR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma</dc:creator>
    </item>
    <item>
      <title>Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles</title>
      <link>https://arxiv.org/abs/2505.08782</link>
      <description>arXiv:2505.08782v2 Announce Type: replace-cross 
Abstract: Practical Quantum Machine Learning (QML) is challenged by noise, limited scalability, and poor trainability in Variational Quantum Circuits (VQCs) on current hardware. We propose a multi-chip ensemble VQC framework that systematically overcomes these hurdles. By partitioning high-dimensional computations across ensembles of smaller, independently operating quantum chips and leveraging controlled inter-chip entanglement boundaries, our approach demonstrably mitigates barren plateaus, enhances generalization, and uniquely reduces both quantum error bias and variance simultaneously without additional mitigation overhead. This allows for robust processing of large-scale data, as validated on standard benchmarks (MNIST, FashionMNIST, CIFAR-10) and a real-world PhysioNet EEG dataset, aligning with emerging modular quantum hardware and paving the way for more scalable QML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08782v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junghoon Justin Park, Jiook Cha, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Shinjae Yoo</dc:creator>
    </item>
    <item>
      <title>CASL-HJX: A Comprehensive Guide to Solving Deterministic and Stochastic Hamilton-Jacobi Equations</title>
      <link>https://arxiv.org/abs/2505.11527</link>
      <description>arXiv:2505.11527v2 Announce Type: replace-cross 
Abstract: CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It provides a flexible and efficient approach to modeling front propagation problems, optimal control problems, and stochastic Hamilton-Jacobi Bellman equations. The framework integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms, ensuring convergence to viscosity solutions while achieving global rather than local optimization. Built with a high-performance C++ core, CASL-HJX efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications across multiple domains. We demonstrate the solver's versatility through tutorial examples covering various PDEs and through applications in neuroscience, where it enables the design of energy-efficient controllers for regulating neural populations to mitigate pathological synchrony. While our examples focus on these applications, the mathematical foundation of the solver makes it applicable to problems in finance, engineering, and machine learning. The modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. CASL-HJX bridges the gap between deterministic control methods and stochastic models, providing a robust tool for managing uncertainty in complex dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11527v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faranak Rajabi, Jacob Fingerman, Andrew Wang, Jeff Moehlis, Frederic Gibou</dc:creator>
    </item>
    <item>
      <title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
      <link>https://arxiv.org/abs/2505.12638</link>
      <description>arXiv:2505.12638v2 Announce Type: replace-cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12638v2</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifeng Jiao, Yuchen Liu, Yu Zhang, Xin Guo, Yushuai Wu, Chen Jiang, Jiyang Li, Hongwei Zhang, Limei Han, Xin Gao, Yuan Qi, Yuan Cheng</dc:creator>
    </item>
  </channel>
</rss>
