<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Jan 2026 02:38:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AI Agents Need Memory Control Over More Context</title>
      <link>https://arxiv.org/abs/2601.11653</link>
      <description>arXiv:2601.11653v1 Announce Type: new 
Abstract: AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11653v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fouad Bousetouane</dc:creator>
    </item>
    <item>
      <title>A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data</title>
      <link>https://arxiv.org/abs/2601.12053</link>
      <description>arXiv:2601.12053v1 Announce Type: new 
Abstract: While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12053v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ma\"el Donoso</dc:creator>
    </item>
    <item>
      <title>Automated Place Preference Paradigm for Optogenetic Stimulation of the Pedunculopontine Nucleus Reveals Motor Arrest-Linked Preference Behavior</title>
      <link>https://arxiv.org/abs/2601.12054</link>
      <description>arXiv:2601.12054v2 Announce Type: new 
Abstract: Understanding how the brain integrates motor suppression with motivational processes remains a fundamental question in neuroscience. The rostral Pedunculopontine nucleus, a brainstem structure involved in motor control, has been shown to induce transient motor arrest upon optogenetic or electrical stimulation. However, our current understanding of its potential role in linking motor suppression with motivational or reinforcement-related processes is still insufficient. To further explore the effects induced by PPN stimulations and infer the potential mechanism underlying its role involved in both motor and emotional regulation, we developed a fully automated, low-cost system combining real-time animal tracking with closed-loop optogenetic stimulation, using the OpenMV Cam H7 Plus and embedded neural network models. The system autonomously detects the rat's position and triggers optical stimulation upon entry into a predefined region of interest, enabling unbiased, unsupervised behavioral assays. Optogenetic activation of CaMKIIa-expressing neurons in the rostral PPN reliably induced transient motor arrest. When motor arrest was spatially paired with a defined region of interest, rats developed a robust place preference after limited training. These results suggest that rostral PPN activation can couple motor inhibition with reinforcement-related behavioral circuitry. Together, our work provides both a technical framework for scalable closed-loop neuroscience experiments and preliminary evidence that the rostral PPN may participate in coordinating motor suppression with motivational processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12054v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guanghui Li, Xingfei Hou, Zhenxiang Zhao</dc:creator>
    </item>
    <item>
      <title>Modeling Dynamic Computations in the Primate Ventral Visual Stream</title>
      <link>https://arxiv.org/abs/2601.12258</link>
      <description>arXiv:2601.12258v1 Announce Type: new 
Abstract: A major goal of computational neuroscience has been to explain how the primate ventral visual stream (VVS) transforms visual input into temporally evolving neural representations that support robust visual perception. Historically, most modeling efforts have assumed static conditions: monkeys fixate a dot, images are briefly flashed, and neural responses are analyzed through time-averaged metrics. Feedforward deep networks trained on static object recognition tasks outperform prior work in approximating these static snapshot-driven VVS responses. However, mounting neurophysiological evidence demonstrates that VVS responses are rich dynamical signals shaped not only by the retinal input but also by intrinsic circuit dynamics, recurrent interactions, and widespread top-down modulation. Moreover, real-world vision is inherently dynamic: objects move, the observer moves, and the eyes actively sample the environment. Here, we review recent progress in modeling dynamic responses in the macaque ventral stream across three domains: (1) intrinsic dynamics elicited by static images, (2) dynamics evoked by dynamic visual stimuli, and (3) dynamics generated by active sensing during eye movements. We argue that accurately modeling VVS dynamics will require representational, circuit-level, and behavioral perspectives, including multi-area recurrence, structured E/I interactions, and temporal objectives that better reflect natural behavior. We outline some key missing ingredients and propose a roadmap toward dynamic, multi-timescale models of the primate VVS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12258v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Dunnhofer, Maren Wehrheim, Hamidreza Ramezanpour, Sabine Muzellec, Kohitij Kar</dc:creator>
    </item>
    <item>
      <title>If Grid Cells are the Answer, What is the Question? A Review of Normative Grid Cell Theory</title>
      <link>https://arxiv.org/abs/2601.12424</link>
      <description>arXiv:2601.12424v1 Announce Type: new 
Abstract: For 20 years the beautiful structure in the grid cell code has presented an attractive puzzle: what computation do these representations subserve, and why does it manifest so curiously in neurons. The first question quickly attracted an answer: grid cells subserve path-integration, the ability to keep track of one's position as you move about the world. Subsequent work has only solidified this link: bottom-up mechanistic models that perform path-integration match the measured neural responses, while experimental perturbations that selectively disrupt grid cell activity impair performance on path-integration dependent tasks. A more controversial area of work has been top-down normative modelling: why has the brain chosen to compute like this? Floods of ink have been spilt attempting to build a precise link between the population's objective and the measured implementation. The holy grail is a normative link with broad predictive power which generalises to other neural systems. We review this literature and argue that, despite some controversies, the literature largely agrees that grid cells can be explained as a (1) biologically plausible (2) high fidelity, non-linearly decodable code for position that (3) subserves path-integration. As a rare area of neuroscience with mature theoretical and experimental work, this story holds lessons for normative theories of neural computations, and on the risks and rewards of integrating task-optimised neural networks into such theorising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12424v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Dorrell, James C. R. Whittington</dc:creator>
    </item>
    <item>
      <title>Primate-like perceptual decision making emerges through deep recurrent reinforcement learning</title>
      <link>https://arxiv.org/abs/2601.12577</link>
      <description>arXiv:2601.12577v1 Announce Type: new 
Abstract: Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12577v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman</dc:creator>
    </item>
    <item>
      <title>Cognition spaces: natural, artificial, and hybrid</title>
      <link>https://arxiv.org/abs/2601.12837</link>
      <description>arXiv:2601.12837v1 Announce Type: new 
Abstract: Cognitive processes are realized across an extraordinary range of natural, artificial, and hybrid systems, yet there is no unified framework for comparing their forms, limits, and unrealized possibilities. Here, we propose a cognition space approach that replaces narrow, substrate-dependent definitions with a comparative representation based on organizational and informational dimensions. Within this framework, cognition is treated as a graded capacity to sense, process, and act upon information, allowing systems as diverse as cells, brains, artificial agents, and human-AI collectives to be analyzed within a common conceptual landscape. We introduce and examine three cognition spaces -- basal aneural, neural, and human-AI hybrid -- and show that their occupation is highly uneven, with clusters of realized systems separated by large unoccupied regions. We argue that these voids are not accidental but reflect evolutionary contingencies, physical constraints, and design limitations. By focusing on the structure of cognition spaces rather than on categorical definitions, this approach clarifies the diversity of existing cognitive systems and highlights hybrid cognition as a promising frontier for exploring novel forms of complexity beyond those produced by biological evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12837v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.NE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricard Sol\'e, Luis F Seoane, Jordi Pla-Mauri, Michael Timothy Bennett, Michael E. Hochberg, Michael Levin</dc:creator>
    </item>
    <item>
      <title>Global stability of a Hebbian/anti-Hebbian network for principal subspace learning</title>
      <link>https://arxiv.org/abs/2601.13170</link>
      <description>arXiv:2601.13170v1 Announce Type: new 
Abstract: Biological neural networks self-organize according to local synaptic modifications to produce stable computations. How modifications at the synaptic level give rise to such computations at the network level remains an open question. Pehlevan et al. [Neur. Comp. 27 (2015), 1461--1495] proposed a model of a self-organizing neural network with Hebbian and anti-Hebbian synaptic updates that implements an algorithm for principal subspace analysis; however, global stability of the nonlinear synaptic dynamics has not been established. Here, for the case that the feedforward and recurrent weights evolve at the same timescale, we prove global stability of the continuum limit of the synaptic dynamics and show that the dynamics evolve in two phases. In the first phase, the synaptic weights converge to an invariant manifold where the `neural filters' are orthonormal. In the second phase, the synaptic dynamics follow the gradient flow of a non-convex potential function whose minima correspond to neural filters that span the principal subspace of the input data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13170v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>math.DS</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Lipshutz, Robert J. Lipshutz</dc:creator>
    </item>
    <item>
      <title>Polyphonic Intelligence: Constraint-Based Emergence, Pluralistic Inference, and Non-Dominating Integration</title>
      <link>https://arxiv.org/abs/2601.13182</link>
      <description>arXiv:2601.13182v1 Announce Type: new 
Abstract: Across neuroscience, artificial intelligence, and related fields, dominant models of intelligence typically privilege convergence: uncertainty is reduced, competing explanations are eliminated, and behaviour is governed by the optimisation of a single objective or policy. While this framing has proved powerful in many settings, it sits uneasily with biological and adaptive systems that maintain redundancy, ambiguity, and parallel explanatory processes over extended timescales. Here we propose an alternative perspective, termed polyphonic intelligence, in which coherent behaviour and meaning emerge from the coordination of multiple semi-independent inferential processes operating under shared constraints. Rather than resolving plurality through dominance or collapse, polyphonic systems sustain multiple explanatory trajectories and integrate them through soft alignment, compatibility relations, and bounded influence. We develop this perspective conceptually and formally, introducing a variational framework in which multiple coordinated approximations are maintained without winner-takes-all selection. This formulation makes explicit how plurality can remain stable, tractable, and productive, and clarifies how polyphonic inference differs from ensemble methods, mixture models, and Bayesian model averaging. Through proof-of-principle examples, we demonstrate that non-dominating, pluralistic inference can be implemented in simple computational systems without requiring centralised control or global convergence. We conclude by discussing implications for neuroscience, psychiatry, and artificial intelligence, and by arguing that intelligence may be more fruitfully understood as coordination without command rather than as the elimination of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13182v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander D Shaw</dc:creator>
    </item>
    <item>
      <title>Multifaceted neural representation of words in naturalistic language</title>
      <link>https://arxiv.org/abs/2601.13297</link>
      <description>arXiv:2601.13297v1 Announce Type: new 
Abstract: Understanding how the brain represents the multifaceted properties of words in context is essential for explaining the neural architecture of human language. Here, we combine large-scale psycholinguistic modeling with naturalistic fMRI to uncover the latent structure of word properties and their neural representations during narrative comprehension. By analyzing 106 psycholinguistic variables across 13,850 English words, we identified eight interpretable latent dimensions spanning lexical usage, word form, phonology orthography mapping, sublexical regularity, and semantic organization. These factors robustly predicted behavioral performance across lexical decision, naming, recognition, and semantic judgment tasks, demonstrating their cognitive relevance. Parcel-based and multivariate fMRI analyses of narrative listening revealed that these latent dimensions are encoded in overlapping yet functionally differentiated cortical systems. Multidimensional scaling and hierarchical clustering analyses further identified four interacting subsystems supporting sensorimotor grounding, controlled semantic retrieval, resolution of lexical competition, and contextual episodic integration. Together, these findings provide a unified neurocognitive framework linking fundamental lexical psycholinguistic dimensions to distributed cortical systems engaged during naturalistic language comprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13297v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuan Yang, Chuanji Gao, Cheng Xiao, Nicholas Riccardi, Rutvik H. Desai</dc:creator>
    </item>
    <item>
      <title>Audio Outperforms Text for Visual Decoding</title>
      <link>https://arxiv.org/abs/2601.13866</link>
      <description>arXiv:2601.13866v1 Announce Type: new 
Abstract: Decoding visual semantic representations from human brain activity is a significant challenge. While recent zero-shot decoding approaches have improved performance by leveraging aligned image-text datasets, they overlook a fundamental aspect of human cognition: semantic understanding is inherently anchored in the auditory modality of speech, not text. To address this, our study introduces the first comparative framework for evaluating auditory versus textual semantic modalities in zero-shot visual neural decoding. We propose a novel brain-visual-auditory multimodal alignment model that directly utilizes auditory representations to encapsulate semantics, serving as a substitute for traditional textual descriptors. Our experimental results demonstrate that the auditory modality not only surpasses the textual modality in decoding accuracy but also achieves higher computational efficiency. These findings indicate that auditory semantic representations are more closely aligned with neural activity patterns during visual processing. This work reveals the critical and previously underestimated role of auditory semantics in decoding visual cognition and provides new insights for developing brain-computer interfaces that are more congruent with natural human cognitive mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13866v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhengdi Zhang, Hao Zhang, Wenjun Xia</dc:creator>
    </item>
    <item>
      <title>MooneyMaker: A Python package to create ambiguous two-tone images</title>
      <link>https://arxiv.org/abs/2601.14077</link>
      <description>arXiv:2601.14077v1 Announce Type: new 
Abstract: Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14077v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lars C. Reining, Thabo Matthies, Luisa Haussner, Rabea Turon, Thomas S. A. Wallis</dc:creator>
    </item>
    <item>
      <title>Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning</title>
      <link>https://arxiv.org/abs/2601.11614</link>
      <description>arXiv:2601.11614v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (&gt;0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%-&gt;83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11614v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Qiu</dc:creator>
    </item>
    <item>
      <title>A First Step for Expansion X-Ray Microscopy: Achieving Contrast in Expanded Tissues Sufficient to Reveal Cell Bodies</title>
      <link>https://arxiv.org/abs/2601.13370</link>
      <description>arXiv:2601.13370v1 Announce Type: cross 
Abstract: Existing methods in nanoscale connectomics are at present too slow to map entire mammalian brains. As an emerging approach, expansion microscopy (ExM) has enormous promise, yet it still suffers from throughput limitations. Mapping the human brain and even mapping nonhuman primate brains therefore remain distant goals. While ExM increases effective resolution linearly, it enlarges tissue volume cubically, which dramatically increases imaging time. As a rapid tomographic technique, X-ray microscopy has potential for drastically speeding up large-volume connectomics. But to the best of my knowledge, no group has so far imaged cellular features within expanded tissue using X-ray microscopy. I herein present an early-stage report featuring the first demonstration of X-ray microscopy reconstruction of cell bodies within expanded tissue. This was achieved by combining a modified enzymatic Unclearing technique with a metallic gold stain and imaging using a laboratory X-ray microscope. I emphasize that a great deal of work remains to develop "expansion X-ray microscopy" (ExXRM) to the point where it can be useful for connectomics since the current iteration of ExXRM only resolves cell bodies and not neurites due to extensive off-target staining. Additionally, the current method must be modified to accommodate for the challenges of synchrotron X-ray microscopy, a vastly speedier approach than laboratory X-ray microscopy. Nonetheless, achieving X-ray contrast in expanded tissues represents a significant first step towards realizing ExXRM as a connectomics imaging modality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13370v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Thrasher Collins</dc:creator>
    </item>
    <item>
      <title>Optimal Calibration of the endpoint-corrected Hilbert Transform</title>
      <link>https://arxiv.org/abs/2601.13962</link>
      <description>arXiv:2601.13962v1 Announce Type: cross 
Abstract: Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13962v1</guid>
      <category>eess.SP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eike Osmers, Dorothea Kolossa</dc:creator>
    </item>
    <item>
      <title>Chaotic Oscillatory Associative Memory</title>
      <link>https://arxiv.org/abs/2401.10922</link>
      <description>arXiv:2401.10922v2 Announce Type: replace 
Abstract: Associative memory models retrieve stored information through content-based addressing, mimicking the neural processes of animal brains. The classical Hopfield network-based models store memories as vectors of discrete values and have good storage capacity but do not consider the role of neuronal synchronization in memory storage and retrieval as observed in brains. This is addressed in phase-oscillator-based models which store memories as time-dependent phase-synchronized states, but suffer from instability and low capacity. The present study addresses these challenges through a novel chaotic oscillator-based associative memory model, by defining a phase relationship in chaotic systems and encoding memory as synchronized states of these phases. The underlying chaos in the network is shown to significantly improve both storage and retrieval and offer insights into the dynamics of memory retrieval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10922v2</guid>
      <category>q-bio.NC</category>
      <category>nlin.CD</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nurani Rajagopal Rohan, V. Srinivasa Chakravarthy, Sayan Gupta</dc:creator>
    </item>
    <item>
      <title>Neural timescales from a computational perspective</title>
      <link>https://arxiv.org/abs/2409.02684</link>
      <description>arXiv:2409.02684v3 Announce Type: replace 
Abstract: Neural activity fluctuates over a wide range of timescales within and across brain areas. Experimental observations suggest that diverse neural timescales reflect information in dynamic environments. However, how timescales are defined and measured from brain recordings vary across the literature. Moreover, these observations do not specify the mechanisms underlying timescale variations, nor whether specific timescales are necessary for neural computation and brain function. Here, we synthesize three directions where computational approaches can distill the broad set of empirical observations into quantitative and testable theories: We review (i) how different data analysis methods quantify timescales across distinct behavioral states and recording modalities, (ii) how biophysical models provide mechanistic explanations for the emergence of diverse timescales, and (iii) how task-performing networks and machine learning models uncover the functional relevance of neural timescales. This integrative computational perspective thus complements experimental investigations, providing a holistic view on how neural timescales reflect the relationship between brain structure, dynamics, and behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02684v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roxana Zeraati, Anna Levina, Jakob H. Macke, Richard Gao</dc:creator>
    </item>
    <item>
      <title>Graceful forgetting: Memory as a process</title>
      <link>https://arxiv.org/abs/2502.11105</link>
      <description>arXiv:2502.11105v5 Announce Type: replace 
Abstract: A rational framework is proposed to explain how we accommodate unbounded sensory input within bounded memory. According to this framework, memory is stored as a statistic-like representation that is repeatedly summarized and compressed to make room for new input. Summarization of sensory input must be rapid; that of abstract trace might be slower and more deliberative, drawing on elaborative processes some of which might occasionally reach consciousness (as in mind-wandering). Short-term sensory traces are summarized as simple statistics organized into structures such as a time series, graph or dictionary, and longer-term abstract traces as more complex statistic-like structures. Summarization at multiple time scales requires an intensive process of memory curation which might account for the high metabolic consumption of the brain at rest. Summarization may be guided by heuristics to help choose which statistics to apply at each step, so that the trace is useful for a wide range of future needs, the objective being to "represent the past" rather than tune for a specific task. However, the choice of statistics (or of heuristics to guide that choice) is a potential target for learning, possibly over long-term scales of development or evolution. The framework is intended as an aid to make sense of our extensive empirical and theoretical knowledge of memory and bring us closer to understanding it in functional and mechanistic terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11105v5</guid>
      <category>q-bio.NC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain de Cheveign\'e</dc:creator>
    </item>
    <item>
      <title>Shared representations in brains and models reveal a two-route cortical organization during scene perception</title>
      <link>https://arxiv.org/abs/2507.13941</link>
      <description>arXiv:2507.13941v2 Announce Type: replace 
Abstract: The brain transforms visual inputs into high-dimensional cortical representations that support diverse cognitive and behavioral goals. Characterizing how this information is organized and routed across the human brain is essential for understanding how we process complex visual scenes. Here, we applied representational similarity analysis to 7T fMRI data collected during natural scene viewing. We quantified representational geometry shared across individuals and compared it to hierarchical features from vision and language neural networks. This analysis revealed two distinct processing routes: a ventromedial pathway specialized for scene layout and environmental context, and a lateral occipitotemporal pathway selective for animate content. Vision models aligned with shared structure in both routes, whereas language models corresponded primarily with the lateral pathway. These findings refine classical visual-stream models by characterizing scene perception as a distributed cortical network with separable representational routes for context and animate content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13941v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Marcos-Manch\'on, Llu\'is Fuentemilla</dc:creator>
    </item>
    <item>
      <title>Resting-State EEG Network Profiles Associated with Creative Engagement and Creative Self-Efficacy</title>
      <link>https://arxiv.org/abs/2510.22364</link>
      <description>arXiv:2510.22364v2 Announce Type: replace 
Abstract: Creativity is a core cognitive capacity underlying innovation and adaptive problem solving, yet how it is represented in the brain's intrinsic functional architecture is not fully understood. While resting-state fMRI studies have identified large-scale network correlates associated with differences in creativity, EEG provides the temporal resolution for examining oscillatory dynamics contributing to intrinsic network organization. We examined whether resting-state EEG connectivity patterns are associated with individual differences across multiple creativity-related measures. Thirty healthy young adults completed a multidimensional creativity battery comprising the Inventory of Creative Activities and Achievements (ICAA), the Divergent Association Task (DAT), the Matchstick Arithmetic Puzzles Task (MAPT) and a Self-rating (SR) of creative ability. Graph-theoretical analyses of alpha-band functional connectivity revealed two participant groups, each with distinct patterns of neural activity: Cluster 1 showed reduced global connectivity with relatively preserved left frontal connectivity and greater network modularity; Cluster 0 exhibited stronger overall connectivity strength, reduced modularity and higher local clustering. Notably, Cluster 1 reported higher self-rated creative ability and more frequent engagement in real-world creative activities. These findings suggest that resting-state EEG connectivity patterns are associated with variation in creative self-efficacy and creative engagement, highlighting characteristic patterns of alpha-band network organization observed at rest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22364v2</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samir Damji, Simrut Kurry, Shazia'Ayn Babul, Joydeep Bhattacharya, Naznin Virji-Babul</dc:creator>
    </item>
    <item>
      <title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
      <link>https://arxiv.org/abs/2512.12802</link>
      <description>arXiv:2512.12802v3 Announce Type: replace 
Abstract: Scientific theories of consciousness should be falsifiable and non-trivial. Recent research has given us formal tools to analyze these requirements of falsifiability and non-triviality for theories of consciousness. Surprisingly, many contemporary theories of consciousness fail to pass this bar, including theories based on causal structure but also (as I demonstrate) theories based on function. Herein, I show these requirements of falsifiability and non-triviality especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any falsifiable and non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12802v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Hoel</dc:creator>
    </item>
    <item>
      <title>Gyral-Sulcal-Net: An Integrated Network Representation of Brain Folding Patterns</title>
      <link>https://arxiv.org/abs/2601.08818</link>
      <description>arXiv:2601.08818v2 Announce Type: replace 
Abstract: Our brain functions as a complex communication network, and studying it from a network perspective offers valuable insights into its organizational principles and links to cognitive functions and brain disorders. However, most current network studies typically use brain regions as nodes, often overlooking the intricate folding patterns of finer-scale anatomical landmarks within these regions. In this study, we introduce a novel approach to integrate the brain's two primary folding patterns - gyri and sulci - into a unified network termed the Gyral-Sulcal-Net (GS-Net), in which three different types of finer-scale landmarks have been successfully identified. We evaluated the proposed GS-Net across multiple datasets, comprising over 1,600 brains, spanning different age groups (from 34 gestational weeks to elderly adults) and cohorts (healthy brains and those with pathological conditions). The experimental results demonstrate that the GS-Net can effectively integrate and represent diverse cortical folding patterns from a network perspective. More importantly, this approach offers a promising way for integrating different folding patterns into a unified anatomical brain network, alongside structural and functional networks, providing a comprehensive framework for studying brain networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08818v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Cao, Tong Chen, Nan Zhao, Minheng Chen, Michael Qu, Zeyu Zhang, Xiao Shi, Xiang Li, Tianming Liu, Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Spot solutions to a neural field equation on oblate spheroids</title>
      <link>https://arxiv.org/abs/2504.16342</link>
      <description>arXiv:2504.16342v3 Announce Type: replace-cross 
Abstract: Understanding the dynamics of excitation patterns in neural fields is an important topic in neuroscience. Neural field equations are mathematical models that describe the excitation dynamics of interacting neurons to perform the theoretical analysis. Although many analyses of neural field equations focus on the effect of neuronal interactions on the flat surface, the geometric constraint of the dynamics is also an attractive topic when modeling organs such as the brain. This paper reports pattern dynamics in a neural field equation defined on spheroids as model curved surfaces. We treat spot solutions as localized patterns and discuss how the geometric properties of the curved surface change their properties. To analyze spot patterns on spheroids with small flattening, we first construct exact stationary spot solutions on the spherical surface and reveal their stability. We then extend the analysis to show the existence and stability of stationary spot solutions in the spheroidal case. One of our theoretical results is the derivation of a stability criterion for stationary spot solutions localized at poles on oblate spheroids. The criterion determines whether a spot solution remains at a pole or moves away. Finally, we conduct numerical simulations to discuss the dynamics of spot solutions with the insight of our theoretical predictions. Our results show that the dynamics of spot solutions depend on the curved surface and the coordination of neural interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16342v3</guid>
      <category>nlin.PS</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cnsns.2025.109172</arxiv:DOI>
      <arxiv:journal_reference>Communications in Nonlinear Science and Numerical Simulation, Volume 152, Part A, January 2026, 109172</arxiv:journal_reference>
      <dc:creator>Hiroshi Ishii, Riku Watanabe</dc:creator>
    </item>
    <item>
      <title>SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</title>
      <link>https://arxiv.org/abs/2512.21881</link>
      <description>arXiv:2512.21881v2 Announce Type: replace-cross 
Abstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21881v2</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen</dc:creator>
    </item>
  </channel>
</rss>
