<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 05:02:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Akaike-type information criterion of SEM for jump-diffusion processes based on high-frequency data</title>
      <link>https://arxiv.org/abs/2511.14333</link>
      <description>arXiv:2511.14333v1 Announce Type: new 
Abstract: Structural equation modeling (SEM) is a statistical method used to investigate relationships among latent variables. In SEM, the model must be specified in advance. However, in practice, statisticians often have several candidate models and need to select the most appropriate one. Consequently, model selection is a key issue in SEM, and information criteria are commonly used to address this issue. In this study, we develop an Akaike-type information criterion of SEM for jump-diffusion processes, which enables model selection for SEM based on high-frequency data with jumps. Simulation studies are conducted to illustrate the finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14333v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kusano, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>Asymptotic Distribution of Bounded Shape Constrained Lasso-Type Estimator for Graph-Structured Signals and Discrete Distributions</title>
      <link>https://arxiv.org/abs/2511.14354</link>
      <description>arXiv:2511.14354v1 Announce Type: new 
Abstract: This paper is dedicated to the asymptotic distribution of bounded shape constrained lasso-type estimator. We obtain the limiting distribution of the estimator and study its properties. It is proved that, under certain assumptions on the penalization parameters, the limiting distribution of the estimator is given by the certain constrained estimator applied to the asymptotic distribution of the unrestricted estimator, and, consequently, the estimator preserves the rate of convergence of the underlying estimator. Next, without the fusion penalisation term, the limiting distribution of the estimator is given by the concatenation of the individual nearly-isotonic estimators applied to the specific sub-vectors of the asymptotic distribution of the unrestricted estimator. This behaviour is similar to the case of isotonic regression. The obtained results are also applied to the constrained estimation of a discrete distribution supported on a directed acyclic graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14354v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Pastukhov</dc:creator>
    </item>
    <item>
      <title>Minimax estimation of the structure factor of spatial point processes</title>
      <link>https://arxiv.org/abs/2511.14551</link>
      <description>arXiv:2511.14551v1 Announce Type: new 
Abstract: We investigate the problem of estimating the structure factor, or spectra, of stationary spatial point processes. In the first part, we establish a minimax lower bound for this estimation problem, using an approach tailored to second-order properties of spatial point processes. Although not the main focus, this methodology also extends naturally to a minimax lower bound for the estimation of the pair correlation function of spatial point processes. In the second part, we construct a multitaper estimator that achieves the optimal rate of convergence in squared risk. Under a Brillinger-mixing condition, we further establish a chi-square-type concentration bound. Finally, we propose a data-driven procedure for selecting the number of tapers, supported by an oracle inequality, and we demonstrate the practical effectiveness of the method through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14551v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Mastrilli</dc:creator>
    </item>
    <item>
      <title>Overcoming global sensitivity limitations: using active subspaces to explore discrepancies between global and local parameter sensitivities</title>
      <link>https://arxiv.org/abs/2511.14687</link>
      <description>arXiv:2511.14687v1 Announce Type: new 
Abstract: Global sensitivity metrics are essential tools for assessing parameter importance in complex models, particularly when precise information about parameter values is unavailable. In many cases, such metrics are used to provide parameter rankings that allow for necessary dimension reduction in moderate-to-high dimensional systems. However, globally-derived sensitivity results may obscure localized variability in parameter sensitivities, resulting in misleading conclusions about parameter importance and ensuing consequences for subsequent tasks such as model calibration and surrogate model construction. In this study, we illustrate how discrepancies between globally- and locally-based sensitivity information may arise for an emerging sensitivity metric based on active subspace methodology, as well as for other commonly used sensitivity techniques. In response, we outline a framework that exploits the active subspace to evaluate the stability of parameter sensitivities over the admissible parameter space. This analysis allows one to determine the subregions of the parameter space in which a globally-derived sensitivity metric may be trustworthy. The proposed framework is illustrated on a collection of simple examples for ease of visualization, as well as the highly-applicable Lotka-Volterra model, for which we demonstrate how these issues may be exacerbated in higher dimensions. Our findings suggest that globally-derived sensitivity information should be treated with caution, and that incorporating analysis on local subregions may improve robustness and accuracy in downstream modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14687v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyan Zou, Allison L. Lewis</dc:creator>
    </item>
    <item>
      <title>Pseudo-Poisson Distributions with Nonlinear Conditional Rates</title>
      <link>https://arxiv.org/abs/2511.14741</link>
      <description>arXiv:2511.14741v1 Announce Type: new 
Abstract: Arnold &amp; Manjunath (2021) claim that the bivariate pseudo-Poisson distribution is well suited to bivariate count data with one equidispersed and one overdispersed marginal, owing to its parsimonious structure and straightforward parameter estimation. In the formulation of Leiter &amp; Hamdan (1973), the conditional mean of $X_2$ was specified as a function of $X_1$; Arnold &amp; Manjunath (2021) subsequently augmented this specification by adding an intercept, yielding a linear conditional rate. A direct implication of this construction is that the bivariate pseudo-Poisson distribution can represent only positive correlation between the two variables. This study generalizes the conditional rate to accommodate negatively correlated datasets by introducing curvature. This augmentation provides the additional benefit of allowing the model to behave approximately linear when appropriate, while adequately handling the boundary case $(x_1,x_2)=(0,0)$. According to the Akaike Information Criterion (AIC), the models proposed in this study outperform Arnold &amp; Manjunath (2021)'s linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14741v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood for Random Forests and Ensembles</title>
      <link>https://arxiv.org/abs/2511.13934</link>
      <description>arXiv:2511.13934v1 Announce Type: cross 
Abstract: We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13934v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold D. Chiang, Yukitoshi Matsushita, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Covariate-Dependent Discrete Graphical Models and Dynamic Ising Models</title>
      <link>https://arxiv.org/abs/2511.14123</link>
      <description>arXiv:2511.14123v1 Announce Type: cross 
Abstract: We propose a covariate-dependent discrete graphical model for capturing dynamic networks among discrete random variables, allowing the dependence structure among vertices to vary with covariates. This discrete dynamic network encompasses the dynamic Ising model as a special case. We formulate a likelihood-based approach for parameter estimation and statistical inference. We achieve efficient parameter estimation in high-dimensional settings through the use of the pseudo-likelihood method. To perform model selection, a birth-and-death Markov chain Monte Carlo algorithm is proposed to explore the model space and select the most suitable model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14123v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyndsay Roach, Qiong Li, Nanwei Wang, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Nonparametric Uniform Inference in Binary Classification and Policy Values</title>
      <link>https://arxiv.org/abs/2511.14700</link>
      <description>arXiv:2511.14700v1 Announce Type: cross 
Abstract: We develop methods for nonparametric uniform inference in cost-sensitive binary classification, a framework that encompasses maximum score estimation, predicting utility maximizing actions, and policy learning. These problems are well known for slow convergence rates and non-standard limiting behavior, even under point identified parametric frameworks. In nonparametric settings, they may further suffer from failures of identification. To address these challenges, we introduce a strictly convex surrogate loss that point-identifies a representative nonparametric policy function. We then estimate this surrogate policy to conduct inference on both the optimal classification policy and the optimal policy value. This approach enables Gaussian inference, substantially simplifying empirical implementation relative to working directly with the original classification problem. In particular, we establish root-$n$ asymptotic normality for the optimal policy value and derive a Gaussian approximation for the optimal classification policy at the standard nonparametric rate. Extensive simulation studies corroborate the theoretical findings. We apply our method to the National JTPA Study to conduct inference on the optimal treatment assignment policy and its associated welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14700v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nan Liu andYanbo Liu, Yuya Sasaki, Yuanyuan Wan</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theory for Semiparametric Data Fusion with Individual-Level Data</title>
      <link>https://arxiv.org/abs/2409.09973</link>
      <description>arXiv:2409.09973v3 Announce Type: replace 
Abstract: We address the goal of conducting inference about a smooth finite-dimensional parameter by utilizing individual-level data from various independent sources. Recent advancements have led to the development of a comprehensive theory capable of handling scenarios where different data sources align with, possibly distinct subsets of, conditional distributions of a single factorization of the joint target distribution. While this theory proves effective in many significant contexts, it falls short in certain common data fusion problems, such as two-sample instrumental variable analysis, settings that integrate data from epidemiological studies with diverse designs (e.g., prospective cohorts and retrospective case-control studies), and studies with variables prone to measurement error that are supplemented by validation studies. In this paper, we extend the aforementioned comprehensive theory to allow for the fusion of individual-level data from sources aligned with conditional distributions that do not correspond to a single factorization of the target distribution. Assuming conditional and marginal distribution alignments, we provide universal results that characterize the class of all influence functions of regular asymptotically linear estimators and the efficient influence function of any pathwise differentiable parameter, irrespective of the number of data sources, the specific parameter of interest, or the statistical model for the target distribution. This theory paves the way for machine-learning debiased, semiparametric efficient estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09973v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellen Graham (University of Washington), Marco Carone (University of Washington), Andrea Rotnitzky (University of Washington)</dc:creator>
    </item>
    <item>
      <title>How should we aggregate ratings? Accounting for personal rating scales via Wasserstein barycenters</title>
      <link>https://arxiv.org/abs/2410.00865</link>
      <description>arXiv:2410.00865v2 Announce Type: replace 
Abstract: A common method of comparing items is to collect numerical ratings on a linear scale and compare the average rating for each item. However, averaging ratings does not account for people rating according to differing personal rating scales. With this in mind, we investigate the problem of calculating aggregate numerical ratings from individual numerical ratings and propose a new, non-parametric model for the problem. We show that, with minimal modeling assumptions, the standard average is inconsistent for estimating the quality of items. Analyzing the problem of heterogeneous personal rating scales from the perspective of optimal transport, we derive an alternative rating estimator, which we show is asymptotically consistent almost surely and in L^p for estimating quality, with an optimal rate of convergence. Further, we generalize Kendall's W, a non-parametric coefficient of preference concordance between raters, from the special case of rankings to the more general case of arbitrary numerical ratings. Along the way, we prove Glivenko--Cantelli-type theorems for uniform convergence of the cumulative distribution functions and quantile functions for Wasserstein-2 barycenters on [0,1].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00865v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Raban</dc:creator>
    </item>
    <item>
      <title>Detecting practically significant dependencies in metric spaces via distance correlations</title>
      <link>https://arxiv.org/abs/2411.16177</link>
      <description>arXiv:2411.16177v2 Announce Type: replace 
Abstract: We take a different look at the problem of testing the independence of two metric-space-valued random variables using the distance correlation. Instead of testing if the distance correlation vanishes exactly, we are interested in the hypothesis that it does not exceed a certain threshold. Our testing problem is motivated by the observation that in many cases it is more reasonable to test for a practically significant dependency since it is rare that a hypothesis of perfect independence is exactly satisfied. This point of view also reflects statistical practice, where one often classifies the strength of the association in categories such as `small', `medium' and `large' and the precise definitions depend on the specific application. To address these problems we develop a pivotal test for the hypothesis that the distance correlation between two random variables does not exceed a pre-specified threshold $\Delta$. We also determine a minimum value $\hat \Delta_\alpha$ from the data such that the hypothesis is rejected for all $\Delta \leq \hat \Delta_\alpha$ at controlled type I error $\alpha$. This quantity can be interpreted as a measure of evidence against the hypothesis that the distance correlation is less or equal than $\Delta$. The new test is applicable to processes taking values in separable metric spaces of strong negative type, covering Euclidean as well as functional data. We do not assume independent observations, and instead prove our results for absolutely regular sample generating processes, which includes many time series such as ARMA and GARCH models. Our approach is based on a new functional limit theorem for the sequential distance correlation process, and can also be used to construct confidence intervals for the distance correlation without the need for resampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16177v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Marius Kroll</dc:creator>
    </item>
    <item>
      <title>Gradient descent inference in empirical risk minimization</title>
      <link>https://arxiv.org/abs/2412.09498</link>
      <description>arXiv:2412.09498v3 Announce Type: replace 
Abstract: Gradient descent is one of the most widely used iterative algorithms in modern statistical learning. However, its precise algorithmic dynamics in high-dimensional settings remain only partially understood, which has limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic joint distributional characterization of gradient descent iterates and their debiased statistics in a broad class of empirical risk minimization problems, in the so-called mean-field regime where the sample size is proportional to the signal dimension. Our non-asymptotic state evolution theory holds for both general non-convex loss functions and non-Gaussian data, and reveals the central role of two Onsager correction matrices that precisely characterize the non-trivial dependence among all gradient descent iterates in the mean-field regime.
  Leveraging the joint state evolution characterization, we show that the gradient descent iterate retrieves approximate normality after a debiasing correction via a linear combination of all past iterates, where the debiasing coefficients can be estimated by the proposed gradient descent inference algorithm. This leads to a new algorithmic statistical inference framework based on debiased gradient descent, which (i) applies to a broad class of models with both convex and non-convex losses, (ii) remains valid at each iteration without requiring algorithmic convergence, and (iii) exhibits a certain robustness to possible model misspecification. As a by-product, our framework also provides algorithmic estimates of the generalization error at each iteration. As canonical examples, we demonstrate our theory and inference methods in the single-index regression model and a generalized logistic regression model, where the natural loss functions may exhibit arbitrarily non-convex landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09498v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Xiaocong Xu</dc:creator>
    </item>
    <item>
      <title>Correlation tests and sample spectral coherence matrix in the high-dimensional regime</title>
      <link>https://arxiv.org/abs/2501.04371</link>
      <description>arXiv:2501.04371v2 Announce Type: replace 
Abstract: It is established that the linear spectral statistics (LSS) of the smoothed periodogram estimate of the spectral coherence matrix of a complex Gaussian high-dimensional times series (yn) n$\in$Z with independent components satisfy at each frequency a central limit theorem in the asymptotic regime where the sample size N , the dimension M of the observation, and the smoothing span B both converge towards +$\infty$ in such a way that M = O(N $\alpha$ ) for $\alpha$ &lt; 1 and M B $\rightarrow$ c, c $\in$ (0, 1). It is deduced that two recentered and renormalized versions of the LSS, one based on an average in the frequency domain and the other one based on a sum of squares also in the frequency domain, and both evaluated over a well-chosen frequency grid, also verify a central limit theorem. These two statistics are proposed to test with controlled asymptotic level the hypothesis that the components of y are independent. Numerical simulations assess the performance of the two tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04371v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Electronic Journal of Statistics 2025, 19 (2), pp.5577-5694</arxiv:journal_reference>
      <dc:creator>Philippe Loubaton (LIGM), Alexis Rosuel (LIGM), Pascal Vallet (IMS)</dc:creator>
    </item>
    <item>
      <title>Minimax Analysis of Estimation Problems in Coherent Imaging</title>
      <link>https://arxiv.org/abs/2508.18503</link>
      <description>arXiv:2508.18503v3 Announce Type: replace 
Abstract: Unlike conventional imaging modalities, such as magnetic resonance imaging, which are often well described by a linear regression framework, coherent imaging systems follow a significantly more complex model. In these systems, the task is to estimate the unknown image ${\boldsymbol x}_o \in \mathbb{R}^n$ from observations ${\boldsymbol y}_1, \ldots, {\boldsymbol y}_L \in \mathbb{R}^m$ of the form \[ {\boldsymbol y}_l = A_l X_o {\boldsymbol w}_l + {\boldsymbol z}_l, \quad l = 1, \ldots, L, \] where $X_o = \mathrm{diag}({\boldsymbol x}_o)$ is an $n \times n$ diagonal matrix, ${\boldsymbol w}_1, \ldots, {\boldsymbol w}_L \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,I_n)$ represent speckle noise, and ${\boldsymbol z}_1, \ldots, {\boldsymbol z}_L \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,\sigma_z^2 I_m)$ denote additive noise. The matrices $A_1, \ldots, A_L$ are known forward operators determined by the imaging system.
  The fundamental limits of conventional imaging systems have been extensively studied through sparse linear regression models. However, the limits of coherent imaging systems remain largely unexplored. Our goal is to close this gap by characterizing the minimax risk of estimating ${\boldsymbol x}_o$ in high-dimensional settings.
  Motivated by insights from sparse regression, we observe that the structure of ${\boldsymbol x}_o$ plays a crucial role in determining the estimation error. In this work, we adopt a general notion of structure based on the covering numbers, which is more appropriate for coherent imaging systems. We show that the minimax mean squared error (MSE) scales as \[ \frac{\max\{\sigma_z^4,\, m^2,\, n^2\}\, k \log n}{m^2 n L}, \] where $k$ is a parameter that quantifies the effective complexity of the class of images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18503v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Xing, Soham Jana, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Transfer Learning and Locally Linear Regression for Locally Stationary Time Series</title>
      <link>https://arxiv.org/abs/2511.12948</link>
      <description>arXiv:2511.12948v2 Announce Type: replace 
Abstract: This paper investigates locally linear regression for locally stationary time series and develops theoretical results for locally linear smoothing and transfer learning. Existing analyses have focused on local constant estimators and given samples, leaving the principles of transferring knowledge from auxiliary sources across heterogeneous time-varying domains insufficiently established. We derive uniform convergence for multivariate locally linear estimators under strong mixing. The resulting error expansion decomposes stochastic variation, smoothing bias, and a term induced by local stationarity. This additional term, originating from the locally stationary structure, has smaller order than in the Nadaraya-Watson benchmark, explaining the improved local linear performance. Building on these results, we propose bias-corrected transfer learned estimators that connect a sparsely observed series with densely observed related sources through a smoothly varying bias function defined over rescaled time and covariates. An additional refinement shows how local temporal adjustment of this bias enhances stability and enables efficient information borrowing across domains. Simulation studies and an empirical analysis of international fuel prices support the theoretical predictions and demonstrate the practical advantages of transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12948v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Park</dc:creator>
    </item>
    <item>
      <title>Improved Sample Complexity Bounds for Diffusion Model Training</title>
      <link>https://arxiv.org/abs/2311.13745</link>
      <description>arXiv:2311.13745v4 Announce Type: replace-cross 
Abstract: Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the sample complexity of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an exponential improvement in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13745v4</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Aditya Parulekar, Eric Price, Zhiyang Xun</dc:creator>
    </item>
    <item>
      <title>Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities</title>
      <link>https://arxiv.org/abs/2406.13036</link>
      <description>arXiv:2406.13036v3 Announce Type: replace-cross 
Abstract: Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback--Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar\'e inequality offers improved bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13036v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/imaiai/iaaf021</arxiv:DOI>
      <dc:creator>Matthew T. C. Li, Tiangang Cui, Fengyi Li, Youssef Marzouk, Olivier Zahm</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v4 Announce Type: replace-cross 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, are a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent U.S. Food and Drug Administration and European Medicines Agency guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. However, when the number of baseline covariates $p$ is large, the recommendation is somewhat murky. In this paper, we show that HOIF-motivated estimators for the treatment-specific mean have significantly improved statistical properties compared to popular adjusted estimators in practice when $p$ is relatively large relative to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. More importantly, we demonstrate that several state-of-the-art adjusted estimators proposed recently can be interpreted as particular HOIF-motivated estimators, thereby placing these estimators in a more unified framework. Numerical and empirical studies are conducted to corroborate our theoretical findings. An accompanying R package can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Sample complexity of optimal transport barycenters with discrete support</title>
      <link>https://arxiv.org/abs/2505.21274</link>
      <description>arXiv:2505.21274v3 Announce Type: replace-cross 
Abstract: Computational implementation of optimal transport barycenters for a set of target probability measures requires a form of approximation, a widespread solution being empirical approximation of measures. We provide an $O(\sqrt{N/n})$ statistical generalization bounds for the empirical sparse optimal transport barycenters problem, where $N$ is the maximum cardinality of the barycenter (sparse support) and $n$ is the sample size of the target measures empirical approximation. Our analysis includes various optimal transport divergences including Wasserstein, Sinkhorn and Sliced-Wasserstein. We discuss the application of our result to specific settings including K-means, constrained K-means, free and fixed support Wasserstein barycenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21274v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'eo Portales, Edouard Pauwels, Elsa Cazelles</dc:creator>
    </item>
    <item>
      <title>Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2510.26324</link>
      <description>arXiv:2510.26324v2 Announce Type: replace-cross 
Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26324v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyang Xun, Shivam Gupta, Eric Price</dc:creator>
    </item>
    <item>
      <title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v3 Announce Type: replace-cross 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition leverages association structure:
  vertex-level coefficients measuring directional concordance between outcome
  and features, or between feature pairs, define embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>Robust Cauchy-Based Methods for Predictive Regressions</title>
      <link>https://arxiv.org/abs/2511.09249</link>
      <description>arXiv:2511.09249v2 Announce Type: replace-cross 
Abstract: This paper develops robust inference methods for predictive regressions that address key challenges posed by endogenously persistent or heavy-tailed regressors, as well as persistent volatility in errors. Building on the Cauchy estimation framework, we propose two novel tests: one based on $t$-statistic group inference and the other employing a hybrid approach that combines Cauchy and OLS estimation. These methods effectively mitigate size distortions that commonly arise in standard inference procedures under endogeneity, near nonstationarity, heavy tails, and persistent volatility. The proposed tests are simple to implement and applicable to both continuous- and discrete-time models. Extensive simulation experiments demonstrate favorable finite-sample performance across a range of realistic settings. An empirical application examines the predictability of excess stock returns using the dividend-price and earnings-price ratios as predictors. The results suggest that the dividend-price ratio possesses predictive power, whereas the earnings-price ratio does not significantly forecast returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09249v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rustam Ibragimov, Jihyun Kim, Anton Skrobotov</dc:creator>
    </item>
  </channel>
</rss>
