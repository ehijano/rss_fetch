<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 02:44:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FaaSRCA: Full Lifecycle Root Cause Analysis for Serverless Applications</title>
      <link>https://arxiv.org/abs/2412.02239</link>
      <description>arXiv:2412.02239v1 Announce Type: new 
Abstract: Serverless becomes popular as a novel computing paradigms for cloud native services. However, the complexity and dynamic nature of serverless applications present significant challenges to ensure system availability and performance. There are many root cause analysis (RCA) methods for microservice systems, but they are not suitable for precise modeling serverless applications. This is because: (1) Compared to microservice, serverless applications exhibit a highly dynamic nature. They have short lifecycle and only generate instantaneous pulse-like data, lacking long-term continuous information. (2) Existing methods solely focus on analyzing the running stage and overlook other stages, failing to encompass the entire lifecycle of serverless applications. To address these limitations, we propose FaaSRCA, a full lifecycle root cause analysis method for serverless applications. It integrates multi-modal observability data generated from platform and application side by using Global Call Graph. We train a Graph Attention Network (GAT) based graph auto-encoder to compute reconstruction scores for the nodes in global call graph. Based on the scores, we determine the root cause at the granularity of the lifecycle stage of serverless functions. We conduct experimental evaluations on two serverless benchmarks, the results show that FaaSRCA outperforms other baseline methods with a top-k precision improvement ranging from 21.25% to 81.63%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02239v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Huang, Pengfei Chen, Guangba Yu, Yilun Wang, Haiyu Huang, Zilong He</dc:creator>
    </item>
    <item>
      <title>Theory building for empirical software engineering in qualitative research: Operationalization</title>
      <link>https://arxiv.org/abs/2412.02384</link>
      <description>arXiv:2412.02384v1 Announce Type: new 
Abstract: Context: This work is part of a research project whose ultimate goal is to systematize theory building in qualitative research in the field of software engineering. The proposed methodology involves four phases: conceptualization, operationalization, testing, and application. In previous work, we performed the conceptualization of a theory that investigates the structure of IT departments and teams when software-intensive organizations adopt a culture called DevOps. Objective: This paper presents a set of procedures to systematize the operationalization phase in theory building and their application in the context of DevOps team structures. Method: We operationalize the concepts and propositions that make up our theory to generate constructs and empirically testable hypotheses. Instead of using causal relations to operationalize the propositions, we adopt logical implication, which avoids the problems associated with causal reasoning. Strategies are proposed to ensure that the resulting theory aligns with the criterion of parsimony. Results: The operationalization phase is described from three perspectives: specification, implementation, and practical application. First, the operationalization process is formally defined. Second, a set of procedures for operating both concepts and propositions is described. Finally, the usefulness of the proposed procedures is demonstrated in a case study. Conclusions: This paper is a pioneering contribution in offering comprehensive guidelines for theory operationalization using logical implication. By following established procedures and using concrete examples, researchers can better ensure the success of their theory-building efforts through careful operationalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02384v1</guid>
      <category>cs.SE</category>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge P\'erez, Jessica D\'iaz, \'Angel Gonz\'alez-Prieto, Sergio Gil-Borr\'as</dc:creator>
    </item>
    <item>
      <title>A Multi-Agent Framework for Extensible Structured Text Generation in PLCs</title>
      <link>https://arxiv.org/abs/2412.02410</link>
      <description>arXiv:2412.02410v1 Announce Type: new 
Abstract: Programmable Logic Controllers (PLCs) are microcomputers essential for automating factory operations. Structured Text (ST), a high-level language adhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to express logic succinctly and to seamlessly integrate with other languages within the same standard. However, vendors develop their own customized versions of ST, and the lack of comprehensive and standardized documentation for the full semantics of ST has contributed to inconsistencies in how the language is implemented. Consequently, the steep learning curve associated with ST, combined with ever-evolving industrial requirements, presents significant challenges for developers. In response to these issues, we present AutoPLC, an LLM-based approach designed to automate the generation of vendor-specific ST code. To facilitate effective code generation, we first built a comprehensive knowledge base, including Rq2ST Case Library (requirements and corresponding implementations) and Instruction libraries. Then we developed a retrieval module to incorporate the domain-specific knowledge by identifying pertinent cases and instructions, guiding the LLM to generate code that meets the requirements. In order to verify and improve the quality of the generated code, we designed an adaptable code checker. If errors are detected, we initiate an iterative self-improvement process to instruct the LLM to revise the generated code. We evaluate AutoPLC's performance against seven state-of-the-art baselines using three benchmarks, one for open-source basic ST and two for commercial Structured Control Language (SCL) from Siemens. The results show that our approach consistently achieves superior performance across all benchmarks. Ablation study emphasizes the significance of our modules. Further manual analysis confirm the practical utility of the ST code generated by AutoPLC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02410v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donghao Yang, Aolang Wu, Tianyi Zhang, Li Zhang, Fang Liu, Xiaoli Lian, Yuming Ren, Jiaji Tian</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey on Dynamic Software Updating Techniques in IoTs</title>
      <link>https://arxiv.org/abs/2412.02450</link>
      <description>arXiv:2412.02450v1 Announce Type: new 
Abstract: This comprehensive survey paper provides an in-depth analysis of Dynamic Software Updating (DSU) techniques in the Internet of Things (IoT). This study critically examines eight significant research papers that employ diverse methodologies to address the challenges of DSU in IoT devices. The primary objectives include comparative analysis to identify the application domains of DSU tools, classification of program alterations accommodated by these systems, evaluation of the advantages and disadvantages of various DSU tools, and identification of potential paths for future research. This paper emphasizes the critical function of DSU in improving energy efficiency, extending operational durability, and bolstering security within IoT environments that demand high availability, including applications in smart cities and connected vehicles. It delves into the basic approaches and mechanisms of DSU, ranging from traditional methods to advanced practices like Over-the-Air updates and container-based solutions. This survey highlights the evolving nature of DSU techniques, balancing operational efficiency, security, and adaptability amidst the complexities of diverse IoT applications. Through this exploration, the paper aims to guide future developments in DSU strategies, enhancing IoT devices' resilience, functionality, and sustainability in a connected world. The insights from this survey are pivotal for researchers, practitioners, and policymakers in shaping effective DSU strategies to meet the growing needs of the IoT ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02450v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Madhav Neupane</dc:creator>
    </item>
    <item>
      <title>Can I do it</title>
      <link>https://arxiv.org/abs/2412.02569</link>
      <description>arXiv:2412.02569v1 Announce Type: new 
Abstract: Knowledge about how well a robot can perform a specific task is currently present only in engineering reports which are inaccessible to the robot. Artificial Intelligence techniques, such as hypergraphs and automated reasoning, can provide such engineering knowledge online while enabling updates in the knowledge with new experiences. This requires a sound knowledge structure and maintenance routines for keeping this knowledge-base about the robot's capabilities truthful. A robot with such up-to-date information can reason about if and how well it can accomplish a task. This article introduces a knowledge representation that combines an ontology on system engineering, a deductive reasoning on the connections between system components, and an inductive reasoning on the performance of these components in the current system configuration. This representation is further used to derive the expected performance for the overall system based on a continuous evaluation of the actual performance per component. Our real-life implementation shows a robot that can answer questions on whether it can do a specific task with the desired performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02569v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joris Sijs, Carlos Hernandez-Corbato, Willeke van Vught, Julio Oliveira</dc:creator>
    </item>
    <item>
      <title>Influence of Skill and Knowledge of Programmers on Program Behavior Visualization by CS Unplugged</title>
      <link>https://arxiv.org/abs/2412.01831</link>
      <description>arXiv:2412.01831v1 Announce Type: cross 
Abstract: Computer science unplugged (CS unplugged) is a method of teaching computer science and computational thinking. It does not use a computer but employs physical materials. As CS unplugged, past studies proposed a new method to visualize programming behavior, and evaluated the method based on the understanding test of program. However, the past studied did not consider the factors affecting the test such as participants knowledge. This study analyzed the relationship between the factors and the test results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01831v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumika Jinnouchi, Masateru Tsunoda</dc:creator>
    </item>
    <item>
      <title>Impact of Data Snooping on Deep Learning Models for Locating Vulnerabilities in Lifted Code</title>
      <link>https://arxiv.org/abs/2412.02048</link>
      <description>arXiv:2412.02048v1 Announce Type: cross 
Abstract: This study examines the impact of data snooping on neural networks for vulnerability detection in lifted code, building on previous research which used word2vec, and unidirectional and bidirectional transformer-based embeddings. The research specifically focuses on how model performance is affected when embedding models are trained on datasets, including samples also used for neural network training and validation. The results show that introducing data snooping did not significantly alter model performance, suggesting that data snooping had a minimal impact or that samples randomly dropped as part of the methodology contained hidden features critical to achieving optimal performance. In addition, the findings reinforce the conclusions of previous research, which found that models trained with GPT-2 embeddings consistently outperformed neural networks trained with other embeddings. The fact that this holds even when data snooping is introduced into the embedding model indicates GPT-2's robustness in representing complex code features, even under less-than-ideal conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02048v1</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gary A. McCully, John D. Hastings, Shengjie Xu</dc:creator>
    </item>
    <item>
      <title>Generating Critical Scenarios for Testing Automated Driving Systems</title>
      <link>https://arxiv.org/abs/2412.02574</link>
      <description>arXiv:2412.02574v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) have demonstrated significant potential in revolutionizing transportation, yet ensuring their safety and reliability remains a critical challenge, especially when exposed to dynamic and unpredictable environments. Real-world testing of an Autonomous Driving System (ADS) is both expensive and risky, making simulation-based testing a preferred approach. In this paper, we propose AVASTRA, a Reinforcement Learning (RL)-based approach to generate realistic critical scenarios for testing ADSs in simulation environments. To capture the complexity of driving scenarios, AVASTRA comprehensively represents the environment by both the internal states of an ADS under-test (e.g., the status of the ADS's core components, speed, or acceleration) and the external states of the surrounding factors in the simulation environment (e.g., weather, traffic flow, or road condition). AVASTRA trains the RL agent to effectively configure the simulation environment that places the AV in dangerous situations and potentially leads it to collisions. We introduce a diverse set of actions that allows the RL agent to systematically configure both environmental conditions and traffic participants. Additionally, based on established safety requirements, we enforce heuristic constraints to ensure the realism and relevance of the generated test scenarios. AVASTRA is evaluated on two popular simulation maps with four different road configurations. Our results show AVASTRA's ability to outperform the state-of-the-art approach by generating 30% to 115% more collision scenarios. Compared to the baseline based on Random Search, AVASTRA achieves up to 275% better performance. These results highlight the effectiveness of AVASTRA in enhancing the safety testing of AVs through realistic comprehensive critical scenario generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02574v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Trung-Hieu Nguyen, Truong-Giang Vuong, Hong-Nam Duong, Son Nguyen, Hieu Dinh Vo, Toshiaki Aoki, Thu-Trang Nguyen</dc:creator>
    </item>
    <item>
      <title>AI-Driven Resource Allocation Framework for Microservices in Hybrid Cloud Platforms</title>
      <link>https://arxiv.org/abs/2412.02610</link>
      <description>arXiv:2412.02610v1 Announce Type: cross 
Abstract: The increasing demand for scalable, efficient resource management in hybrid cloud environments has led to the exploration of AI-driven approaches for dynamic resource allocation. This paper presents an AI-driven framework for resource allocation among microservices in hybrid cloud platforms. The framework employs reinforcement learning (RL)-based resource utilization optimization to reduce costs and improve performance. The framework integrates AI models with cloud management tools to respond to challenges of dynamic scaling and cost-efficient low-latency service delivery. The reinforcement learning model continuously adjusts provisioned resources as required by the microservices and predicts the future consumption trends to minimize both under- and over-provisioning of resources. Preliminary simulation results indicate that using AI in the provision of resources related to costs can reduce expenditure by up to 30-40% compared to manual provisioning and threshold-based auto-scaling approaches. It is also estimated that the efficiency in resource utilization is expected to improve by 20%-30% with a corresponding latency cut of 15%-20% during the peak demand periods. This study compares the AI-driven approach with existing static and rule-based resource allocation methods, demonstrating the capability of this new model to outperform them in terms of flexibility and real-time interests. The results indicate that reinforcement learning can make optimization of hybrid cloud platforms even better, offering a 25-35% improvement in cost efficiency and the power of scaling for microservice-based applications. The proposed framework is a strong and scalable solution to managing cloud resources in dynamic and performance-critical environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02610v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>Morescient GAI for Software Engineering (Extended Version)</title>
      <link>https://arxiv.org/abs/2406.04710</link>
      <description>arXiv:2406.04710v2 Announce Type: replace 
Abstract: The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness - they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of "Morescient" GAI is needed that is "aware" of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision and roadmap for how such "Morescient" GAI models can be engineered, evolved and disseminated according to the principles of open science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04710v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcus Kessel, Colin Atkinson</dc:creator>
    </item>
    <item>
      <title>Model Editing for LLMs4Code: How Far are We?</title>
      <link>https://arxiv.org/abs/2411.06638</link>
      <description>arXiv:2411.06638v2 Announce Type: replace 
Abstract: Large Language Models for Code (LLMs4Code) have been found to exhibit outstanding performance in the software engineering domain, especially the remarkable performance in coding tasks. However, even the most advanced LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to the high cost of training LLMs4Code, it is impractical to re-train the models for fixing these problematic code knowledge. Model editing is a new technical field for effectively and efficiently correcting erroneous knowledge in LLMs, where various model editing techniques and benchmarks have been proposed recently. Despite that, a comprehensive study that thoroughly compares and analyzes the performance of the state-of-the-art model editing techniques for adapting the knowledge within LLMs4Code across various code-related tasks is notably absent. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help of CLMEEval, we evaluate six advanced model editing techniques on three LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings include that the external memorization-based GRACE approach achieves the best knowledge editing effectiveness and specificity (the editing does not influence untargeted knowledge), while generalization (whether the editing can generalize to other semantically-identical inputs) is a universal challenge for existing techniques. Furthermore, building on in-depth case analysis, we introduce an enhanced version of GRACE called A-GRACE, which incorporates contrastive learning to better capture the semantics of the inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06638v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Bin Ji, Weimin Zhang</dc:creator>
    </item>
    <item>
      <title>Personalization of Code Readability Evaluation Based on LLM Using Collaborative Filtering</title>
      <link>https://arxiv.org/abs/2411.10583</link>
      <description>arXiv:2411.10583v2 Announce Type: replace 
Abstract: Code readability is an important indicator of software maintenance as it can significantly impact maintenance efforts. Recently, LLM (large language models) have been utilized for code readability evaluation. However, readability evaluation differs among developers, so personalization of the evaluation by LLM is needed. This study proposes a method which calibrates the evaluation, using collaborative filtering. Our preliminary analysis suggested that the method effectively enhances the accuracy of the readability evaluation using LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10583v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buntaro Hiraki, Kensei Hamamoto, Ami Kimura, Masateru Tsunoda, Amjed Tahir, Kwabena Ebo Bennin, Akito Monden, Keitaro Nakasai</dc:creator>
    </item>
    <item>
      <title>FullStack Bench: Evaluating LLMs as Full Stack Coders</title>
      <link>https://arxiv.org/abs/2412.00535</link>
      <description>arXiv:2412.00535v2 Announce Type: replace-cross 
Abstract: As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00535v2</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Jing Mai, Ge Zhang, Wenhao Huang, Kai Shen, Liang Xiang</dc:creator>
    </item>
  </channel>
</rss>
