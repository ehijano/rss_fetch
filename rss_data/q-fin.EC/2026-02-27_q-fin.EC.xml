<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:01:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection</title>
      <link>https://arxiv.org/abs/2602.22412</link>
      <description>arXiv:2602.22412v1 Announce Type: cross 
Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22412v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>econ.GN</category>
      <category>math.IT</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Zhou, Donghao Zhu, Houcai Shen</dc:creator>
    </item>
    <item>
      <title>The Inference Bottleneck: Antitrust and Neutrality Duties in the Age of Cognitive Infrastructure</title>
      <link>https://arxiv.org/abs/2602.22750</link>
      <description>arXiv:2602.22750v1 Announce Type: cross 
Abstract: As generative AI commercializes, competitive advantage is shifting from one-time model training toward continuous inference, distribution, and routing. At the frontier, large-scale inference can function as cognitive infrastructure: a bottleneck input that downstream applications rely on to compete, controlled by firms that often compete downstream through integrated assistants, productivity suites, and developer tooling. Foreclosure risk is not limited to price. It can be executed through non-price discrimination (latency, throughput, error rates, context limits, feature gating) and, where models select tools and services, through steering and default routing that is difficult to observe and harder to litigate. This essay makes three moves. First, it defines cognitive infrastructure as a falsifiable concept built around measurable reliance, vertical incentives, and discrimination capacity, without assuming a clean market definition. Second, it frames theories of harm using raising-rivals'-costs logic for vertically related and platform markets, where foreclosure can be profitable without anticompetitive pricing. Third, it proposes Neutral Inference: a targeted, auditable conduct approach built around (i) quality-of-service parity, (ii) routing transparency, and (iii) FRAND-style non-discrimination for similarly situated buyers, applied only when observable evidence indicates functional gatekeeper status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22750v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaston Besanson, Marcelo Celani</dc:creator>
    </item>
    <item>
      <title>Preference Analysis Using Random Spanning Trees: A Stochastic Sampling Approach to Inconsistent Pairwise Comparisons</title>
      <link>https://arxiv.org/abs/2107.01731</link>
      <description>arXiv:2107.01731v2 Announce Type: replace 
Abstract: Eliciting preferences from human judgements is inherently imprecise, yet most decision analysis methods force a single priority vector from pairwise comparisons, discarding the information embedded in inconsistencies. We instead leverage inconsistency to characterise preference uncertainty by examining all priority vectors consistent with the decision maker's judgements. Spanning tree analysis enumerates combinations of evaluation and weighting vectors from pairwise comparison subsets, each yielding a distinct preference vector and collectively defining a distribution over possible preference orderings. Since exponential growth renders complete enumeration prohibitive, we propose a stochastic random walk sampling approach with sample sizes formally established via statistical sampling theory. This enables two key metrics: Pairwise Winning Indices (PWIs), the probability one alternative is preferred to another, and Rank Acceptability Indices (RAIs), the probability an alternative attains a given rank. A notable advantage is applicability to incomplete pairwise comparisons, common in large-scale problems. We validate the methodology against complete enumeration on a didactic example, then demonstrate scalability on a telecommunications backbone infrastructure selection case study involving billions of spanning tree combinations. The approach yields probabilistic insights into preference robustness and ranking uncertainty, supporting informed decisions without the burden of exact enumeration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.01731v2</guid>
      <category>econ.GN</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salvatore Greco, Sajid Siraj, Michele Lundy</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?</title>
      <link>https://arxiv.org/abs/2301.07543</link>
      <description>arXiv:2301.07543v2 Announce Type: replace 
Abstract: We argue that newly-developed large language models (LLMs), because of how they are trained and designed, are implicit computational models of humans -- a Homo silicus. LLMs can be used like economists use Homo economicus: they can be given endowments, information, preferences, and so on, and then their behavior can be explored in scenarios via simulation. Experiments using this approach, derived from Charness and Rabin (2002), Kahneman et al. (1986), Samuelson and Zeckhauser (1988), Oprea (2024b), and Horton (2025), show qualitatively similar results to the original, and when they differ, it is often generative for future research. We discuss potential applications, conceptual issues, and why this approach can inform the study of humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07543v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John J. Horton, Apostolos Filippas, Benjamin S. Manning</dc:creator>
    </item>
    <item>
      <title>Is there a secular decline in disruptive patents? Correcting for measurement bias</title>
      <link>https://arxiv.org/abs/2306.10774</link>
      <description>arXiv:2306.10774v2 Announce Type: replace 
Abstract: Despite tremendous growth in the volume of new scientific and technological knowledge, the popular press has recently raised concerns that disruptive innovation is slowing. These dire prognoses were driven in part by Park et al. (2023), a Nature publication that uses decades of data and millions of observations coupled with a novel quantitative metric (the CD index) that characterizes innovation in science and technology as either consolidating or disruptive. We challenge the Park et al. (2023) patent findings, principally around concerns of truncation bias and exclusion bias. We show that 88 percent of the decrease in the average CD index over 1980-2010 reported by the authors can be explained by their truncation of all backward patent citations before 1976. We also show that this truncation bias varies by technology class. We further account for a change in U.S. patent law that allows for citations to patent applications in addition to patent grants-something ignored by the authors in their analysis--and update the analysis to 2016. We show that the number of highly disruptive patents has increased since 1980--particularly since 2008. Our results suggest caution in using the Park et al. (2023) patent findings and conclusions as a basis for research and decision-making in public policy, industry restructuring or firm reorganization aimed at altering the current innovation landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10774v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.respol.2024.104992</arxiv:DOI>
      <arxiv:journal_reference>Research Policy 53.5 (2024): 104992</arxiv:journal_reference>
      <dc:creator>Jeffrey T. Macher, Christian Rutzer, Rolf Weder</dc:creator>
    </item>
    <item>
      <title>The Market Consequences of Perceived Strategic Generosity: An Empirical Examination of NFT Charity Fundraisers</title>
      <link>https://arxiv.org/abs/2401.12064</link>
      <description>arXiv:2401.12064v3 Announce Type: replace 
Abstract: Crypto donations now represent a significant fraction of charitable giving worldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale of NFTs of artistic works with the proceeds donated to philanthropic causes, have emerged as a novel development in this space. A unique aspect of NFT charity fundraisers is the significant potential for donors to reap financial gains from the rising value of purchased NFTs. Questions may arise about donors' motivations in these charity fundraisers, potentially resulting in a negative social image. NFT charity fundraisers thus offer a unique opportunity to understand the economic consequences of a donor's social image. We investigate these effects in the context of a large NFT charity fundraiser. We identify the causal effect of purchasing an NFT within the charity fundraiser on a donor's later market outcomes by leveraging random variation in transaction processing times on the blockchain. Further, we demonstrate a clear pattern of heterogeneity based on an individual's decision to relist (versus hold) the purchased charity NFTs (a sign of perceived strategic generosity) and based on an individual's social exposure within the NFT marketplace. We show that charity-NFT 're-listers' experience significant penalties in the market regarding the prices they can command for their other NFTs, particularly among those who are more socially exposed. Finally, we report the results of a scenario-based online experiment, which again support our findings, highlighting that the re-listing a charity NFT for sale at a profit leads others to perceive their initial donation as strategic generosity and reduces those others' willingness to purchase NFTs from the donor. Our study underscores the growing importance of digital visibility and traceability, features that characterize crypto-philanthropy, and online philanthropy more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12064v3</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Liang, Murat Tunc, Gordon Burtch</dc:creator>
    </item>
    <item>
      <title>Swiss-system chess tournaments and unfairness</title>
      <link>https://arxiv.org/abs/2410.19333</link>
      <description>arXiv:2410.19333v3 Announce Type: replace 
Abstract: The Swiss-system is an increasingly popular competition format as it provides a favourable trade-off between the number of matches and ranking accuracy. However, there is no empirical study on the potential unfairness of Swiss-system chess tournaments caused by the odd number of rounds played. To analyse this issue, our paper compares the number of points scored in the tournament between players who played one game more with the white pieces and players who played one game less with the white pieces. Using data from 28 highly prestigious competitions, we find that players with an extra white game score significantly more points. In particular, the advantage exceeds the value of a draw in the four Grand Swiss tournaments. A potential solution to this unfairness could be organising Swiss-system chess tournaments with an even number of rounds, and guaranteeing a balanced colour assignment for all players using a recently proposed pairing mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19333v3</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Alex Krumer</dc:creator>
    </item>
    <item>
      <title>Beyond pay: AI skills reward more job benefits</title>
      <link>https://arxiv.org/abs/2507.20410</link>
      <description>arXiv:2507.20410v3 Announce Type: replace 
Abstract: This study investigates the non-monetary rewards associated with artificial intelligence (AI) skills in the U.S. labour market. Using a dataset of approximately ten million online job vacancies from 2018 to 2024, we identify AI roles-positions requiring at least one AI-related skill-and examine the extent to which these roles offer non-monetary benefits such as tuition assistance, paid leave, health and well-being perks, parental leave, workplace culture enhancements, and remote work options. While previous research has documented substantial wage premiums for AI-related roles due to growing demand and limited talent supply, our study asks whether this demand also translates into enhanced non-monetary compensation. We find that AI roles are significantly more likely to offer such perks, even after controlling for education requirements, industry, and occupation type. It is twice as likely for an AI role to offer parental leave and almost three times more likely to provide remote working options. Moreover, the highest-paying AI roles tend to bundle these benefits, suggesting a compound premium where salary increases coincide with expanded non-monetary rewards. AI roles offering parental leave or health benefits show salaries that are, on average, 12% to 20% higher than AI roles without this benefit. This pattern is particularly pronounced in years and occupations experiencing the highest AI-related demand, pointing to a demand-driven dynamic. Our findings underscore the strong pull of AI talent in the labor market and challenge narratives of technological displacement, highlighting instead how employers compete for scarce talent through both financial and non-financial incentives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20410v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandra Castaneda, Matthew Bone, Fabian Stephany</dc:creator>
    </item>
  </channel>
</rss>
