<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 04:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On A Necessary Condition For Posterior Inconsistency: New Insights From A Classic Counterexample</title>
      <link>https://arxiv.org/abs/2510.18126</link>
      <description>arXiv:2510.18126v1 Announce Type: new 
Abstract: The consistency of posterior distributions in density estimation is at the core of Bayesian statistical theory. Classical work established sufficient conditions, typically combining KL support with complexity bounds on sieves of high prior mass, to guarantee consistency with respect to the Hellinger distance. Yet no systematic theory explains a widely held belief: under KL support, Hellinger consistency is exceptionally hard to violate. This suggests that existing sufficient conditions, while useful in practice, may overlook some key aspects of posterior behavior. We address this gap by directly investigating what must fail for inconsistency to arise, aiming to identify a substantive necessary condition for Hellinger inconsistency. Our starting point is Andrew Barron's classical counterexample, the only known violation of Hellinger consistency under KL support, which relies on a contrived family of oscillatory densities and a prior with atoms. We show that, within a broad class of models including Barron's, inconsistency requires persistent posterior concentration on densities with exponentially high likelihood ratios. In turn, such behavior demands a prior encoding implausibly precise knowledge of the true, yet unknown data-generating distribution, making inconsistency essentially unattainable in any realistic inference problem. Our results confirm the long-standing intuition that posterior inconsistency in density estimation is not a natural phenomenon, but rather an artifact of pathological prior constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18126v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Bariletto, Stephen G. Walker</dc:creator>
    </item>
    <item>
      <title>The Picard-Lagrange Framework for Higher-Order Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.18242</link>
      <description>arXiv:2510.18242v1 Announce Type: new 
Abstract: Sampling from log-concave distributions is a central problem in statistics and machine learning. Prior work establishes theoretical guarantees for Langevin Monte Carlo algorithm based on overdamped and underdamped Langevin dynamics and, more recently, some third-order variants. In this paper, we introduce a new sampling algorithm built on a general $K$th-order Langevin dynamics, extending beyond second- and third-order methods. To discretize the $K$th-order dynamics, we approximate the drift induced by the potential via Lagrange interpolation and refine the node values at the interpolation points using Picard-iteration corrections, yielding a flexible scheme that fully utilizes the acceleration of higher-order Langevin dynamics. For targets with smooth, strongly log-concave densities, we prove dimension-dependent convergence in Wasserstein distance: the sampler achieves $\varepsilon$-accuracy within $\widetilde O(d^{\frac{K-1}{2K-3}}\varepsilon^{-\frac{2}{2K-3}})$ gradient evaluations for $K \ge 3$. To our best knowledge, this is the first sampling algorithm achieving such query complexity. The rate improves with the order $K$ increases, yielding better rates than existing first to third-order approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18242v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaideep Mahajan, Kaihong Zhang, Feng Liang, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Consistency of Nonparametric Density Estimators in CAT(0) Orthant Space</title>
      <link>https://arxiv.org/abs/2510.18290</link>
      <description>arXiv:2510.18290v1 Announce Type: new 
Abstract: The inference of evolutionary histories is a central problem in evolutionary biology. The analysis of a sample of phylogenetic trees can be conducted in Billera-Holmes-Vogtmann tree space, which is a CAT(0) metric space of phylogenetic trees. The globally non-positively curved (CAT(0)) property of this space enables the extension of various statistical techniques. In the problem of nonparametric density estimation, two primary methods, kernel density estimation and log-concave maximum likelihood estimation, have been proposed, yet their theoretical properties remain largely unexplored.
  In this paper, we address this gap by proving the consistency of these estimators in a more general setting$\unicode{x2014}$CAT(0) orthant spaces, which include BHV tree space. We extend log-concave approximation techniques to this setting and establish consistency via the continuity of the log-concave projection map. We also modify the kernel density estimator to correct boundary bias and establish uniform consistency using empirical process theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18290v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Takazawa, Tomonari Sei</dc:creator>
    </item>
    <item>
      <title>Wasserstein projection estimators for circular distributions</title>
      <link>https://arxiv.org/abs/2510.18367</link>
      <description>arXiv:2510.18367v1 Announce Type: new 
Abstract: For statistical models on circles, we investigate performance of estimators defined as the projections of the empirical distribution with respect to the Wasserstein distance. We develop algorithms for computing the Wasserstein projection estimators based on a formula of the Wasserstein distances on circles. Numerical results on the von Mises, wrapped Cauchy, and sine-skewed von Mises distributions show that the accuracy of the Wasserstein projection estimators is comparable to the maximum likelihood estimator. In addition, the $L^1$-Wasserstein projection estimator is found to be robust against noise contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18367v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naoki Otani, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>New closed-form estimators for discrete distributions</title>
      <link>https://arxiv.org/abs/2510.18503</link>
      <description>arXiv:2510.18503v1 Announce Type: new 
Abstract: We revisit the problem of parameter estimation for discrete probability distributions with values in $\mathbb{Z}^d$. To this end, we adapt a technique called Stein's Method of Moments to discrete distributions which often gives closed-form estimators when standard methods such as maximum likelihood estimation (MLE) require numerical optimization. These new estimators exhibit good performance in small-sample settings which is demonstrated by means of a comparison to the MLE through simulation studies. We pay special attention to truncated distributions and show that the asymptotic behavior of our estimators is not affected by an unknown (rectangular) truncation domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18503v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Fischer</dc:creator>
    </item>
    <item>
      <title>Hyperparameter Selection via Early Stopping for Bayesian Semilinear PDEs</title>
      <link>https://arxiv.org/abs/2510.18547</link>
      <description>arXiv:2510.18547v1 Announce Type: new 
Abstract: We study non-linear Bayesian inverse problems arising from semilinear partial differential equations (PDEs) that can be transformed into linear Bayesian inverse problems. We are then able to extend the early stopping for Ensemble Kalman-Bucy Filter (EnKBF) to these types of linearisable nonlinear problems as a way to tune the prior distribution. Using the linearisation method introduced in \cite{koers2024}, we transform the non-linear problem into a linear one, apply early stopping based on the discrepancy principle, and then pull back the resulting posterior to the posterior for the original parameter of interest. Following \cite{tienstra2025}, we show that this approach yields adaptive posterior contraction rates and frequentist coverage guarantees, under mild conditions on the prior covariance operator. From this, it immediately follows that Tikhonov regularisation coupled with the discrepancy principle contracts at the same rate. The proposed method thus provides a data-driven way to tune Gaussian priors via early stopping, which is both computationally efficient and statistically near optimal for nonlinear problems. Lastly, we demonstrate our results theoretically and numerically for the classical benchmark problem, the time-independent Schr\"odinger equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18547v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maia Tienstra, Gottfried Hastermann</dc:creator>
    </item>
    <item>
      <title>Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization</title>
      <link>https://arxiv.org/abs/2510.18410</link>
      <description>arXiv:2510.18410v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer from overfitting due to their high capacity. We introduce Momentum-Adaptive Gradient Dropout (MAGDrop), a novel regularization method that dynamically adjusts dropout rates on activations based on current gradients and accumulated momentum, enhancing stability in non-convex optimization landscapes. To theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes generalization bound that accounts for its adaptive nature, achieving up to 20% sharper bounds compared to standard approaches by leveraging momentum-driven perturbation control. Empirically, the activation-based MAGDrop outperforms baseline regularization techniques, including standard dropout and adaptive gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively. Our work bridges theoretical insights and practical advancements, offering a robust framework for enhancing DNN generalization suitable for high-stakes applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18410v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeel Safder</dc:creator>
    </item>
    <item>
      <title>Measuring deviations from spherical symmetry</title>
      <link>https://arxiv.org/abs/2510.18598</link>
      <description>arXiv:2510.18598v1 Announce Type: cross 
Abstract: Most of the work on checking spherical symmetry assumptions on the distribution of the $p$-dimensional random vector $Y$ has its focus on statistical tests for the null hypothesis of exact spherical symmetry. In this paper, we take a different point of view and propose a measure for the deviation from spherical symmetry, which is based on the minimum distance between the distribution of the vector $\big (\|Y\|, Y/ \|Y\| )^\top $ and its best approximation by a distribution of a vector $\big (\|Y_s\|, Y_s/ \|Y_s \| )^\top $ corresponding to a random vector $Y_s$ with a spherical distribution. We develop estimators for the minimum distance with corresponding statistical guarantees (provided by asymptotic theory) and demonstrate the applicability of our approach by means of a simulation study and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18598v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujia Bai, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2510.18843</link>
      <description>arXiv:2510.18843v1 Announce Type: cross 
Abstract: We provide an inferential framework to assess variable importance for heterogeneous treatment effects. This assessment is especially useful in high-risk domains such as medicine, where decision makers hesitate to rely on black-box treatment recommendation algorithms. The variable importance measures we consider are local in that they may differ across individuals, while the inference is global in that it tests whether a given variable is important for any individual. Our approach builds on recent developments in semiparametric theory for function-valued parameters, and is valid even when statistical machine learning algorithms are employed to quantify treatment effect heterogeneity. We demonstrate the applicability of our method to infectious disease prevention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18843v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Morzywolek, Peter B. Gilbert, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Early Stopping for Ensemble Kalman-Bucy Inversion</title>
      <link>https://arxiv.org/abs/2403.18353</link>
      <description>arXiv:2403.18353v4 Announce Type: replace 
Abstract: Bayesian linear inverse problems aim to recover an unknown signal from noisy observations, incorporating prior knowledge. This paper analyses a data-dependent method to choose the scale parameter of a Gaussian prior. The method we study arises from early stopping methods, which have been successfully applied to a range of problems, such as statistical inverse problems, in the frequentist setting. These results are extended to the Bayesian setting. We study the use of a discrepancy-based stopping rule in the setting of random noise, which allows for adaptation. Our proposed stopping rule results in optimal rates for the reparameterized problem under certain conditions on the prior covariance operator. We furthermore derive for which class of signals this method is adaptive. It is also shown that the associated posterior contracts at the same rate as the MAP estimator and provides a conservative measure of uncertainty. We implement the proposed stopping rule using the continuous-time ensemble Kalman--Bucy filter (EnKBF). The fictitious time parameter replaces the scale parameter, and the ensemble size is appropriately adjusted in order not to lose the statistical optimality of the computed estimator. With this Monte Carlo algorithm, we extend our results numerically to a nonlinear problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18353v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maia Tienstra, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Transition of $\alpha$-mixing in Random Iterations with Applications in Queuing Theory</title>
      <link>https://arxiv.org/abs/2410.05056</link>
      <description>arXiv:2410.05056v4 Announce Type: replace 
Abstract: Nonlinear time series models with exogenous regressors are essential in econometrics, queuing theory, and machine learning, though their statistical analysis remains incomplete. Key results, such as the law of large numbers and the functional central limit theorem, are known for weakly dependent variables. We demonstrate the transfer of mixing properties from the exogenous regressor to the response via coupling arguments. Additionally, we study Markov chains in random environments with drift and minorization conditions, even under non-stationary environments with favorable mixing properties, and apply this framework to single-server queuing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05056v4</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Attila Lovas</dc:creator>
    </item>
    <item>
      <title>Elicitability and identifiability of tail risk measures</title>
      <link>https://arxiv.org/abs/2404.14136</link>
      <description>arXiv:2404.14136v3 Announce Type: replace-cross 
Abstract: Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk, Expected Shortfall and Range Value-at-Risk being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14136v3</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fissler, Fangda Liu, Ruodu Wang, Linxiao Wei</dc:creator>
    </item>
    <item>
      <title>A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI</title>
      <link>https://arxiv.org/abs/2501.04641</link>
      <description>arXiv:2501.04641v2 Announce Type: replace-cross 
Abstract: Multi-modal generative AI systems, such as those combining vision and language, rely on contrastive pre-training to learn representations across different modalities. While their practical benefits are widely acknowledged, a rigorous theoretical understanding of the contrastive pre-training framework remains limited. This paper develops a theoretical framework to explain the success of contrastive pre-training in downstream tasks, such as zero-shot classification, conditional diffusion models, and vision-language models. We introduce the concept of approximate sufficient statistics, a generalization of the classical sufficient statistics, and show that near-minimizers of the contrastive pre-training loss are approximately sufficient, making them adaptable to diverse downstream tasks. We further propose the Joint Generative Hierarchical Model for the joint distribution of images and text, showing that transformers can efficiently approximate relevant functions within this model via belief propagation. Building on this framework, we derive sample complexity guarantees for multi-modal learning based on contrastive pre-trained representations. Numerical simulations validate these theoretical findings, demonstrating the strong generalization performance of contrastively pre-trained transformers in various multi-modal tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04641v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazusato Oko, Licong Lin, Yuhang Cai, Song Mei</dc:creator>
    </item>
    <item>
      <title>The $\varphi$ Curve: The Shape of Generalization through the Lens of Norm-based Capacity Control</title>
      <link>https://arxiv.org/abs/2502.01585</link>
      <description>arXiv:2502.01585v3 Announce Type: replace-cross 
Abstract: Understanding how the test risk scales with model complexity is a central question in machine learning. Classical theory is challenged by the learning curves observed for large over-parametrized deep networks. Capacity measures based on parameter count typically fail to account for these empirical observations. To tackle this challenge, we consider norm-based capacity measures and develop our study for random features based estimators, widely used as simplified theoretical models for more complex networks. In this context, we provide a precise characterization of how the estimator's norm concentrates and how it governs the associated test error. Our results show that the predicted learning curve admits a phase transition from under- to over-parameterization, but no double descent behavior. This confirms that more classical U-shaped behavior is recovered considering appropriate capacity measures based on models norms rather than size. From a technical point of view, we leverage deterministic equivalence as the key tool and further develop new deterministic quantities which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01585v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichen Wang, Yudong Chen, Lorenzo Rosasco, Fanghui Liu</dc:creator>
    </item>
    <item>
      <title>Beyond Benign Overfitting in Nadaraya-Watson Interpolators</title>
      <link>https://arxiv.org/abs/2502.07480</link>
      <description>arXiv:2502.07480v3 Announce Type: replace-cross 
Abstract: In recent years, there has been much interest in understanding the generalization behavior of interpolating predictors, which overfit on noisy training data. Whereas standard analyses are concerned with whether a method is consistent or not, recent observations have shown that even inconsistent predictors can generalize well. In this work, we revisit the classic interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method), and study its generalization capabilities through this modern viewpoint. In particular, by varying a single bandwidth-like hyperparameter, we prove the existence of multiple overfitting behaviors, ranging non-monotonically from catastrophic, through benign, to tempered. Our results highlight how even classical interpolating methods can exhibit intricate generalization behaviors. In addition, for the purpose of tuning the hyperparameter, the results suggest that over-estimating the intrinsic dimension of the data is less harmful than under-estimating it. Numerical experiments complement our theory, demonstrating the same phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07480v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Barzilai, Guy Kornowski, Ohad Shamir</dc:creator>
    </item>
    <item>
      <title>Spike-timing-dependent Hebbian learning as noisy gradient descent</title>
      <link>https://arxiv.org/abs/2505.10272</link>
      <description>arXiv:2505.10272v2 Announce Type: replace-cross 
Abstract: Hebbian learning is a key principle underlying learning in biological neural networks. We relate a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent with respect to a non-convex loss function on the probability simplex. Despite the constant injection of noise and the non-convexity of the underlying optimization problem, one can rigorously prove that the considered Hebbian learning dynamic identifies the presynaptic neuron with the highest activity and that the convergence is exponentially fast in the number of iterations. This is non-standard and surprising as typically noisy gradient descent with fixed noise level only converges to a stationary regime where the noise causes the dynamic to fluctuate around a minimiser.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10272v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Dexheimer, Sascha Gaudlitz, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Counterfactual reasoning: an analysis of in-context emergence</title>
      <link>https://arxiv.org/abs/2506.05188</link>
      <description>arXiv:2506.05188v2 Announce Type: replace-cross 
Abstract: Large-scale neural language models exhibit remarkable performance in in-context learning: the ability to learn and reason about the input context on the fly. This work studies in-context counterfactual reasoning in language models, that is, the ability to predict consequences of a hypothetical scenario. We focus on a well-defined, synthetic linear regression task that requires noise abduction. Accurate prediction is based on (1) inferring an unobserved latent concept and (2) copying contextual noise from factual observations. We show that language models are capable of counterfactual reasoning. Further, we enhance existing identifiability results and reduce counterfactual reasoning for a broad class of functions to a transformation on in-context observations. In Transformers, we find that self-attention, model depth and pre-training data diversity drive performance. Moreover, we provide mechanistic evidence that the latent concept is linearly represented in the residual stream and we introduce designated \textit{noise abduction heads} central to performing counterfactual reasoning. Lastly, our findings extend to counterfactual reasoning under SDE dynamics and reflect that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under https://github.com/mrtzmllr/iccr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05188v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Miller, Bernhard Sch\"olkopf, Siyuan Guo</dc:creator>
    </item>
    <item>
      <title>Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods</title>
      <link>https://arxiv.org/abs/2506.10959</link>
      <description>arXiv:2506.10959v2 Announce Type: replace-cross 
Abstract: While in-context learning (ICL) has achieved remarkable success in natural language and vision domains, its theoretical understanding-particularly in the context of structured geometric data-remains unexplored. This paper initiates a theoretical study of ICL for regression of H\"older functions on manifolds. We establish a novel connection between the attention mechanism and classical kernel methods, demonstrating that transformers effectively perform kernel-based prediction at a new query through its interaction with the prompt. This connection is validated by numerical experiments, revealing that the learned query-prompt scores for H\"older functions are highly correlated with the Gaussian kernel. Building on this insight, we derive generalization error bounds in terms of the prompt length and the number of training tasks. When a sufficient number of training tasks are observed, transformers give rise to the minimax regression rate of H\"older functions on manifolds, which scales exponentially with the intrinsic dimension of the manifold, rather than the ambient space dimension. Our result also characterizes how the generalization error scales with the number of training tasks, shedding light on the complexity of transformers as in-context kernel algorithm learners. Our findings provide foundational insights into the role of geometry in ICL and novels tools to study ICL of nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10959v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaiming Shen, Alexander Hsu, Rongjie Lai, Wenjing Liao</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem</title>
      <link>https://arxiv.org/abs/2509.22597</link>
      <description>arXiv:2509.22597v2 Announce Type: replace-cross 
Abstract: The stochastic inverse problem is a key ingredient in making inferences, predictions, and decisions for complex science and engineering systems. We formulate and analyze a nonparametric Bayesian solution for the stochastic inverse problem. Key properties of the solution are proved and the convergence and error of a computational solution obtained by random sampling is analyzed. Several applications illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22597v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyi Shi, Lei Yang, Jiarui Chi, Troy Butler, Haonan Wang, Derek Bingham, Don Estep</dc:creator>
    </item>
    <item>
      <title>Kernel-Based Nonparametric Tests For Shape Constraints</title>
      <link>https://arxiv.org/abs/2510.16745</link>
      <description>arXiv:2510.16745v2 Announce Type: replace-cross 
Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for nonparametric mean-variance optimization and inference on shape constraints of the optimal rule. We derive statistical properties of the sample estimator and provide rigorous theoretical guarantees, such as asymptotic consistency, a functional central limit theorem, and a finite-sample deviation bound that matches the Monte Carlo rate up to regularization. Building on these findings, we introduce a joint Wald-type statistic to test for shape constraints over finite grids. The approach comes with an efficient computational procedure based on a pivoted Cholesky factorization, facilitating scalability to large datasets. Empirical tests suggest favorably of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16745v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Sen</dc:creator>
    </item>
  </channel>
</rss>
