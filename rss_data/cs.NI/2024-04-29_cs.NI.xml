<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Benefits of Coding for Network Slicing</title>
      <link>https://arxiv.org/abs/2404.17686</link>
      <description>arXiv:2404.17686v1 Announce Type: new 
Abstract: Network slicing has emerged as an integral concept in 5G, aiming to partition the physical network infrastructure into isolated slices, customized for specific applications. We theoretically formulate the key performance metrics of an application, in terms of goodput and delivery delay, at a cost of network resources in terms of bandwidth. We explore an un-coded communication protocol that uses feedback-based repetitions, and a coded protocol, implementing random linear network coding and using coding-aware acknowledgments. We find that coding reduces the resource demands of a slice to meet the requirements for an application, thereby serving more applications efficiently. Coded slices thus free up resources for other slices, be they coded or not. Based on these results, we propose a hybrid approach, wherein coding is introduced selectively in certain network slices. This approach not only facilitates a smoother transition from un-coded systems to coded systems but also reduces costs across all slices. Theoretical findings in this paper are validated and expanded upon through real-time simulations of the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17686v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Homa Esfahanizadeh, Vipindev Adat Vasudevan, Benjamin D. Kim, Shruti Siva, Jennifer Kim, Alejandro Cohen, Muriel M\'edard</dc:creator>
    </item>
    <item>
      <title>Generative AI for Low-Carbon Artificial Intelligence of Things</title>
      <link>https://arxiv.org/abs/2404.18077</link>
      <description>arXiv:2404.18077v1 Announce Type: new 
Abstract: By integrating Artificial Intelligence (AI) with the Internet of Things (IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields. However, AIoT is facing the challenges of energy consumption and carbon emissions due to the continuous advancement of mobile technology. Fortunately, Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT due to its excellent reasoning and generation capabilities. In this article, we explore the potential of GAI for carbon emissions reduction and propose a novel GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main impacts that cause carbon emissions in AIoT, and then introduce GAI techniques and their relations to carbon emissions. We then explore the application prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon emissions of network components. Subsequently, we propose a Large Language Model (LLM)-enabled carbon emission optimization framework, in which we design pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more accurate and reliable optimization problems. Furthermore, we utilize Generative Diffusion Models (GDMs) to identify optimal strategies for carbon emission reduction. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we insightfully provide open research directions for low-carbon AIoT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18077v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Hongyang Du, Yang Zhang, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Age-minimal Multicast by Graph Attention Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.18084</link>
      <description>arXiv:2404.18084v1 Announce Type: new 
Abstract: Age of Information (AoI) is an emerging metric used to assess the timeliness of information, gaining research interest in real-time multicast applications such as video streaming and metaverse platforms. In this paper, we consider a dynamic multicast network with energy constraints, where our objective is to minimize the expected time-average AoI through energy-constrained multicast routing and scheduling. The inherent complexity of the problem, given the NP-hardness and intertwined scheduling and routing decisions, makes existing approaches inapplicable. To address these challenges, we decompose the original problem into two subtasks, each amenable to reinforcement learning (RL) methods. Subsequently, we propose an innovative framework based on graph attention networks (GATs) to effectively capture graph information with superior generalization capabilities. To validate our framework, we conduct experiments on three datasets including a real-world dataset called AS-733, and show that our proposed scheme reduces the energy consumption by $75.7\%$ while achieving a similar AoI compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18084v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanning Zhang, Guocheng Liao, Shengbin Cao, Ning Yang, Meng Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Resource Sharing in Network Slicing via Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2404.18254</link>
      <description>arXiv:2404.18254v1 Announce Type: new 
Abstract: In network slicing, the network operator needs to satisfy the service level agreements of multiple slices at the same time and on the same physical infrastructure. To do so with reduced provisioned resources, the operator may consider resource sharing mechanisms. However, each slice then becomes susceptible to traffic surges in other slices which degrades performance isolation. To maintain both high efficiency and high isolation, we propose the introduction of hypothesis testing in resource sharing. Our approach comprises two phases. In the trial phase, the operator obtains a stochastic model for each slice that describes its normal behavior, provisions resources and then signs the service level agreements. In the regular phase, whenever there is resource contention, hypothesis testing is conducted to check which slices follow their normal behavior. Slices that fail the test are excluded from resource sharing to protect the well-behaved ones. We test our approach on a mobile traffic dataset. Results show that our approach fortifies the service level agreements against unexpected traffic patterns and achieves high efficiency via resource sharing. Overall, our approach provides an appealing tradeoff between efficiency and isolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18254v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Nikolaidis, John Baras</dc:creator>
    </item>
    <item>
      <title>Multi-Link Operation and Wireless Digital Twin to Support Enhanced Roaming in Next-Gen Wi-Fi</title>
      <link>https://arxiv.org/abs/2404.18313</link>
      <description>arXiv:2404.18313v1 Announce Type: new 
Abstract: The next generation of Wi-Fi is meant to achieve ultra-high reliability for wireless communication. Several approaches are available to this extent, some of which are being considered for inclusion in standards specifications, including coordination of access points to reduce interference.
  In this paper, we propose a centralized architecture based on digital twins, called WiTwin, with the aim of supporting wireless stations in selecting the optimal association according to a set of parameters. Unlike prior works, we assume that Wi-Fi 7 features like multi-link operation (MLO) are available. Moreover, one of the main goals of this architecture is to preserve communication quality in the presence of mobility, by helping stations to perform reassociation at the right time and in the best way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18313v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>20th IEEE International Conference on Factory Communication Systems (WFCS 2024)</arxiv:journal_reference>
      <dc:creator>Stefano Scanzio, Matteo Rosani, Gabriele Formis, Dave Cavalcanti, Valerio Frascolla, Guido Marchetto, Gianluca Cena</dc:creator>
    </item>
    <item>
      <title>6G comprehensive intelligence: network operations and optimization based on Large Language Models</title>
      <link>https://arxiv.org/abs/2404.18373</link>
      <description>arXiv:2404.18373v1 Announce Type: new 
Abstract: The sixth generation mobile communication standard (6G) can promote the development of Industrial Internet and Internet of Things (IoT). To achieve comprehensive intelligent development of the network and provide customers with higher quality personalized services. This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system. The Large Language Model, with more parameters and stronger learning ability, can more accurately capture patterns and features in data, which can achieve more accurate content output and high intelligence and provide strong support for related research such as network data security, privacy protection, and health assessment. This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18373v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sifan Long, Fengxiao Tang, Yangfan Li, Tiao Tan, Zhengjie Jin, Ming Zhao, Nei Kato</dc:creator>
    </item>
    <item>
      <title>Network Intent Decomposition and Optimization for Energy-Aware Radio Access Network</title>
      <link>https://arxiv.org/abs/2404.18386</link>
      <description>arXiv:2404.18386v1 Announce Type: new 
Abstract: With recent advancements in the sixth generation (6G) communication technologies, more vertical industries have encountered diverse network services. How to reduce energy consumption is critical to meet the expectation of the quality of diverse network services. In particular, the number of base stations in 6G is huge with coupled adjustable network parameters. However, the problem is complex with multiple network objectives and parameters. Network intents are difficult to map to individual network elements and require enhanced automation capabilities. In this paper, we present a network intent decomposition and optimization mechanism in an energy-aware radio access network scenario. By characterizing the intent ontology with a standard template, we present a generic network intent representation framework. Then we propose a novel intent modeling method using Knowledge Acquisition in automated Specification language, which can model the network ontology. To clarify the number and types of network objectives and energy-saving operations, we develop a Softgoal Interdependency Graph-based network intent decomposition model, and thus, a network intent decomposition algorithm is presented. Simulation results demonstrate that the proposed algorithm outperforms without conflict analysis in intent decomposition time. Moreover, we design a deep Q-network-assisted intent optimization scheme to validate the performance gain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18386v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Wang, Yijun Yu, Yexing Li, Dong Li, Xiaoxue Zhao, Chungang Yang</dc:creator>
    </item>
    <item>
      <title>Decomposition Model Assisted Energy-Saving Design in Radio Access Network</title>
      <link>https://arxiv.org/abs/2404.18418</link>
      <description>arXiv:2404.18418v1 Announce Type: new 
Abstract: The continuous emergence of novel services and massive connections involve huge energy consumption towards ultra-dense radio access networks. Moreover, there exist much more number of controllable parameters that can be adjusted to reduce the energy consumption from a network-wide perspective. However, a network-level energy-saving intent usually contains multiple network objectives and constraints. Therefore, it is critical to decompose a network-level energy-saving intent into multiple levels of configurated operations from a top-down refinement perspective. In this work, we utilize a softgoal interdependency graph decomposition model to assist energy-saving scheme design. Meanwhile, we propose an energy-saving approach based on deep Q-network, which achieve a better trade-off among the energy consumption, the throughput, and the first packet delay. In addition, we illustrate how the decomposition model can assist in making energy-saving decisions. Evaluation results demonstrate the performance gain of the proposed scheme in accelerating the model training process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18418v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxue Zhao, Yijun Yu, Yexing Li, Dong Li, Yao Wang, Chungang Yang</dc:creator>
    </item>
    <item>
      <title>Mobile Networks on the Move: Optimizing Moving Base Stations Dynamics in Urban Scenarios</title>
      <link>https://arxiv.org/abs/2404.18476</link>
      <description>arXiv:2404.18476v1 Announce Type: new 
Abstract: Base station densification is one of the key approaches for delivering high capacity in radio access networks. However, current static deployments are often impractical and financially unsustainable, as they increase both capital and operational expenditures of the network. An alternative paradigm is the moving base stations (MBSs) approach, by which part of base stations are installed on vehicles. However, to the best of our knowledge, it is still unclear if and up to which point MBSs allow decreasing the number of static base stations (BSs) deployed in urban settings. In this work, we start tackling this issue by proposing a modeling approach for a first-order evaluation of potential infrastructure savings enabled by the MBSs paradigm. Starting from a set of stochastic geometry results, and a traffic demand profile over time, we formulate an optimization problem for the derivation of the optimal combination of moving and static BSs which minimizes the overall amount of BSs deployed, while guaranteeing a target mean QoS for users. Initial results on a two-district scenario with measurement-based network traffic profiles suggest that substantial infrastructure savings are achievable. We show that these results are robust against different values of user density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18476v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laura Finarelli, Falko Dressler, Marco Marsan Ajmone, Gianluca Rizzo</dc:creator>
    </item>
    <item>
      <title>Implementation of Big AI Models for Wireless Networks with Collaborative Edge Computing</title>
      <link>https://arxiv.org/abs/2404.17766</link>
      <description>arXiv:2404.17766v1 Announce Type: cross 
Abstract: Big Artificial Intelligence (AI) models have emerged as a crucial element in various intelligent applications at the edge, such as voice assistants in smart homes and autonomous robotics in smart factories. Training big AI models, e.g., for personalized fine-tuning and continual model refinement, poses significant challenges to edge devices due to the inherent conflict between limited computing resources and intensive workload associated with training. Despite the constraints of on-device training, traditional approaches usually resort to aggregating training data and sending it to a remote cloud for centralized training. Nevertheless, this approach is neither sustainable, which strains long-range backhaul transmission and energy-consuming datacenters, nor safely private, which shares users' raw data with remote infrastructures. To address these challenges, we alternatively observe that prevalent edge environments usually contain a diverse collection of trusted edge devices with untapped idle resources, which can be leveraged for edge training acceleration. Motivated by this, in this article, we propose collaborative edge training, a novel training mechanism that orchestrates a group of trusted edge devices as a resource pool for expedited, sustainable big AI model training at the edge. As an initial step, we present a comprehensive framework for building collaborative edge training systems and analyze in-depth its merits and sustainable scheduling choices following its workflow. To further investigate the impact of its parallelism design, we empirically study a case of four typical parallelisms from the perspective of energy demand with realistic testbeds. Finally, we discuss open challenges for sustainable collaborative edge training to point to future directions of edge-centric big AI model training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17766v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liekang Zeng, Shengyuan Ye, Xu Chen, Yang Yang</dc:creator>
    </item>
    <item>
      <title>Joint Spectrum Partitioning and Power Allocation for Energy Efficient Semi-Integrated Sensing and Communications</title>
      <link>https://arxiv.org/abs/2404.18187</link>
      <description>arXiv:2404.18187v1 Announce Type: cross 
Abstract: With spectrum resources becoming congested and the emergence of sensing-enabled wireless applications, conventional resource allocation methods need a revamp to support communications-only, sensing-only, and integrated sensing and communication (ISaC) services together. In this letter, we propose two joint spectrum partitioning (SP) and power allocation (PA) schemes to maximize the aggregate sensing and communication performance as well as corresponding energy efficiency (EE) of a semi-ISaC system that supports all three services in a unified manner. The proposed framework captures the priority of the distinct services, impact of target clutters, power budget and bandwidth constraints, and sensing and communication quality-of-service (QoS) requirements. We reveal that the former problem is jointly convex and the latter is a non-convex problem that can be solved optimally by exploiting fractional and parametric programming techniques. Numerical results verify the effectiveness of proposed schemes and extract novel insights related to the impact of the priority and QoS requirements of distinct services on the performance of semi-ISaC networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18187v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ammar Mohamed Abouelmaati, Sylvester Aboagye, Hina Tabassum</dc:creator>
    </item>
    <item>
      <title>Quantum Backbone Networks for Hybrid Quantum Dataframe Transmission</title>
      <link>https://arxiv.org/abs/2404.18521</link>
      <description>arXiv:2404.18521v1 Announce Type: cross 
Abstract: To realize a global quantum Internet, there is a need for communication between quantum subnetworks. To accomplish this task, there have been multiple design proposals for a quantum backbone network and quantum subnetworks. In this work, we elaborate on the design that uses entanglement and quantum teleportation to build the quantum backbone between packetized quantum networks. We design a network interface to interconnect packetized quantum networks with entanglement-based quantum backbone networks and, moreover, design a scheme to accomplish data transmission over this hybrid quantum network model. We analyze the use of various implementations of the backbone network, focusing our study on backbone networks that use satellite links to continuously distribute entanglement resources. For feasibility, we analyze various system parameters via simulation to benchmark the performance of the overall network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18521v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Vista, Daniel Holme, Stephen DiAdamo</dc:creator>
    </item>
    <item>
      <title>Quantum Semantic Communications for Resource-Efficient Quantum Networking</title>
      <link>https://arxiv.org/abs/2205.02422</link>
      <description>arXiv:2205.02422v3 Announce Type: replace 
Abstract: Quantum communication networks (QCNs) utilize quantum mechanics for secure information transmission, but the reliance on fragile and expensive photonic quantum resources renders QCN resource optimization challenging. Unlike prior QCN works that relied on blindly compressing direct quantum embeddings of classical data, this letter proposes a novel quantum semantic communications (QSC) framework exploiting advancements in quantum machine learning and quantum semantic representations to extracts and embed only the relevant information from classical data into minimal high-dimensional quantum states that are accurately communicated over quantum channels with quantum communication and semantic fidelity measures. Simulation results indicate that, compared to semantic-agnostic QCN schemes, the proposed framework achieves approximately 50-75% reduction in quantum communication resources needed, while achieving a higher quantum semantic fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.02422v3</guid>
      <category>cs.NI</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Chehimi, Christina Chaccour, Christo Kurisummoottil Thomas, Walid Saad</dc:creator>
    </item>
    <item>
      <title>Data-driven Bandwidth Adaptation for Radio Access Network Slices</title>
      <link>https://arxiv.org/abs/2311.17347</link>
      <description>arXiv:2311.17347v2 Announce Type: replace 
Abstract: The need to satisfy the QoS requirements of multiple network slices deployed at the same base station poses a major challenge to network operators. The problem becomes even harder when the desired QoS involves packet delays. In that case, network utility maximization is not directly applicable since the utilities of the slices are unknown. As a result, most related works learn online the utilities of all slices and how to split the resources among them. Unfortunately, this approach does not scale well for many slices. Instead, it is needed to perform learning separately for each slice. To this end, we develop a bandwidth demand estimator; a network function that periodically receives as input the traffic of the slice and outputs the amount of bandwidth that its MAC scheduler needs to deliver the desired QoS. We develop the bandwidth demand estimator for QoS involving packet delay metrics based on a model-based reinforcement learning algorithm. We implement the algorithm on a cellular testbed and conduct experiments with time-varying traffic loads. Results show that the algorithm delivers the desired QoS but with significantly less bandwidth than non-adaptive approaches and other baseline online learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17347v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Nikolaidis, Asim Zoulkarni, John Baras</dc:creator>
    </item>
    <item>
      <title>Monitoring the Venice Lagoon: an IoT Cloud-Based Sensor Nerwork Approach</title>
      <link>https://arxiv.org/abs/2403.06915</link>
      <description>arXiv:2403.06915v2 Announce Type: replace 
Abstract: Monitoring the coastal area of the Venice Lagoon is of significant importance. While the impact of global warming is felt worldwide, coastal and littoral regions bear the brunt more prominently. These areas not only face the threat of rising sea levels but also contend with the escalating occurrence of seaquakes and floods. Additionally, the intricate ecosystems of rivers, seas, and lakes undergo profound transformations due to climate change and pollutants.
  Employing devices like the SENSWICH floating wireless sensor presented in this article and similar measurement instruments proves invaluable to automate environmental monitoring, hence eliminating the need for manual sampling campaigns. The utilization of wireless measurement devices offers cost-effectiveness, real-time analysis, and a reduction in human resource requirements. Storing data in cloud services further enhances the ability to monitor parameter changes over extended time intervals.
  In this article, we present an enhanced sensing device aimed at automating water quality assessment, while considering power consumption and reducing circuit complexity. Specifically, we will introduce the new schematic and circuit of SENSWICH which had changes in circuit and electronic aspects. Furthermore, we outline the methodology for aggregating data in a cloud service environment, such as Amazon Web Service (AWS), and using Grafana for visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06915v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Campagnaro, Matin Ghalkhani, Riccardo Tumiati, Federico Marin, Matteo Del Grande, Alessandro Pozzebon, Davide De Battisti, Roberto Francescon, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed &amp; Timed Transport with Recycled Entropies</title>
      <link>https://arxiv.org/abs/2404.01630</link>
      <description>arXiv:2404.01630v2 Announce Type: replace 
Abstract: With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale. ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong fairness. We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair. We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments. At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver. We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links. Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01630v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Bonato, Abdul Kabbani, Daniele De Sensi, Rong Pan, Yanfang Le, Costin Raiciu, Mark Handley, Timo Schneider, Nils Blach, Ahmad Ghalayini, Daniel Alves, Michael Papamichael, Adrian Caulfield, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Networking: Workflow, Advances and Challenges</title>
      <link>https://arxiv.org/abs/2404.12901</link>
      <description>arXiv:2404.12901v2 Announce Type: replace 
Abstract: The networking field is characterized by its high complexity and rapid iteration, requiring extensive expertise to accomplish network tasks, ranging from network design, configuration, diagnosis and security. The inherent complexity of these tasks, coupled with the ever-changing landscape of networking technologies and protocols, poses significant hurdles for traditional machine learning-based methods. These methods often struggle to generalize and automate complex tasks in networking, as they require extensive labeled data, domain-specific feature engineering, and frequent retraining to adapt to new scenarios. However, the recent emergence of large language models (LLMs) has sparked a new wave of possibilities in addressing these challenges. LLMs have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. These models, trained on extensive data, can benefit the networking domain. Some efforts have already explored the application of LLMs in the networking domain and revealed promising results. By reviewing recent advances, we present an abstract workflow to describe the fundamental process involved in applying LLM for Networking. We introduce the highlights of existing works by category and explain in detail how they operate at different stages of the workflow. Furthermore, we delve into the challenges encountered, discuss potential solutions, and outline future research prospects. We hope that this survey will provide insight for researchers and practitioners, promoting the development of this interdisciplinary research field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12901v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Liu, Xiaohui Xie, Xinggong Zhang, Yong Cui</dc:creator>
    </item>
  </channel>
</rss>
