<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps</title>
      <link>https://arxiv.org/abs/2510.03286</link>
      <description>arXiv:2510.03286v1 Announce Type: new 
Abstract: Cognitive maps provide a powerful framework for understanding spatial and abstract reasoning in biological and artificial agents. While recent computational models link cognitive maps to hippocampal-entorhinal mechanisms, they often rely on global optimization rules (e.g., backpropagation) that lack biological plausibility. In this work, we propose a novel cognitive architecture for structuring episodic memories into cognitive maps using local, Hebbian-like learning rules, compatible with neural substrate constraints. Our model integrates the Successor Features framework with episodic memories, enabling incremental, online learning through agent-environment interaction. We demonstrate its efficacy in a partially observable grid-world, where the architecture autonomously organizes memories into structured representations without centralized optimization. This work bridges computational neuroscience and AI, offering a biologically grounded approach to cognitive map formation in artificial adaptive agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03286v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. A. Dzhivelikian, A. I. Panov</dc:creator>
    </item>
    <item>
      <title>Stability of Fractional-Order Discrete-Time Systems with Application to Rulkov Neural Networks and Asymmetric Memristor Synapses</title>
      <link>https://arxiv.org/abs/2510.03304</link>
      <description>arXiv:2510.03304v1 Announce Type: new 
Abstract: Memristors have emerged as ideal components for modeling synaptic connections in neural networks due to their ability to emulate synaptic plasticity and memory effects. Discrete models of memristor-coupled neurons are crucial for simplifying computations and efficiently analyzing large-scale neural networks. Furthermore, incorporating fractional-order calculus into discrete models enhances their capacity to capture the memory and hereditary properties inherent in biological neurons, thus reducing numerical discretization errors compared to integer-order models. Despite this potential, discrete fractional-order neural models coupled through memristors have received limited attention. To address this gap, we introduce two novel discrete fractional-order neural systems. The first system consists of two Rulkov neurons coupled via dual memristors to emulate synaptic functions. The second system expands this configuration into a ring-shaped network of neurons consisting of multiple similar subnetworks. We present a novel theorem that defines stability regions for discrete fractional-order systems, applicable to both proposed models. Integrating discrete fractional-order calculus into memristor-coupled neural models provides a foundation for more accurate and efficient simulations of neural dynamics. This work advances the understanding of neural network stability and paves the way for future research into efficient neural computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03304v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leila Eftekhari, Moein Khalighi, Saeid Abbasbandy</dc:creator>
    </item>
    <item>
      <title>Atlas-free Brain Network Transformer</title>
      <link>https://arxiv.org/abs/2510.03306</link>
      <description>arXiv:2510.03306v1 Announce Type: new 
Abstract: Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predefined regions, and atlas-selection biases, collectively undermining the reliability and interpretability of the derived brain networks. To address these challenges, we propose a novel atlas-free brain network transformer (atlas-free BNT) that leverages individualized brain parcellations derived directly from subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel connectivity features in a standardized voxel-based feature space, which are subsequently processed using the BNT architecture to produce comparable subject-level embeddings. Experimental evaluations on sex classification and brain-connectome age prediction tasks demonstrate that our atlas-free BNT consistently outperforms state-of-the-art atlas-based methods, including elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach significantly improves the precision, robustness, and generalizability of brain network analyses. This advancement holds great potential to enhance neuroimaging biomarkers and clinical diagnostic tools for personalized precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03306v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Huang, Xuan Kan, James J. Lah, Deqiang Qiu</dc:creator>
    </item>
    <item>
      <title>Not Even Wrong: On the Limits of Prediction as Explanation in Cognitive Science</title>
      <link>https://arxiv.org/abs/2510.03311</link>
      <description>arXiv:2510.03311v1 Announce Type: new 
Abstract: We offer a comment on the Centaur (Binz et al., 2025) transformer-based model of human behavior. In particular, Centaur was cast as a path towards unified theories of cognition. We offer a counter claim with supporting argument: Centaur is a path divergent from unified theories of cognition, one that moves towards a unified model of behavior sans cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03311v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Orr, Drew Cranford, Ken Ford, Kevin Gluck, Will Hancock, Christian Lebiere, Pete Pirolli, Frank Ritter, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>Model-Guided Microstimulation Steers Primate Visual Behavior</title>
      <link>https://arxiv.org/abs/2510.03684</link>
      <description>arXiv:2510.03684v1 Announce Type: new 
Abstract: Brain stimulation is a powerful tool for understanding cortical function and holds promise for therapeutic interventions in neuropsychiatric disorders. Initial visual prosthetics apply electric microstimulation to early visual cortex which can evoke percepts of simple symbols such as letters. However, these approaches are fundamentally limited by hardware constraints and the low-level representational properties of this cortical region. In contrast, higher-level visual areas encode more complex object representations and therefore constitute a promising target for stimulation - but determining representational targets that reliably evoke object-level percepts constitutes a major challenge. We here introduce a computational framework to causally model and guide stimulation of high-level cortex, comprising three key components: (1) a perturbation module that translates microstimulation parameters into spatial changes to neural activity, (2) topographic models that capture the spatial organization of cortical neurons and thus enable prototyping of stimulation experiments, and (3) a mapping procedure that links model-optimized stimulation sites back to primate cortex. Applying this framework in two macaque monkeys performing a visual recognition task, model-predicted stimulation experiments produced significant in-vivo changes in perceptual choices. Per-site model predictions and monkey behavior were strongly correlated, underscoring the promise of model-guided stimulation. Image generation further revealed a qualitative similarity between in-silico stimulation of face-selective sites and a patient's report of facephenes. This proof-of-principle establishes a foundation for model-guided microstimulation and points toward next-generation visual prosthetics capable of inducing more complex visual experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03684v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Mehrer, Ben Lonnqvist, Anna Mitola, Abdulkadir Gokce, Paolo Papale, Martin Schrimpf</dc:creator>
    </item>
    <item>
      <title>Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents</title>
      <link>https://arxiv.org/abs/2510.03699</link>
      <description>arXiv:2510.03699v1 Announce Type: new 
Abstract: Larval zebrafish hunting provides a tractable setting to study how ecological and energetic constraints shape adaptive behavior in both biological brains and artificial agents. Here we develop a minimal agent-based model, training recurrent policies with deep reinforcement learning in a bout-based zebrafish simulator. Despite its simplicity, the model reproduces hallmark hunting behaviors -- including eye vergence-linked pursuit, speed modulation, and stereotyped approach trajectories -- that closely match real larval zebrafish. Quantitative trajectory analyses show that pursuit bouts systematically reduce prey angle by roughly half before strike, consistent with measurements. Virtual experiments and parameter sweeps vary ecological and energetic constraints, bout kinematics (coupled vs. uncoupled turns and forward motion), and environmental factors such as food density, food speed, and vergence limits. These manipulations reveal how constraints and environments shape pursuit dynamics, strike success, and abort rates, yielding falsifiable predictions for neuroscience experiments. These sweeps identify a compact set of constraints -- binocular sensing, the coupling of forward speed and turning in bout kinematics, and modest energetic costs on locomotion and vergence -- that are sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors arise in minimal agents without detailed biomechanics, fluid dynamics, circuit realism, or imitation learning from real zebrafish data. Taken together, this work provides a normative account of zebrafish hunting as the optimal balance between energetic cost and sensory benefit, highlighting the trade-offs that structure vergence and trajectory dynamics. We establish a virtual lab that narrows the experimental search space and generates falsifiable predictions about behavior and neural coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03699v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Raaghav Malik, Satpreet H. Singh, Sonja Johnson-Yu, Nathan Wu, Roy Harpaz, Florian Engert, Kanaka Rajan</dc:creator>
    </item>
    <item>
      <title>Intrinsic cause-effect power: the tradeoff between differentiation and specification</title>
      <link>https://arxiv.org/abs/2510.03881</link>
      <description>arXiv:2510.03881v1 Announce Type: new 
Abstract: Integrated information theory (IIT) starts from the existence of consciousness and characterizes its essential properties: every experience is intrinsic, specific, unitary, definite, and structured. IIT then formulates existence and its essential properties operationally in terms of cause-effect power of a substrate of units. Here we address IIT's operational requirements for existence by considering that, to have cause-effect power, to have it intrinsically, and to have it specifically, substrate units in their actual state must both (i) ensure the intrinsic availability of a repertoire of cause-effect states, and (ii) increase the probability of a specific cause-effect state. We showed previously that requirement (ii) can be assessed by the intrinsic difference of a state's probability from maximal differentiation. Here we show that requirement (i) can be assessed by the intrinsic difference from maximal specification. These points and their consequences for integrated information are illustrated using simple systems of micro units. When applied to macro units and systems of macro units such as neural systems, a tradeoff between differentiation and specification is a necessary condition for intrinsic existence, i.e., for consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03881v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William G. P. Mayner, William Marshall, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>Bridging integrated information theory and the free-energy principle in living neuronal networks</title>
      <link>https://arxiv.org/abs/2510.04084</link>
      <description>arXiv:2510.04084v1 Announce Type: new 
Abstract: The relationship between Integrated Information Theory (IIT) and the Free-Energy Principle (FEP) remains unresolved, particularly with respect to how integrated information, proposed as the intrinsic substrate of consciousness, behaves within variational Bayesian inference. We investigated this issue using dissociated neuronal cultures, previously shown to perform perceptual inference consistent with the FEP. Repeated stimulation from hidden sources induced robust source selectivity: variational free energy (VFE) decreased across sessions, whereas accuracy and Bayesian surprise (complexity) increased. Network-level analyses revealed that a proxy measure of integrated information and the size of the main complex followed a hill-shaped trajectory, with informational cores organizing diverse neuronal activity. Across experiments, integrated information correlated strongly and positively with Bayesian surprise, modestly and heterogeneously with accuracy, and showed no significant relationship with VFE. The positive coupling between {\Phi} and Bayesian surprise likely reflects the diversity of activity observed in critical dynamics. These findings suggest that integrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency. The hill-shaped trajectory of {\Phi} during inference can be functionally interpreted as a transition from exploration to exploitation. This work provides empirical evidence linking the physical account of consciousness advanced by IIT with the functional perspective offered by the FEP, contributing to a unified framework for the mechanisms and adaptive roles of phenomenology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04084v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teruki Mayama, Sota Shimizu, Yuki Takano, Dai Akita, Hirokazu Takahashi</dc:creator>
    </item>
    <item>
      <title>Simultaneously Determining Regional Heterogeneity and Connection Directionality from Neural Activity and Symmetric Connection</title>
      <link>https://arxiv.org/abs/2510.04110</link>
      <description>arXiv:2510.04110v1 Announce Type: new 
Abstract: The spatiotemporal patterns of neural dynamics are jointly shaped by directed structural interactions and heterogeneous intrinsic features of the neural components. Despite well-developed methods for estimating directionality in network connections from network of homogeneous nodes, how local heterogeneity impacts on directionality estimation remains poorly understood. In particular, the role of excitatory-inhibitory interactions in shaping network directionality and how these interactions should be incorporated into reconstruction frameworks remain largely unexplored. Here, we present a novel reconstruction framework that simultaneously estimates effective heterogeneity across network nodes and asymmetric network connections from neural activity and symmetric connection, both are assessible in experimental data, validated using macaque cortical connectivity data and several circuit models. We found that the estimated local heterogeneity remains consistent across various forms of parameterized local circuit heterogeneity. Furthermore, we demonstrated and quantified how hidden local inhibitory populations only modify within-region connection strengths, elucidating the functional equivalence between dynamics of excitatory-inhibitory networks and purely observing excitatory networks when estimating effective heterogeneity and asymmetry. Finally, we demonstrated the sampling interval effect in estimating network interactions with respect to the sampling resolution. Together, our results not only provide a unified framework for evaluating relative functional contributions of local heterogeneity and asymmetry to overall system dynamics but also reveal the fundamental limitations and scaling principles in reconstructing neural circuit connectivity from experimental observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04110v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Chang, Zhuda Yang, Changsong Zhou</dc:creator>
    </item>
    <item>
      <title>The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities</title>
      <link>https://arxiv.org/abs/2510.04698</link>
      <description>arXiv:2510.04698v1 Announce Type: new 
Abstract: Understanding the representation of probability in the human mind has been of great interest to understanding human decision making. Classical paradoxes in decision making suggest that human perception distorts probability magnitudes. Previous accounts postulate a Probability Weighting Function that transforms perceived probabilities; however, its motivation has been debated. Recent work has sought to motivate this function in terms of noisy representations of probabilities in the human mind. Here, we present an account of the Probability Weighting Function grounded in rational inference over optimal decoding from noisy neural encoding of quantities. We show that our model accurately accounts for behavior in a lottery task and a dot counting task. It further accounts for adaptation to a bimodal short-term prior. Taken together, our results provide a unifying account grounding the human representation of probability in rational inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04698v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Tong, Thi Thu Uyen Hoang, Xue-Xin Wei, Michael Hahn</dc:creator>
    </item>
    <item>
      <title>On graphical domination for threshold-linear networks with recurrent excitation and global inhibition</title>
      <link>https://arxiv.org/abs/2510.05098</link>
      <description>arXiv:2510.05098v1 Announce Type: new 
Abstract: Graphical domination was first introduced in [1] in the context of combinatorial threshold-linear networks (CTLNs). There it was shown that when a domination relationship exists between a pair of vertices in a graph, certain fixed points in the corresponding CTLN can be ruled out. Here we prove two new theorems about graphical domination, and show that they apply to a significantly more general class of recurrent networks called generalized CTLNs (gCTLNs). Theorem 1 establishes that if a dominated node is removed from a network, the reduced network has exactly the same fixed points. Theorem 2 tells us that by iteratively removing dominated nodes from an initial graph $G$, the final (irreducible) graph $\widetilde{G}$ is unique. We also introduce another new family of TLNs, called E-I TLNs, consisting of $n$ excitatory nodes and a single inhibitory node providing global inhibition. We provide a concrete mapping between the parameters of gCTLNs and E-I TLNs built from the same graph such that corresponding networks have the same fixed points. We also show that Theorems 1 and 2 apply equally well to E-I TLNs, and that the dynamics of gCTLNs and E-I TLNs with the same underlying graph $G$ exhibit similar behavior that is well predicted by the fixed points of the reduced graph $\widetilde{G}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05098v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carina Curto</dc:creator>
    </item>
    <item>
      <title>Error correction in multiclass image classification of facial emotion on unbalanced samples</title>
      <link>https://arxiv.org/abs/2510.03337</link>
      <description>arXiv:2510.03337v1 Announce Type: cross 
Abstract: This paper considers the problem of error correction in multi-class classification of face images on unbalanced samples. The study is based on the analysis of a data frame containing images labeled by seven different emotional states of people of different ages. Particular attention is paid to the problem of class imbalance, in which some emotions significantly prevail over others. To solve the classification problem, a neural network model based on LSTM with an attention mechanism focusing on key areas of the face that are informative for emotion recognition is used. As part of the experiments, the model is trained on all possible configurations of subsets of six classes with subsequent error correction for the seventh class, excluded at the training stage. The results show that correction is possible for all classes, although the degree of success varies: some classes are better restored, others are worse. In addition, on the test sample, when correcting some classes, an increase in key quality metrics for small classes was recorded, which indicates the promise of the proposed approach in solving applied problems related to the search for rare events, for example, in anti-fraud systems. Thus, the proposed method can be effectively applied in facial expression analysis systems and in tasks requiring stable classification under skewed class distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03337v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey A. Lebedev, Victor B. Kazantsev, Sergey V. Stasenko</dc:creator>
    </item>
    <item>
      <title>D\'esentrelacement Fr\'equentiel Doux pour les Codecs Audio Neuronaux</title>
      <link>https://arxiv.org/abs/2510.03741</link>
      <description>arXiv:2510.03741v1 Announce Type: cross 
Abstract: While neural-based models have led to significant advancements in audio feature extraction, the interpretability of the learned representations remains a critical challenge. To address this, disentanglement techniques have been integrated into discrete neural audio codecs to impose structure on the extracted tokens. However, these approaches often exhibit strong dependencies on specific datasets or task formulations. In this work, we propose a disentangled neural audio codec that leverages spectral decomposition of time-domain signals to enhance representation interpretability. Experimental evaluations demonstrate that our method surpasses a state-of-the-art baseline in both reconstruction fidelity and perceptual quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03741v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beno\^it Gini\`es, Xiaoyu Bie, Olivier Fercoq, Ga\"el Richard</dc:creator>
    </item>
    <item>
      <title>Internal World Models as Imagination Networks in Cognitive Agents</title>
      <link>https://arxiv.org/abs/2510.04391</link>
      <description>arXiv:2510.04391v1 Announce Type: cross 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04391v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saurabh Ranjan, Brian Odegaard</dc:creator>
    </item>
    <item>
      <title>What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data</title>
      <link>https://arxiv.org/abs/2510.04984</link>
      <description>arXiv:2510.04984v1 Announce Type: cross 
Abstract: Electroencephalogram monitoring devices and online data repositories hold large amounts of data from individuals participating in research and medical studies without direct reference to personal identifiers. This paper explores what types of personal and health information have been detected and classified within task-free EEG data. Additionally, we investigate key characteristics of the collected resting-state and sleep data, in order to determine the privacy risks involved with openly available EEG data. We used Google Scholar, Web of Science and searched relevant journals to find studies which classified or detected the presence of various disorders and personal information in resting state and sleep EEG. Only English full-text peer-reviewed journal articles or conference papers about classifying the presence of medical disorders between individuals were included. A quality analysis carried out by 3 reviewers determined general paper quality based on specified evaluation criteria. In resting state EEG, various disorders including Autism Spectrum Disorder, Parkinson's disease, and alcohol use disorder have been classified with high classification accuracy, often requiring only 5 mins of data or less. Sleep EEG tends to hold classifiable information about sleep disorders such as sleep apnea, insomnia, and REM sleep disorder, but usually involve longer recordings or data from multiple sleep stages. Many classification methods are still developing but even today, access to a person's EEG can reveal sensitive personal health information. With an increasing ability of machine learning methods to re-identify individuals from their EEG data, this review demonstrates the importance of anonymization, and the development of improved tools for keeping study participants and medical EEG users' privacy safe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04984v1</guid>
      <category>cs.NE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>J. E. M. Scanlon, A. Pelzer, M. Gharleghi, K. C. Fuhrmeister, T. K\"ollmer, P. Aichroth, R. G\"oder, C. Hansen, K. I. Wolf</dc:creator>
    </item>
    <item>
      <title>Foveated Retinotopy Improves Classification and Localization in CNNs</title>
      <link>https://arxiv.org/abs/2402.15480</link>
      <description>arXiv:2402.15480v4 Announce Type: replace-cross 
Abstract: From a falcon detecting prey to humans recognizing faces, many species exhibit extraordinary abilities in rapid visual localization and classification. These are made possible by a specialized retinal region called the fovea, which provides high acuity at the center of vision while maintaining lower resolution in the periphery. This distinctive spatial organization, preserved along the early visual pathway through retinotopic mapping, is fundamental to biological vision, yet remains largely unexplored in machine learning. Our study investigates how incorporating foveated retinotopy may benefit deep convolutional neural networks (CNNs) in image classification tasks. By implementing a foveated retinotopic transformation in the input layer of standard ResNet models and re-training them, we maintain comparable classification accuracy while enhancing the network's robustness to scale and rotational perturbations. Although this architectural modification introduces increased sensitivity to fixation point shifts, we demonstrate how this apparent limitation becomes advantageous: variations in classification probabilities across different gaze positions serve as effective indicators for object localization. Our findings suggest that foveated retinotopic mapping encodes implicit knowledge about visual object geometry, offering an efficient solution to the visual search problem - a capability crucial for many living species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15480v4</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Nicolas J\'er\'emie, Emmanuel Dauc\'e, Laurent U Perrinet</dc:creator>
    </item>
    <item>
      <title>Deep Learning without Weight Symmetry</title>
      <link>https://arxiv.org/abs/2405.20594</link>
      <description>arXiv:2405.20594v2 Announce Type: replace-cross 
Abstract: Backpropagation, a foundational algorithm for training artificial neural networks, predominates in contemporary deep learning. Although highly successful, it is widely considered biologically implausible, because it relies on precise symmetry between feedforward and feedback weights to accurately propagate gradient signals that assign credit. The so-called weight transport problem concerns how biological brains learn to align feedforward and feedback paths while avoiding the non-biological transport of feedforward weights into feedback weights. To address this, several credit assignment algorithms, such as feedback alignment and the Kollen-Pollack rule, have been proposed. While they can achieve the desired weight alignment, these algorithms imply that if a neuron sends a feedforward synapse to another neuron, it should also receive an identical or at least partially correlated feedback synapse from the latter neuron, thereby forming a bidirectional connection. However, this idealized connectivity pattern contradicts experimental observations in the brain, a discrepancy we refer to as the weight symmetry problem. To address this challenge posed by considering biological constraints on connectivity, we introduce the Product Feedback Alignment (PFA) algorithm. We demonstrate that PFA can eliminate explicit weight symmetry entirely while closely approximating backpropagation and achieving comparable performance in deep convolutional networks. Our results offer a novel approach to solve the longstanding problem of credit assignment in the brain, leading to more biologically plausible learning in deep networks compared to previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20594v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ji-An, Marcus K. Benna</dc:creator>
    </item>
  </channel>
</rss>
