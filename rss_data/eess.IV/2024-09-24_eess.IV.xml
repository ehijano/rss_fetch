<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 01:58:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Implicit Neural Representation for Sparse-view Photoacoustic Computed Tomography</title>
      <link>https://arxiv.org/abs/2409.13696</link>
      <description>arXiv:2409.13696v1 Announce Type: new 
Abstract: High-quality imaging in photoacoustic computed tomography (PACT) usually requires a high-channel count system for dense spatial sampling around the object to avoid aliasing-related artefacts. To reduce system complexity, various image reconstruction approaches, such as model-based (MB) and deep learning based methods, have been explored to mitigate the artefacts associated with sparse-view acquisition. However, the explored methods formulated the reconstruction problem in a discrete framework, making it prone to measurement errors, discretization errors, and the extend of the ill-poseness of the problem scales with the discretization resolution. In this work, an implicit neural representation (INR) framework is proposed for image reconstruction in PACT with ring transducer arrays to address these issues. pecially, the initial heat distribution is represented as a continuous function of spatial coordinates using a multi-layer perceptron (MLP). The weights of the MLP are then determined by a training process in a self-supervised manner, by minimizing the errors between the measured and model predicted PA signals. After training, PA images can be mapped by feeding the coordinates to the network. Simulation and phantom experiments showed that the INR method performed best in preserving image fidelity and in artefacts suppression for the same acquisition condition, compared to universal back-projection and MB methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13696v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowei Yao, Shilong Cui, Haizhao Dai, Qing Wu, Youshen Xiao, Fei Gao, Jingyi Yu, Yuyao Zhang, Xiran Cai</dc:creator>
    </item>
    <item>
      <title>Efficient Classification of Histopathology Images</title>
      <link>https://arxiv.org/abs/2409.13720</link>
      <description>arXiv:2409.13720v1 Announce Type: new 
Abstract: This work addresses how to efficiently classify challenging histopathology images, such as gigapixel whole-slide images for cancer diagnostics with image-level annotation. We use images with annotated tumor regions to identify a set of tumor patches and a set of benign patches in a cancerous slide. Due to the variable nature of region of interest the tumor positive regions may refer to an extreme minority of the pixels. This creates an important problem during patch-level classification, where the majority of patches from an image labeled as 'cancerous' are actually tumor-free. This problem is different from semantic segmentation which associates a label to every pixel in an image, because after patch extraction we are only dealing with patch-level labels.Most existing approaches address the data imbalance issue by mitigating the data shortage in minority classes in order to prevent the model from being dominated by the majority classes. These methods include data re-sampling, loss re-weighting, margin modification, and data augmentation. In this work, we mitigate the patch-level class imbalance problem by taking a divide-and-conquer approach. First, we partition the data into sub-groups, and define three separate classification sub-problems based on these data partitions. Then, using an information-theoretic cluster-based sampling of deep image patch features, we sample discriminative patches from the sub-groups. Using these sampled patches, we build corresponding deep models to solve the new classification sub-problems. Finally, we integrate information learned from the respective models to make a final decision on the patches. Our result shows that the proposed approach can perform competitively using a very low percentage of the available patches in a given whole-slide image.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13720v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Iqbal Nouyed, Mary-Anne Hartley, Gianfranco Doretto, Donald A. Adjeroh</dc:creator>
    </item>
    <item>
      <title>AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble</title>
      <link>https://arxiv.org/abs/2409.13779</link>
      <description>arXiv:2409.13779v1 Announce Type: new 
Abstract: Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for diagnosing, managing, and planning treatment for various cancers. Developing reliable deep learning models for the segmentation of tumor lesions in PET/CT scans in a multi-tracer multicenter environment, is a critical area of research. Different tracers, such as Fluorodeoxyglucose (FDG) and Prostate-Specific Membrane Antigen (PSMA), have distinct physiological uptake patterns and data from different centers often vary in terms of acquisition protocols, scanner types, and patient populations. Because of this variability, it becomes more difficult to design reliable segmentation algorithms and generalization techniques due to variations in image quality and lesion detectability. To address this challenge, We trained a 3D Residual encoder U-Net within the no new U-Net framework, aiming to generalize the performance of automatic lesion segmentation of whole body PET/CT scans, across different tracers and clinical sites. Further, We explored several preprocessing techniques and ultimately settled on using the Total Segmentator to crop our training data. Additionally, we applied resampling during this process. During inference, we leveraged test-time augmentations and other post-processing techniques to enhance tumor lesion segmentation. Our team currently hold the top position in the Auto-PET III challenge and outperformed the challenge baseline model in the preliminary test set with Dice score of 0.9627.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13779v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanya Chutani, Saikiran Bonthu, Pranab Samanta, Nitin Singhal</dc:creator>
    </item>
    <item>
      <title>Reduced bit median quantization: A middle process for Efficient Image Compression</title>
      <link>https://arxiv.org/abs/2409.13789</link>
      <description>arXiv:2409.13789v1 Announce Type: new 
Abstract: Image compression techniques have made remarkable progress when it comes to file size reduction with a tolerable quality reduction; nonetheless, they are facing some challenges when it comes to applying more compression with the same perceptible quality or in accounting for specific use cases such as deep archive files and more efficient image transfers. Previous techniques have tried to solve the former problem by applying one specific or a combination of different algorithms. However, none of these methods were able to achieve additional file size reduction beyond a certain compression. We introduce Reduced Bit Median Quantization (RBMQ), a middle-process image compression technique designed to enhance file size reduction so that it can be stored with already existing file extension formats.in RBMQ by applying only the first step in which the quantization of valued further file size reduction can be achieved without a noticeable decrease in the image quality. Furthermore, more size reduction can be achieved by reducing the representing bits for the quantized values which can be optimal for deep archival storage or big-size image transfer in which the image quality is not suitable for the human eye since it is dark and dim but can be much efficient to interact with network and storage components later to be decoded to get the only quantized value image that almost the same quality with the original one. RBMQ introduces redundancy to the pixel values to be taken advantage of by existing compression techniques furthermore it introduces bit reduction from 8 to 5 bits for image file extensions such as jpeg which substantially reduces the file size to be used for JPEG file transfers and deep archive storage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13789v1</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fikresilase Wondmeneh Abebayew</dc:creator>
    </item>
    <item>
      <title>Unsupervised Learning of Multi-modal Affine Registration for PET/CT</title>
      <link>https://arxiv.org/abs/2409.13863</link>
      <description>arXiv:2409.13863v1 Announce Type: new 
Abstract: Affine registration plays a crucial role in PET/CT imaging, where aligning PET with CT images is challenging due to their respective functional and anatomical representations. Despite the significant promise shown by recent deep learning (DL)-based methods in various medical imaging applications, their application to multi-modal PET/CT affine registration remains relatively unexplored. This study investigates a DL-based approach for PET/CT affine registration. We introduce a novel method using Parzen windowing to approximate the correlation ratio, which acts as the image similarity measure for training DNNs in multi-modal registration. Additionally, we propose a multi-scale, instance-specific optimization scheme that iteratively refines the DNN-generated affine parameters across multiple image resolutions. Our method was evaluated against the widely used mutual information metric and a popular optimization-based technique from the ANTs package, using a large public FDG-PET/CT dataset with synthetic affine transformations. Our approach achieved a mean Dice Similarity Coefficient (DSC) of 0.870, outperforming the compared methods and demonstrating its effectiveness in multi-modal PET/CT image registration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13863v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyu Chen, Yihao Liu, Shuwen Wei, Aaron Carass, Yong Du</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation</title>
      <link>https://arxiv.org/abs/2409.13868</link>
      <description>arXiv:2409.13868v1 Announce Type: new 
Abstract: This paper introduces a novel deep-learning method for the automatic detection and segmentation of lung nodules, aimed at advancing the accuracy of early-stage lung cancer diagnosis. The proposed approach leverages a unique "Channel Squeeze U-Structure" that optimizes feature extraction and information integration across multiple semantic levels of the network. This architecture includes three key modules: shallow information processing, channel residual structure, and channel squeeze integration. These modules enhance the model's ability to detect and segment small, imperceptible, or ground-glass nodules, which are critical for early diagnosis. The method demonstrates superior performance in terms of sensitivity, Dice similarity coefficient, precision, and mean Intersection over Union (IoU). Extensive experiments were conducted on the Lung Image Database Consortium (LIDC) dataset using five-fold cross-validation, showing excellent stability and robustness. The results indicate that this approach holds significant potential for improving computer-aided diagnosis systems, providing reliable support for radiologists in clinical practice and aiding in the early detection of lung cancer, especially in resource-limited settings</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13868v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxiu Sui, Jiacheng Hu, Tong Zhou, Zibo Liu, Likang Wen, Junliang Du</dc:creator>
    </item>
    <item>
      <title>RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2409.13930</link>
      <description>arXiv:2409.13930v1 Announce Type: new 
Abstract: Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, leading to distorted CT images with artifacts. This results in an ill-posed problem known as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, our experiments are conducted on two different LACT datasets, i.e., ChromSTEM and C4KC-KiTS. Through extensive experiments, we show that by leveraging learned Mean-Reverting SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, RN-SDEs can restore high-quality images from severe degradation and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13930v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Guo, Santiago Lopez-Tapia, Wing Shun Li, Yunnan Wu, Marcelo Carignano, Vadim Backman, Vinayak P. Dravid, Aggelos K. Katsaggelos</dc:creator>
    </item>
    <item>
      <title>MSDet: Receptive Field Enhanced Multiscale Detection for Tiny Pulmonary Nodule</title>
      <link>https://arxiv.org/abs/2409.14028</link>
      <description>arXiv:2409.14028v1 Announce Type: new 
Abstract: Pulmonary nodules are critical indicators for the early diagnosis of lung cancer, making their detection essential for timely treatment. However, traditional CT imaging methods suffered from cumbersome procedures, low detection rates, and poor localization accuracy. The subtle differences between pulmonary nodules and surrounding tissues in complex lung CT images, combined with repeated downsampling in feature extraction networks, often lead to missed or false detections of small nodules. Existing methods such as FPN, with its fixed feature fusion and limited receptive field, struggle to effectively overcome these issues. To address these challenges, our paper proposed three key contributions: Firstly, we proposed MSDet, a multiscale attention and receptive field network for detecting tiny pulmonary nodules. Secondly, we proposed the extended receptive domain (ERD) strategy to capture richer contextual information and reduce false positives caused by nodule occlusion. We also proposed the position channel attention mechanism (PCAM) to optimize feature learning and reduce multiscale detection errors, and designed the tiny object detection block (TODB) to enhance the detection of tiny nodules. Lastly, we conducted thorough experiments on the public LUNA16 dataset, achieving state-of-the-art performance, with an mAP improvement of 8.8% over the previous state-of-the-art method YOLOv8. These advancements significantly boosted detection accuracy and reliability, providing a more effective solution for early lung cancer diagnosis. The code will be available at https://github.com/CaiGuoHui123/MSDet</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14028v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guohui Cai, Ying Cai, Zeyu Zhang, Daji Ergu, Yuanzhouhan Cao, Binbin Hu, Zhibin Liao, Yang Zhao</dc:creator>
    </item>
    <item>
      <title>$\chi$-sepnet: Deep neural network for magnetic susceptibility source separation</title>
      <link>https://arxiv.org/abs/2409.14030</link>
      <description>arXiv:2409.14030v2 Announce Type: new 
Abstract: Magnetic susceptibility source separation ($\chi$-separation), an advanced quantitative susceptibility mapping (QSM) method, enables the separate estimation of para- and diamagnetic susceptibility source distributions in the brain. The method utilizes reversible transverse relaxation (R2'=R2*-R2) to complement frequency shift information for estimating susceptibility source concentrations, requiring time-consuming data acquisition for R2 in addition R2*. To address this challenge, we develop a new deep learning network, $\chi$-sepnet, and propose two deep learning-based susceptibility source separation pipelines, $\chi$-sepnet-R2' for inputs with multi-echo GRE and multi-echo spin-echo, and $\chi$-sepnet-R2* for input with multi-echo GRE only. $\chi$-sepnet is trained using multiple head orientation data that provide streaking artifact-free labels, generating high-quality $\chi$-separation maps. The evaluation of the pipelines encompasses both qualitative and quantitative assessments in healthy subjects, and visual inspection of lesion characteristics in multiple sclerosis patients. The susceptibility source-separated maps of the proposed pipelines delineate detailed brain structures with substantially reduced artifacts compared to those from conventional regularization-based reconstruction methods. In quantitative analysis, $\chi$-sepnet-R2' achieves the best outcomes followed by $\chi$-sepnet-R2*, outperforming the conventional methods. When the lesions of multiple sclerosis patients are assessed, both pipelines report identical lesion characteristics in most lesions ($\chi$para: 99.6% and $\chi$dia: 98.4% out of 250 lesions). The $\chi$-sepnet-R2* pipeline, which only requires multi-echo GRE data, has demonstrated its potential to offer broad clinical and scientific applications, although further evaluations for various diseases and pathological conditions are necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14030v2</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjun Kim, Sooyeon Ji, Jiye Kim, Kyeongseon Min, Hwihun Jeong, Jonghyo Youn, Taechang Kim, Jinhee Jang, Berkin Bilgic, Hyeong-Geol Shin, Jongho Lee</dc:creator>
    </item>
    <item>
      <title>Window-based Channel Attention for Wavelet-enhanced Learned Image Compression</title>
      <link>https://arxiv.org/abs/2409.14090</link>
      <description>arXiv:2409.14090v1 Announce Type: new 
Abstract: Learned Image Compression (LIC) models have achieved superior rate-distortion performance than traditional codecs. Existing LIC models use CNN, Transformer, or Mixed CNN-Transformer as basic blocks. However, limited by the shifted window attention, Swin-Transformer-based LIC exhibits a restricted growth of receptive fields, affecting the ability to model large objects in the image. To address this issue, we incorporate window partition into channel attention for the first time to obtain large receptive fields and capture more global information. Since channel attention hinders local information learning, it is important to extend existing attention mechanisms in Transformer codecs to the space-channel attention to establish multiple receptive fields, being able to capture global correlations with large receptive fields while maintaining detailed characterization of local correlations with small receptive fields. We also incorporate the discrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for efficient frequency-dependent down-sampling and further enlarging receptive fields. Experiment results demonstrate that our method achieves state-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and 24.71% on four standard datasets compared to VTM-23.1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14090v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heng Xu, Bowen Hai, Yushun Tang, Zhihai He</dc:creator>
    </item>
    <item>
      <title>Accelerated Multi-Contrast MRI Reconstruction via Frequency and Spatial Mutual Learning</title>
      <link>https://arxiv.org/abs/2409.14113</link>
      <description>arXiv:2409.14113v1 Announce Type: new 
Abstract: To accelerate Magnetic Resonance (MR) imaging procedures, Multi-Contrast MR Reconstruction (MCMR) has become a prevalent trend that utilizes an easily obtainable modality as an auxiliary to support high-quality reconstruction of the target modality with under-sampled k-space measurements. The exploration of global dependency and complementary information across different modalities is essential for MCMR. However, existing methods either struggle to capture global dependency due to the limited receptive field or suffer from quadratic computational complexity. To tackle this dilemma, we propose a novel Frequency and Spatial Mutual Learning Network (FSMNet), which efficiently explores global dependencies across different modalities. Specifically, the features for each modality are extracted by the Frequency-Spatial Feature Extraction (FSFE) module, featuring a frequency branch and a spatial branch. Benefiting from the global property of the Fourier transform, the frequency branch can efficiently capture global dependency with an image-size receptive field, while the spatial branch can extract local features. To exploit complementary information from the auxiliary modality, we propose a Cross-Modal Selective fusion (CMS-fusion) module that selectively incorporate the frequency and spatial features from the auxiliary modality to enhance the corresponding branch of the target modality. To further integrate the enhanced global features from the frequency branch and the enhanced local features from the spatial branch, we develop a Frequency-Spatial fusion (FS-fusion) module, resulting in a comprehensive feature representation for the target modality. Extensive experiments on the BraTS and fastMRI datasets demonstrate that the proposed FSMNet achieves state-of-the-art performance for the MCMR task with different acceleration factors. The code is available at: https://github.com/qic999/FSMNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14113v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Chen, Xiaohan Xing, Zhen Chen, Zhiwei Xiong</dc:creator>
    </item>
    <item>
      <title>A Sinkhorn Regularized Adversarial Network for Image Guided DEM Super-resolution using Frequency Selective Hybrid Graph Transformer</title>
      <link>https://arxiv.org/abs/2409.14198</link>
      <description>arXiv:2409.14198v1 Announce Type: new 
Abstract: Digital Elevation Model (DEM) is an essential aspect in the remote sensing (RS) domain to analyze various applications related to surface elevations. Here, we address the generation of high-resolution (HR) DEMs using HR multi-spectral (MX) satellite imagery as a guide by introducing a novel hybrid transformer model consisting of Densely connected Multi-Residual Block (DMRB) and multi-headed Frequency Selective Graph Attention (M-FSGA). To promptly regulate this process, we utilize the notion of discriminator spatial maps as the conditional attention to the MX guide. Further, we present a novel adversarial objective related to optimizing Sinkhorn distance with classical GAN. In this regard, we provide both theoretical and empirical substantiation of better performance in terms of vanishing gradient issues and numerical convergence. Based on our experiments on 4 different DEM datasets, we demonstrate both qualitative and quantitative comparisons with available baseline methods and show that the performance of our proposed model is superior to others with sharper details and minimal errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14198v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Pattern Recognition (ICPR), 2024</arxiv:journal_reference>
      <dc:creator>Subhajit Paul, Ashutosh Gupta</dc:creator>
    </item>
    <item>
      <title>UniMo: Universal Motion Correction For Medical Images without Network Retraining</title>
      <link>https://arxiv.org/abs/2409.14204</link>
      <description>arXiv:2409.14204v1 Announce Type: new 
Abstract: In this paper, we introduce a Universal Motion Correction (UniMo) framework, leveraging deep neural networks to tackle the challenges of motion correction across diverse imaging modalities. Our approach employs advanced neural network architectures with equivariant filters, overcoming the limitations of current models that require iterative inference or retraining for new image modalities. UniMo enables one-time training on a single modality while maintaining high stability and adaptability for inference across multiple unseen image modalities. We developed a joint learning framework that integrates multimodal knowledge from both shape and images that faithfully improve motion correction accuracy despite image appearance variations. UniMo features a geometric deformation augmenter that enhances the robustness of global motion correction by addressing any local deformations whether they are caused by object deformations or geometric distortions, and also generates augmented data to improve the training process. Our experimental results, conducted on various datasets with four different image modalities, demonstrate that UniMo surpasses existing motion correction methods in terms of accuracy. By offering a comprehensive solution to motion correction, UniMo marks a significant advancement in medical imaging, especially in challenging applications with wide ranges of motion, such as fetal imaging. The code for this work is available online, https://github.com/IntelligentImaging/UNIMO/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14204v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Wang, Razieh Faghihpirayesh, Danny Joca, Polina Golland, Ali Gholipour</dc:creator>
    </item>
    <item>
      <title>FeDETR: a Federated Approach for Stenosis Detection in Coronary Angiography</title>
      <link>https://arxiv.org/abs/2409.14268</link>
      <description>arXiv:2409.14268v1 Announce Type: new 
Abstract: Assessing the severity of stenoses in coronary angiography is critical to the patient's health, as coronary stenosis is an underlying factor in heart failure. Current practice for grading coronary lesions, i.e. fractional flow reserve (FFR) or instantaneous wave-free ratio (iFR), suffers from several drawbacks, including time, cost and invasiveness, alongside potential interobserver variability. In this context, some deep learning methods have emerged to assist cardiologists in automating the estimation of FFR/iFR values. Despite the effectiveness of these methods, their reliance on large datasets is challenging due to the distributed nature of sensitive medical data. Federated learning addresses this challenge by aggregating knowledge from multiple nodes to improve model generalization, while preserving data privacy. We propose the first federated detection transformer approach, FeDETR, to assess stenosis severity in angiography videos based on FFR/iFR values estimation. In our approach, each node trains a detection transformer (DETR) on its local dataset, with the central server federating the backbone part of the network. The proposed method is trained and evaluated on a dataset collected from five hospitals, consisting of 1001 angiographic examinations, and its performance is compared with state-of-the-art federated learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14268v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-51026-7_17</arxiv:DOI>
      <dc:creator>Raffaele Mineo, Amelia Sorrenti, Federica Proietto Salanitri</dc:creator>
    </item>
    <item>
      <title>Thinking in Granularity: Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues</title>
      <link>https://arxiv.org/abs/2409.14330</link>
      <description>arXiv:2409.14330v1 Announce Type: new 
Abstract: Dynamic quantization has attracted rising attention in image super-resolution (SR) as it expands the potential of heavy SR models onto mobile devices while preserving competitive performance. Existing methods explore layer-to-bit configuration upon varying local regions, adaptively allocating the bit to each layer and patch. Despite the benefits, they still fall short in the trade-off of SR accuracy and quantization efficiency. Apart from this, adapting the quantization level for each layer individually can disturb the original inter-layer relationships, thus diminishing the representation capability of quantized models. In this work, we propose Granular-DQ, which capitalizes on the intrinsic characteristics of images while dispensing with the previous consideration for layer sensitivity in quantization. Granular-DQ conducts a multi-granularity analysis of local patches with further exploration of their information densities, achieving a distinctive patch-wise and layer-invariant dynamic quantization paradigm. Specifically, Granular-DQ initiates by developing a granularity-bit controller (GBC) to apprehend the coarse-to-fine granular representations of different patches, matching their proportional contribution to the entire image to determine the proper bit-width allocation. On this premise, we investigate the relation between bit-width and information density, devising an entropy-to-bit (E2B) mechanism that enables further fine-grained dynamic bit adaption of high-bit patches. Extensive experiments validate the superiority and generalization ability of Granular-DQ over recent state-of-the-art methods on various SR models. Code will be available at \url{https://github.com/MmmingS/Granular-DQ.git}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14330v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingshen Wang, Zhao Zhang, Feng Li, Ke Xu, Kang Miao, Meng Wang</dc:creator>
    </item>
    <item>
      <title>Quantitative and Qualitative Evaluation of NLM and Wavelet Methods in Image Enhancement</title>
      <link>https://arxiv.org/abs/2409.14334</link>
      <description>arXiv:2409.14334v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of image denoising techniques, primarily focusing on Non-local Means (NLM) and Daubechies Soft Wavelet Thresholding, and their efficacy across various datasets. These methods are applied to the CURE-OR, CURE-TSD, CURE-TSR, SSID, and Set-12 datasets, followed by an evaluation using Image Quality Assessment (IQA) metrics PSNR, SSIM, CW-SSIM, UNIQUE, MS-UNIQUE, CSV, and SUMMER. The results indicate that NLM and Wavelet Thresholding perform optimally on Set12 and SIDD datasets, attributed to their ability to effectively handle general additive and multiplicative noise masks. However, their performance on CURE datasets is limited due to the presence of complex distortions like Dirty Lens and Codec Error, which these methods are not well-suited to address. Analysis between NLM and Wavelet Thresholding shows that while NLM generally offers superior visual quality, Wavelet Thresholding excels in specific IQA metrics, particularly SUMMER, due to its enhancement in the frequency domain as opposed to NLM's spatial domain approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14334v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cameron Khanpour</dc:creator>
    </item>
    <item>
      <title>Frequency-regularized Neural Representation Method for Sparse-view Tomographic Reconstruction</title>
      <link>https://arxiv.org/abs/2409.14394</link>
      <description>arXiv:2409.14394v1 Announce Type: new 
Abstract: Sparse-view tomographic reconstruction is a pivotal direction for reducing radiation dose and augmenting clinical applicability. While many research works have proposed the reconstruction of tomographic images from sparse 2D projections, existing models tend to excessively focus on high-frequency information while overlooking low-frequency components within the sparse input images. This bias towards high-frequency information often leads to overfitting, particularly intense at edges and boundaries in the reconstructed slices. In this paper, we introduce the Frequency Regularized Neural Attenuation/Activity Field (Freq-NAF) for self-supervised sparse-view tomographic reconstruction. Freq-NAF mitigates overfitting by incorporating frequency regularization, directly controlling the visible frequency bands in the neural network input. This approach effectively balances high-frequency and low-frequency information. We conducted numerical experiments on CBCT and SPECT datasets, and our method demonstrates state-of-the-art accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14394v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingmou Xian, Jian Zhu, Haolin Liao, Si Li</dc:creator>
    </item>
    <item>
      <title>Real-time Detection and Auto focusing of Beam Profiles from Silicon Photonics Gratings using YOLO model</title>
      <link>https://arxiv.org/abs/2409.14413</link>
      <description>arXiv:2409.14413v1 Announce Type: new 
Abstract: When observing the chip-to-free-space light beams from silicon photonics (SiPh) to free-space, manual adjustment of camera lens is often required to obtain a focused image of the light beams. In this letter, we demonstrated an auto-focusing system based on you-only-look-once (YOLO) model. The trained YOLO model exhibits high classification accuracy of 99.7% and high confidence level &gt;0.95 when detecting light beams from SiPh gratings. A video demonstration of real-time light beam detection, real-time computation of beam width, and auto focusing of light beams are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14413v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Dian Lim, Hong Yu Li, Simon Chun Kiat Goh, Xiangyu Wang, Peng Zhao, Chuan Seng Tan</dc:creator>
    </item>
    <item>
      <title>Detection of pulmonary pathologies using convolutional neural networks, Data Augmentation, ResNet50 and Vision Transformers</title>
      <link>https://arxiv.org/abs/2409.14446</link>
      <description>arXiv:2409.14446v1 Announce Type: new 
Abstract: Pulmonary diseases are a public health problem that requires accurate and fast diagnostic techniques. In this paper, a method based on convolutional neural networks (CNN), Data Augmentation, ResNet50 and Vision Transformers (ViT) is proposed to detect lung pathologies from medical images. A dataset of X-ray images and CT scans of patients with different lung diseases, such as cancer, pneumonia, tuberculosis and fibrosis, is used. The results obtained by the proposed method are compared with those of other existing methods, using performance metrics such as accuracy, sensitivity, specificity and area under the ROC curve. The results show that the proposed method outperforms the other methods in all metrics, achieving an accuracy of 98% and an area under the ROC curve of 99%. It is concluded that the proposed method is an effective and promising tool for the diagnosis of pulmonary pathologies by medical imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14446v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Ramirez Amador, Dinarle Milagro Ortega, Arnold Cesarano</dc:creator>
    </item>
    <item>
      <title>Lesion Segmentation in Whole-Body Multi-Tracer PET-CT Images; a Contribution to AutoPET 2024 Challenge</title>
      <link>https://arxiv.org/abs/2409.14475</link>
      <description>arXiv:2409.14475v1 Announce Type: new 
Abstract: The automatic segmentation of pathological regions within whole-body PET-CT volumes has the potential to streamline various clinical applications such as diagno-sis, prognosis, and treatment planning. This study aims to address this challenge by contributing to the AutoPET MICCAI 2024 challenge through a proposed workflow that incorporates image preprocessing, tracer classification, and lesion segmentation steps. The implementation of this pipeline led to a significant enhancement in the segmentation accuracy of the models. This improvement is evidenced by an average overall Dice score of 0.548 across 1611 training subjects, 0.631 and 0.559 for classi-fied FDG and PSMA subjects of the training set, and 0.792 on the preliminary testing phase dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14475v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehdi Astaraki, Simone Bendazzoli</dc:creator>
    </item>
    <item>
      <title>Sampling-Pattern-Agnostic MRI Reconstruction through Adaptive Consistency Enforcement with Diffusion Model</title>
      <link>https://arxiv.org/abs/2409.14479</link>
      <description>arXiv:2409.14479v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) is a powerful, non-invasive diagnostic tool; however, its clinical applicability is constrained by prolonged acquisition times. Whilst present deep learning-based approaches have demonstrated potential in expediting MRI processes, these methods usually rely on known sampling patterns and exhibit limited generalisability to novel patterns. In the paper, we propose a sampling-pattern-agnostic MRI reconstruction method via a diffusion model through adaptive consistency enforcement. Our approach effectively reconstructs high-fidelity images with varied under-sampled acquisitions, generalising across contrasts and acceleration factors regardless of sampling trajectories. We train and validate across all contrasts in the MICCAI 2024 Cardiac MRI Reconstruction Challenge (CMRxRecon) dataset for the ``Random sampling CMR reconstruction'' task. Evaluation results indicate that our proposed method significantly outperforms baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14479v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anurag Malyala, Zhenlin Zhang, Chengyan Wang, Chen Qin</dc:creator>
    </item>
    <item>
      <title>TransUKAN:Computing-Efficient Hybrid KAN-Transformer for Enhanced Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2409.14676</link>
      <description>arXiv:2409.14676v1 Announce Type: new 
Abstract: U-Net is currently the most widely used architecture for medical image segmentation. Benefiting from its unique encoder-decoder architecture and skip connections, it can effectively extract features from input images to segment target regions. The commonly used U-Net is typically based on convolutional operations or Transformers, modeling the dependencies between local or global information to accomplish medical image analysis tasks. However, convolutional layers, fully connected layers, and attention mechanisms used in this process introduce a significant number of parameters, often requiring the stacking of network layers to model complex nonlinear relationships, which can impact the training process. To address these issues, we propose TransUKAN. Specifically, we have improved the KAN to reduce memory usage and computational load. On this basis, we explored an effective combination of KAN, Transformer, and U-Net structures. This approach enhances the model's capability to capture nonlinear relationships by introducing only a small number of additional parameters and compensates for the Transformer structure's deficiency in local information extraction. We validated TransUKAN on multiple medical image segmentation tasks. Experimental results demonstrate that TransUKAN achieves excellent performance with significantly reduced parameters. The code will be available athttps://github.com/wuyanlin-wyl/TransUKAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14676v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlin Wu, Tao Li, Zhihong Wang, Hong Kang, Along He</dc:creator>
    </item>
    <item>
      <title>Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images</title>
      <link>https://arxiv.org/abs/2409.14874</link>
      <description>arXiv:2409.14874v2 Announce Type: new 
Abstract: We explore the feasibility and potential of building a ground-truth-free evaluation model to assess the quality of segmentations generated by the Segment Anything Model (SAM) and its variants in medical imaging. This evaluation model estimates segmentation quality scores by analyzing the coherence and consistency between the input images and their corresponding segmentation predictions. Based on prior research, we frame the task of training this model as a regression problem within a supervised learning framework, using Dice scores (and optionally other metrics) along with mean squared error to compute the training loss. The model is trained utilizing a large collection of public datasets of medical images with segmentation predictions from SAM and its variants. We name this model EvanySeg (Evaluation of Any Segmentation in Medical Images). Our exploration of convolution-based models (e.g., ResNet) and transformer-based models (e.g., ViT) suggested that ViT yields better performance for this task. EvanySeg can be employed for various tasks, including: (1) identifying poorly segmented samples by detecting low-percentile segmentation quality scores; (2) benchmarking segmentation models without ground truth by averaging quality scores across test samples; (3) alerting human experts to poor-quality segmentation predictions during human-AI collaboration by applying a threshold within the score space; and (4) selecting the best segmentation prediction for each test sample at test time when multiple segmentation models are available, by choosing the prediction with the highest quality score. Models and code will be made available at https://github.com/ahjolsenbics/EvanySeg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14874v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahjol Senbi, Tianyu Huang, Fei Lyu, Qing Li, Yuhui Tao, Wei Shao, Qiang Chen, Chengyan Wang, Shuo Wang, Tao Zhou, Yizhe Zhang</dc:creator>
    </item>
    <item>
      <title>Compressive radio-interferometric sensing with random beamforming as rank-one signal covariance projections</title>
      <link>https://arxiv.org/abs/2409.15031</link>
      <description>arXiv:2409.15031v1 Announce Type: new 
Abstract: Radio-interferometry (RI) observes the sky at unprecedented angular resolutions, enabling the study of several far-away galactic objects such as galaxies and black holes. In RI, an array of antennas probes cosmic signals coming from the observed region of the sky. The covariance matrix of the vector gathering all these antenna measurements offers, by leveraging the Van Cittert-Zernike theorem, an incomplete and noisy Fourier sensing of the image of interest. The number of noisy Fourier measurements -- or visibilities -- scales as $\mathcal O(Q^2B)$ for $Q$ antennas and $B$ short-time integration (STI) intervals. We address the challenges posed by this vast volume of data, which is anticipated to increase significantly with the advent of large antenna arrays, by proposing a compressive sensing technique applied directly at the level of the antenna measurements. First, this paper shows that beamforming -- a common technique of dephasing antenna signals -- usually used to focus some region of the sky, is equivalent to sensing a rank-one projection (ROP) of the signal covariance matrix. We build upon our recent work arXiv:2306.12698v3 [eess.IV] to propose a compressive sensing scheme relying on random beamforming, trading the $Q^2$-dependence of the data size for a smaller number $N_{\mathrm p}$ ROPs. We provide image recovery guarantees for sparse image reconstruction. Secondly, the data size is made independent of $B$ by applying $N_{\mathrm m}$ Bernoulli modulations of the ROP vectors obtained for the STI. The resulting sample complexities, theoretically derived in a simpler case without modulations and numerically obtained in phase transition diagrams, are shown to scale as $\mathcal O(K)$ where $K$ is the image sparsity. This illustrates the potential of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15031v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Leblanc, Yves Wiaux, Laurent Jacques</dc:creator>
    </item>
    <item>
      <title>Towards Accountable AI-Assisted Eye Disease Diagnosis: Workflow Design, External Validation, and Continual Learning</title>
      <link>https://arxiv.org/abs/2409.15087</link>
      <description>arXiv:2409.15087v1 Announce Type: new 
Abstract: Timely disease diagnosis is challenging due to increasing disease burdens and limited clinician availability. AI shows promise in diagnosis accuracy but faces real-world application issues due to insufficient validation in clinical workflows and diverse populations. This study addresses gaps in medical AI downstream accountability through a case study on age-related macular degeneration (AMD) diagnosis and severity classification. We designed and implemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic performance with and without AI assistance among 24 clinicians from 12 institutions with real patient data sampled from the Age-Related Eye Disease Study (AREDS). Additionally, we demonstrated continual enhancement of an existing AI model by incorporating approximately 40,000 additional medical images (named AREDS2 dataset). The improved model was then systematically evaluated using both AREDS and AREDS2 test sets, as well as an external test set from Singapore. AI assistance markedly enhanced diagnostic accuracy and classification for 23 out of 24 clinicians, with the average F1-score increasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value &lt; 0.0001), achieving an improvement of over 50% in some cases. In terms of efficiency, AI assistance reduced diagnostic times for 17 out of the 19 clinicians tracked, with time savings of up to 40%. Furthermore, a model equipped with continual learning showed robust performance across three independent datasets, recording a 29% increase in accuracy, and elevating the F1-score from 42 to 54 in the Singapore population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15087v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyu Chen, Tiarnan D L Keenan, Elvira Agron, Alexis Allot, Emily Guan, Bryant Duong, Amr Elsawy, Benjamin Hou, Cancan Xue, Sanjeeb Bhandari, Geoffrey Broadhead, Chantal Cousineau-Krieger, Ellen Davis, William G Gensheimer, David Grasic, Seema Gupta, Luis Haddock, Eleni Konstantinou, Tania Lamba, Michele Maiberger, Dimosthenis Mantopoulos, Mitul C Mehta, Ayman G Nahri, Mutaz AL-Nawaflh, Arnold Oshinsky, Brittany E Powell, Boonkit Purt, Soo Shin, Hillary Stiefel, Alisa T Thavikulwat, Keith James Wroblewski, Tham Yih Chung, Chui Ming Gemmy Cheung, Ching-Yu Cheng, Emily Y Chew, Michelle R. Hribar, Michael F. Chiang, Zhiyong Lu</dc:creator>
    </item>
    <item>
      <title>MAR-DTN: Metal Artifact Reduction using Domain Transformation Network for Radiotherapy Planning</title>
      <link>https://arxiv.org/abs/2409.15155</link>
      <description>arXiv:2409.15155v1 Announce Type: new 
Abstract: For the planning of radiotherapy treatments for head and neck cancers, Computed Tomography (CT) scans of the patients are typically employed. However, in patients with head and neck cancer, the quality of standard CT scans generated using kilo-Voltage (kVCT) tube potentials is severely degraded by streak artifacts occurring in the presence of metallic implants such as dental fillings. Some radiotherapy devices offer the possibility of acquiring Mega-Voltage CT (MVCT) for daily patient setup verification, due to the higher energy of X-rays used, MVCT scans are almost entirely free from artifacts making them more suitable for radiotherapy treatment planning.
  In this study, we leverage the advantages of kVCT scans with those of MVCT scans (artifact-free). We propose a deep learning-based approach capable of generating artifact-free MVCT images from acquired kVCT images. The outcome offers the benefits of artifact-free MVCT images with enhanced soft tissue contrast, harnessing valuable information obtained through kVCT technology for precise therapy calibration. Our proposed method employs UNet-inspired model, and is compared with adversarial learning and transformer networks. This first and unique approach achieves remarkable success, with PSNR of 30.02 dB across the entire patient volume and 27.47 dB in artifact-affected regions exclusively. It is worth noting that the PSNR calculation excludes the background, concentrating solely on the region of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15155v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bel\'en Serrano-Ant\'on, Mubashara Rehman, Niki Martinel, Michele Avanzo, Riccardo Spizzo, Giuseppe Fanetti, Alberto P. Mu\~nuzuri, Christian Micheloni</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Automated Post-Operative Gross Tumor Volume Segmentation in Glioblastoma Patients</title>
      <link>https://arxiv.org/abs/2409.15177</link>
      <description>arXiv:2409.15177v1 Announce Type: new 
Abstract: Precise automated delineation of post-operative gross tumor volume in glioblastoma cases is challenging and time-consuming owing to the presence of edema and the deformed brain tissue resulting from the surgical tumor resection. To develop a model for automated delineation of post-operative gross tumor volumes in glioblastoma, we proposed a novel 3D double pocket U-Net architecture that has two parallel pocket U-Nets. Both U-Nets were trained simultaneously with two different subsets of MRI sequences and the output from the models was combined to do the final prediction. We strategically combined the MRI input sequences (T1, T2, T1C, FL) for model training to achieve improved segmentation accuracy. The dataset comprised 82 post-operative studies collected from 23 glioblastoma patients who underwent maximal safe tumor resection. All had gross tumor volume (GTV) segmentations performed by human experts, and these were used as a reference standard. The results of 3D double pocket U-Net were compared with baseline 3D pocket U-Net models and the ensemble of 3D pocket U-Net models. All the models were evaluated with fivefold cross-validation in terms of the Dice similarity coefficient and Hausdorff distance. Our proposed double U-Net model trained with input sequences [T1, T1C, FL + T2, T1C] achieved a better mean Dice score of 0.8585 and Hausdorff distance of 4.1942 compared to all the baseline models and ensemble models trained. The presence of infiltrating tumors and vasogenic edema in the post-operative MRI scans tends to reduce segmentation accuracy when considering the MRI sequences T1, T2, T1C, and FL together for model training. The double U-Net approach of combining subsets of the MRI sequences as distinct inputs for model training improves segmentation accuracy by 7% when compared with the conventional method of model training with all four sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15177v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajarajeswari Muthusivarajan, Adrian Celaya, Maguy Farhat, Wasif Talpur, Holly Langshaw, Victoria White, Andrew Elliott, Sara Thrower, Dawid Schellingerhout, David Fuentes, Caroline Chung</dc:creator>
    </item>
    <item>
      <title>Intrinsic Single-Image HDR Reconstruction</title>
      <link>https://arxiv.org/abs/2409.13803</link>
      <description>arXiv:2409.13803v1 Announce Type: cross 
Abstract: The low dynamic range (LDR) of common cameras fails to capture the rich contrast in natural scenes, resulting in loss of color and details in saturated pixels. Reconstructing the high dynamic range (HDR) of luminance present in the scene from single LDR photographs is an important task with many applications in computational photography and realistic display of images. The HDR reconstruction task aims to infer the lost details using the context present in the scene, requiring neural networks to understand high-level geometric and illumination cues. This makes it challenging for data-driven algorithms to generate accurate and high-resolution results. In this work, we introduce a physically-inspired remodeling of the HDR reconstruction problem in the intrinsic domain. The intrinsic model allows us to train separate networks to extend the dynamic range in the shading domain and to recover lost color details in the albedo domain. We show that dividing the problem into two simpler sub-tasks improves performance in a wide variety of photographs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13803v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Dille, Chris Careaga, Ya\u{g}{\i}z Aksoy</dc:creator>
    </item>
    <item>
      <title>Reflection Matrix Imaging for Wave Velocity Tomography</title>
      <link>https://arxiv.org/abs/2409.13901</link>
      <description>arXiv:2409.13901v1 Announce Type: cross 
Abstract: Besides controlling wave trajectory inside complex media, wave velocity constitutes a relevant bio-marker for medical imaging. In a transmission configuration, wave-front distortions can be unscrambled to provide a map of the wave velocity landscape $c(\mathbf{r})$. However, most in-vivo applications correspond to a reflection configuration for which only back-scattered echoes generated by short-scale fluctuations of $c(\mathbf{r})$ can be harvested. Under a single scattering assumption, this speckle wave-field cannot provide the long-scale variations of $c(\mathbf{r})$. In this paper, we go beyond the first Born approximation and show how a map of $c(\mathbf{r})$ can be retrieved in epi-detection. To that aim, a reflection matrix approach of wave imaging is adopted. While standard reflection imaging methods generally rely on confocal focusing operations, matrix imaging consists in decoupling the location of the incident and received focal spots. Following this principle, a self-portrait of the focusing process can be obtained around each point of the medium. The Gouy phase shift exhibited by each focal spot is leveraged to finely monitor the wave velocity distribution $c(\mathbf{r})$ inside the medium. Experiment in a tissue-mimicking phantom and numerical simulations are first presented to validate our method. Speed-of-sound tomography is then applied to ultrasound data collected on the liver of a difficult-to-image patient. Beyond paving the way towards quantitative ultrasound, our approach can also be extremely rewarding for standard imaging. Indeed, each echo can be assigned to the actual position of a scatterer. It allows an absolute measurement of distance, an observable often used for diagnosis but generally extremely sensitive to wave velocity fluctuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13901v1</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>physics.app-ph</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavien Bureau, Elsa Giraudat, Arthur Le Ber, William Lambert, Louis Carmier, Aymeric Guibal, Mathias Fink, Alexandre Aubry</dc:creator>
    </item>
    <item>
      <title>Deep learning for fast segmentation and critical dimension metrology &amp; characterization enabling AR/VR design and fabrication</title>
      <link>https://arxiv.org/abs/2409.13951</link>
      <description>arXiv:2409.13951v1 Announce Type: cross 
Abstract: Quantitative analysis of microscopy images is essential in the design and fabrication of components used in augmented reality/virtual reality (AR/VR) modules. However, segmenting regions of interest (ROIs) from these complex images and extracting critical dimensions (CDs) requires novel techniques, such as deep learning models which are key for actionable decisions on process, material and device optimization. In this study, we report on the fine-tuning of a pre-trained Segment Anything Model (SAM) using a diverse dataset of electron microscopy images. We employed methods such as low-rank adaptation (LoRA) to reduce training time and enhance the accuracy of ROI extraction. The model's ability to generalize to unseen images facilitates zero-shot learning and supports a CD extraction model that precisely extracts CDs from the segmented ROIs. We demonstrate the accurate extraction of binary images from cross-sectional images of surface relief gratings (SRGs) and Fresnel lenses in both single and multiclass modes. Furthermore, these binary images are used to identify transition points, aiding in the extraction of relevant CDs. The combined use of the fine-tuned segmentation model and the CD extraction model offers substantial advantages to various industrial applications by enhancing analytical capabilities, time to data and insights, and optimizing manufacturing processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13951v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kundan Chaudhary, Subhei Shaar, Raja Muthinti</dc:creator>
    </item>
    <item>
      <title>Memory Matching is not Enough: Jointly Improving Memory Matching and Decoding for Video Object Segmentation</title>
      <link>https://arxiv.org/abs/2409.14343</link>
      <description>arXiv:2409.14343v1 Announce Type: cross 
Abstract: Memory-based video object segmentation methods model multiple objects over long temporal-spatial spans by establishing memory bank, which achieve the remarkable performance. However, they struggle to overcome the false matching and are prone to lose critical information, resulting in confusion among different objects. In this paper, we propose an effective approach which jointly improving the matching and decoding stages to alleviate the false matching issue.For the memory matching stage, we present a cost aware mechanism that suppresses the slight errors for short-term memory and a shunted cross-scale matching for long-term memory which establish a wide filed matching spaces for various object scales. For the readout decoding stage, we implement a compensatory mechanism aims at recovering the essential information where missing at the matching stage. Our approach achieves the outstanding performance in several popular benchmarks (i.e., DAVIS 2016&amp;2017 Val (92.4%&amp;88.1%), and DAVIS 2017 Test (83.9%)), and achieves 84.8%&amp;84.6% on YouTubeVOS 2018&amp;2019 Val.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14343v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jintu Zheng, Yun Liang, Yuqing Zhang, Wanchao Su</dc:creator>
    </item>
    <item>
      <title>MANTA -- Model Adapter Native generations that's Affordable</title>
      <link>https://arxiv.org/abs/2409.14363</link>
      <description>arXiv:2409.14363v1 Announce Type: cross 
Abstract: The presiding model generation algorithms rely on simple, inflexible adapter selection to provide personalized results. We propose the model-adapter composition problem as a generalized problem to past work factoring in practical hardware and affordability constraints, and introduce MANTA as a new approach to the problem. Experiments on COCO 2014 validation show MANTA to be superior in image task diversity and quality at the cost of a modest drop in alignment. Our system achieves a $94\%$ win rate in task diversity and a $80\%$ task quality win rate versus the best known system, and demonstrates strong potential for direct use in synthetic data generation and the creative art domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14363v1</guid>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Chaurasia</dc:creator>
    </item>
    <item>
      <title>Secrets of Edge-Informed Contrast Maximization for Event-Based Vision</title>
      <link>https://arxiv.org/abs/2409.14611</link>
      <description>arXiv:2409.14611v1 Announce Type: cross 
Abstract: Event cameras capture the motion of intensity gradients (edges) in the image plane in the form of rapid asynchronous events. When accumulated in 2D histograms, these events depict overlays of the edges in motion, consequently obscuring the spatial structure of the generating edges. Contrast maximization (CM) is an optimization framework that can reverse this effect and produce sharp spatial structures that resemble the moving intensity gradients by estimating the motion trajectories of the events. Nonetheless, CM is still an underexplored area of research with avenues for improvement. In this paper, we propose a novel hybrid approach that extends CM from uni-modal (events only) to bi-modal (events and edges). We leverage the underpinning concept that, given a reference time, optimally warped events produce sharp gradients consistent with the moving edge at that time. Specifically, we formalize a correlation-based objective to aid CM and provide key insights into the incorporation of multiscale and multireference techniques. Moreover, our edge-informed CM method yields superior sharpness scores and establishes new state-of-the-art event optical flow benchmarks on the MVSEC, DSEC, and ECD datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14611v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pritam P. Karmokar, Quan H. Nguyen, William J. Beksi</dc:creator>
    </item>
    <item>
      <title>Hybrid iterating-averaging low photon budget Gabor holographic microscopy</title>
      <link>https://arxiv.org/abs/2409.14746</link>
      <description>arXiv:2409.14746v1 Announce Type: cross 
Abstract: One of the primary challenges in live cell culture observation is achieving high-contrast imaging with minimal impact on sample behavior. Quantitative phase imaging (QPI) techniques address this by providing label-free, high-contrast images of transparent samples. Measurement system influence may be further reduced by imaging samples under low illumination intensity (low photon budget - LPB), thereby minimizing photostimulation, phototoxicity, and photodamage, and enabling high-speed imaging. LPB imaging is challenging in QPI due to significant camera shot noise and quantification noise. Digital in-line holographic microscopy (DIHM), working with or without lenses, is a QPI technique known for its robustness to quantification noise. However, reducing simultaneously the camera shot noise and the inherent in-line holographic twin image disturbances remain a critical, yet unaddressed, challenge. This study introduces an innovative iterative Gabor averaging (IGA) algorithm designed specifically for filling this important scientific gap in multi-frame DIHM under LPB conditions. We evaluated the performance of the IGA on simulated data showing that it outperformed traditional algorithms in terms of reconstruction accuracy under high noise conditions. Those results were corroborated by experimental validation involving high-speed imaging of dynamic sperm cells and a phase test target under significantly reduced illumination power. Additionally, the IGA algorithm proved successful in reconstructing optically thin samples, which typically produce low signal-to-noise ratio holograms even under high photon budget conditions. These advancements facilitate photostimulation-free and high-speed imaging of dynamic biological samples and enhance the capability to image samples with extremely low optical thickness, potentially transforming various applications in biomedical and environmental imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14746v1</guid>
      <category>physics.optics</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miko{\l}aj Rogalski, Piotr Arcab, Emilia Wdowiak, Jos\'e \'Angel Picazo-Bueno, Vicente Mic\'o, Micha{\l} J\'ozwik, Maciej Trusiak</dc:creator>
    </item>
    <item>
      <title>RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience</title>
      <link>https://arxiv.org/abs/2409.14829</link>
      <description>arXiv:2409.14829v1 Announce Type: cross 
Abstract: In recent years, digital watermarking techniques based on deep learning have been widely studied. To achieve both imperceptibility and robustness of image watermarks, most current methods employ convolutional neural networks to build robust watermarking frameworks. However, despite the success of CNN-based watermarking models, they struggle to achieve robustness against geometric attacks due to the limitations of convolutional neural networks in capturing global and long-range relationships. To address this limitation, we propose a robust watermarking framework based on the Swin Transformer, named RoWSFormer. Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as the core of both the encoder and decoder. This block utilizes the self-attention mechanism to capture global and long-range information, thereby significantly improving adaptation to geometric distortions. Additionally, we construct the Frequency-Enhanced Transformer Block to extract frequency domain information, which further strengthens the robustness of the watermarking framework. Experimental results demonstrate that our RoWSFormer surpasses existing state-of-the-art watermarking methods. For most non-geometric attacks, RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction accuracy. In the case of geometric attacks (such as rotation, scaling, and affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR, with extraction accuracy exceeding 97\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14829v1</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weitong Chen, Yuheng Li</dc:creator>
    </item>
    <item>
      <title>FusionRF: High-Fidelity Satellite Neural Radiance Fields from Multispectral and Panchromatic Acquisitions</title>
      <link>https://arxiv.org/abs/2409.15132</link>
      <description>arXiv:2409.15132v1 Announce Type: cross 
Abstract: We introduce FusionRF, a novel neural rendering terrain reconstruction method from optically unprocessed satellite imagery. While previous methods depend on external pansharpening methods to fuse low resolution multispectral imagery and high resolution panchromatic imagery, FusionRF directly performs reconstruction based on optically unprocessed acquisitions with no prior knowledge. This is accomplished through the addition of a sharpening kernel which models the resolution loss in multispectral images. Additionally, novel modal embeddings allow the model to perform image fusion as a bottleneck to novel view synthesis. We evaluate our method on multispectral and panchromatic satellite images from the WorldView-3 satellite in various locations, and FusionRF outperforms previous State-of-The-Art methods in depth reconstruction on unprocessed imagery, renders sharp training and novel views, and retains multi-spectral information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15132v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Sprintson, Rama Chellappa, Cheng Peng</dc:creator>
    </item>
    <item>
      <title>A Coding Framework and Benchmark towards Low-Bitrate Video Understanding</title>
      <link>https://arxiv.org/abs/2202.02813</link>
      <description>arXiv:2202.02813v3 Announce Type: replace 
Abstract: Video compression is indispensable to most video analysis systems. Despite saving transportation bandwidth, it also deteriorates downstream video understanding tasks, especially at low-bitrate settings. To systematically investigate this problem, we first thoroughly review the previous methods, revealing that three principles, i.e., task-decoupled, label-free, and data-emerged semantic prior, are critical to a machine-friendly coding framework but are not fully satisfied so far. In this paper, we propose a traditional-neural mixed coding framework that simultaneously fulfills all these principles, by taking advantage of both traditional codecs and neural networks (NNs). On one hand, the traditional codecs can efficiently encode the pixel signal of videos but may distort the semantic information. On the other hand, highly non-linear NNs are proficient in condensing video semantics into a compact representation. The framework is optimized by ensuring that a transportation-efficient semantic representation of the video is preserved w.r.t. the coding procedure, which is spontaneously learned from unlabeled data in a self-supervised manner. The videos collaboratively decoded from two streams (codec and NN) are of rich semantics, as well as visually photo-realistic, empirically boosting several mainstream downstream video analysis task performances without any post-adaptation procedure. Furthermore, by introducing the attention mechanism and adaptive modeling scheme, the video semantic modeling ability of our approach is further enhanced. Finally, we build a low-bitrate video understanding benchmark with three downstream tasks on eight datasets, demonstrating the notable superiority of our approach. All codes, data, and models will be available at \url{https://github.com/tianyuan168326/VCS-Pytorch}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.02813v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Tian, Guo Lu, Yichao Yan, Guangtao Zhai, Li Chen, Zhiyong Gao</dc:creator>
    </item>
    <item>
      <title>CloudBrain-ReconAI: An Online Platform for MRI Reconstruction and Image Quality Evaluation</title>
      <link>https://arxiv.org/abs/2212.01878</link>
      <description>arXiv:2212.01878v2 Announce Type: replace 
Abstract: Efficient collaboration between engineers and radiologists is important for image reconstruction algorithm development and image quality evaluation in magnetic resonance imaging (MRI). Here, we develop CloudBrain-ReconAI, an online cloud computing platform, for algorithm deployment, fast and blind reader study. This platform supports online image reconstruction using state-of-the-art artificial intelligence and compressed sensing algorithms with applications to fast imaging and high-resolution diffusion imaging. Through visiting the website, radiologists can easily score and mark the images. Then, automatic statistical analysis will be provided. CloudBrain-ReconAI is now open accessed at https://csrc.xmu.edu.cn/CloudBrain.html and will be continually improved to serve the MRI research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01878v2</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yirong Zhou, Chen Qian, Jiayu Li, Zi Wang, Yu Hu, Biao Qu, Liuhong Zhu, Jianjun Zhou, Taishan Kang, Jianzhong Lin, Qing Hong, Jiyang Dong, Di Guo, Xiaobo Qu</dc:creator>
    </item>
    <item>
      <title>Z-SSMNet: Zonal-aware Self-supervised Mesh Network for Prostate Cancer Detection and Diagnosis with Bi-parametric MRI</title>
      <link>https://arxiv.org/abs/2212.05808</link>
      <description>arXiv:2212.05808v2 Announce Type: replace 
Abstract: Bi-parametric magnetic resonance imaging (bpMRI) has become a pivotal modality in the detection and diagnosis of clinically significant prostate cancer (csPCa). Developing AI-based systems to identify csPCa using bpMRI can transform PCa management by improving efficiency and cost-effectiveness. However, current state-of-the-art methods using convolutional neural networks (CNNs) are limited in learning in-plane and three-dimensional spatial information from anisotropic images. Their performances also depend on the availability of large, diverse, and well-annotated bpMRI datasets. We propose a Zonal-aware Self-supervised Mesh Network (Z-SSMNet) that adaptively integrates multi-dimensional (2D/2.5D/3D) convolutions to learn dense intra-slice information and sparse inter-slice information of the anisotropic bpMRI in a balanced manner. A self-supervised learning (SSL) technique is proposed to pre-train our network using large-scale unlabeled data to learn the appearance, texture, and structure semantics of bpMRI. It aims to capture both intra-slice and inter-slice information during the pre-training stage. Furthermore, we constrained our network to focus on the zonal anatomical regions to further improve the detection and diagnosis capability of csPCa. We conducted extensive experiments on the PI-CAI dataset comprising 10000+ multi-center and multi-scanner data. Our Z-SSMNet excelled in both lesion-level detection (AP score of 0.633) and patient-level diagnosis (AUROC score of 0.881), securing the top position in the Open Development Phase of the PI-CAI challenge and maintained strong performance, achieving an AP score of 0.690 and an AUROC score of 0.909, and securing the second-place ranking in the Closed Testing Phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05808v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Yuan, Euijoon Ahn, Dagan Feng, Mohamad Khadra, Jinman Kim</dc:creator>
    </item>
    <item>
      <title>The Brain Tumor Segmentation (BraTS) Challenge: Local Synthesis of Healthy Brain Tissue via Inpainting</title>
      <link>https://arxiv.org/abs/2305.08992</link>
      <description>arXiv:2305.08992v3 Announce Type: replace 
Abstract: A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with an already pathological scan. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantee for images featuring lesions. Examples include, but are not limited to, algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS inpainting challenge. Here, the participants explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later, it will be updated to summarize the findings of the challenge. The challenge is organized as part of the ASNR-BraTS MICCAI challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08992v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kofler, Felix Meissen, Felix Steinbauer, Robert Graf, Stefan K Ehrlich, Annika Reinke, Eva Oswald, Diana Waldmannstetter, Florian Hoelzl, Izabela Horvath, Oezguen Turgut, Suprosanna Shit, Christina Bukas, Kaiyuan Yang, Johannes C. Paetzold, Ezequiel de da Rosa, Isra Mekki, Shankeeth Vinayahalingam, Hasan Kassem, Juexin Zhang, Ke Chen, Ying Weng, Alicia Durrer, Philippe C. Cattin, Julia Wolleb, M. S. Sadique, M. M. Rahman, W. Farzana, A. Temtam, K. M. Iftekharuddin, Maruf Adewole, Syed Muhammad Anwar, Ujjwal Baid, Anastasia Janas, Anahita Fathi Kazerooni, Dominic LaBella, Hongwei Bran Li, Ahmed W Moawad, Gian-Marco Conte, Keyvan Farahani, James Eddy, Micah Sheller, Sarthak Pati, Alexandros Karagyris, Alejandro Aristizabal, Timothy Bergquist, Verena Chung, Russell Takeshi Shinohara, Farouk Dako, Walter Wiggins, Zachary Reitman, Chunhao Wang, Xinyang Liu, Zhifan Jiang, Elaine Johanson, Zeke Meier, Ariana Familiar, Christos Davatzikos, John Freymann, Justin Kirby, Michel Bilello, Hassan M Fathallah-Shaykh, Roland Wiest, Jan Kirschke, Rivka R Colen, Aikaterini Kotrotsou, Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-Andr\'e Weber, Abhishek Mahajan, Suyash Mohan, John Mongan, Christopher Hess, Soonmee Cha, Javier Villanueva-Meyer, Errol Colak, Priscila Crivellaro, Andras Jakab, Abiodun Fatade, Olubukola Omidiji, Rachel Akinola Lagos, O O Olatunji, Goldey Khanna, John Kirkpatrick, Michelle Alonso-Basanta, Arif Rashid, Miriam Bornhorst, Ali Nabavizadeh, Natasha Lepore, Joshua Palmer, Antonio Porras, Jake Albrecht, Udunna Anazodo, Mariam Aboian, Evan Calabrese, Jeffrey David Rudie, Marius George Linguraru, Juan Eugenio Iglesias, Koen Van Leemput, Spyridon Bakas, Benedikt Wiestler, Ivan Ezhov, Marie Piraud, Bjoern H Menze</dc:creator>
    </item>
    <item>
      <title>Intelligent Scoliosis Screening and Diagnosis: A Survey</title>
      <link>https://arxiv.org/abs/2310.08756</link>
      <description>arXiv:2310.08756v2 Announce Type: replace 
Abstract: Scoliosis is a three-dimensional spinal deformity, which may lead to abnormal morphologies, such as thoracic deformity, and pelvic tilt. Severe patients may suffer from nerve damage and urinary abnormalities. At present, the number of scoliosis patients in primary and secondary schools has exceeded five million in China, the incidence rate is about 3% to 5% which is growing every year. The research on scoliosis, therefore, has important clinical value. This paper systematically introduces computer-assisted scoliosis screening and diagnosis as well as analyzes the advantages and limitations of different algorithm models in the current issue field. Moreover, the paper also discusses the current development bottlenecks in this field and looks forward to future development trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08756v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenlin Zhang, Lixin Pu, Ang Li, Jun Zhang, Xianjie Li, Jipeng Fan</dc:creator>
    </item>
    <item>
      <title>Continual Learning in Medical Image Analysis: A Comprehensive Review of Recent Advancements and Future Prospects</title>
      <link>https://arxiv.org/abs/2312.17004</link>
      <description>arXiv:2312.17004v3 Announce Type: replace 
Abstract: Medical imaging analysis has witnessed remarkable advancements even surpassing human-level performance in recent years, driven by the rapid development of advanced deep-learning algorithms. However, when the inference dataset slightly differs from what the model has seen during one-time training, the model performance is greatly compromised. The situation requires restarting the training process using both the old and the new data which is computationally costly, does not align with the human learning process, and imposes storage constraints and privacy concerns. Alternatively, continual learning has emerged as a crucial approach for developing unified and sustainable deep models to deal with new classes, tasks, and the drifting nature of data in non-stationary environments for various application areas. Continual learning techniques enable models to adapt and accumulate knowledge over time, which is essential for maintaining performance on evolving datasets and novel tasks. This systematic review paper provides a comprehensive overview of the state-of-the-art in continual learning techniques applied to medical imaging analysis. We present an extensive survey of existing research, covering topics including catastrophic forgetting, data drifts, stability, and plasticity requirements. Further, an in-depth discussion of key components of a continual learning framework such as continual learning scenarios, techniques, evaluation schemes, and metrics is provided. Continual learning techniques encompass various categories, including rehearsal, regularization, architectural, and hybrid strategies. We assess the popularity and applicability of continual learning categories in various medical sub-fields like radiology and histopathology...</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17004v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratibha Kumari, Joohi Chauhan, Afshin Bozorgpour, Boqiang Huang, Reza Azad, Dorit Merhof</dc:creator>
    </item>
    <item>
      <title>One-Shot Image Restoration</title>
      <link>https://arxiv.org/abs/2404.17426</link>
      <description>arXiv:2404.17426v2 Announce Type: replace 
Abstract: Image restoration, or inverse problems in image processing, has long been an extensively studied topic. In recent years supervised learning approaches have become a popular strategy attempting to tackle this task. Unfortunately, most supervised learning-based methods are highly demanding in terms of computational resources and training data (sample complexity). In addition, trained models are sensitive to domain changes, such as varying acquisition systems, signal sampling rates, resolution and contrast. In this work, we try to answer a fundamental question: Can supervised learning models generalize well solely by learning from one image or even part of an image? If so, then what is the minimal amount of patches required to achieve acceptable generalization? To this end, we focus on an efficient patch-based learning framework that requires a single image input-output pair for training. Experimental results demonstrate the applicability, robustness and computational efficiency of the proposed approach for supervised image deblurring and super-resolution. Our results showcase significant improvement of learning models' sample efficiency, generalization and time complexity, that can hopefully be leveraged for future real-time applications, and applied to other signals and modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17426v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Deborah Pereg</dc:creator>
    </item>
    <item>
      <title>Advancing Medical Image Segmentation with Mini-Net: A Lightweight Solution Tailored for Efficient Segmentation of Medical Images</title>
      <link>https://arxiv.org/abs/2405.17520</link>
      <description>arXiv:2405.17520v4 Announce Type: replace 
Abstract: Accurate segmentation of anatomical structures and abnormalities in medical images is crucial for computer-aided diagnosis and analysis. While deep learning techniques excel at this task, their computational demands pose challenges. Additionally, some cutting-edge segmentation methods, though effective for general object segmentation, may not be optimised for medical images. To address these issues, we propose Mini-Net, a lightweight segmentation network specifically designed for medical images. With fewer than 38,000 parameters, Mini-Net efficiently captures both high- and low-frequency features, enabling real-time applications in various medical imaging scenarios. We evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016, ISIC-2018, and MoNuSeg, demonstrating its robustness and good performance compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17520v4</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Syed Javed, Tariq M. Khan, Abdul Qayyum, Hamid Alinejad-Rokny, Arcot Sowmya, Imran Razzak</dc:creator>
    </item>
    <item>
      <title>Generative Iris Prior Embedded Transformer for Iris Restoration</title>
      <link>https://arxiv.org/abs/2407.00261</link>
      <description>arXiv:2407.00261v2 Announce Type: replace 
Abstract: Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00261v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICME55011.2023.00094</arxiv:DOI>
      <arxiv:journal_reference>2023 IEEE International Conference on Multimedia and Expo (ICME), Brisbane, Australia, 2023, pp. 510-515</arxiv:journal_reference>
      <dc:creator>Yubo Huang, Jia Wang, Peipei Li, Liuyu Xiang, Peigang Li, Zhaofeng He</dc:creator>
    </item>
    <item>
      <title>CIC: Circular Image Compression</title>
      <link>https://arxiv.org/abs/2407.15870</link>
      <description>arXiv:2407.15870v2 Announce Type: replace 
Abstract: Learned image compression (LIC) is currently the cutting-edge method. However, the inherent difference between testing and training images of LIC results in performance degradation to some extent. Especially for out-of-sample, out-of-distribution, or out-of-domain testing images, the performance of LIC dramatically degraded. Classical LIC is a serial image compression (SIC) approach that utilizes an open-loop architecture with serial encoding and decoding units. Nevertheless, according to the theory of automatic control, a closed-loop architecture holds the potential to improve the dynamic and static performance of LIC. Therefore, a circular image compression (CIC) approach with closed-loop encoding and decoding elements is proposed to minimize the gap between testing and training images and upgrade the capability of LIC. The proposed CIC establishes a nonlinear loop equation and proves that steady-state error between reconstructed and original images is close to zero by Taylor series expansion. The proposed CIC method possesses the property of Post-Training and plug-and-play which can be built on any existing advanced SIC methods. Experimental results on five public image compression datasets demonstrate that the proposed CIC outperforms five competing state-of-the-art open-source SIC algorithms in reconstruction capacity. Experimental results further show that the proposed method is suitable for out-of-sample testing images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15870v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honggui Li, Sinan Chen, Nahid Md Lokman Hossain, Maria Trocan, Dimitri Galayko, Mohamad Sawan</dc:creator>
    </item>
    <item>
      <title>Bi-modality Images Transfer with a Discrete Process Matching Method</title>
      <link>https://arxiv.org/abs/2409.03977</link>
      <description>arXiv:2409.03977v2 Announce Type: replace 
Abstract: Recently, medical image synthesis gains more and more popularity, along with the rapid development of generative models. Medical image synthesis aims to generate an unacquired image modality, often from other observed data modalities. Synthesized images can be used for clinical diagnostic assistance, data augmentation for model training and validation or image quality improving. In the meanwhile, the flow-based models are among the successful generative models for the ability of generating realistic and high-quality synthetic images. However, most flow-based models require to calculate flow ordinary different equation (ODE) evolution steps in transfer process, for which the performances are significantly limited by heavy computation time due to a large number of time iterations. In this paper, we propose a novel flow-based model, namely Discrete Process Matching (DPM) to accomplish the bi-modality image transfer tasks. Different to other flow matching based models, we propose to utilize both forward and backward ODE flow and enhance the consistency on the intermediate images of few discrete time steps, resulting in a transfer process with much less iteration steps while maintaining high-quality generations for both modalities. Our experiments on three datasets of MRI T1/T2 and CT/MRI demonstrate that DPM outperforms other state-of-the-art flow-based methods for bi-modality image synthesis, achieving higher image quality with less computation time cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03977v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Xiong, Qiaoqiao Ding, Xiaoqun Zhang</dc:creator>
    </item>
    <item>
      <title>Superpixel perception graph neural network for intelligent defect detection of aero-engine blade</title>
      <link>https://arxiv.org/abs/2210.07539</link>
      <description>arXiv:2210.07539v2 Announce Type: replace-cross 
Abstract: Aero-engine is the core component of aircraft and other spacecraft. The high-speed rotating blades provide power by sucking in air and fully combusting, and various defects will inevitably occur, threatening the operation safety of aero-engine. Therefore, regular inspections are essential for such a complex system. However, existing traditional technology which is borescope inspection is labor-intensive, time-consuming, and experience-dependent. To endow this technology with intelligence, a novel superpixel perception graph neural network (SPGNN) is proposed by utilizing a multi-stage graph convolutional network (MSGCN) for feature extraction and superpixel perception region proposal network (SPRPN) for region proposal. First, to capture complex and irregular textures, the images are transformed into a series of patches, to obtain their graph representations. Then, MSGCN composed of several GCN blocks extracts graph structure features and performs graph information processing at graph level. Last but not least, the SPRPN is proposed to generate perceptual bounding boxes by fusing graph representation features and superpixel perception features. Therefore, the proposed SPGNN always implements feature extraction and information transmission at the graph level in the whole SPGNN pipeline, to alleviate the reduction of receptive field and information loss. To verify the effectiveness of SPGNN, we construct a simulated blade dataset with 3000 images. A public aluminum dataset is also used to validate the performances of different methods. The experimental results demonstrate that the proposed SPGNN has superior performance compared with the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07539v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmsy.2024.08.009</arxiv:DOI>
      <arxiv:journal_reference>Journal of Manufacturing Systems. 77 (2024) 112-126</arxiv:journal_reference>
      <dc:creator>Hongbing Shang, Qixiu Yang, Chuang Sun, Xuefeng Chen, Ruqiang Yan</dc:creator>
    </item>
    <item>
      <title>Deep Evidential Learning for Radiotherapy Dose Prediction</title>
      <link>https://arxiv.org/abs/2404.17126</link>
      <description>arXiv:2404.17126v2 Announce Type: replace-cross 
Abstract: In this work, we present a novel application of an uncertainty-quantification framework called Deep Evidential Learning in the domain of radiotherapy dose prediction. Using medical images of the Open Knowledge-Based Planning Challenge dataset, we found that this model can be effectively harnessed to yield uncertainty estimates that inherited correlations with prediction errors upon completion of network training. This was achieved only after reformulating the original loss function for a stable implementation. We found that (i)epistemic uncertainty was highly correlated with prediction errors, with various association indices comparable or stronger than those for Monte-Carlo Dropout and Deep Ensemble methods, (ii)the median error varied with uncertainty threshold much more linearly for epistemic uncertainty in Deep Evidential Learning relative to these other two conventional frameworks, indicative of a more uniformly calibrated sensitivity to model errors, (iii)relative to epistemic uncertainty, aleatoric uncertainty demonstrated a more significant shift in its distribution in response to Gaussian noise added to CT intensity, compatible with its interpretation as reflecting data noise. Collectively, our results suggest that Deep Evidential Learning is a promising approach that can endow deep-learning models in radiotherapy dose prediction with statistical robustness. Towards enhancing its clinical relevance, we demonstrate how we can use such a model to construct the predicted Dose-Volume-Histograms' confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17126v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compbiomed.2024.109172</arxiv:DOI>
      <arxiv:journal_reference>Computers in Biology and Medicine, Vol. 182, Nov 2024, 109172</arxiv:journal_reference>
      <dc:creator>Hai Siong Tan, Kuancheng Wang, Rafe Mcbeth</dc:creator>
    </item>
  </channel>
</rss>
