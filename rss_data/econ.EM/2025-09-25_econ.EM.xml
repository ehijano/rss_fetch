<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 01:46:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Decomposing Co-Movements in Matrix-Valued Time Series: A Pseudo-Structural Reduced-Rank Approach</title>
      <link>https://arxiv.org/abs/2509.19911</link>
      <description>arXiv:2509.19911v1 Announce Type: new 
Abstract: We propose a pseudo-structural framework for analyzing contemporaneous co-movements in reduced-rank matrix autoregressive (RRMAR) models. Unlike conventional vector-autoregressive (VAR) models that would discard the matrix structure, our formulation preserves it, enabling a decomposition of co-movements into three interpretable components: row-specific, column-specific, and joint (row-column) interactions across the matrix-valued time series. Our estimator admits standard asymptotic inference and we propose a BIC-type criterion for the joint selection of the reduced ranks and the autoregressive lag order. We validate the method's finite-sample performance in terms of estimation accuracy, coverage and rank selection in simulation experiments, including cases of rank misspecification. We illustrate the method's practical usefelness in identifying co-movement structures in two empirical applications: U.S. state-level coincident and leading indicators, and cross-country macroeconomic indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19911v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Hecq, Ivan Ricardo, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Seller Risk Aversion in Ascending Auctions</title>
      <link>https://arxiv.org/abs/2509.19945</link>
      <description>arXiv:2509.19945v1 Announce Type: new 
Abstract: How sellers choose reserve prices is central to auction theory, and the optimal reserve price depends on the seller's risk attitude. Numerous studies have found that observed reserve prices lie below the optimal level implied by risk-neutral sellers, while the theoretical literature suggests that risk-averse sellers can rationalize these empirical findings. In this paper, we develop an econometric model of ascending auctions with a risk-averse seller under independent private values. We provide primitive conditions for the identification of the Arrow-Pratt measures of risk aversion and an estimator for these measures that is consistent and converges in distribution to a normal distribution at the parametric rate under standard regularity conditions. A Monte Carlo study demonstrates good finite-sample performance of the estimator, and we illustrate the approach using data from foreclosure real estate auctions in S\~{a}o Paulo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19945v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathalie Gimenes, Tonghui Qi, Sorawoot Srisuma</dc:creator>
    </item>
    <item>
      <title>Identification and Semiparametric Estimation of Conditional Means from Aggregate Data</title>
      <link>https://arxiv.org/abs/2509.20194</link>
      <description>arXiv:2509.20194v1 Announce Type: cross 
Abstract: We introduce a new method for estimating the mean of an outcome variable within groups when researchers only observe the average of the outcome and group indicators across a set of aggregation units, such as geographical areas. Existing methods for this problem, also known as ecological inference, implicitly make strong assumptions about the aggregation process. We first formalize weaker conditions for identification, which motivates estimators that can efficiently control for many covariates. We propose a debiased machine learning estimator that is based on nuisance functions restricted to a partially linear form. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying assumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under additional assumptions. Simulations and validation on real-world data where ground truth is available demonstrate the advantages of our approach over existing methods. Open-source software is available which implements the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20194v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cory McCartan, Shiro Kuriwaki</dc:creator>
    </item>
    <item>
      <title>A Misuse of Specification Tests</title>
      <link>https://arxiv.org/abs/2211.11915</link>
      <description>arXiv:2211.11915v2 Announce Type: replace 
Abstract: Empirical researchers often perform model specification tests, such as Hausman tests and overidentifying restrictions tests, to assess the validity of estimators rather than that of models. This paper examines the effectiveness of such specification pretests in detecting invalid estimators. We analyze the local asymptotic properties of test statistics and estimators and show that locally unbiased specification tests cannot determine whether asymptotically efficient estimators are asymptotically biased. In particular, an estimator may remain valid even when the null hypothesis of correct model specification is false, and it may be invalid even when the null hypothesis is true. The main message of the paper is that correct model specification and valid estimation are distinct issues: correct specification is neither necessary nor sufficient for asymptotically unbiased estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11915v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoya Sueishi</dc:creator>
    </item>
    <item>
      <title>Modelling with Sensitive Variables</title>
      <link>https://arxiv.org/abs/2403.15220</link>
      <description>arXiv:2403.15220v3 Announce Type: replace 
Abstract: The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15220v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Chan, Laszlo Matyas, Agoston Reguly</dc:creator>
    </item>
    <item>
      <title>Dyadic Regression with Sample Selection</title>
      <link>https://arxiv.org/abs/2405.17787</link>
      <description>arXiv:2405.17787v3 Announce Type: replace 
Abstract: This paper addresses the sample selection problem in panel dyadic regression analysis. Dyadic data often include many zeros in the main outcomes due to the underlying network formation process. This not only contaminates popular estimators used in practice but also complicates the inference due to the dyadic dependence structure. We extend Kyriazidou (1997)'s approach to dyadic data and characterize the asymptotic distribution of our proposed estimator. The convergence rates are $\sqrt{n}$ or $\sqrt{n^{2}h_{n}}$, depending on the degeneracy of the H\'{a}jek projection part of the estimator, where $n$ is the number of nodes and $h_{n}$ is a bandwidth. We propose a bias-corrected confidence interval and a variance estimator that adapts to the degeneracy. A Monte Carlo simulation shows the good finite sample performance of our estimator and highlights the importance of bias correction in both asymptotic regimes when the fraction of zeros in outcomes varies. We illustrate our procedure using data from Moretti and Wilson (2017)'s paper on migration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17787v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensuke Sakamoto</dc:creator>
    </item>
    <item>
      <title>Macroeconomic Forecasting with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.00890</link>
      <description>arXiv:2407.00890v4 Announce Type: replace 
Abstract: This paper presents a comparative analysis evaluating the accuracy of Large Language Models (LLMs) against traditional macro time series forecasting approaches. In recent times, LLMs have surged in popularity for forecasting due to their ability to capture intricate patterns in data and quickly adapt across very different domains. However, their effectiveness in forecasting macroeconomic time series data compared to conventional methods remains an area of interest. To address this, we conduct a rigorous evaluation of LLMs against traditional macro forecasting methods, using as common ground the FRED-MD database. Our findings provide valuable insights into the strengths and limitations of LLMs in forecasting macroeconomic time series, shedding light on their applicability in real-world scenarios</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00890v4</guid>
      <category>econ.EM</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Carriero, Davide Pettenuzzo, Shubhranshu Shekhar</dc:creator>
    </item>
    <item>
      <title>Publication Design with Incentives in Mind</title>
      <link>https://arxiv.org/abs/2504.21156</link>
      <description>arXiv:2504.21156v2 Announce Type: replace 
Abstract: The publication process both determines which research receives the most attention, and influences the supply of research through its impact on researchers' private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs. We illustrate the implications of our model in an application to medical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21156v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ravi Jagadeesan, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Adaptive Neyman Allocation</title>
      <link>https://arxiv.org/abs/2309.08808</link>
      <description>arXiv:2309.08808v4 Announce Type: replace-cross 
Abstract: In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. We demonstrate the effectiveness of our adaptive Neyman allocation algorithm using both online A/B testing data from a social media site and synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08808v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Properties of Generalized Ridge Estimators in Nonlinear Models</title>
      <link>https://arxiv.org/abs/2504.19018</link>
      <description>arXiv:2504.19018v2 Announce Type: replace-cross 
Abstract: This paper addresses the longstanding challenge of analyzing the mean squared error (MSE) of ridge-type estimators in nonlinear models, including duration, Poisson, and multinomial choice models, where theoretical results have been scarce. Using a finite-sample approximation technique from the econometrics literature, we derive new results showing that the generalized ridge maximum likelihood estimator (MLE) with a sufficiently small penalty achieves lower finite-sample MSE for both estimation and prediction than the conventional MLE, regardless of whether the hypotheses incorporated in the penalty are valid. A key theoretical contribution is to demonstrate that generalized ridge estimators generate a variance-bias trade-off in the first-order MSE of nonlinear likelihood-based models -- a feature absent for the conventional MLE -- which enables ridge-type estimators to attain smaller MSE when the penalty is properly selected. Extensive simulations and an empirical application to the estimation of marginal mean and quantile treatment effects further confirm the superior performance and practical relevance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19018v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masamune Iwasawa</dc:creator>
    </item>
  </channel>
</rss>
