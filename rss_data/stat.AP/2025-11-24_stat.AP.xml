<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 03:51:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion</title>
      <link>https://arxiv.org/abs/2511.16816</link>
      <description>arXiv:2511.16816v1 Announce Type: new 
Abstract: The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16816v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lekha Patel, Craig Ulmer, Stephen J. Verzi, Daniel J. Krofcheck, Indu Manickam, Asmeret Naugle, Jaideep Ray</dc:creator>
    </item>
    <item>
      <title>Optimising pandemic response through vaccination strategies using neural networks</title>
      <link>https://arxiv.org/abs/2511.16932</link>
      <description>arXiv:2511.16932v1 Announce Type: new 
Abstract: Epidemic risk assessment poses inherent challenges, with traditional approaches often failing to balance health outcomes and economic constraints. This paper presents a data-driven decision support tool that models epidemiological dynamics and optimises vaccination strategies to control disease spread whilst minimising economic losses. The proposed economic-epidemiological framework comprises three phases: modelling, optimising, and analysing. First, a stochastic compartmental model captures epidemic dynamics. Second, an optimal control problem is formulated to derive vaccination strategies that minimise pandemic-related expenditure. Given the analytical intractability of epidemiological models, neural networks are employed to calibrate parameters and solve the high-dimensional control problem. The framework is demonstrated using COVID-19 data from Victoria, Australia, empirically deriving optimal vaccination strategies that simultaneously minimise disease incidence and governmental expenditure. By employing this three-phase framework, policymakers can adjust input values to reflect evolving transmission dynamics and continuously update strategies, thereby minimising aggregate costs, aiding future pandemic preparedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16932v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Zhai, Ping Chen, Zhuo Jin, David Pitt</dc:creator>
    </item>
    <item>
      <title>Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score</title>
      <link>https://arxiv.org/abs/2511.16954</link>
      <description>arXiv:2511.16954v1 Announce Type: new 
Abstract: The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\ell_1$ and $\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16954v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyuan Liu, Qirui Zhang, Jinhong Du, Siming Zhao, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>A spatiotemporal Bayesian hierarchical model of heat-related mortality in Catalonia, Spain (2012--2022): The role of environmental and socioeconomic modifiers</title>
      <link>https://arxiv.org/abs/2511.17148</link>
      <description>arXiv:2511.17148v1 Announce Type: new 
Abstract: Background: Extreme heat is a major public health risk, yet its relationship with mortality may be confounded or modified by air pollution and social determinants. Objectives: We aimed to quantify the effects of extreme maximum temperatures and heatwaves on daily mortality in Catalonia (2012--2022), and to assess the modifying and confounding roles of air pollutants and socioeconomic factors. Methods: We conducted a time--series ecological study across 379 basic health areas (ABS) during summer months. Mortality data from the Spanish National Statistics Institute were linked with meteorological and air pollution data. A hierarchical Bayesian spatiotemporal model, incorporating structured and unstructured random effects, was used to account for spatial and temporal dependencies, as well as observed socioeconomic confounders. Results: In total, 730,634 deaths occurred, with 216,989 in summer. Extreme heat alone was not independently associated with mortality, as its effect was fully confounded by high ozone levels and partly by socioeconomic indicators. Ozone concentrations ($\ge 120 \mu g/m^3$) significantly increased mortality risk, especially among individuals aged $\ge 85$ years. Greater income inequality and higher proportions of older residents also amplified vulnerability. Conclusion: Mortality risks from extreme heat in Catalonia were strongly influenced by ozone levels and social determinants. Adaptation strategies should address both compound environmental exposures together with socioeconomic vulnerability to better protect older and disadvantaged populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17148v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Solano, Marta Solans, Xavier Perafita, Anna Ruiz-Comellas, Marc Saez, Maria A. Barcel\'o</dc:creator>
    </item>
    <item>
      <title>Flexible unimodal density estimation in hidden Markov models</title>
      <link>https://arxiv.org/abs/2511.17071</link>
      <description>arXiv:2511.17071v1 Announce Type: cross 
Abstract: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17071v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Fanny Dupont, Marie Auger-M\'eth\'e, Marianne Marcoux, Nancy Heckman</dc:creator>
    </item>
    <item>
      <title>ggskewboxplots: Enhanced Boxplots for Skewed Data in R</title>
      <link>https://arxiv.org/abs/2511.17091</link>
      <description>arXiv:2511.17091v1 Announce Type: cross 
Abstract: Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17091v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Cavus</dc:creator>
    </item>
    <item>
      <title>Automobile demand forecasting: Spatiotemporal and hierarchical modeling, life cycle dynamics, and user-generated online information</title>
      <link>https://arxiv.org/abs/2511.17275</link>
      <description>arXiv:2511.17275v1 Announce Type: cross 
Abstract: Premium automotive manufacturers face increasingly complex forecasting challenges due to high product variety, sparse variant-level data, and volatile market dynamics. This study addresses monthly automobile demand forecasting across a multi-product, multi-market, and multi-level hierarchy using data from a German premium manufacturer. The methodology combines point and probabilistic forecasts across strategic and operational planning levels, leveraging ensembles of LightGBM models with pooled training sets, quantile regression, and a mixed-integer linear programming reconciliation approach. Results highlight that spatiotemporal dependencies, as well as rounding bias, significantly affect forecast accuracy, underscoring the importance of integer forecasts for operational feasibility. Shapley analysis shows that short-term demand is reactive, shaped by life cycle maturity, autoregressive momentum, and operational signals, whereas medium-term demand reflects anticipatory drivers such as online engagement, planning targets, and competitive indicators, with online behavioral data considerably improving accuracy at disaggregated levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17275v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Nahrendorf, Stefan Minner, Helfried Binder, Richard Zinck</dc:creator>
    </item>
    <item>
      <title>On treating right-censoring events like treatments</title>
      <link>https://arxiv.org/abs/2511.17379</link>
      <description>arXiv:2511.17379v1 Announce Type: cross 
Abstract: In causal inference literature, potential outcomes are often indexed by the "elimination of all right-censoring events," leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17379v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Aaron L. Sarvet, Jessica G. Young</dc:creator>
    </item>
    <item>
      <title>Extending the Accelerated Failure Conditionals Model to Location-Scale Families</title>
      <link>https://arxiv.org/abs/2511.17463</link>
      <description>arXiv:2511.17463v1 Announce Type: cross 
Abstract: Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\mathbb{R}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17463v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>A Spatio-temporal CP decomposition analysis of New England region in the US</title>
      <link>https://arxiv.org/abs/2510.10322</link>
      <description>arXiv:2510.10322v3 Announce Type: replace 
Abstract: Spatio temporal data consist of measurement for one or more raster fields such as weather, traffic volume, crime rate, or disease incidents. Advances in modern technology have increased the number of available information for this type of data hence the rise of multidimensional data. In this paper we take advantage of the multidimensional structure of the data but also its temporal and spatial structure. In fact, we will be using the NCAR Climate Data Gateway website which provides data discovery and access services for global and regional climate model data. The daily values of total precipitation (prec), maximum (tmax), and minimum (tmin) temperature are combined to create a multidimensional data called tensor (a multidimensional array). In this paper, we propose a spatio temporal principal component analysis to initialize CP decomposition component. We take full advantage of the spatial and temporal structure of the data in the initialization step for cp component analysis. The performance of our method is tested via comparison with most popular initialization method. We also run a clustering analysis to further show the performance of our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10322v3</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatoumata Sanogo</dc:creator>
    </item>
    <item>
      <title>A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2511.06204</link>
      <description>arXiv:2511.06204v2 Announce Type: replace 
Abstract: Popular technologies for generating spatially resolved transcriptomic data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify and profile discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that DUET can achieve better clustering and deconvolution performance than existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06204v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyun Jung Koo, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>Waiting for Dabo: A machine learning model for predicting Power 4 college football coaching hire success</title>
      <link>https://arxiv.org/abs/2511.14035</link>
      <description>arXiv:2511.14035v2 Announce Type: replace 
Abstract: Using data on 103 recent P4 college football hires, we built a statistical model for predicting a coach's success at their new school. For each hire, we collected data about their background and experiences, the previous success as a head coach or coordinator and their success since hiring. Over 50 variables on these factors were recorded though we used 29 of these in building our predictive model. Our measure of success is based upon Bill Connelly's SP+ team ratings relative to the performance on the same metric of the school in the 15 year prior to their selection as head coach. Using a cross-validated regularized linear regression, we obtain a predictive model for coaching success. Among the important factors for predicting a successful hire are having been a previous college head coach, leaving a job as an Offensive Coordinator, age and quality of the hiring school's team in the previous 15 years. While we do find these factors are important for the prediction of a successful coaching hire, the trends here are weak. With 66\% accuracy, the model does identify coaching hires that will outperform team performance in the 15 years before the hire. However, no combination of these factors leads to high predictability of identifying a successful coaching hire. All of the data and code for this paper are available in a Github repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14035v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Schuckers, Austin Hayes</dc:creator>
    </item>
    <item>
      <title>A statistical method for crack pre-detection in 3D concrete images</title>
      <link>https://arxiv.org/abs/2402.16126</link>
      <description>arXiv:2402.16126v2 Announce Type: replace-cross 
Abstract: In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. Classical image-processing techniques and modern deep-learning models both face substantial computational challenges when applied directly to high resolution big data volumes. This paper introduces a statistical framework for crack pre-localization, whose purpose is not to replace or compete with segmentation networks, but to identify, with controlled error rates, the regions of a 3D CT image that are most likely to contain cracks. The method combines a simple Hessian-based filter, geometric descriptors computed on a regular spatial partition, and a spatial multiple testing procedure to detect anomalous regions while relying only on minimal calibration data, rather than large annotated datasets. Experiments on semi-synthetic and real 3D CT scans demonstrate that the proposed approach reliably highlights regions likely to contain cracks while preserving linear computational complexity. By restricting subsequent high resolution segmentation to these localized regions, deep-learning models can be trained and operate more efficiently, reducing both training runtime as well as resource consumption. The framework thus offers a practical and interpretable preprocessing step for large-scale CT inspection pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16126v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vitalii Makogin, Duc Nguyen, Evgeny Spodarev</dc:creator>
    </item>
    <item>
      <title>Analyzing distortion riskmetrics and weighted entropy for unimodal and symmetric distributions under partial information constraints</title>
      <link>https://arxiv.org/abs/2504.19725</link>
      <description>arXiv:2504.19725v2 Announce Type: replace-cross 
Abstract: In this paper, we develop the lower and upper bounds of worst-case distortion riskmetrics and weighted entropy for unimodal, and symmetric unimodal distributions when mean and variance information are available. We also consider the sharp upper bounds of distortion riskmetrics and weighted entropy for symmetric distribution under known mean and variance. These results are applied to (weighted) entropies, shortfalls and other risk measures. Specifically, entropies include cumulative Tsallis past entropy, cumulative residual Tsallis entropy of order {\alpha}, extended Gini coefficient, fractional generalized cumulative residual entropy, and fractional generalized cumulative entropy. Shortfalls include extended Gini shortfall, Gini shortfall, shortfall of cumulative residual entropy, and shortfall of cumulative residual Tsallis entropy. Other risk measures include nth-order expected shortfall, dual power principle and proportional hazard principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19725v2</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baishuai Zuo, Chuancun Yin</dc:creator>
    </item>
    <item>
      <title>Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient</title>
      <link>https://arxiv.org/abs/2505.13846</link>
      <description>arXiv:2505.13846v2 Announce Type: replace-cross 
Abstract: In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13846v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoru Shinoda, Hideaki Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Multidimensional constructs and moderated linear and nonlinear factor analysis</title>
      <link>https://arxiv.org/abs/2509.05443</link>
      <description>arXiv:2509.05443v2 Announce Type: replace-cross 
Abstract: Multidimensional factor models with moderations on all model parameters have so far been limited to single-factor and two-factor models. This does not align well with existing psychological measures, which are commonly intended to assess 3-5 dimensions of a latent construct. In this paper, I introduce a multidimensional MNLFA model that permits the moderation of item intercepts, loadings, residual variances, factor means, variances, and correlations across three or more latent factors. I describe efforts to implement the model using Bayesian methods through Stan and penalized maximum likelihood approaches to stabilize estimation and detect partial measurement non-invariance while preserving model interpretability. Closed-form analytic gradients of the likelihood, eliminating the need for costly numerical or MCMC-based approximations. We conclude by discussing the theoretical implications of penalization for measurement invariance, computational considerations, and future directions for extending the framework to categorical indicators, longitudinal data, and applied research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05443v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett</dc:creator>
    </item>
    <item>
      <title>Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2511.16628</link>
      <description>arXiv:2511.16628v2 Announce Type: replace-cross 
Abstract: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16628v2</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tammam Bakeer, Max Herbers, Steffen Marx</dc:creator>
    </item>
  </channel>
</rss>
