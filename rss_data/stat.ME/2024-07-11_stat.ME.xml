<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</title>
      <link>https://arxiv.org/abs/2407.07933</link>
      <description>arXiv:2407.07933v1 Announce Type: new 
Abstract: We consider the challenging problem of estimating causal effects from purely observational data in the bi-directional Mendelian randomization (MR), where some invalid instruments, as well as unmeasured confounding, usually exist. To address this problem, most existing methods attempt to find proper valid instrumental variables (IVs) for the target causal effect by expert knowledge or by assuming that the causal model is a one-directional MR model. As such, in this paper, we first theoretically investigate the identification of the bi-directional MR from observational data. In particular, we provide necessary and sufficient conditions under which valid IV sets are correctly identified such that the bi-directional MR model is identifiable, including the causal directions of a pair of phenotypes (i.e., the treatment and outcome). Moreover, based on the identification theory, we develop a cluster fusion-like method to discover valid IV sets and estimate the causal effects of interest. We theoretically demonstrate the correctness of the proposed algorithm. Experimental results show the effectiveness of our method for estimating causal effects in bi-directional MR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07933v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Xie, Zhen Yao, Lin Xie, Yan Zeng, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Identifying macro conditional independencies and macro total effects in summary causal graphs with latent confounding</title>
      <link>https://arxiv.org/abs/2407.07934</link>
      <description>arXiv:2407.07934v1 Announce Type: new 
Abstract: Understanding causal relationships in dynamic systems is essential for numerous scientific fields, including epidemiology, economics, and biology. While causal inference methods have been extensively studied, they often rely on fully specified causal graphs, which may not always be available or practical in complex dynamic systems. Partially specified causal graphs, such as summary causal graphs (SCGs), provide a simplified representation of causal relationships, omitting temporal information and focusing on high-level causal structures. This simplification introduces new challenges concerning the types of queries of interest: macro queries, which involve relationships between clusters represented as vertices in the graph, and micro queries, which pertain to relationships between variables that are not directly visible through the vertices of the graph. In this paper, we first clearly distinguish between macro conditional independencies and micro conditional independencies and between macro total effects and micro total effects. Then, we demonstrate the soundness and completeness of the d-separation to identify macro conditional independencies in SCGs. Furthermore, we establish that the do-calculus is sound and complete for identifying macro total effects in SCGs. Conversely, we also show through various examples that these results do not hold when considering micro conditional independencies and micro total effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07934v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Variational Bayes for Mixture of Gaussian Structural Equation Models</title>
      <link>https://arxiv.org/abs/2407.08140</link>
      <description>arXiv:2407.08140v1 Announce Type: new 
Abstract: Structural equation models (SEMs) are commonly used to study the structural relationship between observed variables and latent constructs. Recently, Bayesian fitting procedures for SEMs have received more attention thanks to their potential to facilitate the adoption of more flexible model structures, and variational approximations have been shown to provide fast and accurate inference for Bayesian analysis of SEMs. However, the application of variational approximations is currently limited to very simple, elemental SEMs. We develop mean-field variational Bayes algorithms for two SEM formulations for data that present non-Gaussian features such as skewness and multimodality. The proposed models exploit the use of mixtures of Gaussians, include covariates for the analysis of latent traits and consider missing data. We also examine two variational information criteria for model selection that are straightforward to compute in our variational inference framework. The performance of the MFVB algorithms and information criteria is investigated in a simulated data study and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08140v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khue-Dung Dang, Luca Maestrini, Francis K. C. Hui</dc:creator>
    </item>
    <item>
      <title>Wasserstein $k$-Centres Clustering for Distributional Data</title>
      <link>https://arxiv.org/abs/2407.08228</link>
      <description>arXiv:2407.08228v1 Announce Type: new 
Abstract: We develop a novel clustering method for distributional data, where each data point is regarded as a probability distribution on the real line. For distributional data, it has been challenging to develop a clustering method that utilizes the mode of variation of data because the space of probability distributions lacks a vector space structure, preventing the application of existing methods for functional data. In this study, we propose a novel clustering method for distributional data on the real line, which takes account of difference in both the mean and mode of variation structures of clusters, in the spirit of the $k$-centres clustering approach proposed for functional data. Specifically, we consider the space of distributions equipped with the Wasserstein metric and define a geodesic mode of variation of distributional data using geodesic principal component analysis. Then, we utilize the geodesic mode of each cluster to predict the cluster membership of each distribution. We theoretically show the validity of the proposed clustering criterion by studying the probability of correct membership. Through a simulation study and real data application, we demonstrate that the proposed distributional clustering method can improve cluster quality compared to conventional clustering algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08228v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Okano, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Structuring, Sequencing, Staging, Selecting: the 4S method for the longitudinal analysis of multidimensional measurement scales in chronic diseases</title>
      <link>https://arxiv.org/abs/2407.08278</link>
      <description>arXiv:2407.08278v1 Announce Type: new 
Abstract: In clinical studies, measurement scales are often collected to report disease-related manifestations from clinician or patient perspectives. Their analysis can help identify relevant manifestations throughout the disease course, enhancing knowledge of disease progression and guiding clinicians in providing appropriate support. However, the analysis of measurement scales in health studies is not straightforward as made of repeated, ordinal, and potentially multidimensional item data. Their sum-score summaries may considerably reduce information and impend interpretation, their change over time occurs along clinical progression, and as many other longitudinal processes, their observation may be truncated by events. This work establishes a comprehensive strategy in four consecutive steps to leverage repeated data from multidimensional measurement scales. The 4S method successively (1) identifies the scale structure into subdimensions satisfying three calibration assumptions (unidimensionality, conditional independence, increasing monotonicity), (2) describes each subdimension progression using a joint latent process model which includes a continuous-time item response theory model for the longitudinal subpart, (3) aligns each subdimension's progression with disease stages through a projection approach, and (4) identifies the most informative items across disease stages using the Fisher's information. The method is comprehensively illustrated in multiple system atrophy (MSA), an alpha-synucleinopathy, with the analysis of daily activity and motor impairments over disease progression. The 4S method provides an effective and complete analytical strategy for any measurement scale repeatedly collected in health studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08278v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiphaine Saulnier, Wassilios G. Meissner, Margherita Fabbri, Alexandra Foubert-Samier, C\'ecile Proust-Lima</dc:creator>
    </item>
    <item>
      <title>Inference procedures in sequential trial emulation with survival outcomes: comparing confidence intervals based on the sandwich variance estimator, bootstrap and jackknife</title>
      <link>https://arxiv.org/abs/2407.08317</link>
      <description>arXiv:2407.08317v1 Announce Type: new 
Abstract: Sequential trial emulation (STE) is an approach to estimating causal treatment effects by emulating a sequence of target trials from observational data. In STE, inverse probability weighting is commonly utilised to address time-varying confounding and/or dependent censoring. Then structural models for potential outcomes are applied to the weighted data to estimate treatment effects. For inference, the simple sandwich variance estimator is popular but conservative, while nonparametric bootstrap is computationally expensive, and a more efficient alternative, linearised estimating function (LEF) bootstrap, has not been adapted to STE. We evaluated the performance of various methods for constructing confidence intervals (CIs) of marginal risk differences in STE with survival outcomes by comparing the coverage of CIs based on nonparametric/LEF bootstrap, jackknife, and the sandwich variance estimator through simulations. LEF bootstrap CIs demonstrated the best coverage with small/moderate sample sizes, low event rates and low treatment prevalence, which were the motivating scenarios for STE. They were less affected by treatment group imbalance and faster to compute than nonparametric bootstrap CIs. With large sample sizes and medium/high event rates, the sandwich-variance-estimator-based CIs had the best coverage and were the fastest to compute. These findings offer guidance in constructing CIs in causal survival analysis using STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08317v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juliette M. Limozin, Shaun R. Seaman, Li Su</dc:creator>
    </item>
    <item>
      <title>Adjusting for Participation Bias in Case-Control Genetic Association Studies for Rare Diseases</title>
      <link>https://arxiv.org/abs/2407.08382</link>
      <description>arXiv:2407.08382v1 Announce Type: new 
Abstract: Collection of genotype data in case-control genetic association studies may often be incomplete for reasons related to genes themselves. This non-ignorable missingness structure, if not appropriately accounted for, can result in participation bias in association analyses. To deal with this issue, Chen et al. (2016) proposed to collect additional genetic information from family members of individuals whose genotype data were not available, and developed a maximum likelihood method for bias correction. In this study, we develop an estimating equation approach to analyzing data collected from this design that allows adjustment of covariates. It jointly estimates odds ratio parameters for genetic association and missingness, where a logistic regression model is used to relate missingness to genotype and other covariates. Our method allows correlation between genotype and covariates while using genetic information from family members to provide information on the missing genotype data. In the estimating equation for genetic association parameters, we weight the contribution of each genotyped subject to the empirical likelihood score function by the inverse probability that the genotype data are available. We evaluate large and finite sample performance of our method via simulation studies and apply it to a family-based case-control study of breast cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08382v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Wang, Zhengbang Li, Ben Fitzpatrick, Clarice Weinberg, Jinbo Chen</dc:creator>
    </item>
    <item>
      <title>Information matrix test for normality of innovations in stationary time series models</title>
      <link>https://arxiv.org/abs/2407.08565</link>
      <description>arXiv:2407.08565v1 Announce Type: new 
Abstract: This study focuses on the problem of testing for normality of innovations in stationary time series models.To achieve this, we introduce an information matrix (IM) based test. While the IM test was originally developed to test for model misspecification, our study addresses that the test can also be used to test for the normality of innovations in various time series models. We provide sufficient conditions under which the limiting null distribution of the test statistics exists. As applications, a first-order threshold moving average model, GARCH model and double autoregressive model are considered. We conduct simulations to evaluate the performance of the proposed test and compare with other tests, and provide a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08565v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Liu, Junmo Song</dc:creator>
    </item>
    <item>
      <title>Goodness of fit of relational event models</title>
      <link>https://arxiv.org/abs/2407.08599</link>
      <description>arXiv:2407.08599v1 Announce Type: new 
Abstract: A type of dynamic network involves temporally ordered interactions between actors, where past network configurations may influence future ones. The relational event model can be used to identify the underlying dynamics that drive interactions among system components. Despite the rapid development of this model over the past 15 years, an ongoing area of research revolves around evaluating the goodness of fit of this model, especially when it incorporates time-varying and random effects. Current methodologies often rely on comparing observed and simulated events using specific statistics, but this can be computationally intensive, and requires various assumptions.
  We propose an additive mixed-effect relational event model estimated via case-control sampling, and introduce a versatile framework for testing the goodness of fit of such models using weighted martingale residuals. Our focus is on a Kolmogorov-Smirnov type test designed to assess if covariates are accurately modeled. Our approach can be easily extended to evaluate whether other features of network dynamics have been appropriately incorporated into the model. We assess the goodness of fit of various relational event models using synthetic data to evaluate the test's power and coverage. Furthermore, we apply the method to a social study involving 57,791 emails sent by 159 employees of a Polish manufacturing company in 2010.
  The method is implemented in the R package mgcv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08599v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, Ernst-Jan Camiel Wit</dc:creator>
    </item>
    <item>
      <title>Reduced-Rank Matrix Autoregressive Models: A Medium $N$ Approach</title>
      <link>https://arxiv.org/abs/2407.07973</link>
      <description>arXiv:2407.07973v1 Announce Type: cross 
Abstract: Reduced-rank regressions are powerful tools used to identify co-movements within economic time series. However, this task becomes challenging when we observe matrix-valued time series, where each dimension may have a different co-movement structure. We propose reduced-rank regressions with a tensor structure for the coefficient matrix to provide new insights into co-movements within and between the dimensions of matrix-valued time series. Moreover, we relate the co-movement structures to two commonly used reduced-rank models, namely the serial correlation common feature and the index model. Two empirical applications involving U.S.\ states and economic indicators for the Eurozone and North American countries illustrate how our new tools identify co-movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07973v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Hecq, Ivan Ricardo, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Gradual changes in functional time series</title>
      <link>https://arxiv.org/abs/2407.07996</link>
      <description>arXiv:2407.07996v1 Announce Type: cross 
Abstract: We consider the problem of detecting gradual changes in the sequence of mean functions from a not necessarily stationary functional time series. Our approach is based on the maximum deviation (calculated over a given time interval) between a benchmark function and the mean functions at different time points. We speak of a gradual change of size $\Delta $, if this quantity exceeds a given threshold $\Delta&gt;0$. For example, the benchmark function could represent an average of yearly temperature curves from the pre-industrial time, and we are interested in the question if the yearly temperature curves afterwards deviate from the pre-industrial average by more than $\Delta =1.5$ degrees Celsius, where the deviations are measured with respect to the sup-norm. Using Gaussian approximations for high-dimensional data we develop a test for hypotheses of this type and estimators for the time where a deviation of size larger than $\Delta$ appears for the first time. We prove the validity of our approach and illustrate the new methods by a simulation study and a data example, where we analyze yearly temperature curves at different stations in Australia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07996v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Gaussian process interpolation with conformal prediction: methods and comparative analysis</title>
      <link>https://arxiv.org/abs/2407.08271</link>
      <description>arXiv:2407.08271v1 Announce Type: cross 
Abstract: This article advocates the use of conformal prediction (CP) methods for Gaussian process (GP) interpolation to enhance the calibration of prediction intervals. We begin by illustrating that using a GP model with parameters selected by maximum likelihood often results in predictions that are not optimally calibrated. CP methods can adjust the prediction intervals, leading to better uncertainty quantification while maintaining the accuracy of the underlying GP model. We compare different CP variants and introduce a novel variant based on an asymmetric score. Our numerical experiments demonstrate the effectiveness of CP methods in improving calibration without compromising accuracy. This work aims to facilitate the adoption of CP methods in the GP community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08271v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>LOD 2024, 10th International Conference on Machine Learning, Optimization, and Data Science, Sep 2024, Castiglione della Pescaia Grosseto Italy, Italy</arxiv:journal_reference>
      <dc:creator>Aur\'elien Pion, Emmanuel Vazquez</dc:creator>
    </item>
    <item>
      <title>Local logistic regression for dimension reduction in classification</title>
      <link>https://arxiv.org/abs/2407.08485</link>
      <description>arXiv:2407.08485v1 Announce Type: cross 
Abstract: Sufficient dimension reduction has received much interest over the past 30 years. Most existing approaches focus on statistical models linking the response to the covariate through a regression equation, and as such are not adapted to binary classification problems. We address the question of dimension reduction for binary classification by fitting a localized nearest-neighbor logistic model with $\ell_1$-penalty in order to estimate the gradient of the conditional probability of interest. Our theoretical analysis shows that the pointwise convergence rate of the gradient estimator is optimal under very mild conditions. The dimension reduction subspace is estimated using an outer product of such gradient estimates at several points in the covariate space. Our implementation uses cross-validation on the misclassification rate to estimate the dimension of this subspace. We find that the proposed approach outperforms existing competitors in synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08485v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Fran\c{c}ois Portier, Gilles Stupfler</dc:creator>
    </item>
    <item>
      <title>Causal inference through multi-stage learning and doubly robust deep neural networks</title>
      <link>https://arxiv.org/abs/2407.08560</link>
      <description>arXiv:2407.08560v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) have demonstrated remarkable empirical performance in large-scale supervised learning problems, particularly in scenarios where both the sample size $n$ and the dimension of covariates $p$ are large. This study delves into the application of DNNs across a wide spectrum of intricate causal inference tasks, where direct estimation falls short and necessitates multi-stage learning. Examples include estimating the conditional average treatment effect and dynamic treatment effect. In this framework, DNNs are constructed sequentially, with subsequent stages building upon preceding ones. To mitigate the impact of estimation errors from early stages on subsequent ones, we integrate DNNs in a doubly robust manner. In contrast to previous research, our study offers theoretical assurances regarding the effectiveness of DNNs in settings where the dimensionality $p$ expands with the sample size. These findings are significant independently and extend to degenerate single-stage learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08560v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqian Zhang, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>The exact non-Gaussian weak lensing likelihood: A framework to calculate analytic likelihoods for correlation functions on masked Gaussian random fields</title>
      <link>https://arxiv.org/abs/2407.08718</link>
      <description>arXiv:2407.08718v1 Announce Type: cross 
Abstract: We present exact non-Gaussian joint likelihoods for auto- and cross-correlation functions on arbitrarily masked spherical Gaussian random fields. Our considerations apply to spin-0 as well as spin-2 fields but are demonstrated here for the spin-2 weak-lensing correlation function.
  We motivate that this likelihood cannot be Gaussian and show how it can nevertheless be calculated exactly for any mask geometry and on a curved sky, as well as jointly for different angular-separation bins and redshift-bin combinations. Splitting our calculation into a large- and small-scale part, we apply a computationally efficient approximation for the small scales that does not alter the overall non-Gaussian likelihood shape.
  To compare our exact likelihoods to correlation-function sampling distributions, we simulated a large number of weak-lensing maps, including shape noise, and find excellent agreement for one-dimensional as well as two-dimensional distributions. Furthermore, we compare the exact likelihood to the widely employed Gaussian likelihood and find significant levels of skewness at angular separations $\gtrsim 1^{\circ}$ such that the mode of the exact distributions is shifted away from the mean towards lower values of the correlation function. We find that the assumption of a Gaussian random field for the weak-lensing field is well valid at these angular separations.
  Considering the skewness of the non-Gaussian likelihood, we evaluate its impact on the posterior constraints on $S_8$. On a simplified weak-lensing-survey setup with an area of $10 \ 000 \ \mathrm{deg}^2$, we find that the posterior mean of $S_8$ is up to $2\%$ higher when using the non-Gaussian likelihood, a shift comparable to the precision of current stage-III surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08718v1</guid>
      <category>astro-ph.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronika Oehl, Tilman Tr\"oster</dc:creator>
    </item>
    <item>
      <title>Doubly ranked tests of location for grouped functional data</title>
      <link>https://arxiv.org/abs/2306.14761</link>
      <description>arXiv:2306.14761v3 Announce Type: replace 
Abstract: Nonparametric tests for functional data are a challenging class of tests to work with because of the potentially high dimensional nature of the data. One of the main challenges for considering rank-based tests, like the Mann-Whitney or Wilcoxon Rank Sum tests (MWW), is that the unit of observation is typically a curve. Thus any rank-based test must consider ways of ranking curves. While several procedures, including depth-based methods, have recently been used to create scores for rank-based tests, these scores are not constructed under the null and often introduce additional, uncontrolled for variability. We therefore reconsider the problem of rank-based tests for functional data and develop an alternative approach that incorporates the null hypothesis throughout. Our approach first ranks realizations from the curves at each measurement occurrence, then calculates a summary statistic for the ranks of each subject, and finally re-ranks the summary statistic in a procedure we refer to as a doubly ranked test. We propose two summaries for the middle step: a sufficient statistic and the average rank. As we demonstrate, doubly rank tests are more powerful while maintaining ideal type I error in the two sample, MWW setting. We also extend our framework to more than two samples, developing a Kruskal-Wallis test for functional data which exhibits good test characteristics as well. Finally, we illustrate the use of doubly ranked tests in functional data contexts from material science, climatology, and public health policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14761v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer</dc:creator>
    </item>
    <item>
      <title>Models for temporal clustering of extreme events with applications to mid-latitude winter cyclones</title>
      <link>https://arxiv.org/abs/2308.14625</link>
      <description>arXiv:2308.14625v2 Announce Type: replace 
Abstract: The occurrence of extreme events like heavy precipitation or storms at a certain location often shows a clustering behaviour and is thus not described well by a Poisson process. We construct a general model for the inter-exceedance times in between such events which combines different candidate models for such behaviour. This allows us to distinguish data generating mechanisms leading to clusters of dependent events with exponential inter-exceedance times in between clusters from independent events with heavy-tailed inter-exceedance times, and even allows us to combine these two mechanisms for better descriptions of such occurrences. We propose a modification of the Cram\'er-von Mises distance for model fitting. An application to mid-latitude winter cyclones illustrates the usefulness of our work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14625v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christina Mathieu, Katharina Hees, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Adaptive Experiments Toward Learning Treatment Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2312.06883</link>
      <description>arXiv:2312.06883v3 Announce Type: replace 
Abstract: Understanding treatment effect heterogeneity has become an increasingly popular task in various fields, as it helps design personalized advertisements in e-commerce or targeted treatment in biomedical studies. However, most of the existing work in this research area focused on either analyzing observational data based on strong causal assumptions or conducting post hoc analyses of randomized controlled trial data, and there has been limited effort dedicated to the design of randomized experiments specifically for uncovering treatment effect heterogeneity. In the manuscript, we develop a framework for designing and analyzing response adaptive experiments toward better learning treatment effect heterogeneity. Concretely, we provide response adaptive experimental design frameworks that sequentially revise the data collection mechanism according to the accrued evidence during the experiment. Such design strategies allow for the identification of subgroups with the largest treatment effects with enhanced statistical efficiency. The proposed frameworks not only unify adaptive enrichment designs and response-adaptive randomization designs but also complement A/B test designs in e-commerce and randomized trial designs in clinical settings. We demonstrate the merit of our design with theoretical justifications and in simulation studies with synthetic e-commerce and clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06883v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waverly Wei, Xinwei Ma, Jingshen Wang</dc:creator>
    </item>
    <item>
      <title>Conjugacy properties of multivariate unified skew-elliptical distributions</title>
      <link>https://arxiv.org/abs/2402.09837</link>
      <description>arXiv:2402.09837v2 Announce Type: replace 
Abstract: The broad class of multivariate unified skew-normal (SUN) distributions has been recently shown to possess important conjugacy properties. When used as priors for the vector of parameters in general probit, tobit, and multinomial probit models, these distributions yield posteriors that still belong to the SUN family. Although such a core result has led to important advancements in Bayesian inference and computation, its applicability beyond likelihoods associated with fully-observed, discretized, or censored realizations from multivariate Gaussian models remains yet unexplored. This article covers such an important gap by proving that the wider family of multivariate unified skew-elliptical (SUE) distributions, which extends SUNs to more general perturbations of elliptical densities, guarantees conjugacy for broader classes of models, beyond those relying on fully-observed, discretized or censored Gaussians. Such a result leverages the closure under linear combinations, conditioning and marginalization of SUE to prove that this family is conjugate to the likelihood induced by general multivariate regression models for fully-observed, censored or dichotomized realizations from skew-elliptical distributions. This advancement enlarges the set of models that enable conjugate Bayesian inference to general formulations arising from elliptical and skew-elliptical families, including the multivariate Student's t and skew-t, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09837v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maicon J. Karling, Daniele Durante, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Robust CATE Estimation Using Novel Ensemble Methods</title>
      <link>https://arxiv.org/abs/2407.03690</link>
      <description>arXiv:2407.03690v3 Announce Type: replace 
Abstract: The estimation of Conditional Average Treatment Effects (CATE) is crucial for understanding the heterogeneity of treatment effects in clinical trials. We evaluate the performance of common methods, including causal forests and various meta-learners, across a diverse set of scenarios, revealing that each of the methods struggles in one or more of the tested scenarios. Given the inherent uncertainty of the data-generating process in real-life scenarios, the robustness of a CATE estimator to various scenarios is critical for its reliability. To address this limitation of existing methods, we propose two new ensemble methods that integrate multiple estimators to enhance prediction stability and performance - Stacked X-Learner which uses the X-Learner with model stacking for estimating the nuisance functions, and Consensus Based Averaging (CBA), which averages only the models with highest internal agreement. We show that these models achieve good performance across a wide range of scenarios varying in complexity, sample size and structure of the underlying-mechanism, including a biologically driven model for PD-L1 inhibition pathway for cancer treatment. Furthermore, we demonstrate improved performance by the Stacked X-Learner also when comparing to other ensemble methods, including R-Stacking, Causal-Stacking and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03690v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oshri Machluf, Tzviel Frostig, Gal Shoham, Tomer Milo, Elad Berkman, Raviv Pryluk</dc:creator>
    </item>
    <item>
      <title>New User Event Prediction Through the Lens of Causal Inference</title>
      <link>https://arxiv.org/abs/2407.05625</link>
      <description>arXiv:2407.05625v2 Announce Type: replace 
Abstract: Modeling and analysis for event series generated by heterogeneous users of various behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. The most commonly adopted approach to this task is to classify users into behavior-based categories and analyze each of them separately. However, this approach requires extensive data to fully understand user behavior, presenting challenges in modeling newcomers without historical knowledge. In this paper, we propose a novel discrete event prediction framework for new users through the lens of causal inference. Our method offers an unbiased prediction for new users without needing to know their categories. We treat the user event history as the ''treatment'' for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, with the new user model trained on an adjusted dataset where each event is re-weighted by its inverse propensity score. We demonstrate the superior performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05625v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Shaowu Yuchi, Shixiang Zhu, Li Dong, Yigit M. Arisoy, Matthew C. Spencer</dc:creator>
    </item>
    <item>
      <title>Adjustment Identification Distance: A gadjid for Causal Structure Learning</title>
      <link>https://arxiv.org/abs/2402.08616</link>
      <description>arXiv:2402.08616v2 Announce Type: replace-cross 
Abstract: Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop new reachability algorithms to compute the distances efficiently and to prove their low polynomial time complexity. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster with proven lower time complexity than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08616v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonard Henckel, Theo W\"urtzen, Sebastian Weichwald</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach with Gaussian priors to the inverse problem of source identification in elliptic PDEs</title>
      <link>https://arxiv.org/abs/2402.19214</link>
      <description>arXiv:2402.19214v2 Announce Type: replace-cross 
Abstract: We consider the statistical linear inverse problem of making inference on an unknown source function in an elliptic partial differential equation from noisy observations of its solution. We employ nonparametric Bayesian procedures based on Gaussian priors, leading to convenient conjugate formulae for posterior inference. We review recent results providing theoretical guarantees on the quality of the resulting posterior-based estimation and uncertainty quantification, and we discuss the application of the theory to the important classes of Gaussian series priors defined on the Dirichlet-Laplacian eigenbasis and Mat\'ern process priors. We provide an implementation of posterior inference for both classes of priors, and investigate its performance in a numerical simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19214v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Automatic Outlier Rectification via Optimal Transport</title>
      <link>https://arxiv.org/abs/2403.14067</link>
      <description>arXiv:2403.14067v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize the optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the outlier during the optimization process. We demonstrate the effectiveness of our approach over conventional approaches in simulations and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14067v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Blanchet, Jiajin Li, Markus Pelger, Greg Zanotti</dc:creator>
    </item>
  </channel>
</rss>
