<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 01:45:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The 2023/24 VIEWS Prediction Challenge: Predicting the Number of Fatalities in Armed Conflict, with Uncertainty</title>
      <link>https://arxiv.org/abs/2407.11045</link>
      <description>arXiv:2407.11045v1 Announce Type: new 
Abstract: This draft article outlines a prediction challenge where the target is to forecast the number of fatalities in armed conflicts, in the form of the UCDP `best' estimates, aggregated to the VIEWS units of analysis. It presents the format of the contributions, the evaluation metric, and the procedures, and a brief summary of the contributions. The article serves a function analogous to a pre-analysis plan: a statement of the forecasting models made publicly available before the true future prediction window commences. More information on the challenge, and all data referred to in this document, can be found at https://viewsforecasting.org/research/prediction-challenge-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11045v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H{\aa}vard Hegre (Peace Research Institute Oslo), Paola Vesco (Peace Research Institute Oslo), Michael Colaresi (Department of Peace and Conflict Research, Uppsala University), Jonas Vestby (Peace Research Institute Oslo), Alexa Timlick (Peace Research Institute Oslo), Noorain Syed Kazmi (Peace Research Institute Oslo), Friederike Becker (Institute of Statistics), Marco Binetti (Center for Crisis Early Warning, University of the Bundeswehr Munich), Tobias Bodentien (Institute of Statistics), Tobias Bohne (Center for Crisis Early Warning, University of the Bundeswehr Munich), Patrick T. Brandt (School of Economic, Political, and Policy Sciences, University of Texas, Dallas), Thomas Chadefaux (Trinity College Dublin), Simon Drauz (Institute of Statistics), Christoph Dworschak (University of York), Vito D'Orazio (West Virginia University), Cornelius Fritz (Pennsylvania State University), Hannah Frank (Trinity College Dublin), Kristian Skrede Gleditsch (University of Essex), Sonja H\"affner (Center for Crisis Early Warning, University of the Bundeswehr Munich), Martin Hofer (University College London), Finn L. Klebe (University College London), Luca Macis (Department of Economics and Statistics Cognetti de Martiis, University of Turin), Alexandra Malaga (Institute for Economic Analysis, Barcelona), Marius Mehrl (University of Leeds), Nils W. Metternich (University College London), Daniel Mittermaier (Center for Crisis Early Warning, University of the Bundeswehr Munich), David Muchlinski (Georgia Tech), Hannes Mueller (Institute for Economic Analysis, Barcelona), Christian Oswald (Center for Crisis Early Warning, University of the Bundeswehr Munich), Paola Pisano (Department of Economics and Statistics Cognetti de Martiis, University of Turin), David Randahl (Department of Peace and Conflict Research, Uppsala University), Christopher Rauh (University of Cambridge), Lotta R\"uter (Institute of Statistics), Thomas Schincariol (Trinity College Dublin), Benjamin Seimon (Fundaci\'o Economia Analitica), Elena Siletti (Department of Economics and Statistics Cognetti de Martiis, University of Turin), Marco Tagliapietra (Department of Economics and Statistics Cognetti de Martiis, University of Turin), Chandler Thornhill (Georgia Tech), Johan Vegelius (Department of Medical Sciences, Uppsala University), Julian Walterskirchen (Center for Crisis Early Warning, University of the Bundeswehr Munich)</dc:creator>
    </item>
    <item>
      <title>Detecting Outbreaks Using a Latent Field: Part II -- Scalable Estimation</title>
      <link>https://arxiv.org/abs/2407.11233</link>
      <description>arXiv:2407.11233v1 Announce Type: new 
Abstract: In this paper, we explore whether the infection-rate of a disease can serve as a robust monitoring variable in epidemiological surveillance algorithms. The infection-rate is dependent on population mixing patterns that do not vary erratically day-to-day; in contrast, daily case-counts used in contemporary surveillance algorithms are corrupted by reporting errors. The technical challenge lies in estimating the latent infection-rate from case-counts. Here we devise a Bayesian method to estimate the infection-rate across multiple adjoining areal units, and then use it, via an anomaly detector, to discern a change in epidemiological dynamics. We extend an existing model for estimating the infection-rate in an areal unit by incorporating a Markov random field model, so that we may estimate infection-rates across multiple areal units, while preserving spatial correlations observed in the epidemiological dynamics. To carry out the high-dimensional Bayesian inverse problem, we develop an implementation of mean-field variational inference specific to the infection model and integrate it with the random field model to incorporate correlations across counties. The method is tested on estimating the COVID-19 infection-rates across all 33 counties in New Mexico using data from the summer of 2020, and then employing them to detect the arrival of the Fall 2020 COVID-19 wave. We perform the detection using a temporal algorithm that is applied county-by-county. We also show how the infection-rate field can be used to cluster counties with similar epidemiological dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11233v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wyatt Bridgman, Cosmin Safta, Jaideep Ray</dc:creator>
    </item>
    <item>
      <title>Redistricting Reforms Reduce Gerrymandering by Constraining Partisan Actors</title>
      <link>https://arxiv.org/abs/2407.11336</link>
      <description>arXiv:2407.11336v1 Announce Type: new 
Abstract: Political actors frequently manipulate redistricting plans to gain electoral advantages, a process commonly known as gerrymandering. To address this problem, several states have implemented institutional reforms including the establishment of map-drawing commissions. It is difficult to assess the impact of such reforms because each state structures bundles of complex rules in different ways. We propose to model redistricting processes as a sequential game. The equilibrium solution to the game summarizes multi-step institutional interactions as a single dimensional score. This score measures the leeway political actors have over the partisan lean of the final plan. Using a differences-in-differences design, we demonstrate that reforms reduce partisan bias and increase competitiveness when they constrain partisan actors. We perform a counterfactual policy analysis to estimate the partisan effects of enacting recent institutional reforms nationwide. We find that instituting redistricting commissions generally reduces the current Republican advantage, but Michigan-style reforms would yield a much greater pro-Democratic effect than types of redistricting commissions adopted in Ohio and New York.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11336v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cory McCartan, Christopher T. Kenny, Tyler Simko, Emma Ebowe, Michael Y. Zhao, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Bayesian Emulation of Geotechnical Deterioration Curves Using Quadratic and B-Spline Hierarchical Models</title>
      <link>https://arxiv.org/abs/2407.11597</link>
      <description>arXiv:2407.11597v1 Announce Type: new 
Abstract: The stability of geotechnical infrastructure assets, such as cuttings and embankments, is crucial to the safe and efficient delivery of transport services. The successful emulation of geotechnical models of deterioration of infrastructure slopes has the potential to inform slope design, maintenance and remediation by introducing the time dependency of deterioration into geotechnical asset management. We have performed computer experiments of deterioration, measured by the factor of safety (FoS), for a set of cutting slope geometries and soil properties that are common in the southern UK. Whilst computer experiments are an extremely useful and cost-effective method of better understanding deterioration mechanisms, it would not be practical to run enough experiments to understand relations between high-dimensional inputs and outputs. Therefore, we trained a fully-Bayesian Gaussian process emulator using an ensemble of 75 computer experiments to predict the FoS. We construct two different emulator models, one approximating the FoS temporal evolution with a quadratic model and one approximating the temporal evolution with a B-spline model; and we emulated their parameters. We also compare the ability of our models to predict failure time. The developed models could be used to inform infrastructure cutting slope design and management, and extend serviceable life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11597v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan L. Oakley, Aleksandra Svalova, Peter Helm, Mohamed Rouainia, Stephanie Glendinning, Dennis Prangle, Darren Wilkinson</dc:creator>
    </item>
    <item>
      <title>Urban mobility and learning: analyzing the influence of commuting time on students' GPA at Politecnico di Milano</title>
      <link>https://arxiv.org/abs/2407.11893</link>
      <description>arXiv:2407.11893v1 Announce Type: new 
Abstract: Despite its crucial role in students' daily lives, commuting time remains an underexplored dimension in higher education research. To address this gap, this study focuses on challenges that students face in urban environments and investigates the impact of commuting time on the academic performance of first-year bachelor students of Politecnico di Milano, Italy. This research employs an innovative two-step methodology. In the initial phase, machine learning algorithms trained on privacy-preserving GPS data from anonymous users are used to construct accessibility maps to the university and to obtain an estimate of students' commuting times. In the subsequent phase, authors utilize polynomial linear mixed-effects models and investigate the factors influencing students' academic performance, with a particular emphasis on commuting time. Notably, this investigation incorporates a causal framework, which enables the establishment of causal relationships between commuting time and academic outcomes. The findings underscore the significant impact of travel time on students' performance and may support policies and implications aiming at improving students' educational experience in metropolitan areas. The study's innovation lies both in its exploration of a relatively uncharted factor and the novel methodologies applied in both phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11893v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/03075079.2024.2374005</arxiv:DOI>
      <arxiv:journal_reference>Studies in Higher Education 2024</arxiv:journal_reference>
      <dc:creator>Arianna Burzacchi, Lidia Rossi, Tommaso Agasisti, Anna Maria Paganoni, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Learning Cellular Network Connection Quality with Conformal</title>
      <link>https://arxiv.org/abs/2407.10976</link>
      <description>arXiv:2407.10976v1 Announce Type: cross 
Abstract: In this paper, we address the problem of uncertainty quantification for cellular network speed. It is a well-known fact that the actual internet speed experienced by a mobile phone can fluctuate significantly, even when remaining in a single location. This high degree of variability underscores that mere point estimation of network speed is insufficient. Rather, it is advantageous to establish a prediction interval that can encompass the expected range of speed variations. In order to build an accurate network estimation map, numerous mobile data need to be collected at different locations. Currently, public datasets rely on users to upload data through apps. Although massive data has been collected, the datasets suffer from significant noise due to the nature of cellular networks and various other factors. Additionally, the uneven distribution of population density affects the spatial consistency of data collection, leading to substantial uncertainty in the network quality maps derived from this data. We focus our analysis on large-scale internet-quality datasets provided by Ookla to construct an estimated map of connection quality. To improve the reliability of this map, we introduce a novel conformal prediction technique to build an uncertainty map. We identify regions with heightened uncertainty to prioritize targeted, manual data collection. In addition, the uncertainty map quantifies how reliable the prediction is in different areas. Our method also leads to a sampling strategy that guides researchers to selectively gather high-quality data that best complement the current dataset to improve the overall accuracy of the prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10976v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanyang Jiang, Elizabeth Belding, Ellen Zegure, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian inference for high-resolution spatial disaggregation using alternative data sources</title>
      <link>https://arxiv.org/abs/2407.11173</link>
      <description>arXiv:2407.11173v1 Announce Type: cross 
Abstract: This paper addresses the challenge of obtaining precise demographic information at a fine-grained spatial level, a necessity for planning localized public services such as water distribution networks, or understanding local human impacts on the ecosystem. While population sizes are commonly available for large administrative areas, such as wards in India, practical applications often demand knowledge of population density at smaller spatial scales. We explore the integration of alternative data sources, specifically satellite-derived products, including land cover, land use, street density, building heights, vegetation coverage, and drainage density. Using a case study focused on Bangalore City, India, with a ward-level population dataset for 198 wards and satellite-derived sources covering 786,702 pixels at a resolution of 30mX30m, we propose a semiparametric Bayesian spatial regression model for obtaining pixel-level population estimates. Given the high dimensionality of the problem, exact Bayesian inference is deemed impractical; we discuss an approximate Bayesian inference scheme based on the recently proposed max-and-smooth approach, a combination of Laplace approximation and Markov chain Monte Carlo. A simulation study validates the reasonable performance of our inferential approach. Mapping pixel-level estimates to the ward level demonstrates the effectiveness of our method in capturing the spatial distribution of population sizes. While our case study focuses on a demographic application, the methodology developed here readily applies to count-type spatial datasets from various scientific disciplines, where high-resolution alternative data sources are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anis Pakrashi, Arnab Hazra, Sooraj M Raveendran, Krishnachandran Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Scaling Hawkes processes to one million COVID-19 cases</title>
      <link>https://arxiv.org/abs/2407.11349</link>
      <description>arXiv:2407.11349v1 Announce Type: cross 
Abstract: Hawkes stochastic point process models have emerged as valuable statistical tools for analyzing viral contagion. The spatiotemporal Hawkes process characterizes the speeds at which viruses spread within human populations. Unfortunately, likelihood-based inference using these models requires $O(N^2)$ floating-point operations, for $N$ the number of observed cases. Recent work responds to the Hawkes likelihood's computational burden by developing efficient graphics processing unit (GPU)-based routines that enable Bayesian analysis of tens-of-thousands of observations. We build on this work and develop a high-performance computing (HPC) strategy that divides 30 Markov chains between 4 GPU nodes, each of which uses multiple GPUs to accelerate its chain's likelihood computations. We use this framework to apply two spatiotemporal Hawkes models to the analysis of one million COVID-19 cases in the United States between March 2020 and June 2023. In addition to brute-force HPC, we advocate for two simple strategies as scalable alternatives to successful approaches proposed for small data settings. First, we use known county-specific population densities to build a spatially varying triggering kernel in a manner that avoids computationally costly nearest neighbors search. Second, we use a cut-posterior inference routine that accounts for infections' spatial location uncertainty by iteratively sampling latent locations uniformly within their respective counties of occurrence, thereby avoiding full-blown latent variable inference for 1,000,000 infection locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11349v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyoon Ko, Marc A. Suchard, Andrew J. Holbrook</dc:creator>
    </item>
    <item>
      <title>Using shrinkage methods to estimate treatment effects in overlapping subgroups in randomized clinical trials with a time-to-event endpoint</title>
      <link>https://arxiv.org/abs/2407.11729</link>
      <description>arXiv:2407.11729v1 Announce Type: cross 
Abstract: In randomized controlled trials, forest plots are frequently used to investigate the homogeneity of treatment effect estimates in subgroups. However, the interpretation of subgroup-specific treatment effect estimates requires great care due to the smaller sample size of subgroups and the large number of investigated subgroups. Bayesian shrinkage methods have been proposed to address these issues, but they often focus on disjoint subgroups while subgroups displayed in forest plots are overlapping, i.e., each subject appears in multiple subgroups. In our approach, we first build a flexible Cox model based on all available observations, including categorical covariates that identify the subgroups of interest and their interactions with the treatment group variable. We explore both penalized partial likelihood estimation with a lasso or ridge penalty for treatment-by-covariate interaction terms, and Bayesian estimation with a regularized horseshoe prior. One advantage of the Bayesian approach is the ability to derive credible intervals for shrunken subgroup-specific estimates. In a second step, the Cox model is marginalized to obtain treatment effect estimates for all subgroups. We illustrate these methods using data from a randomized clinical trial in follicular lymphoma and evaluate their properties in a simulation study. In all simulation scenarios, the overall mean-squared error is substantially smaller for penalized and shrinkage estimators compared to the standard subgroup-specific treatment effect estimator but leads to some bias for heterogeneous subgroups. We recommend that subgroup-specific estimators, which are typically displayed in forest plots, are more routinely complemented by treatment effect estimators based on shrinkage methods. The proposed methods are implemented in the R package bonsaiforest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11729v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcel Wolbers, Mar V\'azquez Rabu\~nal, Ke Li, Kaspar Rufibach, Daniel Saban\'es Bov\'e</dc:creator>
    </item>
    <item>
      <title>Self-Duplicating Random Walks for Resilient Decentralized Learning on Graphs</title>
      <link>https://arxiv.org/abs/2407.11762</link>
      <description>arXiv:2407.11762v1 Announce Type: cross 
Abstract: Consider the setting of multiple random walks (RWs) on a graph executing a certain computational task. For instance, in decentralized learning via RWs, a model is updated at each iteration based on the local data of the visited node and then passed to a randomly chosen neighbor. RWs can fail due to node or link failures. The goal is to maintain a desired number of RWs to ensure failure resilience. Achieving this is challenging due to the lack of a central entity to track which RWs have failed to replace them with new ones by forking (duplicating) surviving ones. Without duplications, the number of RWs will eventually go to zero, causing a catastrophic failure of the system. We propose a decentralized algorithm called DECAFORK that can maintain the number of RWs in the graph around a desired value even in the presence of arbitrary RW failures. Nodes continuously estimate the number of surviving RWs by estimating their return time distribution and fork the RWs when failures are likely to happen. We present extensive numerical simulations that show the performance of DECAFORK regarding fast detection and reaction to failures. We further present theoretical guarantees on the performance of this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11762v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Egger, Ghadir Ayache, Rawad Bitar, Antonia Wachter-Zeh, Salim El Rouayheb</dc:creator>
    </item>
    <item>
      <title>On the optimal prediction of extreme events in heavy-tailed time series with applications to solar flare forecasting</title>
      <link>https://arxiv.org/abs/2407.11887</link>
      <description>arXiv:2407.11887v1 Announce Type: cross 
Abstract: The prediction of extreme events in time series is a fundamental problem arising in many financial, scientific, engineering, and other applications. We begin by establishing a general Neyman-Pearson-type characterization of optimal extreme event predictors in terms of density ratios. This yields new insights and several closed-form optimal extreme event predictors for additive models. These results naturally extend to time series, where we study optimal extreme event prediction for heavy-tailed autoregressive and moving average models. Using a uniform law of large numbers for ergodic time series, we establish the asymptotic optimality of an empirical version of the optimal predictor for autoregressive models. Using multivariate regular variation, we also obtain expressions for the optimal extremal precision in heavy-tailed infinite moving averages, which provide theoretical bounds on the ability to predict extremes in this general class of models. The developed theory and methodology is applied to the important problem of solar flare prediction based on the state-of-the-art GOES satellite flux measurements of the Sun. Our results demonstrate the success and limitations of long-memory autoregressive as well as long-range dependent heavy-tailed FARIMA models for the prediction of extreme solar flares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11887v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Victor Verma, Stilian Stoev, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian Causal Forests for Longitudinal Data: Assessing the Impact of Part-Time Work on Growth in High School Mathematics Achievement</title>
      <link>https://arxiv.org/abs/2407.11927</link>
      <description>arXiv:2407.11927v1 Announce Type: cross 
Abstract: Modelling growth in student achievement is a significant challenge in the field of education. Understanding how interventions or experiences such as part-time work can influence this growth is also important. Traditional methods like difference-in-differences are effective for estimating causal effects from longitudinal data. Meanwhile, Bayesian non-parametric methods have recently become popular for estimating causal effects from single time point observational studies. However, there remains a scarcity of methods capable of combining the strengths of these two approaches to flexibly estimate heterogeneous causal effects from longitudinal data. Motivated by two waves of data from the High School Longitudinal Study, the NCES' most recent longitudinal study which tracks a representative sample of over 20,000 students in the US, our study introduces a longitudinal extension of Bayesian Causal Forests. This model allows for the flexible identification of both individual growth in mathematical ability and the effects of participation in part-time work. Simulation studies demonstrate the predictive performance and reliable uncertainty quantification of the proposed model. Results reveal the negative impact of part time work for most students, but hint at potential benefits for those students with an initially low sense of school belonging. Clear signs of a widening achievement gap between students with high and low academic achievement are also identified. Potential policy implications are discussed, along with promising areas for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11927v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan McJames, Ann O'Shea, Andrew Parnell</dc:creator>
    </item>
    <item>
      <title>A Robust Bayesian Meta-Analysis for Estimating the Hubble Constant via Time Delay Cosmography</title>
      <link>https://arxiv.org/abs/2308.13018</link>
      <description>arXiv:2308.13018v2 Announce Type: replace 
Abstract: We propose a Bayesian meta-analysis to infer the current expansion rate of the Universe, called the Hubble constant ($H_0$), via time delay cosmography. Inputs of the meta-analysis are estimates of two properties for each pair of gravitationally lensed images; time delay and Fermat potential difference estimates with their standard errors. A meta-analysis can be appealing in practice because obtaining each estimate from even a single lens system involves substantial human efforts, and thus estimates are often separately obtained and published. Moreover, numerous estimates are expected to be available once the Rubin Observatory starts monitoring thousands of strong gravitational lens systems. This work focuses on combining these estimates from independent studies to infer $H_0$ in a robust manner. The robustness is crucial because currently up to eight lens systems are used to infer $H_0$, and thus any biased input can severely affect the resulting $H_0$ estimate. For this purpose, we adopt Student's $t$ error for the input estimates. We investigate properties of the resulting $H_0$ estimate via two simulation studies with realistic imaging data. It turns out that the meta-analysis can infer $H_0$ with sub-percent bias and about 1% level of coefficient of variation, even when 30% of inputs are manipulated to be outliers. We also apply the meta-analysis to three gravitationally lensed systems to obtain an $H_0$ estimate and compare it with existing estimates. An R package, h0, is publicly available for fitting the proposed meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13018v2</guid>
      <category>stat.AP</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyungsuk Tak, Xuheng Ding</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Modeling of Age-Specific Counts in Many Demographic Subpopulations</title>
      <link>https://arxiv.org/abs/2401.08247</link>
      <description>arXiv:2401.08247v2 Announce Type: replace 
Abstract: Analyzing age-specific mortality, fertility, and migration patterns is a crucial task in demography, with significant policy relevance. In practice, such analysis is challenging when studying a large number of subpopulations, due to small observation counts within groups and increasing demographic heterogeneity between groups. This article proposes a Bayesian model for the joint analysis of age-specific counts in many, potentially small, demographic subpopulations. The model utilizes smooth latent factors to capture common age-specific patterns across subpopulations and encourages additional information sharing through a hierarchical prior. It provides smoothed estimates of the latent age pattern in each subpopulation, allows testing for heterogeneity, and can be used to assess the impact of covariates on the demographic process. An in-depth case study of age-specific immigration flows to Austria, disaggregated by sex and 155 countries of origin, is discussed. Comparative analysis demonstrates that the model outperforms commonly used benchmark frameworks in both in-sample imputation and out-of-sample predictive exercises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08247v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens</dc:creator>
    </item>
    <item>
      <title>Impacts of Climate Change on Mortality: An extrapolation of temperature effects based on time series data in France</title>
      <link>https://arxiv.org/abs/2406.02054</link>
      <description>arXiv:2406.02054v2 Announce Type: replace 
Abstract: Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02054v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Guibert (CEREMADE), Ga\"elle Pincemin (SAF), Fr\'ed\'eric Planchet (SAF)</dc:creator>
    </item>
    <item>
      <title>SciConNav: Knowledge navigation through contextual learning of extensive scientific research trajectories</title>
      <link>https://arxiv.org/abs/2401.11742</link>
      <description>arXiv:2401.11742v3 Announce Type: replace-cross 
Abstract: New knowledge builds upon existing foundations, which means an interdependent relationship exists between knowledge, manifested in the historical development of the scientific system for hundreds of years. By leveraging natural language processing techniques, this study introduces the Scientific Concept Navigator (SciConNav), an embedding-based navigation model to infer the "knowledge pathway" from the research trajectories of millions of scholars. We validate that the learned representations effectively delineate disciplinary boundaries and capture the intricate relationships between diverse concepts. The utility of the inferred navigation space is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the interconnectivity between concepts in different disciplines. Secondly, we formulated the attribute dimensions of knowledge across domains, observing the distributional shifts in the arrangement of 19 disciplines along these conceptual dimensions, including "Theoretical" to "Applied", and "Chemical" to "Biomedical', highlighting the evolution of functional attributes within knowledge domains. Lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects with shorter global pathways, and interdisciplinary knowledge plays a critical role in the accessibility of the global knowledge network. Our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific progression patterns, tailoring scientific learning trajectories, and accelerating scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11742v3</guid>
      <category>cs.IR</category>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shibing Xiang, Xin Jiang, Bing Liu, Yurui Huang, Chaolin Tian, Yifang Ma</dc:creator>
    </item>
  </channel>
</rss>
