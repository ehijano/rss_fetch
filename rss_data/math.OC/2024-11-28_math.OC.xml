<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Improved Dung Beetle Optimizer for Random Forest Optimization</title>
      <link>https://arxiv.org/abs/2411.17738</link>
      <description>arXiv:2411.17738v1 Announce Type: new 
Abstract: To improve the convergence speed and optimization accuracy of the Dung Beetle Optimizer (DBO), this paper proposes an improved algorithm based on circle mapping and longitudinal-horizontal crossover strategy (CICRDBO). First, the Circle method is used to map the initial population to increase diversity. Second, the longitudinal-horizontal crossover strategy is applied to enhance the global search ability by ensuring the position updates of the dung beetle. Simulations were conducted on 10 benchmark test functions, and the results demonstrate that the improved algorithm performs well in both convergence speed and optimization accuracy. The improved algorithm is further applied to the hyperparameter selection of the Random Forest classification algorithm for binary classification prediction in the retail industry. Various combination comparisons prove the practicality of the improved algorithm, followed by SHapley Additive exPlanations (SHAP) analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17738v1</guid>
      <category>math.OC</category>
      <category>cs.NE</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lianghao Tan, Xiaoyi Liu, Dong Liu, Shubing Liu, Weixi Wu, Huangqi Jiang</dc:creator>
    </item>
    <item>
      <title>Efficient Mathematical Programming Formulation and Algorithmic Framework for Optimal Camera Placement</title>
      <link>https://arxiv.org/abs/2411.17942</link>
      <description>arXiv:2411.17942v1 Announce Type: new 
Abstract: Optimal camera placement plays a crucial role in applications such as surveillance, environmental monitoring, and infrastructure inspection. Even highly abstracted versions of this problem are NP-hard due to the high-dimensional continuous domain of camera configurations (i.e., positions and orientations) and difficulties in efficiently and accurately calculating camera coverage. In this paper, we present a novel framework for optimal camera placement that uses integer programming and adaptive sampling strategies to maximize coverage, given a limited camera budget. We develop a modified maximum k-coverage formulation and two adaptive sampling strategies, Explore and Exploit (E&amp;E) and Target Uncovered Spaces (TUS), that iteratively add new camera configurations to the candidate set in order to improve the solution. E&amp;E focuses on local search around camera configurations chosen in previous iterations, whereas TUS focuses specifically on covering regions that were previously uncovered. We first conduct theoretical analysis to provide bounds on the probability of finding an optimal solution and expected sampling needs, while ensuring monotonic improvements in coverage. Then, we conduct a detailed numerical analysis over different environments. Results show that E&amp;E achieves coverage improvements of 3.3-16.0% over all baseline random sampling approaches, while maintaining manageable computational times. Meanwhile, TUS performs well in open environments and with tight camera budgets, achieving gains of 6.9-9.1% in such conditions. Compared to the baseline, our approach achieves similar coverage using only 30-70% of the sampling budget, demonstrating its computational efficiency. Through a case study, we obtain insights into optimal camera placement decisions for a typical indoor surveillance application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17942v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yash Kumar, Raghu Bollapragada, Benjamin D. Leibowicz</dc:creator>
    </item>
    <item>
      <title>First-order hold lossless convexification: theoretical guarantees for discrete-time optimal control problems</title>
      <link>https://arxiv.org/abs/2411.18004</link>
      <description>arXiv:2411.18004v1 Announce Type: new 
Abstract: Lossless Convexification (LCvx) is a clever trick that transforms a class of nonconvex optimal control problems (where the nonconvexity arises from a lower bound on the control norm) into equivalent convex problems via convex relaxations, the goal being to solve these problems efficiently via polynomial-time numerical solvers. However, to solve these infinite-dimensional problems in practice, they must first be converted into finite-dimensional problems, and it remains an open area of research to ensure the theoretical guarantees of LCvx are maintained across this discretization step. Prior work has proven guarantees for zero-order hold control parameterization. In this work, we extend these results to the more general, and practically useful, first-order hold control parameterization. We first show that under mild assumptions, we are guaranteed a solution that violates our nonconvex constraint at no more than $n_x + 1$ vertices in our discretized trajectory (where $n_x$ is the dimension of our state-space). Then, we discuss an algorithm that, for a specific case of problems, finds a solution where our nonconvex constraint is violated along no more than $2n_x + 2$ edges in at most $\lceil \log_2 ((\rho_{\max} - \rho_{\min}) / \varepsilon_\rho) \rceil + 1$ calls to our solver (where $[\rho_{\min}, \rho_{\max}]$ represent the bounds on our control norm and $\varepsilon_\rho$ is some desired suboptimality tolerance). Finally, we provide numerical results demonstrating the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18004v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shosuke Kiami</dc:creator>
    </item>
    <item>
      <title>Derivative-free stochastic bilevel optimization for inverse problems</title>
      <link>https://arxiv.org/abs/2411.18100</link>
      <description>arXiv:2411.18100v1 Announce Type: new 
Abstract: Inverse problems are key issues in several scientific areas, including signal processing and medical imaging. Data-driven approaches for inverse problems aim for learning model and regularization parameters from observed data samples, and investigate their generalization properties when confronted with unseen data. This approach dictates a statistical approach to inverse problems, calling for stochastic optimization methods. In order to learn model and regularisation parameters simultaneously, we develop in this paper a stochastic bilevel optimization approach in which the lower level problem represents a variational reconstruction method formulated as a convex non-smooth optimization problem, depending on the observed sample. The upper level problem represents the learning task of the regularisation parameters. Combining the lower level and the upper level problem leads to a stochastic non-smooth and non-convex optimization problem, for which standard gradient-based methods are not straightforward to implement. Instead, we develop a unified and flexible methodology, building on a derivative-free approach, which allows us to solve the bilevel optimization problem only with samples of the objective function values. We perform a complete complexity analysis of this scheme. Numerical results on signal denoising and experimental design demonstrate the computational efficiency and the generalization properties of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18100v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Staudigl, Simon Weissmann, Tristan van Leeuwen</dc:creator>
    </item>
    <item>
      <title>Adjoint-based Recovery of Thermal Fields from Displacement or Strain Measurements</title>
      <link>https://arxiv.org/abs/2411.18118</link>
      <description>arXiv:2411.18118v1 Announce Type: new 
Abstract: A finite-element method dependant adjoint-based procedure to determine the temperature field of structures based on measured displacements or strains and a set of standard loads is developed and tested. Given a series of force and deformation measurements, the temperature field is obtained by minimizing the adequately weighted differences between the measured and computed values. Three numerical examples - a Plate With a Hole, a Bridge, and a Hoover Dam example - each with multiple sensors distributed in different configurations, demonstrate the procedure's capabilities. A target temperature distribution is prescribed in all cases, and the displacement sensor data is recorded. The optimization algorithm (here, steepest descent with Barzilai-Borwein step) uses this data to optimize the temperatures such that the same deformation is obtained at the sensor locations. Vertex Morphing is used as a filter to mitigate the ill-conditioning. Results show that the proposed approach can accurately reconstruct the target thermal distribution, especially when more sensors are used. Additionally, it is observed that the sensors do not need to be positioned in the region of interest; the method remains effective as long as the sensors can detect changes related to that area. A comparison with standard spatial interpolation techniques, namely, k-nearest neighbors and ordinary and universal kriging, is performed using temperature sensors in the same configurations. The proposed approach performs remarkably better than the interpolation techniques with a reduction in the root-mean-squared error of up to 38.4%, 94%, and 40%, for the Plate With a Hole, the Bridge, and the Dam examples, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18118v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Talhah Shamshad Ali Ansari, Rainald L\"ohner, Roland W\"uchner, Harbir Antil, Suneth Warnakulasuriya, Ihar Antonau, Facundo Airaudo</dc:creator>
    </item>
    <item>
      <title>Optimizing Flexibility in Power Systems by Maximizing the Region of Manageable Uncertainties</title>
      <link>https://arxiv.org/abs/2411.18178</link>
      <description>arXiv:2411.18178v1 Announce Type: new 
Abstract: Motivated by the increasing need to hedge against load and generation uncertainty in the operation of power grids, we propose flexibility maximization during operation.
  We consider flexibility explicitly as the amount of uncertainty that can be handled while still ensuring nominal grid operation in the worst-case. We apply the proposed flexibility optimization in the context of a DC flow approximation. By using a corresponding parameterization, we can find the maximal range of uncertainty and a range for the manageable power transfer between two parts of a network subject to uncertainty.
  We formulate the corresponding optimization problem as an (existence-constrained) semi-infinite optimization problem and specialize an existing algorithm for its solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18178v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aron Zingler, Stephane Fliscounakis, Patrick Panciatici, Alexander Mitsos</dc:creator>
    </item>
    <item>
      <title>On analysis of open optimization algorithms</title>
      <link>https://arxiv.org/abs/2411.18219</link>
      <description>arXiv:2411.18219v1 Announce Type: new 
Abstract: We develop analysis results for optimization algorithms that are open, that is, with inputs and outputs. Such algorithms arise for instance, when analyzing the effect of noise or disturbance on an algorithm, or when an algorithm is part of control loop without timescale separation. To be precise, we consider an incremental small gain problem to analyze robustness. Moreover, we investigate the behaviors of the closed loop between incrementally dissipative nonlinear plants and optimization algorithms. The framework we develop is built upon the theories of incremental dissipativity and monotone operators, and yields tests in the form of linear matrix inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18219v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaap Eising, Florian D\"orfler</dc:creator>
    </item>
    <item>
      <title>Learning optimal objective values for MILP</title>
      <link>https://arxiv.org/abs/2411.18321</link>
      <description>arXiv:2411.18321v1 Announce Type: new 
Abstract: Modern Mixed Integer Linear Programming (MILP) solvers use the Branch-and-Bound algorithm together with a plethora of auxiliary components that speed up the search. In recent years, there has been an explosive development in the use of machine learning for enhancing and supporting these algorithmic components. Within this line, we propose a methodology for predicting the optimal objective value, or, equivalently, predicting if the current incumbent is optimal. For this task, we introduce a predictor based on a graph neural network (GNN) architecture, together with a set of dynamic features. Experimental results on diverse benchmarks demonstrate the efficacy of our approach, achieving high accuracy in the prediction task and outperforming existing methods. These findings suggest new opportunities for integrating ML-driven predictions into MILP solvers, enabling smarter decision-making and improved performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18321v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lara Scavuzzo, Karen Aardal, Neil Yorke-Smith</dc:creator>
    </item>
    <item>
      <title>Flow Shop Scheduling with Inter-Stage Flexibility and Blocking Constraints</title>
      <link>https://arxiv.org/abs/2411.18381</link>
      <description>arXiv:2411.18381v1 Announce Type: new 
Abstract: We investigate a scheduling problem arising from a material handling and processing problem in a production line of an Austrian company building prefabricated house walls. The addressed problem is a permutation flow shop with blocking constraints in which the machine of at least one stage can process a number operations of two other stages in the system. This situation is usually referred to as multi-task or inter-stage flexibility. The problem is in general NP-hard, but we derive a number of special cases that can be solved in polynomial time. For the general case, we present a variety of heuristic algorithms, with a focus on matheuristics that are grounded in two different mixed-integer linear programming (MIP) formulations of the problem. These matheuristics leverage the strengths of exact optimization techniques while introducing flexibility to address limits on computation time. To assess the performance of the proposed approaches, we conduct an extensive computational study on randomly generated test cases based on real-world instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18381v1</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gaia Nicosia, Andrea Pacifici, Ulrich Pferschy, Anna Russo Russo, Cecilia Salvatore</dc:creator>
    </item>
    <item>
      <title>Inertial dynamics with vanishing Tikhonov regularization for multobjective optimization</title>
      <link>https://arxiv.org/abs/2411.18422</link>
      <description>arXiv:2411.18422v1 Announce Type: new 
Abstract: In this paper we introduce, in a Hilbert space setting, a second order dynamical system with asymptotically vanishing damping and vanishing Tikhonov regularization that approaches a multiobjective optimization problem with convex and differentiable components of the objective function. Trajectory solutions are shown to exist in finite dimensions. We prove fast convergence of the function values, quantified in terms of a merit function. Based on the regime considered, we establish both weak and, in some cases, strong convergence of trajectory solutions toward a weak Pareto optimal solution. To achieve this, we apply Tikhonov regularization individually to each component of the objective function. This work extends results from single objective convex optimization into the multiobjective setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18422v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bot, Konstantin Sonntag</dc:creator>
    </item>
    <item>
      <title>Insensitizing controls for stochastic parabolic equations with dynamic boundary conditions</title>
      <link>https://arxiv.org/abs/2411.18501</link>
      <description>arXiv:2411.18501v1 Announce Type: new 
Abstract: In this paper, we continue the study of some controllability issues for the forward stochastic heat equation with dynamic boundary conditions. The main novelty in the present paper consists of considering only one control without extra forces in the noise parts. Under a strong measurability condition, and using a spectral inequality, we first establish an appropriate observability inequality for the corresponding adjoint system. Then, by the classical duality approach, the null and approximate controllability results are established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18501v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Boulite, Abdellatif Elgrou, Lahcen Maniar, Omar Oukdach</dc:creator>
    </item>
    <item>
      <title>A neural network approach to learning solutions of a class of elliptic variational inequalities</title>
      <link>https://arxiv.org/abs/2411.18565</link>
      <description>arXiv:2411.18565v1 Announce Type: new 
Abstract: We develop a weak adversarial approach to solving obstacle problems using neural networks. By employing (generalised) regularised gap functions and their properties we rewrite the obstacle problem (which is an elliptic variational inequality) as a minmax problem, providing a natural formulation amenable to learning. Our approach, in contrast to much of the literature, does not require the elliptic operator to be symmetric. We provide an error analysis for suitable discretisations of the continuous problem, estimating in particular the approximation and statistical errors. Parametrising the solution and test function as neural networks, we apply a modified gradient descent ascent algorithm to treat the problem and conclude the paper with various examples and experiments. Our solution algorithm is in particular able to easily handle obstacle problems that feature biactivity (or lack of strict complementarity), a situation that poses difficulty for traditional numerical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18565v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Alphonse, Michael Hinterm\"uller, Alexander Kister, Chin Hang Lun, Clemens Sirotenko</dc:creator>
    </item>
    <item>
      <title>Generalized Fast Krasnosel'ki\u{\i}-Mann Method with Preconditioners</title>
      <link>https://arxiv.org/abs/2411.18574</link>
      <description>arXiv:2411.18574v1 Announce Type: new 
Abstract: We study accelerated Krasnosel'ki\u{\i}-Mann-type methods with preconditioners in both continuous and discrete time. From a continuous time model, we derive a generalized fast Krasnosel'ki\u{\i}-Mann method, providing a new yet simple proof of convergence that allows for unprecedented flexibility in parameter tuning. Our analysis unifies inertial and anchoring acceleration mechanisms and offers a broad range of parameter choices, which prove beneficial in practice. Additionally, we extend our analysis to the case where preconditioners are allowed to be degenerate, unifying the treatment of various splitting methods and establishing the weak convergence of shadow sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18574v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu I. Bo\c{t}, Enis Chenchene, Jalal M. Fadili</dc:creator>
    </item>
    <item>
      <title>Scalable iterative pruning of large language and vision models using block coordinate descent</title>
      <link>https://arxiv.org/abs/2411.17796</link>
      <description>arXiv:2411.17796v1 Announce Type: cross 
Abstract: Pruning neural networks, which involves removing a fraction of their weights, can often maintain high accuracy while significantly reducing model complexity, at least up to a certain limit. We present a neural network pruning technique that builds upon the Combinatorial Brain Surgeon, but solves an optimization problem over a subset of the network weights in an iterative, block-wise manner using block coordinate descent. The iterative, block-based nature of this pruning technique, which we dub ``iterative Combinatorial Brain Surgeon'' (iCBS) allows for scalability to very large models, including large language models (LLMs), that may not be feasible with a one-shot combinatorial optimization approach. When applied to large models like Mistral and DeiT, iCBS achieves higher performance metrics at the same density levels compared to existing pruning methods such as Wanda. This demonstrates the effectiveness of this iterative, block-wise pruning method in compressing and optimizing the performance of large deep learning models, even while optimizing over only a small fraction of the weights. Moreover, our approach allows for a quality-time (or cost) tradeoff that is not available when using a one-shot pruning technique alone. The block-wise formulation of the optimization problem enables the use of hardware accelerators, potentially offsetting the increased computational costs compared to one-shot pruning methods like Wanda. In particular, the optimization problem solved for each block is quantum-amenable in that it could, in principle, be solved by a quantum computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17796v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Elton Yechao Zhu, Serdar Kad{\i}o\u{g}lu, Sima E. Borujeni, Helmut G. Katzgraber</dc:creator>
    </item>
    <item>
      <title>Benchmarking Agility and Reconfigurability in Satellite Systems for Tropical Cyclone Monitoring</title>
      <link>https://arxiv.org/abs/2411.18317</link>
      <description>arXiv:2411.18317v1 Announce Type: cross 
Abstract: Tropical cyclones (TCs) are highly dynamic natural disasters that travel vast distances and occupy a large spatial scale, leading to loss of life, economic strife, and destruction of infrastructure. The severe impact of TCs makes them crucial to monitor such that the collected data contributes to forecasting their trajectory and severity, as well as the provision of information to relief agencies. Among the various methods used to monitor TCs, Earth observation satellites are the most flexible, allowing for frequent observations with a wide variety of instruments. Traditionally, satellite scheduling algorithms assume nadir-directional observations, a limitation that can be alleviated by incorporating satellite agility and constellation reconfigurability -- two state-of-the-art concepts of operations (CONOPS) that extend the amount of time TCs can be observed from orbit. This paper conducts a systematic comparative analysis between both CONOPS to present the performance of each relative to baseline nadir-directional observations in monitoring TCs. A dataset of 100 historical TCs is used to provide a benchmark concerning real-world data through maximizing the number of quality observations. The results of the comparative analysis indicate that constellation reconfigurability allowing plane-change maneuvers outperforms satellite agility in the majority of TCs analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18317v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brycen D. Pearl, Logan P. Gold, Hang Woon Lee</dc:creator>
    </item>
    <item>
      <title>SRG Analysis of Lur'e Systems and the Generalized Circle Criterion</title>
      <link>https://arxiv.org/abs/2411.18318</link>
      <description>arXiv:2411.18318v1 Announce Type: cross 
Abstract: Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of nonlinear systems. However, we show that the current SRG analysis suffers from some pitfalls that limit its applicability in analysing practical nonlinear systems. We overcome these pitfalls by modifying the SRG of a linear time invariant operator, combining the SRG with the Nyquist criterion, and apply our result to Lur'e systems. We thereby obtain a generalization of the celebrated circle criterion, which deals with broader class of nonlinearities, and provides (incremental) $L^2$-gain performance bounds. We illustrate the power of the new approach on the analysis of the controlled Duffing oscillator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18318v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius P. J. Krebbekx, Roland T\'oth, Amritam Das</dc:creator>
    </item>
    <item>
      <title>Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks</title>
      <link>https://arxiv.org/abs/2411.18384</link>
      <description>arXiv:2411.18384v1 Announce Type: cross 
Abstract: The rise of programmable data plane (PDP) and in-network computing (INC) paradigms paves the way for the development of network devices (switches, network interface cards, etc.) capable of performing advanced computing tasks. This allows to execute algorithms of various nature, including machine learning ones, within the network itself to support user and network services. In particular, this paper delves into the issue of implementing in-network learning models to support distributed intrusion detection systems (IDS). It proposes a model that optimally distributes the IDS workload, resulting from the subdivision of a "Strong Learner" (SL) model into lighter distributed "Weak Learner" (WL) models, among data plane devices; the objective is to ensure complete network security without excessively burdening their normal operations. Furthermore, a meta-heuristic approach is proposed to reduce the long computational time required by the exact solution provided by the mathematical model, and its performance is evaluated. The analysis conducted and the results obtained demonstrate the enormous potential of the proposed new approach to the creation of intelligent data planes that effectively act as a first line of defense against cyber attacks, with minimal additional workload on network devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18384v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mattia Giovanni Spina, Edoardo Scalzo, Floriano De Rango, Francesca Guerriero, Antonio Iera</dc:creator>
    </item>
    <item>
      <title>An End-to-End Smart Predict-then-Optimize Framework for Vehicle Relocation Problems in Large-Scale Vehicle Crowd Sensing</title>
      <link>https://arxiv.org/abs/2411.18432</link>
      <description>arXiv:2411.18432v1 Announce Type: cross 
Abstract: Ubiquitous mobile devices have catalyzed the development of vehicle crowd sensing (VCS). In particular, vehicle sensing systems show great potential in the flexible acquisition of spatio-temporal urban data through built-in sensors under diverse sensing scenarios. However, vehicle systems often exhibit biased coverage due to the heterogeneous nature of trip requests and routes. To achieve a high sensing coverage, a critical challenge lies in optimally relocating vehicles to minimize the divergence between vehicle distributions and target sensing distributions. Conventional approaches typically employ a two-stage predict-then-optimize (PTO) process: first predicting real-time vehicle distributions and subsequently generating an optimal relocation strategy based on the predictions. However, this approach can lead to suboptimal decision-making due to the propagation of errors from upstream prediction. To this end, we develop an end-to-end Smart Predict-then-Optimize (SPO) framework by integrating optimization into prediction within the deep learning architecture, and the entire framework is trained by minimizing the task-specific matching divergence rather than the upstream prediction error. Methodologically, we formulate the vehicle relocation problem by quadratic programming (QP) and incorporate a novel unrolling approach based on the Alternating Direction Method of Multipliers (ADMM) within the SPO framework to compute gradients of the QP layer, facilitating backpropagation and gradient-based optimization for end-to-end learning. The effectiveness of the proposed framework is validated by real-world taxi datasets in Hong Kong. Utilizing the alternating differentiation method, the general SPO framework presents a novel concept of addressing decision-making problems with uncertainty, demonstrating significant potential for advancing applications in intelligent transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18432v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Wang, Yiyang Peng, Wei Ma</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Recoverable Robust Optimization in the Polynomial Hierarchy</title>
      <link>https://arxiv.org/abs/2411.18590</link>
      <description>arXiv:2411.18590v1 Announce Type: cross 
Abstract: Recoverable robust optimization is a popular multi-stage approach, in which it is possible to adjust a first-stage solution after the uncertain cost scenario is revealed. We consider recoverable robust optimization in combination with discrete budgeted uncertainty. In this setting, it seems plausible that many problems become $\Sigma^p_3$-complete and therefore it is impossible to find compact IP formulations of them (unless the unlikely conjecture NP $= \Sigma^p_3$ holds). Even though this seems plausible, few concrete results of this kind are known. In this paper, we fill that gap of knowledge. We consider recoverable robust optimization for the nominal problems of Sat, 3Sat, vertex cover, dominating set, set cover, hitting set, feedback vertex set, feedback arc set, uncapacitated facility location, $p$-center, $p$-median, independent set, clique, subset sum, knapsack, partition, scheduling, Hamiltonian path/cycle (directed/undirected), TSP, $k$-disjoint path ($k \geq 2$), and Steiner tree. We show that for each of these problems, and for each of three widely used distance measures, the recoverable robust problem becomes $\Sigma^p_3$-complete. Concretely, we show that all these problems share a certain abstract property and prove that this property implies that their robust recoverable counterpart is $\Sigma^p_3$-complete. This reveals the insight that all the above problems are $\Sigma^p_3$-complete 'for the same reason'. Our result extends a recent framework by Gr\"une and Wulf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18590v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Conditions for representation of a function of many arguments as the difference of convex functions</title>
      <link>https://arxiv.org/abs/1708.06999</link>
      <description>arXiv:1708.06999v4 Announce Type: replace 
Abstract: There are given conditions for represention of a function of many arguments as the difference of convex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1708.06999v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Proudnikov</dc:creator>
    </item>
    <item>
      <title>Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints</title>
      <link>https://arxiv.org/abs/2212.11143</link>
      <description>arXiv:2212.11143v4 Announce Type: replace 
Abstract: In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints. Prior to our work, the best complexity bound was $\mathcal{O}(1/{\varepsilon})$, regardless of the strong convexity of the constraint function. It is unclear whether the strong convexity assumption can enable even better convergence results. To address this issue, we have developed novel techniques to progressively estimate the strong convexity of the Lagrangian function. Our approach, for the first time, effectively leverages the constraint strong convexity, obtaining an improved complexity of $\mathcal{O}(1/\sqrt{\varepsilon})$. This rate matches the complexity lower bound for strongly-convex-concave saddle point optimization and is therefore order-optimal. We show the superior performance of our methods in sparsity-inducing constrained optimization, notably Google's personalized PageRank problem. Furthermore, we show that a restarted version of the proposed methods can effectively identify the optimal solution's sparsity pattern within a finite number of steps, a result that appears to have independent significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11143v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenwei Lin, Qi Deng</dc:creator>
    </item>
    <item>
      <title>Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\"om method</title>
      <link>https://arxiv.org/abs/2403.02811</link>
      <description>arXiv:2403.02811v3 Announce Type: replace 
Abstract: In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\"om approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\"om approximation. More precisely, we study the linear quadratic regulator problem, showing that the approximated Riccati operator converges at the rate $m^{-1/2}$, and the regulator objective, for the associated solution of the optimal control problem, converges at the rate $m^{-1}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02811v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Caldarelli, Antoine Chatalic, Adri\`a Colom\'e, Cesare Molinari, Carlos Ocampo-Martinez, Carme Torras, Lorenzo Rosasco</dc:creator>
    </item>
    <item>
      <title>Distributed Prescribed-Time Convex Optimization: Cascade Design and Time-Varying Gain Approach</title>
      <link>https://arxiv.org/abs/2407.11413</link>
      <description>arXiv:2407.11413v2 Announce Type: replace 
Abstract: In this paper, we address the distributed prescribed-time convex optimization (DPTCO) problem for a class of nonlinear multi-agent systems (MASs) under undirected connected graph. A cascade design framework is proposed such that the DPTCO implementation is divided into two parts: distributed optimal trajectory generator design and local reference trajectory tracking controller design. The DPTCO problem is then transformed into the prescribed-time stabilization problem of a cascaded system. Changing Lyapunov function method and time-varying state transformation method together with the sufficient conditions are proposed to prove the prescribed-time stabilization of the cascaded system as well as the uniform boundedness of internal signals in the closed-loop systems. The proposed framework is then utilized to solve robust DPTCO problem for a class of chain-integrator MASs with external disturbances by constructing a novel variables and exploiting the property of time-varying gains. The proposed framework is further utilized to solve the adaptive DPTCO problem for a class of strict-feedback MASs with parameter uncertainty, in which backstepping method with prescribed-time dynamic filter is adopted. The descending power state transformation is introduced to compensate the growth of increasing rate induced by the derivative of time-varying gains in recursive steps and the high-order derivative of local reference trajectory is not required. Finally, theoretical results are verified by two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11413v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gewei Zuo, Lijun Zhu, Yujuan Wang, Zhiyong Chen</dc:creator>
    </item>
    <item>
      <title>Uncertain standard quadratic optimization under distributional assumptions: a chance-constrained epigraphic approach</title>
      <link>https://arxiv.org/abs/2411.14884</link>
      <description>arXiv:2411.14884v2 Announce Type: replace 
Abstract: The standard quadratic optimization problem (StQP) consists of minimizing a quadratic form over the standard simplex. Without convexity or concavity of the quadratic form, the StQP is NP-hard. This problem has many relevant real-life applications ranging portfolio optimization to pairwise clustering and replicator dynamics. Sometimes, the data matrix is uncertain. We investigate models where the distribution of the data matrix is known but where both the StQP after realization of the data matrix and the here-and-now problem are indefinite. We test the performance of a chance-constrained epigraphic StQP to the uncertain StQP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14884v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Immanuel M. Bomze, Daniel de Vicente</dc:creator>
    </item>
    <item>
      <title>Asymptotic analysis of dynamical systems driven by Poisson random measures with periodic sampling</title>
      <link>https://arxiv.org/abs/2207.08388</link>
      <description>arXiv:2207.08388v2 Announce Type: replace-cross 
Abstract: In this article, we study the dynamics of a nonlinear system governed by an ordinary differential equation under the combined influence of fast periodic sampling with period $\delta$ and small jump noise of size $\varepsilon, 0&lt; \varepsilon,\delta \ll 1.$ The noise is a combination of Brownian motion and Poisson random measure. The instantaneous rate of change of the state depends not only on its current value but on the most recent measurement of the state, as the state is measured at certain discrete-time instants. As $\varepsilon,\delta \searrow 0,$ the stochastic process of interest converges, in a suitable sense, to the dynamics of the deterministic equation. Next, the study of rescaled fluctuations of the stochastic process around its mean is found to vary depending on the relative rates of convergence of small parameters $\varepsilon, \delta$ in different asymptotic regimes. We show that the rescaled process converges, in a strong (path-wise) sense, to an effective process having an extra drift term capturing both the sampling and noise effect. Consequently, we obtain a first-order perturbation expansion of the stochastic process of interest, in terms of the effective process along with error bounds on the remainder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08388v2</guid>
      <category>math.PR</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivam Singh Dhama</dc:creator>
    </item>
    <item>
      <title>Completeness in the Polynomial Hierarchy for many natural Problems in Bilevel and Robust Optimization</title>
      <link>https://arxiv.org/abs/2311.10540</link>
      <description>arXiv:2311.10540v3 Announce Type: replace-cross 
Abstract: In bilevel and robust optimization we are concerned with combinatorial min-max problems, for example from the areas of min-max regret robust optimization, network interdiction, most vital vertex problems, blocker problems, and two-stage adjustable robust optimization. Even though these areas are well-researched for over two decades and one would naturally expect many (if not most) of the problems occurring in these areas to be complete for the classes $\Sigma^p_2$ or $\Sigma^p_3$ from the polynomial hierarchy, almost no hardness results in this regime are currently known. However, such complexity insights are important, since they imply that no polynomial-sized integer program for these min-max problems exist, and hence conventional IP-based approaches fail. We address this lack of knowledge by introducing over 70 new $\Sigma^p_2$-complete and $\Sigma^p_3$-complete problems. The majority of all earlier publications on $\Sigma^p_2$- and $\Sigma^p_3$-completeness in said areas are special cases of our meta-theorem. Precisely, we introduce a large list of problems for which the meta-theorem is applicable (including clique, vertex cover, knapsack, TSP, facility location and many more). We show that for every single of these problems, the corresponding min-max (i.e. interdiction/regret) variant is $\Sigma^p_2$- and the min-max-min (i.e. two-stage) variant is $\Sigma^p_3$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10540v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>GSE: Group-wise Sparse and Explainable Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2311.17434</link>
      <description>arXiv:2311.17434v4 Announce Type: replace-cross 
Abstract: Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\%$ attack success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17434v4</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Learning to Optimize for Mixed-Integer Non-linear Programming</title>
      <link>https://arxiv.org/abs/2410.11061</link>
      <description>arXiv:2410.11061v5 Announce Type: replace-cross 
Abstract: Mixed-integer non-linear programs (MINLPs) arise in various domains, such as energy systems and transportation, but are notoriously difficult to solve. Recent advances in machine learning have led to remarkable successes in optimization tasks, an area broadly known as learning to optimize. This approach includes using predictive models to generate solutions for optimization problems with continuous decision variables, thereby avoiding the need for computationally expensive optimization algorithms. However, applying learning to MINLPs remains challenging primarily due to the presence of integer decision variables, which complicate gradient-based learning. To address this limitation, we propose two differentiable correction layers that generate integer outputs while preserving gradient information. Combined with a soft penalty for constraint violation, our framework can tackle both the integrality and non-linear constraints in a MINLP. Experiments on three problem classes with convex/non-convex objective/constraints and integer/mixed-integer variables show that the proposed learning-based approach consistently produces high-quality solutions for parametric MINLPs extremely quickly. As problem size increases, traditional exact solvers and heuristic methods struggle to find feasible solutions, whereas our approach continues to deliver reliable results. Our work extends the scope of learning-to-optimize to MINLP, paving the way for integrating integer constraints into deep learning models. Our code is available at https://github.com/pnnl/L2O-pMINLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11061v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Tang, Elias B. Khalil, J\'an Drgo\v{n}a</dc:creator>
    </item>
    <item>
      <title>S-CFE: Simple Counterfactual Explanations</title>
      <link>https://arxiv.org/abs/2410.15723</link>
      <description>arXiv:2410.15723v3 Announce Type: replace-cross 
Abstract: We study the problem of finding optimal sparse, manifold-aligned counterfactual explanations for classifiers. Canonically, this can be formulated as an optimization problem with multiple non-convex components, including classifier loss functions and manifold alignment (or \emph{plausibility}) metrics. The added complexity of enforcing \emph{sparsity}, or shorter explanations, complicates the problem further. Existing methods often focus on specific models and plausibility measures, relying on convex $\ell_1$ regularizers to enforce sparsity. In this paper, we tackle the canonical formulation using the accelerated proximal gradient (APG) method, a simple yet efficient first-order procedure capable of handling smooth non-convex objectives and non-smooth $\ell_p$ (where $0 \leq p &lt; 1$) regularizers. This enables our approach to seamlessly incorporate various classifiers and plausibility measures while producing sparser solutions. Our algorithm only requires differentiable data-manifold regularizers and supports box constraints for bounded feature ranges, ensuring the generated counterfactuals remain \emph{actionable}. Finally, experiments on real-world datasets demonstrate that our approach effectively produces sparse, manifold-aligned counterfactual explanations while maintaining proximity to the factual data and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15723v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shpresim Sadiku, Moritz Wagner, Sai Ganesh Nagarajan, Sebastian Pokutta</dc:creator>
    </item>
  </channel>
</rss>
