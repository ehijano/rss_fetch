<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 01:46:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluation of A Spatial Microsimulation Framework for Small-Area Estimation of Population Health Outcomes Using the Behavioral Risk Factor Surveillance System</title>
      <link>https://arxiv.org/abs/2510.22080</link>
      <description>arXiv:2510.22080v1 Announce Type: new 
Abstract: This study introduces the Spatial Health and Population Estimator (SHAPE), a spatial microsimulation framework that applies hierarchical iterative proportional fitting (IPF) to estimate two health risk behaviors and eleven health outcomes across multiple spatial scales. SHAPE was evaluated using county-level direct estimates from the Behavioral Risk Factor Surveillance System (BRFSS) and both county and census tract level data from CDC PLACES for New York (2021) and Florida (2019). Results show that SHAPE's SAEs are moderately consistent with BRFSS (average Pearson's correlation coefficient r of about 0.5), similar to CDC PLACES (average r of about 0.6), and are strongly aligned with CDC PLACES model-based estimates at both county (average r of about 0.8) and census tract (average r of about 0.7) levels. SHAPE is an open, reproducible, and transparent framework programmed in R that meets a need for accessible SAE methods in public health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22080v1</guid>
      <category>stat.AP</category>
      <category>cs.MA</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Von Hoene, Aanya Gupta, Hamdi Kavak, Amira Roess, Taylor Anderson</dc:creator>
    </item>
    <item>
      <title>Understanding Carbon Trade Dynamics: A European Union Emissions Trading System Perspective</title>
      <link>https://arxiv.org/abs/2510.22341</link>
      <description>arXiv:2510.22341v1 Announce Type: new 
Abstract: The European Union Emissions Trading System (EU ETS), the worlds largest cap-and-trade carbon market, is central to EU climate policy. This study analyzes its efficiency, price behavior, and market structure from 2010 to 2020. Using an AR-GARCH framework, we find pronounced price clustering and short-term return predictability, with 60.05 percent directional accuracy and a 70.78 percent hit rate within forecast intervals. Network analysis of inter-country transactions shows a concentrated structure dominated by a few registries that control most high-value flows. Country-specific log-log regressions of price on traded quantity reveal heterogeneous and sometimes positive elasticities exceeding unity, implying that trading volumes often rise with prices. These results point to persistent inefficiencies in the EU ETS, including partial predictability, asymmetric market power, and unconventional price-volume relationships, suggesting that while the system contributes to decarbonization, its trading dynamics and price formation remain imperfect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22341v1</guid>
      <category>stat.AP</category>
      <category>q-fin.TR</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avirup Chakraborty</dc:creator>
    </item>
    <item>
      <title>Multimodal Fusion and Interpretability in Human Activity Recognition: A Reproducible Framework for Sensor-Based Modeling</title>
      <link>https://arxiv.org/abs/2510.22410</link>
      <description>arXiv:2510.22410v1 Announce Type: new 
Abstract: The research presents a comprehensive framework for consolidating multimodal sensor data collected under naturalistic conditions, grounded in the Carnegie Mellon University Multi-Modal Activity Database (CMU-MMAC). Focusing on Subject 07-Brownie, the study investigates the entire processing pipeline, from data alignment and transformation to fusion method evaluation, interpretability, and modality contribution. A unified preprocessing pipeline is developed to temporally align heterogeneous video and audio data. Fusion is performed through resampling, grayscale conversion, segmentation, and feature standardization. Semantic richness is confirmed via heatmaps, spectrograms, and luminance time series, while frame-aligned waveform overlays demonstrate temporal consistency. Results indicate that late fusion yields the highest validation accuracy, followed by hybrid fusion, with early fusion performing the lowest. To assess the interpretability and discriminative power of audio and video in fused activity recognition, PCA and t-SNE visualize feature coherence over time. Classification results show limited performance for audio alone, moderate for video, and significant improvement with multimodal fusion, underscoring the strengths of combined data. Incorporating RFID data, which captures sparse interactions asynchronously, further enhances recognition accuracy by over 50% and improves macro-averaged ROC-AUC. The framework demonstrates the potential to transform raw, asynchronous sensor data into aligned, semantically meaningful representations, providing a reproducible approach for multimodal data integration and interpretation in intelligent systems designed to perceive complex human activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22410v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiyao Yang, Yasemin Gulbahar</dc:creator>
    </item>
    <item>
      <title>Doubly Smoothed Density Estimation with Application on Miners' Unsafe Act Detection</title>
      <link>https://arxiv.org/abs/2510.22482</link>
      <description>arXiv:2510.22482v1 Announce Type: new 
Abstract: We study anomaly detection in images under a fixed-camera environment and propose a \emph{doubly smoothed} (DS) density estimator that exploits spatial structure to improve estimation accuracy. The DS estimator applies kernel smoothing twice: first over the value domain to obtain location-wise classical nonparametric density (CD) estimates, and then over the spatial domain to borrow information from neighboring locations. Under appropriate regularity conditions, we show that the DS estimator achieves smaller asymptotic bias, variance, and mean squared error than the CD estimator. To address the increased computational cost of the DS estimator, we introduce a grid point approximation (GPA) technique that reduces the computation cost of inference without sacrificing the estimation accuracy. A rule-of-thumb bandwidth is derived for practical use. Extensive simulations show that GPA-DS achieves the lowest MSE with near real-time speed. In a large-scale case study on underground mine surveillance, GPA-DS enables remarkable sub-image extraction of anomalous regions after which a lightweight MobileNet classifier achieves $\approx$99\% out-of-sample accuracy for unsafe act detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22482v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qianhan Zeng, Miao Han, Ke Xu, Feifei Wang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Regularization method in the variable selection for logistic regression on BRFSS data</title>
      <link>https://arxiv.org/abs/2510.22550</link>
      <description>arXiv:2510.22550v1 Announce Type: new 
Abstract: Stroke remains a leading cause of death and disability worldwide, yet effective prediction of stroke risk using large-scale population data remains challenging due to data imbalance and high-dimensional features. In this study, we develop and evaluate regularized logistic regression models for stroke prediction using data from the 2022 Behavioral Risk Factor Surveillance System (BRFSS), comprising 445132 U.S. adult respondents and 328 health-related variables. To address data imbalance, we apply several resampling techniques including oversampling, undersampling, class weighting, and the Synthetic Minority Oversampling Technique (SMOTE). We further employ Lasso, Elastic Net, and Group Lasso regularization methods to perform feature selection and dimensionality reduction. Model performance is assessed using ROC-AUC, sensitivity, and specificity metrics. Among all methods, the Lasso-based model achieved the highest predictive performance (AUC = 0.761), while the Group Lasso method identified a compact set of key predictors: Age, Heart Disease, Physical Health, and Dental Health. These findings demonstrate the potential of regularized regression techniques for interpretable and efficient prediction of stroke risk from large-scale behavioral health data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22550v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Niu</dc:creator>
    </item>
    <item>
      <title>Imaging Genetics Analysis of Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2510.22723</link>
      <description>arXiv:2510.22723v2 Announce Type: new 
Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline, structural brain changes, and genetic predispositions. This study leverages machine-learning and statistical techniques to investigate the mechanistic relationships between cognitive function, genetic markers, and neuroimaging biomarkers in AD progression. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we perform both low-dimensional and high-dimensional analyses to identify key predictors of disease states, including cognitively normal (CN), mild cognitive impairment (MCI), and AD. Our low-dimensional approach utilizes multiple linear and ordinal logistic regression to examine the influence of cognitive scores, cerebrospinal fluid (CSF) biomarkers, and demographic factors on disease classification. The results highlight significant associations between Mini-Mental State Examination (MMSE), Clinical Dementia Rating Sum of Boxes (CDRSB), and phosphorylated tau levels in predicting cognitive decline. The high-dimensional analysis employs Sure Independence Screening (SIS) and LASSO regression to reduce dimensionality and identify genetic markers correlated with cognitive impairment and white matter integrity. Genes such as CLIC1, NAB2, and TGFBR1 emerge as significant predictors across multiple analyses, linking genetic expression to neurodegeneration. Additionally, imaging genetic analysis reveals shared genetic influences across brain hemispheres and the corpus callosum, suggesting distinct genetic contributions to white matter degradation. These findings enhance our understanding of AD pathology by integrating cognitive, genetic, and imaging data. Future research should explore longitudinal analyses and potential gene-environment interactions to further elucidate the biological mechanisms underlying AD progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22723v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riddhik Basu, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>On the simultaneous inference of susceptibility distributions and intervention effects from epidemic curves</title>
      <link>https://arxiv.org/abs/2510.22791</link>
      <description>arXiv:2510.22791v1 Announce Type: new 
Abstract: Susceptible-Exposed-Infectious-Recovered (SEIR) models with inter-individual variation in susceptibility or exposure to infection were proposed early in the COVID-19 pandemic as a potential element of the mathematical/statistical toolset available to policy development. In comparison with other models employed at the time, those designed to fully estimate the effects of such variation tended to predict small epidemic waves and hence require less containment to achieve the same outcomes. However, these models never made it to mainstream COVID-19 policy making due to lack of prior validation of their inference capabilities. Here we report the results of the first systematic investigation of this matter. We simulate datasets using the model with strategically chosen parameter values, and then conduct maximum likelihood estimation to assess how well we can retrieve the assumed parameter values. We identify some identifiability issues which can be overcome by creatively fitting multiple epidemics with shared parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22791v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ibrahim Mohammed, Chris Robertson, M. Gabriela M. Gomes</dc:creator>
    </item>
    <item>
      <title>Mortality Models Ensemble via Shapley Value</title>
      <link>https://arxiv.org/abs/2510.23014</link>
      <description>arXiv:2510.23014v1 Announce Type: new 
Abstract: Model averaging techniques in the actuarial literature aim to forecast future longevity appropriately by combining forecasts derived from various models. This approach often yields more accurate predictions than those generated by a single model. The key to enhancing forecast accuracy through model averaging lies in identifying the optimal weights from a finite sample. Utilizing sub-optimal weights in computations may adversely impact the accuracy of the model-averaged longevity forecasts. By proposing a game-theoretic approach employing Shapley values for weight selection, our study clarifies the distinct impact of each model on the collective predictive outcome. This analysis not only delineates the importance of each model in decision-making processes, but also provides insight into their contribution to the overall predictive performance of the ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23014v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giovanna Bimonte, Maria Russolillo, Han Lin Shang, Yang Yang</dc:creator>
    </item>
    <item>
      <title>A Physics-Informed Variational Inference Framework for Identifying Attributions of Extreme Stress Events in Low-Grain Polycrystals</title>
      <link>https://arxiv.org/abs/2510.23437</link>
      <description>arXiv:2510.23437v1 Announce Type: new 
Abstract: Polycrystalline metal failure often begins with stress concentration at grain boundaries. Identifying which microstructural features trigger these events is important but challenging because these extreme damage events are rare and the failure mechanisms involve multiple complex processes across scales. Most existing inference methods focus on average behavior rather than rare events, whereas standard sample-based methods are computationally expensive for high-dimensional complex systems. In this paper, we develop a new variational inference framework that integrates a recently developed computationally efficient physics-informed statistical model with extreme value statistics to significantly facilitate the identification of material failure attributions. First, we reformulate the objective to emphasize observed exceedances by incorporating extreme-value theory into the likelihood, thereby highlighting tail behavior. Second, we constrain inference via a physics-informed statistical model that characterizes microstructure-stress relationships, which uniquely provides physically consistent predictions for these rare events. Third, mixture models in a reduced latent space are developed to capture the non-Gaussian characteristics of microstructural features, allowing the identification of multiple underlying mechanisms. In both controlled and realistic experimental tests for the bicrystal configuration, the framework achieves reliable extreme-event prediction and reveals the microstructural features associated with material failure, providing physical insights for material design with uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23437v1</guid>
      <category>stat.AP</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinling Zhang, Samuel D. Dunham, Curt A. Bronkhorst, Nan Chen</dc:creator>
    </item>
    <item>
      <title>Model Proficiency in Centralized Multi-Agent Systems: A Performance Study</title>
      <link>https://arxiv.org/abs/2510.23447</link>
      <description>arXiv:2510.23447v1 Announce Type: new 
Abstract: Autonomous agents are increasingly deployed in dynamic environments where their ability to perform a given task depends on both individual and team-level proficiency. While proficiency self-assessment (PSA) has been studied for single agents, its extension to a team of agents remains underexplored. This letter addresses this gap by presenting a framework for team PSA in centralized settings. We investigate three metrics for centralized team PSA: the measurement prediction bound (MPB), the Kolmogorov-Smirnov (KS) statistic, and the Kullback-Leibler (KL) divergence. These metrics quantify the discrepancy between predicted and actual measurements. We use the KL divergence as a reference metric since it compares the true and predictive distributions, whereas the MPB and KS provide efficient indicators for in situ assessment. Simulation results in a target tracking scenario demonstrate that both MPB and KS metrics accurately capture model mismatches, align with the KL divergence reference, and enable real-time proficiency assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23447v1</guid>
      <category>stat.AP</category>
      <category>cs.MA</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Guerra, Francesco Guidi, Pau Closas, Davide Dardari, Petar M. Djuric</dc:creator>
    </item>
    <item>
      <title>Beyond the Trade-off Curve: Multivariate and Advanced Risk-Utility Maps for Evaluating Anonymized and Synthetic Data</title>
      <link>https://arxiv.org/abs/2510.23500</link>
      <description>arXiv:2510.23500v1 Announce Type: new 
Abstract: Anonymizing microdata requires balancing the reduction of disclosure risk with the preservation of data utility. Traditional evaluations often rely on single measures or two-dimensional risk-utility (R-U) maps, but real-world assessments involve multiple, often correlated, indicators of both risk and utility. Pairwise comparisons of these measures can be inefficient and incomplete. We therefore systematically compare six visualization approaches for simultaneous evaluation of multiple risk and utility measures: heatmaps, dot plots, composite scatterplots, parallel coordinate plots, radial profile charts, and PCA-based biplots. We introduce blockwise PCA for composite scatterplots and joint PCA for biplots that simultaneously reveal method performance and measure interrelationships. Through systematic identification of Pareto-optimal methods in all approaches, we demonstrate how multivariate visualization supports a more informed selection of anonymization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23500v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Thees, Roman M\"uller, Matthias Templ</dc:creator>
    </item>
    <item>
      <title>Stochastic Boundaries in Spatial General Equilibrium: A Diffusion-Based Approach to Causal Inference with Spillover Effects</title>
      <link>https://arxiv.org/abs/2508.06594</link>
      <description>arXiv:2508.06594v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for causal inference in spatial economics that explicitly models the stochastic transition from partial to general equilibrium effects. We develop a Denoising Diffusion Probabilistic Model (DDPM) integrated with boundary detection methods from stochastic process theory to identify when and how treatment effects propagate beyond local markets. Our approach treats the evolution of spatial spillovers as a L\'evy process with jump-diffusion dynamics, where the first passage time to critical thresholds indicates regime shifts from partial to general equilibrium. Using CUSUM-based sequential detection, we identify the spatial and temporal boundaries at which local interventions become systemic. Applied to AI adoption across Japanese prefectures, we find that treatment effects exhibit L\'evy jumps at approximately 35km spatial scales, with general equilibrium effects amplifying partial equilibrium estimates by 42\%. Monte Carlo simulations show that ignoring these stochastic boundaries leads to underestimation of treatment effects by 28-67\%, with particular severity in densely connected economic regions. Our framework provides the first rigorous method for determining when spatial spillovers necessitate general equilibrium analysis, offering crucial guidance for policy evaluation in interconnected economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06594v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Spatial and Temporal Treatment Effect Boundaries: Theory and Identification</title>
      <link>https://arxiv.org/abs/2510.00754</link>
      <description>arXiv:2510.00754v2 Announce Type: cross 
Abstract: This paper develops a unified theoretical framework for detecting and estimating boundaries in treatment effects across both spatial and temporal dimensions. We formalize the concept of treatment effect boundaries as structural parameters characterizing regime transitions where causal effects cease to operate. Building on reaction-diffusion models of information propagation, we establish conditions under which spatial and temporal boundaries share common dynamics governed by diffusion parameters (delta, lambda), yielding the testable prediction d^*/tau^* = 3.32 lambda sqrt{delta} for standard detection thresholds. We derive formal identification results under staggered treatment adoption and develop a three-stage estimation procedure implementable with standard panel data. Monte Carlo simulations demonstrate excellent finite-sample performance, with boundary estimates achieving RMSE below 10% in realistic configurations. We apply the framework to two empirical settings: EU broadband diffusion (2006-2021) and US wildfire economic impacts (2017-2022). The broadband application reveals a scope limitation -- our framework assumes depreciation dynamics and fails when effects exhibit increasing returns through network externalities. The wildfire application provides strong validation: estimated boundaries satisfy d^* = 198 km and tau^* = 2.7 years, with the empirical ratio (72.5) exactly matching the theoretical prediction 3.32 lambda sqrt{delta} = 72.5. The framework provides practical tools for detecting when localized treatments become systemic and identifying critical thresholds for policy intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00754v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification and Estimation of Spatial Treatment Effect Boundaries: Evidence from 42 Million Pollution Observations</title>
      <link>https://arxiv.org/abs/2510.12289</link>
      <description>arXiv:2510.12289v2 Announce Type: cross 
Abstract: This paper develops a nonparametric framework for identifying and estimating spatial boundaries of treatment effects in settings with geographic spillovers. While atmospheric dispersion theory predicts exponential decay of pollution under idealized assumptions, these assumptions -- steady winds, homogeneous atmospheres, flat terrain -- are systematically violated in practice. I establish nonparametric identification of spatial boundaries under weak smoothness and monotonicity conditions, propose a kernel-based estimator with data-driven bandwidth selection, and derive asymptotic theory for inference. Using 42 million satellite observations of NO$_2$ concentrations near coal plants (2019-2021), I find that nonparametric kernel regression reduces prediction errors by 1.0 percentage point on average compared to parametric exponential decay assumptions, with largest improvements at policy-relevant distances: 2.8 percentage points at 10 km (near-source impacts) and 3.7 percentage points at 100 km (long-range transport). Parametric methods systematically underestimate near-source concentrations while overestimating long-range decay. The COVID-19 pandemic provides a natural experiment validating the framework's temporal sensitivity: NO$_2$ concentrations dropped 4.6\% in 2020, then recovered 5.7\% in 2021. These results demonstrate that flexible, data-driven spatial methods substantially outperform restrictive parametric assumptions in environmental policy applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12289v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification of Spatial Treatment Effect Boundaries: Evidence from Bank Branch Consolidation</title>
      <link>https://arxiv.org/abs/2510.13148</link>
      <description>arXiv:2510.13148v2 Announce Type: cross 
Abstract: I develop a nonparametric framework for identifying spatial boundaries of treatment effects without imposing parametric functional form restrictions. The method employs local linear regression with data-driven bandwidth selection to flexibly estimate spatial decay patterns and detect treatment effect boundaries. Monte Carlo simulations demonstrate that the nonparametric approach exhibits lower bias and correctly identifies the absence of boundaries when none exist, unlike parametric methods that may impose spurious spatial patterns. I apply this framework to bank branch openings during 2015--2020, matching 5,743 new branches to 5.9 million mortgage applications across 14,209 census tracts. The analysis reveals that branch proximity significantly affects loan application volume (8.5\% decline per 10 miles) but not approval rates, consistent with branches stimulating demand through local presence while credit decisions remain centralized. Examining branch survival during the digital transformation era (2010--2023), I find a non-monotonic relationship with area income: high-income areas experience more closures despite conventional wisdom. This counterintuitive pattern reflects strategic consolidation of redundant branches in over-banked wealthy urban areas rather than discrimination against poor neighborhoods. Controlling for branch density, urbanization, and competition, the direct income effect diminishes substantially, with branch density emerging as the primary determinant of survival. These findings demonstrate the necessity of flexible nonparametric methods for detecting complex spatial patterns that parametric models would miss, and challenge simplistic narratives about banking deserts by revealing the organizational complexity underlying spatial consolidation decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13148v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Quantifying the AI Gap: A Comparative Index of Development in the United States and Chinese Regions</title>
      <link>https://arxiv.org/abs/2510.21832</link>
      <description>arXiv:2510.21832v1 Announce Type: cross 
Abstract: This study develops a comprehensive Artificial Intelligence (AI) Index with seven primary dimensions, designed for provincial-level and industry-specific analysis. We employ an anchor point method for data normalization, using fixed upper and lower bounds as benchmarks, and devise a hierarchical indicator weighting system that combines expert judgment with objective data. The index draws from authoritative data sources across domains including official statistics, patents and research outputs, education and talent, industrial economy, policy and governance, and social impact. The China-US comparison indicates that under a unified framework, the US composite score (68.1) exceeds China's (59.4). We further dissect China into seven main areas to form a sub-national index. The findings reveal stark regional disparities in China's AI development: the North, East, and South regions lead in composite scores, whereas central and western regions lag significantly, underscoring the effects of regional concentration of innovation and industry resources. This research provides an academic reference and decision support tool for government agencies and research institutions, informing more targeted regional AI development strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21832v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanxi Li, Lei Yin</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approach to Capitation Reform in Rwanda</title>
      <link>https://arxiv.org/abs/2510.21851</link>
      <description>arXiv:2510.21851v1 Announce Type: cross 
Abstract: As part of Rwanda's transition toward universal health coverage, the national Community-Based Health Insurance (CBHI) scheme is moving from retrospective fee-for-service reimbursements to prospective capitation payments for public primary healthcare providers. This report outlines a data-driven approach to designing, calibrating, and monitoring the capitation model using individual-level claims data from the Intelligent Health Benefits System (IHBS). We introduce a transparent, interpretable formula for allocating payments to Health Centers and their affiliated Health Posts. The formula is based on catchment population, service utilization patterns, and patient inflows, with parameters estimated via regression models calibrated on national claims data. Repeated validation exercises show the payment scheme closely aligns with historical spending while promoting fairness and adaptability across diverse facilities. In addition to payment design, the same dataset enables actionable behavioral insights. We highlight the use case of monitoring antibiotic prescribing patterns, particularly in pediatric care, to flag potential overuse and guideline deviations. Together, these capabilities lay the groundwork for a learning health financing system: one that connects digital infrastructure, resource allocation, and service quality to support continuous improvement and evidence-informed policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21851v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babaniyi Olaniyi, Ina Kalisa, Ana Fern\'andez del R\'io, Jean Marie Vianney Hakizayezu, Enric Jan\'e, Eniola Olaleye, Juan Francisco Garamendi, Ivan Nazarov, Aditya Rastogi, Mateo Diaz-Quiroz, \'Africa Peri\'a\~nez, Regis Hitimana</dc:creator>
    </item>
    <item>
      <title>Testing for a common subspace in compositional datasets with structural zeros</title>
      <link>https://arxiv.org/abs/2510.22853</link>
      <description>arXiv:2510.22853v1 Announce Type: cross 
Abstract: In real world applications dealing with compositional datasets, it is easy to face the presence of structural zeros. The latter arise when, due to physical limitations, one or more variables are intrinsically zero for a subset of the population under study. The classical Aitchison approach requires all the components of a composition to be strictly positive, since the adaptation of the most widely used statistical techniques to the compositional framework relies on computing the logratios of these components. Therefore, datasets containing structural zeros are usually split in two subsets, the one containing the observations with structural zeros and the one containing all the other data. Then statistical analysis is performed on the two subsets separately, assuming the two datasets are drawn from two different subpopulations. However, this approach may lead to incomplete results when the split into two populations is merely artificial. To overcome this limitation and increase the robustness of such an approach, we introduce a statistical test to check whether the first K principal components of the two datasets generate the same vector space. An approximation of the corresponding null distribution is derived analytically when data are normally distributed on the simplex and through a nonparametric bootstrap approach in the other cases. Results from simulated data demonstrate that the proposed procedure can discriminate scenarios where the subpopulations share a common subspace from those where they are actually distinct. The performance of the proposed method is also tested on an experimental dataset concerning microbiome measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22853v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Porro, Fabio Rapallo, Sara Sommariva</dc:creator>
    </item>
    <item>
      <title>Cross-Lingual Sponsored Search via Dual-Encoder and Graph Neural Networks for Context-Aware Query Translation in Advertising Platforms</title>
      <link>https://arxiv.org/abs/2510.22957</link>
      <description>arXiv:2510.22957v1 Announce Type: cross 
Abstract: Cross-lingual sponsored search is crucial for global advertising platforms, where users from different language backgrounds interact with multilingual ads. Traditional machine translation methods often fail to capture query-specific contextual cues, leading to semantic ambiguities that negatively impact click-through rates (CTR) and conversion rates (CVR). To address this challenge, we propose AdGraphTrans, a novel dual-encoder framework enhanced with graph neural networks (GNNs) for context-aware query translation in advertising. Specifically, user queries and ad contents are independently encoded using multilingual Transformer-based encoders (mBERT/XLM-R), and contextual relations-such as co-clicked ads, user search sessions, and query-ad co-occurrence-are modeled as a heterogeneous graph. A graph attention network (GAT) is then applied to refine embeddings by leveraging semantic and behavioral context. These embeddings are aligned via contrastive learning to reduce translation ambiguity. Experiments conducted on a cross-lingual sponsored search dataset collected from Google Ads and Amazon Ads (EN-ZH, EN-ES, EN-FR pairs) demonstrate that AdGraphTrans significantly improves query translation quality, achieving a BLEU score of 38.9 and semantic similarity (cosine score) of 0.83, outperforming strong baselines such as mBERT and M2M-100. Moreover, in downstream ad retrieval tasks, AdGraphTrans yields +4.67% CTR and +1.72% CVR improvements over baseline methods. These results confirm that incorporating graph-based contextual signals with dual-encoder translation provides a robust solution for enhancing cross-lingual sponsored search in advertising platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22957v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Gao, Yuanliang Qu, Yi Han</dc:creator>
    </item>
    <item>
      <title>Set-valued data analysis for interlaboratory comparisons</title>
      <link>https://arxiv.org/abs/2510.23170</link>
      <description>arXiv:2510.23170v1 Announce Type: cross 
Abstract: This article introduces tools to analyze set-valued data statistically. The tools were initially developed to analyze results from an interlaboratory comparison made by the Electromagnetic Compatibility Working Group of Eurolab France, where the goal was to select a consensual set of injection points on an electrical device. Families based on the Hamming-distance from a consensus set are introduced and Fisher's noncentral hypergeometric distribution is proposed to model the number of deviations. A Bayesian approach is used and two types of techniques are proposed for the inference. Hierarchical models are also considered to quantify a possible within-laboratory effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23170v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Petit (LNE), S\'ebastien Marmin (LNE), Nicolas Fischer (LNE)</dc:creator>
    </item>
    <item>
      <title>Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks</title>
      <link>https://arxiv.org/abs/2510.23347</link>
      <description>arXiv:2510.23347v1 Announce Type: cross 
Abstract: Accurate macroeconomic forecasting has become harder amid geopolitical disruptions, policy reversals, and volatile financial markets. Conventional vector autoregressions (VARs) overfit in high dimensional settings, while threshold VARs struggle with time varying interdependencies and complex parameter structures. We address these limitations by extending the Sims Zha Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed shrinkage and four newspaper based uncertainty shocks such as economic policy uncertainty, geopolitical risk, US equity market volatility, and US monetary policy uncertainty. The framework improves structural interpretability, mitigates dimensionality, and imposes empirically guided regularization. Using G7 data, we study spillovers from uncertainty shocks to five core variables (unemployment, real broad effective exchange rates, short term rates, oil prices, and CPI inflation), combining wavelet coherence (time frequency dynamics) with nonlinear local projections (state dependent impulse responses). Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms 14 benchmarks, including classical VARs and leading machine learning models, as confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests, and Giacomini White predictability tests. Credible Bayesian prediction intervals deliver robust uncertainty quantification for scenario analysis and risk management. The proposed SZBVARx offers G7 policymakers a transparent, well calibrated tool for modern macroeconomic forecasting under pervasive uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23347v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shovon Sengupta, Sunny Kumar Singh, Tanujit Chakraborty</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with Application to the Richards Equation</title>
      <link>https://arxiv.org/abs/2510.23550</link>
      <description>arXiv:2510.23550v1 Announce Type: cross 
Abstract: The estimation of unknown parameters in nonlinear partial differential equations (PDEs) offers valuable insights across a wide range of scientific domains. In this work, we focus on estimating plant root parameters in the Richards equation, which is essential for understanding the soil-plant system in agricultural studies. Since conventional methods are computationally intensive and often yield unstable estimates, we develop a new Gaussian process collocation method for efficient Bayesian inference. Unlike existing Gaussian process-based approaches, our method constructs an approximate posterior distribution using samples drawn from a Gaussian process model fitted to the observed data, which does not require any structural assumption about the underlying PDE. Further, we propose to use an importance sampling procedure to correct for the discrepancy between the approximate and true posterior distributions. As an alternative, we also devise a prior-guided Bayesian optimization algorithm leveraging the approximate posterior. Simulation studies demonstrate that our method yields robust estimates under various settings. Finally, we apply our method on a real agricultural data set and estimate the plant root parameters with uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23550v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumo Yang, Anass Ben Bouazza, Xuejun Dong, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>A time warping model for seasonal data with application to age estimation from narwhal tusks</title>
      <link>https://arxiv.org/abs/2410.05843</link>
      <description>arXiv:2410.05843v3 Announce Type: replace 
Abstract: Signals with varying periodicity frequently appear in real-world phenomena, necessitating the development of efficient modelling techniques to map the measured nonlinear timeline to linear time. Here we propose a regression model that allows for a representation of periodic and dynamic patterns observed in time series data. The model incorporates a hidden strictly positive stochastic process that represents the instantaneous frequency, allowing the model to adapt and accurately capture varying time scales. A case study focusing on age estimation of narwhal tusks is presented, where cyclic element signals associated with annual growth layer groups are analyzed. We apply the methodology to data from one such tusk collected in West Greenland and use the fitted model to estimate the age of the narwhal. The proposed method is validated using simulated signals with known cycle counts and practical considerations and modelling challenges are discussed in detail. This research contributes to the field of time series analysis, providing a tool and valuable insights for understanding and modeling complex cyclic patterns in diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05843v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars N{\o}rtoft Reiter, Adam Gorm Hoffmann, Mads Peter Heide-J{\o}rgensen, Eva Garde, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Evaluating Cooling Center Coverage Using Persistent Homology of a Filtered Witness Complex</title>
      <link>https://arxiv.org/abs/2410.09067</link>
      <description>arXiv:2410.09067v2 Announce Type: replace 
Abstract: In light of the increase in frequency of extreme heat events, there is a critical need to develop tools to identify geographic locations that are at risk of heat-related mortality. This paper aims to identify locations by assessing holes in cooling-center coverage using persistent homology (PH), a method from topological data analysis (TDA). Persistent homology has shown promising results in identifying holes in coverage of specific resources. We adapt these methods using a witness complex construction to study the coverage of cooling centers. We test our approach on four locations (central Boston, MA; central Austin, TX; Portland, OR; and Miami, FL) and use death times, a measurement of the size and scale of the gap in coverage, to identify most at risk regions. For comparison, we implement a standard technique for studying the risk of heat-related mortality called a heat vulnerability index (HVI). The HVI is a numerical score calculated for a geographic area based on demographic information. PH and the HVI identify different locations as vulnerable, thus indicating a potential value of assessing vulnerability from multiple perspectives. By using the regions identified by both persistent homology and the HVI, we provide a more holistic understanding of coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09067v2</guid>
      <category>stat.AP</category>
      <category>cs.CG</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin O'Neil, Sarah Tymochko</dc:creator>
    </item>
    <item>
      <title>Scaling Methods To Estimate Macroscopic Fundamental Diagrams in Urban Networks with Sparse Sensor Coverage</title>
      <link>https://arxiv.org/abs/2411.19721</link>
      <description>arXiv:2411.19721v3 Announce Type: replace 
Abstract: Accurately estimating traffic variables across unequipped portions of a network remains a significant challenge due to the limited coverage of sensor-equipped links, such as loop detectors and probe vehicles. A common approach is to apply uniform scaling, treating unequipped links as equivalent to equipped ones. This study introduces a novel framework to improve traffic variable estimation by integrating statistical scaling methods with geospatial imputation techniques. Two main approaches are proposed: (1) Statistical Scaling, which includes hierarchical and non-hierarchical network approaches, and (2) Geospatial Imputation, based on variogram modeling. The hierarchical scaling method categorizes the network into several levels according to spatial and functional characteristics, applying tailored scaling factors to each category. In contrast, the non-hierarchical method uses a uniform scaling factor across all links, ignoring network heterogeneity. The variogram-based geospatial imputation leverages spatial correlations to estimate traffic variables for unequipped links, capturing spatial dependencies in urban road networks. Validation results indicate that the hierarchical scaling approach provides the most accurate estimates, achieving reliable performance even with as low as 5% uniform detector coverage. Although the variogram-based method yields strong results, it is slightly less effective than the hierarchical scaling approach but outperforms the non-hierarchical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19721v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trc.2025.105213</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part C: Emerging Technologies, Volume 178, September 2025, 105213</arxiv:journal_reference>
      <dc:creator>Nandan Maiti, Manon Seppecher, Ludovic Leclercq</dc:creator>
    </item>
    <item>
      <title>Multivariate Functional Linear Discriminant Analysis: An Application to Inflammatory Bowel Disease Classification</title>
      <link>https://arxiv.org/abs/2503.13372</link>
      <description>arXiv:2503.13372v2 Announce Type: replace 
Abstract: Inflammatory Bowel Disease (IBD), including Crohn's Disease (CD) and Ulcerative Colitis (UC), presents significant public health challenges due to its complex etiology. Motivated by the IBD study of the Integrative Human Microbiome Project, our objective is to identify microbial pathways that distinguish between CD, UC and non-IBD over time. Most current research relies on simplistic analyses that examine one variable or time point at a time, or address binary classification problems, limiting our understanding of the dynamic interactions within the microbiome over time. To address these limitations, we develop a novel functional data analysis approach for discriminant analysis of multivariate functional data that can effectively handle multiple high-dimensional predictors, sparse time points, and categorical outcomes. Our method seeks linear combinations of functions (i.e., discriminant functions) that maximize separation between two or more groups over time. We impose a sparsity-inducing penalty when estimating the discriminant functions, allowing us to identify relevant discriminating variables over time. Applications of our method to the motivating data identified microbial features related to mucin degradation, amino acid metabolism, and peptidoglycan recognition, which are implicated in the progression and development of IBD. Furthermore, our method highlighted the role of multiple vitamin B deficiencies in the context of IBD. By moving beyond traditional analytical frameworks, our innovative approach holds the potential for uncovering clinically meaningful discoveries in IBD research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13372v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Limeng Liu, Guannan Wang, Sandra E. Safo</dc:creator>
    </item>
    <item>
      <title>BlockingPy: approximate nearest neighbours for blocking of records for entity resolution</title>
      <link>https://arxiv.org/abs/2504.04266</link>
      <description>arXiv:2504.04266v3 Announce Type: replace 
Abstract: Entity resolution (probabilistic record linkage, deduplication) is a key step in scientific analysis and data science pipelines involving multiple data sources. The objective of entity resolution is to link records without common unique identifiers that refer to the same entity (e.g., person, company). However, without identifiers, researchers need to specify which records to compare in order to calculate matching probability and reduce computational complexity. One solution is to deterministically block records based on some common variables, such as names, dates of birth or sex or use phonetic algorithms. However, this approach assumes that these variables are free of errors and completely observed, which is often not the case. To address this challenge, we have developed a Python package, BlockingPy, which uses blocking using modern approximate nearest neighbour search and graph algorithms to reduce the number of comparisons. The package supports both CPU and GPU execution. In this paper, we present the design of the package, its functionalities and two case studies related to official statistics. The presented software will be useful for researchers interested in linking data from various sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04266v3</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tymoteusz Strojny, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Method: Using generalized additive models in the livestock animal sciences</title>
      <link>https://arxiv.org/abs/2507.06281</link>
      <description>arXiv:2507.06281v2 Announce Type: replace 
Abstract: Nonlinear relationships between covariates and a response variable of interest are frequently encountered in animal science research. Within statistical models, these nonlinear effects have, traditionally, been handled using a range of approaches including transformation of the response, parametric nonlinear models based on theory or phenomenological grounds, or through fixed degree spline or polynomial terms. If it is desirable to learn the shape of these relationships then generalized additive models (GAMs) are an excellent alternative. GAMs extend the generalized linear model such that the linear predictor includes one or more smooth functions, parameterised using penalised splines. A wiggliness penalty on each function is used to avoid over fitting while estimating the parameters of the spline basis functions to maximise fit to the data. Modern GAMs include automatic smoothness selection methods to find an optimal balance between fit and complexity of the estimated functions. Because GAMs learn the shapes of functions from the data, the user can avoid forcing a particular model to their data. Here, I provide a brief description of GAMs and visually illustrate how they work. I then demonstrate the utility of GAMs on three example data sets of increasing complexity, to show i) how learning from data can produce a better fit to data than that of parametric models, ii) how hierarchical GAMs can be used to estimate growth data from multiple animals in a single model, and iii) how hierarchical GAMs can be used for formal statistical inference in a designed experiment. The examples are supported by R code that demonstrates how to fit each of the models considered, and reproduces the results of the statistical analyses reported here. Ultimately, I show that GAMs are a modern, flexible, and highly usable statistical model that is amenable to many research problems in animal science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06281v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin L. Simpson</dc:creator>
    </item>
    <item>
      <title>Machine learning augmented diagnostic testing to identify sources of variability in test performance</title>
      <link>https://arxiv.org/abs/2404.03678</link>
      <description>arXiv:2404.03678v2 Announce Type: replace-cross 
Abstract: Diagnostic tests that can detect pre-clinical or sub-clinical infection, are one of the most powerful tools in our armoury of weapons to control infectious diseases. Considerable effort has been paid to improving diagnostic testing for human, plant and animal diseases, including strategies for targeting the use of diagnostic tests towards individuals who are more likely to be infected. We use machine learning to assess the surrounding risk landscape under which a diagnostic test is applied to augment its interpretation. We develop this to predict the occurrence of bovine tuberculosis incidents in cattle herds, exploiting the availability of exceptionally detailed testing records. We show that, without compromising test specificity, test sensitivity can be improved so that the proportion of infected herds detected improves by over 5 percentage points, or 240 additional infected herds detected in one year beyond those detected by the skin test alone. We also use feature importance testing for assessing the weighting of risk factors. While many factors are associated with increased risk of incidents, of note are several factors that suggest that in some herds there is a higher risk of infection going undetected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03678v2</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Banks, Aeron Sanchez, Vicki Stewart, Kate Bowen, Thomas Doherty, Oliver Tearne, Graham Smith, Rowland R. Kao</dc:creator>
    </item>
    <item>
      <title>Measuring Research Interest Similarity with Transition Probabilities</title>
      <link>https://arxiv.org/abs/2409.18240</link>
      <description>arXiv:2409.18240v2 Announce Type: replace-cross 
Abstract: We introduce a family of paper and author similarity measures based on the concept that papers are more similar if they are more likely to be retrieved during a literature search following backward and forward citations. Since this browsing process resembles a walk in a citation network, we operationalize the concept using the transition probability (TP) of random walkers. The proposed measures are continuous, symmetric, and can be implemented on any citation network. We conduct validation tests of the TP concept and other extant alternatives to gauge which metric can classify papers and predict future co-authors most consistently across different scales of analysis (co-authorships, journals, and disciplines). Our results show that the proposed basic TP measure outperforms alternative metrics such as personalized PageRank and the Node2vec machine-learning technique in classification tasks at various scales. Additionally, we discuss how publication-level data can be leveraged to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package that implements all the tested metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18240v2</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/qss.a.13</arxiv:DOI>
      <arxiv:journal_reference>Quantitative Science Studies, 6, 922-939 (2025)</arxiv:journal_reference>
      <dc:creator>Attila Varga, Sadamori Kojaku, Filipi Nascimento Silva</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Hierarchical Data</title>
      <link>https://arxiv.org/abs/2411.13479</link>
      <description>arXiv:2411.13479v3 Announce Type: replace-cross 
Abstract: We consider conformal prediction for multivariate data and focus on hierarchical data, where some components are linear combinations of others. Intuitively, the hierarchical structure can be leveraged to reduce the size of prediction regions for the same coverage level. We implement this intuition by including a projection step (also called a reconciliation step) in the split conformal prediction [SCP] procedure, and prove that the resulting prediction regions are indeed globally smaller. We do so both under the classic objective of joint coverage and under a new and challenging task: component-wise coverage, for which efficiency results are more difficult to obtain. The associated strategies and their analyses are based both on the literature of SCP and of forecast reconciliation, which we connect. We also illustrate the theoretical findings, for different scales of hierarchies on simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13479v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Principato, Gilles Stoltz, Yvenn Amara-Ouali, Yannig Goude, Bachir Hamrouche, Jean-Michel Poggi</dc:creator>
    </item>
    <item>
      <title>Prognostic Framework for Robotic Manipulators Operating Under Dynamic Task Severities</title>
      <link>https://arxiv.org/abs/2412.00538</link>
      <description>arXiv:2412.00538v3 Announce Type: replace-cross 
Abstract: Robotic manipulators are critical in many applications but are known to degrade over time. This degradation is influenced by the nature of the tasks performed by the robot. Tasks with higher severity, such as handling heavy payloads, can accelerate the degradation process. One way this degradation is reflected is in the position accuracy of the robot's end-effector. In this paper, we present a prognostic modeling framework that predicts a robotic manipulator's Remaining Useful Life (RUL) while accounting for the effects of task severity. Our framework represents the robot's position accuracy as a Brownian motion process with a random drift parameter that is influenced by task severity. The dynamic nature of task severity is modeled using a continuous-time Markov chain (CTMC). To evaluate RUL, we discuss two approaches -- (1) a novel closed-form expression for Remaining Lifetime Distribution (RLD), and (2) Monte Carlo simulations, commonly used in prognostics literature. Theoretical results establish the equivalence between these RUL computation approaches. We validate our framework through experiments using two distinct physics-based simulators for planar and spatial robot fleets. Our findings show that robots in both fleets experience shorter RUL when handling a higher proportion of high-severity tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00538v3</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Mohanty, Jason Dekarske, Stephen K. Robinson, Sanjay Joshi, Nagi Gebraeel</dc:creator>
    </item>
    <item>
      <title>Stronger together? The homophily trap in networks</title>
      <link>https://arxiv.org/abs/2412.20158</link>
      <description>arXiv:2412.20158v2 Announce Type: replace-cross 
Abstract: While homophily -- the tendency to link with similar others -- may nurture a sense of belonging and shared values, it can also hinder diversity and widen inequalities. Here, we unravel this trade-off analytically, revealing homophily traps for minority groups: scenarios where increased homophilic interaction among minorities negatively affects their structural opportunities within a network. We demonstrate that homophily traps arise when minority size falls below 25% of a network, at which point homophily comes at the expense of lower structural visibility for the minority group. Our work reveals that social groups require a critical size to benefit from homophily without incurring structural costs, providing insights into core processes underlying the emergence of group inequality in networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20158v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcos Oliveira, Leonie Neuhauser, Fariba Karimi</dc:creator>
    </item>
    <item>
      <title>Larger cities, more commuters, more crime? The role of inter-city commuting in the scaling of urban crime</title>
      <link>https://arxiv.org/abs/2505.20822</link>
      <description>arXiv:2505.20822v2 Announce Type: replace-cross 
Abstract: Cities attract a daily influx of non-resident commuters, reflecting their roles within wider urban networks -- not as isolated places. However, it remains unclear how this interconnectivity shapes the way crime scales with population, given that larger cities tend to receive more commuters and experience more crime. In this work, we investigate how inter-city commuting relates to the population-crime relationship. We find that larger cities receive proportionately more commuters, which in turn is associated with higher levels of burglary, drug possession, robbery, shoplifting, and theft. For example, each 1% increase in inbound commuters corresponds to a 0.32% rise in theft and 0.20% rise in burglary, holding population size constant. We demonstrate that models incorporating both population size and commuter inflows explain variation in these offenses better than population-only models. Our findings underscore the importance of considering how cities are connected -- not just their population size -- in disentangling the population-crime relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20822v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Puttock, Umberto Barros, Diego Pinheiro, Marcos Oliveira</dc:creator>
    </item>
    <item>
      <title>Curious Causality-Seeking Agents Learn Meta Causal World</title>
      <link>https://arxiv.org/abs/2506.23068</link>
      <description>arXiv:2506.23068v3 Announce Type: replace-cross 
Abstract: When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23068v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyu Zhao, Haoxuan Li, Haifeng Zhang, Jun Wang, Francesco Faccio, J\"urgen Schmidhuber, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>Continental-scale habitat distribution modelling with multimodal earth observation foundation models</title>
      <link>https://arxiv.org/abs/2507.09732</link>
      <description>arXiv:2507.09732v2 Announce Type: replace-cross 
Abstract: Habitats integrate the abiotic conditions, vegetation composition and structure that support biodiversity and sustain nature's contributions to people. Most habitats face mounting pressures from human activities, which requires accurate, high-resolution habitat mapping for effective conservation and restoration. Yet, current habitat maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicates exhaustive multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat mapping across large geographical extents at fine spatial and thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat classifications resolved classification ambiguities, especially in fragmented habitats. Integrating satellite-borne multispectral and radar imagery, particularly through Earth Observation (EO) Foundation models (EO-FMs), enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted predictive accuracy even further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of habitat dynamics, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in situ observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09732v2</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Si-Moussi, Stephan Hennekens, Sander Mucher, Stan Los, Yoann Cartier, Borja Jim\'enez-Alfaro, Fabio Attorre, Jens-Christian Svenning, Wilfried Thuiller</dc:creator>
    </item>
    <item>
      <title>Approaches for modelling the term-structure of default risk under IFRS 9: A tutorial using discrete-time survival analysis</title>
      <link>https://arxiv.org/abs/2507.15441</link>
      <description>arXiv:2507.15441v2 Announce Type: replace-cross 
Abstract: Under the International Financial Reporting Standards (IFRS) 9, credit losses ought to be recognised timeously and accurately. This requirement belies a certain degree of dynamicity when estimating the constituent parts of a credit loss event, most notably the probability of default (PD). It is notoriously difficult to produce such PD-estimates at every point of loan life that are adequately dynamic and accurate, especially when considering the ever-changing macroeconomic background. In rendering these lifetime PD-estimates, the choice of modelling technique plays an important role, which is why we first review a few classes of techniques, including the merits and limitations of each. Our main contribution however is the development of an in-depth and data-driven tutorial using a particular class of techniques called discrete-time survival analysis. This tutorial is accompanied by a diverse set of reusable diagnostic measures for evaluating various aspects of a survival model and the underlying data. A comprehensive R-based codebase is further contributed. We believe that our work can help cultivate common modelling practices under IFRS 9, and should be valuable to practitioners, model validators, and regulators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15441v2</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster</dc:creator>
    </item>
    <item>
      <title>Regression approaches for modelling genotype-environment interaction and making predictions into unseen environments</title>
      <link>https://arxiv.org/abs/2507.18125</link>
      <description>arXiv:2507.18125v2 Announce Type: replace-cross 
Abstract: In plant breeding and variety testing, there is an increasing interest in making use of environmental information to enhance predictions for new environments. Here, we will review linear mixed models that have been proposed for this purpose. The emphasis will be on predictions and on methods to assess the uncertainty of predictions for new environments. Our point of departure is straight-line regression, which may be extended to multiple environmental covariates and genotype-specific responses. When observable environmental covariates are used, this is also known as factorial regression. Early work along these lines can be traced back to Stringfield &amp; Salter (1934) and Yates &amp; Cochran (1938), who proposed a method nowadays best known as Finlay-Wilkinson regression. This method, in turn, has close ties with regression on latent environmental covariates and factor-analytic variance-covariance structures for genotype-environment interaction. Extensions of these approaches - reduced rank regression, kernel- or kinship-based approaches, random coefficient regression, and extended Finlay-Wilkinson regression - will be the focus of this paper. Our objective is to demonstrate how seemingly disparate methods are very closely linked and fall within a common model-based prediction framework. The framework considers environments as random throughout, with genotypes also modelled as random in most cases. We will discuss options for assessing uncertainty of predictions, including cross validation and model-based estimates of uncertainty, the latter one being estimated using our new suggested approach. The methods are illustrated using a long-term rice variety trial dataset from Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18125v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Hrachov, Hans-Peter Piepho, Niaz Md. Farhat Rahman, Waqas Ahmed Malik</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v5 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Beyond the Average: Distributional Causal Inference under Imperfect Compliance</title>
      <link>https://arxiv.org/abs/2509.15594</link>
      <description>arXiv:2509.15594v2 Announce Type: replace-cross 
Abstract: We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect-the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15594v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui</dc:creator>
    </item>
    <item>
      <title>Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence</title>
      <link>https://arxiv.org/abs/2509.16599</link>
      <description>arXiv:2509.16599v3 Announce Type: replace-cross 
Abstract: Background: Evidence synthesis facilitates evidence-based medicine. This task becomes increasingly difficult to accomplished with applying computational solutions, since the medical literature grows at astonishing rates. Objective: This study evaluates an information retrieval-driven workflow, CASMA, to enhance the efficiency, transparency, and reproducibility of systematic reviews. Endometriosis recurrence serves as the ideal case due to its complex and ambiguous literature. Methods: The hybrid approach integrates PRISMA guidelines with fuzzy matching and regular expression (regex) to facilitate semi-automated deduplication and filtered records before manual screening. The workflow synthesised evidence from randomised controlled trials on the efficacy of a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified splitting method addressed unit-of-analysis errors in multi-arm trials. Results: The workflow sharply reduced the screening workload, taking only 11 days to fetch and filter 33,444 records. Seven eligible RCTs were synthesized (841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of $0.64$ ($95\%$ CI $0.48$ to $0.86$), demonstrating a $36\%$ reduction in recurrence, with non-significant heterogeneity ($I^2=0.00\%$, $\tau^2=0.00$). The findings were robust and stable, as they were backed by sensitivity analyses. Conclusion: This study demonstrates an application of an information-retrieval-driven workflow for medical evidence synthesis. The approach yields valuable clinical results and a generalisable framework to scale up the evidence synthesis, bridging the gap between clinical research and computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16599v3</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandro Tsang</dc:creator>
    </item>
    <item>
      <title>Generalized Jeffreys's approximate objective Bayes factor: Model-selection consistency, finite-sample accuracy, and statistical evidence in 71,126 clinical trial findings</title>
      <link>https://arxiv.org/abs/2510.10358</link>
      <description>arXiv:2510.10358v3 Announce Type: replace-cross 
Abstract: Concerns about the misuse and misinterpretation of p-values and statistical significance have motivated alternatives for quantifying evidence. We define a generalized form of Jeffreys's approximate objective Bayes factor (eJAB), a one-line calculation that is a function of the p-value, sample size, and parameter dimension. We establish conditions under which eJAB is model-selection consistent and verify them for ten statistical tests. We assess finite-sample accuracy by comparing eJAB with Markov chain Monte Carlo computed Bayes factors in 12 simulation studies. We then apply eJAB to 71,126 results from ClinicalTrials.gov (CTG) and find that the proportion of findings with $\text{p-value} \le \alpha$ yet $eJAB_{01}&gt;1$ (favoring the null) closely tracks the significance level $\alpha$, suggesting that such contradictions are pointing to the type I errors. We catalog 4,088 such candidate type I errors and provide details for 131 with reported $\text{p-value} \le 0.01$. We also identify 487 instances of the Jeffreys-Lindley paradox. Finally, we estimate that 75% (6%) of clinical trial plans from CTG set $\alpha \ge 0.05$ as the target evidence threshold, and that 35.5% (0.22%) of results significant at $\alpha =0.05$ correspond to evidence that is no stronger than anecdotal under eJAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10358v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Velidi, Zhengxiao Wei, Shreena Nisha Kalaria, Yimeng Liu, C\'eline M. Laumont, Brad H. Nelson, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Are penalty shootouts better than a coin toss? Evidence from European football</title>
      <link>https://arxiv.org/abs/2510.17641</link>
      <description>arXiv:2510.17641v2 Announce Type: replace-cross 
Abstract: Penalty shootouts play an important role in the knockout stage of major football tournaments, especially since the 2021/22 season, when the Union of European Football Associations (UEFA) scrapped the away goals rule in its club competitions. Inspired by this rule change, our paper examines whether the outcome of a penalty shootout can be predicted in UEFA club competitions. Based on all shootouts between 2000 and 2025, we find no evidence for the effect of the kicking order, the field of the match, or psychological momentum. In contrast to previous results, stronger teams, defined first by Elo ratings, do not perform better than their weaker opponents. Consequently, penalty shootouts are equivalent to a perfect lottery in top European football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17641v2</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
  </channel>
</rss>
