<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A non-parametric optimal design algorithm for population pharmacokinetics</title>
      <link>https://arxiv.org/abs/2502.15848</link>
      <description>arXiv:2502.15848v1 Announce Type: new 
Abstract: This paper introduces a non-parametric estimation algorithm designed to effectively estimate the joint distribution of model parameters with application to population pharmacokinetics. Our research group has previously developed the non-parametric adaptive grid (NPAG) algorithm, which while accurate, explores parameter space using an ad-hoc method to suggest new support points. In contrast, the non-parametric optimal design (NPOD) algorithm uses a gradient approach to suggest new support points, which reduces the amount of time spent evaluating non-relevant points and by this the overall number of cycles required to reach convergence. In this paper, we demonstrate that the NPOD algorithm achieves similar solutions to NPAG across two datasets, while being significantly more efficient in both the number of cycles required and overall runtime. Given the importance of developing robust and efficient algorithms for determining drug doses quickly in pharmacokinetics, the NPOD algorithm represents a valuable advancement in non-parametric modeling. Further analysis is needed to determine which algorithm performs better under specific conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15848v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Hovd, Alona Kryshchenko, Michael N. Neely, Julian Otalvaro, Alan Schumitzky, Walter M. Yamada</dc:creator>
    </item>
    <item>
      <title>Improving ex ante accuracy assessment in predicting house price dispersion: evidence from the USA</title>
      <link>https://arxiv.org/abs/2502.15905</link>
      <description>arXiv:2502.15905v1 Announce Type: new 
Abstract: The study focuses on improving the ex ante prediction accuracy assessment in the case of forecasting various house price dispersion measures in the USA. It addresses a critical gap in real estate market forecasting by proposing a novel method for assessing ex ante prediction accuracy under unanticipated shocks. The proposal is based on a parametric bootstrap approach under a misspecified model, allowing for the simulation of future values and estimation of prediction errors in case of unexpected price changes. The study highlights the limitations of the traditional approach that fails to account for unforeseen market events and provides a more in-depth understanding of how prediction accuracy changes under unexpected scenarios. The proposed methods offers valuable insights for real estate market management by enabling more robust risk assessment and decision-making in the face of unexpected market fluctuations. Real data application is based on longitudinal U.S. data on real estate transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15905v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Chwila, Monika Hada\'s-Dyduch, Ma{\l}gorzata Krzciuk, Tomasz Stachurski, Alicja Wolny-Dominiak, Tomasz \.Z\k{a}d{\l}o</dc:creator>
    </item>
    <item>
      <title>Empirical Best Prediction of Poverty Indicators via Nested Error Regression with High Dimensional Parameters</title>
      <link>https://arxiv.org/abs/2502.15933</link>
      <description>arXiv:2502.15933v1 Announce Type: new 
Abstract: This paper extends the Nested Error Regression Model with High-Dimensional Parameters (NERHDP) to address challenges in small area poverty estimation. Building on the NERHDP framework, we develop a robust and flexible approach to derive empirical best predictors (EBPs) of small area poverty indicators, while accommodating heterogeneity in regression coefficients and sampling variances across areas. To overcome computational limitations in the existing algorithm, we introduce an efficient method that significantly reduces computation time, enhancing scalability for large datasets. Additionally, we propose a novel method for generating area-specific poverty estimates for out-of-sample areas, improving the reliability of synthetic estimates. To quantify uncertainty, we introduce a parametric bootstrap method tailored to the extended model. Through design-based simulation studies, we demonstrate that the proposed method has better performance in terms of relative bias and relative root mean squared prediction error compared to existing approaches. Furthermore, the proposed method is applied to household survey data from the 2002 Albania Living Standards Measurement Survey to estimate poverty indicators for 374 municipalities using auxiliary information from the 2001 census.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15933v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Partha Lahiri, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>Gaussianized Design Optimization for Covariate Balance in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2502.16042</link>
      <description>arXiv:2502.16042v1 Announce Type: new 
Abstract: Achieving covariate balance in randomized experiments enhances the precision of treatment effect estimation. However, existing methods often require heuristic adjustments based on domain knowledge and are primarily developed for binary treatments. This paper presents Gaussianized Design Optimization, a novel framework for optimally balancing covariates in experimental design. The core idea is to Gaussianize the treatment assignments: we model treatments as transformations of random variables drawn from a multivariate Gaussian distribution, converting the design problem into a nonlinear continuous optimization over Gaussian covariance matrices. Compared to existing methods, our approach offers significant flexibility in optimizing covariate balance across a diverse range of designs and covariate types. Adapting the Burer-Monteiro approach for solving semidefinite programs, we introduce first-order local algorithms for optimizing covariate balance, improving upon several widely used designs. Furthermore, we develop inferential procedures for constructing design-based confidence intervals under Gaussianization and extend the framework to accommodate continuous treatments. Simulations demonstrate the effectiveness of Gaussianization in multiple practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16042v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxuan Guo, Tengyuan Liang, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Local False Sign Rate and the Role of Prior Covariance Rank in Multivariate Empirical Bayes Multiple Testing</title>
      <link>https://arxiv.org/abs/2502.16118</link>
      <description>arXiv:2502.16118v1 Announce Type: new 
Abstract: We study the relationship between the rank of the prior covariance matrix and the local false sign rate in a multivariate empirical Bayes normal mean model. It has been observed that the false sign rate is inflated when the prior assigns weight to low-rank covariance matrices. We show that this issue arises due to the rank deficiency of prior covariance matrices and propose an adjustment to mitigate it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16118v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dongyue Xie</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric inference on a Fr\'echet class</title>
      <link>https://arxiv.org/abs/2502.16192</link>
      <description>arXiv:2502.16192v1 Announce Type: new 
Abstract: Let $(\mathcal{X},\mathcal{F},\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be probability spaces and $(Z_n)$ a sequence of random variables with values in $(\mathcal{X}\times\mathcal{Y},\,\mathcal{F}\otimes\mathcal{G})$. Let $\Gamma(\mu,\nu)$ be the collection of all probability measures $p$ on $\mathcal{F}\otimes\mathcal{G}$ such that $$p\bigl(A\times\mathcal{Y}\bigr)=\mu(A)\quad\text{and}\quad p\bigl(\mathcal{X}\times B\bigr)=\nu(B)\quad\text{for all }A\in\mathcal{F}\text{ and }B\in\mathcal{G}.$$ In this paper, we build some probability measures $\Pi$ on $\Gamma(\mu,\nu)$. In addition, for each such $\Pi$, we assume that $(Z_n)$ is exchangeable with de Finetti's measure $\Pi$ and we evaluate the conditional distribution $\Pi(\cdot\mid Z_1,\ldots,Z_n)$. In Bayesian nonparametrics, if $(Z_1,\ldots, Z_n)$ are the available data, $\Pi$ and $\Pi(\cdot\mid Z_1,\ldots, Z_n)$ can be regarded as the prior and the posterior, respectively. To support this interpretation, it suffices to think of a problem where the unknown probability distribution of some bivariate phenomenon is constrained to have marginals $\mu$ and $\nu$. Finally, analogous results are obtained for the set $\Gamma(\mu)$ of those probability measures on $\mathcal{F}\otimes\mathcal{G}$ with marginal $\mu$ on $\mathcal{F}$ (but arbitrary marginal on $\mathcal{G}$). That is, we introduce some priors on $\Gamma(\mu)$ and we evaluate the corresponding posteriors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16192v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emanuela Dreassi, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>Meta-analysis in dental research</title>
      <link>https://arxiv.org/abs/2502.16250</link>
      <description>arXiv:2502.16250v1 Announce Type: new 
Abstract: Recently, importance of meta-analysis is increasing in the field of dentistry, since it is not easy to settle controversies arising from conflicting studies. Meta-analysis is the statistical method of combining results from two or more individual studies that have been done on the same topic. Merits of meta-analysis includes an increase in power, an improvement in precision, and the ability to address solution not provided by individual studies. However, it might mislead researchers when variation across studies and publication bias are not carefully taken into consideration. The purpose of this study is to help understand meta-analysis by making use of individual results in dental research paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16250v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoi-Jeong Lim</dc:creator>
    </item>
    <item>
      <title>A step-by-step guide to generalized estimating equations using SPSS in the field of dentistry</title>
      <link>https://arxiv.org/abs/2502.16261</link>
      <description>arXiv:2502.16261v1 Announce Type: new 
Abstract: The Generalized Estimating Equations (GEE) approach is a widely used statistical method for analyzing longitudinal data and clustered data in clinic studies. In dentistry, due to multiple outcomes obtained from one patient, the outcomes produced from individual patients are correlated with one another. This study focuses on the basic ideas of GEE and introduces the types of covariance matrix and working correlation matrix. The quasi-likelihood information criterion(QIC) and quasi-likelihood information criterion approximation(QICu) were used to select the best working matrix and the best fitting model for the correlated outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16261v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoi-Jeong Lim, Soo-Hyeon Park</dc:creator>
    </item>
    <item>
      <title>Constrained Shape Analysis with Applications to RNA Structure</title>
      <link>https://arxiv.org/abs/2502.16270</link>
      <description>arXiv:2502.16270v1 Announce Type: new 
Abstract: In many applications of shape analysis, lengths between some landmarks are constrained. For instance, biomolecules often have some bond lengths and some bond angles constrained, and variation occurs only along unconstrained bonds and constrained bonds' torsions where the latter are conveniently modelled by dihedral angles. Our work has been motivated by low resolution biomolecular chain RNA where only some prominent atomic bonds can be well identified. Here, we propose a new modelling strategy for such constrained shape analysis starting with a product of polar coordinates (polypolars), where, due to constraints, for example, some radial coordinates should be omitted, leaving products of spheres (polyspheres). We give insight into these coordinates for particular cases such as five landmarks which are motivated by a practical RNA application. We also discuss distributions for polypolar coordinates and give a specific methodology with illustration when the constrained size-and-shape variables are concentrated. There are applications of this in clustering and we give some insight into a modified version of the MINT-AGE algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16270v1</guid>
      <category>stat.ME</category>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanti V. Mardia, Benjamin Eltzner, Stephan F. Huckemann</dc:creator>
    </item>
    <item>
      <title>Local Information for Global Network Estimation in Latent Space Models</title>
      <link>https://arxiv.org/abs/2502.16504</link>
      <description>arXiv:2502.16504v1 Announce Type: new 
Abstract: In social networks, neighborhood is crucial for understanding individual behavior in response to environments, and thus it is essential to analyze an individual's local perspective within the global network. This paper studies how to utilize a partial information network centered around a given individual for global network estimation by fitting a general latent space model. Compared to the entire network, the partial information network contains a significant proportion of missing edges with its structure depending on a random, potentially sparse neighborhood, posing significant challenges for estimation. We address the challenges by proposing a projected gradient descent algorithm for maximizing the likelihood of the observed data and develop theoretical guarantees for its convergence under different neighborhood structures. Our convergence rates and estimation error bounds highlight the impact of bias in an individual's local view of the global network, and we further show that the bias can be quantified with an imbalance measure. Using simulated and real networks, we demonstrate the performance of our estimation method and how our approach enables researchers to gain additional insights into the structure of social networks, such as the tradeoff between degrees and imbalance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16504v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lijia Wang, Xiao Han, Yanhui Wu, Y. X. Rachel Wang</dc:creator>
    </item>
    <item>
      <title>Revamping Conformal Selection With Optimal Power: A Neyman--Pearson Perspective</title>
      <link>https://arxiv.org/abs/2502.16513</link>
      <description>arXiv:2502.16513v1 Announce Type: new 
Abstract: This paper introduces a novel conformal selection procedure, inspired by the Neyman--Pearson paradigm, to maximize the power of selecting qualified units while maintaining false discovery rate (FDR) control. Existing conformal selection methods may yield suboptimal power due to their reliance on conformal p-values, which are derived by substituting unobserved future outcomes with thresholds set by the null hypothesis. This substitution invalidates the exchangeability between imputed nonconformity scores for test data and those derived from calibration data, resulting in reduced power. In contrast, our approach circumvents the need for conformal p-values by constructing a likelihood-ratio-based decision rule that directly utilizes observed covariates from both calibration and test samples. The asymptotic optimality and FDR control of the proposed method are established under a correctly specified model, and modified selection procedures are introduced to improve power under model misspecification. The proposed methods are computationally efficient and can be readily extended to handle covariate shifts, making them well-suited for real-world applications. Simulation results show that these methods consistently achieve comparable or higher power than existing conformal p-value-based selection rules, particularly when the underlying distribution deviates from location-shift models, while effectively maintaining FDR control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16513v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Qin, Yukun Liu, Moming Li, Chiung-Yu Huang</dc:creator>
    </item>
    <item>
      <title>Regularized zero-inflated Bernoulli regression model</title>
      <link>https://arxiv.org/abs/2502.16574</link>
      <description>arXiv:2502.16574v1 Announce Type: new 
Abstract: Logistic regression model is widely used in many studies to investigate the relationship between a binary response variable Y and a set of potential predictors $X_1,\ldots, X_p$ (for example: $Y = 1$ if the outcome occurred and $Y = 0$ otherwise). One problem arising then is that, a proportion of the study subjects cannot experience the outcome of interest. This leads to an excessive presence of zeros in the study sample. This article is interested in estimating parameters of the zero-inflated Bernouilli regression model in a high-dimensional setting, i.e. with a large number of regressors. We use particulary Ridge regression and the Lasso which are typically achieved by constraining the weights of the model. and are useful when the number of predictors is much bigger than the number of observations. We establish the existency, consistency and asymptotic normality of the proposed regularized estimator. Then, we conduct a simulation study to investigate its finite-sample behavior, and application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16574v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mouhamed Ndoye, Aba Diop</dc:creator>
    </item>
    <item>
      <title>Robust transfer regression with corrupted labels</title>
      <link>https://arxiv.org/abs/2502.16594</link>
      <description>arXiv:2502.16594v1 Announce Type: new 
Abstract: In this paper, we introduce a robust transfer regression method designed to handle corrupted labels in target data, under the scenarios that the corruption affects a substantial portion of the labels and the locations of these corruptions are unknown. Theoretical analysis substantiates our approach, illustrating that the estimation error consists of three components: the first relates to the source data; the second encompasses the domain shift ; and the third captures the estimation error attributed to the corrupted vector. Our theoretical framework ensures that the proposed method surpasses estimations based solely on target data. We validate our method through numerical experiments aimed at reconstructing corrupted compressed signals. Additionally, we apply our method to analyze the association between O6-methylguanine-DNA methyltransferase (MGMT) methylation and gene expression in Glioblastoma (GBM) patients. Keywords:</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16594v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sheng Pan</dc:creator>
    </item>
    <item>
      <title>Optimizing Input Data Collection for Ranking and Selection</title>
      <link>https://arxiv.org/abs/2502.16659</link>
      <description>arXiv:2502.16659v1 Announce Type: new 
Abstract: We study a ranking and selection (R&amp;S) problem when all solutions share common parametric Bayesian input models updated with the data collected from multiple independent data-generating sources. Our objective is to identify the best system by designing a sequential sampling algorithm that collects input and simulation data given a budget. We adopt the most probable best (MPB) as the estimator of the optimum and show that its posterior probability of optimality converges to one at an exponential rate as the sampling budget increases. Assuming that the input parameters belong to a finite set, we characterize the $\epsilon$-optimal static sampling ratios for input and simulation data that maximize the convergence rate. Using these ratios as guidance, we propose the optimal sampling algorithm for R&amp;S (OSAR) that achieves the $\epsilon$-optimal ratios almost surely in the limit. We further extend OSAR by adopting the kernel ridge regression to improve the simulation output mean prediction. This not only improves OSAR's finite-sample performance, but also lets us tackle the case where the input parameters lie in a continuous space with a strong consistency guarantee for finding the optimum. We numerically demonstrate that OSAR outperforms a state-of-the-art competitor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16659v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eunhye Song, Taeho Kim</dc:creator>
    </item>
    <item>
      <title>Fitting networks with a cancellation trick</title>
      <link>https://arxiv.org/abs/2502.16728</link>
      <description>arXiv:2502.16728v1 Announce Type: new 
Abstract: The degree-corrected block model (DCBM), latent space model (LSM), and $\beta$-model are all popular network models. We combine their modeling ideas and propose the logit-DCBM as a new model. Similar as the $\beta$-model and LSM, the logit-DCBM contains nonlinear factors, where fitting the parameters is a challenging open problem. We resolve this problem by introducing a cancellation trick. We also propose R-SCORE as a recursive community detection algorithm, where in each iteration, we first use the idea above to update our parameter estimation, and then use the results to remove the nonlinear factors in the logit-DCBM so the renormalized model approximately satisfies a low-rank model, just like the DCBM. Our numerical study suggests that R-SCORE significantly improves over existing spectral approaches in many cases. Also, theoretically, we show that the Hamming error rate of R-SCORE is faster than that of SCORE in a specific sparse region, and is at least as fast outside this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16728v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiashun Jin, Jingming Wang</dc:creator>
    </item>
    <item>
      <title>Weighted model calibration with spatial conditional information</title>
      <link>https://arxiv.org/abs/2502.16785</link>
      <description>arXiv:2502.16785v1 Announce Type: new 
Abstract: Cost functions such as mean square error are often used in environmental model calibration. These treat observations as independent and equally important even though model residuals exhibit spatial dependence and additional observations near existing points do not provide as much information on the system as those elsewhere. To address this issue, we develop a method to derive calibration weights based on spatial conditional information. Using simulation experiments with Gaussian processes and the Tephra2 volcanic tephra dispersion model, we show that the additional accuracy and precision from weighted inference increases with the degree of observation clustering and spatial dependence present. To demonstrate real-world relevance, the methods are applied to tephra load observations from the 2014 eruption of the Kelud volcano in Indonesia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16785v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michele Nguyen, Maricar Rabonza, David Lallemant</dc:creator>
    </item>
    <item>
      <title>Minimum Copula Divergence for Robust Estimation</title>
      <link>https://arxiv.org/abs/2502.16831</link>
      <description>arXiv:2502.16831v1 Announce Type: new 
Abstract: This paper introduces a robust estimation framework based solely on the copula function. We begin by introducing a family of divergence measures tailored for copulas, including the \(\alpha\)-, \(\beta\)-, and \(\gamma\)-copula divergences, which quantify the discrepancy between a parametric copula model and an empirical copula derived from data independently of marginal specifications. Using these divergence measures, we propose the minimum copula divergence estimator (MCDE), an estimation method that minimizes the divergence between the model and the empirical copula. The framework proves particularly effective in addressing model misspecifications and analyzing heavy-tailed data, where traditional methods such as the maximum likelihood estimator (MLE) may fail. Theoretical results show that common copula families, including Archimedean and elliptical copulas, satisfy conditions ensuring the boundedness of divergence-based estimators, thereby guaranteeing the robustness of MCDE, especially in the presence of extreme observations. Numerical examples further underscore MCDE's ability to adapt to varying dependence structures, ensuring its utility in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16831v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinto Eguchi, Shogo Kato</dc:creator>
    </item>
    <item>
      <title>A dynamic copula model for probabilistic forecasting of non-Gaussian multivariate time series</title>
      <link>https://arxiv.org/abs/2502.16874</link>
      <description>arXiv:2502.16874v1 Announce Type: new 
Abstract: Multivariate time series (MTS) data often include a heterogeneous mix of non-Gaussian distributional features (asymmetry, multimodality, heavy tails) and data types (continuous and discrete variables). Traditional MTS methods based on convenient parametric distributions are typically ill-equipped to model this heterogeneity. Copula models provide an appealing alternative, but present significant obstacles for fully Bayesian inference and probabilistic forecasting. To overcome these challenges, we propose a novel and general strategy for posterior approximation in MTS copula models and apply it to a Gaussian copula built from a dynamic factor model. This framework provides scalable, fully Bayesian inference for cross-sectional and serial dependencies and nonparametrically learns heterogeneous marginal distributions. We validate this approach by establishing posterior consistency and confirm excellent finite-sample performance even under model misspecification using simulated data. We apply our method to crime count and macroeconomic MTS data and find superior probabilistic forecasting performance compared to popular MTS models. These results demonstrate that the proposed method is a versatile, general-purpose utility for probabilistic forecasting of MTS that works well across of range of applications with minimal user input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16874v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Zito, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Functional Bayesian Additive Regression Trees with Shape Constraints</title>
      <link>https://arxiv.org/abs/2502.16888</link>
      <description>arXiv:2502.16888v1 Announce Type: new 
Abstract: Motivated by the great success of Bayesian additive regression trees (BART) on regression, we propose a nonparametric Bayesian approach for the function-on-scalar regression problem, termed as Functional BART (FBART). Utilizing spline-based function representation and tree-based domain partition model, FBART offers great flexibility in characterizing the complex and heterogeneous relationship between the response curve and scalar covariates. We devise a tailored Bayesian backfitting algorithm for estimating the parameters in the FBART model. Furthermore, we introduce an FBART model with shape constraints on the response curve, enhancing estimation and prediction performance when prior shape information of response curves is available. By incorporating a shape-constrained prior, we ensure that the posterior samples of the response curve satisfy the required shape constraints (e.g., monotonicity and/or convexity). Our proposed FBART model and its shape-constrained version are the new advances of BART models for functional data. Under certain regularity conditions, we derive the posterior convergence results for both FBART and its shape-constrained version. Finally, the superiority of the proposed methods over other competitive counterparts is validated through simulation experiments under various settings and analyses of two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16888v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Cao, Shiyuan He, Bohai Zhang</dc:creator>
    </item>
    <item>
      <title>A Web-Based Application Leveraging Geospatial Information to Automate On-Farm Trial Design</title>
      <link>https://arxiv.org/abs/2502.17326</link>
      <description>arXiv:2502.17326v1 Announce Type: new 
Abstract: On-farm sensor data have allowed farmers to implement field management techniques and intensively track the corresponding responses. These data combined with historical records open the door for real-time field management improvements with the help of current advancements in computing power. However, despite these advances, the statistical design of experiments is rarely used to evaluate the performance of field management techniques accurately. Traditionally, randomized block design is prevalent in statistical designs of field trials, but in practice it is limited in dealing with large variations in soil classes, management practices, and crop varieties. More specifically, although this experimental design is suited for most trial types, it is not the optimal choice when multiple factors are tested over multifarious natural variations in farms, due to the economic constraints caused by the sheer number of variables involved. Experimental refinement is required to better estimate the effects of the primary factor in the presence of auxiliary factors. In this way, farmers can better understand the characteristics and limitations of the primary factor. This work presents a framework for automating the analysis of local field variations by fusing soil classification data and lidar topography data with historical yield. This framework will be leveraged to automate the designing of field experiments based on multiple topographic features</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17326v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sneha Jha, Yaguang Zhang, J. V. Krogmeier, D Buckmaster</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm</title>
      <link>https://arxiv.org/abs/2502.17367</link>
      <description>arXiv:2502.17367v1 Announce Type: new 
Abstract: Decision making often uses complex computer codes run at the exa-scale (10e18 flops). Such computer codes or models are often run in a hierarchy of different levels of fidelity ranging from the basic to the very sophisticated. The top levels in this hierarchy are expensive to run, limiting the number of possible runs. To make use of runs over all levels, and crucially improve emulation at the top level, we use multi-level Gaussian process emulators (GPs). We will present a new method of building GP emulators from hierarchies of models. In order to share information across the different levels, l=1,...,L, we define the form of the prior of the l+1th level to be the posterior of the lth level, hence building a Bayesian hierarchical structure for the top Lth level. This enables us to not only learn about the GP hyperparameters as we move up the multi-level hierarchy, but also allows us to limit the total number of parameters in the full model, whilst maintaining accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17367v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louise Kimpton, James Salter, Xiaoyu Xiong, Peter Challenor</dc:creator>
    </item>
    <item>
      <title>Stronger Neyman Regret Guarantees for Adaptive Experimental Design</title>
      <link>https://arxiv.org/abs/2502.17427</link>
      <description>arXiv:2502.17427v1 Announce Type: new 
Abstract: We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual "multigroup" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17427v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgy Noarov, Riccardo Fogliato, Martin Bertran, Aaron Roth</dc:creator>
    </item>
    <item>
      <title>Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness</title>
      <link>https://arxiv.org/abs/2502.16391</link>
      <description>arXiv:2502.16391v1 Announce Type: cross 
Abstract: In this paper, we explore the theoretical properties of subspace recovery using Winsorized Principal Component Analysis (WPCA), utilizing a common data transformation technique that caps extreme values to mitigate the impact of outliers. Despite the widespread use of winsorization in various tasks of multivariate analysis, its theoretical properties, particularly for subspace recovery, have received limited attention. We provide a detailed analysis of the accuracy of WPCA, showing that increasing the number of samples while decreasing the proportion of outliers guarantees the consistency of the sample subspaces from WPCA with respect to the true population subspace. Furthermore, we establish perturbation bounds that ensure the WPCA subspace obtained from contaminated data remains close to the subspace recovered from pure data. Additionally, we extend the classical notion of breakdown points to subspace-valued statistics and derive lower bounds for the breakdown points of WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to outliers while maintaining consistency under mild assumptions. A toy example is provided to numerically illustrate the behavior of the upper bounds for perturbation bounds and breakdown points, emphasizing winsorization's utility in subspace recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16391v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangil Han, Kyoowon Kim, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Optimal Kernel Learning for Gaussian Process Models with High-Dimensional Input</title>
      <link>https://arxiv.org/abs/2502.16617</link>
      <description>arXiv:2502.16617v1 Announce Type: cross 
Abstract: Gaussian process (GP) regression is a popular surrogate modeling tool for computer simulations in engineering and scientific domains. However, it often struggles with high computational costs and low prediction accuracy when the simulation involves too many input variables. For some simulation models, the outputs may only be significantly influenced by a small subset of the input variables, referred to as the ``active variables''. We propose an optimal kernel learning approach to identify these active variables, thereby overcoming GP model limitations and enhancing system understanding. Our method approximates the original GP model's covariance function through a convex combination of kernel functions, each utilizing low-dimensional subsets of input variables. Inspired by the Fedorov-Wynn algorithm from optimal design literature, we develop an optimal kernel learning algorithm to determine this approximation. We incorporate the effect heredity principle, a concept borrowed from the field of ``design and analysis of experiments'', to ensure sparsity in active variable selection. Through several examples, we demonstrate that the proposed method outperforms alternative approaches in correctly identifying active input variables and improving prediction accuracy. It is an effective solution for interpreting the surrogate GP regression and simplifying the complex underlying system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16617v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lulu Kang, Minshen Xu</dc:creator>
    </item>
    <item>
      <title>Time Series Domain Adaptation via Latent Invariant Causal Mechanism</title>
      <link>https://arxiv.org/abs/2502.16637</link>
      <description>arXiv:2502.16637v1 Announce Type: cross 
Abstract: Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16637v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruichu Cai, Junxian Huang, Zhenhui Yang, Zijian Li, Emadeldeen Eldele, Min Wu, Fuchun Sun</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v1 Announce Type: cross 
Abstract: This paper examines the construction of confidence sets for parameters defined as a linear functional of the solution to an integral equation involving conditional expectations. We show that any confidence set uniformly valid over a broad class of probability laws, allowing the integral equation to be arbitrarily ill-posed must have, with high probability under some laws, a diameter at least as large as the diameter of the parameter's range over the model. Additionally, we establish that uniformly consistent estimators of the parameter do not exist. We show that, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid. Furthermore, we argue that inverting the score test, a successful approach in that literature, does not extend to the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the general class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>On Quantile Regression Forests for Modelling Mixed-Frequency and Longitudinal Data</title>
      <link>https://arxiv.org/abs/2502.17137</link>
      <description>arXiv:2502.17137v1 Announce Type: cross 
Abstract: The aim of this thesis is to extend the applications of the Quantile Regression Forest (QRF) algorithm to handle mixed-frequency and longitudinal data. To this end, standard statistical approaches have been exploited to build two novel algorithms: the Mixed- Frequency Quantile Regression Forest (MIDAS-QRF) and the Finite Mixture Quantile Regression Forest (FM-QRF). The MIDAS-QRF combines the flexibility of QRF with the Mixed Data Sampling (MIDAS) approach, enabling non-parametric quantile estimation with variables observed at different frequencies. FM-QRF, on the other hand, extends random effects machine learning algorithms to a QR framework, allowing for conditional quantile estimation in a longitudinal data setting. The contributions of this dissertation lie both methodologically and empirically. Methodologically, the MIDAS-QRF and the FM-QRF represent two novel approaches for handling mixed-frequency and longitudinal data in QR machine learning framework. Empirically, the application of the proposed models in financial risk management and climate-change impact evaluation demonstrates their validity as accurate and flexible models to be applied in complex empirical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17137v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mila Andreani</dc:creator>
    </item>
    <item>
      <title>Joint Value Estimation and Bidding in Repeated First-Price Auctions</title>
      <link>https://arxiv.org/abs/2502.17292</link>
      <description>arXiv:2502.17292v1 Announce Type: cross 
Abstract: We study regret minimization in repeated first-price auctions (FPAs), where a bidder observes only the realized outcome after each auction -- win or loss. This setup reflects practical scenarios in online display advertising where the actual value of an impression depends on the difference between two potential outcomes, such as clicks or conversion rates, when the auction is won versus lost. We analyze three outcome models: (1) adversarial outcomes without features, (2) linear potential outcomes with features, and (3) linear treatment effects in features. For each setting, we propose algorithms that jointly estimate private values and optimize bidding strategies, achieving near-optimal regret bounds. Notably, our framework enjoys a unique feature that the treatments are also actively chosen, and hence eliminates the need for the overlap condition commonly required in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17292v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Wen, Yanjun Han, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Statistical Inferences and Predictions for Areal Data and Spatial Data Fusion with Hausdorff--Gaussian Processes</title>
      <link>https://arxiv.org/abs/2208.07900</link>
      <description>arXiv:2208.07900v3 Announce Type: replace 
Abstract: Accurate modeling of spatial dependence is pivotal in analyzing spatial data, influencing parameter estimation and predictions. The spatial structure of the data significantly impacts valid statistical inference. Existing models for areal data often rely on adjacency matrices, struggling to differentiate between polygons of varying sizes and shapes. Conversely, data fusion models rely on computationally intensive numerical integrals, presenting challenges for moderately large datasets. In response to these issues, we propose the Hausdorff-Gaussian process (HGP), a versatile model utilizing the Hausdorff distance to capture spatial dependence in both point and areal data. Integration into generalized linear mixed-effects models enhances its applicability, particularly in addressing data fusion challenges. We validate our approach through a comprehensive simulation study and application to two real-world scenarios: one involving areal data and another demonstrating its effectiveness in data fusion. The results suggest that the HGP is competitive with specialized models regarding goodness-of-fit and prediction performances. In summary, the HGP offers a flexible and robust solution for modeling spatial data of various types and shapes, with potential applications spanning fields such as public health and climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07900v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Angular Combining of Forecasts of Probability Distributions</title>
      <link>https://arxiv.org/abs/2305.16735</link>
      <description>arXiv:2305.16735v2 Announce Type: replace 
Abstract: When multiple forecasts are available for a probability distribution, forecast combining enables a pragmatic synthesis of the information to extract the wisdom of the crowd. The linear opinion pool has been widely used, whereby the combining is applied to the probabilities of the distributional forecasts. However, it has been argued that this will tend to deliver overdispersed distributions, prompting the combination to be applied, instead, to the quantiles of the distributional forecasts. Results from different applications are mixed, leaving it as an empirical question whether to combine probabilities or quantiles. In this paper, we present an alternative approach. Looking at the distributional forecasts, combining the probabilities can be viewed as vertical combining, with quantile combining seen as horizontal combining. Our proposal is to allow combining to take place on an angle between the extreme cases of vertical and horizontal combining. We term this angular combining. The angle is a parameter that can be optimized using a proper scoring rule. For implementation, we provide a pragmatic numerical approach and a simulation algorithm. Among our theoretical results, we show that, as with vertical and horizontal averaging, angular averaging results in a distribution with mean equal to the average of the means of the distributions that are being combined. We also show that angular averaging produces a distribution with lower variance than vertical averaging, and, under certain assumptions, greater variance than horizontal averaging. We provide empirical results for distributional forecasts of Covid mortality, macroeconomic survey data, and electricity prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16735v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James W. Taylor, Xiaochun Meng</dc:creator>
    </item>
    <item>
      <title>A flexible Bayesian g-formula for causal survival analyses with time-dependent confounding</title>
      <link>https://arxiv.org/abs/2402.02306</link>
      <description>arXiv:2402.02306v4 Announce Type: replace 
Abstract: In longitudinal observational studies with time-to-event outcomes, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios. The g-formula is a useful tool for this analysis. To enhance the traditional parametric g-formula, we developed an alternative g-formula estimator, which incorporates the Bayesian Additive Regression Trees (BART) into the modeling of the time-evolving generative components, aiming to mitigate the bias due to model misspecification. We focus on binary time-varying treatments and introduce a general class of g-formulas for discrete survival data that can incorporate the longitudinal balancing scores. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment strategies, i.e., static or dynamic. For each type of treatment strategy, we provide posterior sampling algorithms. We conducted simulations to illustrate the empirical performance of the proposed method and demonstrate its practical utility using data from the Yale New Haven Health System's electronic health records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02306v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Liangyuan Hu, Fan Li</dc:creator>
    </item>
    <item>
      <title>Detection and inference of changes in high-dimensional linear regression with non-sparse structures</title>
      <link>https://arxiv.org/abs/2402.06915</link>
      <description>arXiv:2402.06915v4 Announce Type: replace 
Abstract: For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the exact sparsity of neither regression parameters nor their differences, a.k.a.\ differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package \texttt{inferchange} on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06915v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haeran Cho, Tobias Kley, Housen Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Selecting Relevant External Data (BASE): Application to a study of Long-Term Outcomes in a Hemophilia Gene Therapy Trial</title>
      <link>https://arxiv.org/abs/2403.13260</link>
      <description>arXiv:2403.13260v3 Announce Type: replace 
Abstract: Gene therapies aim to address the root causes of diseases, particularly those stemming from rare genetic defects that can be life-threatening or severely debilitating. Although an increasing number of gene therapies have received regulatory approvals in recent years, understanding their long-term efficacy in trials with limited follow-up time remains challenging. To address this critical question, we propose a novel Bayesian framework designed to selectively integrate relevant external data with internal trial data to improve the inference of the durability of long-term efficacy. We proved that the proposed method has desired theoretical properties, such as identifying and favoring external subsets deemed relevant, where the relevance is defined as the similarity, induced by the marginal likelihood, between the generating mechanisms of the internal data and the selected external data. We also conducted comprehensive simulations to evaluate its performance under various scenarios. Furthermore, we apply this method to predict and infer the endogenous factor IX (FIX) levels of patients who receive Etranacogene dezaparvovec over the long-term. Our estimated long-term FIX levels, validated by recent trial data, indicate that Etranacogene dezaparvovec induces sustained FIX production. Together, the theoretical findings, simulation results, and successful application of this framework underscore its potential to address similar long-term effectiveness estimation and inference questions in real world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13260v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Pan, Xiaoling Mei, Xiang Zhang, Weining Shen, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Deep Learning and Conformal prediction</title>
      <link>https://arxiv.org/abs/2406.04874</link>
      <description>arXiv:2406.04874v3 Announce Type: replace 
Abstract: Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these ``user-choices''. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance, and tolerance threshold.
  Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).
  Our method, named ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or other moment type functionals), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters and amortized, we test this new method on four different applications and compare it with other ABC methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04874v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meili Baragatti, Casenave C\'eline, Bertrand Cloez, David M\'etivier, Isabelle Sanchez</dc:creator>
    </item>
    <item>
      <title>Positive and Unlabeled Data: Model, Estimation, Inference, and Classification</title>
      <link>https://arxiv.org/abs/2407.09735</link>
      <description>arXiv:2407.09735v3 Announce Type: replace 
Abstract: This study introduces a new approach to addressing positive and unlabeled (PU) data through the double exponential tilting model (DETM). Traditional methods often fall short because they only apply to selected completely at random (SCAR) PU data, where the labeled positive and unlabeled positive data are assumed to be from the same distribution. In contrast, our DETM's dual structure effectively accommodates the more complex and underexplored selected at random PU data, where the labeled and unlabeled positive data can be from different distributions. We rigorously establish the theoretical foundations of DETM, including identifiability, parameter estimation, and asymptotic properties. Additionally, we move forward to statistical inference by developing a goodness-of-fit test for the SCAR condition and constructing confidence intervals for the proportion of positive instances in the target domain. We leverage an approximated Bayes classifier for classification tasks, demonstrating DETM's robust performance in prediction. Through theoretical insights and practical applications, this study highlights DETM as a comprehensive framework for addressing the challenges of PU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09735v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyan Liu, Chi-Kuang Yeh, Xin Zhang, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Identifying treatment response subgroups in observational time-to-event data</title>
      <link>https://arxiv.org/abs/2408.03463</link>
      <description>arXiv:2408.03463v4 Announce Type: replace 
Abstract: Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for treatment effect estimation primarily rely on Randomised Controlled Trials (RCTs), which are often limited by insufficient power, multiple comparisons, and unbalanced covariates. In addition, RCTs tend to feature more homogeneous patient groups, making them less relevant for uncovering subgroups in the population encountered in real-world clinical practice. Subgroup analyses established for RCTs suffer from significant statistical biases when applied to observational studies, which benefit from larger and more representative populations. Our work introduces a novel, outcome-guided, subgroup analysis strategy for identifying subgroups of treatment response in both RCTs and observational studies alike. It hence positions itself in-between individualised and average treatment effect estimation to uncover patient subgroups with distinct treatment responses, critical for actionable insights that may influence treatment guidelines. In experiments, our approach significantly outperforms the current state-of-the-art method for subgroup analysis in both randomised and observational treatment regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03463v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</dc:creator>
    </item>
    <item>
      <title>Regression-based proximal causal inference for right-censored time-to-event data</title>
      <link>https://arxiv.org/abs/2409.08924</link>
      <description>arXiv:2409.08924v2 Announce Type: replace 
Abstract: Unmeasured confounding is one of the major concerns in causal inference from observational data. Proximal causal inference (PCI) is an emerging methodological framework to detect and potentially account for confounding bias by carefully leveraging a pair of negative control exposure (NCE) and outcome (NCO) variables, also known as treatment and outcome confounding proxies. Although regression-based PCI is well developed for binary and continuous outcomes, analogous PCI regression methods for right-censored time-to-event outcomes are currently lacking. In this paper, we propose a novel two-stage regression PCI approach for right-censored survival data under an additive hazard structural model. We provide theoretical justification for the proposed approach tailored to different types of NCOs, including continuous, count, and right-censored time-to-event variables. We illustrate the approach with an evaluation of the effectiveness of right heart catheterization among critically ill patients using data from the SUPPORT study. Our method is implemented in the open-access R package 'pci2s'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08924v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendrick Li, George C. Linderman, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Model with Change-Points and Its Application to Time Pressure Effects in Educational Assessment</title>
      <link>https://arxiv.org/abs/2410.22300</link>
      <description>arXiv:2410.22300v2 Announce Type: replace 
Abstract: Educational assessments are valuable tools for measuring student knowledge and skills, but their validity can be compromised when test takers exhibit changes in response behavior due to factors such as time pressure. To address this issue, we introduce a novel latent factor model with change-points for item response data, designed to detect and account for individual-level shifts in response patterns during testing. This model extends traditional Item Response Theory (IRT) by incorporating person-specific change-points, which enables simultaneous estimation of item parameters, person latent traits, and the location of behavioral changes. We evaluate the proposed model through extensive simulation studies, which demonstrate its ability to accurately recover item parameters, change-point locations, and individual ability estimates under various conditions. Our findings show that accounting for change-points significantly reduces bias in ability estimates, particularly for respondents affected by time pressure. Application of the model to two real-world educational testing datasets reveals distinct patterns of change-point occurrence between high-stakes and lower-stakes tests, providing insights into how test-taking behavior evolves during the tests. This approach offers a more nuanced understanding of test-taking dynamics, with important implications for test design, scoring, and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22300v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Wallin, Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based Inference for Bivariate Heteroscedastic Extremes with a Changing Tail Copula</title>
      <link>https://arxiv.org/abs/2411.15819</link>
      <description>arXiv:2411.15819v2 Announce Type: replace 
Abstract: This paper introduces a copula-based model for independent but non-identically distributed data with heteroscedastic extremes marginal and changing tail dependence structures. We establish a unified framework for inference by proving the weak convergence of the bivariate sequential tail empirical process and its empirical bootstrap counterpart. We derive the asymptotic properties of several estimators on the tail, including the quasi-tail copula, integrated scedasis function, and Hill estimator, treating them as functionals of the bivariate sequential tail empirical process. This process-centric approach enables the development of bootstrap-based methods and ensures the theoretical validity of the derived statistics. As an application of our inference method, we propose bootstrap-based tests for the equivalence of extreme value indices, the equivalence of scedasis functions, and non-changing tail dependence when marginal scedasis functions are identical. Our simulations validate the robustness and efficiency of the bootstrap-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15819v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Hu, Yanxi Hou</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v2 Announce Type: replace 
Abstract: This paper introduces a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, effectively balancing quadratic ($\ell_2$) and absolute linear ($\ell_1$) loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data efficiently. The generalized Bayesian approach constructs a working likelihood based on the SPH loss, facilitating efficient and stable estimation while providing rigorous uncertainty quantification for all model parameters. Notably, this approach allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients--such as ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings--the framework ensures principled inference. We establish rigorous theoretical guarantees for accurate parameter estimation and correct predictor selection under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superior performance of our approach compared to traditional Bayesian regression methods based on $\ell_2$ and $\ell_1$-loss functions. The results highlight its flexibility and robustness, particularly in challenging high-dimensional settings characterized by data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Biomarker combination based on the Youden index with and without gold standard</title>
      <link>https://arxiv.org/abs/2412.17471</link>
      <description>arXiv:2412.17471v2 Announce Type: replace 
Abstract: In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the Area Under the ROC Curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this paper, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17471v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Sun, Yanting Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v3 Announce Type: replace 
Abstract: In text analysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent in text data by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancing text mining efforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilaria Bombelli, Domenica Fioredistella Iezzi, Emiliano Seri, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Databases</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v3 Announce Type: replace 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU, available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example using a simulated real-world dataset is presented to examine various analytical aspects, including three key dimensions of MHSU detection based on the DDDM framework, as well as temporal analysis to demonstrate the package's application for healthcare policymakers. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
    <item>
      <title>Large covariance matrix estimation with factor-assisted variable clustering</title>
      <link>https://arxiv.org/abs/2501.10942</link>
      <description>arXiv:2501.10942v2 Announce Type: replace 
Abstract: This paper studies the covariance matrix estimation for high-dimensional time series within a new framework that combines low-rank factor and latent variable-specific cluster structures. The popular methods based on assuming the sparse error covariance matrix after taking out common factors may be invalid for many financial applications. Our formulation postulates a latent model-based error cluster structure after removing observable factors, which not only leads to more interpretable cluster patterns but also accounts for non-sparse cross-sectional correlations among the variable-specific residuals. Our method begins with using least-squares to estimate the factor loadings, followed by identifying the latent cluster structure by thresholding the scaled covariance difference measures of residuals. A novel ratio-based criterion is introduced to determine the threshold parameter when performing the developed clustering algorithm. We then establish the cluster recovery consistency of our method and derive the convergence rates of our proposed covariance matrix estimators under different norms. Finally, we demonstrate the superior finite sample performance of our proposal over the competing methods through both extensive simulations and a real data application on minimum variance portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10942v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Li, Xinghao Qiao, Cheng Yu</dc:creator>
    </item>
    <item>
      <title>Sequential Methods for Error Correction of Probabilistic Wind Power Forecasts</title>
      <link>https://arxiv.org/abs/2501.14805</link>
      <description>arXiv:2501.14805v3 Announce Type: replace 
Abstract: Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in wind power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method, we beat state-of-the-art methods and achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14805v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensen, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>A general, flexible and harmonious framework to construct interpretable functions in regression analysis</title>
      <link>https://arxiv.org/abs/2501.15526</link>
      <description>arXiv:2501.15526v2 Announce Type: replace 
Abstract: An interpretable model or method has several appealing features, such as reliability to adversarial examples, transparency of decision-making, and communication facilitator. However, interpretability is a subjective concept, and even its definition can be diverse. The same model may be deemed as interpretable by a study team, but regarded as a black-box algorithm by another squad. Simplicity, accuracy and generalizability are some additional important aspects of evaluating interpretability. In this work, we present a general, flexible and harmonious framework to construct interpretable functions in regression analysis with a focus on continuous outcomes. We formulate a functional skeleton in light of users' expectations of interpretability. A new measure based on Mallows's $C_p$-statistic is proposed for model selection to balance approximation, generalizability, and interpretability. We apply this approach to derive a sample size formula in adaptive clinical trial designs to demonstrate the general workflow, and to explain operating characteristics in a Bayesian Go/No-Go paradigm to show the potential advantages of using meaningful intermediate variables. Generalization to categorical outcomes is illustrated in an example of hypothesis testing based on Fisher's exact test. A real data analysis of NHANES (National Health and Nutrition Examination Survey) is conducted to investigate relationships between some important laboratory measurements. We also discuss some extensions of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15526v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhan, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Conformal novelty detection for replicate point patterns with FDR or FWER control</title>
      <link>https://arxiv.org/abs/2501.18195</link>
      <description>arXiv:2501.18195v2 Announce Type: replace 
Abstract: Monte Carlo tests are widely used for computing valid p-values without requiring known distributions of test statistics. When performing multiple Monte Carlo tests, it is essential to maintain control of the type I error. Some techniques for multiplicity control pose requirements on the joint distribution of the p-values, for instance independence, which can be computationally intensive to achieve using na\"ive multiple Monte Carlo testing. We highlight in this work that multiple Monte Carlo testing is an instance of conformal novelty detection. Leveraging this insight enables a more efficient multiple Monte Carlo testing procedure, avoiding excessive simulations while still ensuring exact control over the false discovery rate or the family-wise error rate. We call this approach conformal multiple Monte Carlo testing. The performance is investigated in the context of global envelope tests for point pattern data through a simulation study and an application to a sweat gland data set. Results reveal that with a fixed number of simulations under the null hypothesis, our proposed method yields substantial improvements in power of the testing procedure as compared to the na\"ive multiple Monte Carlo testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18195v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe A. N. Biscio, Adrien Mazoyer, Martin V. Vejling</dc:creator>
    </item>
    <item>
      <title>A critical evaluation of longitudinal proportional effect models</title>
      <link>https://arxiv.org/abs/2502.00214</link>
      <description>arXiv:2502.00214v2 Announce Type: replace 
Abstract: Nonlinear longitudinal models for repeated continuous measures with proportional treatment effects have been proposed to improve power and provide direct estimates of the proportional treatment effect in randomized clinical trials. These models make a strong assumption about a fixed proportional treatment effect over time, which can lead to bias and Type I error inflation when the assumption is violated. Even when the proportional effect assumption holds, we demonstrate that these models are biased and their inference is sensitive to the labeling of treatment groups. Typically, this bias favors the active group, inflates Type I error, and can result in one-sided testing. Conversely, the bias can make it more difficult to detect treatment harm, creating a safety concern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00214v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. Donohue, Philip S. Insel, Oliver Langford</dc:creator>
    </item>
    <item>
      <title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable A/B-Testing</title>
      <link>https://arxiv.org/abs/2502.04793</link>
      <description>arXiv:2502.04793v2 Announce Type: replace 
Abstract: A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.
  To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04793v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Jeunen</dc:creator>
    </item>
    <item>
      <title>A New Causal Rule Learning Approach to Interpretable Estimation of Heterogeneous Treatment Effect</title>
      <link>https://arxiv.org/abs/2310.06746</link>
      <description>arXiv:2310.06746v2 Announce Type: replace-cross 
Abstract: Interpretability plays a critical role in the application of statistical learning for estimating heterogeneous treatment effects (HTE) for complex diseases. In this study, we leverage a rule-based workflow, namely causal rule learning (CRL) to estimate and enhance our understanding of HTE for atrial septal defect, addressing an overlooked question in previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The CRL process consists of three steps: rule discovery, which generates a set of causal rules with corresponding subgroup average treatment effects; rule selection, which identifies a subset of these rules to deconstruct individual-level treatment effects as a linear combination of subgroup-level effects; and rule analysis, which outlines a detailed procedure for further analyzing each selected rule from multiple perspectives to identify the most promising rules for validation. Extensive simulation studies and real-world data analysis demonstrate that CRL outperforms other methods in providing interpretable estimates of HTE, especially when dealing with complex ground truth and sufficient sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06746v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Wu, Hanzhong Liu, Kai Ren, Shujie Ma, Xiangyu Chang</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Causal Discovery without Acyclicity Constraints</title>
      <link>https://arxiv.org/abs/2408.13448</link>
      <description>arXiv:2408.13448v3 Announce Type: replace-cross 
Abstract: Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13448v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Duong, Hung Le, Biwei Huang, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>Certified Causal Defense with Generalizable Robustness</title>
      <link>https://arxiv.org/abs/2408.15451</link>
      <description>arXiv:2408.15451v2 Announce Type: replace-cross 
Abstract: While machine learning models have proven effective across various scenarios, it is widely acknowledged that many models are vulnerable to adversarial attacks. Recently, there have emerged numerous efforts in adversarial defense. Among them, certified defense is well known for its theoretical guarantees against arbitrary adversarial perturbations on input within a certain range (e.g., $l_2$ ball). However, most existing works in this line struggle to generalize their certified robustness in other data domains with distribution shifts. This issue is rooted in the difficulty of eliminating the negative impact of spurious correlations on robustness in different domains. To address this problem, in this work, we propose a novel certified defense framework GLEAN, which incorporates a causal perspective into the generalization problem in certified defense. More specifically, our framework integrates a certifiable causal factor learning component to disentangle the causal relations and spurious correlations between input and label, and thereby exclude the negative effect of spurious correlations on defense. On top of that, we design a causally certified defense strategy to handle adversarial attacks on latent causal factors. In this way, our framework is not only robust against malicious noises on data in the training distribution but also can generalize its robustness across domains with distribution shifts. Extensive experiments on benchmark datasets validate the superiority of our framework in certified robustness generalization in different data domains. Code is available in the supplementary materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15451v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Qiao, Yu Yin, Chen Chen, Jing Ma</dc:creator>
    </item>
    <item>
      <title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
      <link>https://arxiv.org/abs/2411.05869</link>
      <description>arXiv:2411.05869v2 Announce Type: replace-cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05869v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi</dc:creator>
    </item>
    <item>
      <title>Practical Performative Policy Learning with Strategic Agents</title>
      <link>https://arxiv.org/abs/2412.01344</link>
      <description>arXiv:2412.01344v3 Announce Type: replace-cross 
Abstract: This paper studies the performative policy learning problem, where agents adjust their features in response to a released policy to improve their potential outcomes, inducing an endogenous distribution shift. There has been growing interest in training machine learning models in strategic environments, including strategic classification and performative prediction. However, existing approaches often rely on restrictive parametric assumptions: micro-level utility models in strategic classification and macro-level data distribution maps in performative prediction, severely limiting scalability and generalizability. We approach this problem as a complex causal inference task, relaxing parametric assumptions on both micro-level agent behavior and macro-level data distribution. Leveraging bounded rationality, we uncover a practical low-dimensional structure in distribution shifts and construct an effective mediator in the causal path from the deployed model to the shifted data. We then propose a gradient-based policy optimization algorithm with a differentiable classifier as a substitute for the high-dimensional distribution map. Our algorithm efficiently utilizes batch feedback and limited manipulation patterns. Our approach achieves high sample efficiency compared to methods reliant on bandit feedback or zero-order optimization. We also provide theoretical guarantees for algorithmic convergence. Extensive and challenging experiments on high-dimensional settings demonstrate our method's practical efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01344v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Ying Chen, Bo Li</dc:creator>
    </item>
  </channel>
</rss>
