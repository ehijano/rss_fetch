<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 01:27:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Understanding Spatial Regression Models from a Weighting Perspective in an Observational Study of Superfund Remediation</title>
      <link>https://arxiv.org/abs/2508.19572</link>
      <description>arXiv:2508.19572v1 Announce Type: new 
Abstract: Superfund sites are locations in the United States with high levels of environmental toxicants, often resulting from industrial activity or improper waste management. Given mounting evidence linking prenatal environmental exposures to adverse birth outcomes, estimating the impact of Superfund remediation is of substantial policy relevance. A widespread approach is to fit a spatial regression, i.e., a linear regression of the outcome (e.g., birth weight) on binary treatment (e.g., indicator for Superfund site remediation) and covariates, along with a spatially structured error term to account for unmeasured spatial confounding. Despite this common practice, it remains unclear to what extent spatial regression models account for unmeasured spatial confounding in finite samples and whether such adjustments can be reformulated within a design-based framework for causal inference. To fill this knowledge gap, we introduce a weighting framework that encompasses three canonical types of spatial regression models: random effects, conditional autoregressive, and Gaussian process models. This framework yields new insights into how spatial regression models build causal contrasts between treated and control units. Specifically, we show that: 1) the spatially autocorrelated error term produces approximate balance on a hidden set of covariates, thereby adjusting for a specific class of unmeasured confounders; and 2) the error covariance structure can be equivalently expressed as regressors in a linear model. We also introduce a new average treatment effect estimator that simultaneously accounts for multiple forms of unmeasured spatial confounding, as well as diagnostics that enhance interpretability. In a study of Superfund remediation, our approach illuminates the role of design-based adjustment for confounding and provides guidance for evaluating environmental interventions in spatial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19572v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie M. Woodward, Francesca Dominici, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Comment on Garc\'ia-Donato et al. (2025) "Model uncertainty and missing data: An objective Bayesian perspective"</title>
      <link>https://arxiv.org/abs/2508.19939</link>
      <description>arXiv:2508.19939v1 Announce Type: new 
Abstract: Garcia-Donato et al. (2025) present a methodology for handling missing data in a model selection problem using an objective Bayesian approach. The current comment discusses an alternative, existing objective Bayesian method for this problem. First, rather than using the g prior, O'Hagan's fractional Bayes factor (O'Hagan, 1995) is utilized based on a minimal fraction. Second, and more importantly due to the focus on missing data, Rubin's rules for multiple imputation can directly be used as the fractional Bayes factor can be written as a Savage-Dickey density ratio for a variable selection problem. The current comment derives the methodology for a variable selection problem. Moreover, its implied behavior is illustrated in a numerical experiment, showing competitive results as the method of Garcia-Donato et al. (2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19939v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joris Mulder</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Simulation for Complex Spatial Processes</title>
      <link>https://arxiv.org/abs/2508.20067</link>
      <description>arXiv:2508.20067v1 Announce Type: new 
Abstract: A key objective in spatial statistics is to simulate from the distribution of a spatial process at a selection of unobserved locations conditional on observations (i.e., a predictive distribution) to enable spatial prediction and uncertainty quantification. However, exact conditional simulation from this predictive distribution is intractable or inefficient for many spatial process models. In this paper, we propose neural conditional simulation (NCS), a general method for spatial conditional simulation that is based on neural diffusion models. Specifically, using spatial masks, we implement a conditional score-based diffusion model that evolves Gaussian noise into samples from a predictive distribution when given a partially observed spatial field and spatial process parameters as inputs. The diffusion model relies on a neural network that only requires unconditional samples from the spatial process for training. Once trained, the diffusion model is amortized with respect to the observations in the partially observed field, the number and locations of those observations, and the spatial process parameters, and can therefore be used to conditionally simulate from a broad class of predictive distributions without retraining the neural network. We assess the NCS-generated simulations against simulations from the true conditional distribution of a Gaussian process model, and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick process model for spatial extremes. In the latter case, we show that it is more efficient and accurate to conditionally simulate using NCS than classical MCMC techniques implemented in standard software. We conclude that NCS enables efficient and accurate conditional simulation from spatial predictive distributions that are challenging to sample from using traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20067v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Walchessen, Andrew Zammit-Mangion, Rapha\"el Huser, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models</title>
      <link>https://arxiv.org/abs/2508.19249</link>
      <description>arXiv:2508.19249v1 Announce Type: cross 
Abstract: We present a new efficient hybrid parameter estimation method based on the idea, that if nonlinear dynamic models are stated in terms of a system of equations that is linear in terms of the parameters, then regularized ordinary least squares can be used to estimate these parameters from time series data. We introduce the term "Physics-Informed Regression" (PIR) to describe the proposed data-driven hybrid technique as a way to bridge theory and data by use of ordinary least squares to efficiently perform parameter estimation of the model coefficients of different parameter-linear models; providing examples of models based on nonlinear ordinary equations (ODE) and partial differential equations (PDE). The focus is on parameter estimation on a selection of ODE and PDE models, each illustrating performance in different model characteristics. For two relevant epidemic models of different complexity and number of parameters, PIR is tested and compared against the related technique, physics-informed neural networks (PINN), both on synthetic data generated from known target parameters and on real public Danish time series data collected during the COVID-19 pandemic in Denmark. Both methods were able to estimate the target parameters, while PIR showed to perform noticeably better, especially on a compartment model with higher complexity. Given the difference in computational speed, it is concluded that the PIR method is superior to PINN for the models considered. It is also demonstrated how PIR can be applied to estimate the time-varying parameters of a compartment model that is fitted using real Danish data from the COVID-19 pandemic obtained during a period from 2020 to 2021. The study shows how data-driven and physics-informed techniques may support reliable and fast -- possibly real-time -- parameter estimation in parameter-linear nonlinear dynamic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19249v1</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas S{\o}eborg Nielsen (DTU Compute), Marcus Galea Jacobsen (DTU Compute), Albert Brincker Olson (DTU Compute), Mads Peter S{\o}rensen (DTU Compute), Allan Peter Engsig-Karup (DTU Compute)</dc:creator>
    </item>
    <item>
      <title>Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models</title>
      <link>https://arxiv.org/abs/2508.19441</link>
      <description>arXiv:2508.19441v1 Announce Type: cross 
Abstract: Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long time integration of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local "stencil" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented synthetic stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19441v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Jantre, Deepak Akhare, Xiaoning Qian, Nathan M. Urban</dc:creator>
    </item>
    <item>
      <title>Simultaneous Detection and Localization of Mean and Covariance Changes in High Dimensions</title>
      <link>https://arxiv.org/abs/2508.19523</link>
      <description>arXiv:2508.19523v1 Announce Type: cross 
Abstract: Existing methods for high-dimensional changepoint detection and localization typically focus on changes in either the mean vector or the covariance matrix separately. This separation reduces detection power and localization accuracy when both parameters change simultaneously. We propose a simple yet powerful method that jointly monitors shifts in both the mean and covariance structures. Under mild conditions, the test statistics for detecting these shifts jointly converge in distribution to a bivariate standard normal distribution, revealing their asymptotic independence. This independence enables the combination of the individual p-values using Fisher's method, and the development of an adaptive p-value-based estimator for the changepoint. Theoretical analysis and extensive simulations demonstrate the superior performance of our method in terms of both detection power and localization accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19523v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Cui, Guangming Pan, Guanghui Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Optimal Cox regression under federated differential privacy: coefficients and cumulative hazards</title>
      <link>https://arxiv.org/abs/2508.19640</link>
      <description>arXiv:2508.19640v1 Announce Type: cross 
Abstract: We study two foundational problems in distributed survival analysis: estimating Cox regression coefficients and cumulative hazard functions, under federated differential privacy constraints, allowing for heterogeneous per-sever sample sizes and privacy budgets. To quantify the fundamental cost of privacy, we derive minimax lower bounds along with matching (up to poly-logarithmic factors) upper bounds. In particular, to estimate the cumulative hazard function, we design a private tree-based algorithm for nonparametric integral estimation. Our results reveal server-level phase transitions between the private and non-private rates, as well as the reduced estimation accuracy from imposing privacy constraints on distributed subsets of data.
  To address scenarios with partially public information, we also consider a relaxed differential privacy framework and provide a corresponding minimax analysis. To our knowledge, this is the first treatment of partially public data in survival analysis, and it establishes a no-gain in accuracy phenomenon. Finally, we conduct extensive numerical experiments, with an accompanying R package FDPCox, validating our theoretical findings. These experiments also include a fully-interactive algorithm with tighter privacy composition, which demonstrates improved estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19640v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elly K. H. Hung, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A mixture logistic model for panel data with a Markov structure</title>
      <link>https://arxiv.org/abs/2302.01683</link>
      <description>arXiv:2302.01683v3 Announce Type: replace 
Abstract: In this study, we propose a mixture logistic regression model with a Markov structure, and consider the estimation of model parameters using maximum likelihood estimation. We also provide a forward type variable selection algorithm to choose the important explanatory variables to reduce the number of parameters in the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01683v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu-Hsiang Cheng, Tzee-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Structure Learning for Gaussian Graphical Models Using Marginal Pseudo-likelihood</title>
      <link>https://arxiv.org/abs/2307.00127</link>
      <description>arXiv:2307.00127v4 Announce Type: replace 
Abstract: Bayesian methods for learning Gaussian graphical models offer a principled framework for quantifying model uncertainty and incorporating prior knowledge. However, their scalability is constrained by the computational cost of jointly exploring graph structures and precision matrices. To address this challenge, we perform inference directly on the graph by integrating out the precision matrix. We adopt a marginal pseudo-likelihood approach, eliminating the need to compute intractable normalizing constants and perform computationally intensive precision matrix sampling. Building on this framework, we develop continuous-time (birth-death) and discrete-time (reversible jump) Markov chain Monte Carlo (MCMC) algorithms that efficiently explore the posterior over graph space. We establish theoretical guarantees for posterior contraction, convergence, and graph selection consistency. The algorithms scale to large graph spaces, enabling parallel exploration for graphs with over 1,000 nodes, while providing uncertainty quantification and supporting flexible prior specification over the graph space. Extensive simulations show substantial computational gains over state-of-the-art Bayesian approaches without sacrificing graph recovery accuracy. Applications to human and mouse gene expression datasets demonstrate the ability of our approach to recover biologically meaningful structures and quantify uncertainty in complex networks. An implementation is available in the R package BDgraph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00127v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Mohammadi, Marit Schoonhoven, Lucas Vogels, S. Ilker Birbil</dc:creator>
    </item>
    <item>
      <title>The Bayesian Context Trees State Space Model for time series modelling and forecasting</title>
      <link>https://arxiv.org/abs/2308.00913</link>
      <description>arXiv:2308.00913v3 Announce Type: replace 
Abstract: A hierarchical Bayesian framework is introduced for developing tree-based mixture models for time series, partly motivated by applications in finance and forecasting. At the top level, meaningful discrete states are identified as appropriately quantised values of some of the most recent samples. At the bottom level, a different, arbitrary base model is associated with each state. This defines a very general framework that can be used in conjunction with any existing model class to build flexible and interpretable mixture models. We call this the Bayesian Context Trees State Space Model, or the BCT-X framework. Appropriate algorithmic tools are described, which allow for effective and efficient Bayesian inference and learning; these algorithms can be updated sequentially, facilitating online forecasting. The utility of the general framework is illustrated in the particular instances when AR or ARCH models are used as base models. The latter results in a mixture model that offers a powerful way of modelling the well-known volatility asymmetries in financial data, revealing a novel, important feature of stock market index data, in the form of an enhanced leverage effect. In forecasting, the BCT-X methods are found to outperform several state-of-the-art techniques, both in terms of accuracy and computational requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00913v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioannis Papageorgiou, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>Empirically assessing the plausibility of unconfoundedness in observational studies</title>
      <link>https://arxiv.org/abs/2402.10156</link>
      <description>arXiv:2402.10156v2 Announce Type: replace 
Abstract: The likelihood of unmeasured confounding is one of the main limitations for causal inference from observational studies. There are different methods for (partially) empirically assessing the plausibility of unconfoundedness. However, most currently available methods require (at least partial) assumptions about the confounding structure, which may be difficult to know in practice. In this paper we describe a simple strategy for empirically assessing the plausibility of conditional unconfoundedness (i.e., whether the candidate adjustment set of covariates suffices for confounding adjustment) which does not require any explicit assumptions about the confounding structure, relying instead on assumptions related to temporal ordering between covariates, exposure and outcome (which can be guaranteed by design) and selection into the study. The proposed method essentially relies on testing the association between a subset of the covariates included in the adjustment set (those associated with the exposure, given all other covariates) and the outcome conditional on the remaining covariates and the exposure. We describe the assumptions underlying the method, provide proofs, use simulations to corroborate the theory and illustrate the method with an applied example assessing the causal effect of delivery mode and intelligence quotient measured in adulthood using data from the 1982 Pelotas (Brazil) birth cohort. We also discuss the implications of measurement error and some important limitations of the suggested approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10156v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Pires Hartwig, Kate Tilling, George Davey Smith</dc:creator>
    </item>
    <item>
      <title>Inspection-Guided Randomization: A Flexible and Transparent Restricted Randomization Framework for Better Experimental Design</title>
      <link>https://arxiv.org/abs/2408.14669</link>
      <description>arXiv:2408.14669v2 Announce Type: replace 
Abstract: Randomized experiments are considered the gold standard for estimating causal effects. However, out of the set of possible randomized assignments, some may be likely to produce poor effect estimates and misleading conclusions. Restricted randomization is an experimental design strategy that filters out undesirable treatment assignments, but its application has primarily been limited to ensuring covariate balance in two-arm studies where the target estimand is the average treatment effect. Other experimental settings with different design desiderata and target effect estimands could also stand to benefit from a restricted randomization approach. We introduce Inspection-Guided Randomization (IGR), a transparent and flexible framework for restricted randomization that filters out undesirable treatment assignments by inspecting assignments against analyst-specified, domain-informed design desiderata. In IGR, the acceptable treatment assignments are locked in ex ante and pre-registered in the trial protocol, thus safeguarding against $p$-hacking and promoting reproducibility. Through illustrative simulation studies motivated by education and behavioral health interventions, we demonstrate how IGR can be used to improve effect estimates compared to benchmark designs in group formation experiments and experiments with interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14669v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3102/10769986251342292</arxiv:DOI>
      <dc:creator>Maggie Wang, Ren\'e F. Kizilcec, Michael Baiocchi</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing for predictive inference under unidentified transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v4 Announce Type: replace 
Abstract: Reliable Bayesian predictive inference has long been an open problem under unidentified transformation models, since the Markov Chain Monte Carlo (MCMC) chains of posterior predictive distribution (PPD) values are generally poorly mixed. We address the poorly mixed PPD value chains under unidentified transformation models through an adaptive scheme for prior adjustment. Specifically, we originate a conception of sufficient informativeness, which explicitly quantifies the information level provided by nonparametric priors, and assesses MCMC mixing by comparison with the within-chain MCMC variance. We formulate the prior information level by a set of hyperparameters induced from the nonparametric prior elicitation with an analytic expression, which is guaranteed by asymptotic theory for the posterior variance under unidentified transformation models. The analytic prior information level consequently drives a hyperparameter tuning procedure to achieve MCMC mixing. The proposed method is general enough to cover various data domains through a multiplicative error working model. Comprehensive simulations and real-world data analysis demonstrate that our method successfully achieves MCMC mixing and outperforms state-of-the-art competitors in predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Zhaohai Li, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Watermarks for Large Language Models Under Human Edits</title>
      <link>https://arxiv.org/abs/2411.13868</link>
      <description>arXiv:2411.13868v3 Announce Type: replace 
Abstract: Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13868v3</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Efficient Analysis of Latent Spaces in Heterogeneous Networks</title>
      <link>https://arxiv.org/abs/2412.02151</link>
      <description>arXiv:2412.02151v3 Announce Type: replace 
Abstract: This work proposes a unified framework for efficient estimation under latent space modeling of heterogeneous networks. We consider a class of latent space models that decompose latent vectors into shared and network-specific components across networks. We develop a novel procedure that first identifies the shared latent vectors and further refines estimates through efficient score equations to achieve statistical efficiency. Oracle error rates for estimating the shared and heterogeneous latent vectors are established simultaneously. The analysis framework offers remarkable flexibility, accommodating various types of edge weights under general distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02151v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Tian, Jiajin Sun, Yinqiu He</dc:creator>
    </item>
    <item>
      <title>SMART-MC: Characterizing the Dynamics of Multiple Sclerosis Therapy Transitions Using a Covariate-Based Markov Model</title>
      <link>https://arxiv.org/abs/2412.03596</link>
      <description>arXiv:2412.03596v3 Announce Type: replace 
Abstract: Treatment switching is a common occurrence in the management of Multiple Sclerosis (MS), where patients transition across various disease-modifying therapies (DMTs) due to heterogeneous treatment responses, differences in disease progression, patient characteristics, and therapy-associated adverse effects. To investigate how patient-level covariates influence the likelihood of treatment transitions among DMTs, we adopt a Markovian framework, Sparse Matrix Estimation with Covariate-Based Transitions in Markov Chain Modeling (SMART-MC), in which the transition probabilities are modeled as functions of these covariates. Modeling real-world treatment transitions under this framework presents several challenges, including ensuring parameter identifiability and handling sparse transitions without overfitting. To address identifiability, we constrain each transition-specific covariate coefficient vectors to have a fixed L2 norm. Furthermore, our method automatically estimates transition probabilities for sparsely observed transitions as constants and enforces zero transition probabilities for transitions that are empirically unobserved. This approach mitigates the need for additional model complexity to handle sparsity while maintaining interpretability and efficiency. To optimize the multi-modal likelihood function, we develop a scalable, parallelized global optimization routine, which is validated through benchmark comparisons and supported by key theoretical properties. Our analysis uncovers meaningful patterns in DMT transitions, revealing variations across MS patient subgroups defined by age, race, and other clinical factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03596v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomchang Kim, Zongqi Xia, Priyam Das</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Graphical Models with Skew-Elliptical Distributions</title>
      <link>https://arxiv.org/abs/2501.08033</link>
      <description>arXiv:2501.08033v2 Announce Type: replace 
Abstract: We propose semiparametric estimators called elliptical skew-(S)KEPTIC for efficiently and robustly estimating non-Gaussian graphical models, relaxing the assumption of semiparametric elliptical distributions to the family of meta skew-elliptical, which accommodates a skewness component. Theoretically, we demonstrate that the elliptical skew-(S)KEPTIC estimators achieve robust convergence rates in both graph recovery and parameters estimation. We conduct numerical simulations that prove the reliable graph recovery performance of the elliptical skew-(S)KEPTIC estimators. Finally, the new method is applied to the daily log-returns of the stocks of the S\&amp;P500 index and shows higher sparsity compared to the Gaussian copula graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08033v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Di Luzio, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate Markers Applied to Vaccinology</title>
      <link>https://arxiv.org/abs/2502.03030</link>
      <description>arXiv:2502.03030v3 Announce Type: replace 
Abstract: In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralising antibody response. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03030v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Hughes, Layla Parast, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges through enriched validation and targeted sampling to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v5 Announce Type: replace 
Abstract: The allostatic load index (ALI) is a 10-component measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in learning health systems; however, these data are prone to missingness and errors. Validation (e.g., through chart reviews) provides better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of ALI and healthcare utilization. Employing semiparametric maximum likelihood estimation, we robustly incorporate all available patient information into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote EHR data quality and completeness. Chart reviews uncovered few errors (99% matched source documents) and recovered some missing data through auxiliary information in patients' charts. On average, validation increased the number of non-missing ALI components per patient from 6 to 7. Through simulations based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Incorporating validation data, statistical models indicated that worse whole-person health (higher ALI) was associated with higher odds of engaging in the healthcare system, adjusting for age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>Graphical Transformation Models</title>
      <link>https://arxiv.org/abs/2503.17845</link>
      <description>arXiv:2503.17845v4 Announce Type: replace 
Abstract: Graphical Transformation Models (GTMs) are introduced as a novel approach to effectively model multivariate data with intricate marginals and complex dependency structures semiparametrically, while maintaining interpretability through the identification of varying conditional independencies. GTMs extend multivariate transformation models by replacing the Gaussian copula with a custom-designed multivariate transformation, offering two major advantages. Firstly, GTMs can capture more complex interdependencies using penalized splines, which also provide an efficient regularization scheme. Secondly, we demonstrate how to approximately regularize GTMs towards pairwise conditional independencies using a lasso penalty, akin to Gaussian graphical models. The model's robustness and effectiveness are validated through simulations, showcasing its ability to accurately learn complex dependencies and identify conditional independencies. Additionally, the model is applied to a benchmark astrophysics dataset, where the GTM demonstrates favorable performance compared to non-parametric vine copulas in learning complex multivariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17845v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Herp, Johannes Brachem, Michael Altenbuchinger, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>Network Modeling of Asynchronous Change-Points in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.15801</link>
      <description>arXiv:2506.15801v2 Announce Type: replace 
Abstract: This article introduces a novel Bayesian method for asynchronous change-point detection in multivariate time series. This method allows for change-points to occur earlier in some (leading) series followed, after a short delay, by change-points in some other (lagging) series. Such dynamic dependence structure is common in fields such as seismology and neurology where a latent event such as an earthquake or seizure causes certain sensors to register change-points before others. We model these lead-lag dependencies via a latent directed graph and provide a hierarchical prior for learning the graph's structure and parameters. Posterior inference is made tractable by modifying particle MCMC methods designed for univariate change-point problems. We apply our method to both simulated and real datasets from the fields of seismology and neurology. In the simulated data, we find that our method outperforms competing methods in settings where the change-point locations are dependent across series. In the real data applications we show that our model can also uncover interpretable network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15801v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carson McKee, Maria Kalli</dc:creator>
    </item>
    <item>
      <title>Marginal and conditional summary measures: transportability and compatibility across studies</title>
      <link>https://arxiv.org/abs/2507.21925</link>
      <description>arXiv:2507.21925v3 Announce Type: replace 
Abstract: Marginal and conditional summary measures do not generally coincide, have different interpretations and correspond to different decision questions. While these aspects have primarily been recognized for non-collapsible summary measures, they are equally problematic for some collapsible measures in the presence of effect modification. We clarify the interpretation and properties of several marginal and conditional summary measures, considering different types of outcomes and hypothetical outcome-generating mechanisms. We describe implications of the choice of summary measure for transportability, highlighting that covariates not conventionally described as effect modifiers can modify population-level treatment effects. Finally, we illustrate existing summary measure incompatibility issues in the context of evidence synthesis, using the case of covariate adjustment methods for indirect treatment comparisons. Because marginal and conditional summary measures do not generally coincide, their na\"ive pooling in evidence synthesis can produce bias. Almost invariably, care is needed to ensure that evidence synthesis methods are combining compatible summary measures, and this may be easier to accomplish with full access to individual patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21925v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Remiro-Az\'ocar, David M. Phillippo, Nicky J. Welton, Sofia Dias, A. E. Ades, Anna Heath, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>The purpose of an estimator is what it does: Misspecification, estimands, and over-identification</title>
      <link>https://arxiv.org/abs/2508.13076</link>
      <description>arXiv:2508.13076v3 Announce Type: replace-cross 
Abstract: In over-identified models, misspecification -- the norm rather than exception -- fundamentally changes what estimators estimate. Different estimators imply different estimands rather than different efficiency for the same target. A review of recent applications of generalized method of moments in the American Economic Review suggests widespread acceptance of this fact: There is little formal specification testing and widespread use of estimators that would be inefficient were the model correct, including the use of "hand-selected" moments and weighting matrices. Motivated by these observations, we review and synthesize recent results on estimation under model misspecification, providing guidelines for transparent and robust empirical research. We also provide a new theoretical result, showing that Hansen's J-statistic measures, asymptotically, the range of estimates achievable at a given standard error. Given the widespread use of inefficient estimators and the resulting researcher degrees of freedom, we thus particularly recommend the broader reporting of J-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13076v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Jiafeng Chen, Otavio Tecchio</dc:creator>
    </item>
  </channel>
</rss>
