<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Jul 2025 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference with weights: Residualization produces short, valid intervals for varying estimands and varying resampling processes</title>
      <link>https://arxiv.org/abs/2507.19607</link>
      <description>arXiv:2507.19607v1 Announce Type: new 
Abstract: Weighting procedures are used in observational causal inference to adjust for covariate imbalance within the sample. Common practice for inference is to estimate robust standard errors from a weighted regression of outcome on treatment. However, it is well known that weighting can inflate variance estimates, sometimes significantly, leading to standard errors and confidence intervals that are overly conservative. We instead examine and recommend the use of robust standard errors from a weighted regression that additionally includes the balancing covariates and their interactions with treatment. We show that these standard errors are more precise and asymptotically correct for weights that achieve exact balance under multiple common resampling frameworks, including design-based and model-based inference, as well as superpopulation sampling with a finite sample correction. Gains to precision can be quite significant when the balancing weights adjust for prognostic covariates. For procedures that balance only approximately or in expectation, such as inverse propensity weighting or approximate balancing weights, our proposed method improves precision by reducing residuals through augmentation with the parametric model. We demonstrate our approach through simulation and re-analysis of multiple empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19607v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin Hartman, Chad Hazlett, Arisa Sadeghpour</dc:creator>
    </item>
    <item>
      <title>Adaptive Proximal Causal Inference with Some Invalid Proxies</title>
      <link>https://arxiv.org/abs/2507.19623</link>
      <description>arXiv:2507.19623v1 Announce Type: new 
Abstract: Proximal causal inference (PCI) is a recently proposed framework to identify and estimate the causal effect of an exposure on an outcome in the presence of hidden confounders, using observed proxies. Specifically, PCI relies on two types of proxies: a treatment-inducing confounding proxy, related to the outcome only through its association with unmeasured confounders (given treatment and covariates), and an outcome-inducing confounding proxy, related to the treatment only through such association (given covariates). These proxies must satisfy stringent exclusion restrictions - namely, the treatment proxy must not affect the outcome, and the outcome proxy must not be affected by the treatment. To improve identification and potentially efficiency, multiple proxies are often used, raising concerns about bias from exclusion violations. To address this, we introduce necessary and sufficient conditions for identifying causal effects in the presence of many proxies, some potentially invalid. Under a canonical proximal linear structural equations model, we propose a LASSO-based median estimator that jointly selects valid proxies and estimates the causal effect, with theoretical guarantees. Recognizing LASSO's limitations in consistently selecting valid treatment proxies, we develop an adaptive LASSO-based estimator with differential penalization. We show that it is root-n consistent and yields valid confidence intervals when a valid outcome proxy is available. We also extend the approach to settings with many potentially invalid outcome proxies. Theoretical results are supported by simulations and an application assessing the effect of right heart catheterization on 30-day survival in ICU patient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19623v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prabrisha Rakshit, Xu Shi, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A direct approach to tree-guided feature aggregation for high-dimensional regression</title>
      <link>https://arxiv.org/abs/2507.19650</link>
      <description>arXiv:2507.19650v1 Announce Type: new 
Abstract: In high-dimensional linear models, sparsity is often exploited to reduce variability and achieve parsimony. Equi-sparsity, where one assumes that predictors can be aggregated into groups sharing the same effects, is an alternative parsimonious structure that can be more suitable in certain applications. Previous work has clearly demonstrated the benefits of exploiting equi-sparsity in the presence of ``rare features'' (Yan and Bien 2021). In this work, we propose a new tree-guided regularization scheme for simultaneous estimation and feature aggregation. Unlike existing methods, our estimator avoids synthetic overparameterization and its detrimental effects. Even though our penalty is applied to hierarchically overlapped groups, we show that its proximal operator can be solved with a one-pass, non-iterative algorithm. Novel techniques are developed to study the finite-sample error bound of this seminorm-induced regularizer under least squares and binomial deviance losses. Theoretically, compared to existing methods, the proposed method offers a faster or equivalent rate depending on the true equi-sparisty structure. Extensive simulation studies verify these findings. Finally, we illustrate the usefulness of the proposed method with an application to a microbiome dataset, where we conduct post-selection inference on the aggregated features' effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19650v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinwen Fu, Aaron J. Molstad, Hui Zou</dc:creator>
    </item>
    <item>
      <title>A Comparison of the Bayesian Posterior Probability and the Frequentist $p$-Value in Testing Equivalence Hypotheses</title>
      <link>https://arxiv.org/abs/2507.19685</link>
      <description>arXiv:2507.19685v1 Announce Type: new 
Abstract: Equivalence tests, otherwise known as parity or similarity tests, are frequently used in ``bioequivalence studies" to establish practical equivalence rather than the usual statistical significant difference. In this article, we propose an equivalence test using both the $p$-value and a Bayesian procedure by computing the posterior probability that the null hypothesis is true. Since these posterior probabilities follow the uniform $[0,1]$ distribution under the null hypothesis, we use them in a Two One-Sided Test (TOST) procedure to perform equivalence tests. For certain specifications of the prior parameters, test based on these posterior probabilities are more powerful and less conservative than those based on the $p$-value. We compare the parameter values that maximize the power functions of tests based on these two measures of evidence when using different equivalence margins. We also derive the correlation coefficient between these two measures of evidence. Furthermore, we also consider the effect of the prior variance on the conservativity and power function of the test based on the posterior probabilities. Finally, we provide examples and a small-scale simulation study to compare their performance in terms of type I error rate control and power in a single test, as well as in multiple testing, considering the power of the false discovery rate procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19685v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Ochieng</dc:creator>
    </item>
    <item>
      <title>Location Tests with Noisy Proxies for Latent Variables</title>
      <link>https://arxiv.org/abs/2507.19696</link>
      <description>arXiv:2507.19696v1 Announce Type: new 
Abstract: We investigate inference in a latent binary variable model where a noisy proxy of the latent variable is available, motivated by the variable perturbation effectiveness problem in single-cell CRISPR screens. The baseline approach is to ignore the perturbation effectiveness problem, while a recent proposal employs a weighted average based on the proxies. Our main goals are to determine how accurate the proxies must be in order for a weighted test to gain power over the unweighted baseline, and to develop tests that are powerful regardless of the accuracy of the proxies. To address the first goal, we compute the Pitman relative efficiency of the weighted test relative to the unweighted test, yielding an interpretable quantification of proxy quality that drives the power of the weighted test. To address the second goal, we propose two strategies. First, we propose a maximum-likelihood based approach that adapts the proxies to the data. Second, we propose an estimator of the Pitman efficiency if a "positive control outcome variable" is available (as is often the case in single-cell CRISPR screens), which facilitates an adaptive choice of whether to use the proxies at all. Our numerical simulations support the Pitman efficiency as the key quantity for determining whether the weighted test gains power over the baseline, and demonstrate that the two proposed adaptive tests can improve on both existing approaches across a range of proxy qualities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19696v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Deutsch, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Temporal network analysis via a degree-corrected Cox model</title>
      <link>https://arxiv.org/abs/2507.19868</link>
      <description>arXiv:2507.19868v1 Announce Type: new 
Abstract: Temporal dynamics, characterised by time-varying degree heterogeneity and homophily effects, are often exhibited in many real-world networks. As observed in an MIT Social Evolution study, the in-degree and out-degree of the nodes show considerable heterogeneity that varies with time. Concurrently, homophily effects, which explain why nodes with similar characteristics are more likely to connect with each other, are also time-dependent. To facilitate the exploration and understanding of these dynamics, we propose a novel degree-corrected Cox model for directed networks, where the way for degree-heterogeneity or homophily effects to change with time is left completely unspecified. Because each node has individual-specific in- and out-degree parameters that vary over time, the number of unknown parameters grows with the number of nodes, leading to a high-dimensional estimation problem. Therefore, it is highly nontrivial to make inference. We develop a local estimating equations approach to estimate the unknown parameters and establish the consistency and asymptotic normality of the proposed estimators in the high-dimensional regime. We further propose test statistics to check whether temporal variation or degree heterogeneity is present in the network and develop a graphically diagnostic method to evaluate goodness-of-fit for dynamic network models. Simulation studies and two real data analyses are provided to assess the finite sample performance of the proposed method and illustrate its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19868v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuguo Chen, Lianqiang Qu, Jinfeng Xu, Ting Yan, Yunpeng Zhou</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Circular Data</title>
      <link>https://arxiv.org/abs/2507.19889</link>
      <description>arXiv:2507.19889v1 Announce Type: new 
Abstract: In causal inference, a fundamental task is to estimate the effect resulting from a specific treatment, which is often handled with inverse probability weighting. Despite an abundance of attention to the advancement of this task, most articles have focused on linear data rather than circular data, which are measured in angles. In this article, we extend the causal inference framework to accommodate circular data. Specifically, two new treatment effects, average direction treatment effect (ADTE) and average length treatment effect (ALTE), are introduced to offer a proper causal explanation for these data. As the average direction and average length describe the location and concentration of a random sample of circular data, the ADTE and ALTE measure the change in direction and length between two counterfactual outcomes. With inverse probability weighting, we propose estimators that exhibit ideal theoretical properties, which are validated by a simulation study. To illustrate the practical utility of our estimator, we analyze the effect of different job types on dispatchers' sleep patterns using data from Federal Railroad Administration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19889v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan-Hsun Wu</dc:creator>
    </item>
    <item>
      <title>Retrospective score tests versus prospective score tests for genetic association with case-control data</title>
      <link>https://arxiv.org/abs/2507.19893</link>
      <description>arXiv:2507.19893v1 Announce Type: new 
Abstract: Since the seminal work by Prentice and Pyke (1979), the prospective logistic likelihood has become the standard method of analysis for retrospectively collected case-control data, in particular for testing the association between a single genetic marker and a disease outcome in genetic case-control studies. When studying multiple genetic markers with relatively small effects, especially those with rare variants, various aggregated approaches based on the same prospective likelihood have been developed to integrate subtle association evidence among all considered markers. In this paper we show that using the score statistic derived from a prospective likelihood is not optimal in the analysis of retrospectively sampled genetic data. We develop the locally most powerful genetic aggregation test derived through the retrospective likelihood under a random effect model assumption. In contrast to the fact that the disease prevalence information cannot be used to improve the efficiency for the estimation of odds ratio parameters in logistic regression models, we show that it can be utilized to enhance the testing power in genetic association studies. Extensive simulations demonstrate the advantages of the proposed method over the existing ones. One real genome-wide association study is analyzed for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19893v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13270</arxiv:DOI>
      <arxiv:journal_reference>Liu Y., Li P., Song L., Yu K., Qin J. (2021) Retrospective score tests versus prospective score tests for genetic association with case-control data. Biometrics, 77, 102-112</arxiv:journal_reference>
      <dc:creator>Yukun Liu, Pengfei Li, Lei Song, Kai Yu, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Effective Bayesian Modeling of Large Spatiotemporal Count Data Using Autoregressive Gamma Processes</title>
      <link>https://arxiv.org/abs/2507.19915</link>
      <description>arXiv:2507.19915v1 Announce Type: new 
Abstract: We put forward a new Bayesian modeling strategy for spatiotemporal count data that enables efficient posterior sampling. Most previous models for such data decompose logarithms of the response Poisson rates into fixed effects and spatial random effects, where the latter is typically assumed to follow a latent Gaussian process, the conditional autoregressive model, or the intrinsic conditional autoregressive model. Since log-Gaussian is not conjugate to Poisson, such implementations must resort to either approximation methods like INLA or Metropolis moves on latent states in MCMC algorithms for model fitting and exhibit several approximation and posterior sampling challenges. Instead of modeling logarithms of spatiotemporal frailties jointly as a Gaussian process, we construct a spatiotemporal autoregressive gamma process guaranteed stationary across the time dimension. We decompose latent Poisson variables to permit fully conjugate Gibbs sampling of spatiotemporal frailties and design a sparse spatial dependence structure to get a linear computational complexity that facilitates efficient posterior computation. Our model permits convenient Bayesian predictive machinery based on posterior samples that delivers satisfactory performance in predicting at new spatial locations and time intervals. We have performed extensive simulation experiments and real data analyses, which corroborated our model's accurate parameter estimation, model fitting, and out-of-sample prediction capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19915v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cheng, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Discrete Gaussian Vector Fields On Meshes</title>
      <link>https://arxiv.org/abs/2507.20024</link>
      <description>arXiv:2507.20024v1 Announce Type: new 
Abstract: Though the underlying fields associated with vector-valued environmental data are continuous, observations themselves are discrete. For example, climate models typically output grid-based representations of wind fields or ocean currents, and these are often downscaled to a discrete set of points. By treating the area of interest as a two-dimensional manifold that can be represented as a triangular mesh and embedded in Euclidean space, this work shows that discrete intrinsic Gaussian processes for vector-valued data can be developed from discrete differential operators defined with respect to a mesh. These Gaussian processes account for the geometry and curvature of the manifold whilst also providing a flexible and practical formulation that can be readily applied to any two-dimensional mesh. We show that these models can capture harmonic flows, incorporate boundary conditions, and model non-stationary data. Finally, we apply these models to downscaling stationary and non-stationary gridded wind data on the globe, and to inference of ocean currents from sparse observations in bounded domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20024v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Gillan (University of Exeter), Stefan Siegert (University of Exeter), Ben Youngman (University of Exeter)</dc:creator>
    </item>
    <item>
      <title>Lasso Penalization for High-Dimensional Beta Regression Models: Computation, Analysis, and Inference</title>
      <link>https://arxiv.org/abs/2507.20079</link>
      <description>arXiv:2507.20079v1 Announce Type: new 
Abstract: Beta regression is commonly employed when the outcome variable is a proportion. Since its conception, the approach has been widely used in applications spanning various scientific fields. A series of extensions have been proposed over time, several of which address variable selection and penalized estimation, e.g., with an $\ell_1$-penalty (LASSO). However, a theoretical analysis of this popular approach in the context of Beta regression with high-dimensional predictors is lacking. In this paper, we aim to close this gap. A particular challenge arises from the non-convexity of the associated negative log-likelihood, which we address by resorting to a framework for analyzing stationary points in a neighborhood of the target parameter. Leveraging this framework, we derive a non-asymptotic bound on the $\ell_1$-error of such stationary points. In addition, we propose a debiasing approach to construct confidence intervals for the regression parameters. A proximal gradient algorithm is devised for optimizing the resulting penalized negative log-likelihood function. Our theoretical analysis is corroborated via simulation studies, and a real data example concerning the prediction of county-level proportions of incarceration is presented to showcase the practical utility of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20079v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niloofar Ramezani, Martin Slawski</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixed-Effects Models for Multilevel Two-way Functional Data: Applications to EEG Experiments</title>
      <link>https://arxiv.org/abs/2507.20092</link>
      <description>arXiv:2507.20092v1 Announce Type: new 
Abstract: In multi-condition EEG experiments, brain activity is recorded as subjects perform various tasks or are exposed to different stimuli. The recorded signals are commonly transformed into time-frequency representations, which often display smooth variations across time and frequency dimensions. These representations are naturally structured as two-way functional data, with experimental conditions nested within subjects. Existing analytical methods fail to jointly account for the data's multilevel structure, functional nature, and dependence on subject-level covariates. To address these limitations, we propose a Bayesian mixed-effects model for two-way functional data that incorporates covariate-dependent fixed effects at the condition level and multilevel random effects. For enhanced model interpretability and parsimony, we introduce a novel covariate-dependent CANDECOMP/PARAFAC (CP) decomposition for the fixed effects, with marginally interpretable time and frequency patterns. We further propose a sparsity-inducing prior for CP rank selection and an efficient algorithm for posterior sampling. The proposed method is evaluated through extensive simulations and applied to EEG data collected to investigate the effects of alcoholism on cognitive processing in response to visual stimuli. Our analysis reveals distinct patterns of time-frequency activity associated with alcoholism, offering new insights into the neural processing differences between subject groups and experimental conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20092v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaomeng Ju, Thaddeus Tarpey, Hyung G Park</dc:creator>
    </item>
    <item>
      <title>A Markov switching discrete-time Hawkes process: application to the monitoring of bats behavior</title>
      <link>https://arxiv.org/abs/2507.20153</link>
      <description>arXiv:2507.20153v1 Announce Type: new 
Abstract: Over the past few decades, the Hawkes process has become a popular framework for modeling temporal events thanks to its flexibility to capture different dependency structures. The objective of this work is to model call sequences emitted by bats for echolocation, whose patterns are known to change depending on the animal's activity. The novelty of the model lies in the combination of a Hawkes-type dependency from past events, as well as a latent variable that encodes changes in bat behavior. More precisely, we consider a discrete-time version of the Hawkes process, with an exponential kernel, where the immigration term varies according to a latent Markov chain. We prove that this model is identifiable and can be reformulated in terms of a Hidden Markov Model, with Poisson emissions. Based on these properties, we show that maximum likelihood inference of the model parameters can be performed using an EM algorithm, which involves a recursive M-step. A simulation study demonstrates the performance of our approach method for estimating the parameters, recovering the number of hidden states and classifying each bin of the trajectory. Finally, we illustrate the use of the proposed modeling to distinguish different behaviors of bats, based on the recording of their cries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, St\'ephane Robin</dc:creator>
    </item>
    <item>
      <title>Causal Inference when Intervention Units and Outcome Units Differ</title>
      <link>https://arxiv.org/abs/2507.20231</link>
      <description>arXiv:2507.20231v1 Announce Type: new 
Abstract: We study causal inference in settings characterized by interference with a bipartite structure. There are two distinct sets of units: intervention units to which an intervention can be applied and outcome units on which the outcome of interest can be measured. Outcome units may be affected by interventions on some, but not all, intervention units, as captured by a bipartite graph. Examples of this setting can be found in analyses of the impact of pollution abatement in plants on health outcomes for individuals, or the effect of transportation network expansions on regional economic activity. We introduce and discuss a variety of old and new causal estimands for these bipartite settings. We do not impose restrictions on the functional form of the exposure mapping and the potential outcomes, thus allowing for heterogeneity, non-linearity, non-additivity, and potential interactions in treatment effects. We propose unbiased weighting estimators for these estimands from a design-based perspective, based on the knowledge of the bipartite network under general experimental designs. We derive their variance and prove consistency for increasing number of outcome units. Using the Chinese high-speed rail construction study, analyzed in Borusyak and Hull [2023], we discuss non-trivial positivity violations that depend on the estimands, the adopted experimental design, and the structure of the bipartite graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20231v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georgia Papadogeorgou, Zhaoyan Song, Guido Imbens, Fabrizia Mealli</dc:creator>
    </item>
    <item>
      <title>A nonparametric approach to practical identifiability of nonlinear mixed effects models</title>
      <link>https://arxiv.org/abs/2507.20288</link>
      <description>arXiv:2507.20288v1 Announce Type: new 
Abstract: Mathematical modelling is a widely used approach to understand and interpret clinical trial data. This modelling typically involves fitting mechanistic mathematical models to data from individual trial participants. Despite the widespread adoption of this individual-based fitting, it is becoming increasingly common to take a hierarchical approach to parameter estimation, where modellers characterize the population parameter distributions, rather than considering each individual independently. This hierarchical parameter estimation is standard in pharmacometric modelling. However, many of the existing techniques for parameter identifiability do not immediately translate from the individual-based fitting to the hierarchical setting. Here, we propose a nonparametric approach to study practical identifiability within a hierarchical parameter estimation framework. We focus on the commonly used nonlinear mixed effects framework and investigate two well-studied examples from the pharmacometrics and viral dynamics literature to illustrate the potential utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20288v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler Cassidy, Stuart T. Johnston, Michael Plank, Imke Botha, Jennifer A. Flegg, Ryan J. Murphy, Sara Hamis</dc:creator>
    </item>
    <item>
      <title>Clustering data with values missing at random using scale mixtures of multivariate skew-normal distributions</title>
      <link>https://arxiv.org/abs/2507.20329</link>
      <description>arXiv:2507.20329v1 Announce Type: new 
Abstract: Handling missing data is a major challenge in model-based clustering, especially when the data exhibit skewness and heavy tails. We address this by extending the finite mixture of scale mixtures of multivariate skew-normal (FMSMSN) family to accommodate incomplete data under a missing at random (MAR) mechanism. Unlike previous work that is limited to one of the special cases of the FMSMSN family, our method offers a cluster analysis methodology for the entire family that accounts for skewness and excess kurtosis amidst data with missing values. The multivariate skew-normal distribution, as parameterised by \cite{azzalini1996} and \cite{arnoldbeaver} includes the normal distribution as a special case, which ensures that our method is flexible toward existing symmetric model-based clustering techniques under a normality assumption. We derive the distributional properties of the missing components of the data and propose an augmented EM-type algorithm tailored for incomplete observations. The modified E-step yields closed-form expressions for the conditional expectations of the missing values. The simulation experiments showcase the flexibility of the FMSMSN family in both clustering performance and parameter recovery for varying percentages of missing values, while incorporating the effects of sample size and cluster proximity. Finally, we illustrate the practical utility of the proposed method by applying special cases of the FMSMSN family to global CO2 emissions data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20329v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Pillay, Cristina Tortora, Antonio Punzo, Andriette Bekker</dc:creator>
    </item>
    <item>
      <title>Recurrent Event Analysis with Ordinary Differential Equations</title>
      <link>https://arxiv.org/abs/2507.20396</link>
      <description>arXiv:2507.20396v1 Announce Type: new 
Abstract: This paper introduces a general framework for analyzing recurrent event data by modeling the conditional mean function of the recurrent event process as the solution to an Ordinary Differential Equation (ODE). This approach not only accommodates a wide range of semi-parametric recurrent event models, including both non-homogeneous Poisson processes (NHPPs) and non-Poisson processes, but also is scalable and easy-to-implement. Based on this framework, we propose a Sieve Maximum Pseudo-Likelihood Estimation (SMPLE) method, employing the NHPP as a working model. We establish the consistency and asymptotic normality of the proposed estimator, demonstrating that it achieves semi-parametric efficiency when the NHPP working model is valid. Furthermore, we develop an efficient resampling procedure to estimate the asymptotic covariance matrix. To assess the statistical efficiency and computational scalability of the proposed method, we conduct extensive numerical studies, including simulations under various settings and an application to a real-world dataset analyzing risk factors associated with Intensive Care Unit (ICU) readmission frequency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20396v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Meng, Weijing Tang, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Nullstrap-DE: A General Framework for Calibrating FDR and Preserving Power in DE Methods, with Applications to DESeq2 and edgeR</title>
      <link>https://arxiv.org/abs/2507.20598</link>
      <description>arXiv:2507.20598v1 Announce Type: new 
Abstract: Differential expression (DE) analysis is a key task in RNA-seq studies, aiming to identify genes with expression differences across conditions. A central challenge is balancing false discovery rate (FDR) control with statistical power. Parametric methods such as DESeq2 and edgeR achieve high power by modeling gene-level counts using negative binomial distributions and applying empirical Bayes shrinkage. However, these methods may suffer from FDR inflation when model assumptions are mildly violated, especially in large-sample settings. In contrast, non-parametric tests like Wilcoxon offer more robust FDR control but often lack power and do not support covariate adjustment. We propose Nullstrap-DE, a general add-on framework that combines the strengths of both approaches. Designed to augment tools like DESeq2 and edgeR, Nullstrap-DE calibrates FDR while preserving power, without modifying the original method's implementation. It generates synthetic null data from a model fitted under the gene-specific null (no DE), applies the same test statistic to both observed and synthetic data, and derives a threshold that satisfies the target FDR level. We show theoretically that Nullstrap-DE asymptotically controls FDR while maintaining power consistency. Simulations confirm that it achieves reliable FDR control and high power across diverse settings, where DESeq2, edgeR, or Wilcoxon often show inflated FDR or low power. Applications to real datasets show that Nullstrap-DE enhances statistical rigor and identifies biologically meaningful genes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20598v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenxin Jiang, Changhu Wang, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Independence Testing for Mixed Data</title>
      <link>https://arxiv.org/abs/2507.20609</link>
      <description>arXiv:2507.20609v1 Announce Type: new 
Abstract: We consider the problem of testing independence in mixed-type data that combine count variables with positive, absolutely continuous variables. We first introduce two distinct classes of test statistics in the bivariate setting, designed to test independence between the components of a bivariate mixed-type vector. These statistics are then extended to the multivariate context to accommodate: (i) testing independence between vectors of different types and possibly different dimensions, and (ii) testing total independence among all components of vectors with different types. The construction is based on the recently introduced Baringhaus-Gaigall transformation, which characterizes the joint distribution of such data. We establish the asymptotic properties of the resulting tests and, through an extensive power study, demonstrate that the proposed approach is both competitive and flexible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20609v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dana Bucalo Jeli\'c, Marija Cupari\'c, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Permutation Tests Based on the Copula-Graphic Estimator and Their Use for Survival Tree Construction</title>
      <link>https://arxiv.org/abs/2507.20799</link>
      <description>arXiv:2507.20799v1 Announce Type: new 
Abstract: Survival trees are popular alternatives to Cox or Aalen regression models that offer both modelling flexibility and graphical interpretability. This paper introduces a new algorithm for survival trees that relaxes the assumption of independent censoring. To this end, we use the copula-graphic estimator to estimate survival functions. This allows us to flexibly specify shape and strength of the dependence of survival and censoring times within survival trees. For splitting, we present a permutation test for the null hypothesis of equal survival. Our test statistic consists of the integrated absolute distance of the group's copula-graphic estimators. A first simulation study shows a good type I error and power behavior of the new test. We thereby asses simulation settings of various group sizes, censoring percentages and grades of dependence generated by Clayton and Frank copulas. Using this test as splitting criterion, a second simulation study studies the performance of the resulting trees and compares it with that of the usual logrank-based tree. Lastly, the tree algorithm is applied to real-world clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20799v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pauline Baur, Markus Pauly, Takeshi Emura</dc:creator>
    </item>
    <item>
      <title>A multivariate spatial model for ordinal survey-based data</title>
      <link>https://arxiv.org/abs/2507.20944</link>
      <description>arXiv:2507.20944v1 Announce Type: new 
Abstract: Health surveys provide valuable information for monitoring population health, identifying risk factors and informing public health policies. Most of the questions included are coded as ordinal variables and organized into thematic blocks. Accordingly, multivariate modeling provides a natural framework for considering these variables as true groups, thereby accounting for potential dependencies among the responses within each block. In this paper, we propose a multivariate spatial analysis of ordinal survey-based data. This multivariate approach enables the joint analysis of sets of ordinal responses that are likely to be correlated, accounting for individual-level effects, while simultaneously improving the estimation of the geographical patterns for each variable and capturing their interdependencies. We apply this methodology to describe the spatial distribution of several mental health indicators from the Health Survey of the Region of Valencia (Spain) for the year 2022. Specifically, we analyze the block of questions from the 12-item General Health Questionnaire included in the survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20944v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel \'Angel Beltr\'an-S\'anchez, Miguel \'Angel Mart\'inez-Beneito, Ana Corber\'an-Vallet</dc:creator>
    </item>
    <item>
      <title>A safety governor for learning explicit MPC controllers from data</title>
      <link>https://arxiv.org/abs/2507.19531</link>
      <description>arXiv:2507.19531v1 Announce Type: cross 
Abstract: We tackle neural networks (NNs) to approximate model predictive control (MPC) laws. We propose a novel learning-based explicit MPC structure, which is reformulated into a dual-mode scheme over maximal constrained feasible set. The scheme ensuring the learning-based explicit MPC reduces to linear feedback control while entering the neighborhood of origin. We construct a safety governor to ensure that learning-based explicit MPC satisfies all the state and input constraints. Compare to the existing approach, our approach is computationally easier to implement even in high-dimensional system. The proof of recursive feasibility for the safety governor is given. Our approach is demonstrated on numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19531v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjie Mao, Zheming Wang, Hao Gu, Bo Chen, Li Yu</dc:creator>
    </item>
    <item>
      <title>Uniform Critical Values for Likelihood Ratio Tests in Boundary Problems</title>
      <link>https://arxiv.org/abs/2507.19603</link>
      <description>arXiv:2507.19603v1 Announce Type: cross 
Abstract: Limit distributions of likelihood ratio statistics are well-known to be discontinuous in the presence of nuisance parameters at the boundary of the parameter space, which lead to size distortions when standard critical values are used for testing. In this paper, we propose a new and simple way of constructing critical values that yields uniformly correct asymptotic size, regardless of whether nuisance parameters are at, near or far from the boundary of the parameter space. Importantly, the proposed critical values are trivial to compute and at the same time provide powerful tests in most settings. In comparison to existing size-correction methods, the new approach exploits the monotonicity of the two components of the limiting distribution of the likelihood ratio statistic, in conjunction with rectangular confidence sets for the nuisance parameters, to gain computational tractability. Uniform validity is established for likelihood ratio tests based on the new critical values, and we provide illustrations of their construction in two key examples: (i) testing a coefficient of interest in the classical linear regression model with non-negativity constraints on control coefficients, and, (ii) testing for the presence of exogenous variables in autoregressive conditional heteroskedastic models (ARCH) with exogenous regressors. Simulations confirm that the tests have desirable size and power properties. A brief empirical illustration demonstrates the usefulness of our proposed test in relation to testing for spill-overs and ARCH effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19603v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Adam McCloskey, Rasmus S. Pedersen, Anders Rahbek</dc:creator>
    </item>
    <item>
      <title>Uniform inference in linear mixed models</title>
      <link>https://arxiv.org/abs/2507.19633</link>
      <description>arXiv:2507.19633v1 Announce Type: cross 
Abstract: We provide finite-sample distribution approximations, that are uniform in the parameter, for inference in linear mixed models. Focus is on variances and covariances of random effects in cases where existing theory fails because their covariance matrix is nearly or exactly singular, and hence near or at the boundary of the parameter set. Quantitative bounds on the differences between the standard normal density and those of linear combinations of the score function enable, for example, the assessment of sufficient sample size. The bounds also lead to useful asymptotic theory in settings where both the number of parameters and the number of random effects grow with the sample size. We consider models with independent clusters and ones with a possibly diverging number of crossed random effects, which are notoriously complicated. Simulations indicate the theory leads to practically relevant methods. In particular, the studied confidence regions, which are straightforward to implement, have near-nominal coverage in finite samples even when some random effects have variances near or equal to zero, or correlations near or equal to $\pm 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19633v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Oskar Ekvall, Matteo Bottai</dc:creator>
    </item>
    <item>
      <title>Binary Classification with the Maximum Score Model and Linear Programming</title>
      <link>https://arxiv.org/abs/2507.19654</link>
      <description>arXiv:2507.19654v1 Announce Type: cross 
Abstract: This paper presents a computationally efficient method for binary classification using Manski's (1975,1985) maximum score model when covariates are discretely distributed and parameters are partially but not point identified. We establish conditions under which it is minimax optimal to allow for either non-classification or random classification and derive finite-sample and asymptotic lower bounds on the probability of correct classification. We also describe an extension of our method to continuous covariates. Our approach avoids the computational difficulty of maximum score estimation by reformulating the problem as two linear programs. Compared to parametric and nonparametric methods, our method balances extrapolation ability with minimal distributional assumptions. Monte Carlo simulations and empirical applications demonstrate its effectiveness and practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19654v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joel L. Horowitz, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Extreme value theory for singular subspace estimation in the matrix denoising model</title>
      <link>https://arxiv.org/abs/2507.19978</link>
      <description>arXiv:2507.19978v1 Announce Type: cross 
Abstract: This paper studies fine-grained singular subspace estimation in the matrix denoising model where a deterministic low-rank signal matrix is additively perturbed by a stochastic matrix of Gaussian noise. We establish that the maximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned difference between the leading sample and population singular vectors approaches the Gumbel distribution in the large-matrix limit, under suitable signal-to-noise conditions and after appropriate centering and scaling. We apply our novel asymptotic distributional theory to test hypotheses of low-rank signal structure encoded in the leading singular vectors and their corresponding principal subspace. We provide de-biased estimators for the corresponding nuisance signal singular values and show that our proposed plug-in test statistic has desirable properties. Notably, compared to using the Frobenius norm subspace distance, our test statistic based on the two-to-infinity norm has higher power to detect structured alternatives that differ from the null in only a few matrix entries or rows. Our main results are obtained by a novel synthesis of and technical analysis involving entrywise matrix perturbation analysis, extreme value theory, saddle point approximation methods, and random matrix theory. Our contributions complement the existing literature for matrix denoising focused on minimaxity, mean squared error analysis, unitarily invariant distances between subspaces, component-wise asymptotic distributional theory, and row-wise uniform error bounds. Numerical simulations illustrate our main results and demonstrate the robustness properties of our testing procedure to non-Gaussian noise distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19978v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Chang, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Computation of Optimal Type-II Progressing Censoring Scheme Using Genetic Algorithm Approach</title>
      <link>https://arxiv.org/abs/2507.20001</link>
      <description>arXiv:2507.20001v1 Announce Type: cross 
Abstract: The experimenter must perform a legitimate search in the entire set of feasible censoring schemes to identify the optimal type II progressive censoring scheme, when applied to a life-testing experiment. Current recommendations are limited to small sample sizes. Exhaustive search strategies are not practically feasible for large sample sizes. This paper proposes a meta-heuristic algorithm based on the genetic algorithm for large sample sizes. The algorithm is found to provide optimal or near-optimal solutions for small sample sizes and large sample sizes. Our suggested optimal criterion is based on the cost function and is scale-invariant for both location-scale and log-location-scale distribution families. To investigate how inaccurate parameter values or cost coefficients may affect the optimal solution, a sensitivity analysis is also taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20001v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ujjwal Roy, Ritwik Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Irredundant k-Fold Cross-Validation</title>
      <link>https://arxiv.org/abs/2507.20048</link>
      <description>arXiv:2507.20048v1 Announce Type: cross 
Abstract: In traditional k-fold cross-validation, each instance is used ($k\!-\!1$) times for training and once for testing, leading to redundancy that lets many instances disproportionately influence the learning phase. We introduce Irredundant $k$--fold cross-validation, a novel method that guarantees each instance is used exactly once for training and once for testing across the entire validation procedure. This approach ensures a more balanced utilization of the dataset, mitigates overfitting due to instance repetition, and enables sharper distinctions in comparative model analysis. The method preserves stratification and remains model-agnostic, i.e., compatible with any classifier. Experimental results demonstrate that it delivers consistent performance estimates across diverse datasets --comparable to $k$--fold cross-validation-- while providing less optimistic variance estimates because training partitions are non-overlapping, and significantly reducing the overall computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20048v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jesus S. Aguilar-Ruiz</dc:creator>
    </item>
    <item>
      <title>Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems</title>
      <link>https://arxiv.org/abs/2507.20072</link>
      <description>arXiv:2507.20072v1 Announce Type: cross 
Abstract: Equation discovery is a fundamental learning task for uncovering the underlying dynamics of complex systems, with wide-ranging applications in areas such as brain connectivity analysis, climate modeling, gene regulation, and physical system simulation. However, many existing approaches rely on accurate derivative estimation and are limited to first-order dynamical systems, restricting their applicability to real-world scenarios. In this work, we propose sparse equation matching (SEM), a unified framework that encompasses several existing equation discovery methods under a common formulation. SEM introduces an integral-based sparse regression method using Green's functions, enabling derivative-free estimation of differential operators and their associated driving functions in general-order dynamical systems. The effectiveness of SEM is demonstrated through extensive simulations, benchmarking its performance against derivative-based approaches. We then apply SEM to electroencephalographic (EEG) data recorded during multiple oculomotor tasks, collected from 52 participants in a brain-computer interface experiment. Our method identifies active brain regions across participants and reveals task-specific connectivity patterns. These findings offer valuable insights into brain connectivity and the underlying neural mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20072v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaqiang Li, Jianbin Tan, Xueqin Wang</dc:creator>
    </item>
    <item>
      <title>Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning</title>
      <link>https://arxiv.org/abs/2507.20089</link>
      <description>arXiv:2507.20089v1 Announce Type: cross 
Abstract: Developing effective multimodal data fusion strategies has become increasingly essential for improving the predictive power of statistical machine learning methods across a wide range of applications, from autonomous driving to medical diagnosis. Traditional fusion methods, including early, intermediate, and late fusion, integrate data at different stages, each offering distinct advantages and limitations. In this paper, we introduce Meta Fusion, a flexible and principled framework that unifies these existing strategies as special cases. Motivated by deep mutual learning and ensemble learning, Meta Fusion constructs a cohort of models based on various combinations of latent representations across modalities, and further boosts predictive performance through soft information sharing within the cohort. Our approach is model-agnostic in learning the latent representations, allowing it to flexibly adapt to the unique characteristics of each modality. Theoretically, our soft information sharing mechanism reduces the generalization error. Empirically, Meta Fusion consistently outperforms conventional fusion strategies in extensive simulation studies. We further validate our approach on real-world applications, including Alzheimer's disease detection and neural decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20089v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziyi Liang, Annie Qu, Babak Shahbaba</dc:creator>
    </item>
    <item>
      <title>From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery</title>
      <link>https://arxiv.org/abs/2507.20349</link>
      <description>arXiv:2507.20349v1 Announce Type: cross 
Abstract: Causal discovery from observational data is challenging, especially with large datasets and complex relationships. Traditional methods often struggle with scalability and capturing global structural information. To overcome these limitations, we introduce a novel graph neural network (GNN)-based probabilistic framework that learns a probability distribution over the entire space of causal graphs, unlike methods that output a single deterministic graph. Our framework leverages a GNN that encodes both node and edge attributes into a unified graph representation, enabling the model to learn complex causal structures directly from data. The GNN model is trained on a diverse set of synthetic datasets augmented with statistical and information-theoretic measures, such as mutual information and conditional entropy, capturing both local and global data properties. We frame causal discovery as a supervised learning problem, directly predicting the entire graph structure. Our approach demonstrates superior performance, outperforming both traditional and recent non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy and scalability on synthetic and real-world datasets without further training. This probabilistic framework significantly improves causal structure learning, with broad implications for decision-making and scientific discovery across various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20349v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rezaur Rashid, Gabriel Terejanu</dc:creator>
    </item>
    <item>
      <title>Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling</title>
      <link>https://arxiv.org/abs/2507.20459</link>
      <description>arXiv:2507.20459v1 Announce Type: cross 
Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A, 185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling data as a mixture of one-dimensional Gaussians, moment-based estimation methods have proliferated. Among these methods, the generalized method of moments (GMM) improves the statistical efficiency of MM by weighting the moments appropriately. However, the computational complexity and storage complexity of MM and GMM grow exponentially with the dimension, making these methods impractical for high-dimensional data or when higher-order moments are required. Such computational bottlenecks are more severe in GMM since it additionally requires estimating a large weighting matrix. To overcome these bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a balance among statistical efficiency, computational complexity, and numerical stability. We apply DGMM to study the parameter estimation problem for weakly separated heteroscedastic low-rank Gaussian mixtures and design a computationally efficient and numerically stable algorithm that obtains the DGMM estimator without explicitly computing or storing the moment tensors. We implement the proposed algorithm and empirically validate the advantages of DGMM: in numerical studies, DGMM attains smaller estimation errors while requiring substantially shorter runtime than MM and GMM. The code and data will be available upon publication at https://github.com/liu-lzhang/dgmm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20459v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Zhang, Oscar Mickelin, Sheng Xu, Amit Singer</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Differentially Private Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2507.20560</link>
      <description>arXiv:2507.20560v1 Announce Type: cross 
Abstract: Privacy preservation in machine learning, particularly through Differentially Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data analysis. However, existing statistical inference methods for SGD predominantly focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This paper first bridges this gap by establishing the asymptotic properties of SGD under the randomized rule and extending these results to DP-SGD. For the output of DP-SGD, we show that the asymptotic variance decomposes into statistical, sampling, and privacy-induced components. Two methods are proposed for constructing valid confidence intervals: the plug-in method and the random scaling method. We also perform extensive numerical analysis, which shows that the proposed confidence intervals achieve nominal coverage rates while maintaining privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20560v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintao Xia, Linjun Zhang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>Multivariate Conformal Prediction via Conformalized Gaussian Scoring</title>
      <link>https://arxiv.org/abs/2507.20941</link>
      <description>arXiv:2507.20941v1 Announce Type: cross 
Abstract: While achieving exact conditional coverage in conformal prediction is unattainable without making strong, untestable regularity assumptions, the promise of conformal prediction hinges on finding approximations to conditional guarantees that are realizable in practice. A promising direction for obtaining conditional dependence for conformal sets--in particular capturing heteroskedasticity--is through estimating the conditional density $\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this vein has focused on nonconformity scores based on the empirical cumulative distribution function (CDF). Such scores are, however, computationally costly, typically requiring expensive sampling methods. To avoid the need for sampling, we observe that the CDF-based score reduces to a Mahalanobis distance in the case of Gaussian scores, yielding a closed-form expression that can be directly conformalized. Moreover, the use of a Gaussian-based score opens the door to a number of extensions of the basic conformal method; in particular, we show how to construct conformal sets with missing output values, refine conformal sets as partial information about $Y$ becomes available, and construct conformal sets on transformations of the output space. Finally, empirical results indicate that our approach produces conformal sets that more closely approximate conditional coverage in multivariate settings compared to alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20941v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacha Braun, Eug\`ene Berta, Michael I. Jordan, Francis Bach</dc:creator>
    </item>
    <item>
      <title>Separate Exchangeability as Modeling Principle in Bayesian Nonparametrics</title>
      <link>https://arxiv.org/abs/2112.07755</link>
      <description>arXiv:2112.07755v3 Announce Type: replace 
Abstract: We argue for the use of separate exchangeability as a modeling principle in Bayesian nonparametric (BNP) inference. Separate exchangeability is de facto widely applied in the Bayesian parametric case, e.g., it naturally arises in simple mixed models. However, while in some areas, such as random graphs, separate and (closely related) joint exchangeable models are widely used, they are curiously underused for several other applications in BNP. We briefly review the definition of separate exchangeability, focusing on the implications of such a definition in Bayesian modeling. We then discuss two tractable classes of models that implement separate exchangeability, which are the natural counterparts of familiar partially exchangeable BNP models.
  The first is nested random partitions for a data matrix, defining a partition of columns and nested partitions of rows, nested within column clusters. Many recent models for nested partitions implement partially exchangeable models related to variations of the well-known nested Dirichlet process. We argue that inference under such models in some cases ignores important features of the experimental setup. We obtain the separately exchangeable counterpart of such partially exchangeable partition structures.
  The second class is about setting up separately exchangeable priors for a nonparametric regression model when multiple sets of experimental units are involved. We highlight how a Dirichlet process mixture of linear models, known as ANOVA DDP, can naturally implement separate exchangeability in such regression problems. Finally, we illustrate how to perform inference under such models in two real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.07755v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Rebaudo, Qiaohui Lin, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixed Multidimensional Scaling for Auditory Processing</title>
      <link>https://arxiv.org/abs/2209.00102</link>
      <description>arXiv:2209.00102v3 Announce Type: replace 
Abstract: The human brain distinguishes speech sounds by mapping acoustic signals into a latent perceptual space. This space can be estimated via multidimensional scaling (MDS), preserving the similarity structure in lower dimensions. However, individual and group-level heterogeneity, especially between native and non-native listeners, remains poorly understood. Prior approaches often ignore such variability or cannot capture shared structure, limiting principled comparison. Moreover, the literature typically focuses on latent distances rather than the underlying features themselves. To address these issues, we develop a Bayesian mixed MDS method that accounts for both subject- and group-level heterogeneity, enabling recovery of biologically interpretable latent features. Simulations and an auditory neuroscience application demonstrate how these features reconstruct observed distances and vary with individual and language background, revealing novel insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00102v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Rebaudo, Fernando Llanos, Bharath Chandrasekaran, Abhra Sarkar</dc:creator>
    </item>
    <item>
      <title>The Effect of Alcohol intake on Brain White Matter Microstructural Integrity: A New Causal Inference Framework for Incomplete Phenomic Data</title>
      <link>https://arxiv.org/abs/2303.03520</link>
      <description>arXiv:2303.03520v3 Announce Type: replace 
Abstract: Although substance use, such as alcohol intake, is known to be associated with cognitive decline during aging, its direct influence on the central nervous system remains incompletely understood. In this study, we investigate the influence of alcohol intake frequency on reduction of brain white matter microstructural integrity in the fornix, a brain region considered a promising marker of age-related microstructural degeneration, using a large UK Biobank (UKB) cohort with extensive phenomic data reflecting a comprehensive lifestyle profile. Two major challenges arise: 1) potentially nonlinear confounding effects from phenomic variables and 2) a limited proportion of participants with complete phenomic data. To address these challenges, we develop a novel ensemble learning framework tailored for robust causal inference and introduce a data integration step to incorporate information from UKB participants with incomplete phenomic data, improving estimation efficiency. Our analysis reveals that daily alcohol intake may significantly reduce fractional anisotropy, a neuroimaging-derived measure of white matter structural integrity, in the fornix and increase systolic and diastolic blood pressure levels. Moreover, extensive numerical studies demonstrate the superiority of our method over competing approaches in terms of estimation bias, while outcome regression-based estimators may be preferred when minimizing mean squared error is prioritized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03520v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chixiang Chen, Shuo Chen, Zhenyao Ye, Xu Shi, Tianzhou Ma, Michelle Shardell</dc:creator>
    </item>
    <item>
      <title>A semi-parametric model for assessing the effect of temperature on ice accumulation rate from Antarctic ice core data</title>
      <link>https://arxiv.org/abs/2309.03782</link>
      <description>arXiv:2309.03782v2 Announce Type: replace 
Abstract: In this paper, we present a semiparametric model for describing the effect of temperature on Antarctic ice accumulation on a paleoclimatic time scale. The model is motivated by sharp ups and downs in the rate of ice accumulation apparent from ice core data records, which are synchronous with movements of temperature. We prove strong consistency of the estimators under reasonable conditions. We conduct extensive simulations to assess the performance of the estimators and bootstrap based standard errors and confidence limits for the requisite range of sample sizes. Analysis of ice core data from two Antarctic locations over several hundred thousand years shows a reasonable fit. The apparent accumulation rate exhibits a thinning pattern that should facilitate the understanding of ice condensation, transformation and flow over the ages. There is a very strong linear relationship between temperature and the apparent accumulation rate adjusted for thinning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03782v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Radhendushka Srivastava, Debasis Sengupta</dc:creator>
    </item>
    <item>
      <title>Model-free Change-point Detection using AUC of a Classifier</title>
      <link>https://arxiv.org/abs/2404.06995</link>
      <description>arXiv:2404.06995v3 Announce Type: replace 
Abstract: In contemporary data analysis, it is increasingly common to work with non-stationary complex data sets. These data sets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions. This paper proposes a novel offline change-point detection method that leverages classifiers developed in the statistics and machine learning community. With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence. It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation. The proposed method is characterized by its complete nonparametric nature, high versatility, considerable flexibility, and absence of stringent assumptions on the underlying data or any distributional shifts. Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives. The localization rate of the change-point estimator is also provided. Extensive simulation studies and the analysis of two real-world data sets illustrate the superior performance of our approach compared to existing model-free change-point detection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06995v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Kanrar, Feiyu Jiang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>Identification and estimation for matrix time series CP-factor models</title>
      <link>https://arxiv.org/abs/2410.05634</link>
      <description>arXiv:2410.05634v3 Announce Type: replace 
Abstract: We propose a new method for identifying and estimating the CP-factor models for matrix time series. Unlike the generalized eigenanalysis-based method of Chang et al. (2023) for which the convergence rates of the associated estimators may suffer from small eigengaps as the asymptotic theory is based on some matrix perturbation analysis, the proposed new method enjoys faster convergence rates which are free from any eigengaps. It achieves this by turning the problem into a joint diagonalization of several matrices whose elements are determined by a basis of a linear system, and by choosing the basis carefully to avoid near co-linearity (see Proposition 5 and Section 4.3). Furthermore, unlike Chang et al. (2023) which requires the two factor loading matrices to be full-ranked, the proposed new method can handle rank-deficient factor loading matrices. Illustration with both simulated and real matrix time series data shows the advantages of the proposed new method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05634v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Guanglin Huang, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Analysis of multivariate event times under informative censoring using vine copula</title>
      <link>https://arxiv.org/abs/2502.20608</link>
      <description>arXiv:2502.20608v2 Announce Type: replace 
Abstract: The study of times to nonterminal events of different types and their interrelation is a compelling area of interest. The primary challenge in analyzing such multivariate event times is the presence of informative censoring by the terminal event. While numerous statistical methods have been proposed for a single nonterminal event, i.e., semi-competing risks data, there remains a dearth of tools for analyzing times to multiple nonterminal events. This article introduces a novel analysis framework that leverages the vine copula to directly estimate the joint density of multivariate times to nonterminal and terminal events. Unlike the few existing methods based on multivariate or nested copulas, the developed approach excels in capturing the heterogeneous dependence between each pair of event times (nonterminal-terminal and between-nonterminal) in terms of strength and structure. We propose a likelihood-based estimation and inference procedure, which can be implemented efficiently in sequential stages. Through extensive simulation studies, we demonstrate the satisfactory finite-sample performance of our proposed stage-wise estimators and analytical variance estimators, as well as their advantages over existing methods. We apply the developed approach to data from a crowdfunding platform to investigate the relationship between various types of creator-backer interactions and a creator's lifetime on the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20608v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Yiwei Li, Qian M. Zhou</dc:creator>
    </item>
    <item>
      <title>A covariate-adaptive test for replicability across multiple studies with false discovery rate control</title>
      <link>https://arxiv.org/abs/2505.15328</link>
      <description>arXiv:2505.15328v3 Announce Type: replace 
Abstract: Replicability is a lynchpin for credible discoveries. The partial conjunction (PC) p-value, which combines individual base p-values from multiple similar studies, can gauge whether a feature of interest exhibits replicated signals across studies. However, when a large set of features are examined as in high-throughput experiments, testing for their replicated signals simultaneously can pose a very underpowered problem, due to both the multiplicity burden and inherent limitations of PC $p$-values. This power deficiency is markedly severe when replication is demanded for all studies under consideration, which is nonetheless the most natural and appealing benchmark for scientific generalizability a practitioner may request.
  We propose ParFilter, a general framework that marries the ideas of filtering and covariate-adaptiveness to power up large-scale testing for replicated signals as described above. It reduces the multiplicity burden by partitioning studies into smaller groups and borrowing the cross-group information to filter out unpromising features. Moreover, harnessing side information offered by auxiliary covariates whenever they are available, it can train informative hypothesis weights to encourage rejections of features more likely to exhibit replicated signals. We prove its finite-sample control on the false discovery rate, under both independence and arbitrary dependence among the base $p$-values across features. In simulations as well as a real case study on autoimmunity based on RNA-Seq data obtained from thymic cells, the ParFilter has demonstrated competitive performance against other existing methods for such replicability analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15328v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ninh Tran, Dennis Leung</dc:creator>
    </item>
    <item>
      <title>Joint modeling for learning decision-making dynamics in behavioral experiments</title>
      <link>https://arxiv.org/abs/2506.02394</link>
      <description>arXiv:2506.02394v2 Announce Type: replace 
Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality, is associated with reward-processing abnormalities and concentration issues. Motivated by the probabilistic reward task from the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel framework that integrates the reinforcement learning (RL) model and drift-diffusion model (DDM) to jointly analyze reward-based decision-making with response times. To account for emerging evidence suggesting that decision-making may alternate between multiple interleaved strategies, we model latent state switching using a hidden Markov model (HMM). In the ''engaged'' state, decisions follow an RL-DDM, simultaneously capturing reward processing, decision dynamics, and temporal structure. In contrast, in the ''lapsed'' state, decision-making is modeled using a simplified DDM, where specific parameters are fixed to approximate random guessing with equal probability. The proposed method is implemented using a computationally efficient generalized expectation-maximization (EM) algorithm with forward-backward procedures. Through extensive numerical studies, we demonstrate that our proposed method outperforms competing approaches across various reward-generating distributions, under both strategy-switching and non-switching scenarios, as well as in the presence of input perturbations. When applied to the EMBARC study, our framework reveals that MDD patients exhibit lower overall engagement than healthy controls and experience longer decision times when they do engage. Additionally, we show that neuroimaging measures of brain activities are associated with decision-making characteristics in the ''engaged'' state but not in the ''lapsed'' state, providing evidence of brain-behavior association specific to the ''engaged'' state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02394v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Bian, Xingche Guo, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v3 Announce Type: replace 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Unveiling Complex Territorial Socio-Economic Dynamics: A Statistical Mechanics Approach</title>
      <link>https://arxiv.org/abs/2506.16872</link>
      <description>arXiv:2506.16872v2 Announce Type: replace 
Abstract: This study proposes a novel approach based on the Ising model for analyzing the observed territorial configuration of a network of municipalities classified as being central hubs or peripheral areas. This is interpreted as being a reference of a system of interacting territorial binary units. The socio-economic structure of the municipalities is synthesized into interpretable composite indices, which are further aggregated by means of Principal Components Analysis in order to reduce dimensionality and construct a univariate external field compatible with the Ising framework. Monte Carlo simulations via parallel computing are conducted adopting a Simulated Annealing variant of the classic Metropolis-Hastings algorithm. This ensures an efficient local exploration of the configuration space in the neighbourhood of to the reference of the system. Model consistency is assessed both in terms of energy stability and the likelihood of these configurations. The comparison between observed configuration and simulated ones is crucial in the analysis of multivariate phenomena, concomitantly accounting for territorial interactions. Model uncertainty in estimating the probability of each municipality being a central hub or peripheral area is quantified by adopting the model-agnostic Conformal Prediction framework which yields adaptive intervals with guaranteed coverage. The innovative use of geographical maps of the prediction intervals renders this approach an effective tool. It combines statistical mechanics, multivariate analysis and uncertainty quantification, providing a robust and interpretable framework for modeling socio-economic territorial dynamics, with potential applications in Official Statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16872v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierpaolo Massoli</dc:creator>
    </item>
    <item>
      <title>Spatialize v1.0: A Python/C++ Library for Ensemble Spatial Interpolation</title>
      <link>https://arxiv.org/abs/2507.17867</link>
      <description>arXiv:2507.17867v2 Announce Type: replace 
Abstract: In this paper, we present Spatialize, an open-source library that implements ensemble spatial interpolation, a novel method that combines the simplicity of basic interpolation methods with the power of classical geostatistical tools, like Kriging. It leverages the richness of stochastic modelling and ensemble learning, making it robust, scalable and suitable for large datasets. In addition, Spatialize provides a powerful framework for uncertainty quantification, offering both point estimates and empirical posterior distributions. It is implemented in Python 3.x, with a C++ core for improved performance, and is designed to be easy to use, requiring minimal user intervention. This library aims to bridge the gap between expert and non-expert users of geostatistics by providing automated tools that rival traditional geostatistical methods. Here, we present a detailed description of Spatialize along with a wealth of examples of its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17867v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felipe Navarro, Alvaro F. Ega\~na, Alejandro Ehrenfeld, Felipe Garrido, Mar\'ia Jes\'us Valenzuela, Juan F. S\'anchez-P\'erez</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2507.19028</link>
      <description>arXiv:2507.19028v2 Announce Type: replace 
Abstract: This paper addresses classification problems with matrix-valued data, which commonly arises in applications such as neuroimaging and signal processing. Building on the assumption that the data from each class follows a matrix normal distribution, we propose a novel extension of Fisher's Linear Discriminant Analysis (LDA) tailored for matrix-valued observations. To effectively capture structural information while maintaining estimation flexibility, we adopt a nonparametric empirical Bayes framework based on Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and scaled matrices. The NPMLE method has been shown to provide robust, flexible, and accurate estimates for vector-valued data with various structures in the mean vector or covariance matrix. By leveraging its strengths, our method is effectively generalized to the matrix setting, thereby improving classification performance. Through extensive simulation studies and real data applications, including electroencephalography (EEG) and magnetic resonance imaging (MRI) analysis, we demonstrate that the proposed method consistently outperforms existing approaches across a variety of data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19028v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungyeon Oh, Seongoh Park, Hoyoung Park</dc:creator>
    </item>
    <item>
      <title>Identifying Socially Disruptive Policies</title>
      <link>https://arxiv.org/abs/2306.15000</link>
      <description>arXiv:2306.15000v3 Announce Type: replace-cross 
Abstract: Social disruption occurs when a policy creates or destroys many network connections between agents. It is a costly side effect of many interventions and so a growing empirical literature recommends measuring and accounting for social disruption when evaluating the welfare impact of a policy. However, there is currently little work characterizing what can actually be learned about social disruption from data in practice. In this paper, we consider the problem of identifying social disruption in an experimental setting. We show that social disruption is not generally point identified, but informative bounds can be constructed by rearranging the eigenvalues of the marginal distribution of network connections between pairs of agents identified from the experiment. We apply our bounds to the setting of Banerjee et al. (2021) and find large disruptive effects that the authors miss by only considering regression estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15000v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Yong Cai</dc:creator>
    </item>
    <item>
      <title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2506.09853</link>
      <description>arXiv:2506.09853v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09853v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang</dc:creator>
    </item>
  </channel>
</rss>
