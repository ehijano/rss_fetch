<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2024 04:01:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Computationally Scalable Bayesian SPDE Modeling for Censored Spatial Responses</title>
      <link>https://arxiv.org/abs/2403.15670</link>
      <description>arXiv:2403.15670v1 Announce Type: new 
Abstract: Observations of groundwater pollutants, such as arsenic or Perfluorooctane sulfonate (PFOS), are riddled with left censoring. These measurements have impact on the health and lifestyle of the populace. Left censoring of these spatially correlated observations are usually addressed by applying Gaussian processes (GPs), which have theoretical advantages. However, this comes with a challenging computational complexity of $\mathcal{O}(n^3)$, which is impractical for large datasets. Additionally, a sizable proportion of the data being left-censored creates further bottlenecks, since the likelihood computation now involves an intractable high-dimensional integral of the multivariate Gaussian density. In this article, we tackle these two problems simultaneously by approximating the GP with a Gaussian Markov random field (GMRF) approach that exploits an explicit link between a GP with Mat\'ern correlation function and a GMRF using stochastic partial differential equations (SPDEs). We introduce a GMRF-based measurement error into the model, which alleviates the likelihood computation for the censored data, drastically improving the speed of the model while maintaining admirable accuracy. Our approach demonstrates robustness and substantial computational scalability, compared to state-of-the-art methods for censored spatial responses across various simulation settings. Finally, the fit of this fully Bayesian model to the concentration of PFOS in groundwater available at 24,959 sites across California, where 46.62\% responses are censored, produces prediction surface and uncertainty quantification in real time, thereby substantiating the applicability and scalability of the proposed method. Code for implementation is made available via GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15670v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Indranil Sahoo, Suman Majumder, Arnab Hazra, Ana G. Rappold, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic</title>
      <link>https://arxiv.org/abs/2403.15755</link>
      <description>arXiv:2403.15755v1 Announce Type: new 
Abstract: Agent-based simulation with a synthetic population can help us compare different treatment conditions while keeping everything else constant within the same population (i.e., as digital twins). Such population-scale simulations require large computational power (i.e., CPU resources) to get accurate estimates for treatment effects. We can use meta models of the simulation results to circumvent the need to simulate every treatment condition. Selecting the best estimating model at a given sample size (number of simulation runs) is a crucial problem. Depending on the sample size, the ability of the method to estimate accurately can change significantly. In this paper, we discuss different methods to explore what model works best at a specific sample size. In addition to the empirical results, we provide a mathematical analysis of the MSE equation and how its components decide which model to select and why a specific method behaves that way in a range of sample sizes. The analysis showed why the direction estimation method is better than model-based methods in larger sample sizes and how the between-group variation and the within-group variation affect the MSE equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15755v1</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdulrahman A. Ahmed, M. Amin Rahimian, Mark S. Roberts</dc:creator>
    </item>
    <item>
      <title>Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier</title>
      <link>https://arxiv.org/abs/2403.15778</link>
      <description>arXiv:2403.15778v1 Announce Type: new 
Abstract: Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Classifier (FVC) is proposed to demonstrate how different functional representations leading to augmented diversity can increase predictive accuracy. Many real-world datasets from several domains are used to display that the FVC can significantly enhance performance compared to individual models. The framework presented provides a foundation for voting ensembles with functional data and can stimulate a highly encouraging line of research in the FDA context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15778v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donato Riccio, Fabrizio Maturo, Elvira Romano</dc:creator>
    </item>
    <item>
      <title>Augmented Doubly Robust Post-Imputation Inference for Proteomic Data</title>
      <link>https://arxiv.org/abs/2403.15802</link>
      <description>arXiv:2403.15802v1 Announce Type: new 
Abstract: Quantitative measurements produced by mass spectrometry proteomics experiments offer a direct way to explore the role of proteins in molecular mechanisms. However, analysis of such data is challenging due to the large proportion of missing values. A common strategy to address this issue is to utilize an imputed dataset, which often introduces systematic bias into downstream analyses if the imputation errors are ignored. In this paper, we propose a statistical framework inspired by doubly robust estimators that offers valid and efficient inference for proteomic data. Our framework combines powerful machine learning tools, such as variational autoencoders, to augment the imputation quality with high-dimensional peptide data, and a parametric model to estimate the propensity score for debiasing imputed outcomes. Our estimator is compatible with the double machine learning framework and has provable properties. In application to both single-cell and bulk-cell proteomic data our method utilizes the imputed data to gain additional, meaningful discoveries and yet maintains good control of false positives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15802v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haeun Moon, Jin-Hong Du, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Non-monotone dependence modeling with copulas: an application to the volume-return relationship</title>
      <link>https://arxiv.org/abs/2403.15862</link>
      <description>arXiv:2403.15862v1 Announce Type: new 
Abstract: This paper introduces an innovative method for constructing copula models capable of describing arbitrary non-monotone dependence structures. The proposed method enables the creation of such copulas in parametric form, thus allowing the resulting models to adapt to diverse and intricate real-world data patterns. We apply this novel methodology to analyze the relationship between returns and trading volumes in financial markets, a domain where the existence of non-monotone dependencies is well-documented in the existing literature. Our approach exhibits superior adaptability compared to other models which have previously been proposed in the literature, enabling a deeper understanding of the dependence structure among the considered variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15862v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manfred Marvin Marchione, Fabio Baione</dc:creator>
    </item>
    <item>
      <title>Integrated path stability selection</title>
      <link>https://arxiv.org/abs/2403.15877</link>
      <description>arXiv:2403.15877v1 Announce Type: new 
Abstract: Stability selection is a widely used method for improving the performance of feature selection algorithms. However, stability selection has been found to be highly conservative, resulting in low sensitivity. Further, the theoretical bound on the expected number of false positives, E(FP), is relatively loose, making it difficult to know how many false positives to expect in practice. In this paper, we introduce a novel method for stability selection based on integrating the stability paths rather than maximizing over them. This yields a tighter bound on E(FP), resulting in a feature selection criterion that has higher sensitivity in practice and is better calibrated in terms of matching the target E(FP). Our proposed method requires the same amount of computation as the original stability selection algorithm, and only requires the user to specify one input parameter, a target value for E(FP). We provide theoretical bounds on performance, and demonstrate the method on simulations and real data from cancer gene expression studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15877v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Melikechi, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Bayesian segmented Gaussian copula factor model for single-cell sequencing data</title>
      <link>https://arxiv.org/abs/2403.15983</link>
      <description>arXiv:2403.15983v1 Announce Type: new 
Abstract: Single-cell sequencing technologies have significantly advanced molecular and cellular biology, offering unprecedented insights into cellular heterogeneity by allowing for the measurement of gene expression at an individual cell level. However, the analysis of such data is challenged by the prevalence of low counts due to dropout events and the skewed nature of the data distribution, which conventional Gaussian factor models struggle to handle effectively. To address these challenges, we propose a novel Bayesian segmented Gaussian copula model to explicitly account for inflation of zero and near-zero counts, and to address the high skewness in the data. By employing a Dirichlet-Laplace prior for each column of the factor loadings matrix, we shrink the loadings of unnecessary factors towards zero, which leads to a simple approach to automatically determine the number of latent factors, and resolve the identifiability issue inherent in factor models due to the rotational invariance of the factor loadings matrix. Through simulation studies, we demonstrate the superior performance of our method over existing approaches in conducting factor analysis on data exhibiting the characteristics of single-cell data, such as excessive low counts and high skewness. Furthermore, we apply the proposed method to a real single-cell RNA-sequencing dataset from a lymphoblastoid cell line, successfully identifying biologically meaningful latent factors and detecting previously uncharacterized cell subtypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15983v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junsouk Choi, Hee Cheol Chung, Irina Gaynanova, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Covariate-adjusted marginal cumulative incidence curves for competing risk analysis</title>
      <link>https://arxiv.org/abs/2403.16256</link>
      <description>arXiv:2403.16256v1 Announce Type: new 
Abstract: Covariate imbalance between treatment groups makes it difficult to compare cumulative incidence curves in competing risk analyses. In this paper we discuss different methods to estimate adjusted cumulative incidence curves including inverse probability of treatment weighting and outcome regression modeling. For these methods to work, correct specification of the propensity score model or outcome regression model, respectively, is needed. We introduce a new doubly robust estimator, which requires correct specification of only one of the two models. We conduct a simulation study to assess the performance of these three methods, including scenarios with model misspecification of the relationship between covariates and treatment and/or outcome. We illustrate their usage in a cohort study of breast cancer patients estimating covariate-adjusted marginal cumulative incidence curves for recurrence, second primary tumour development and death after undergoing mastectomy treatment or breast-conserving therapy. Our study points out the advantages and disadvantages of each covariate adjustment method when applied in competing risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16256v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick van Hage, Saskia le Cessie, Marissa C. van Maaren, Hein Putter, Nan van Geloven</dc:creator>
    </item>
    <item>
      <title>Sample Empirical Likelihood Methods for Causal Inference</title>
      <link>https://arxiv.org/abs/2403.16283</link>
      <description>arXiv:2403.16283v1 Announce Type: new 
Abstract: Causal inference is crucial for understanding the true impact of interventions, policies, or actions, enabling informed decision-making and providing insights into the underlying mechanisms that shape our world. In this paper, we establish a framework for the estimation and inference of average treatment effects using a two-sample empirical likelihood function. Two different approaches to incorporating propensity scores are developed. The first approach introduces propensity scores calibrated constraints in addition to the standard model-calibration constraints; the second approach uses the propensity scores to form weighted versions of the model-calibration constraints. The resulting estimators from both approaches are doubly robust. The limiting distributions of the two sample empirical likelihood ratio statistics are derived, facilitating the construction of confidence intervals and hypothesis tests for the average treatment effect. Bootstrap methods for constructing sample empirical likelihood ratio confidence intervals are also discussed for both approaches. Finite sample performances of the methods are investigated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16283v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyue Huang, Changbao Wu, Leilei Zeng</dc:creator>
    </item>
    <item>
      <title>Round Robin Active Sequential Change Detection for Dependent Multi-Channel Data</title>
      <link>https://arxiv.org/abs/2403.16297</link>
      <description>arXiv:2403.16297v1 Announce Type: new 
Abstract: This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint. Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant. The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes. Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant. The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level. The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule. A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16297v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anamitra Chaudhuri, Georgios Fellouris, Ali Tajer</dc:creator>
    </item>
    <item>
      <title>The Role of Mean Absolute Deviation Function in Obtaining Smooth Estimation for Distribution and Density Functions: Beta Regression Approach</title>
      <link>https://arxiv.org/abs/2403.16544</link>
      <description>arXiv:2403.16544v1 Announce Type: new 
Abstract: Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment. This article introduces a simple approach but novel for estimating both functions in one process to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach. Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates. The derivation for these final smooth estimates under conditions of nondecreasing distribution function and nonnegative density function are performed by applying beta regression of a polynomial degree on the first derivative of left MAD function where the degree of polynomial is chosen among the models that have less mean absolute residuals under the constraint of nonnegativity for the first derivative of regression vector of expected values. A general class of normal, logistic and Gumbel distributions is derived as proposed smooth estimators for the distribution and density functions using logit, probit and cloglog links, respectively. This approach is applied to simulated data from unimodal, bimodal, tri-modal and skew distributions and an application to real data set is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16544v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elsayed A. H. Elamir</dc:creator>
    </item>
    <item>
      <title>Extremal properties of max-autoregressive moving average processes for modelling extreme river flows</title>
      <link>https://arxiv.org/abs/2403.16590</link>
      <description>arXiv:2403.16590v1 Announce Type: new 
Abstract: Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models. River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay. Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows. This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes. We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process. We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold. We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable. We illustrate results for river flow data from the UK River Thames.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16590v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleanor D'Arcy, Jonathan A Tawn</dc:creator>
    </item>
    <item>
      <title>Quasi-randomization tests for network interference</title>
      <link>https://arxiv.org/abs/2403.16673</link>
      <description>arXiv:2403.16673v1 Announce Type: new 
Abstract: Many classical inferential approaches fail to hold when interference exists among the population units. This amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for such spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity. Such approaches can pose computational challenges as finding these appropriate sub-populations based on experimental design can involve solving an NP-hard problem. In this paper, we view the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. Our main idea is to build the (non-sharp) null distribution of no spillover effects using random graph null models. We show that our method is exactly valid in finite-samples under mild assumptions. Our method displays enhanced power over other methods, with substantial improvement in complex experimental designs. We highlight that the method reduces to a simple permutation test, making it easy to implement in practice. We conduct a simulation study to verify the finite-sample validity of our approach and illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16673v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Supriya Tiwari, Pallavi Basu</dc:creator>
    </item>
    <item>
      <title>An alternative measure for quantifying the heterogeneity in meta-analysis</title>
      <link>https://arxiv.org/abs/2403.16706</link>
      <description>arXiv:2403.16706v1 Announce Type: new 
Abstract: Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the $I^2$ statistic is most commonly used. In this paper, we first illustrate with a simple example that the $I^2$ statistic is heavily dependent on the study sample sizes, mainly because it is used to quantify the heterogeneity between the observed effect sizes. To reduce the influence of sample sizes, we introduce an alternative measure that aims to directly measure the heterogeneity between the study populations involved in the meta-analysis. We further propose a new estimator, namely the $I_A^2$ statistic, to estimate the newly defined measure of heterogeneity. For practical implementation, the exact formulas of the $I_A^2$ statistic are also derived under two common scenarios with the effect size as the mean difference (MD) or the standardized mean difference (SMD). Simulations and real data analysis demonstrate that the $I_A^2$ statistic provides an asymptotically unbiased estimator for the absolute heterogeneity between the study populations, and it is also independent of the study sample sizes as expected. To conclude, our newly defined $I_A^2$ statistic can be used as a supplemental measure of heterogeneity to monitor the situations where the study effect sizes are indeed similar with little biological difference. In such scenario, the fixed-effect model can be appropriate; nevertheless, when the sample sizes are sufficiently large, the $I^2$ statistic may still increase to 1 and subsequently suggest the random-effects model for meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16706v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Yang, Enxuan Lin, Wangli Xu, Liping Zhu, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>Privacy-Protected Spatial Autoregressive Model</title>
      <link>https://arxiv.org/abs/2403.16773</link>
      <description>arXiv:2403.16773v1 Announce Type: new 
Abstract: Spatial autoregressive (SAR) models are important tools for studying network effects. However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable. In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements. However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be formulated. To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses. However, the derivatives are biased owing to the noise in the covariates. Therefore, we develop techniques that can correct the biases introduced by noise. Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator. To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction. These two estimation methods ensure both data security and the attainment of statistically valid estimators. Theoretical analysis of both estimators is carefully conducted, and statistical inference methods are discussed. The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16773v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danyang Huang, Ziyi Kong, Shuyuan Wu, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials</title>
      <link>https://arxiv.org/abs/2403.16813</link>
      <description>arXiv:2403.16813v1 Announce Type: new 
Abstract: The sequential multiple assignment randomized trial (SMART) is the
  ideal study design for the evaluation of multistage treatment
  regimes, which comprise sequential decision rules that recommend
  treatments for a patient at each of a series of decision points
  based on their evolving characteristics. A common goal is to
  compare the set of so-called embedded regimes represented in the
  design on the basis of a primary outcome of interest. In the study
  of chronic diseases and disorders, this outcome is often a time to
  an event, and a goal is to compare the distributions of the
  time-to-event outcome associated with each regime in the set. We
  present a general statistical framework in which we develop a
  logrank-type test for comparison of the survival distributions
  associated with regimes within a specified set based on the data
  from a SMART with an arbitrary number of stages that allows
  incorporation of covariate information to enhance efficiency and can
  also be used with data from an observational study. The framework
  provides clarification of the assumptions required to yield a
  principled test procedure, and the proposed test subsumes or offers
  an improved alternative to existing methods. We demonstrate
  performance of the methods in a suite of simulation
  studies. The methods are applied to a SMART in patients with acute
  promyelocytic leukemia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16813v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios A. Tsiatis, Marie Davidian</dc:creator>
    </item>
    <item>
      <title>Testing for sufficient follow-up in survival data with a cure fraction</title>
      <link>https://arxiv.org/abs/2403.16832</link>
      <description>arXiv:2403.16832v1 Announce Type: new 
Abstract: In order to estimate the proportion of `immune' or `cured' subjects who will never experience failure, a sufficiently long follow-up period is required. Several statistical tests have been proposed in the literature for assessing the assumption of sufficient follow-up, meaning that the study duration is longer than the support of the survival times for the uncured subjects. However, for practical purposes, the follow-up would be considered sufficiently long if the probability for the event to happen after the end of the study is very small. Based on this observation, we formulate a more relaxed notion of `practically' sufficient follow-up characterized by the quantiles of the distribution and develop a novel nonparametric statistical test. The proposed method relies mainly on the assumption of a non-increasing density function in the tail of the distribution. The test is then based on a shape constrained density estimator such as the Grenander or the kernel smoothed Grenander estimator and a bootstrap procedure is used for computation of the critical values. The performance of the test is investigated through an extensive simulation study, and the method is illustrated on breast cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16832v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsz Pang Yuen, Eni Musta</dc:creator>
    </item>
    <item>
      <title>Comparing basic statistical concepts with diagnostic probabilities based on directly observed proportions to help understand the replication crisis</title>
      <link>https://arxiv.org/abs/2403.16906</link>
      <description>arXiv:2403.16906v1 Announce Type: new 
Abstract: Instead of regarding an observed proportion as a sample from a population with an unknown parameter, diagnosticians intuitively use the observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might also regard a continuous Gaussian probability distribution of an outcome conditional on a study selection criterion to represents posterior probabilities. Fitting a distribution to its mean and standard deviation (SD) can be regarded as pooling data from an infinite number of imaginary or theoretical studies with an identical mean and SD but randomly different numerical values. For a distribution of possible means based on a SEM, the posterior probability Q of any theoretically true mean falling into a specified tail would be equal to the tail area as a proportion of the whole. If the reverse likelihood distribution of possible study means conditional on the same hypothetical tail threshold is assumed to be the same as the posterior probability distribution of means (as is customary) then by Bayes rule the P value equals Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. Thus, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying the double variance to power calculations, the required number of observations is doubled compared to conventional approaches. If these theoretical probabilities of replication are consistent with empirical replication study results, this might explain the replication crisis and make the concepts of statistics easier to understand by diagnosticians and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16906v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huw Llewelyn</dc:creator>
    </item>
    <item>
      <title>A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners</title>
      <link>https://arxiv.org/abs/2403.15499</link>
      <description>arXiv:2403.15499v1 Announce Type: cross 
Abstract: This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study's findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question's specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development practices, emphasizing the importance of considering unintended consequences in policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15499v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Emtiazi Naeini, Zahra Saberi, Khadijeh Hassanzadeh</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference of higher order interaction patterns in networks</title>
      <link>https://arxiv.org/abs/2403.15635</link>
      <description>arXiv:2403.15635v1 Announce Type: cross 
Abstract: We propose a method for obtaining parsimonious decompositions of networks into higher order interactions which can take the form of arbitrary motifs.The method is based on a class of analytically solvable generative models, where vertices are connected via explicit copies of motifs, which in combination with non-parametric priors allow us to infer higher order interactions from dyadic graph data without any prior knowledge on the types or frequencies of such interactions. Crucially, we also consider 'degree--corrected' models that correctly reflect the degree distribution of the network and consequently prove to be a better fit for many real world--networks compared to non-degree corrected models. We test the presented approach on simulated data for which we recover the set of underlying higher order interactions to a high degree of accuracy. For empirical networks the method identifies concise sets of atomic subgraphs from within thousands of candidates that cover a large fraction of edges and include higher order interactions of known structural and functional significance. The method not only produces an explicit higher order representation of the network but also a fit of the network to analytically tractable models opening new avenues for the systematic study of higher order network structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15635v1</guid>
      <category>cs.SI</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anatol E. Wegner, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>Identifiable Latent Neural Causal Models</title>
      <link>https://arxiv.org/abs/2403.15711</link>
      <description>arXiv:2403.15711v1 Announce Type: cross 
Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15711v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</dc:creator>
    </item>
    <item>
      <title>Reviving pseudo-inverses: Asymptotic properties of large dimensional Moore-Penrose and Ridge-type inverses with applications</title>
      <link>https://arxiv.org/abs/2403.15792</link>
      <description>arXiv:2403.15792v1 Announce Type: cross 
Abstract: In this paper, we derive high-dimensional asymptotic properties of the Moore-Penrose inverse and the ridge-type inverse of the sample covariance matrix. In particular, the analytical expressions of the weighted sample trace moments are deduced for both generalized inverse matrices and are present by using the partial exponential Bell polynomials which can easily be computed in practice. The existent results are extended in several directions: (i) First, the population covariance matrix is not assumed to be a multiple of the identity matrix; (ii) Second, the assumption of normality is not used in the derivation; (iii) Third, the asymptotic results are derived under the high-dimensional asymptotic regime. Our findings are used to construct improved shrinkage estimators of the precision matrix, which asymptotically minimize the quadratic loss with probability one. Finally, the finite sample properties of the derived theoretical results are investigated via an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15792v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taras Bodnar, Nestor Parolya</dc:creator>
    </item>
    <item>
      <title>Learning Directed Acyclic Graphs from Partial Orderings</title>
      <link>https://arxiv.org/abs/2403.16031</link>
      <description>arXiv:2403.16031v1 Announce Type: cross 
Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16031v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Shojaie, Wenyu Chen</dc:creator>
    </item>
    <item>
      <title>Predictive Inference in Multi-environment Scenarios</title>
      <link>https://arxiv.org/abs/2403.16336</link>
      <description>arXiv:2403.16336v1 Announce Type: cross 
Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16336v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Optimal testing in a class of nonregular models</title>
      <link>https://arxiv.org/abs/2403.16413</link>
      <description>arXiv:2403.16413v1 Announce Type: cross 
Abstract: This paper studies optimal hypothesis testing for nonregular statistical models with parameter-dependent support. We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on the likelihood ratio process. The proposed one-sided test involves randomization to achieve asymptotic size control, some tuning constant to avoid discontinuities in the limiting likelihood ratio process, and a user-specified alternative hypothetical value to achieve the asymptotic optimality. Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness. Simulation results illustrate desirable power properties of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16413v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Optimal convex $M$-estimation via score matching</title>
      <link>https://arxiv.org/abs/2403.16688</link>
      <description>arXiv:2403.16688v1 Announce Type: cross 
Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much efficiency. Numerical experiments confirm the practical merits of our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16688v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Y. Feng, Yu-Chun Kao, Min Xu, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Asymptotics of predictive distributions driven by sample means and variances</title>
      <link>https://arxiv.org/abs/2403.16828</link>
      <description>arXiv:2403.16828v1 Announce Type: cross 
Abstract: Let $\alpha_n(\cdot)=P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)$ be the predictive distributions of a sequence $(X_1,X_2,\ldots)$ of $p$-variate random variables. Suppose $$\alpha_n=\mathcal{N}_p(M_n,Q_n)$$ where $M_n=\frac{1}{n}\sum_{i=1}^nX_i$ and $Q_n=\frac{1}{n}\sum_{i=1}^n(X_i-M_n)(X_i-M_n)^t$. Then, there is a random probability measure $\alpha$ on $\mathbb{R}^p$ such that $\alpha_n\rightarrow\alpha$ weakly a.s. If $p\in\{1,2\}$, one also obtains $\lVert\alpha_n-\alpha\rVert\overset{a.s.}\longrightarrow 0$ where $\lVert\cdot\rVert$ is total variation distance. Moreover, the convergence rate of $\lVert\alpha_n-\alpha\rVert$ is arbitrarily close to $n^{-1/2}$. These results (apart from the one regarding the convergence rate) still apply even if $\alpha_n=\mathcal{L}_p(M_n,Q_n)$, where $\mathcal{L}_p$ belongs to a class of distributions much larger than the normal. Finally, the asymptotic behavior of copula-based predictive distributions (introduced in [13]) is investigated and a numerical experiment is performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16828v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuele Garelli, Fabrizio Leisen, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>Resistant Inference in Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2403.16844</link>
      <description>arXiv:2403.16844v1 Announce Type: cross 
Abstract: The classical tests in the instrumental variable model can behave arbitrarily if the data is contaminated. For instance, one outlying observation can be enough to change the outcome of a test. We develop a framework to construct testing procedures that are robust to weak instruments, outliers and heavy-tailed errors in the instrumental variable model. The framework is constructed upon M-estimators. By deriving the influence functions of the classical weak instrument robust tests, such as the Anderson-Rubin test, K-test and the conditional likelihood ratio (CLR) test, we prove their unbounded sensitivity to infinitesimal contamination. Therefore, we construct contamination resistant/robust alternatives. In particular, we show how to construct a robust CLR statistic based on Mallows type M-estimators and show that its asymptotic distribution is the same as that of the (classical) CLR statistic. The theoretical results are corroborated by a simulation study. Finally, we revisit three empirical studies affected by outliers and demonstrate how the new robust tests can be used in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16844v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens Klooster, Mikhail Zhelonkin</dc:creator>
    </item>
    <item>
      <title>Broadcasted Nonparametric Tensor Regression</title>
      <link>https://arxiv.org/abs/2008.12927</link>
      <description>arXiv:2008.12927v3 Announce Type: replace 
Abstract: We propose a novel use of a broadcasting operation, which distributes univariate functions to all entries of the tensor covariate, to model the nonlinearity in tensor regression nonparametrically. A penalized estimation and the corresponding algorithm are proposed. Our theoretical investigation, which allows the dimensions of the tensor covariate to diverge, indicates that the proposed estimation yields a desirable convergence rate. We also provide a minimax lower bound, which characterizes the optimality of the proposed estimator for a wide range of scenarios. Numerical experiments are conducted to confirm the theoretical findings, and they show that the proposed model has advantages over its existing linear counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.12927v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Raymond K. W. Wong, Kejun He</dc:creator>
    </item>
    <item>
      <title>G-Formula for Observational Studies under Stratified Interference, with Application to Bed Net Use on Malaria</title>
      <link>https://arxiv.org/abs/2102.01155</link>
      <description>arXiv:2102.01155v2 Announce Type: replace 
Abstract: Assessing population-level effects of vaccines and other infectious disease prevention measures is important to the field of public health. In infectious disease studies, one person's treatment may affect another individual's outcome, i.e., there may be interference between units. For example, the use of bed nets to prevent malaria by one individual may have an indirect effect on other individuals living in close proximity. In some settings, individuals may form groups or clusters where interference only occurs within groups, i.e., there is partial interference. Inverse probability weighted estimators have previously been developed for observational studies with partial interference. Unfortunately, these estimators are not well suited for studies with large clusters. Therefore, in this paper, the parametric g-formula is extended to allow for partial interference. G-formula estimators are proposed for overall effects, effects when treated, and effects when untreated. The proposed estimators can accommodate large clusters and do not suffer from the g-null paradox that may occur in the absence of interference. The large sample properties of the proposed estimators are derived assuming no unmeasured confounders and that the partial interference takes a particular form (referred to as `weak stratified interference'). Simulation studies are presented demonstrating the finite-sample performance of the proposed estimators. The Demographic and Health Survey from the Democratic Republic of the Congo is then analyzed using the proposed g-formula estimators to assess the effects of bed net use on malaria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.01155v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayla W. Kilpatrick, Chanhwa Lee, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>An unbiased estimator of the case fatality rate</title>
      <link>https://arxiv.org/abs/2109.03087</link>
      <description>arXiv:2109.03087v2 Announce Type: replace 
Abstract: During an epidemic outbreak of a new disease, the probability of dying once infected is considered an important though difficult task to be computed. Since it is very hard to know the true number of infected people, the focus is placed on estimating the case fatality rate, which is defined as the probability of dying once tested and confirmed as infected. The estimation of this rate at the beginning of an epidemic remains challenging for several reasons, including the time gap between diagnosis and death, and the rapid growth in the number of confirmed cases. In this work, an unbiased estimator of the case fatality rate of a virus is presented. The consistency of the estimator is demonstrated, and its asymptotic distribution is derived, enabling the corresponding confidence intervals (C.I.) to be established. The proposed method is based on the distribution F of the time between confirmation and death of individuals who die because of the virus. The estimator's performance is analyzed in both simulation scenarios and the real-world context of Argentina in 2020 for the COVID-19 pandemic, consistently achieving excellent results when compared to an existing proposal as well as to the conventional \naive" estimator that was employed to report the case fatality rates during the last COVID-19 pandemic. In the simulated scenarios, the empirical coverage of our C.I. is studied, both using the F employed to generate the data and an estimated F, and it is observed that the desired level of confidence is reached quickly when using real F and in a reasonable period of time when estimating F.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.03087v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agust\'in Alvarez, Marina Fragal\'a, Marina Valdora</dc:creator>
    </item>
    <item>
      <title>Tandem clustering with invariant coordinate selection</title>
      <link>https://arxiv.org/abs/2212.06108</link>
      <description>arXiv:2212.06108v5 Announce Type: replace 
Abstract: For multivariate data, tandem clustering is a well-known technique aiming to improve cluster identification through initial dimension reduction. Nevertheless, the usual approach using principal component analysis (PCA) has been criticized for focusing solely on inertia so that the first components do not necessarily retain the structure of interest for clustering. To address this limitation, a new tandem clustering approach based on invariant coordinate selection (ICS) is proposed. By jointly diagonalizing two scatter matrices, ICS is designed to find structure in the data while providing affine invariant components. Certain theoretical results have been previously derived and guarantee that under some elliptical mixture models, the group structure can be highlighted on a subset of the first and/or last components. However, ICS has garnered minimal attention within the context of clustering. Two challenges associated with ICS include choosing the pair of scatter matrices and selecting the components to retain. For effective clustering purposes, it is demonstrated that the best scatter pairs consist of one scatter matrix capturing the within-cluster structure and another capturing the global structure. For the former, local shape or pairwise scatters are of great interest, as is the minimum covariance determinant (MCD) estimator based on a carefully chosen subset size that is smaller than usual. The performance of ICS as a dimension reduction method is evaluated in terms of preserving the cluster structure in the data. In an extensive simulation study and empirical applications with benchmark data sets, various combinations of scatter matrices as well as component selection criteria are compared in situations with and without outliers. Overall, the new approach of tandem clustering with ICS shows promising results and clearly outperforms the PCA-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06108v5</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ecosta.2024.03.002</arxiv:DOI>
      <dc:creator>Andreas Alfons, Aurore Archimbaud, Klaus Nordhausen, Anne Ruiz-Gazen</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification of MLE for Entity Ranking with Covariates</title>
      <link>https://arxiv.org/abs/2212.09961</link>
      <description>arXiv:2212.09961v2 Announce Type: replace 
Abstract: This paper concerns with statistical estimation and inference for the ranking problems based on pairwise comparisons with additional covariate information such as the attributes of the compared items. Despite extensive studies, few prior literatures investigate this problem under the more realistic setting where covariate information exists. To tackle this issue, we propose a novel model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate information. Specifically, instead of assuming every compared item has a fixed latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and ${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th item, respectively. We impose natural identifiability conditions and derive the $\ell_{\infty}$- and $\ell_2$-optimal rates for the maximum likelihood estimator of $\{\alpha_i^*\}_{i=1}^{n}$ and $\beta^*$ under a sparse comparison graph, using a novel `leave-one-out' technique (Chen et al., 2019) . To conduct statistical inferences, we further derive asymptotic distributions for the MLE of $\{\alpha_i^*\}_{i=1}^n$ and $\beta^*$ with minimal sample complexity. This allows us to answer the question whether some covariates have any explanation power for latent scores and to threshold some sparse parameters to improve the ranking performance. We improve the approximation method used in (Gao et al., 2021) for the BLT model and generalize it to the CARE model. Moreover, we validate our theoretical results through large-scale numerical studies and an application to the mutual fund stock holding dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09961v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Jikai Hou, Mengxin Yu</dc:creator>
    </item>
    <item>
      <title>Improving instrumental variable estimators with post-stratification</title>
      <link>https://arxiv.org/abs/2303.10016</link>
      <description>arXiv:2303.10016v2 Announce Type: replace 
Abstract: Experiments studying get-out-the-vote (GOTV) efforts estimate the causal effect of various mobilization efforts on voter turnout. However, there is often substantial noncompliance in these studies. A usual approach is to use an instrumental variable (IV) analysis to estimate impacts for compliers, here being those actually contacted by the investigators. Unfortunately, popular IV estimators can be unstable in studies with a small fraction of compliers. We explore post-stratifying the data (e.g., taking a weighted average of IV estimates within each stratum) using variables that predict complier status (and, potentially, the outcome) to mitigate this. We present the benefits of post-stratification in terms of bias, variance, and improved standard error estimates, and provide a finite-sample asymptotic variance formula. We also compare the performance of different IV approaches and discuss the advantages of our design-based post-stratification approach over incorporating compliance-predictive covariates into the two-stage least squares estimator. In the end, we show that covariates predictive of compliance can increase precision, but only if one is willing to make a bias-variance trade-off by down-weighting or dropping strata with few compliers. By contrast, standard approaches such as two-stage least squares fail to use such information. We finally examine the benefits of our approach in two GOTV applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10016v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicole E. Pashley, Luke Keele, Luke W. Miratrix</dc:creator>
    </item>
    <item>
      <title>Testing for linearity in scalar-on-function regression with responses missing at random</title>
      <link>https://arxiv.org/abs/2304.04712</link>
      <description>arXiv:2304.04712v2 Announce Type: replace 
Abstract: A goodness-of-fit test for the Functional Linear Model with Scalar Response (FLMSR) with responses Missing at Random (MAR) is proposed in this paper. The test statistic relies on a marked empirical process indexed by the projected functional covariate and its distribution under the null hypothesis is calibrated using a wild bootstrap procedure. The computation and performance of the test rely on having an accurate estimator of the functional slope of the FLMSR when the sample has MAR responses. Three estimation methods based on the Functional Principal Components (FPCs) of the covariate are considered. First, the simplified method estimates the functional slope by simply discarding observations with missing responses. Second, the imputed method estimates the functional slope by imputing the missing responses using the simplified estimator. Third, the inverse probability weighted method incorporates the missing response generation mechanism when imputing. Furthermore, both cross-validation and LASSO regression are used to select the FPCs used by each estimator. Several Monte Carlo experiments are conducted to analyze the behavior of the testing procedure in combination with the functional slope estimators. Results indicate that estimators performing missing-response imputation achieve the highest power. The testing procedure is applied to check for linear dependence between the average number of sunny days per year and the mean curve of daily temperatures at weather stations in Spain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04712v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-023-01445-2</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics, 2024</arxiv:journal_reference>
      <dc:creator>Manuel Febrero-Bande, Pedro Galeano, Eduardo Garc\'ia-Portugu\'es, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Conformal link prediction for false discovery rate control</title>
      <link>https://arxiv.org/abs/2306.14693</link>
      <description>arXiv:2306.14693v2 Announce Type: replace 
Abstract: Most link prediction methods return estimates of the connection probability of missing edges in a graph. Such output can be used to rank the missing edges from most to least likely to be a true edge, but does not directly provide a classification into true and non-existent. In this work, we consider the problem of identifying a set of true edges with a control of the false discovery rate (FDR). We propose a novel method based on high-level ideas from the literature on conformal inference. The graph structure induces intricate dependence in the data, which we carefully take into account, as this makes the setup different from the usual setup in conformal inference, where data exchangeability is assumed. The FDR control is empirically demonstrated for both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14693v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariane Marandon</dc:creator>
    </item>
    <item>
      <title>Causal Meta-Analysis by Integrating Multiple Observational Studies with Multivariate Outcomes</title>
      <link>https://arxiv.org/abs/2306.16715</link>
      <description>arXiv:2306.16715v3 Announce Type: replace 
Abstract: Integrating multiple observational studies to make unconfounded causal or descriptive comparisons of group potential outcomes in a large natural population is challenging. Moreover, retrospective cohorts, being convenience samples, are usually unrepresentative of the natural population of interest and have groups with unbalanced covariates. We propose a general covariate-balancing framework based on pseudo-populations that extends established weighting methods to the meta-analysis of multiple retrospective cohorts with multiple groups. Additionally, by maximizing the effective sample sizes of the cohorts, we propose a FLEXible, Optimized, and Realistic (FLEXOR) weighting method appropriate for integrative analyses. We develop new weighted estimators for unconfounded inferences on wide-ranging population-level features and estimands relevant to group comparisons of quantitative, categorical, or multivariate outcomes. The asymptotic properties of these estimators are examined. Through simulation studies and meta-analyses of TCGA datasets, we demonstrate the versatility and reliability of the proposed weighting strategy, especially for the FLEXOR pseudo-population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16715v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Yi Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Structure Learning in Undirected Gaussian Graphical Models: Literature Review with Empirical Comparison</title>
      <link>https://arxiv.org/abs/2307.02603</link>
      <description>arXiv:2307.02603v2 Announce Type: replace 
Abstract: Gaussian graphical models provide a powerful framework to reveal the conditional dependency structure between multivariate variables. The process of uncovering the conditional dependency network is known as structure learning. Bayesian methods can measure the uncertainty of conditional relationships and include prior information. However, frequentist methods are often preferred due to the computational burden of the Bayesian approach. Over the last decade, Bayesian methods have seen substantial improvements, with some now capable of generating accurate estimates of graphs up to a thousand variables in mere minutes. Despite these advancements, a comprehensive review or empirical comparison of all recent methods has not been conducted. This paper delves into a wide spectrum of Bayesian approaches used for structure learning and evaluates their efficacy through a simulation study. We also demonstrate how to apply Bayesian structure learning to a real-world data set and provide directions for future research. This study gives an exhaustive overview of this dynamic field for newcomers, practitioners, and experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02603v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lucas Vogels, Reza Mohammadi, Marit Schoonhoven, S. Ilker Birbil</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation and Comparison of Distance Distributions from Censored Data</title>
      <link>https://arxiv.org/abs/2311.02658</link>
      <description>arXiv:2311.02658v4 Announce Type: replace 
Abstract: Transportation distance information is a powerful resource, but location records are often censored due to privacy concerns or regulatory mandates. We outline methods to approximate, sample from, and compare distributions of distances between censored location pairs, a task with applications to public health informatics, logistics, and more. We validate empirically via simulation and demonstrate applicability to practical geospatial data analysis tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02658v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas H. McCabe</dc:creator>
    </item>
    <item>
      <title>Bayesian Functional Analysis for Untargeted Metabolomics Data with Matching Uncertainty and Small Sample Sizes</title>
      <link>https://arxiv.org/abs/2312.03257</link>
      <description>arXiv:2312.03257v3 Announce Type: replace 
Abstract: Untargeted metabolomics based on liquid chromatography-mass spectrometry technology is quickly gaining widespread application given its ability to depict the global metabolic pattern in biological samples. However, the data is noisy and plagued by the lack of clear identity of data features measured from samples. Multiple potential matchings exist between data features and known metabolites, while the truth can only be one-to-one matches. Some existing methods attempt to reduce the matching uncertainty, but are far from being able to remove the uncertainty for most features. The existence of the uncertainty causes major difficulty in downstream functional analysis. To address these issues, we develop a novel approach for Bayesian Analysis of Untargeted Metabolomics data (BAUM) to integrate previously separate tasks into a single framework, including matching uncertainty inference, metabolite selection, and functional analysis. By incorporating the knowledge graph between variables and using relatively simple assumptions, BAUM can analyze datasets with small sample sizes. By allowing different confidence levels of feature-metabolite matching, the method is applicable to datasets in which feature identities are partially known. Simulation studies demonstrate that, compared with other existing methods, BAUM achieves better accuracy in selecting important metabolites that tend to be functionally consistent and assigning confidence scores to feature-metabolite matches. We analyze a COVID-19 metabolomics dataset and a mouse brain metabolomics dataset using BAUM. Even with a very small sample size of 16 mice per group, BAUM is robust and stable. It finds pathways that conform to existing knowledge, as well as novel pathways that are biologically plausible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03257v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Jian Kang, Tianwei Yu</dc:creator>
    </item>
    <item>
      <title>Posterior Ramifications of Prior Dependence Structures</title>
      <link>https://arxiv.org/abs/2312.06437</link>
      <description>arXiv:2312.06437v2 Announce Type: replace 
Abstract: In fully Bayesian analyses, prior distributions are specified before observing data. Prior elicitation methods transfigure prior information into quantifiable prior distributions. Recently, methods that leverage copulas have been proposed to accommodate more flexible dependence structures when eliciting multivariate priors. We prove that under broad conditions, the posterior cannot retain many of these flexible prior dependence structures in large-sample settings. We emphasize the impact of this result by overviewing several objectives for prior specification to help practitioners select prior dependence structures that align with their objectives for posterior analysis. Because correctly specifying the dependence structure a priori can be difficult, we consider how the choice of prior copula impacts the posterior distribution in terms of asymptotic convergence of the posterior mode. Our resulting recommendations streamline the prior elicitation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06437v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Handling incomplete outcomes and covariates in cluster-randomized trials: doubly-robust estimation, efficiency considerations, and sensitivity analysis</title>
      <link>https://arxiv.org/abs/2401.11278</link>
      <description>arXiv:2401.11278v3 Announce Type: replace 
Abstract: In cluster-randomized trials (CRTs), missing data can occur in various ways, including missing values in outcomes and baseline covariates at the individual or cluster level, or completely missing information for non-participants. Among the various types of missing data in CRTs, missing outcomes have attracted the most attention. However, no existing methods can simultaneously address all aforementioned types of missing data in CRTs. To fill in this gap, we propose a new doubly-robust estimator for the average treatment effect on a variety of scales. The proposed estimator simultaneously handles missing outcomes under missingness at random, missing covariates without constraining the missingness mechanism, and missing cluster-population sizes via a uniform sampling mechanism. Furthermore, we detail key considerations to improve precision by specifying the optimal weights, leveraging machine learning, and modeling the treatment assignment mechanism. Finally, to evaluate the impact of violating missing data assumptions, we contribute a new sensitivity analysis framework tailored to CRTs. Simulation studies and a real data application both demonstrate that our proposed methods are effective in handling missing data in CRTs and superior to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11278v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingkai Wang, Fan Li, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Debiased Multivariable Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2402.00307</link>
      <description>arXiv:2402.00307v2 Announce Type: replace 
Abstract: Multivariable Mendelian randomization (MVMR) uses genetic variants as instrumental variables to infer the direct effect of multiple exposures on an outcome. Compared to univariable Mendelian randomization, MVMR is less prone to horizontal pleiotropy and enables estimation of the direct effect of each exposure on the outcome. However, MVMR faces greater challenges with weak instruments -- genetic variants that are weakly associated with some exposures conditional on the other exposures. This article focuses on MVMR using summary data from genome-wide association studies (GWAS). We provide a new asymptotic regime to analyze MVMR estimators with many weak instruments, allowing for linear combinations of exposures to have different degrees of instrument strength, and formally show that the popular multivariable inverse-variance weighted (MV-IVW) estimator's asymptotic behavior is highly sensitive to instruments' strength. We then propose a multivariable debiased IVW (MV-dIVW) estimator, which effectively reduces the asymptotic bias from weak instruments in MV-IVW, and introduce an adjusted version, MV-adIVW, for improved finite-sample robustness. We establish the theoretical properties of our proposed estimators and extend them to handle balanced horizontal pleiotropy. We conclude by demonstrating the performance of our proposed methods in simulated and real datasets. We implement this method in the R package mr.divw.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00307v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiang Wu, Hyunseung Kang, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Stable Reduced-Rank VAR Identification</title>
      <link>https://arxiv.org/abs/2403.00237</link>
      <description>arXiv:2403.00237v2 Announce Type: replace 
Abstract: The vector autoregression (VAR) has been widely used in system identification, econometrics, natural science, and many other areas. However, when the state dimension becomes large the parameter dimension explodes. So rank reduced modelling is attractive and is well developed. But a fundamental requirement in almost all applications is stability of the fitted model. And this has not been addressed in the rank reduced case. Here, we develop, for the first time, a closed-form formula for an estimator of a rank reduced transition matrix which is guaranteed to be stable. We show that our estimator is consistent and asymptotically statistically efficient and illustrate it in comparative simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00237v2</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhui Rong, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage</title>
      <link>https://arxiv.org/abs/2403.03868</link>
      <description>arXiv:2403.03868v2 Announce Type: replace 
Abstract: Conformal prediction builds marginally valid prediction intervals that cover the unknown outcome of a randomly drawn new test point with a prescribed probability. However, a common scenario in practice is that, after seeing the data, practitioners decide which test unit(s) to focus on in a data-driven manner and seek for uncertainty quantification of the focal unit(s). In such cases, marginally valid conformal prediction intervals may not provide valid coverage for the focal unit(s) due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected by a given procedure. The general form of our method works for arbitrary selection rules that are invariant to the permutation of the calibration units, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out the computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03868v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Generalized Rosenbaum Bounds Sensitivity Analysis for Matched Observational Studies with Treatment Doses: Sufficiency, Consistency, and Efficiency</title>
      <link>https://arxiv.org/abs/2403.14152</link>
      <description>arXiv:2403.14152v2 Announce Type: replace 
Abstract: In matched observational studies with binary treatments, the Rosenbaum bounds framework is arguably the most widely used sensitivity analysis framework for assessing sensitivity to unobserved covariates. Unlike the binary treatment case, although widely needed in practice, sensitivity analysis for matched observational studies with treatment doses (i.e., non-binary treatments such as ordinal treatments or continuous treatments) still lacks solid foundations and valid methodologies. We fill in this blank by establishing theoretical foundations and novel methodologies under a generalized Rosenbaum bounds sensitivity analysis framework. First, we present a criterion for assessing the validity of sensitivity analysis in matched observational studies with treatment doses and use that criterion to justify the necessity of incorporating the treatment dose information into sensitivity analysis through generalized Rosenbaum sensitivity bounds. We also generalize Rosenbaum's classic sensitivity parameter $\Gamma$ to the non-binary treatment case and prove its sufficiency. Second, we study the asymptotic properties of sensitivity analysis by generalizing Rosenbaum's classic design sensitivity and Bahadur efficiency for testing Fisher's sharp null to the non-binary treatment case and deriving novel formulas for them. Our theoretical results showed the importance of appropriately incorporating the treatment dose into a test, which is an intrinsic distinction with the binary treatment case. Third, for testing Neyman's weak null (i.e., null sample average treatment effect), we propose the first valid sensitivity analysis procedure for matching with treatment dose through generalizing an existing optimization-based sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14152v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Iterative execution of discrete and inverse discrete Fourier transforms with applications for signal denoising via sparsification</title>
      <link>https://arxiv.org/abs/2211.09284</link>
      <description>arXiv:2211.09284v3 Announce Type: replace-cross 
Abstract: We describe a family of iterative algorithms that involve the repeated execution of discrete and inverse discrete Fourier transforms. One interesting member of this family is motivated by the discrete Fourier transform uncertainty principle and involves the application of a sparsification operation to both the real domain and frequency domain data with convergence obtained when real domain sparsity hits a stable pattern. This sparsification variant has practical utility for signal denoising, in particular the recovery of a periodic spike signal in the presence of Gaussian noise. General convergence properties and denoising performance relative to existing methods are demonstrated using simulation studies. An R package implementing this technique and related resources can be found at https://hrfrost.host.dartmouth.edu/IterativeFT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09284v3</guid>
      <category>eess.SP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Robert Frost</dc:creator>
    </item>
    <item>
      <title>Bias-Free Estimation of Signals on Top of Unknown Backgrounds</title>
      <link>https://arxiv.org/abs/2306.17667</link>
      <description>arXiv:2306.17667v2 Announce Type: replace-cross 
Abstract: We present a method for obtaining unbiased signal estimates in the presence of a significant unknown background, eliminating the need for a parametric model for the background itself. Our approach is based on a minimal set of conditions for observation and background estimators, which are typically satisfied in practical scenarios. To showcase the effectiveness of our method, we apply it to simulated data from the planned dielectric axion haloscope MADMAX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17667v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.nima.2024.169259</arxiv:DOI>
      <arxiv:journal_reference>NIM A 1063 (2024) 169259</arxiv:journal_reference>
      <dc:creator>Johannes Diehl, Jakob Knollm\"uller, Oliver Schulz</dc:creator>
    </item>
    <item>
      <title>Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions</title>
      <link>https://arxiv.org/abs/2311.02695</link>
      <description>arXiv:2311.02695v2 Announce Type: replace-cross 
Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02695v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge</dc:creator>
    </item>
  </channel>
</rss>
