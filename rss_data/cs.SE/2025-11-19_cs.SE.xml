<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 02:41:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation</title>
      <link>https://arxiv.org/abs/2511.13972</link>
      <description>arXiv:2511.13972v1 Announce Type: new 
Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13972v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremiah Bohr</dc:creator>
    </item>
    <item>
      <title>Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education</title>
      <link>https://arxiv.org/abs/2511.13996</link>
      <description>arXiv:2511.13996v1 Announce Type: new 
Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13996v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daihan Xu, Diana Martin</dc:creator>
    </item>
    <item>
      <title>LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering</title>
      <link>https://arxiv.org/abs/2511.13998</link>
      <description>arXiv:2511.13998v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13998v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Roshan Ram, Akshara Prabhakar, Tulika Awalgaonkar, Zixiang Chen, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang</dc:creator>
    </item>
    <item>
      <title>FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale</title>
      <link>https://arxiv.org/abs/2511.14002</link>
      <description>arXiv:2511.14002v1 Announce Type: new 
Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14002v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengpeng Li, Farnaz Behrang, August Shi, Peng Liu</dc:creator>
    </item>
    <item>
      <title>Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning</title>
      <link>https://arxiv.org/abs/2511.14022</link>
      <description>arXiv:2511.14022v1 Announce Type: new 
Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14022v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pradeep Kumar Sharma, Ishaan Puri, Mantinder Jit Singh, Swapnil Shivaprasad, Hritvik Shrivastava</dc:creator>
    </item>
    <item>
      <title>LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering</title>
      <link>https://arxiv.org/abs/2511.14062</link>
      <description>arXiv:2511.14062v1 Announce Type: new 
Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14062v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenglin Zhang, Ziang Chen, Zijing Que, Yilun Liu, Yongqian Sun, Sicheng Wei, Dan Pei, Hailin Li</dc:creator>
    </item>
    <item>
      <title>A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints</title>
      <link>https://arxiv.org/abs/2511.14215</link>
      <description>arXiv:2511.14215v1 Announce Type: new 
Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&amp;V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14215v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malik Muhammad Umer</dc:creator>
    </item>
    <item>
      <title>KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation</title>
      <link>https://arxiv.org/abs/2511.14224</link>
      <description>arXiv:2511.14224v1 Announce Type: new 
Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14224v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anji Li, Mingwei Liu, Zhenxi Chen, Zheng Pei, Zike Li, Dekun Dai, Yanlin Wang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?</title>
      <link>https://arxiv.org/abs/2511.14367</link>
      <description>arXiv:2511.14367v1 Announce Type: new 
Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14367v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dulaji Hidellaarachchi, Sebastian Baltes, John Grundy</dc:creator>
    </item>
    <item>
      <title>Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems</title>
      <link>https://arxiv.org/abs/2511.14435</link>
      <description>arXiv:2511.14435v1 Announce Type: new 
Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14435v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.436.8</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 436, 2025, pp. 80-87</arxiv:journal_reference>
      <dc:creator>Angelo Ferrando (University of Modena,Reggio Emilia)</dc:creator>
    </item>
    <item>
      <title>LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations</title>
      <link>https://arxiv.org/abs/2511.14528</link>
      <description>arXiv:2511.14528v1 Announce Type: new 
Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14528v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiane Ornelas, Allysson Allex Ara\'ujo, J\'ulia Ara\'ujo, Marina Ara\'ujo, Bianca Trinkenreich, Marcos Kalinowski</dc:creator>
    </item>
    <item>
      <title>FHIRconnect: Towards a seamless integration of openEHR and FHIR</title>
      <link>https://arxiv.org/abs/2511.14618</link>
      <description>arXiv:2511.14618v1 Announce Type: new 
Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14618v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Severin Kohler, Jordi Piera Jim\'enez, Michael Anywar, Lars Fuhrmann, Heather Leslie, Maximilian Meixner, Julian Sa{\ss}, Florian K\"archer, Diego Bosc\'a, Birger Haarbrandt, Michael Marschollek, Roland Eils</dc:creator>
    </item>
    <item>
      <title>Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice</title>
      <link>https://arxiv.org/abs/2511.14711</link>
      <description>arXiv:2511.14711v1 Announce Type: new 
Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14711v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaliyah Chang, Mariam Guizani, Brittany Johnson</dc:creator>
    </item>
    <item>
      <title>Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels</title>
      <link>https://arxiv.org/abs/2511.13764</link>
      <description>arXiv:2511.13764v1 Announce Type: cross 
Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13764v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arun Thangamani, Md Asghar Ahmad Shahid, Adam Siemieniuk, Rolf Morel, Renato Golin, Alexander Heinecke</dc:creator>
    </item>
    <item>
      <title>Towards LLM-Based Usability Analysis for Recommender User Interfaces</title>
      <link>https://arxiv.org/abs/2511.14359</link>
      <description>arXiv:2511.14359v1 Announce Type: cross 
Abstract: Usability is a key factor in the effectiveness of recommender systems. However, the analysis of user interfaces is a time-consuming process that requires expertise. Recent advances in multimodal large language models (LLMs) offer promising opportunities to automate such evaluations. In this work, we explore the potential of multimodal LLMs to assess the usability of recommender system interfaces by considering a variety of publicly available systems as examples. We take user interface screenshots from multiple of these recommender platforms to cover both preference elicitation and recommendation presentation scenarios. An LLM is instructed to analyze these interfaces with regard to different usability criteria and provide explanatory feedback. Our evaluation demonstrates how LLMs can support heuristic-style usability assessments at scale to support the improvement of user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14359v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Lubos, Alexander Felfernig, Damian Garber, Viet-Man Le, Thi Ngoc Trang Tran</dc:creator>
    </item>
    <item>
      <title>Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains</title>
      <link>https://arxiv.org/abs/2511.14433</link>
      <description>arXiv:2511.14433v1 Announce Type: cross 
Abstract: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14433v1</guid>
      <category>cs.LO</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.436.6</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 436, 2025, pp. 48-68</arxiv:journal_reference>
      <dc:creator>Diana C. Benjumea (University of Manchester), Marie Farrell (University of Manchester), Louise A. Dennis (University of Manchester)</dc:creator>
    </item>
    <item>
      <title>Towards A Catalogue of Requirement Patterns for Space Robotic Missions</title>
      <link>https://arxiv.org/abs/2511.14438</link>
      <description>arXiv:2511.14438v1 Announce Type: cross 
Abstract: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14438v1</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.436.12</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 436, 2025, pp. 136-166</arxiv:journal_reference>
      <dc:creator>Mahdi Etumi (University of Manchester), Hazel M. Taylor (University of Manchester), Marie Farrell (University of Manchester)</dc:creator>
    </item>
    <item>
      <title>UniDebugger: Hierarchical Multi-Agent Framework for Unified Software Debugging</title>
      <link>https://arxiv.org/abs/2404.17153</link>
      <description>arXiv:2404.17153v3 Announce Type: replace 
Abstract: Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, FixAgent, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that FixAgent significantly outperforms state-of-the-art repair methods, fixing 1.25$\times$ to 2.56$\times$ bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on https://github.com/AcceptePapier/UniDebugger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17153v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Cheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>Token Interdependency Parsing (Tipping) -- Fast and Accurate Log Parsing</title>
      <link>https://arxiv.org/abs/2408.00645</link>
      <description>arXiv:2408.00645v2 Announce Type: replace 
Abstract: In the last decade, an impressive increase in software adaptions has led to a surge in log data production, making manual log analysis impractical and establishing the necessity for automated methods. Conversely, most automated analysis tools include a component designed to separate log templates from their parameters, commonly referred to as a "log parser". This paper aims to introduce a new fast and accurate log parser, named "Tipping". Tipping combines rule-based tokenizers, interdependency token graphs, strongly connected components, and various techniques to ensure rapid, scalable, and precise log parsing. Furthermore, Tipping is parallelized and capable of running on multiple processing cores with close to linear efficiency. We evaluated Tipping against other state-of-the-art log parsers in terms of accuracy, performance, and the downstream task of anomaly detection. Accordingly, we found that Tipping outperformed existing methods in accuracy and performance in our evaluations. More in-depth, Tipping can parse 11 million lines of logs in less than 20 seconds on a laptop machine. Furthermore, we re-implemented a parallelized version of the past IpLom algorithm to demonstrate the effect of parallel processing, and it became the second-fastest parser. As logs keep growing in volume and complexity, the software engineering community needs to ensure automated log analysis tools keep up with the demand, being capable of efficiently handling massive volumes of logs with high accuracy. Tipping's robustness, versatility, efficiency, and scalability make it a viable tool for the modern automated log analysis task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00645v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayan Hashemi, Mika M\"antyl\"a</dc:creator>
    </item>
    <item>
      <title>Generating Streamlining Constraints with Large Language Models</title>
      <link>https://arxiv.org/abs/2408.10268</link>
      <description>arXiv:2408.10268v3 Announce Type: replace 
Abstract: Streamlining constraints (or streamliners, for short) narrow the search space, enhancing the speed and feasibility of solving complex constraint satisfaction problems. Traditionally, streamliners were crafted manually or generated through systematically combined atomic constraints with high-effort offline testing. Our approach utilizes the creativity of Large Language Models (LLMs) to propose effective streamliners for problems specified in the MiniZinc constraint programming language and integrates feedback to the LLM with quick empirical tests for validation. Evaluated across seven diverse constraint satisfaction problems, our method achieves substantial runtime reductions. We compare the results to obfuscated and disguised variants of the problem to see whether the results depend on LLM memorization. We also analyze whether longer off-line runs improve the quality of streamliners and whether the LLM can propose good combinations of streamliners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10268v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1613/jair.1.18965</arxiv:DOI>
      <arxiv:journal_reference>F. Voboril, V. P. Ramaswamy, S. Szeider, Generating Streamlining Constraints with Large Language Models, Journal of Artificial Intelligence Research, volume 84, pages 16:1-16:19, 2025</arxiv:journal_reference>
      <dc:creator>Florentina Voboril, Vaidyanathan Peruvemba Ramaswamy, Stefan Szeider</dc:creator>
    </item>
    <item>
      <title>Assessing UML Diagrams by GPT: Implications for Education</title>
      <link>https://arxiv.org/abs/2412.17200</link>
      <description>arXiv:2412.17200v4 Announce Type: replace 
Abstract: In software engineering (SE) research and practice, UML is well known as an essential modeling methodology for requirements analysis and software modeling in both academia and industry. In particular, fundamental knowledge of UML modeling and practice in creating high-quality UML diagrams are included in SE-relevant courses in the undergraduate programs of many universities. This leads to a time-consuming and labor-intensive task for educators to review and grade a large number of UML diagrams created by the students. Recent advances in generative AI techniques, such as GPT, have paved new ways to automate many SE tasks. However, current research or tools seldom explore the capabilities of GPT in evaluating the quality of UML diagrams. This paper aims to investigate the feasibility and performance of GPT in assessing the quality of UML use case diagrams, class diagrams, and sequence diagrams. First, 11 evaluation criteria with grading details were proposed for these UML diagrams. Next, a series of experiments was designed and conducted on 40 students' UML modeling reports to explore the performance of GPT in evaluating and grading these UML diagrams. The research findings reveal that GPT can complete this assessment task, but it cannot replace human experts yet. Meanwhile, there are five evaluation discrepancies between GPT and human experts. These discrepancies vary in the use of different evaluation criteria in different types of UML diagrams, presenting GPT's strengths and weaknesses in this automatic evaluation task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17200v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Wang, Beian Wang, Peng Liang, Jie Liang</dc:creator>
    </item>
    <item>
      <title>GraphCodeAgent: Dual Graph-Guided LLM Agent for Retrieval-Augmented Repo-Level Code Generation</title>
      <link>https://arxiv.org/abs/2504.10046</link>
      <description>arXiv:2504.10046v2 Announce Type: replace 
Abstract: Writing code requires significant time and effort in software development. To automate this process, researchers have made substantial progress for code generation. Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts. Existing studies attempt to use retrieval-augmented code generation (RACG) approaches to mitigate this demand. However, there is a gap between natural language (NL) requirements and programming implementations. This results in the failure to retrieve the relevant code of these fine-grained subtasks. To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations. Our approach constructs two interconnected graphs: a Requirement Graph (RG) to model requirement relations of code snippets within the repository, as well as the relations between the target requirement and the requirements of these code snippets, and a Structural-Semantic Code Graph (SSCG) to capture the repository's intricate code dependencies. Guided by this, an LLM-powered agent performs multi-hop reasoning to systematically retrieve all context code snippets, including implicit and explicit code snippets, even if they are not explicitly expressed in requirements. We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval. Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10046v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Li, Xianjie Shi, Kechi Zhang, Ge Li, Zhi Jin, Lei Li, Huangzhao Zhang, Jia Li, Fang Liu, Yuwei Zhang, Zhengwei Tao, Yihong Dong, Yuqi Zhu, Chongyang Tao</dc:creator>
    </item>
    <item>
      <title>Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges</title>
      <link>https://arxiv.org/abs/2506.04418</link>
      <description>arXiv:2506.04418v2 Announce Type: replace 
Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in practice, yet remain underrepresented in automated repair. Existing techniques and benchmarks pre-dominantly target single-hunk scenarios, overlooking the added complexity of coordinating semantically related changes across the codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches derived from 372 real-world defects. We propose hunk divergence, a metric that quantifies the variation among edits in a patch by capturing lexical, structural, and file-level differences, while incorporating the number of hunks involved. We further define spatial proximity, a classification that models how hunks are spatially distributed across the program hierarchy. Our empirical study spanning six LLMs reveals that model success rates decline with increased divergence and spatial dispersion. Notably, when using the LLM alone, no model succeeds in the most dispersed Fragment class. These findings highlight a critical gap in LLM capabilities and motivate divergence-aware repair strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04418v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noor Nashid, Daniel Ding, Keheliya Gallaba, Ahmed E. Hassan, Ali Mesbah</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance</title>
      <link>https://arxiv.org/abs/2506.23535</link>
      <description>arXiv:2506.23535v2 Announce Type: replace 
Abstract: Safety-critical systems are engineered systems whose failure or malfunction could result in catastrophic consequences. The software development for safety-critical systems necessitates rigorous engineering practices and adherence to certification standards like DO-178C for avionics. DO-178C is a guidance document which requires compliance to well-defined software coding standards like MISRA C++ to enforce coding guidelines that prevent the use of ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have demonstrated significant capabilities in automatic code generation across a wide range of programming languages, including C++. Despite their impressive performance, code generated by LLMs in safety-critical domains must be carefully analyzed for conformance to MISRA C++ coding standards. In this paper, I have conducted a comparative analysis of the C++ code generated by popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and Microsoft Copilot for compliance with MISRA C++. The study revealed that none of the evaluated LLMs generated MISRA-compliant code despite clear prompts, with DeepSeek showing the fewest violations and Meta AI the most. While all models could correct individual violations when explicitly instructed, only ChatGPT consistently identified and resolved all targeted rule violations across complete code snippets, whereas others achieved partial success. Overall, LLMs show promise as aids for initial code generation, but they are not yet dependable for producing fully MISRA-compliant code required in safety-critical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23535v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2025.3633086</arxiv:DOI>
      <dc:creator>Malik Muhammad Umer</dc:creator>
    </item>
    <item>
      <title>SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks</title>
      <link>https://arxiv.org/abs/2511.02352</link>
      <description>arXiv:2511.02352v3 Announce Type: replace 
Abstract: AI coding agents have shown great progress on Python software engineering benchmarks like SWE-Bench, and for other languages like Java and C in benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language ranking #5 in the TIOBE index -- remains absent from such benchmarks. We introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for C# featuring 150 instances from 17 repositories. Evaluating identical model-agent configurations across languages reveals a significant performance gap: while 70% of Python tasks in SWE-Bench Verified are solved, only 40% of our C# tasks are resolved. We open-source SWE-Sharp-Bench and our entire curation pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02352v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sanket Mhatre, Yasharth Bajpai, Sumit Gulwani, Emerson Murphy-Hill, Gustavo Soares</dc:creator>
    </item>
    <item>
      <title>Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis</title>
      <link>https://arxiv.org/abs/2511.08644</link>
      <description>arXiv:2511.08644v2 Announce Type: replace 
Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08644v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Punit Kumar, Asif Imran, Tevfik Kosar</dc:creator>
    </item>
    <item>
      <title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
      <link>https://arxiv.org/abs/2508.01249</link>
      <description>arXiv:2508.01249v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \&amp; data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01249v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiran Wang, Yang Liu, Yunfei Lu, Yifeng Cai, Hongbo Chen, Qingyou Yang, Jie Zhang, Jue Hong, Ye Wu</dc:creator>
    </item>
    <item>
      <title>Geospatial Machine Learning Libraries</title>
      <link>https://arxiv.org/abs/2510.02572</link>
      <description>arXiv:2510.02572v2 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have been supported by the emergence of domain-specific software libraries, enabling streamlined workflows and increased reproducibility. For geospatial machine learning (GeoML), the availability of Earth observation data has outpaced the development of domain libraries to handle its unique challenges, such as varying spatial resolutions, spectral properties, temporal cadence, data coverage, coordinate systems, and file formats. This chapter presents a comprehensive overview of GeoML libraries, analyzing their evolution, core functionalities, and the current ecosystem. It also introduces popular GeoML libraries such as TorchGeo, eo-learn, and Raster Vision, detailing their architecture, supported data types, and integration with ML frameworks. Additionally, it discusses common methodologies for data preprocessing, spatial--temporal joins, benchmarking, and the use of pretrained models. Through a case study in crop type mapping, it demonstrates practical applications of these tools. Best practices in software design, licensing, and testing are highlighted, along with open challenges and future directions, particularly the rise of foundation models and the need for governance in open-source geospatial software. Our aim is to guide practitioners, developers, and researchers in navigating and contributing to the rapidly evolving GeoML landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02572v2</guid>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam J. Stewart, Caleb Robinson, Arindam Banerjee</dc:creator>
    </item>
    <item>
      <title>VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization</title>
      <link>https://arxiv.org/abs/2511.11896</link>
      <description>arXiv:2511.11896v2 Announce Type: replace-cross 
Abstract: The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.
  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11896v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youpeng Li, Fuxun Yu, Xinda Wang</dc:creator>
    </item>
  </channel>
</rss>
