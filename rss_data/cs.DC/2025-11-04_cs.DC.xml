<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios</title>
      <link>https://arxiv.org/abs/2511.00038</link>
      <description>arXiv:2511.00038v1 Announce Type: new 
Abstract: Drone fleets equipped with onboard cameras, computer vision, and Deep Neural Network (DNN) models present a powerful paradigm for real-time spatio-temporal decision-making. In wildfire response, such drones play a pivotal role in monitoring fire dynamics, supporting firefighter coordination, and facilitating safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV framework designed for scalable, resilient, and collaborative escape route planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration architecture comprising service drones (SDs) and coordinator drones (CDs), each performing specialized roles. SDs survey fire-affected areas, detect stranded individuals using onboard edge accelerators running fire detection and human pose identification DNN models, and issue requests for assistance. CDs, equipped with lightweight data stores such as Apache IoTDB, dynamically generate optimal ground escape routes and monitor firefighter movements along these routes. The framework proposes a collaborative path-planning approach based on a weighted A* search algorithm, where CDs compute context-aware escape paths. AeroResQ further incorporates intelligent load-balancing and resilience mechanisms: CD failures trigger automated data redistribution across IoTDB replicas, while SD failures initiate geo-fenced re-partitioning and reassignment of spatial workloads to operational SDs. We evaluate AeroResQ using realistic wildfire emulated setup modeled on recent Southern California wildfires. Experimental results demonstrate that AeroResQ achieves a nominal end-to-end latency of &lt;=500ms, much below the 2s request interval, while maintaining over 98% successful task reassignment and completion, underscoring its feasibility for real-time, on-field deployment in emergency response and firefighter safety operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00038v1</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Suman Raj, Radhika Mittal, Rajiv Mayani, Pawel Zuk, Anirban Mandal, Michael Zink, Yogesh Simmhan, Ewa Deelman</dc:creator>
    </item>
    <item>
      <title>COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement</title>
      <link>https://arxiv.org/abs/2511.00263</link>
      <description>arXiv:2511.00263v1 Announce Type: new 
Abstract: COOL (Chen'21) is an error-free, information-theoretically secure Byzantine agreement (BA) protocol proven to achieve BA consensus in the synchronous setting for an $\ell$-bit message, with a total communication complexity of $O(\max\{n\ell, nt \log q\})$ bits, four communication rounds in the worst case, and a single invocation of a binary BA, under the optimal resilience assumption $n \geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may behave dishonestly. Here, $q$ denotes the alphabet size of the error correction code used in the protocol.
  In this work, we present an adaptive variant of COOL, called OciorACOOL, which achieves error-free, information-theoretically secure BA consensus in the asynchronous setting with total $O(\max\{n\ell, n t \log q\})$ communication bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA protocol, still under the optimal resilience assumption $n \geq 3t + 1$. Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$ error-correction encoding and decoding as COOL, with $k=t/3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00263v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chen</dc:creator>
    </item>
    <item>
      <title>Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2511.00294</link>
      <description>arXiv:2511.00294v1 Announce Type: new 
Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a flexible and scalable infrastructure. This paradigm can minimize latency by processing data closer to the source at the edge while leveraging the vast computational power of the cloud for more intensive tasks. In this context, module application placement requires strategic allocation plans that align user demands with infrastructure constraints, aiming for efficient resource use. Therefore, we propose Tetris, an application placement strategy that utilizes a heuristic algorithm to distribute computational services across edge and cloud resources efficiently. Tetris prioritizes services based on SLA urgencies and resource efficiency to avoid system overloading. Our results demonstrate that Tetris reduces SLA violations by approximately 76% compared to the baseline method, which serves as a reference point for benchmarking performance in this scenario. Therefore, Tetris offers an effective placement approach for managing latency-sensitive applications in Edge-Cloud Continuum environments, enhancing Quality of Service (QoS) for users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00294v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Almeida, Maycon Peixoto</dc:creator>
    </item>
    <item>
      <title>EPARA: Parallelizing Categorized AI Inference in Edge Clouds</title>
      <link>https://arxiv.org/abs/2511.00603</link>
      <description>arXiv:2511.00603v1 Announce Type: new 
Abstract: With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00603v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie</dc:creator>
    </item>
    <item>
      <title>AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs</title>
      <link>https://arxiv.org/abs/2511.00796</link>
      <description>arXiv:2511.00796v1 Announce Type: new 
Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is essential to democratize this advanced technique. One promising but challenging approach is to deploy such a computational workflow over heterogeneous GPUs. Unlike conventional large-scale LLM pretraining, RL training generally decomposes into three coupled stages, i.e., rollout generation, reward computation, and policy/value updates, which exhibit markedly different compute intensities, memory footprints, and communication patterns. Recent research shows that fully asynchronous RL training can disaggregate these stages across disjoint hardware pools without sacrificing training stability, creating a great opportunity for real-world heterogeneous deployment. To this end, we present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that effectively schedules how to execute rollout generation and policy model training over heterogeneous GPUs while enforcing data staleness bounds. Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to select per-stage parallelization strategies and workload assignments given a resource budget, and (ii) a graph-partitioning step that allocates heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound generation and compute-bound optimization to more cost-efficient resources and balances their producer-consumer interactions to avoid both idleness and stale rollout trajectories. On the mathematical reasoning task with various model scales (1.5B, 7B, and 14B), compared to homogeneous deployments of state-of-the-art asynchronous RL systems: (i) When maintaining the same total budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When achieving the same training throughput, AReaL-Hex results in up to 1.46x reduction in training cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00796v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, Binhang Yuan</dc:creator>
    </item>
    <item>
      <title>FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs</title>
      <link>https://arxiv.org/abs/2511.00807</link>
      <description>arXiv:2511.00807v1 Announce Type: new 
Abstract: The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query traffic and serving patterns. LLM queries running on advanced GPUs during a high-emission hour at one location can lead to significantly higher carbon footprints versus same queries running on mid-level GPUs at a low-emission time and location. By observing LLM serving requirements and leveraging spatiotemporal computation flexibility, we consider the joint routing and scheduling problem, and propose FREESH to cooperatively run a group of data centers while minimizing user-specified carbon or energy objectives. FREESH identifies the optimal configurations of balanced load serving by matching distinct GPU instance's power-throughput characteristics with predictable LLM query length and workloads. To ensure both latency and fairness requirements, FREESH identifies optimized parallelism and query routing schedules together with dynamic GPU frequency scaling for power saving, and Least-Laxity-First (LLF) serving strategy for query scheduling. During the 1-hour serving on production workloads, FREESH reduces energy by 28.6% and emissions by 45.45% together with improvements in SLO attainment and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00807v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen</dc:creator>
    </item>
    <item>
      <title>Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver</title>
      <link>https://arxiv.org/abs/2511.01001</link>
      <description>arXiv:2511.01001v1 Announce Type: new 
Abstract: Current climate change has posed a grand challenge in the field of numerical modeling due to its complex, multiscale dynamics. In hydrological modeling, the increasing demand for high-resolution, real-time simulations has led to the adoption of GPU-accelerated platforms and performance portable programming frameworks such as Kokkos. In this work, we present a comprehensive performance study of the SERGHEI-SWE solver, a shallow water equations code, across four state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs, demonstrating consistent scalability with a speedup of 32 and an efficiency upwards of 90\% for most almost all the test range. Roofline analysis reveals that memory bandwidth is the dominant performance bottleneck, with key solver kernels residing in the memory-bound region. To evaluate performance portability, we apply both harmonic and arithmetic mean-based metrics while varying problem size. Results indicate that while SERGHEI-SWE achieves portability across devices with tuned problem sizes (&lt;70\%), there is room for kernel optimization within the solver with more granular control of the architecture specifically by using Kokkos teams and architecture specific tunable parameters. These findings position SERGHEI-SWE as a robust, scalable, and portable simulation tool for large-scale geophysical applications under evolving HPC architectures with potential to enhance its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01001v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SBAC-PAD66369.2025.00025</arxiv:DOI>
      <dc:creator>Johansell Villalobos, Daniel Caviedes-Voulli\`eme, Silvio Rizzi, Esteban Meneses</dc:creator>
    </item>
    <item>
      <title>Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2511.01127</link>
      <description>arXiv:2511.01127v1 Announce Type: new 
Abstract: Traditional task offloading strategies in edge computing often rely on static heuristics or data-intensive machine learning models, which are not always suitable for highly dynamic and resource-constrained environments. In this paper, we propose a novel task-offloading framework based on Spiking Neural Networks inspired by the efficiency and adaptability of biological neural systems. Our approach integrates an SNN-based decision module into edge nodes to perform real-time, energy-efficient task orchestration. We evaluate the model under various IoT workload scenarios using a hybrid simulation environment composed of YAFS and Brian2. The results demonstrate that our SNN-based framework significantly reduces task processing latency and energy consumption while improving task success rates. Compared to traditional heuristic and ML-based strategies, our model achieves up to 26% lower latency, 32% less energy consumption, and 25\% higher success rate under high-load conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01127v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Diniz Rossi</dc:creator>
    </item>
    <item>
      <title>Scalable Maxflow Processing for Dynamic Graphs</title>
      <link>https://arxiv.org/abs/2511.01235</link>
      <description>arXiv:2511.01235v1 Announce Type: new 
Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and combinatorial optimization, aiming to determine the largest possible flow from a designated source node to a sink node within a capacitated flow network. It has extensive applications across diverse domains such as computer networking, transportation systems, and image segmentation. The objective is to maximize the total throughput while respecting edge capacity constraints and maintaining flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the Push--Relabel algorithm is particularly notable for its efficiency and suitability for parallelization, owing to its localized vertex-based operations. This property has motivated extensive research into GPU-accelerated Max-Flow computation, leveraging the high degree of parallelism inherent to modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of incrementally recomputing the maximum flow of a dynamic graph following a batch of edge updates. In addition, we introduce a high-performance static GPU algorithm designed for efficiently computing the initial Max-Flow on static graphs. We further describe a series of CUDA-specific implementation optimizations that enhance performance, scalability, and memory efficiency on GPU platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01235v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shruthi Kannappan, Ashwina Kumar, Rupesh Nasre</dc:creator>
    </item>
    <item>
      <title>Design of quasi phase matching crystal based on differential gray wolf algorithm</title>
      <link>https://arxiv.org/abs/2511.01255</link>
      <description>arXiv:2511.01255v1 Announce Type: new 
Abstract: This paper focuses on the key problem in the development of nonlinear optical technology, the performance optimization of aperiodically polarized crystals. The performance of the crystal depends on the precise control of the micro distribution of crystal domains, but its optimization belongs to the high-dimensional discrete combination "NP hard" problem. The traditional algorithm has the bottleneck of slow convergence and easy to fall into local optimization, while the heuristic methods such as genetic algorithm are limited by the CPU serial calculation and inefficient. In order to solve the above challenges, this paper proposes the fusion scheme of hwsda hybrid optimization algorithm and GPU parallel acceleration technology: the differential evolution algorithm (DE) is used to realize the global search, and the gray wolf optimization algorithm (GWO) is used to strengthen the local search and convergence speed, and the two coordinate to balance the global and local optimization requirements; At the same time, it relies on GPU multi-core architecture to realize thread level parallel computing and improve optimization efficiency. This scheme effectively breaks through the optimization problem of high-dimensional discrete space, improves the accuracy of crystal domain control, improves the efficiency of quasi phase matching design by hundreds to thousands of times compared with traditional CPU serial computing, provides a new paradigm for the design of complex nonlinear optical devices, and helps promote the performance breakthrough and industrial application of related devices in the fields of quantum optics and laser processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01255v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Chen, ZiHua Zheng, JingHua Sun</dc:creator>
    </item>
    <item>
      <title>Transformer-Based Sparse CSI Estimation for Non-Stationary Channels</title>
      <link>https://arxiv.org/abs/2511.01333</link>
      <description>arXiv:2511.01333v1 Announce Type: new 
Abstract: Accurate and efficient estimation of Channel State Information (CSI) is critical for next-generation wireless systems operating under non-stationary conditions, where user mobility, Doppler spread, and multipath dynamics rapidly alter channel statistics. Conventional pilot aided estimators incur substantial overhead, while deep learning approaches degrade under dynamic pilot patterns and time varying fading. This paper presents a pilot-aided Flash-Attention Transformer framework that unifies model-driven pilot acquisition with data driven CSI reconstruction through patch-wise self-attention and a physics aware composite loss function enforcing phase alignment, correlation consistency, and time frequency smoothness. Under a standardized 3GPP NR configuration, the proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB in phase invariant normalized mean-square error (NMSE) with markedly lower bit-error rate (BER), while reducing pilot overhead by 16 times. These results demonstrate that attention based architectures enable reliable CSI recovery and enhanced spectral efficiency without compromising link quality, addressing a fundamental bottleneck in adaptive, low-overhead channel estimation for non-stationary 5G and beyond-5G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01333v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Hassan Rizwan, Sagnik Bhattacharya, Muhammad Ali Jamshed, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Gradient Clock Synchronization with Practically Constant Local Skew</title>
      <link>https://arxiv.org/abs/2511.01420</link>
      <description>arXiv:2511.01420v1 Announce Type: new 
Abstract: Gradient Clock Synchronization (GCS) is the task of minimizing the local skew, i.e., the clock offset between neighboring clocks, in a larger network. While asymptotically optimal bounds are known, from a practical perspective they have crucial shortcomings:
  - Local skew bounds are determined by upper bounds on offset estimation that need to be guaranteed throughout the entire lifetime of the system.
  - Worst-case frequency deviations of local oscillators from their nominal rate are assumed, yet frequencies tend to be much more stable in the (relevant) short term.
  State-of-the-art deployed synchronization methods adapt to the true offset measurement and frequency errors, but achieve no non-trivial guarantees on the local skew.
  In this work, we provide a refined model and novel analysis of existing techniques for solving GCS in this model. By requiring only stability of measurement and frequency errors, we can circumvent existing lower bounds, leading to dramatic improvements under very general conditions. For example, if links exhibit a uniform worst-case estimation error of $\Delta$ and a change in estimation errors of $\delta\ll \Delta$ on relevant time scales, we bound the local skew by $O(\Delta+\delta \log D)$ for networks of diameter $D$, effectively ``breaking'' the established $\Omega(\Delta\log D)$ lower bound, which holds when $\delta=\Delta$. Similarly, we show how to limit the influence of local oscillators on $\delta$ to scale with the change of frequency of an individual oscillator on relevant time scales, rather than a worst-case bound over all oscillators and the lifetime of the system.
  Moreover, we show how to ensure self-stabilization in this challenging setting. Last, but not least, we extend all of our results to the scenario of external synchronization, at the cost of a limited increase in stabilization time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01420v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Lenzen</dc:creator>
    </item>
    <item>
      <title>Adaptive Multidimensional Quadrature on Multi-GPU Systems</title>
      <link>https://arxiv.org/abs/2511.01573</link>
      <description>arXiv:2511.01573v1 Announce Type: new 
Abstract: We introduce a distributed adaptive quadrature method that formulates multidimensional integration as a hierarchical domain decomposition problem on multi-GPU architectures. The integration domain is recursively partitioned into subdomains whose refinement is guided by local error estimators. Each subdomain evolves independently on a GPU, which exposes a significant load imbalance as the adaptive process progresses. To address this challenge, we introduce a decentralised load redistribution schemes based on a cyclic round-robin policy. This strategy dynamically rebalance subdomains across devices through non-blocking, CUDA-aware MPI communication that overlaps with computation. The proposed strategy has two main advantages compared to a state-of-the-art GPU-tailored package: higher efficiency in high dimensions; and improved robustness w.r.t the integrand regularity and the target accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01573v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melanie Tonarelli, Simone Riva, Pietro Benedusi, Fabrizio Ferrandi, Rolf Krause</dc:creator>
    </item>
    <item>
      <title>LARK - Linearizability Algorithms for Replicated Keys in Aerospike</title>
      <link>https://arxiv.org/abs/2511.01843</link>
      <description>arXiv:2511.01843v1 Announce Type: new 
Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a synchronous replication protocol that achieves linearizability while minimizing latency and infrastructure cost, at significantly higher availability than traditional quorum-log consensus. LARK introduces Partition Availability Conditions (PAC) that reason over the entire database cluster rather than fixed replica sets, improving partition availability under independent failures by roughly 3x when tolerating one failure and 10x when tolerating two. Unlike Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs, enabling immediate partition readiness after leader changes -- with at most a per-key duplicate-resolution round trip when the new leader lacks the latest copy. Under equal storage budgets -- where both systems maintain only f+1 data copies to tolerate f failures -- LARK continues committing through data-node failures while log-based protocols must pause commits for replica rebuilding. These properties also enable zero-downtime rolling restarts even when maintaining only two copies. We provide formal safety arguments and a TLA+ specification, and we demonstrate through analysis and experiments that LARK achieves significant availability gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01843v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Goodng, Kevin Porter, Thomas Lopatic, Ashish Shinde, Sunil Sayyaparaju, Srinivasan Seshadri, V. Srinivasan</dc:creator>
    </item>
    <item>
      <title>Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</title>
      <link>https://arxiv.org/abs/2511.00037</link>
      <description>arXiv:2511.00037v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00037v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riya Gupta, Alexander Chowdhury, Sahil Nalawade</dc:creator>
    </item>
    <item>
      <title>Fix: externalizing network I/O in serverless computing</title>
      <link>https://arxiv.org/abs/2511.00205</link>
      <description>arXiv:2511.00205v1 Announce Type: cross 
Abstract: We describe a system for serverless computing where users, programs,
  and the underlying platform share a common representation of a
  computation: a deterministic procedure, run in an environment
  of well-specified data or the outputs of other computations. This
  representation externalizes I/O: data movement over the network is
  performed exclusively by the platform. Applications can describe the
  precise data needed at each stage, helping the provider schedule
  tasks and network transfers to reduce starvation. The design
  suggests an end-to-end argument for outsourced computing, shifting
  the service model from ``pay-for-effort'' to ``pay-for-results.''</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00205v1</guid>
      <category>cs.OS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769387</arxiv:DOI>
      <dc:creator>Yuhan Deng, Akshay Srivatsan, Sebastian Ingino, Francis Chua, Yasmine Mitchell, Matthew Vilaysack, Keith Winstein</dc:creator>
    </item>
    <item>
      <title>LongCat-Flash-Omni Technical Report</title>
      <link>https://arxiv.org/abs/2511.00279</link>
      <description>arXiv:2511.00279v1 Announce Type: cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00279v1</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Meituan LongCat Team, Bairui Wang,  Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing, Xuezhi Cao, Xunliang Cai, Yang Yang, Yanli Tan, Yao Yao, Yerui Sun, Yi Chen, Yifan Lu, Yin Gong, Yining Zhang, Yitian Chen, Yiyang Gan, Yuchen Tang, Yuchen Xie, Yueqian Wang, Yuewen Zheng, Yufei Zhang, Yufeng Zhong, Yulei Qian, Yuqi Peng, Yuwei Jiang, Zeyang Hu, Zheng Zhang, Zhengkun Tian, Zhiqing Hong, Zhixiong Zeng, Zhuqi Mi, Ziran Li, Ziwen Wang, Ziyi Zhao, Ziyuan Zhuang, Zizhe Zhao</dc:creator>
    </item>
    <item>
      <title>Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems</title>
      <link>https://arxiv.org/abs/2511.00336</link>
      <description>arXiv:2511.00336v1 Announce Type: cross 
Abstract: The rapid growth of Internet of Medical Things (IoMT) devices has resulted in significant security risks, particularly the risk of malware attacks on resource-constrained devices. Conventional deep learning methods are impractical due to resource limitations, while Federated Learning (FL) suffers from high communication overhead and vulnerability to non-IID (heterogeneous) data. In this paper, we propose a split learning (SL) based framework for IoT malware detection through image-based classification. By dividing the neural network training between the clients and an edge server, the framework reduces computational burden on resource-constrained clients while ensuring data privacy. We formulate a joint optimization problem that balances computation cost and communication efficiency by using a game-theoretic approach for attaining better training performance. Experimental evaluations show that the proposed framework outperforms popular FL methods in terms of accuracy (+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less resource consumption (33.83%). These results establish the potential of SL as a scalable and secure paradigm for next-generation IoT security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00336v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization</title>
      <link>https://arxiv.org/abs/2511.00592</link>
      <description>arXiv:2511.00592v1 Announce Type: cross 
Abstract: Automatic code optimization remains a difficult challenge, particularly for complex loop nests on modern hardware. This paper investigates a novel approach to code optimization where Large Language Models (LLMs) guide the process through a closed-loop interaction with a compiler. We present ComPilot, an experimental framework that leverages off-the-shelf LLMs, without any task-specific fine-tuning, as interactive optimization agents. ComPilot establishes a feedback loop where an LLM proposes transformations for a given loop nest to a compiler. The compiler attempts the transformations, reporting back legality status and measured speedup or slowdown. The LLM utilizes this concrete feedback to iteratively refine its optimization strategy. Our extensive evaluation across the PolyBench benchmark suite demonstrates the effectiveness of this zero-shot approach. ComPilot achieves geometric mean speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original code. Furthermore, ComPilot demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases. This experimental study demonstrates that general-purpose LLMs can effectively guide the code optimization process when grounded by compiler feedback, opening promising research directions for agentic AI in code optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00592v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Massinissa Merouani, Islem Kara Bernou, Riyadh Baghdadi</dc:creator>
    </item>
    <item>
      <title>TINC: Trusted Intelligent NetChain</title>
      <link>https://arxiv.org/abs/2511.00823</link>
      <description>arXiv:2511.00823v1 Announce Type: cross 
Abstract: Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00823v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Xia, Hu Xia, Isaac Amankona Obiri, Adjei-Arthur Bonsu, Grace Mupoyi Ntuala, Ansu Badjie, Tienin Bole Wilfried, Jiaqin Liu, Lan Ma, Jianbin Gao, Feng Yao</dc:creator>
    </item>
    <item>
      <title>A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems</title>
      <link>https://arxiv.org/abs/2511.00870</link>
      <description>arXiv:2511.00870v1 Announce Type: cross 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve imaging inverse problems and quantify estimation uncertainties, a key requirement in absence of ground-truth data. To improve estimation quality, Plug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to accommodate priors encoded by a denoising neural network. Designing scalable samplers for high-dimensional imaging inverse problems remains a challenge: drawing and storing high-dimensional samples can be prohibitive, especially for high-resolution images. To address this issue, this work proposes a distributed sampler based on approximate data augmentation and PnP-ULA to solve very large problems. The proposed sampler uses lightweight denoising convolutional neural network, to efficiently exploit multiple GPUs on a Single Program Multiple Data architecture. Reconstruction performance and scalability are evaluated on several imaging problems. Communication and computation overheads due to the denoiser are carefully discussed. The proposed distributed approach noticeably combines three very precious qualities: it is scalable, enables uncertainty quantification, for a reconstruction performance comparable to other PnP methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00870v1</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxime Bouton, Pierre-Antoine Thouvenin, Audrey Repetti, Pierre Chainais</dc:creator>
    </item>
    <item>
      <title>Boosting performance of computer vision applications through embedded GPUs on the edge</title>
      <link>https://arxiv.org/abs/2511.01129</link>
      <description>arXiv:2511.01129v1 Announce Type: cross 
Abstract: Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01129v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Diniz Rossi</dc:creator>
    </item>
    <item>
      <title>Real-time Continual Learning on Intel Loihi 2</title>
      <link>https://arxiv.org/abs/2511.01553</link>
      <description>arXiv:2511.01553v1 Announce Type: cross 
Abstract: AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without catastrophic forgetting--remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intel's Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms), and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01553v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke H\"ullermeier, Yulia Sandamirskaya, Mike Davies</dc:creator>
    </item>
    <item>
      <title>Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems</title>
      <link>https://arxiv.org/abs/2511.01583</link>
      <description>arXiv:2511.01583v1 Announce Type: cross 
Abstract: Detecting malware, especially ransomware, is essential to securing today's interconnected ecosystems, including cloud storage, enterprise file-sharing, and database services. Training high-performing artificial intelligence (AI) detectors requires diverse datasets, which are often distributed across multiple organizations, making centralization necessary. However, centralized learning is often impractical due to security, privacy regulations, data ownership issues, and legal barriers to cross-organizational sharing. Compounding this challenge, ransomware evolves rapidly, demanding models that are both robust and adaptable.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, which enables multiple organizations to collaboratively train a ransomware detection model while keeping raw data local and secure. This paradigm is particularly relevant for cybersecurity companies (including both software and hardware vendors) that deploy ransomware detection or firewall systems across millions of endpoints. In such environments, data cannot be transferred outside the customer's device due to strict security, privacy, or regulatory constraints. Although FL applies broadly to malware threats, we validate the approach using the Ransomware Storage Access Patterns (RanSAP) dataset.
  Our experiments demonstrate that FL improves ransomware detection accuracy by a relative 9% over server-local models and achieves performance comparable to centralized training. These results indicate that FL offers a scalable, high-performing, and privacy-preserving framework for proactive ransomware detection across organizational and regulatory boundaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01583v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria</dc:creator>
    </item>
    <item>
      <title>Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?</title>
      <link>https://arxiv.org/abs/2511.01737</link>
      <description>arXiv:2511.01737v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge intelligence, enabling collaborative model training while preserving data privacy across distributed personal devices. However, the inherent volatility of edge environments, characterized by dynamic resource availability and heterogeneous client capabilities, poses significant challenges for achieving high accuracy and fairness in client participation. This paper investigates the fundamental trade-off between model accuracy and fairness in highly volatile edge environments. This paper provides an extensive empirical evaluation of fairness-based client selection algorithms such as RBFF and RBCSF against random and greedy client selection regarding fairness, model performance, and time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This work aims to shed light on the fairness-performance and fairness-speed trade-offs in a volatile edge environment and explore potential future research opportunities to address existing pitfalls in \textit{fair client selection} strategies in FL. Our results indicate that more equitable client selection algorithms, while providing a marginally better opportunity among clients, can result in slower global training in volatile environments\footnote{The code for our experiments can be found at https://github.com/obaidullahzaland/FairFL_FLTA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01737v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Obaidullah Zaland, Feras M. Awaysheh, Sawsan Al Zubi, Abdul Rahman Safi, Monowar Bhuyan</dc:creator>
    </item>
    <item>
      <title>Space-efficient population protocols for exact majority on general graphs</title>
      <link>https://arxiv.org/abs/2508.11384</link>
      <description>arXiv:2508.11384v2 Announce Type: replace 
Abstract: We study exact majority consensus in the population protocol model. In this model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in each time step, a scheduler samples uniformly at random a pair of adjacent nodes to interact. In the exact majority consensus task, each node is given a binary input, and the goal is to design a protocol that almost surely reaches a stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for exact majority in general graphs. First, we give asymptotically tight time lower bounds for general (unbounded space) protocols. Second, we obtain new upper bounds parameterized by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically, we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta} \tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high probability and uses $O\left( \log n \cdot \left( \log\left(\tfrac{\Delta}{\delta}\right) + \log \left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any graph with minimum degree at least $\delta$ and maximum degree at most $\Delta$.
  For regular expander graphs, this matches the optimal space complexity of $\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA 2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of $O(n \log^2 n)$ steps. Finally, we give a new upper bound of $O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a constant-state protocol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11384v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Rybicki, Jakob Solnerzik, Olivier Stietel, Robin Vacus</dc:creator>
    </item>
    <item>
      <title>OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data</title>
      <link>https://arxiv.org/abs/2508.13374</link>
      <description>arXiv:2508.13374v2 Announce Type: replace 
Abstract: Earth observation analytics have the potential to serve many time-sensitive applications. However, due to limited bandwidth and duration of ground-satellite connections, it takes hours or even days to download and analyze data from existing Earth observation satellites, making real-time demands like timely disaster response impossible. Toward real-time analytics, we introduce OrbitChain, a collaborative analytics framework that orchestrates computational resources across multiple satellites in an Earth observation constellation. OrbitChain decomposes analytics applications into microservices and allocates computational resources for time-constrained analysis. A traffic routing algorithm is devised to minimize the inter-satellite communication overhead. OrbitChain adopts a pipeline workflow that completes Earth observation tasks in real-time, facilitates time-sensitive applications and inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain, we implement a hardware-in-the-loop orbital computing testbed. Experiments show that our system can complete up to 60% analytics workload than existing Earth observation analytics framework while reducing the communication overhead by up to 72%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13374v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhouyu Li, Zhijin Yang, Huayue Gu, Xiaojian Wang, Yuchen Liu, Ruozhou Yu</dc:creator>
    </item>
    <item>
      <title>Declarative Data Pipeline for Large Scale ML Services</title>
      <link>https://arxiv.org/abs/2508.15105</link>
      <description>arXiv:2508.15105v3 Announce Type: replace 
Abstract: Modern distributed data processing systems struggle to balance performance, maintainability, and developer productivity when integrating machine learning at scale. These challenges intensify in large collaborative environments due to high communication overhead and coordination complexity. We present a "Declarative Data Pipeline" (DDP) architecture that addresses these challenges while processing billions of records efficiently. Our modular framework seamlessly integrates machine learning within Apache Spark using logical computation units called Pipes, departing from traditional microservice approaches. By establishing clear component boundaries and standardized interfaces, we achieve modularity and optimization without sacrificing maintainability. Enterprise case studies demonstrate substantial improvements: 50% better development efficiency, collaboration efforts compressed from weeks to days, 500x scalability improvement, and 10x throughput gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15105v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Madhu Chandra, Chris Kong</dc:creator>
    </item>
    <item>
      <title>CARMA: Collocation-Aware Resource Manager</title>
      <link>https://arxiv.org/abs/2508.19073</link>
      <description>arXiv:2508.19073v2 Announce Type: replace 
Abstract: GPUs running deep learning (DL) workloads are frequently underutilized. Collocating multiple DL training tasks on the same GPU can improve utilization but introduces two key risks: (1) out-of-memory (OOM) crashes for newly scheduled tasks, and (2) severe performance interference among co-running tasks, which can negate any throughput gains. These issues reduce system robustness, quality of service, and energy efficiency. We present CARMA, a task-level, collocation-aware resource management system for the server-scale. CARMA addresses collocation challenges via (1) fine-grained monitoring and bookkeeping of GPUs and a collocation risk analysis that filters out the high-risk GPUs; (2) task placement policies that cap GPU utilization to avoid OOMs and limit interference; (3) integration of GPU memory need estimators for DL tasks to minimize OOMs during collocation; and (4) a lightweight recovery method that relaunches jobs crashed due to OOMs. Our evaluation on a DL training workload derived from real-world traces shows that CARMA uses GPUs more efficiently by making more informed collocation decisions: for the best-performing collocation policy, CARMA increases GPU streaming multiprocessor (SM) utilization by 54%, the parallelism achieved per SM by 61%, and memory use by 62%. This results in a $\sim$35% and $\sim$15% reduction in the end-to-end execution time (makespan) and GPU energy consumption, respectively, for this workload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19073v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Yousefzadeh-Asl-Miandoab, Reza Karimzadeh, Bulat Ibragimov, Florina M. Ciorba, P{\i}nar T\"oz\"un</dc:creator>
    </item>
    <item>
      <title>A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales</title>
      <link>https://arxiv.org/abs/2510.23993</link>
      <description>arXiv:2510.23993v2 Announce Type: replace 
Abstract: High-speed chemically active flows present significant computational challenges due to their disparate space and time scales, where stiff chemistry often dominates simulation time. While modern supercomputing scientific codes achieve exascale performance by leveraging graphics processing units (GPUs), existing GPU-based compressible combustion solvers face critical limitations in memory management, load balancing, and handling the highly localized nature of chemical reactions. To this end, we present a high-performance compressible reacting flow solver built on the AMReX framework and optimized for multi-GPU settings. Our approach addresses three GPU performance bottlenecks: memory access patterns through column-major storage optimization, computational workload variability via a bulk-sparse integration strategy for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. The solver adapts existing matrix-based chemical kinetics formulations to multigrid contexts. Using representative combustion applications including hydrogen-air detonations and jet in supersonic crossflow configurations, we demonstrate $2-5\times$ performance improvements over initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4 \times$) routines, confirming efficient utilization of GPU memory bandwidth and computational resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23993v2</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Carreon, Jagmohan Singh, Shivank Sharma, Shuzhi Zhang, Venkat Raman</dc:creator>
    </item>
    <item>
      <title>Tight analyses of first-order methods with error feedback</title>
      <link>https://arxiv.org/abs/2506.05271</link>
      <description>arXiv:2506.05271v2 Announce Type: replace-cross 
Abstract: Communication between agents often constitutes a major computational bottleneck in distributed learning. One of the most common mitigation strategies is to compress the information exchanged, thereby reducing communication overhead. To counteract the degradation in convergence associated with compressed communication, error feedback schemes -- most notably $\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we provide a tight analysis of both of these methods. Specifically, we find the Lyapunov function that yields the best possible convergence rate for each method -- with matching lower bounds. This principled approach yields sharp performance guarantees and enables a rigorous, apples-to-apples comparison between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our analysis is carried out in the simplified single-agent setting, which allows for clean theoretical insights and fair comparison of the underlying mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05271v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Berg Thomsen, Adrien Taylor, Aymeric Dieuleveut</dc:creator>
    </item>
  </channel>
</rss>
