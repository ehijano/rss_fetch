<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Chinese Postman to Salesman and Beyond II: Inapproximability and Parameterized Complexity</title>
      <link>https://arxiv.org/abs/2502.18541</link>
      <description>arXiv:2502.18541v1 Announce Type: new 
Abstract: A well-studied continuous model of graphs considers each edge as a continuous unit-length interval of points. In the problem $\delta$-Tour defined within this model, the objective to find a shortest tour that comes within a distance of $\delta$ of every point on every edge. This parameterized problem was introduced in the predecessor to this article and shown to be essentially equivalent to the Chinese Postman problem for $\delta = 0$, to the graphic Travel Salesman Problem (TSP) for $\delta = 1/2$, and close to first Vertex Cover and then Dominating Set for even larger $\delta$. Moreover, approximation algorithms for multiple parameter ranges were provided. In this article, we provide complementing inapproximability bounds and examine the fixed-parameter tractability of the problem. On the one hand, we show the following:
  (1) For every fixed $0 &lt; \delta &lt; 3/2$, the problem $\delta$-Tour is APX-hard, while for every fixed $\delta \geq 3/2$, the problem has no polynomial-time $o(\log{n})$-approximation unless P = NP.
  Our techniques also yield the new result that TSP remains APX-hard on cubic (and even cubic bipartite) graphs.
  (2) For every fixed $0 &lt; \delta &lt; 3/2$, the problem $\delta$-Tour is fixed-parameter tractable (FPT) when parameterized by the length of a shortest tour, while it is W[2]-hard for every fixed $\delta \geq 3/2$ and para-NP-hard for $\delta$ being part of the input.
  On the other hand, if $\delta$ is considered to be part of the input, then an interesting nontrivial phenomenon occurs when $\delta$ is a constant fraction of the number of vertices:
  (3) If $\delta$ is part of the input, then the problem can be solved in time $f(k)n^{O(k)}$, where $k = \lceil n/\delta \rceil$; however, assuming the Exponential-Time Hypothesis (ETH), there is no algorithm that solves the problem and runs in time $f(k)n^{o(k/\log k)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18541v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Frei, Ahmed Ghazy, Tim A. Hartmann, Florian H\"orsch, D\'aniel Marx</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Multi-draft Speculative Decoding</title>
      <link>https://arxiv.org/abs/2502.18779</link>
      <description>arXiv:2502.18779v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18779v1</guid>
      <category>cs.DS</category>
      <category>cs.CL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhengmian Hu, Tong Zheng, Vignesh Viswanathan, Ziyi Chen, Ryan A. Rossi, Yihan Wu, Dinesh Manocha, Heng Huang</dc:creator>
    </item>
    <item>
      <title>On Aggregation Queries over Predicted Nearest Neighbors</title>
      <link>https://arxiv.org/abs/2502.18803</link>
      <description>arXiv:2502.18803v1 Announce Type: new 
Abstract: We introduce Aggregation Queries over Nearest Neighbors (AQNNs), a novel type of aggregation queries over the predicted neighborhood of a designated object. AQNNs are prevalent in modern applications where, for instance, a medical professional may want to compute "the average systolic blood pressure of patients whose predicted condition is similar to a given insomnia patient". Since prediction typically involves an expensive deep learning model or a human expert, we formulate query processing as the problem of returning an approximate aggregate by combining an expensive oracle and a cheaper model (e.g, a simple ML model) to compute the predictions. We design the Sampler with Precision-Recall in Target (SPRinT) framework for answering AQNNs. SPRinT consists of sampling, nearest neighbor refinement, and aggregation, and is tailored for various aggregation functions. It enjoys provable theoretical guarantees, including bounds on sample size and on error in approximate aggregates. Our extensive experiments on medical, e-commerce, and video datasets demonstrate that SPRinT consistently achieves the lowest aggregation error with minimal computation cost compared to its baselines. Scalability results show that SPRinT's execution time and aggregation error remain stable as the dataset size increases, confirming its suitability for large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18803v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carrie Wang, Sihem Amer-Yahia, Laks V. S. Lakshmanan, Reynold Cheng</dc:creator>
    </item>
    <item>
      <title>Optimal Approximate Matrix Multiplication over Sliding Windows</title>
      <link>https://arxiv.org/abs/2502.18830</link>
      <description>arXiv:2502.18830v1 Announce Type: new 
Abstract: We explore the problem of approximate matrix multiplication (AMM) within the sliding window model, where algorithms utilize limited space to perform large-scale matrix multiplication in a streaming manner. This model has garnered increasing attention in the fields of machine learning and data mining due to its ability to handle time sensitivity and reduce the impact of outdated data. However, despite recent advancements, determining the optimal space bound for this problem remains an open question. In this paper, we introduce the DS-COD algorithm for AMM over sliding windows. This novel and deterministic algorithm achieves optimal performance regarding the space-error tradeoff. We provide theoretical error bounds and the complexity analysis for the proposed algorithm, and establish the corresponding space lower bound for the AMM sliding window problem. Additionally, we present an adaptive version of DS-COD, termed aDS-COD, which improves computational efficiency and demonstrates superior empirical performance. Extensive experiments conducted on both synthetic and real-world datasets validate our theoretical findings and highlight the practical effectiveness of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18830v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqi Yao, Mingsong Chen, Cheng Chen</dc:creator>
    </item>
    <item>
      <title>Approximate $2$-hop neighborhoods on incremental graphs: An efficient lazy approach</title>
      <link>https://arxiv.org/abs/2502.19205</link>
      <description>arXiv:2502.19205v1 Announce Type: new 
Abstract: In this work, we propose, analyze and empirically validate a lazy-update approach to maintain accurate approximations of the $2$-hop neighborhoods of dynamic graphs resulting from sequences of edge insertions.
  We first show that under random input sequences, our algorithm exhibits an optimal trade-off between accuracy and insertion cost: it only performs $O(\frac{1}{\varepsilon})$ (amortized) updates per edge insertion, while the estimated size of any vertex's $2$-hop neighborhood is at most a factor $\varepsilon$ away from its true value in most cases, regardless of the underlying graph topology and for any $\varepsilon &gt; 0$.
  As a further theoretical contribution, we explore adversarial scenarios that can force our approach into a worst-case behavior at any given time $t$ of interest. We show that while worst-case input sequences do exist, a necessary condition for them to occur is that the girth of the graph released up to time $t$ be at most $4$.
  Finally, we conduct extensive experiments on a collection of real, incremental social networks of different sizes, which typically have low girth. Empirical results are consistent with and typically better than our theoretical analysis anticipates. This further supports the robustness of our theoretical findings: forcing our algorithm into a worst-case behavior not only requires topologies characterized by a low girth, but also carefully crafted input sequences that are unlikely to occur in practice.
  Combined with standard sketching techniques, our lazy approach proves an effective and efficient tool to support key neighborhood queries on large, incremental graphs, including neighborhood size, Jaccard similarity between neighborhoods and, in general, functions of the union and/or intersection of $2$-hop neighborhoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19205v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Becchetti, Andrea Clementi, Luciano Gual\`a, Luca Pep\`e Sciarria, Alessandro Straziota, Matteo Stromieri</dc:creator>
    </item>
    <item>
      <title>Noisy (Binary) Searching: Simple, Fast and Correct</title>
      <link>https://arxiv.org/abs/2107.05753</link>
      <description>arXiv:2107.05753v4 Announce Type: replace 
Abstract: This work considers the problem of the noisy binary search in a sorted array. The noise is modeled by a parameter $p$ that dictates that a comparison can be incorrect with probability $p$, independently of other queries. We state two types of upper bounds on the number of queries: the worst-case and expected query complexity scenarios. The bounds improve the ones known to date, i.e., our algorithms require fewer queries. Additionally, they have simpler statements, and work for the full range of parameters. All query complexities for the expected query scenarios are tight up to lower order terms. For the problem where the target prior is uniform over all possible inputs, we provide an algorithm with expected complexity upperbounded by $(\log_2 n + \log_2 \delta^{-1} + 3)/I(p)$, where $n$ is the domain size, $0\le p &lt; 1/2$ is the noise ratio, and $\delta&gt;0$ is the failure probability, and $I(p)$ is the information gain function. As a side-effect, we close some correctness issues regarding previous work. Also, en route, we obtain new and improved query complexities for the search generalized to arbitrary graphs. This paper continues and improves the lines of research of Burnashev--Zigangirov [Prob. Per. Informatsii, 1974], Ben-Or and Hassidim [FOCS 2008], Gu and Xu [STOC 2023], and Emamjomeh-Zadeh et al. [STOC 2016], Dereniowski et al. [SOSA@SODA 2019].</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.05753v4</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.STACS.2025.29</arxiv:DOI>
      <arxiv:journal_reference>STACS 2025: 29:1-29:18</arxiv:journal_reference>
      <dc:creator>Dariusz Dereniowski, Aleksander {\L}ukasiewicz, Przemys{\l}aw Uzna\'nski</dc:creator>
    </item>
    <item>
      <title>Subsequence Matching and LCS with Segment Number Constraints</title>
      <link>https://arxiv.org/abs/2407.19796</link>
      <description>arXiv:2407.19796v3 Announce Type: replace 
Abstract: The longest common subsequence (LCS) is a fundamental problem in string processing which has numerous algorithmic studies, extensions, and applications. A sequence $u_1, \ldots, u_f$ of $f$ strings s said to be an ($f$-)segmentation of a string $P$ if $P = u_1 \cdots u_f$. Li et al. [BIBM 2022] proposed a new variant of the LCS problem for given strings $T_1, T_2$ and an integer $f$, which we hereby call the segmental LCS problem (SegLCS), of finding (the length of) a longest string $P$ that has an $f$-segmentation which can be embedded into both $T_1$ and $T_2$. Li et al. [IJTCS-FAW 2024] gave a dynamic programming solution that solves SegLCS in $O(fn_1n_2)$ time with $O(fn_1 + n_2)$ space, where $n_1 = |T_1|$, $n_2 = |T_2|$, and $n_1 \le n_2$. Recently, Banerjee et al. [ESA 2024] presented an algorithm which, for a constant $f \geq 3$, solves SegLCS in $\tilde{O}((n_1n_2)^{1-(1/3)^{f-2}})$ time. In this paper, we deal with SegLCS as well as the problem of segmental subsequence pattern matching, SegE, that asks to determine whether a pattern $P$ of length $m$ has an $f$-segmentation that can be embedded into a text $T$ of length $n$. When $f = 1$, this is equivalent to substring matching, and when $f = |P|$, this is equivalent to subsequence matching. Our focus in this article is the case of general values of $f$, and our main contributions are threefold: (1) $O((mn)^{1-\epsilon})$-time conditional lower bound for SegE under the strong exponential-time hypothesis (SETH), for any constant $\epsilon &gt; 0$. (2) $O(mn)$-time algorithm for SegE. (3) $O(fn_2(n_1 - \ell+1))$-time algorithm for SegLCS where $\ell$ is the solution length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19796v3</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuki Yonemoto, Takuya Mieno, Shunsuke Inenaga, Ryo Yoshinaka, Ayumi Shinohara</dc:creator>
    </item>
    <item>
      <title>AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval</title>
      <link>https://arxiv.org/abs/2404.06004</link>
      <description>arXiv:2404.06004v2 Announce Type: replace-cross 
Abstract: Graph-based approximate nearest neighbor search (ANNS) algorithms work effectively against large-scale vector retrieval. Among such methods, DiskANN achieves good recall-speed tradeoffs using both DRAM and storage. DiskANN adopts product quantization (PQ) to reduce memory usage, which is still proportional to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads compressed vectors to the SSD index. Our method achieves $\sim$10 MB memory usage in query search with billion-scale datasets without critical latency degradation. AiSAQ also reduces the index load time for query search preparation, which enables fast switch between muitiple billion-scale indices.This method can be applied to retrievers of retrieval-augmented generation (RAG) and be scaled out with multiple-server systems for emerging datasets. Our DiskANN-based implementation is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06004v2</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, Jun Deguchi</dc:creator>
    </item>
    <item>
      <title>Differentially Private Release of Israel's National Registry of Live Births</title>
      <link>https://arxiv.org/abs/2405.00267</link>
      <description>arXiv:2405.00267v2 Announce Type: replace-cross 
Abstract: In February 2024, Israel's Ministry of Health released microdata of live births in Israel in 2014. The dataset is based on Israel's National Registry of Live Births and offers substantial value in multiple areas, such as scientific research and policy-making, while providing pure differential privacy guarantee with $\varepsilon = 9.98$ for 2014's mothers and newborns. The release was co-designed by the authors along with stakeholders from both inside and outside the Ministry of Health. This paper presents the methodology used to obtain that release, which, to the best of our knowledge, is the first of its kind in the world. The design process has been challenging and required flexibility and open-mindedness on all sides involved, along with substantial technical innovation. In particular, we introduce new concepts regarding the desiderata from dataset releases in a microdata format, as well as a way to bundle together multiple quantitative desiderata for a differentially private release using the private selection algorithm of Liu and Talwar (STOC 2019). We hope that the experiences reported here will be useful to future differentially private releases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00267v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP61157.2025.00101</arxiv:DOI>
      <dc:creator>Shlomi Hod, Ran Canetti</dc:creator>
    </item>
    <item>
      <title>Adaptive Batch Size for Privately Finding Second-Order Stationary Points</title>
      <link>https://arxiv.org/abs/2410.07502</link>
      <description>arXiv:2410.07502v2 Announce Type: replace-cross 
Abstract: There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) claimed that an $\alpha$-SOSP can be found with $\alpha=O(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{3/7})$, where $n$ is the dataset size, $d$ is the dimension, and $\epsilon$ is the differential privacy parameter. However, a recent analysis revealed an issue in their saddle point escape procedure, leading to weaker guarantees. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method not only corrects this issue but also improves the results for privately finding an SOSP, achieving $\alpha=O(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{1/2})$.
  This improved bound matches the state-of-the-art for finding a FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07502v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daogao Liu, Kunal Talwar</dc:creator>
    </item>
  </channel>
</rss>
