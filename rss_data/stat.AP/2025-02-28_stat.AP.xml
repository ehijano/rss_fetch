<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On window mean survival time with interval-censored data</title>
      <link>https://arxiv.org/abs/2502.20011</link>
      <description>arXiv:2502.20011v1 Announce Type: new 
Abstract: In recent years, cancer clinical trials have increasingly encountered non proportional hazards (NPH) scenarios, particularly with the emergence of immunotherapy. In randomized controlled trials comparing immunotherapy with conventional chemotherapy or placebo, late difference and early crossing survivals scenarios are commonly observed. In such cases, window mean survival time (WMST), the area under the survival curve within a pre-specified interval $[\tau_0, \tau_1]$, has gained increasing attention due to its superior power compared to restricted mean survival time (RMST), the area under the survival curve up to a pre-specified time point. Considering the increasing use of progression-free survival as a co-primary endpoint alongside overall survival, there is a critical need to establish a WMST estimation method for interval-censored data; however, sufficient research has yet to be conducted. To bridge this gap, this study proposes a WMST inference method utilizing one-point imputations and Turnbull's method. Extensive numerical simulations demonstrate that the WMST estimation method using mid-point imputation for interval-censored data exhibits comparable performance to that using Turnbull's method. Since the former facilitates standard error calculation, we adopt it as the standard method. Numerical simulations on two-sample tests confirm that the proposed WMST testing method have higher power than RMST in late difference and early crossing survival scenarios, while having compatible power to the log-rank test under the PH. Furthermore, even when pre-specified $\tau_0$ deviated from the clinically desirable time point, WMST consistently maintains higher power than RMST in late difference and early crossing survivals scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20011v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuto Iijima, Tomotaka Momozaki, Shuji Ando</dc:creator>
    </item>
    <item>
      <title>Recorded Versus Synthetic Ground Motions: A Comparative Analysis of Structural Seismic Responses</title>
      <link>https://arxiv.org/abs/2502.19549</link>
      <description>arXiv:2502.19549v1 Announce Type: cross 
Abstract: This paper presents a comparative analysis of structural seismic responses under two types of ground motion inputs: (i) synthetic motions generated by stochastic ground motion models and (ii) recorded motions from an earthquake database. Five key seismic response metrics - probability distributions, statistical moments, correlations, tail indices, and variance-based global sensitivity indices - are systematically evaluated for two archetypal structures: a 12-story medium-period building and a high-rise long-period tower. Both ground motion datasets are calibrated to a shared response spectrum, ensuring consistency in spectral characteristics, including spectral median, variance, and correlation structure. The analysis incorporates both aleatory uncertainties from ground motion variability and epistemic uncertainties associated with structural parameters, providing a comprehensive comparison of seismic responses. The results demonstrate close agreement in global response characteristics, including distributions, correlations, and sensitivity indices, between synthetic and recorded motions, with differences typically within 15\%. However, significant discrepancies are observed under extreme conditions, particularly in tail behavior, higher-order moments, and drift responses of long-period structures, with differences exceeding 50\%. These discrepancies are attributed to the non-Gaussian features and complex characteristics inherent in recorded motions, which are less pronounced in synthetic datasets. The findings support the use of synthetic ground motions for evaluating global seismic response characteristics, while highlighting their limitations in capturing rare-event behavior and long-period structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19549v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jungho Kim, Maijia Su, Ziqi Wang, Marco Broccardo</dc:creator>
    </item>
    <item>
      <title>Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative Model for Quasar Spectra</title>
      <link>https://arxiv.org/abs/2502.19824</link>
      <description>arXiv:2502.19824v1 Announce Type: cross 
Abstract: This work proposes a scalable probabilistic latent variable model based on Gaussian processes (Lawrence, 2004) in the context of multiple observation spaces. We focus on an application in astrophysics where data sets typically contain both observed spectral features and scientific properties of astrophysical objects such as galaxies or exoplanets. In our application, we study the spectra of very luminous galaxies known as quasars, along with their properties, such as the mass of their central supermassive black hole, accretion rate, and luminosity-resulting in multiple observation spaces. A single data point is then characterized by different classes of observations, each with different likelihoods. Our proposed model extends the baseline stochastic variational Gaussian process latent variable model (GPLVM) introduced by Lalchand et al. (2022) to this setting, proposing a seamless generative model where the quasar spectra and scientific labels can be generated simultaneously using a shared latent space as input to different sets of Gaussian process decoders, one for each observation space. Additionally, this framework enables training in a missing data setting where a large number of dimensions per data point may be unknown or unobserved. We demonstrate high-fidelity reconstructions of the spectra and scientific labels during test-time inference and briefly discuss the scientific interpretations of the results, along with the significance of such a generative model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19824v1</guid>
      <category>astro-ph.GA</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research (TMLR), 2025</arxiv:journal_reference>
      <dc:creator>Vidhi Lalchand, Anna-Christina Eilers</dc:creator>
    </item>
    <item>
      <title>A New Method for High-Resolution Dating of Radiocarbon Data: The Example of the First Three Centuries B.C</title>
      <link>https://arxiv.org/abs/2502.20211</link>
      <description>arXiv:2502.20211v1 Announce Type: cross 
Abstract: Radiocarbon dating poses a challenge in many archaeological contexts due to the limited precision of conventional calibration methods. In this study, we introduce a novel approach to fine-dating that is based on the repeated application of OxCal's R_Simulate function. By constructing extensive reference tables and aggregating measures of central tendency (means and medians), uncalibrated 14C measurements are directly mapped to calendar dates. The method is validated through comprehensive simulations and comparisons with dendrochronologically dated tree rings. Despite challenges in segments of the calibration curve with low gradients, the approach demonstrates that a significant improvement in dating precision is achievable. Limitations and potential avenues for further methodological optimisation are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20211v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian F\"urst</dc:creator>
    </item>
    <item>
      <title>A review of Bayesian sensor-based estimation and uncertainty quantification of aerodynamic flows</title>
      <link>https://arxiv.org/abs/2502.20280</link>
      <description>arXiv:2502.20280v1 Announce Type: cross 
Abstract: Many applications in aerodynamics depend on the use of sensors to estimate the evolving state of the flow. In particular, a wide variety of traditional and learning-based strategies for closed-loop control rely on some knowledge of the aerodynamic state in order to decide on actions. This estimation task is inherently accompanied by uncertainty due to the noisy measurements of sensors or the non-uniqueness of the underlying mapping, and knowledge of this uncertainty can be as important for decision-making as that of the state itself. The tracking of uncertainty is challenged by the often-nonlinear relationship between the sensor measurements and the flow state. For example, a collection of passing vortices leaves a footprint in wall pressure sensors that depends nonlinearly on the strengths and positions of the vortices. In this paper, we will review the recent body of work on flow estimation. We will discuss the basic tools of probability, including sampling and estimation, in the powerful setting of Bayesian inference and demonstrate these tools in static flow estimation examples. We will then proceed to unsteady examples and illustrate the application of sequential estimation, and particularly, the ensemble Kalman filter. Finally, we will discuss uncertainty quantification in neural network approximations of the mappings between sensor measurements and flow states. Recent aerodynamic applications of neural networks have shown that the flow state can be encoded into a very low-dimensional latent space, and we will discuss the implications of this encoding on uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20280v1</guid>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jeff D. Eldredge, Hanieh Mousavi</dc:creator>
    </item>
    <item>
      <title>Embedding Complexity In the Data Representation Instead of In the Model: A Case Study Using Heterogeneous Medical Data</title>
      <link>https://arxiv.org/abs/1802.04233</link>
      <description>arXiv:1802.04233v2 Announce Type: replace 
Abstract: Electronic Health Records have become popular sources of data for secondary research, but their use is hampered by the amount of effort it takes to overcome the sparsity, irregularity, and noise that they contain. Modern learning architectures can remove the need for expert-driven feature engineering, but not the need for expert-driven preprocessing to abstract away the inherent messiness of clinical data. This preprocessing effort is often the dominant component of a typical clinical prediction project. In this work we propose using semantic embedding methods to directly couple the raw, messy clinical data to downstream learning architectures with truly minimal preprocessing. We examine this step from the perspective of capturing and encoding complex data dependencies in the data representation instead of in the model, which has the nice benefit of allowing downstream processing to be done with fast, lightweight, and simple models accessible to researchers without machine learning expertise. We demonstrate with three typical clinical prediction tasks that the highly compressed, embedded data representations capture a large amount of useful complexity, although in some cases the compression is not completely lossless.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.04233v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacek M. Bajor, Diego A. Mesa, Travis J. Osterman, Thomas A. Lasko</dc:creator>
    </item>
    <item>
      <title>Efficient mid-term forecasting of hourly electricity load using generalized additive models</title>
      <link>https://arxiv.org/abs/2405.17070</link>
      <description>arXiv:2405.17070v2 Announce Type: replace 
Abstract: Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, planning and building energy storage systems, and energy trading. While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce. In mid-term load forecasting, capturing the multifaceted characteristics of load, including daily, weekly and annual seasonal patterns, as well as autoregressive effects, weather and holiday impacts, and socio-economic non-stationarities, presents significant modeling challenges. To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines that is enhanced with autoregressive post-processing. This model incorporates smoothed temperatures, Error-Trend-Seasonal (ETS) modeled and persistently forecasted non-stationary socio-economic states, a nuanced representation of effects from vacation periods, fixed date and weekday holidays, and seasonal information as inputs. The proposed model is evaluated using load data from 24 European countries over more than 9 years (2015-2024). This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability. Achieving performance akin to day-ahead Transmission System Operator (TSO) forecasts, with computation times of just a few seconds for several years of hourly data, underscores the potential of the model for practical application in the power system industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17070v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.ST</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Zimmermann, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Joint Mean and Correlation Regression Models for Multivariate Data</title>
      <link>https://arxiv.org/abs/2402.12803</link>
      <description>arXiv:2402.12803v2 Announce Type: replace-cross 
Abstract: We propose a joint mean and correlation regression model for multivariate discrete and (semi-)continuous response data, that simultaneously regresses the mean of each response against a set of covariates, and the correlations between responses against a set of similarity/distance measures. A set of joint estimating equations are formulated to construct an estimator of both the mean regression coefficients and the correlation regression parameters. Under a general setting where the number of responses can tend to infinity, the joint estimator is demonstrated to be consistent and asymptotically normally distributed, with differing rates of convergence due to the mean regression coefficients being heterogeneous across responses. An iterative estimation procedure is developed to obtain parameter estimates in the required (constrained) parameter space. Simulations demonstrate the strong finite sample performance of the proposed estimator in terms of point estimation and inference. We apply the proposed model to a count dataset of 38 Carabidae ground beetle species sampled throughout Scotland, along with information about the environmental conditions of each site and the traits of each species. Results show the relationship between mean abundance and environmental covariates differs across the beetle species, and that beetle total length is important in driving the correlations between species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12803v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Francis K. C. Hui, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Valid Randomization Tests for Monotone Spillover Effects</title>
      <link>https://arxiv.org/abs/2501.02454</link>
      <description>arXiv:2501.02454v2 Announce Type: replace-cross 
Abstract: Randomization tests have gained popularity for causal inference under network interference because they are finite-sample valid with minimal assumptions. However, existing procedures are limited as they primarily focus on the existence of spillovers through sharp null hypotheses on potential outcomes. In this paper, we expand the scope of randomization procedures in network settings by developing new tests for the monotonicity of spillover effects. These tests offer insights into whether spillover effects increase, decrease, or exhibit ``diminishing returns'' along certain network dimensions of interest. Our approach partitions the network into multiple (possibly overlapping) parts and tests a monotone contrast hypothesis in each sub-network. The test decisions can then be aggregated in various ways depending on how each test is constructed. We demonstrate our method by re-analyzing a large-scale policing experiment in Colombia, which reveals evidence of monotonicity related to the ``crime displacement hypothesis''. Our analysis suggests that crime spillovers on a control street increase with the number of nearby streets receiving more intense policing but diminish at higher exposure levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02454v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shunzhuang Huang, Xinran Li, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2502.17371</link>
      <description>arXiv:2502.17371v2 Announce Type: replace-cross 
Abstract: The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality. Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17371v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Marcello Petitta, Cristina Cornaro</dc:creator>
    </item>
    <item>
      <title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
      <link>https://arxiv.org/abs/2502.19086</link>
      <description>arXiv:2502.19086v2 Announce Type: replace-cross 
Abstract: We introduce the use of Gaussian Processes (GPs) for the probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which is both zero-inflated and heavy tailed, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19086v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Damato, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
  </channel>
</rss>
