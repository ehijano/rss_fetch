<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 May 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hardware Accelerators for Autonomous Cars: A Review</title>
      <link>https://arxiv.org/abs/2405.00062</link>
      <description>arXiv:2405.00062v1 Announce Type: new 
Abstract: Autonomous Vehicles (AVs) redefine transportation with sophisticated technology, integrating sensors, cameras, and intricate algorithms. Implementing machine learning in AV perception demands robust hardware accelerators to achieve real-time performance at reasonable power consumption and footprint. Lot of research and development efforts using different technologies are still being conducted to achieve the goal of getting a fully AV and some cars manufactures offer commercially available systems. Unfortunately, they still lack reliability because of the repeated accidents they have encountered such as the recent one which happened in California and for which the Cruise company had its license suspended by the state of California for an undetermined period [1]. This paper critically reviews the most recent findings of machine vision systems used in AVs from both hardware and algorithmic points of view. It discusses the technologies used in commercial cars with their pros and cons and suggests possible ways forward. Thus, the paper can be a tangible reference for researchers who have the opportunity to get involved in designing machine vision systems targeting AV</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00062v1</guid>
      <category>cs.AR</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruba Islayem, Fatima Alhosani, Raghad Hashem, Afra Alzaabi, Mahmoud Meribout</dc:creator>
    </item>
    <item>
      <title>Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2405.00314</link>
      <description>arXiv:2405.00314v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have recently garnered considerable attention, emerging as a promising alternative to convolutional neural networks (CNNs) in several vision-related applications. However, their large model sizes and high computational and memory demands hinder deployment, especially on resource-constrained devices. This underscores the necessity of algorithm-hardware co-design specific to ViTs, aiming to optimize their performance by tailoring both the algorithmic structure and the underlying hardware accelerator to each other's strengths. Model quantization, by converting high-precision numbers to lower-precision, reduces the computational demands and memory needs of ViTs, allowing the creation of hardware specifically optimized for these quantized algorithms, boosting efficiency. This article provides a comprehensive survey of ViTs quantization and its hardware acceleration. We first delve into the unique architectural attributes of ViTs and their runtime characteristics. Subsequently, we examine the fundamental principles of model quantization, followed by a comparative analysis of the state-of-the-art quantization techniques for ViTs. Additionally, we explore the hardware acceleration of quantized ViTs, highlighting the importance of hardware-friendly algorithm design. In conclusion, this article will discuss ongoing challenges and future research paths. We consistently maintain the related open-source materials at https://github.com/DD-DuDa/awesome-vit-quantization-acceleration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00314v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dayou Du, Gu Gong, Xiaowen Chu</dc:creator>
    </item>
    <item>
      <title>NeuroBlend: Towards Low-Power yet Accurate Neural Network-Based Inference Engine Blending Binary and Fixed-Point Convolutions</title>
      <link>https://arxiv.org/abs/2307.03784</link>
      <description>arXiv:2307.03784v2 Announce Type: replace 
Abstract: This paper introduces NeuroBlend, a novel neural network architecture featuring a unique building block known as the Blend module. This module incorporates binary and fixed-point convolutions in its main and skip paths, respectively. There is a judicious deployment of batch normalizations on both main and skip paths inside the Blend module and in between consecutive Blend modules. Additionally, we present a compiler and hardware architecture designed to map NeuroBlend models onto FPGA devices, aiming to minimize inference latency while maintaining high accuracy. Our NeuroBlend-20 (NeuroBlend-18) model, derived from ResNet-20 (ResNet-18) trained on CIFAR-10 (CIFAR-100), achieves 88.0\% (73.73\%) classification accuracy, outperforming state-of-the-art binary neural networks by 0.8\% (1.33\%), with an inference time of 0.38ms per image, 1.4x faster than previous FPGA implementation for BNNs. Similarly, our BlendMixer model for CIFAR-10 attains 90.6\% accuracy(1.59\% less than full precision MLPMixer), with a 3.5x reduction in model size compared to full precision MLPMixer. Furthermore, leveraging DSP blocks for 48-bit bitwise logic operations enables low-power FPGA implementation, yielding a 2.5x reduction in power consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03784v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash Fayyazi, Mahdi Nazemi, Arya Fayyazi, Massoud Pedram</dc:creator>
    </item>
    <item>
      <title>The Dawn of AI-Native EDA: Opportunities and Challenges of Large Circuit Models</title>
      <link>https://arxiv.org/abs/2403.07257</link>
      <description>arXiv:2403.07257v2 Announce Type: replace 
Abstract: Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts. We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07257v2</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Chen (Huawei Noah's Ark Lab), Yiqi Chen (Peking University), Zhufei Chu (Ningbo University), Wenji Fang (Hong Kong University of Science and Technology), Tsung-Yi Ho (The Chinese University of Hong Kong), Ru Huang (Peking University, Southeast University), Yu Huang (Huawei HiSilicon), Sadaf Khan (The Chinese University of Hong Kong), Min Li (Huawei Noah's Ark Lab), Xingquan Li (Peng Cheng Laboratory), Yu Li (The Chinese University of Hong Kong), Yun Liang (Peking University), Jinwei Liu (The Chinese University of Hong Kong), Yi Liu (The Chinese University of Hong Kong), Yibo Lin (Peking University), Guojie Luo (Peking University), Zhengyuan Shi (The Chinese University of Hong Kong), Guangyu Sun (Peking University), Dimitrios Tsaras (Huawei Noah's Ark Lab), Runsheng Wang (Peking University), Ziyi Wang (The Chinese University of Hong Kong), Xinming Wei (Peking University), Zhiyao Xie (Hong Kong University of Science and Technology), Qiang Xu (The Chinese University of Hong Kong), Chenhao Xue (Peking University), Junchi Yan (Shanghai Jiao Tong University), Jun Yang (Southeast University), Bei Yu (The Chinese University of Hong Kong), Mingxuan Yuan (Huawei Noah's Ark Lab), Evangeline F. Y. Young (The Chinese University of Hong Kong), Xuan Zeng (Fudan University), Haoyi Zhang (Peking University), Zuodong Zhang (Peking University), Yuxiang Zhao (Peking University), Hui-Ling Zhen (Huawei Noah's Ark Lab), Ziyang Zheng (The Chinese University of Hong Kong), Binwu Zhu (The Chinese University of Hong Kong), Keren Zhu (The Chinese University of Hong Kong), Sunan Zou (Peking University)</dc:creator>
    </item>
    <item>
      <title>SoK: Rowhammer on Commodity Operating Systems</title>
      <link>https://arxiv.org/abs/2201.02986</link>
      <description>arXiv:2201.02986v3 Announce Type: replace-cross 
Abstract: Rowhammer has drawn much attention from both academia and industry in the past years as rowhammer exploitation poses severe consequences to system security. Since the first comprehensive study of rowhammer in 2014, a number of rowhammer attacks have been demonstrated against dynamic random access memory (DRAM)-based commodity systems to break software confidentiality, integrity and availability. Accordingly, numerous software defenses have been proposed to mitigate rowhammer attacks on commodity systems of either legacy (e.g., DDR3) or recent DRAM (e.g., DDR4). Besides, multiple hardware defenses (e.g., Target Row Refresh) from the industry have been deployed into recent DRAM to eliminate rowhammer, which we categorize as production defenses.
  In this paper, we systematize rowhammer attacks and defenses with a focus on DRAM-based commodity systems. Particularly, we have established a unified framework demonstrating how a rowhammer attack affects a commodity system. With the framework, we characterize existing attacks, shedding light on new attack vectors that have not yet been explored. We further leverage the framework to categorize software and production defenses, generalize their key defense strategies and summarize their key limitations, from which potential defense strategies are identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.02986v3</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Zhang, Decheng Chen, Jiahao Qi, Yueqiang Cheng, Shijie Jiang, Yiyang Lin, Yansong Gao, Surya Nepal, Yi Zou, Jiliang Zhang, Yang Xiang</dc:creator>
    </item>
  </channel>
</rss>
