<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Centers and Margins of Modeling Humans in Well-being Technologies: A Decentering Approach</title>
      <link>https://arxiv.org/abs/2503.19132</link>
      <description>arXiv:2503.19132v1 Announce Type: new 
Abstract: This paper critically examines the machine learning (ML) modeling of humans in three case studies of well-being technologies. Through a critical technical approach, it examines how these apps were experienced in daily life (technology in use) to surface breakdowns and to identify the assumptions about the "human" body entrenched in the ML models (technology design). To address these issues, this paper applies agential realism to decenter foundational assumptions, such as body regularity and health/illness binaries, and speculates more inclusive design and ML modeling paths that acknowledge irregularity, human-system entanglements, and uncertain transitions. This work is among the first to explore the implications of decentering theories in computational modeling of human bodies and well-being, offering insights for more inclusive technologies and speculations toward posthuman-centered ML modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19132v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713940</arxiv:DOI>
      <dc:creator>Jichen Zhu, Pedro Sanches, Vasiliki Tsaknaki, Willem van der Maden, Irene Kaklopoulou</dc:creator>
    </item>
    <item>
      <title>Towards Collective Storytelling: Investigating Audience Annotations in Data Visualizations</title>
      <link>https://arxiv.org/abs/2503.19138</link>
      <description>arXiv:2503.19138v1 Announce Type: new 
Abstract: This work investigates personal perspectives in visualization annotations as devices for collective data-driven storytelling. Inspired by existing efforts in critical cartography, we show how people share personal memories in a visualization of COVID-19 data and how comments by other visualization readers influence the reading and understanding of visualizations. Analyzing interaction logs, reader surveys, visualization annotations, and interviews, we find that reader annotations help other viewers relate to other people's stories and reflect on their own experiences. Further, we found that annotations embedded directly into the visualization can serve as social traces guiding through a visualization and help readers contextualize their own stories. With that, they supersede the attention paid to data encodings and become the main focal point of the visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19138v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MCG.2025.3547944</arxiv:DOI>
      <dc:creator>Tobias Kauer, Marian D\"ork, Benjamin Bach</dc:creator>
    </item>
    <item>
      <title>MIRAGE: Multi-model Interface for Reviewing and Auditing Generative Text-to-Image AI</title>
      <link>https://arxiv.org/abs/2503.19252</link>
      <description>arXiv:2503.19252v1 Announce Type: new 
Abstract: While generative AI systems have gained popularity in diverse applications, their potential to produce harmful outputs limits their trustworthiness and usability in different applications. Recent years have seen growing interest in engaging diverse AI users in auditing generative AI that might impact their lives. To this end, we propose MIRAGE as a web-based tool where AI users can compare outputs from multiple AI text-to-image (T2I) models by auditing AI-generated images, and report their findings in a structured way. We used MIRAGE to conduct a preliminary user study with five participants and found that MIRAGE users could leverage their own lived experiences and identities to surface previously unnoticed details around harmful biases when reviewing multiple T2I models' outputs compared to reviewing only one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19252v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matheus Kunzler Maldaner, Wesley Hanwen Deng, Jason Hong, Ken Holstein, Motahhare Eslami</dc:creator>
    </item>
    <item>
      <title>Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment</title>
      <link>https://arxiv.org/abs/2503.19334</link>
      <description>arXiv:2503.19334v1 Announce Type: new 
Abstract: In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19334v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3328756.3328758</arxiv:DOI>
      <arxiv:journal_reference>CASA 2019: Proceedings of the 32nd International Conference on Computer Animation and Social Agents - Year 2019 - Pages 47 - 52</arxiv:journal_reference>
      <dc:creator>Ghazanfar Ali, Hong-Quan Le, Junho Kim, Seoung-won Hwang, Jae-In Hwang</dc:creator>
    </item>
    <item>
      <title>CyanKitten: AI-Driven Markerless Motion Capture for Improved Elderly Well-Being</title>
      <link>https://arxiv.org/abs/2503.19398</link>
      <description>arXiv:2503.19398v1 Announce Type: new 
Abstract: This paper introduces CyanKitten, an interactive virtual companion system tailored for elderly users, integrating advanced posture recognition, behavior recognition, and multimodal interaction capabilities. The system utilizes a three-tier architecture to process and interpret user movements and gestures, leveraging a dual-camera setup and a convolutional neural network trained explicitly on elderly movement patterns. The behavior recognition module identifies and responds to three key interactive gestures: greeting waves, petting motions, and heart-making gestures. A multimodal integration layer also combines visual and audio inputs to facilitate natural and intuitive interactions. This paper outlines the technical implementation of each component, addressing challenges such as elderly-specific movement characteristics, real-time processing demands, and environmental adaptability. The result is an engaging and accessible virtual interaction experience designed to enhance the quality of life for elderly users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19398v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mengyao Guo, Yu Nie, Jinda Han, Zongxing Li, Ze Gao</dc:creator>
    </item>
    <item>
      <title>TrackThinkDashboard: Understanding Student Self-Regulated Learning in Programming Study</title>
      <link>https://arxiv.org/abs/2503.19460</link>
      <description>arXiv:2503.19460v1 Announce Type: new 
Abstract: In programming education, fostering self-regulated learning (SRL) skills is essential for both students and teachers. This paper introduces TrackThinkDashboard, an application designed to visualize the learning workflow by integrating web browsing and programming logs into one unified view. The system aims to (1) help students monitor and reflect on their problem-solving processes, identify knowledge gaps, and cultivate effective SRL strategies; and (2) enable teachers to identify at-risk learners more effectively and provide targeted, data-driven guidance. We conducted a study with 33 participants (32 male, 1 female) from Japanese universities, including individuals with and without prior programming experience, to explore differences in web browsing and coding patterns. The dashboards revealed multiple learning approaches, such as trial-and-error and trial-and-search methods, and highlighted how domain knowledge influenced the overall activity flow. We discuss how this visualization tool can be used continuously or in one-off experiments, consider associated privacy implications, and explore opportunities for expanding data sources to gain richer behavioral insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19460v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ko Watanabe, Yuki Matsuda, Yugo Nakamura, Yutaka Arakawa, Shoya Ishimaru</dc:creator>
    </item>
    <item>
      <title>Agent-Initiated Interaction in Phone UI Automation</title>
      <link>https://arxiv.org/abs/2503.19537</link>
      <description>arXiv:2503.19537v1 Announce Type: new 
Abstract: Phone automation agents aim to autonomously perform a given natural-language user request, such as scheduling appointments or booking a hotel. While much research effort has been devoted to screen understanding and action planning, complex tasks often necessitate user interaction for successful completion. Aligning the agent with the user's expectations is crucial for building trust and enabling personalized experiences. This requires the agent to proactively engage the user when necessary, avoiding actions that violate their preferences while refraining from unnecessary questions where a default action is expected. We argue that such subtle agent-initiated interaction with the user deserves focused research attention.
  To promote such research, this paper introduces a task formulation for detecting the need for user interaction and generating appropriate messages. We thoroughly define the task, including aspects like interaction timing and the scope of the agent's autonomy. Using this definition, we derived annotation guidelines and created AndroidInteraction, a diverse dataset for the task, leveraging an existing UI automation dataset. We tested several text-based and multimodal baseline models for the task, finding that it is very challenging for current LLMs. We suggest that our task formulation, dataset, baseline models and analysis will be valuable for future UI automation research, specifically in addressing this crucial yet often overlooked aspect of agent-initiated interaction. This work provides a needed foundation to allow personalized agents to properly engage the user when needed, within the context of phone UI automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19537v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noam Kahlon, Guy Rom, Anatoly Efros, Filippo Galgani, Omri Berkovitch, Sapir Caduri, William E. Bishop, Oriana Riva, Ido Dagan</dc:creator>
    </item>
    <item>
      <title>Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review</title>
      <link>https://arxiv.org/abs/2503.19607</link>
      <description>arXiv:2503.19607v1 Announce Type: new 
Abstract: In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19607v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Gu, Ho Chit Siu, Melanie Platt, Isabelle Hurley, Jaime Pe\~na, Rohan Paleja</dc:creator>
    </item>
    <item>
      <title>Translating Emotions to Annotations - A Participant Perspective of Physiological Emotion Data Collection</title>
      <link>https://arxiv.org/abs/2503.19636</link>
      <description>arXiv:2503.19636v1 Announce Type: new 
Abstract: Physiological signals hold immense potential for ubiquitous emotion monitoring, presenting numerous applications in emotion recognition. However, harnessing this potential is hindered by significant challenges, particularly in the collection of annotations that align with physiological changes since the process hinges heavily on human participants. In this work, we set out to study human participant perspectives in the emotion data collection procedure. We conducted a lab-based emotion data collection study with 37 participants using 360 degree virtual reality video stimulus followed by semi-structured interviews with the study participants. Our findings presented that intrinsic factors like participant perception, experiment design nuances, and experiment setup suitability impact their emotional response and annotation within lab settings. Drawing from our findings and prior research, we propose recommendations for incorporating participant context into annotations and emphasizing participant-centric experiment designs. Furthermore, we explore current emotion data collection practices followed by AI practitioners and offer insights for future contributions leveraging physiological emotion data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19636v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711093</arxiv:DOI>
      <dc:creator>Pragya Singh, Ritvik Budhiraja, Pankaj Jalote, Mohan Kumar, Pushpendra Singh</dc:creator>
    </item>
    <item>
      <title>Leveraging Cognitive States for Adaptive Scaffolding of Understanding in Explanatory Tasks in HRI</title>
      <link>https://arxiv.org/abs/2503.19692</link>
      <description>arXiv:2503.19692v1 Announce Type: new 
Abstract: Understanding how scaffolding strategies influence human understanding in human-robot interaction is important for developing effective assistive systems. This empirical study investigates linguistic scaffolding strategies based on negation as an important means that de-biases the user from potential errors but increases processing costs and hesitations as a means to ameliorate processing costs. In an adaptive strategy, the user state with respect to the current state of understanding and processing capacity was estimated via a scoring scheme based on task performance, prior scaffolding strategy, and current eye gaze behavior. In the study, the adaptive strategy of providing negations and hesitations was compared with a non-adaptive strategy of providing only affirmations. The adaptive scaffolding strategy was generated using the computational model SHIFT. Our findings indicate that using adaptive scaffolding strategies with SHIFT tends to (1) increased processing costs, as reflected in longer reaction times, but (2) improved task understanding, evidenced by a lower error rate of almost 23%. We assessed the efficiency of SHIFT's selected scaffolding strategies across different cognitive states, finding that in three out of five states, the error rate was lower compared to the baseline condition. We discuss how these results align with the assumptions of the SHIFT model and highlight areas for refinement. Moreover, we demonstrate how scaffolding strategies, such as negation and hesitation, contribute to more effective human-robot explanatory dialogues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19692v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andr\'e Gro{\ss}, Birte Richter, Bjarne Thomzik, Britta Wrede</dc:creator>
    </item>
    <item>
      <title>Culture Clash: When Deceptive Design Meets Diverse Player Expectations</title>
      <link>https://arxiv.org/abs/2503.19858</link>
      <description>arXiv:2503.19858v1 Announce Type: new 
Abstract: Deceptive game designs that manipulate players are increasingly common in the gaming industry, but the impact on players is not well studied. While studies have revealed player frustration, there is a gap in understanding how cultural attributes affect the impact of deceptive design in games. This paper proposes a new research direction on the connection between the representation of culture in games and player response to deceptive designs. We believe that understanding the interplay between cultural attributes and deceptive design can inform the creation of games that are ethical and entertaining for players around the globe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19858v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3665463.3678866</arxiv:DOI>
      <arxiv:journal_reference>CHI PLAY Companion'24, October 14-17, 2024, Tampere, Finland</arxiv:journal_reference>
      <dc:creator>Hilda Hadan, Sabrina A. Sgandurra, Leah Zhang-Kennedy, Lennart E. Nacke</dc:creator>
    </item>
    <item>
      <title>The Case for "Thick Evaluations" of Cultural Representation in AI</title>
      <link>https://arxiv.org/abs/2503.19075</link>
      <description>arXiv:2503.19075v1 Announce Type: cross 
Abstract: Generative AI image models have been increasingly evaluated for their (in)ability to represent non-Western cultures. We argue that these evaluations operate through reductive ideals of representation, abstracted from how people define their own representation and neglecting the inherently interpretive and contextual nature of cultural representation. In contrast to these 'thin' evaluations, we introduce the idea of 'thick evaluations': a more granular, situated, and discursive measurement framework for evaluating representations of social worlds in AI images, steeped in communities' own understandings of representation. We develop this evaluation framework through workshops in South Asia, by studying the 'thick' ways in which people interpret and assign meaning to images of their own cultures. We introduce practices for thicker evaluations of representation that expand the understanding of representation underpinning AI evaluations and by co-constructing metrics with communities, bringing measurement in line with the experiences of communities on the ground.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19075v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rida Qadri, Mark Diaz, Ding Wang, Michael Madaio</dc:creator>
    </item>
    <item>
      <title>A Survey of Large Language Model Agents for Question Answering</title>
      <link>https://arxiv.org/abs/2503.19213</link>
      <description>arXiv:2503.19213v1 Announce Type: cross 
Abstract: This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19213v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murong Yue</dc:creator>
    </item>
    <item>
      <title>CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications</title>
      <link>https://arxiv.org/abs/2503.19225</link>
      <description>arXiv:2503.19225v1 Announce Type: cross 
Abstract: We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average mean-squared error of 0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and 0~4N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of desirable features of CoinFT enables various contact-rich robot interactions at a scale, across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT on drones by performing an attitude-based force control to perform tasks that require careful contact force modulation. The design, fabrication, and firmware of CoinFT are open-sourced at https://hojung-choi.github.io/coinft.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19225v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hojung Choi, Jun En Low, Tae Myung Huh, Gabriela A. Uribe, Seongheon Hong, Kenneth A. W. Hoffman, Julia Di, Tony G. Chen, Andrew A. Stanley, Mark R. Cutkosky</dc:creator>
    </item>
    <item>
      <title>Beyond Object Categories: Multi-Attribute Reference Understanding for Visual Grounding</title>
      <link>https://arxiv.org/abs/2503.19240</link>
      <description>arXiv:2503.19240v1 Announce Type: cross 
Abstract: Referring expression comprehension (REC) aims at achieving object localization based on natural language descriptions. However, existing REC approaches are constrained by object category descriptions and single-attribute intention descriptions, hindering their application in real-world scenarios. In natural human-robot interactions, users often express their desires through individual states and intentions, accompanied by guiding gestures, rather than detailed object descriptions. To address this challenge, we propose Multi-ref EC, a novel task framework that integrates state descriptions, derived intentions, and embodied gestures to locate target objects. We introduce the State-Intention-Gesture Attributes Reference (SIGAR) dataset, which combines state and intention expressions with embodied references. Through extensive experiments with various baseline models on SIGAR, we demonstrate that properly ordered multi-attribute references contribute to improved localization performance, revealing that single-attribute reference is insufficient for natural human-robot interaction scenarios. Our findings underscore the importance of multi-attribute reference expressions in advancing visual-language understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19240v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Guo, Jianfei Zhu, Wei Fan, Chunzhi Yi, Feng Jiang</dc:creator>
    </item>
    <item>
      <title>LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs</title>
      <link>https://arxiv.org/abs/2503.19280</link>
      <description>arXiv:2503.19280v1 Announce Type: cross 
Abstract: The study of propositional logic -- fundamental to the theory of computing -- is a cornerstone of the undergraduate computer science curriculum. Learning to solve logical proofs requires repeated guided practice, but undergraduate students often lack access to on-demand tutoring in a judgment-free environment. In this work, we highlight the need for guided practice tools in undergraduate mathematics education and outline the desiderata of an effective practice tool. We accordingly develop LogicLearner, a web application for guided logic proof practice. LogicLearner consists of an interface to attempt logic proofs step-by-step and an automated proof solver to generate solutions on the fly, allowing users to request guidance as needed. We pilot LogicLearner as a practice tool in two semesters of an undergraduate discrete mathematics course and receive strongly positive feedback for usability and pedagogical value in student surveys. To the best of our knowledge, LogicLearner is the only learning tool that provides an end-to-end practice environment for logic proofs with immediate, judgment-free feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19280v1</guid>
      <category>cs.DM</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amogh Inamdar, Uzay Macar, Michel Vazirani, Michael Tarnow, Zarina Mustapha, Natalia Dittren, Sam Sadeh, Nakul Verma, Ansaf Salleb-Aouissi</dc:creator>
    </item>
    <item>
      <title>Writing as a testbed for open ended agents</title>
      <link>https://arxiv.org/abs/2503.19711</link>
      <description>arXiv:2503.19711v1 Announce Type: cross 
Abstract: Open-ended tasks are particularly challenging for LLMs due to the vast solution space, demanding both expansive exploration and adaptable strategies, especially when success lacks a clear, objective definition. Writing, with its vast solution space and subjective evaluation criteria, provides a compelling testbed for studying such problems. In this paper, we investigate the potential of LLMs to act as collaborative co-writers, capable of suggesting and implementing text improvements autonomously. We analyse three prominent LLMs - Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action diversity, human alignment, and iterative improvement capabilities impact overall performance. This work establishes a framework for benchmarking autonomous writing agents and, more broadly, highlights fundamental challenges and potential solutions for building systems capable of excelling in diverse open-ended domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19711v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sian Gooding, Lucia Lopez-Rivilla, Edward Grefenstette</dc:creator>
    </item>
    <item>
      <title>Computational Analysis of Stress, Depression and Engagement in Mental Health: A Survey</title>
      <link>https://arxiv.org/abs/2403.08824</link>
      <description>arXiv:2403.08824v2 Announce Type: replace 
Abstract: Analysis of stress, depression and engagement is less common and more complex than that of frequently discussed emotions such as happiness, sadness, fear and anger. The importance of these psychological states has been increasingly recognized due to their implications for mental health and well-being. Stress and depression are interrelated and together they impact engagement in daily tasks, highlighting the need to explore their interplay. This survey is the first to simultaneously explore computational methods for analyzing stress, depression and engagement. We present a taxonomy and timeline of the computational approaches used to analyze them and we discuss the most commonly used datasets and input modalities, along with the categories and generic pipeline of these approaches. Subsequently, we describe state-of-the-art computational approaches, including a performance summary on the most commonly used datasets. Following this, we explore the applications of stress, depression and engagement analysis, along with the associated challenges, limitations and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08824v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MM</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Kumar, Alexander Vedernikov, Yuwei Chen, Wenming Zheng, Xiaobai Li</dc:creator>
    </item>
    <item>
      <title>Meta-Objects: Interactive and Multisensory Virtual Objects Learned from the Real World for Use in Augmented Reality</title>
      <link>https://arxiv.org/abs/2404.17179</link>
      <description>arXiv:2404.17179v3 Announce Type: replace 
Abstract: We introduce the concept of a meta-object, a next-generation virtual object that inherits the form, properties, and functions of its real-world counterpart, enabling seamless synchronization, interaction, and sharing between the physical and virtual worlds. While plenty of today's virtual objects provide some sensory feedback and dynamic behavior, meta-objects fully integrate interactive and multisensory features within a structured data framework to enable real-time immersive experiences in a post-metaverse intelligent simulation platform. Three key components underpin the utilization of meta-objects in the post-metaverse: property-embedded modeling for physical and action realism, adaptive multisensory feedback tailored to user interactions, and a scene graph-based intelligence simulation platform for scalable and efficient ecosystem integration. By leveraging meta-objects through wearable AR/VR devices, the post-metaverse facilitates seamless interactions that transcend spatial and temporal barriers, paving the way for a transformative reality-virtuality convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17179v3</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dooyoung Kim, Taewook Ha, Jinseok Hong, Seonji Kim, Selin Choi, Heejeong Ko, Woontack Woo</dc:creator>
    </item>
    <item>
      <title>Visualizing the Invisible: A Generative AR System for Intuitive Multi-Modal Sensor Data Presentation</title>
      <link>https://arxiv.org/abs/2412.13509</link>
      <description>arXiv:2412.13509v2 Announce Type: replace 
Abstract: Understanding sensor data can be difficult for non-experts because of the complexity and different semantic meanings of sensor modalities. This leads to a need for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we propose Vivar, a novel system that integrates multi-modal sensor data and presents 3D volumetric content for AR visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This approach accurately reflects value changes in multi-modal sensor information, ensuring that sensor variations are properly shown in visualization outcomes. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation, demonstrating 11x latency reduction without compromising quality. A user study involving over 503 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13509v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang</dc:creator>
    </item>
    <item>
      <title>A methodology and a platform for high-quality rich personal data</title>
      <link>https://arxiv.org/abs/2501.16864</link>
      <description>arXiv:2501.16864v3 Announce Type: replace 
Abstract: In the last years the pervasive use of sensors, as they exist in smart devices, e.g., phones, watches, medical devices, has increased dramatically the availability of personal data. However, existing research on data collection primarily focuses on the objective view of reality, as provided, for instance, by sensors, often neglecting the integration of subjective human input, as provided, for instance, by user answers to questionnaires. This limits substantially the exploitability of the collected data. In this paper we present a methodology and a platform specifically designed for the collection of a combination of large-scale sensor data and qualitative human feedback. The methodology has been designed to be deployed on top, and enriches the functionalities of, an existing data collection APP, called iLog, which has been used in large scale, worldwide data collection experiments. The main goal is to put the key actors involved in an experiment, i.e., the researcher in charge, the participant, and iLog in better control of the experiment itself, thus enabling a much improved quality and richness of the data collected. The novel functionalities of the resulting platform are: (i) a time-wise representation of the situational context within which the data collection is performed, (ii) an explicit representation of the temporal context within which the data collection is performed, (iii) a calendar-based dashboard for the real-time monitoring of the data collection context(s), and, finally, (iv) a mechanism for the run-time revision of the data collection plan. The practicality and utility of the proposed functionalities are demonstrated by showing how they apply to a case study involving 350 University students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16864v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Kayongo, Leonardo Malcotti, Haonan Zhao, Fausto Giunchiglia</dc:creator>
    </item>
    <item>
      <title>How to Make Your Multi-Image Posts Popular? An Approach to Enhanced Grid for Nine Images on Social Media</title>
      <link>https://arxiv.org/abs/2502.03709</link>
      <description>arXiv:2502.03709v2 Announce Type: replace 
Abstract: The nine-grid layout is commonly used for multi-image posts, arranging nine images in a tic-tac-toe board. This layout effectively presents content within limited space. Moreover, due to the numerous possible arrangements within the nine-image grid, the optimal arrangement that yields the highest level of attractiveness remains unknown. Our study investigates how the arrangement of images within a nine-grid layout affects the overall popularity of the image set, aiming to explore alignment schemes more aligned with user preferences. Based on survey results regarding user preferences in image arrangement, we have identified two ordering sequences that are widely recognized: sequential order and center prioritization, considering both image visual content and aesthetic quality as alignment metrics, resulting in four layout schemes. Finally, we recruited participants to annotate various layout schemes of the same set of images. Our experience-centered evaluation indicates that layout schemes based on aesthetic quality outperformed others. This research yields empirical evidence supporting the optimization of the nine-grid layout for multi-image posts, thereby furnishing content creators with valuable insights to enhance both attractiveness and user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03709v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SWC62898.2024.00237</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE Smart World Congress (SWC), Nadi, Fiji, 2024, pp. 1546-1553</arxiv:journal_reference>
      <dc:creator>Qi Xi, Shulin Li, Zhiqi Gao, Zibo Zhang, Shunye Tang, Jianchao Zhang, Liangxu Wang, Yiru Niu, Yan Zhang, Binhui Wang</dc:creator>
    </item>
    <item>
      <title>Geometry Aware Passthrough Mitigates Cybersickness</title>
      <link>https://arxiv.org/abs/2502.11497</link>
      <description>arXiv:2502.11497v2 Announce Type: replace 
Abstract: Virtual Reality headsets isolate users from the real-world by restricting their perception to the virtual-world. Video See-Through (VST) headsets address this by utilizing world-facing cameras to create Augmented Reality experiences. However, directly displaying camera feeds causes visual discomfort and cybersickness due to the inaccurate perception of scale and exaggerated motion parallax. This paper demonstrates the potential of geometry aware passthrough systems in mitigating cybersickness through accurate depth perception. We first present a methodology to benchmark and compare passthrough algorithms. Furthermore, we design a protocol to quantitatively measure cybersickness experienced by users in VST headsets. Using this protocol, we conduct a user study to compare direct passthrough and geometry aware passthrough systems. To the best of our knowledge, our study is the first one to reveal significantly reduced nausea, disorientation, and total scores of cybersickness with geometry aware passthrough. It also uncovers several potential avenues to further mitigate visually-induced discomfort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11497v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trishia El Chemaly, Mohit Goyal, Tinglin Duan, Vrushank Phadnis, Sakar Khattar, Bjorn Vlaskamp, Achin Kulshrestha, Eric Lee Turner, Aveek Purohit, Gregory Neiswander, Konstantine Tsotsos</dc:creator>
    </item>
    <item>
      <title>Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products</title>
      <link>https://arxiv.org/abs/2503.17955</link>
      <description>arXiv:2503.17955v2 Announce Type: replace 
Abstract: Human-AI Interaction (HAI) guidelines and design principles have become increasingly important in both industry and academia to guide the development of AI systems that align with user needs and expectations. However, large-scale empirical evidence on how HAI principles shape user satisfaction in practice remains limited. This study addresses that gap by analyzing over 100,000 user reviews of AI-related products from G2, a leading review platform for business software and services. Based on widely adopted industry guidelines, we identify seven core HAI dimensions and examine their coverage and sentiment within the reviews. We find that the sentiment on four HAI dimensions-adaptability, customization, error recovery, and security-is positively associated with overall user satisfaction. Moreover, we show that engagement with HAI dimensions varies by professional background: Users with technical job roles are more likely to discuss system-focused aspects, such as reliability, while non-technical users emphasize interaction-focused features like customization and feedback. Interestingly, the relationship between HAI sentiment and overall satisfaction is not moderated by job role, suggesting that once an HAI dimension has been identified by users, its effect on satisfaction is consistent across job roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17955v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefan Pasch, Sun-Young Ha</dc:creator>
    </item>
    <item>
      <title>Virtual Reality in Manufacturing Education: A Scoping Review Indicating State-of-the-Art, Benefits, and Challenges Across Domains, Levels, and Entities</title>
      <link>https://arxiv.org/abs/2503.18805</link>
      <description>arXiv:2503.18805v2 Announce Type: replace 
Abstract: To address the shortage of a skilled workforce in the U.S. manufacturing industry, immersive Virtual Reality (VR)-based training solutions hold promising potential. To effectively utilize VR to meet workforce demands, it is important to understand the role of VR in manufacturing education. Therefore, we conduct a scoping review in the field. As a first step, we used a 5W1H (What, Where, Who, When, Why, How) formula as a problem-solving approach to define a comprehensive taxonomy that can consider the role of VR from all relevant possibilities. Our taxonomy categorizes VR applications across three key aspects: (1) Domains, (2) Levels, and (3) Entities. Using a systematic literature search and analysis, we reviewed 108 research articles to find the current state, benefits, challenges, and future opportunities of VR in the field. It was found that VR has been explored in a variety of areas and provides numerous benefits to learners. Despite these benefits, its adoption in manufacturing education is limited. This review discusses the identified barriers and provides actionable insights to address them. These insights can enable the widespread usage of immersive technology to nurture and develop a workforce equipped with the skills required to excel in the evolving landscape of manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18805v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Ipsita, Ramesh Kaki, Ziyi Liu, Mayank Patel, Runlin Duan, Lakshmi Deshpande, Lin-Ping Yuan, Victoria Lowell, Ashok Maharaj, Kylie Peppler, Steven Feiner, Karthik Ramani</dc:creator>
    </item>
    <item>
      <title>Personalized Continual EEG Decoding: Retaining and Transferring Knowledge</title>
      <link>https://arxiv.org/abs/2411.11874</link>
      <description>arXiv:2411.11874v2 Announce Type: replace-cross 
Abstract: The significant inter-subject variability in electroen-cephalogram (EEG) signals often results in substantial changes to neural network weights as data distributions shift. This variability frequently causes catastrophic forgetting in continual EEG decoding tasks, where previously acquired knowledge is overwritten as new subjects are introduced. While retraining the entire dataset for each new subject can mitigate forgetting, this approach imposes significant computational costs, rendering it impractical for real-world applications. Therefore, an ideal brain-computer interface (BCI) model should incrementally learn new information without requiring complete retraining, thereby reducing computational overhead. Existing EEG decoding meth-ods typically rely on large, centralized source-domain datasets for pre-training to improve model generalization. However, in practical scenarios, data availability is often constrained by privacy concerns. Furthermore, these methods are susceptible to catastrophic forgetting in continual EEG decoding tasks, significantly limiting their utility in long-term learning scenarios. To address these issues, we propose the Personalized Continual EEG Decoding (PCED) framework for continual EEG decoding. The framework uses Euclidean Alignment for fast domain adap-tation, reducing inter-subject variability. To retain knowledge and prevent forgetting, it includes an exemplar replay mechanism that preserves key information from past tasks. A reservoir sampling-based memory management strategy optimizes exemplar storage to handle memory constraints in long-term learning. Experiments on the OpenBMI dataset with 54 subjects show that PCED balances knowledge retention and classification performance, providing an efficient solution for real-world BCI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11874v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Li, Hye-Bin Shin, Kang Yin, Seong-Whan Lee</dc:creator>
    </item>
    <item>
      <title>Jovis: A Visualization Tool for PostgreSQL Query Optimizer</title>
      <link>https://arxiv.org/abs/2411.14788</link>
      <description>arXiv:2411.14788v2 Announce Type: replace-cross 
Abstract: Query optimizers are essential components of relational database management systems that directly impact query performance as they transform input queries into efficient execution plans. While users can obtain the final execution plan using the EXPLAIN command and leverage existing visualization tools for intuitive understanding, the internal decision-making processes of query optimizers are hidden from users, making it difficult to understand how the plan is constructed. To address this challenge, we present Jovis, an interactive visualization tool designed to explore the query optimization process in PostgreSQL. Jovis provides a comprehensive view of the entire optimization workflow through tailored visualization for each optimization strategy. It also includes features that allow users to participate in optimization by providing hints, tuning parameters, and reusing prior optimization results. Jovis serves as both an educational tool for learners and a practical resource for database professionals, helping users understand and improve query optimization by guiding the optimizer to make better decisions or consider previously unexplored plans. The source code, data, and/or other artifacts have been made available at https://github.com/orgs/snu-jovis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14788v2</guid>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoojin Choi, Juhee Han, Kyoseung Koo, Bongki Moon</dc:creator>
    </item>
  </channel>
</rss>
