<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE</title>
      <link>https://arxiv.org/abs/2508.20103</link>
      <description>arXiv:2508.20103v1 Announce Type: cross 
Abstract: The optimal asset allocation between risky and risk-free assets is a persistent challenge due to the inherent volatility in financial markets. Conventional methods rely on strict distributional assumptions or non-additive reward ratios, which limit their robustness and applicability to investment goals. To overcome these constraints, this study formulates the optimal two-asset allocation problem as a sequential decision-making task within a Markov Decision Process (MDP). This framework enables the application of reinforcement learning (RL) mechanisms to develop dynamic policies based on simulated financial scenarios, regardless of prerequisites. We use the Kelly criterion to balance immediate reward signals against long-term investment objectives, and we take the novel step of integrating the Time-series Dense Encoder (TiDE) into the Deep Deterministic Policy Gradient (DDPG) RL framework for continuous decision-making. We compare DDPG-TiDE with a simple discrete-action Q-learning RL framework and a passive buy-and-hold investment strategy. Empirical results show that DDPG-TiDE outperforms Q-learning and generates higher risk adjusted returns than buy-and-hold. These findings suggest that tackling the optimal asset allocation problem by integrating TiDE within a DDPG reinforcement learning framework is a fruitful avenue for further exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20103v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rongwei Liu, Jin Zheng, John Cartlidge</dc:creator>
    </item>
    <item>
      <title>Optimal Quoting under Adverse Selection and Price Reading</title>
      <link>https://arxiv.org/abs/2508.20225</link>
      <description>arXiv:2508.20225v1 Announce Type: cross 
Abstract: Over the past decade, many dealers have implemented algorithmic models to automatically respond to RFQs and manage flows originating from their electronic platforms. In parallel, building on the foundational work of Ho and Stoll, and later Avellaneda and Stoikov, the academic literature on market making has expanded to address trade size distributions, client tiering, complex price dynamics, alpha signals, and the internalization versus externalization dilemma in markets with dealer-to-client and interdealer-broker segments. In this paper, we tackle two critical dimensions: adverse selection, arising from the presence of informed traders, and price reading, whereby the market maker's own quotes inadvertently reveal the direction of their inventory. These risks are well known to practitioners, who routinely face informed flows and algorithms capable of extracting signals from quoting behavior. Yet they have received limited attention in the quantitative finance literature, beyond stylized toy models with limited actionability. Extending the existing literature, we propose a tractable and implementable framework that enables market makers to adjust their quotes with greater awareness of informational risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20225v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Barzykin, Philippe Bergault, Olivier Gu\'eant, Malo Lemmel</dc:creator>
    </item>
    <item>
      <title>An Integrated Approach to Importance Sampling and Machine Learning for Efficient Monte Carlo Estimation of Distortion Risk Measures in Black Box Models</title>
      <link>https://arxiv.org/abs/2408.02401</link>
      <description>arXiv:2408.02401v3 Announce Type: replace 
Abstract: Distortion risk measures play a critical role in quantifying risks associated with uncertain outcomes. Accurately estimating these risk measures in the context of computationally expensive simulation models that lack analytical tractability is fundamental to effective risk management and decision making. In this paper, we propose an efficient important sampling method for distortion risk measures in such models that reduces the computational cost through machine learning. We demonstrate the applicability and efficiency of the Monte Carlo method in numerical experiments on various distortion risk measures and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02401v3</guid>
      <category>q-fin.RM</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\"oren Bettels, Stefan Weber</dc:creator>
    </item>
    <item>
      <title>Russia-Ukraine conflict and the quantile return connectedness of grain futures in the BRICS and international markets</title>
      <link>https://arxiv.org/abs/2409.19307</link>
      <description>arXiv:2409.19307v2 Announce Type: replace 
Abstract: This study investigates quantile-based connectedness among BRICS and international grain futures around the Russia-Ukraine conflict and milestones of the Black Sea Grain Initiative. Using a dynamic quantile VAR combined with a frequency-domain decomposition, we trace spillovers across market states and horizons. Spillovers are heterogeneous across quantiles, as the time-varying total connectedness index hovers near 95% in the tails, remains well above the median, and is higher before the outbreak than after. Furthermore, grain type and regional proximity strengthen pairwise connectedness. South African grain futures are persistent net receivers, whereas Argentine grain futures, U.S. soybean, and Ukrainian wheat are key transmitters. In the frequency domain, short-term components dominate total spillovers. In portfolio applications, the minimum connectedness portfolio delivers a positive Sharpe ratio under both normal and lower tail conditions. Overall, the results inform asset allocation and risk management in grain futures markets under geopolitical instability and support policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19307v2</guid>
      <category>q-fin.RM</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan-Hong Yang, Ying-Hui Shao, Wei-Xing Zhou</dc:creator>
    </item>
  </channel>
</rss>
