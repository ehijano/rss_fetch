<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 04:02:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Factor pre-training in Bayesian multivariate logistic models</title>
      <link>https://arxiv.org/abs/2409.17441</link>
      <description>arXiv:2409.17441v1 Announce Type: cross 
Abstract: This article focuses on inference in logistic regression for high-dimensional binary outcomes. A popular approach induces dependence across the outcomes by including latent factors in the linear predictor. Bayesian approaches are useful for characterizing uncertainty in inferring the regression coefficients, factors and loadings, while also incorporating hierarchical and shrinkage structure. However, Markov chain Monte Carlo algorithms for posterior computation face challenges in scaling to high-dimensional outcomes. Motivated by applications in ecology, we exploit a blessing of dimensionality to motivate pre-estimation of the latent factors. Conditionally on the factors, the outcomes are modeled via independent logistic regressions. We implement Gaussian approximations in parallel in inferring the posterior on the regression coefficients and loadings, including a simple adjustment to obtain credible intervals with valid frequentist coverage. We show posterior concentration properties and excellent empirical performance in simulations. The methods are applied to insect biodiversity data in Madagascar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17441v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Granger Causality for Mixed Time Series Generalized Linear Models: A Case Study on Multimodal Brain Connectivity</title>
      <link>https://arxiv.org/abs/2409.17751</link>
      <description>arXiv:2409.17751v1 Announce Type: cross 
Abstract: This paper is motivated by studies in neuroscience experiments to understand interactions between nodes in a brain network using different types of data modalities that capture different distinct facets of brain activity. To assess Granger-causality, we introduce a flexible framework through a general class of models that accommodates mixed types of data (binary, count, continuous, and positive components) formulated in a generalized linear model (GLM) fashion. Statistical inference for causality is performed based on both frequentist and Bayesian approaches, with a focus on the latter. Here, we develop a procedure for conducting inference through the proposed Bayesian mixed time series model. By introducing spike and slab priors for some parameters in the model, our inferential approach guides causality order selection and provides proper uncertainty quantification. The proposed methods are then utilized to study the rat spike train and local field potentials (LFP) data recorded during the olfaction working memory task. The proposed methodology provides critical insights into the causal relationship between the rat spiking activity and LFP spectral power. Specifically, power in the LFP beta band is predictive of spiking activity 300 milliseconds later, providing a novel analytical tool for this area of emerging interest in neuroscience and demonstrating its usefulness and flexibility in the study of causality in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17751v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luiza S. C. Piancastelli, Wagner Barreto-Souza, Norbert J. Fortin, Keiland W. Cooper, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation Study</title>
      <link>https://arxiv.org/abs/2409.08201</link>
      <description>arXiv:2409.08201v2 Announce Type: replace-cross 
Abstract: The focus of this study is to evaluate the effectiveness of Machine Learning (ML) methods for two-sample testing with right-censored observations. To achieve this, we develop several ML-based methods with varying architectures and implement them as two-sample tests. Each method is an ensemble (stacking) that combines predictions from classical two-sample tests. This paper presents the results of training the proposed ML methods, examines their statistical power compared to classical two-sample tests, analyzes the null distribution of the proposed methods when the null hypothesis is true, and evaluates the significance of the features incorporated into the proposed methods. In total, this work covers 18 methods for two-sample testing under right-censored observations, including the proposed methods and classical well-studied two-sample tests. All results from numerical experiments were obtained from a synthetic dataset generated using the inverse transform sampling method and replicated multiple times through Monte Carlo simulation. To test the two-sample problem with right-censored observations, one can use the proposed two-sample methods (scripts, dataset, and models are available on GitHub and Hugging Face).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08201v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Philonenko, Sergey Postovalov</dc:creator>
    </item>
    <item>
      <title>Self-Organized State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v2 Announce Type: replace-cross 
Abstract: We consider a state-space model (SSM) parametrized by some parameter $\theta$, and our aim is to perform joint parameter and state inference. A simple idea to carry out this task, which almost dates back to the origin of the Kalman filter, is to replace the static parameter $\theta$ by a Markov chain $(\theta_t)_{t\geq 0}$ and then to apply a filtering algorithm to the extended, or self-organized SSM (SO-SSM). However, the practical implementation of this idea in a theoretically justified way has remained an open problem. In this paper we fill this gap by introducing various possible constructions of $(\theta_t)_{t\geq 0}$ that ensure the validity of the SO-SSM for joint parameter and state inference. Notably, we show that such SO-SSMs can be defined even if $\|\mathrm{Var}(\theta_{t}|\theta_{t-1})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This result is important since, as illustrated in our numerical experiments, these models can be efficiently approximated using particle filter algorithms. While SO-SSMs have been introduced for online inference, the development of iterated filtering (IF) algorithms has shown that they can also serve for computing the maximum likelihood estimator of a given SSM. In this work, we also derive constructions of $(\theta_t)_{t\geq 0}$ and theoretical guarantees tailored to these specific applications of SO-SSMs and, as a result, introduce new IF algorithms. From a practical point of view, the algorithms we develop have the merit of being simple to implement and only requiring minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>Trust-Region Sequential Quadratic Programming for Stochastic Optimization with Random Models</title>
      <link>https://arxiv.org/abs/2409.15734</link>
      <description>arXiv:2409.15734v2 Announce Type: replace-cross 
Abstract: In this work, we consider solving optimization problems with a stochastic objective and deterministic equality constraints. We propose a Trust-Region Sequential Quadratic Programming method to find both first- and second-order stationary points. Our method utilizes a random model to represent the objective function, which is constructed from stochastic observations of the objective and is designed to satisfy proper adaptive accuracy conditions with a high but fixed probability. To converge to first-order stationary points, our method computes a gradient step in each iteration defined by minimizing a quadratic approximation of the objective subject to a (relaxed) linear approximation of the problem constraints and a trust-region constraint. To converge to second-order stationary points, our method additionally computes an eigen step to explore the negative curvature of the reduced Hessian matrix, as well as a second-order correction step to address the potential Maratos effect, which arises due to the nonlinearity of the problem constraints. Such an effect may impede the method from moving away from saddle points. Both gradient and eigen step computations leverage a novel parameter-free decomposition of the step and the trust-region radius, accounting for the proportions among the feasibility residual, optimality residual, and negative curvature. We establish global almost sure first- and second-order convergence guarantees for our method, and present computational results on CUTEst problems, regression problems, and saddle-point problems to demonstrate its superiority over existing line-search-based stochastic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15734v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Fang, Sen Na, Michael W. Mahoney, Mladen Kolar</dc:creator>
    </item>
  </channel>
</rss>
