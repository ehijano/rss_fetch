<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inferring the parameters of Taylor's law in ecology</title>
      <link>https://arxiv.org/abs/2408.16023</link>
      <description>arXiv:2408.16023v1 Announce Type: new 
Abstract: Taylor's power law (TL) or fluctuation scaling has been verified empirically for the abundances of many species, human and non-human, and in many other fields including physics, meteorology, computer science, and finance. TL asserts that the variance is directly proportional to a power of the mean, exactly for population moments and, whether or not population moments exist, approximately for sample moments. In many papers, linear regression of log variance as a function of log mean is used to estimate TL's parameters. We provide some statistical guarantees with large-sample asymptotics for this kind of inference under general conditions, and we derive confidence intervals for the parameters. In many ecological applications, the means and variances are estimated over time or across space from arrays of abundance data collected at different locations and time points. When the ratio between the time-series length and the number of spatial points converges to a constant as both become large, the usual normalized statistics are asymptotically biased. We provide a bias correction to get correct confidence intervals. TL, widely studied in multiple sciences, is a source of challenging new statistical problems in a nonstationary spatiotemporal framework. We illustrate our results with both simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16023v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lionel Truquet, Joel E. Cohen, Paul Doukhan</dc:creator>
    </item>
    <item>
      <title>Group Difference in Differences can Identify Effect Heterogeneity in Non-Canonical Settings</title>
      <link>https://arxiv.org/abs/2408.16039</link>
      <description>arXiv:2408.16039v1 Announce Type: new 
Abstract: Consider a very general setting in which data on an outcome of interest is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in the outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. In these non-canonical settings (lacking a control group or a pre-period), some researchers still compute DiD estimators, while others avoid causal inference altogether. In this paper, we will elucidate the group-period-treatment scenarios and corresponding parallel trends assumptions under which a DiD formula identifies meaningful causal estimands and what those causal estimands are. We find that in non-canonical settings, under a group parallel trends assumption the DiD formula identifies effect heterogeneity in the treated across groups or across time periods (depending on the setting).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16039v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Laura Hatfield</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v1 Announce Type: new 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. With respect to assumptions, design-based direct estimators are generally preferable because of their consistency and asymptotic normality. However, when data are sparse at the desired area level, as is often the case when measuring rare events for example, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage towards the local average. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose a unit-level Bayesian model for small area estimation of rare event prevalence which uses design-based direct estimates at a higher area level to increase accuracy, precision, and consistency in aggregation. After introducing the model and its implementation, we conduct a simulation study to compare its properties to alternative models and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 DHS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Statistical comparison of quality attributes_a range-based approach</title>
      <link>https://arxiv.org/abs/2408.16153</link>
      <description>arXiv:2408.16153v1 Announce Type: new 
Abstract: A novel approach for comparing quality attributes of different products when there is considerable product-related variability is proposed. In such a case, the whole range of possible realizations must be considered. Looking, for example, at the respective information published by agencies like the EMA or the FDA, one can see that commonly accepted tests together with the proper statistical framework are not yet available. This work attempts to close this gap in the treatment of range-based comparisons. The question of when two products can be considered similar with respect to a certain property is discussed and a framework for such a statistical comparison is presented, which is based on the proposed concept of kappa-cover. Assuming normally distributed quality attributes a statistical test termed covering-test is proposed. Simulations show that this test possesses desirable statistical properties with respect to small sample size and power. In order to demonstrate the usefulness of the suggested concept, the proposed test is applied to a data set from the pharmaceutical industry</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16153v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerhard G\"ossler, Vera Hofer, Hans Manner, Walter Goessler</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for intervals</title>
      <link>https://arxiv.org/abs/2408.16381</link>
      <description>arXiv:2408.16381v1 Announce Type: new 
Abstract: Data following an interval structure are increasingly prevalent in many scientific applications. In medicine, clinical events are often monitored between two clinical visits, making the exact time of the event unknown and generating outcomes with a range format. As interest in automating healthcare decisions grows, uncertainty quantification via predictive regions becomes essential for developing reliable and trusworthy predictive algorithms. However, the statistical literature currently lacks a general methodology for interval targets, especially when these outcomes are incomplete due to censoring. We propose a uncertainty quantification algorithm and establish its theoretical properties using empirical process arguments based on a newly developed class of functions specifically designed for interval data structures. Although this paper primarily focuses on deriving predictive regions for interval-censored data, the approach can also be applied to other statistical modeling tasks, such as goodness-of-fit assessments. Finally, the applicability of the methods developed here is illustrated through various biomedical applications, including two clinical examples: i) sleep time and its link with cardiovasculuar diseases ii) survival time and physical activity values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16381v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, Michael R. Kosorok, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Nonparametric goodness of fit tests for Pareto type-I distribution with complete and censored data</title>
      <link>https://arxiv.org/abs/2408.16384</link>
      <description>arXiv:2408.16384v1 Announce Type: new 
Abstract: Two new goodness of fit tests for the Pareto type-I distribution for complete and right censored data are proposed using fixed point characterization based on Steins type identity. The asymptotic distributions of the test statistics under both the null and alternative hypotheses are obtained. The performance of the proposed tests is evaluated and compared with existing tests through a Monte Carlo simulation experiment. The newly proposed tests exhibit greater power than existing tests for the Pareto type-I distribution. Finally, the methodology is applied to real-world data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16384v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Avhad Ganesh Vishnu, Ananya Lahiri, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates</title>
      <link>https://arxiv.org/abs/2408.16485</link>
      <description>arXiv:2408.16485v1 Announce Type: new 
Abstract: Medical advancements have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments in terms of both curing the disease and prolonging survival. We may use a Cox proportional hazards (PH) cure model to achieve this. However, a significant challenge in applying such a model is the potential presence of partially observed covariates in the data. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a more general case, where different covariate vectors are used to model the cure probability and the survival of patients who are not cured. We also propose an approximation of the exact conditional distribution using a regression approach, which helps draw imputed values at a lower computational cost. To assess its effectiveness, we compare the proposed approach with a complete case analysis and an analysis without any missing covariates. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16485v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Cipriani, Marta Fiocco, Marco Alf\`o, Maria Quelhas, Eni Musta</dc:creator>
    </item>
    <item>
      <title>A Causal Framework for Evaluating Heterogeneous Policy Mechanisms Using Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2408.16670</link>
      <description>arXiv:2408.16670v1 Announce Type: new 
Abstract: In designing and evaluating public policies, policymakers and researchers often hypothesize about the mechanisms through which a policy may affect a population and aim to assess these mechanisms in practice. For example, when studying an excise tax on sweetened beverages, researchers might explore how cross-border shopping, economic competition, and store-level price changes differentially affect store sales. However, many policy evaluation designs, including the difference-in-differences (DiD) approach, traditionally target the average effect of the intervention rather than the underlying mechanisms. Extensions of these approaches to evaluate policy mechanisms often involve exploratory subgroup analyses or outcome models parameterized by mechanism-specific variables. However, neither approach studies the mechanisms within a causal framework, limiting the analysis to associative relationships between mechanisms and outcomes, which may be confounded by differences among sub-populations exposed to varying levels of the mechanisms. Therefore, rigorous mechanism evaluation requires robust techniques to adjust for confounding and accommodate the interconnected relationship between stores within competitive economic landscapes. In this paper, we present a framework for evaluating policy mechanisms by studying Philadelphia beverage tax. Our approach builds on recent advancements in causal effect curve estimators under DiD designs, offering tools and insights for assessing policy mechanisms complicated by confounding and network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16670v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gary Hettinger, Youjin Lee, Nandita Mitra</dc:creator>
    </item>
    <item>
      <title>Effect Aliasing in Observational Studies</title>
      <link>https://arxiv.org/abs/2408.16708</link>
      <description>arXiv:2408.16708v1 Announce Type: new 
Abstract: In experimental design, aliasing of effects occurs in fractional factorial experiments, where certain low order factorial effects are indistinguishable from certain high order interactions: low order contrasts may be orthogonal to one another, while their higher order interactions are aliased and not identified. In observational studies, aliasing occurs when certain combinations of covariates -- e.g., time period and various eligibility criteria for treatment -- perfectly predict the treatment that an individual will receive, so a covariate combination is aliased with a particular treatment. In this situation, when a contrast among several groups is used to estimate a treatment effect, collections of individuals defined by contrast weights may be balanced with respect to summaries of low-order interactions between covariates and treatments, but necessarily not balanced with respect to summaries of high-order interactions between covariates and treatments. We develop a theory of aliasing in observational studies, illustrate that theory in an observational study whose aliasing is more robust than conventional difference-in-differences, and develop a new form of matching to construct balanced confounded factorial designs from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16708v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul R. Rosenbaum, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Finite Sample Valid Inference via Calibrated Bootstrap</title>
      <link>https://arxiv.org/abs/2408.16763</link>
      <description>arXiv:2408.16763v1 Announce Type: new 
Abstract: While widely used as a general method for uncertainty quantification, the bootstrap method encounters difficulties that raise concerns about its validity in practical applications. This paper introduces a new resampling-based method, termed $\textit{calibrated bootstrap}$, designed to generate finite sample-valid parametric inference from a sample of size $n$. The central idea is to calibrate an $m$-out-of-$n$ resampling scheme, where the calibration parameter $m$ is determined against inferential pivotal quantities derived from the cumulative distribution functions of loss functions in parameter estimation. The method comprises two algorithms. The first, named $\textit{resampling approximation}$ (RA), employs a $\textit{stochastic approximation}$ algorithm to find the value of the calibration parameter $m=m_\alpha$ for a given $\alpha$ in a manner that ensures the resulting $m$-out-of-$n$ bootstrapped $1-\alpha$ confidence set is valid. The second algorithm, termed $\textit{distributional resampling}$ (DR), is developed to further select samples of bootstrapped estimates from the RA step when constructing $1-\alpha$ confidence sets for a range of $\alpha$ values is of interest. The proposed method is illustrated and compared to existing methods using linear regression with and without $L_1$ penalty, within the context of a high-dimensional setting and a real-world data application. The paper concludes with remarks on a few open problems worthy of consideration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16763v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>Reversible jump Markov chain Monte Carlo and multi-model samplers</title>
      <link>https://arxiv.org/abs/1001.2055</link>
      <description>arXiv:1001.2055v2 Announce Type: replace 
Abstract: To appear in the second edition of the MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng (eds), Chapman &amp; Hall.</description>
      <guid isPermaLink="false">oai:arXiv.org:1001.2055v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanan Fan, Scott A. Sisson, Laurence Davies</dc:creator>
    </item>
    <item>
      <title>The synthetic instrument: From sparse association to sparse causation</title>
      <link>https://arxiv.org/abs/2304.01098</link>
      <description>arXiv:2304.01098v2 Announce Type: replace 
Abstract: In many observational studies, researchers are often interested in studying the effects of multiple exposures on a single outcome. Standard approaches for high-dimensional data such as the lasso assume the associations between the exposures and the outcome are sparse. These methods, however, do not estimate the causal effects in the presence of unmeasured confounding. In this paper, we consider an alternative approach that assumes the causal effects in view are sparse. We show that with sparse causation, the causal effects are identifiable even with unmeasured confounding. At the core of our proposal is a novel device, called the synthetic instrument, that in contrast to standard instrumental variables, can be constructed using the observed exposures directly. We show that under linear structural equation models, the problem of causal effect estimation can be formulated as an $\ell_0$-penalization problem, and hence can be solved efficiently using off-the-shelf software. Simulations show that our approach outperforms state-of-art methods in both low-dimensional and high-dimensional settings. We further illustrate our method using a mouse obesity dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01098v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dingke Tang, Dehan Kong, Linbo Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of covariate assisted principal regression for brain functional connectivity</title>
      <link>https://arxiv.org/abs/2306.07181</link>
      <description>arXiv:2306.07181v2 Announce Type: replace 
Abstract: This paper presents a Bayesian reformulation of covariate-assisted principal (CAP) regression of Zhao and others (2021), which aims to identify components in the covariance of response signal that are associated with covariates in a regression framework. We introduce a geometric formulation and reparameterization of individual covariance matrices in their tangent space. By mapping the covariance matrices to the tangent space, we leverage Euclidean geometry to perform posterior inference. This approach enables joint estimation of all parameters and uncertainty quantification within a unified framework, fusing dimension reduction for covariance matrices with regression model estimation. We validate the proposed method through simulation studies and apply it to analyze associations between covariates and brain functional connectivity, utilizing data from the Human Connectome Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07181v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyung G. Park</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</title>
      <link>https://arxiv.org/abs/2310.12000</link>
      <description>arXiv:2310.12000v2 Announce Type: replace 
Abstract: Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12000v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal K\"undig, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Infinite joint species distribution models</title>
      <link>https://arxiv.org/abs/2402.13384</link>
      <description>arXiv:2402.13384v2 Announce Type: replace 
Abstract: Joint species distribution models are popular in ecology for modeling covariate effects on species occurrence, while characterizing cross-species dependence. Data consist of multivariate binary indicators of the occurrences of different species in each sample, along with sample-specific covariates. A key problem is that current models implicitly assume that the list of species under consideration is predefined and finite, while for highly diverse groups of organisms, it is impossible to anticipate which species will be observed in a study and discovery of unknown species is common. This article proposes a new modeling paradigm for statistical ecology, which generalizes traditional multivariate probit models to accommodate large numbers of rare species and new species discovery. We discuss theoretical properties of the proposed modeling paradigm and implement efficient algorithms for posterior computation. Simulation studies and applications to fungal biodiversity data provide compelling support for the new modeling class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13384v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Stolf, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>The Analysis of Criminal Recidivism: A Hierarchical Model-Based Approach for the Analysis of Zero-Inflated, Spatially Correlated recurrent events Data</title>
      <link>https://arxiv.org/abs/2405.02666</link>
      <description>arXiv:2405.02666v2 Announce Type: replace 
Abstract: The life course perspective in criminology has become prominent last years, offering valuable insights into various patterns of criminal offending and pathways. The study of criminal trajectories aims to understand the beginning, persistence and desistence in crime, providing intriguing explanations about these moments in life. Central to this analysis is the identification of patterns in the frequency of criminal victimization and recidivism, along with the factors that contribute to them. Specifically, this work introduces a new class of models that overcome limitations in traditional methods used to analyze criminal recidivism. These models are designed for recurrent events data characterized by excess of zeros and spatial correlation. They extend the Non-Homogeneous Poisson Process, incorporating spatial dependence in the model through random effects, enabling the analysis of associations among individuals within the same spatial stratum. To deal with the excess of zeros in the data, a zero-inflated Poisson mixed model was incorporated. In addition to parametric models following the Power Law process for baseline intensity functions, we propose flexible semi-parametric versions approximating the intensity function using Bernstein Polynomials. The Bayesian approach offers advantages such as incorporating external evidence and modeling specific correlations between random effects and observed data. The performance of these models was evaluated in a simulation study with various scenarios, and we applied them to analyze criminal recidivism data in the Metropolitan Region of Belo Horizonte, Brazil. The results provide a detailed analysis of high-risk areas for recurrent crimes and the behavior of recidivism rates over time. This research significantly enhances our understanding of criminal trajectories, paving the way for more effective strategies in combating criminal recidivism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02666v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alisson C. C. Silva, F\'abio N. Demarqui, Br\'aulio F. Silva, Marcos O. Prates</dc:creator>
    </item>
    <item>
      <title>Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning</title>
      <link>https://arxiv.org/abs/2405.09536</link>
      <description>arXiv:2405.09536v2 Announce Type: replace 
Abstract: Gradient boosting is a sequential ensemble method that fits a new weaker learner to pseudo residuals at each iteration. We propose Wasserstein gradient boosting, a novel extension of gradient boosting that fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input. It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions for each input. In classification and regression, a model typically returns, for each input, a point estimate of a parameter of a noise distribution specified for a response variable, such as the class probability parameter of a categorical distribution specified for a response label. A main application of Wasserstein gradient boosting in this paper is tree-based evidential learning, which returns a distributional estimate of the response parameter for each input. We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09536v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuo Matsubara</dc:creator>
    </item>
    <item>
      <title>Constrained Design of a Binary Instrument in a Partially Linear Model</title>
      <link>https://arxiv.org/abs/2406.05592</link>
      <description>arXiv:2406.05592v2 Announce Type: replace 
Abstract: We study the question of how best to assign an encouragement in a randomized encouragement study. In our setting, units arrive with covariates, receive a nudge toward treatment or control, acquire one of those statuses in a way that need not align with the nudge, and finally have a response observed. The nudge can be seen as a binary instrument that affects the response only via the treatment status. Our goal is to assign the nudge as a function of covariates in a way that best estimates the local average treatment effect (LATE). We assume a partially linear model, wherein the baseline model is non-parametric and the treatment term is linear in the covariates. Under this model, we outline a two-stage procedure to consistently estimate the LATE. Though the variance of the LATE is intractable, we derive a finite sample approximation and thus a design criterion to minimize. This criterion is convex, allowing for constraints that might arise for budgetary or ethical reasons. We prove conditions under which our solution asymptotically recovers the lowest true variance among all possible nudge propensities. We apply our method to a semi-synthetic example involving triage in an emergency department and find significant gains relative to a regression discontinuity design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05592v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Morrison, Minh Nguyen, Jonathan Chen, Michael Baiocchi, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>A Kernel-Based Conditional Two-Sample Test Using Nearest Neighbors (with Applications to Calibration, Regression Curves, and Simulation-Based Inference)</title>
      <link>https://arxiv.org/abs/2407.16550</link>
      <description>arXiv:2407.16550v2 Announce Type: replace 
Abstract: In this paper we introduce a kernel-based measure for detecting differences between two conditional distributions. Using the `kernel trick' and nearest-neighbor graphs, we propose a consistent estimate of this measure which can be computed in nearly linear time (for a fixed number of nearest neighbors). Moreover, when the two conditional distributions are the same, the estimate has a Gaussian limit and its asymptotic variance has a simple form that can be easily estimated from the data. The resulting test attains precise asymptotic level and is universally consistent for detecting differences between two conditional distributions. We also provide a resampling based test using our estimate that applies to the conditional goodness-of-fit problem, which controls Type I error in finite samples and is asymptotically consistent with only a finite number of resamples. A method to de-randomize the resampling test is also presented. The proposed methods can be readily applied to a broad range of problems, ranging from classical nonparametric statistics to modern machine learning. Specifically, we explore three applications: testing model calibration, regression curve evaluation, and validation of emulator models in simulation-based inference. We illustrate the superior performance of our method for these tasks, both in simulations as well as on real data. In particular, we apply our method to (1) assess the calibration of neural network models trained on the CIFAR-10 dataset, (2) compare regression functions for wind power generation across two different turbines, and (3) validate emulator models on benchmark examples with intractable posteriors and for generating synthetic `redshift' associated with galaxy images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16550v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Ziang Niu, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed</title>
      <link>https://arxiv.org/abs/2408.12347</link>
      <description>arXiv:2408.12347v3 Announce Type: replace 
Abstract: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I (familywise) error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12347v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Epistemically robust selection of fitted models</title>
      <link>https://arxiv.org/abs/2408.13414</link>
      <description>arXiv:2408.13414v2 Announce Type: replace 
Abstract: Fitting models to data is an important part of the practice of science, made almost ubiquitous by advances in machine learning. Very often however, fitted solutions are not unique, but form an ensemble of candidate models -- qualitatively different, yet with comparable quantitative performance. One then needs a criterion which can select the best candidate models, or at least falsify (reject) the worst ones. Because standard statistical approaches to model selection rely on assumptions which are usually invalid in scientific contexts, they tend to be overconfident, rejecting models based on little more than statistical noise. The ideal objective for fitting models is generally considered to be the risk: this is the theoretical average loss of a model (assuming unlimited data). In this work we develop a nonparametric method for estimating, for each candidate model, the epistemic uncertainty on its risk: in other words we associate to each model a distribution of scores which accounts for expected modelling errors. We then propose that a model falsification criterion should mirror established experimental practice: a falsification result should be accepted only if it is reproducible across experimental variations. The strength of this approach is illustrated using examples from physics and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13414v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Ren\'e, Andr\'e Longtin</dc:creator>
    </item>
    <item>
      <title>Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses</title>
      <link>https://arxiv.org/abs/2401.11272</link>
      <description>arXiv:2401.11272v2 Announce Type: replace-cross 
Abstract: The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are given which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, \emph{Ann. Statist.}) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in two specific examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11272v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>On the Estimation of bivariate Conditional Transition Rates</title>
      <link>https://arxiv.org/abs/2404.02736</link>
      <description>arXiv:2404.02736v3 Announce Type: replace-cross 
Abstract: Recent literature has found conditional transition rates to be a useful tool for avoiding Markov assumptions in multi-state models. While the estimation of univariate conditional transition rates has been extensively studied, the intertemporal dependencies captured in the bivariate conditional transition rates still require a consistent estimator. We provide an estimator that is suitable for censored data and emphasize the connection to the rich theory of the estimation of bivariate survival functions. Bivariate conditional transition rates are necessary for various applications in the survival context but especially in the calculation of moments in life insurance mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02736v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theis Bathke</dc:creator>
    </item>
    <item>
      <title>A Guide to Feature Importance Methods for Scientific Inference</title>
      <link>https://arxiv.org/abs/2404.12862</link>
      <description>arXiv:2404.12862v2 Announce Type: replace-cross 
Abstract: While machine learning (ML) models are increasingly used due to their high predictive power, their use in understanding the data-generating process (DGP) is limited. Understanding the DGP requires insights into feature-target associations, which many ML models cannot directly provide due to their opaque internal mechanisms. Feature importance (FI) methods provide useful insights into the DGP under certain conditions. Since the results of different FI methods have different interpretations, selecting the correct FI method for a concrete use case is crucial and still requires expert knowledge. This paper serves as a comprehensive guide to help understand the different interpretations of global FI methods. Through an extensive review of FI methods and providing new proofs regarding their interpretation, we facilitate a thorough understanding of these methods and formulate concrete recommendations for scientific inference. We conclude by discussing options for FI uncertainty estimation and point to directions for future research aiming at full statistical inference from black-box ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12862v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-63797-1_22</arxiv:DOI>
      <arxiv:journal_reference>Longo, L., Lapuschkin, S., Seifert, C. (eds) Explainable Artificial Intelligence. xAI 2024. Communications in Computer and Information Science, vol 2154. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Fiona Katharina Ewald, Ludwig Bothmann, Marvin N. Wright, Bernd Bischl, Giuseppe Casalicchio, Gunnar K\"onig</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v2 Announce Type: replace-cross 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least 28 days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. Female sex, history of seasonal allergies, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Enabling Causal Discovery in Post-Nonlinear Models with Normalizing Flows</title>
      <link>https://arxiv.org/abs/2407.04980</link>
      <description>arXiv:2407.04980v2 Announce Type: replace-cross 
Abstract: Post-nonlinear (PNL) causal models stand out as a versatile and adaptable framework for modeling intricate causal relationships. However, accurately capturing the invertibility constraint required in PNL models remains challenging in existing studies. To address this problem, we introduce CAF-PoNo (Causal discovery via Normalizing Flows for Post-Nonlinear models), harnessing the power of the normalizing flows architecture to enforce the crucial invertibility constraint in PNL models. Through normalizing flows, our method precisely reconstructs the hidden noise, which plays a vital role in cause-effect identification through statistical independence testing. Furthermore, the proposed approach exhibits remarkable extensibility, as it can be seamlessly expanded to facilitate multivariate causal discovery via causal order identification, empowering us to efficiently unravel complex causal relationships. Extensive experimental evaluations on both simulated and real datasets consistently demonstrate that the proposed method outperforms several state-of-the-art approaches in both bivariate and multivariate causal discovery tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04980v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nu Hoang, Bao Duong, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>Scalable Variational Causal Discovery Unconstrained by Acyclicity</title>
      <link>https://arxiv.org/abs/2407.04992</link>
      <description>arXiv:2407.04992v2 Announce Type: replace-cross 
Abstract: Bayesian causal discovery offers the power to quantify epistemic uncertainties among a broad range of structurally diverse causal theories potentially explaining the data, represented in forms of directed acyclic graphs (DAGs). However, existing methods struggle with efficient DAG sampling due to the complex acyclicity constraint. In this study, we propose a scalable Bayesian approach to effectively learn the posterior distribution over causal graphs given observational data thanks to the ability to generate DAGs without explicitly enforcing acyclicity. Specifically, we introduce a novel differentiable DAG sampling method that can generate a valid acyclic causal graph by mapping an unconstrained distribution of implicit topological orders to a distribution over DAGs. Given this efficient DAG sampling scheme, we are able to model the posterior distribution over causal graphs using a simple variational distribution over a continuous domain, which can be learned via the variational inference framework. Extensive empirical experiments on both simulated and real datasets demonstrate the superior performance of the proposed model compared to several state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04992v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nu Hoang, Bao Duong, Thin Nguyen</dc:creator>
    </item>
  </channel>
</rss>
