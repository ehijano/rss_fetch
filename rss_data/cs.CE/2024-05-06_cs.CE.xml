<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:01:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Development and Validation of an Artificial Neural Network for the Recognition of Custom Dataset with YOLOv4</title>
      <link>https://arxiv.org/abs/2405.02298</link>
      <description>arXiv:2405.02298v1 Announce Type: new 
Abstract: The expanding applications, utilized by more users, enhance hardware performance and further develop cloud systems for big data processing. This leads to numerous unexplored deep learning applications, especially in advanced computer vision for object recognition. Deep learning in image processing encompasses varied tasks from recognizing elements with diverse shapes and sizes to complex element classification, coping with varying backgrounds and lighting conditions, and text recognition. Its advantages lie in robust setup and high performance for recognizing complex elements. This work aims to develop a deep learning-based detection system for automated recognition of assembly components differing in geometry, size, contour, or color. Implementing the YOLOv4 algorithm, the system detects components based on their characteristics. Testing with 13 components involves capturing them in different orientations, numbers, individual parts, or assembled groups using a Raspberry Pi microcontroller and camera. Evaluation focuses on correct object recognition, confidence values, different compositions, distances between objects, and environmental factors affecting system quality. Results show positive object recognition across all scenarios, irrespective of orientation or number of objects. Even densely packed objects are correctly recognized with high confidence (97-100%). Lighting conditions don't significantly impact results, and all objects are properly labeled. The developed system is suitable for real-time two-dimensional component detection, with potential for extension to three-dimensional analysis using multiple cameras with varied positioning and views.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02298v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. Veysi, M. Adeli, N. Peirov Naziri</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning for Modelling Protein Complexes</title>
      <link>https://arxiv.org/abs/2405.02299</link>
      <description>arXiv:2405.02299v1 Announce Type: new 
Abstract: AlphaFold can be used for both single-chain and multi-chain protein structure prediction, while the latter becomes extremely challenging as the number of chains increases. In this work, by taking each chain as a node and assembly actions as edges, we show that an acyclic undirected connected graph can be used to predict the structure of multi-chain protein complexes (a.k.a., protein complex modelling, PCM). However, there are still two challenges: 1) The huge combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for the PCM problem can easily lead to high computational cost. 2) The scales of protein complexes exhibit distribution shift due to variance in chain numbers, which calls for the generalization in modelling complexes of various scales. To address these challenges, we propose GAPN, a Generative Adversarial Policy Network powered by domain-specific rewards and adversarial loss through policy gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently search through the immense assembly space and optimize the direct docking reward through policy gradient. Importantly, we design an adversarial reward function to enhance the receptive field of our model. In this way, GAPN will simultaneously focus on a specific batch of complexes and the global assembly rules learned from complexes with varied chain numbers. Empirically, we have achieved both significant accuracy (measured by RMSD and TM-Score) and efficiency improvements compared to leading PCM softwares. GAPN outperforms the state-of-the-art method (MoLPC) with up to 27% improvement in TM-Score, with a speed-up of 600 times. Our code is released at \url{https://github.com/ft2023/GAPN}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02299v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Feng, Ziqi Gao, Jiaxuan You, Chenyi Zi, Yan Zhou, Chen Zhang, Jia Li</dc:creator>
    </item>
    <item>
      <title>Enhanced Thermal Management in High-Temperature Applications: Design and Optimization of a Water-Cooled Forced Convection System in a Hollow Cuboid Vapour Chamber Using COMSOL and MATLAB</title>
      <link>https://arxiv.org/abs/2405.02303</link>
      <description>arXiv:2405.02303v1 Announce Type: new 
Abstract: This report details the design and optimisation of a water-cooled forced convection heat dissipation system for use in high-temperature applications (ranges between 700 degrees - 1000 degrees K). A hollow cuboid vapour chamber model was investigated. The space within the hollow cuboid was used as the design space. COMSOL, a FEM software product was used to solve for the physical parameters of each geometry for the heat dissipation system design space. COMSOL in conjunction with MATLAB was used for the parametric and density-based topology optimisation of the geometric design in the design space. The goal of the optimization is the minimisation of a temperature gradient over the design space. This allows the heat to be evenly spread throughout the designed mesh which allows for more effective cooling. To reduce the computational time needed to solve and optimise each geometry in 3D, a 2D representation was created for the front and rear faces of the hollow cuboid setup. These 2D face designs were then extrapolated into 3D over the length of the hollow cube and COMSOL was used to find a solution for each model. This report also proposes a use case for this system wherein it would be used in conjunction with MGA and thermometric technology within coal-fired power stations for the extraction and storage of waste heat for later use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02303v1</guid>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>brandon Curtis Colelough</dc:creator>
    </item>
    <item>
      <title>Simulating the aftermath of Northern European Enclosure Dam (NEED) break and flooding of European coast</title>
      <link>https://arxiv.org/abs/2405.02310</link>
      <description>arXiv:2405.02310v1 Announce Type: new 
Abstract: The Northern European Enclosure Dam (NEED) is a hypothetical project to prevent flooding in European countries following the rising ocean level due to melting arctic glaciers. This project involves the construction of two large dams between Scotland and Norway, as well as England and France. The anticipated cost of this project is 250 to 500 billion euros. In this paper, we present the simulation of the aftermath of flooding on the European coastline caused by a catastrophic break of this hypothetical dam. From our simulation results, we can observe that there is a traveling wave after the accident, with a velocity of around 10 kilometers per hour, raising the sea level permanently inside the dammed region. This observation implies a need to construct additional dams or barriers protecting the northern coastline of the Netherlands and the interior of the Baltic Sea. Our simulations have been obtained using the following building blocks. First, a graph transformation model was applied to generate an adaptive mesh approximating the topography of the Earth. We employ the composition graph grammar model for breaking triangular elements in the mesh without the generation of hanging nodes. Second, the wave equation is formulated in a spherical latitude-longitude system of coordinates and solved by a high-order time integration scheme using the generalized $\alpha$ method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02310v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.ao-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pawe{\l} Maczuga, Marcin {\L}o\'s, Eirik Valseth, Albert Oliver Serra, Leszek Siwik, Elisabede Alberdi Celaya, Anna Paszy\'nska, Maciej Paszy\'nski</dc:creator>
    </item>
    <item>
      <title>Predicting the impact of water transport on carbonation-induced corrosion in variably saturated reinforced concrete</title>
      <link>https://arxiv.org/abs/2405.02611</link>
      <description>arXiv:2405.02611v1 Announce Type: new 
Abstract: A modelling framework for predicting carbonation-induced corrosion in reinforced concrete is presented. The framework constituents include a new model for water transport in cracked concrete, a link between corrosion current density and water saturation, and a theory for characterising concrete carbonation. The theoretical framework is numerically implemented using the finite element method and model predictions are extensively benchmarked against experimental data. The results show that the model is capable of accurately predicting carbonation progress, as well as wetting and drying of cracked and uncracked concrete, revealing a very good agreement with independent experiments from a set of consistent parameters. In addition, insight is gained into the evolution of carbonation penetration and corrosion current density under periodic wetting and drying conditions. Among others, we find that cyclic wetting periods significantly speed up the carbonation progress and that the induced corrosion current density is very sensitive to concrete saturation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02611v1</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.app-ph</category>
      <category>physics.chem-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>E. Korec, L. Mingazzi, F. Freddi, E. Mart\'inez-Pa\~neda</dc:creator>
    </item>
    <item>
      <title>GTFS2STN: Analyzing GTFS Transit Data by Generating Spatiotemporal Transit Network</title>
      <link>https://arxiv.org/abs/2405.02760</link>
      <description>arXiv:2405.02760v1 Announce Type: new 
Abstract: GTFS, the General Transit Feed Specialization, is an open standard format to record transit information used by thousands of transit agencies across the world. By converting a static GTFS transit network to a spatiotemporal network connecting bus stops over space and time, a preliminary tool named GTFS2STN is implemented to analyze the accessibility of the transit system. Furthermore, a simple application is built for users to generate spatiotemporal network online. The online tool also supports some basic analysis including generate isochrone maps given origin, generate travel time variability over time given a pair of origin and destination, etc. Results show that the tool has a similar result compared with Mapnificent, another open source endeavour to generate isochrone maps given GTFS inputs. Compared with Mapnificent, the proposed GTFS2STN tool is suited for research and evaluation purposes because the users can upload any historical GTFS dataset by any transit agencies to evaluate the accessibility and travel time variability of transit networks over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02760v1</guid>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diyi Liu, Jing Guo, Yangsong Gu, Meredith King, Lee D. Han, Candace Brakewood</dc:creator>
    </item>
    <item>
      <title>Predicting Open-Hole Laminates Failure Using Support Vector Machines With Classical and Quantum Kernels</title>
      <link>https://arxiv.org/abs/2405.02903</link>
      <description>arXiv:2405.02903v1 Announce Type: new 
Abstract: Modeling open hole failure of composites is a complex task, consisting in a highly nonlinear response with interacting failure modes. Numerical modeling of this phenomenon has traditionally been based on the finite element method, but requires to tradeoff between high fidelity and computational cost. To mitigate this shortcoming, recent work has leveraged machine learning to predict the strength of open hole composite specimens. Here, we also propose using data-based models but to tackle open hole composite failure from a classification point of view. More specifically, we show how to train surrogate models to learn the ultimate failure envelope of an open hole composite plate under in-plane loading. To achieve this, we solve the classification problem via support vector machine (SVM) and test different classifiers by changing the SVM kernel function. The flexibility of kernel-based SVM also allows us to integrate the recently developed quantum kernels in our algorithm and compare them with the standard radial basis function (RBF) kernel. Finally, thanks to kernel-target alignment optimization, we tune the free parameters of all kernels to best separate safe and failure-inducing loading states. The results show classification accuracies higher than 90% for RBF, especially after alignment, followed closely by the quantum kernel classifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02903v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgio Tosti Balducci, Boyang Chen, Matthias M\"oller, Marc Gerritsma, Roeland De Breuker</dc:creator>
    </item>
    <item>
      <title>Optimal Box Contraction for Solving Linear Systems via Simulated and Quantum Annealing</title>
      <link>https://arxiv.org/abs/2405.03029</link>
      <description>arXiv:2405.03029v1 Announce Type: new 
Abstract: Solving linear systems of equations is an important problem in science and engineering. Many quantum algorithms, such as the Harrow-Hassidim-Lloyd (HHL) algorithm (for quantum-gate computers) and the box algorithm (for quantum-annealing machines), have been proposed for solving such systems.
  The focus of this paper is on improving the efficiency of the box algorithm. The basic principle behind this algorithm is to transform the linear system into a series of quadratic unconstrained binary optimization (QUBO) problems, which are then solved on annealing machines.
  The computational efficiency of the box algorithm is entirely determined by the number of iterations, which, in turn, depends on the box contraction ratio, typically set to 0.5. Here, we show through theory that a contraction ratio of 0.5 is sub-optimal and that we can achieve a speed-up with a contraction ratio of 0.2. This is confirmed through numerical experiments where a speed-up between $20 \%$ to $60 \%$ is observed when the optimal contraction ratio is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03029v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjay Suresh, Krishnan Suresh</dc:creator>
    </item>
    <item>
      <title>Time Series Stock Price Forecasting Based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) Optimization</title>
      <link>https://arxiv.org/abs/2405.03151</link>
      <description>arXiv:2405.03151v1 Announce Type: new 
Abstract: In this paper, a time series algorithm based on Genetic Algorithm (GA) and Long Short-Term Memory Network (LSTM) optimization is used to forecast stock prices effectively, taking into account the trend of the big data era. The data are first analyzed by descriptive statistics, and then the model is built and trained and tested on the dataset. After optimization and adjustment, the mean absolute error (MAE) of the model gradually decreases from 0.11 to 0.01 and tends to be stable, indicating that the model prediction effect is gradually close to the real value. The results on the test set show that the time series algorithm optimized based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) is able to accurately predict the stock prices, and is highly consistent with the actual price trends and values, with strong generalization ability. The MAE on the test set is 2.41, the MSE is 9.84, the RMSE is 3.13, and the R2 is 0.87. This research result not only provides a novel stock price prediction method, but also provides a useful reference for financial market analysis using computer technology and big data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03151v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinye Sha</dc:creator>
    </item>
    <item>
      <title>pyCFS-data: Data Processing Framework in Python for openCFS</title>
      <link>https://arxiv.org/abs/2405.03437</link>
      <description>arXiv:2405.03437v1 Announce Type: new 
Abstract: Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics. Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method. Since 2000, the software has been developed continuously. The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++). In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03437v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Wurzinger, Stefan Schoder</dc:creator>
    </item>
    <item>
      <title>A review on data-driven constitutive laws for solids</title>
      <link>https://arxiv.org/abs/2405.03658</link>
      <description>arXiv:2405.03658v1 Announce Type: new 
Abstract: This review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids. Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales. Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness. We attempt to provide a road map of how these can be reconciled in a data-availability-aware context. We also touch upon relevant aspects such as data sampling techniques, design of experiments, verification, and validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03658v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jan Niklas Fuhg, Govinda Anantha Padmanabha, Nikolaos Bouklas, Bahador Bahmani, WaiChing Sun, Nikolaos N. Vlassis, Moritz Flaschel, Pietro Carrara, Laura De Lorenzis</dc:creator>
    </item>
    <item>
      <title>QBER: Quantifying Cyber Risks for Strategic Decisions</title>
      <link>https://arxiv.org/abs/2405.03513</link>
      <description>arXiv:2405.03513v1 Announce Type: cross 
Abstract: Quantifying cyber risks is essential for organizations to grasp their vulnerability to threats and make informed decisions. However, current approaches still need to work on blending economic viewpoints to provide insightful analysis. To bridge this gap, we introduce QBER approach to offer decision-makers measurable risk metrics. The QBER evaluates losses from cyberattacks, performs detailed risk analyses based on existing cybersecurity measures, and provides thorough cost assessments. Our contributions involve outlining cyberattack probabilities and risks, identifying Technical, Economic, and Legal (TEL) impacts, creating a model to gauge impacts, suggesting risk mitigation strategies, and examining trends and challenges in implementing widespread Cyber Risk Quantification (CRQ). The QBER approach serves as a guided approach for organizations to assess risks and strategically invest in cybersecurity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03513v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muriel Figueredo Franco, Aiatur Rahaman Mullick, Santosh Jha</dc:creator>
    </item>
    <item>
      <title>GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder</title>
      <link>https://arxiv.org/abs/2308.05882</link>
      <description>arXiv:2308.05882v2 Announce Type: replace 
Abstract: Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this prediction uncertainty allows for efficient adaptive training through a greedy selection of additional training data points. This approach does not require prior knowledge of the underlying PDEs. Consequently, GPLaSDI is inherently non-intrusive and can be applied to problems without a known PDE or its residual. We demonstrate the effectiveness of our approach on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem. Our proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05882v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2023.116535</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering, 418A, 116535, 2024</arxiv:journal_reference>
      <dc:creator>Christophe Bonneville, Youngsoo Choi, Debojyoti Ghosh, Jonathan L. Belof</dc:creator>
    </item>
    <item>
      <title>A cable finite element formulation based on exact tension field for static nonlinear analysis of cable structures</title>
      <link>https://arxiv.org/abs/2401.05609</link>
      <description>arXiv:2401.05609v4 Announce Type: replace 
Abstract: This paper presents a numerically exact cable finite element model for static nonlinear analysis of cable structures. The model derives the exact expression of the tension field using the geometrically exact beam theory coupled with the fundamental mechanical characteristics of cables. The equations for the cable element are formulated by addressing the equilibrium conditions at the element boundaries and ensuring compatibility within the element. Unlike previous studies that typically provide explicit expressions for cable models, this study develops a formulation that emphasizes numerical precision and broad applicability. It achieves this by deriving linearized equations with implicit expressions incorporating integrals. The proposed model accurately computes internal forces and deformation states, and determines the unstrained length of the cable. Additionally, it accounts for the variability in cross-sectional stiffness along the cable's length. The paper discusses solution implementations using the complete tangent matrix and element internal iterations. The effectiveness of the proposed cable element is demonstrated through numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05609v4</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxiong Li, Qikun Huang, Suiyin Chen</dc:creator>
    </item>
    <item>
      <title>Towards Code Generation for Octree-Based Multigrid Solvers</title>
      <link>https://arxiv.org/abs/2403.08063</link>
      <description>arXiv:2403.08063v2 Announce Type: replace 
Abstract: This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08063v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Angersbach, Sebastian Kuckuck, Harald K\"ostler</dc:creator>
    </item>
    <item>
      <title>Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow</title>
      <link>https://arxiv.org/abs/2306.07209</link>
      <description>arXiv:2306.07209v3 Announce Type: replace-cross 
Abstract: Various industries such as finance, meteorology, and energy produce vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering large language models (LLMs) showcase promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while interacting and displaying in a human-friendly manner. Based on this, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting as an experienced expert, Data-Copilot autonomously transforms raw data into multi-form output that best matches the user's intent. Specifically, it first designs multiple universal interfaces to satisfy diverse data-related requests, like querying, analysis, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces. The whole process is fully controlled by Data-Copilot, without human assistance. We release Data-Copilot-1.0 using massive Chinese financial data, e.g., stocks, funds, and news. Experiments indicate it achieves reliable performance with lower token consumption, showing promising application prospects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07209v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang</dc:creator>
    </item>
    <item>
      <title>PLMM: Personal Large Language Models on Mobile Devices</title>
      <link>https://arxiv.org/abs/2309.14726</link>
      <description>arXiv:2309.14726v2 Announce Type: replace-cross 
Abstract: Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user experience and produce high quality results. The proposed personal large models can be applied in a wide range of applications such as language and vision tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14726v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhao Gong</dc:creator>
    </item>
    <item>
      <title>A Scoping Review on Simulation-based Design Optimization in Marine Engineering: Trends, Best Practices, and Gaps</title>
      <link>https://arxiv.org/abs/2404.18654</link>
      <description>arXiv:2404.18654v2 Announce Type: replace-cross 
Abstract: This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas. Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins. It also covers marine structures and renewable energy systems. A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches. The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments. Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness. This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18654v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11831-024-10127-1</arxiv:DOI>
      <dc:creator>Andrea Serani, Thomas Scholcz, Valentina Vanzi</dc:creator>
    </item>
  </channel>
</rss>
