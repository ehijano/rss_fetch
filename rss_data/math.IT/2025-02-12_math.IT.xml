<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 02:42:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Downlink and Uplink ISAC in Continuous-Aperture Array (CAPA) Systems</title>
      <link>https://arxiv.org/abs/2502.06967</link>
      <description>arXiv:2502.06967v1 Announce Type: new 
Abstract: A continuous-aperture array (CAPA)-based integrated sensing and communications (ISAC) framework is proposed for both downlink and uplink scenarios. Within this framework, continuous operator-based signal models are employed to describe the sensing and communication processes. The performance of communication and sensing is analyzed using two information-theoretic metrics: the communication rate (CR) and the sensing rate (SR). 1) For downlink ISAC, three continuous beamforming designs are proposed: i) the communications-centric (C-C) design that maximizes the CR, ii) the sensing-centric (S-C) design that maximizes the SR, and iii) the Pareto-optimal design that characterizes the Pareto boundary of the CR-SR region. A signal subspace-based approach is proposed to derive the closed-form optimal beamformers for the considered designs. On this basis, closed-form expressions are derived for the achievable CRs and SRs, and the downlink rate region achieved by CAPAs is characterized. 2) For uplink ISAC, the C-C and S-C successive interference cancellation (SIC)-based methods are proposed to manage inter-functionality interference. Using the subspace approach along with the time-sharing technique, closed-form expressions for the optimal beamformers are derived, and the achievable CRs, SRs, and rate region are analyzed. Numerical results demonstrate that, for both downlink and uplink, CAPA-based ISAC achieves higher CRs and SRs as well as larger CR-SR regions compared to conventional spatially discrete array (SPDA)-based ISAC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06967v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boqun Zhao, Chongjun Ouyang, Xingqi Zhang, Hyundong Shin, Yuanwei Liu</dc:creator>
    </item>
    <item>
      <title>Game of Coding With an Unknown Adversary</title>
      <link>https://arxiv.org/abs/2502.07109</link>
      <description>arXiv:2502.07109v1 Announce Type: new 
Abstract: Motivated by emerging decentralized applications, the \emph{game of coding} framework has been recently introduced to address scenarios where the adversary's control over coded symbols surpasses the fundamental limits of traditional coding theory. Still, the reward mechanism available in decentralized systems, motivates the adversary to act rationally. While the decoder, as the data collector (DC), has an acceptance and rejection mechanism, followed by an estimation module, the adversary aims to maximize its utility, as an increasing function of (1) the chance of acceptance (to increase the reward), and (2) estimation error. On the other hand, the decoder also adjusts its acceptance rule to maximize its own utility, as (1) an increasing function of the chance of acceptance (to keep the system functional), (2) decreasing function of the estimation error. Prior works within this framework rely on the assumption that the game is complete, that is, both the DC and the adversary are fully aware of each other's utility functions. However, in practice, the decoder is often unaware of the utility of the adversary. To address this limitation, we develop an algorithm enabling the DC to commit to a strategy that achieves within the vicinity of the equilibrium, without knowledge of the adversary's utility function. Our approach builds on an observation that at the equilibrium, the relationship between the probability of acceptance and the mean squared error (MSE) follows a predetermined curve independent of the specific utility functions of the players. By exploiting this invariant relationship, the DC can iteratively refine its strategy based on observable parameters, converging to a near-optimal solution. We provide theoretical guarantees on sample complexity and accuracy of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07109v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzaleh Akbarinodehi, Parsa Moradi, Mohammad Ali Maddah-Ali</dc:creator>
    </item>
    <item>
      <title>Expressing entropy and cross-entropy in expansions of common meadows</title>
      <link>https://arxiv.org/abs/2502.07148</link>
      <description>arXiv:2502.07148v1 Announce Type: new 
Abstract: A common meadow is an enrichment of a field with a partial division operation that is made total by assuming that division by zero takes the a default value, a special element $\bot$ adjoined to the field. To a common meadow of real numbers we add a binary logarithm $\log_2(-)$, which we also assume to be total with $\log_2(p) = \bot$ for $p \leq 0$. With these and other auxiliary operations, such as a sign function, we form algebras over which entropy and cross entropy can be defined for probability mass functions on a finite sample space by algebraic formulae that are simple terms built from the operations of the algebras and without case distinctions or conventions to avoid partiality. The discuss the advantages of algebras based on common meadows, whose theory is established, and alternate methods to define entropy and other information measures completely for all arguments using single terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07148v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan A Bergstra, John V Tucker</dc:creator>
    </item>
    <item>
      <title>Nonlinear Reed-Solomon codes and nonlinear skew quasi-cyclic codes</title>
      <link>https://arxiv.org/abs/2502.07242</link>
      <description>arXiv:2502.07242v1 Announce Type: new 
Abstract: This article begins with an exploration of nonlinear codes ($\mathbb{F}_q$-linear subspaces of $\mathbb{F}_{q^m}^n$) which are generalizations of the familiar Reed-Solomon codes. This then leads to a wider exploration of nonlinear analogues of the skew quasi-cyclic codes of index $\ell$ first explored in 2010 by Abualrub et al., i.e., $\mathbb{F}_{q^m}[x;\sigma]$-submodules of $\left(\mathbb{F}_{q^m}[x;\sigma]/(x^n - 1)\right)^\ell$. After introducing nonlinear skew quasi-cyclic codes, we then determine the module structure of these codes using a two-fold iteration of the Smith normal form of matrices over skew polynomial rings. Finally we show that in certain cases, a single use of the Smith normal form will suffice to determine the elementary divisors of the code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07242v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.RA</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Bossaller, Daniel Herden, Indalecio Ruiz-Bolanos</dc:creator>
    </item>
    <item>
      <title>Explicit Codes approaching Generalized Singleton Bound using Expanders</title>
      <link>https://arxiv.org/abs/2502.07308</link>
      <description>arXiv:2502.07308v1 Announce Type: new 
Abstract: We construct a new family of explicit codes that are list decodable to capacity and achieve an optimal list size of $O(\frac{1}{\epsilon})$. In contrast to existing explicit constructions of codes achieving list decoding capacity, our arguments do not rely on algebraic structure but utilize simple combinatorial properties of expander graphs.
  Our construction is based on a celebrated distance amplification procedure due to Alon, Edmonds, and Luby [FOCS'95], which transforms any high-rate code into one with near-optimal rate-distance tradeoff. We generalize it to show that the same procedure can be used to transform any high-rate code into one that achieves list decoding capacity. Our proof can be interpreted as a "local-to-global" phenomenon for (a slight strengthening of) the generalized Singleton bound. Using this construction, for every $R, \epsilon \in (0,1)$ and $k \in \mathbb{N}^+$, we obtain an \emph{explicit} family of codes $\mathcal{C} \subseteq \Sigma^n$, with rate $R$ such that,
  - They achieve the $\epsilon$-relaxed generalized Singleton bound: for any $g \in \Sigma^n$ and any list $\mathcal{H}$ of at most $k$ codewords, we have, \[ \underset{h \in \mathcal{H}}{\mathbb{E}} [\Delta(g,h)] ~\geq~ \frac{|\mathcal{H}|-1}{|\mathcal{H}|} \cdot (1 - R - \epsilon). \]
  - The alphabet size is a constant depending only on $\epsilon$ and $k$.
  - They can be list decoded up to radius $\frac{k-1}{k}(1-R-\epsilon)$, in time $n^{O_{k,\epsilon}(1)}$.
  As a corollary of our result, we also obtain the first explicit construction of LDPC codes achieving list decoding capacity, and in fact arbitrarily close to the generalized Singleton bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07308v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Granha Jeronimo, Tushant Mittal, Shashank Srivastava, Madhur Tulsiani</dc:creator>
    </item>
    <item>
      <title>Beamfocusing Capabilities of a Uniform Linear Array in the Holographic Regime</title>
      <link>https://arxiv.org/abs/2502.07318</link>
      <description>arXiv:2502.07318v1 Announce Type: new 
Abstract: The use of multiantenna technologies in the near field offers the possibility of focusing the energy in spatial regions rather than just in angle. The objective of this paper is to provide a formal framework that allows to establish the region in space where this effect can take place and how efficient this focusing can be, assuming that the transmit architecture is a uniform linear array (ULA). A dyadic Green's channel model is adopted, and the amplitude differences between the receiver and each transmit antenna are effectively incorporated in the model. By considering a second-order expansion of the SNR around the intended receiver, a formal criterion is derived in order to establish whether beamfocusing is feasible or not. An analytic description is provided that determines the shape and position of the asymptotic ellipsoid where a minimum SNR is achieved. Further insights are provided by considering the holographic regime, whereby the number of elements of the ULA increase without bound while the distance between adjacent elements converges to zero. This asymptotic framework allows to simplify the analytical form of the beamfocusing feasibility region, which in turn provides some further insights into the shape of the coverage regions depending on the position of the intended receiver. In particular, it is shown that beamfocusing is only possible if the size of the ULA is at least $4.4\lambda$ where $\lambda$ is the transmission wavelength. Furthermore, a closed form analytical expression is provided that asymptotically determines the maximum distance where beamfocusing is feasible as a function of the elevation angle. In particular, beamfocusing is only feasible when the receiver is located between a minimum and a maximum distance from the array, where these upper and lower distance limits effectively depend on the angle of elevation</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07318v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xavier Mestre, Adrian Agustin</dc:creator>
    </item>
    <item>
      <title>Performance Bounds and Degree-Distribution Optimization of Finite-Length BATS Codes</title>
      <link>https://arxiv.org/abs/2502.07355</link>
      <description>arXiv:2502.07355v1 Announce Type: new 
Abstract: Batched sparse (BATS) codes were proposed as a reliable communication solution for networks with packet loss. In the finite-length regime, the error probability of BATS codes under belief propagation (BP) decoding has been studied in the literature and can be analyzed by recursive formulae. However, all existing analyses have not considered precoding or have treated the BATS code and the precode as two separate entities. In this paper, we analyze the word-wise error probability of finite-length BATS codes with a precode under joint decoding, including BP decoding and maximum-likelihood (ML) decoding. The joint BP decoder performs peeling decoding on a joint Tanner graph constructed from both the BATS and the precode Tanner graphs, and the joint ML decoder solves a single linear system with all linear constraints implied by the BATS code and the precode. We derive closed-form upper bounds on the error probability for both decoders. Specifically, low-density parity-check (LDPC) precodes are used for BP decoding, and any generic precode can be used for ML decoding. Even for BATS codes without a precode, the derived upper bound for BP decoding is more accurate than the approximate recursive formula, and easier to compute than the exact recursive formula. The accuracy of the two upper bounds has been verified by many simulation results. Based on the two upper bounds, we formulate an optimization problem to optimize the degree distribution of LDPC-precoded BATS codes, which improves BP performance, ML performance, or both. In our experiments, to transmit 128 packets over a line network with packet loss, the optimized LDPC-precoded BATS codes reduce the transmission overhead to less than 50% of that of standard BATS codes under comparable decoding complexity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07355v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3536295</arxiv:DOI>
      <dc:creator>Mingyang Zhu, Shenghao Yang, Ming Jiang, Chunming Zhao</dc:creator>
    </item>
    <item>
      <title>Bidirectional Piggybacking Design for Systematic Nodes with Sub-Packetization $l=2$</title>
      <link>https://arxiv.org/abs/2502.07368</link>
      <description>arXiv:2502.07368v1 Announce Type: new 
Abstract: In 2013, Rashmi et al. proposed the piggybacking design framework to reduce the repair bandwidth of $(n,k;l)$ MDS array codes with small sub-packetization $l$ and it has been studied extensively in recent years. In this work, we propose an explicit bidirectional piggybacking design (BPD) with sub-packetization $l=2$ and the field size $q=O(n^{\lfloor r/2 \rfloor \!+\!1})$ for systematic nodes, where $r=n-k$ equals the redundancy of an $(n,k)$ linear code. And BPD has lower average repair bandwidth than previous piggybacking designs for $l=2$ when $r\geq 3$. Surprisingly, we can prove that the field size $q\leq 256$ is sufficient when $n\leq 15$ and $n-k\leq 4$. For example, we provide the BPD for the $(14,10)$ Reed-Solomon (RS) code over $\mathbb{F}_{2^8}$ and obtain approximately $41\%$ savings in the average repair bandwidth for systematic nodes compared with the trivial repair approach. This is the lowest repair bandwidth achieved so far for $(14,10)_{256}$ RS codes with sub-packetization $l=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07368v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Wang</dc:creator>
    </item>
    <item>
      <title>Capacity of the Binary Energy Harvesting Channel</title>
      <link>https://arxiv.org/abs/2502.07566</link>
      <description>arXiv:2502.07566v1 Announce Type: new 
Abstract: The capacity of a channel with an energy-harvesting (EH) encoder and a finite battery remains an open problem, even in the noiseless case. A key instance of this scenario is the binary EH channel (BEHC), where the encoder has a unit-sized battery and binary inputs. Existing capacity expressions for the BEHC are not computable, motivating this work, which determines the capacity to any desired precision via convex optimization. By modeling the system as a finite-state channel with state information known causally at the encoder, we derive single-letter lower and upper bounds using auxiliary directed graphs, termed $Q$-graphs. These $Q$-graphs exhibit a special structure with a finite number of nodes, $N$, enabling the formulation of the bounds as convex optimization problems. As $N$ increases, the bounds tighten and converge to the capacity with a vanishing gap of $O(N)$. For any EH probability parameter $\eta\in \{0.1,0.2, \dots, 0.9\}$, we compute the capacity with a precision of ${1e-6}$, outperforming the best-known bounds in the literature. Finally, we extend this framework to noisy EH channels with feedback, and present numerical achievable rates for the binary symmetric channel using a Markov decision process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07566v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eli Shemuel, Oron Sabag, Haim H. Permuter</dc:creator>
    </item>
    <item>
      <title>Next-to-minimal weight of toric codes defined over hypersimplices</title>
      <link>https://arxiv.org/abs/2502.07718</link>
      <description>arXiv:2502.07718v1 Announce Type: new 
Abstract: Toric codes are a type of evaluation codes introduced by J.P. Hansen in 2000. They are produced by evaluating (a vector space composed by) polynomials at the points of $(\mathbb{F}_q^*)^s$, the monomials of these polynomials being related to a certain polytope. Toric codes related to hypersimplices are the result of the evaluation of a vector space of square-free homogeneous polynomials of degree $d$. The dimension and minimum distance of toric codes related to hypersimplices have been determined by Jaramillo et al. in 2021. The next-to-minimal weight in the case $d = 1$ has been determined by Jaramillo-Velez et al. in 2023. In this work we use tools from Gr\"obner basis theory to determine the next-to-minimal weight of these codes for $d$ such that $3 \leq d \leq \frac{s - 2}{2}$ or $\frac{s + 2}{2} \leq d &lt; s$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07718v1</guid>
      <category>cs.IT</category>
      <category>math.AC</category>
      <category>math.AG</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'icero Carvalho, Nupur Patanker</dc:creator>
    </item>
    <item>
      <title>Information-theoretic Bayesian Optimization: Survey and Tutorial</title>
      <link>https://arxiv.org/abs/2502.06789</link>
      <description>arXiv:2502.06789v1 Announce Type: cross 
Abstract: Several scenarios require the optimization of non-convex black-box functions, that are noisy expensive to evaluate functions with unknown analytical expression, whose gradients are hence not accessible. For example, the hyper-parameter tuning problem of machine learning models. Bayesian optimization is a class of methods with state-of-the-art performance delivering a solution to this problem in real scenarios. It uses an iterative process that employs a probabilistic surrogate model, typically a Gaussian process, of the objective function to be optimized computing a posterior predictive distribution of the black-box function. Based on the information given by this posterior predictive distribution, Bayesian optimization includes the computation of an acquisition function that represents, for every input space point, the utility of evaluating that point in the next iteraiton if the objective of the process is to retrieve a global extremum. This paper is a survey of the information theoretical acquisition functions, whose performance typically outperforms the rest of acquisition functions. The main concepts of the field of information theory are also described in detail to make the reader aware of why information theory acquisition functions deliver great results in Bayesian optimization and how can we approximate them when they are intractable. We also cover how information theory acquisition functions can be adapted to complex optimization scenarios such as the multi-objective, constrained, non-myopic, multi-fidelity, parallel and asynchronous settings and provide further lines of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06789v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo C. Garrido-Merch\'an</dc:creator>
    </item>
    <item>
      <title>Semantics-Aware Updates from Remote IoT Devices to Interconnected LEO Satellites</title>
      <link>https://arxiv.org/abs/2502.07069</link>
      <description>arXiv:2502.07069v1 Announce Type: cross 
Abstract: Providing timely and informative data in Integrated Terrestrial and Non-Terrestrial Networks (T-NTNs) is critical as data volume continues to grow while the resources available on devices remain limited. To address this, we adopt a semantics-aware approach to optimize the Version Age of Information (VAoI) in a status update system in which a remote Energy Harvesting (EH) Internet of Things (IoT) device samples data and transmits it to a network of interconnected Low Earth Orbit (LEO) satellites for dissemination and utilization. The optimal update policy is derived through stochastic modeling and optimization of the VAoI across the network. The results indicate that this policy reduces the frequency of updates by skipping stale or irrelevant data, significantly improving energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07069v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erfan Delfani, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>Effcient classical error correction for parity encoded spin systems</title>
      <link>https://arxiv.org/abs/2502.07170</link>
      <description>arXiv:2502.07170v1 Announce Type: cross 
Abstract: Fast solvers for combinatorial optimization problems (COPs) have attracted engineering interest in various industrial and social applications. Quantum annealing (QA) has emerged as a promising candidate and significant efforts have been dedicated to its development. Since COP is encoded in the Ising interaction between logical spins, its realization requires a spin system with all-to-all connectivity, which poses technical difficulties in the physical implementation of large-scale QA devices. W. Lechner, P. Hauke, and P. Zoller proposed parity-encoding (PE) architecture, consisting of a larger system of physical spins with only local connectivities between them, to avoid this diffculty in the near future QA device development. They suggested that this architecture not only reduces implementation diffculties and improves scalability, but also has intrinsic fault tolerance because logical spins are redundantly and nonlocally encoded into the physical spins. Nevertheless, it remains unclear how these advantageous features can be exploited. This paper addresses how to correct errors in a spin readout of PE architecture. Our work is based on the close connection between PE architecture and classical low-density parity-check (LDPC) codes. We have shown that independent and identically distributed errors in a spin readout can be corrected by a very simple decoding algorithm that can be regarded as a bit flipping (BF) algorithm for the LDPC codes. The BF algorithm was shown to have comparable performance to the belief propagation (BP) decoding algorithm. Furthermore, it is suggested that the introduction of post-readout BF decoding reduces the total computational cost and improves the performance of the global optimal solution search using the PE architecture. We believe that our results indicate that the PE architecture is a promising platform for near-term QA devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07170v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshihiro Nambu</dc:creator>
    </item>
    <item>
      <title>Fixed-Confidence Best Arm Identification with Decreasing Variance</title>
      <link>https://arxiv.org/abs/2502.07199</link>
      <description>arXiv:2502.07199v1 Announce Type: cross 
Abstract: We focus on the problem of best-arm identification in a stochastic multi-arm bandit with temporally decreasing variances for the arms' rewards. We model arm rewards as Gaussian random variables with fixed means and variances that decrease with time. The cost incurred by the learner is modeled as a weighted sum of the time needed by the learner to identify the best arm, and the number of samples of arms collected by the learner before termination. Under this cost function, there is an incentive for the learner to not sample arms in all rounds, especially in the initial rounds. On the other hand, not sampling increases the termination time of the learner, which also increases cost. This trade-off necessitates new sampling strategies. We propose two policies. The first policy has an initial wait period with no sampling followed by continuous sampling. The second policy samples periodically and uses a weighted average of the rewards observed to identify the best arm. We provide analytical guarantees on the performance of both policies and supplement our theoretical results with simulations which show that our polices outperform the state-of-the-art policies for the classical best arm identification problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07199v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tamojeet Roychowdhury, Kota Srinivas Reddy, Krishna P Jagannathan, Sharayu Moharir</dc:creator>
    </item>
    <item>
      <title>Learnable Residual-based Latent Denoising in Semantic Communication</title>
      <link>https://arxiv.org/abs/2502.07319</link>
      <description>arXiv:2502.07319v1 Announce Type: cross 
Abstract: A latent denoising semantic communication (SemCom) framework is proposed for robust image transmission over noisy channels. By incorporating a learnable latent denoiser into the receiver, the received signals are preprocessed to effectively remove the channel noise and recover the semantic information, thereby enhancing the quality of the decoded images. Specifically, a latent denoising mapping is established by an iterative residual learning approach to improve the denoising efficiency while ensuring stable performance. Moreover, channel signal-to-noise ratio (SNR) is utilized to estimate and predict the latent similarity score (SS) for conditional denoising, where the number of denoising steps is adapted based on the predicted SS sequence, further reducing the communication latency. Finally, simulations demonstrate that the proposed framework can effectively and efficiently remove the channel noise at various levels and reconstruct visual-appealing images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07319v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingkai Xu, Yongpeng Wu, Yuxuan Shi, Xiang-Gen Xia, Wenjun Zhang, Ping Zhang</dc:creator>
    </item>
    <item>
      <title>Frequency-selective Dynamic Scattering Arrays for Over-the-air EM Processing</title>
      <link>https://arxiv.org/abs/2502.07336</link>
      <description>arXiv:2502.07336v1 Announce Type: cross 
Abstract: In this paper, we investigate frequency-selective dynamic scattering array (DSA), a versatile antenna structure capable of performing joint wave-based computing and radiation by transitioning signal processing tasks from the digital domain to the electromagnetic (EM) domain. The numerical results demonstrate the potential of DSAs to produce space-frequency superdirective responses with minimal usage of radiofrequency (RF) chains, making it particularly attractive for future holographic multiple-input multiple-output (MIMO) systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07336v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Dardari</dc:creator>
    </item>
    <item>
      <title>Coarse Set Theory: A Mathematical Foundation for Coarse Ethics</title>
      <link>https://arxiv.org/abs/2502.07347</link>
      <description>arXiv:2502.07347v1 Announce Type: cross 
Abstract: In ethical decision-making, individuals are often evaluated based on generalized assessments rather than precise individual performance. This concept, known as Coarse Ethics (CE), has primarily been discussed in natural language without a formal mathematical foundation. This paper introduces Coarse Set Theory (CST) to establish a mathematical framework for CE. We define coarse sets using totally ordered sets and propose axioms that characterize the hierarchical relationships between elements and their groupings. Additionally, we introduce coarse-grained sets, which partition an underlying set into equivalence classes based on predefined criteria. We extend this framework by defining coarse mappings, which transform detailed individual data into coarser representations while maintaining essential structural properties. To measure the information loss, we employ Kullback-Leibler (KL) divergence, demonstrating how different coarse partitions affect the preservation of information. We illustrate how CST can be applied to real-world grading systems through theoretical formulations and empirical analysis. This study provides a rigorous foundation for CE, enabling a more systematic exploration of fairness, interpretability, and decision-making trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07347v1</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.LO</category>
      <category>math.PR</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Takashi Izumo</dc:creator>
    </item>
    <item>
      <title>Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks</title>
      <link>https://arxiv.org/abs/2401.13779</link>
      <description>arXiv:2401.13779v2 Announce Type: replace 
Abstract: Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of nodes. The sampling occurs in a probabilistic manner, and the elements of the mixing matrices, along with their sampling probabilities, are jointly optimized. Simulation results demonstrate that $\texttt{BASS}$ enables faster convergence with fewer transmission slots compared to existing link-based scheduling methods. In conclusion, the inherent broadcasting nature of wireless channels offers intrinsic advantages in accelerating the convergence of decentralized optimization and learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13779v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel P\'erez Herrera, Zheng Chen, Erik G. Larsson</dc:creator>
    </item>
    <item>
      <title>Variable-Length Feedback Codes over Known and Unknown Channels with Non-vanishing Error Probabilities</title>
      <link>https://arxiv.org/abs/2401.16726</link>
      <description>arXiv:2401.16726v2 Announce Type: replace 
Abstract: We study variable-length feedback (VLF) codes with noiseless feedback for discrete memoryless channels. We present a novel non-asymptotic bound, which analyzes the average error probability and average decoding time of our modified Yamamoto--Itoh scheme. We then optimize the parameters of our code in the asymptotic regime where the average error probability $\epsilon$ remains a constant as the average decoding time $N$ approaches infinity. Our second-order achievability bound is an improvement of Polyanskiy et al.'s (2011) achievability bound. We also universalize our code by employing the empirical mutual information in our decoding metric and derive a second-order achievability bound for universal VLF codes. Our results for both VLF and universal VLF codes are extended to the additive white Gaussian noise channel with an average power constraint. The former yields an improvement over Truong and Tan's (2017) achievability bound. The proof of our results for universal VLF codes uses a refined version of the method of types and an asymptotic expansion from the nonlinear renewal theory literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16726v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Recep Can Yavas, Vincent Y. F. Tan</dc:creator>
    </item>
    <item>
      <title>Average entropy of Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2404.07311</link>
      <description>arXiv:2404.07311v2 Announce Type: replace 
Abstract: We calculate the average differential entropy of a $q$-component Gaussian mixture in $\mathbb R^n$. For simplicity, all components have covariance matrix $\sigma^2 {\mathbf 1}$, while the means $\{\mathbf{W}_i\}_{i=1}^{q}$ are i.i.d. Gaussian vectors with zero mean and covariance $s^2 {\mathbf 1}$. We obtain a series expansion in $\mu=s^2/\sigma^2$ for the average differential entropy up to order $\mathcal{O}(\mu^2)$, and we provide a recipe to calculate higher order terms. Our result provides an analytic approximation with a quantifiable order of magnitude for the error, which is not achieved in previous literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07311v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Basheer Joudeh, Boris \v{S}kori\'c</dc:creator>
    </item>
    <item>
      <title>Optimal Configuration of Reconfigurable Intelligent Surfaces With Non-uniform Phase Quantization</title>
      <link>https://arxiv.org/abs/2405.06967</link>
      <description>arXiv:2405.06967v2 Announce Type: replace 
Abstract: The existing methods for reconfigurable intelligent surface (RIS) beamforming in wireless communications are typically limited to uniform phase quantization. However, in practical applications, engineering challenges and design requirements often lead to non-uniform phase and bit resolution of RIS units, which limits the performance potential of these methods. To address this issue, this paper pioneers the study of discrete non-uniform phase configuration in RIS-assisted multiple-input single-output (MISO) communication and formulates an optimization model to characterize the problem. For single-user scenarios, the paper proposes a partition-andtraversal (PAT) algorithm that efficiently achieves the global optimal solution through systematic search and traversal. For larger-scale multi-user scenarios, aiming to balance performance and computational complexity, an enhanced PAT-based algorithm (E-PAT) is developed. By optimizing the search strategy, the E-PAT algorithm significantly reduces computational overhead and achieves linear complexity. Numerical simulations confirm the effectiveness and superiority of the proposed PAT and EPAT algorithms. Additionally, we provide a detailed analysis of the impact of non-uniform phase quantization on system performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06967v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialong Lu, Rujing Xiong, Tiebin Mi, Ke Yin, Robert Caiming Qiu</dc:creator>
    </item>
    <item>
      <title>An Extension of the Adversarial Threat Model in Quantitative Information Flow</title>
      <link>https://arxiv.org/abs/2409.04108</link>
      <description>arXiv:2409.04108v2 Announce Type: replace 
Abstract: In this paper, we propose an extended framework for quantitative information flow (QIF), aligned with the previously proposed core-concave generalization of entropy measures, to include adversaries that use Kolmogorov-Nagumo $f$-mean to infer secrets in a private system. Specifically, in our setting, an adversary uses Kolmogorov-Nagumo $f$-mean to compute its best actions before and after observing the system's randomized outputs. This leads to generalized notions of prior and posterior vulnerability and generalized axiomatic relations that we will derive to elucidate how these $f$-mean based vulnerabilities interact with each other. We demonstrate the usefulness of this framework by showing how some notions of leakage that had been derived outside of the QIF framework and so far seemed incompatible with it are indeed explainable via such an extension of QIF. These leakage measures include $\alpha$-leakage, which is the same as Arimoto mutual information of order $\alpha$, maximal $\alpha$-leakage, which is the $\alpha$-leakage capacity, and maximal $(\alpha,\beta)$-leakage, which is a generalization of the above and captures local differential privacy as a special case. We define the notion of generalized capacity and provide partial results for special classes of functions used in the Kolmogorov-Nagumo mean. We also propose a new pointwise notion of gain function, which we coin pointwise information gain. We show that this pointwise information gain can explain R{\'e}yni divergence and Sibson mutual information of order $\alpha \in [0,\infty]$ as the Kolmogorov-Nagumo average of the gain with a proper choice of function $f$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04108v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Amin Zarrabian, Parastoo Sadeghi</dc:creator>
    </item>
    <item>
      <title>OpenRANet: Neuralized Spectrum Access by Joint Subcarrier and Power Allocation with Optimization-based Deep Learning</title>
      <link>https://arxiv.org/abs/2409.12964</link>
      <description>arXiv:2409.12964v3 Announce Type: replace 
Abstract: The next-generation radio access network (RAN), known as Open RAN, is poised to feature an AI-native interface for wireless cellular networks, including emerging satellite-terrestrial systems, making deep learning integral to its operation. In this paper, we address the nonconvex optimization challenge of joint subcarrier and power allocation in Open RAN, with the objective of minimizing the total power consumption while ensuring users meet their transmission data rate requirements. We propose OpenRANet, an optimization-based deep learning model that integrates machine-learning techniques with iterative optimization algorithms. We start by transforming the original nonconvex problem into convex subproblems through decoupling, variable transformation, and relaxation techniques. These subproblems are then efficiently solved using iterative methods within the standard interference function framework, enabling the derivation of primal-dual solutions. These solutions integrate seamlessly as a convex optimization layer within OpenRANet, enhancing constraint adherence, solution accuracy, and computational efficiency by combining machine learning with convex analysis, as shown in numerical experiments. OpenRANet also serves as a foundation for designing resource-constrained AI-native wireless optimization strategies for broader scenarios like multi-cell systems, satellite-terrestrial networks, and future Open RAN deployments with complex power consumption requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12964v3</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siya Chen, Chee Wei Tan, Xiangping Zhai, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Estimating and using information in inverse problems</title>
      <link>https://arxiv.org/abs/2208.09095</link>
      <description>arXiv:2208.09095v4 Announce Type: replace-cross 
Abstract: In inverse problems, one attempts to infer spatially variable functions from indirect measurements of a system. To practitioners of inverse problems, the concept of "information" is familiar when discussing key questions such as which parts of the function can be inferred accurately and which cannot. For example, it is generally understood that we can identify system parameters accurately only close to detectors, or along ray paths between sources and detectors, because we have "the most information" for these places. Although referenced in many publications, the "information" that is invoked in such contexts is not a well understood and clearly defined quantity.
  Herein, we present a definition of information density that is based on the variance of coefficients as derived from a Bayesian reformulation of the inverse problem. We then discuss three areas in which this information density can be useful in practical algorithms for the solution of inverse problems, and illustrate the usefulness in one of these areas -- how to choose the discretization mesh for the function to be reconstructed -- using numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09095v4</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Bangerth, Chris R. Johnson, Dennis K. Njeru, Bart van Bloemen Waanders</dc:creator>
    </item>
    <item>
      <title>The Causal Information Bottleneck and Optimal Causal Variable Abstractions</title>
      <link>https://arxiv.org/abs/2410.00535</link>
      <description>arXiv:2410.00535v3 Announce Type: replace-cross 
Abstract: To effectively study complex causal systems, it is often useful to construct abstractions of parts of the system by discarding irrelevant details while preserving key features. The Information Bottleneck (IB) method is a widely used approach to construct variable abstractions by compressing random variables while retaining predictive power over a target variable. Traditional methods like IB are purely statistical and ignore underlying causal structures, making them ill-suited for causal tasks. We propose the Causal Information Bottleneck (CIB), a causal extension of the IB, which compresses a set of chosen variables while maintaining causal control over a target variable. This method produces abstractions of (sets of) variables which are causally interpretable, give us insight about the interactions between the abstracted variables and the target variable, and can be used when reasoning about interventions. We present experimental results demonstrating that the learned abstractions accurately capture causal relations as intended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00535v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen</dc:creator>
    </item>
    <item>
      <title>The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal Sample Complexity Analysis</title>
      <link>https://arxiv.org/abs/2410.07616</link>
      <description>arXiv:2410.07616v2 Announce Type: replace-cross 
Abstract: We study the sample complexity of the plug-in approach for learning $\varepsilon$-optimal policies in average-reward Markov decision processes (MDPs) with a generative model. The plug-in approach constructs a model estimate then computes an average-reward optimal policy in the estimated model. Despite representing arguably the simplest algorithm for this problem, the plug-in approach has never been theoretically analyzed. Unlike the more well-studied discounted MDP reduction method, the plug-in approach requires no prior problem information or parameter tuning. Our results fill this gap and address the limitations of prior approaches, as we show that the plug-in approach is optimal in several well-studied settings without using prior knowledge. Specifically it achieves the optimal diameter- and mixing-based sample complexities of $\widetilde{O}\left(SA \frac{D}{\varepsilon^2}\right)$ and $\widetilde{O}\left(SA \frac{\tau_{\mathrm{unif}}}{\varepsilon^2}\right)$, respectively, without knowledge of the diameter $D$ or uniform mixing time $\tau_{\mathrm{unif}}$. We also obtain span-based bounds for the plug-in approach, and complement them with algorithm-specific lower bounds suggesting that they are unimprovable. Our results require novel techniques for analyzing long-horizon problems which may be broadly useful and which also improve results for the discounted plug-in approach, removing effective-horizon-related sample size restrictions and obtaining the first optimal complexity bounds for the full range of sample sizes without reward perturbation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07616v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Zurek, Yudong Chen</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Change Detection for Unnormalized Pre- and Post-Change Distributions</title>
      <link>https://arxiv.org/abs/2410.14615</link>
      <description>arXiv:2410.14615v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of detecting changes when only unnormalized pre- and post-change distributions are accessible. This situation happens in many scenarios in physics such as in ferromagnetism, crystallography, magneto-hydrodynamics, and thermodynamics, where the energy models are difficult to normalize.
  Our approach is based on the estimation of the Cumulative Sum (CUSUM) statistics, which is known to produce optimal performance. We first present an intuitively appealing approximation method. Unfortunately, this produces a biased estimator of the CUSUM statistics and may cause performance degradation. We then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM) algorithm based on thermodynamic integration (TI) in order to estimate the log-ratio of normalizing constants of pre- and post-change distributions. It is proved that this approach gives an unbiased estimate of the log-partition function and the CUSUM statistics, and leads to an asymptotically optimal performance. Moreover, we derive a relationship between the required sample size for thermodynamic integration and the desired detection delay performance, offering guidelines for practical parameter selection. Numerical studies are provided demonstrating the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14615v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Adibi, Sanjeev Kulkarni, H. Vincent Poor, Taposh Banerjee, Vahid Tarokh</dc:creator>
    </item>
  </channel>
</rss>
