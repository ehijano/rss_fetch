<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 05:51:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Missing the Margins: A Systematic Literature Review on the Demographic Representativeness of LLMs</title>
      <link>https://arxiv.org/abs/2511.01864</link>
      <description>arXiv:2511.01864v1 Announce Type: new 
Abstract: Many applications of Large Language Models (LLMs) require them to either simulate people or offer personalized functionality, making the demographic representativeness of LLMs crucial for equitable utility. At the same time, we know little about the extent to which these models actually reflect the demographic attributes and behaviors of certain groups or populations, with conflicting findings in empirical research. To shed light on this debate, we review 211 papers on the demographic representativeness of LLMs. We find that while 29% of the studies report positive conclusions on the representativeness of LLMs, 30% of these do not evaluate LLMs across multiple demographic categories or within demographic subcategories. Another 35% and 47% of the papers concluding positively fail to specify these subcategories altogether for gender and race, respectively. Of the articles that do report subcategories, fewer than half include marginalized groups in their study. Finally, more than a third of the papers do not define the target population to whom their findings apply; of those that do define it either implicitly or explicitly, a large majority study only the U.S. Taken together, our findings suggest an inflated perception of LLM representativeness in the broader community. We recommend more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible use of LLMs for social applications. Our annotated list of papers and analysis code is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01864v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2025.findings-acl.1246</arxiv:DOI>
      <arxiv:journal_reference>Findings of the Association for Computational Linguistics (ACL 2025), page 24263</arxiv:journal_reference>
      <dc:creator>Indira Sen, Marlene Lutz, Elisa Rogers, David Garcia, Markus Strohmaier</dc:creator>
    </item>
    <item>
      <title>Story and essential meaning dynamics in Bangladesh's July 2024 Student-People's Uprising</title>
      <link>https://arxiv.org/abs/2511.01865</link>
      <description>arXiv:2511.01865v1 Announce Type: new 
Abstract: News media serves a crucial role in disseminating information and shaping public perception, especially during periods of political unrest. Using over 50,0000 YouTube comments on news coverage from July 16 to August 6, 2024, we investigate the emotional dynamics and evolving discourse of public perception during the July 2024 Student-People's Uprising in Bangladesh. Through integrated analyses of sentiment, emotion, topic, lexical discourse, timeline progression, sentiment shifts, and allotaxonometry, we show how negative sentiment dominated during the movement. We find a negative correlation between comment happiness and number of protest deaths $(r = -0.45,\p = 0.00)$. Using an ousiometer to measure essential meaning, we find public responses reflect a landscape of power, aggression, and danger, alongside persistent expressions of hope, moral conviction, and empowerment through goodnesses. Topic discourse progressed during the movement, with peaks in `Political Conflict', `Media Flow', and `Student Violence' during crisis surges, while topics like `Social Resistance' and `Digital Movement' persisted amid repression. Sentiment shifts reveal that after the second internet blackout, average happiness increased, driven by the more frequent use of positive words such as `victory', `peace' and `freedom' and a decrease in negative terms such as `death' and `lies'. Finally, through allotaxonometric analysis, we observe a clear shift from protest to justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01865v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tabia Tanzin Prama, Christopher M. Danforth, Peter Sheridan Dodds</dc:creator>
    </item>
    <item>
      <title>Online Behavioral Advertising: A Literature Review and Research Agenda</title>
      <link>https://arxiv.org/abs/2511.01895</link>
      <description>arXiv:2511.01895v1 Announce Type: new 
Abstract: Advertisers are increasingly monitoring people's online behavior and using the information collected to show people individually targeted advertisements. This phenomenon is called online behavioral advertising (OBA). Although advertisers can benefit from OBA, the practice also raises concerns about privacy. Therefore, OBA has received much attention from advertisers, consumers, policymakers, and scholars. Despite this attention, there is neither a strong definition of OBA nor a clear accumulation of empirical findings. This article defines OBA and provides an overview of the empirical findings by developing a framework that identifies and integrates all factors that can explain consumer responses toward OBA. The framework suggests that the outcomes of OBA are dependent on advertiser-controlled factors (e.g., the level of personalization) and consumer-controlled factors (e.g., knowledge and perceptions about OBA and individual characteristics). The article also overviews the theoretical positioning of OBA by placing the theories that are used to explain consumers' responses to OBA in our framework. Finally, we develop a research agenda and discuss implications for policymakers and advertisers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01895v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/00913367.2017.1339368</arxiv:DOI>
      <arxiv:journal_reference>Journal of Advertising 2017, 46(3), 363-376</arxiv:journal_reference>
      <dc:creator>Sophie C. Boerman, Sanne Kruikemeier, Frederik J. Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Before the Clinic: Transparent and Operable Design Principles for Healthcare AI</title>
      <link>https://arxiv.org/abs/2511.01902</link>
      <description>arXiv:2511.01902v1 Announce Type: new 
Abstract: The translation of artificial intelligence (AI) systems into clinical practice requires bridging fundamental gaps between explainable AI theory, clinician expectations, and governance requirements. While conceptual frameworks define what constitutes explainable AI (XAI) and qualitative studies identify clinician needs, little practical guidance exists for development teams to prepare AI systems prior to clinical evaluation. We propose two foundational design principles, Transparent Design and Operable Design, that operationalize pre-clinical technical requirements for healthcare AI. Transparent Design encompasses interpretability and understandability artifacts that enable case-level reasoning and system traceability. Operable Design encompasses calibration, uncertainty, and robustness to ensure reliable, predictable system behavior under real-world conditions. We ground these principles in established XAI frameworks, map them to documented clinician needs, and demonstrate their alignment with emerging governance requirements. This pre-clinical playbook provides actionable guidance for development teams, accelerates the path to clinical evaluation, and establishes a shared vocabulary bridging AI researchers, healthcare practitioners, and regulatory stakeholders. By explicitly scoping what can be built and verified before clinical deployment, we aim to reduce friction in clinical AI translation while remaining cautious about what constitutes validated, deployed explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01902v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Bakumenko (Clemson University, USA), Aaron J. Masino (Clemson University, USA), Janine Hoelscher (Clemson University, USA)</dc:creator>
    </item>
    <item>
      <title>Thinking Like a Student: AI-Supported Reflective Planning in a Theory-Intensive Computer Science Course</title>
      <link>https://arxiv.org/abs/2511.01906</link>
      <description>arXiv:2511.01906v1 Announce Type: new 
Abstract: In the aftermath of COVID-19, many universities implemented supplementary "reinforcement" roles to support students in demanding courses. Although the name for such roles may differ between institutions, the underlying idea of providing structured supplementary support is common. However, these roles were often poorly defined, lacking structured materials, pedagogical oversight, and integration with the core teaching team. This paper reports on the redesign of reinforcement sessions in a challenging undergraduate course on formal methods and computational models, using a large language model (LLM) as a reflective planning tool. The LLM was prompted to simulate the perspective of a second-year student, enabling the identification of conceptual bottlenecks, gaps in intuition, and likely reasoning breakdowns before classroom delivery. These insights informed a structured, repeatable session format combining targeted review, collaborative examples, independent student work, and guided walkthroughs. Conducted over a single semester, the intervention received positive student feedback, indicating increased confidence, reduced anxiety, and improved clarity, particularly in abstract topics such as the pumping lemma and formal language expressive power comparisons. The findings suggest that reflective, instructor-facing use of LLMs can enhance pedagogical design in theoretically dense domains and may be adaptable to other cognitively demanding computer science courses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01906v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noa Izsak</dc:creator>
    </item>
    <item>
      <title>Between Myths and Metaphors: Rethinking LLMs for SRH in Conservative Contexts</title>
      <link>https://arxiv.org/abs/2511.01907</link>
      <description>arXiv:2511.01907v1 Announce Type: new 
Abstract: Low-resource countries represent over 90% of maternal deaths, with Pakistan among the top four countries contributing nearly half in 2023. Since these deaths are mostly preventable, large language models (LLMs) can help address this crisis by automating health communication and risk assessment. However, sexual and reproductive health (SRH) communication in conservative contexts often relies on indirect language that obscures meaning, complicating LLM-based interventions. We conduct a two-stage study in Pakistan: (1) analyzing data from clinical observations, interviews, and focus groups with clinicians and patients, and (2) evaluating the interpretive capabilities of five popular LLMs on this data. Our analysis identifies two axes of communication (referential domain and expression approach) and shows LLMs struggle with semantic drift, myths, and polysemy in clinical interactions. We contribute: (1) empirical themes in SRH communication, (2) a categorization framework for indirect communication, (3) evaluation of LLM performance, and (4) design recommendations for culturally-situated SRH communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01907v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ameemah Humayun, Bushra Zubair, Maryam Mustafa</dc:creator>
    </item>
    <item>
      <title>When Assurance Undermines Intelligence: The Efficiency Costs of Data Governance in AI-Enabled Labor Markets</title>
      <link>https://arxiv.org/abs/2511.01923</link>
      <description>arXiv:2511.01923v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) like Large Language Model (LLM) is increasingly integrated into digital platforms to enhance information access, deliver personalized experiences, and improve matching efficiency. However, these algorithmic advancements rely heavily on large-scale user data, creating a fundamental tension between information assurance-the protection, integrity, and responsible use of privacy data-and artificial intelligence-the learning capacity and predictive accuracy of models. We examine this assurance-intelligence trade-off in the context of LinkedIn, leveraging a regulatory intervention that suspended the use of user data for model training in Hong Kong. Using large-scale employment and job posting data from Revelio Labs and a Difference-in-Differences design, we show that restricting data use significantly reduced GenAI efficiency, leading to lower matching rates, higher employee turnover, and heightened labor market frictions. These effects were especially pronounced for small and fast-growing firms that rely heavily on AI for talent acquisition. Our findings reveal the unintended efficiency costs of well-intentioned data governance and highlight that information assurance, while essential for trust, can undermine intelligence-driven efficiency when misaligned with AI system design. This study contributes to emerging research on AI governance and digital platform by theorizing data assurance as an institutional complement-and potential constraint-to GenAI efficacy in data-intensive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01923v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Chen, Chaoyue Gao, Alvin Leung, Xiaoning Wang</dc:creator>
    </item>
    <item>
      <title>Vibe Learning: Education in the age of AI</title>
      <link>https://arxiv.org/abs/2511.01956</link>
      <description>arXiv:2511.01956v1 Announce Type: new 
Abstract: The debate over whether "thinking machines" could replace human intellectual labor has existed in both public and expert discussions since the mid-twentieth century, when the concept and terminology of Artificial Intelligence (AI) first emerged. For decades, this idea remained largely theoretical. However, with the recent advent of Generative AI - particularly Large Language Models (LLMs) - and the widespread adoption of tools such as ChatGPT, the issue has become a practical reality. Many fields that rely on human intellectual effort are now being reshaped by AI tools that both expand human capabilities and challenge the necessity of certain forms of work once deemed uniquely human but now easily automated. Education, somewhat unexpectedly, faces a pivotal responsibility: to devise long-term strategies for cultivating human skills that will remain relevant in an era of pervasive AI in the intellectual domain. In this context, we identify the limitations of current AI systems - especially those rooted in LLM technology - argue that the fundamental causes of these weaknesses cannot be resolved through existing methods, and propose directions within the constructivist paradigm for transforming education to preserve the long-term advantages of human intelligence over AI tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01956v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Florencio, Francielle Prieto</dc:creator>
    </item>
    <item>
      <title>The Other Side of the Screen: Motivations to Watch and Engage in Software Development Live Streams</title>
      <link>https://arxiv.org/abs/2511.02588</link>
      <description>arXiv:2511.02588v1 Announce Type: new 
Abstract: Background: With the popularity of live streaming platforms at an all-time high, and many people turning to alternative venues for educational needs, this full research paper explores the viewership habits of software and game development live streams through the lens of informal education opportunities. Purpose: We investigate why developers watch software and game development live streams to understand the educational and social benefits they derive from this emerging form of informal learning. Methods: We implement a mixed-methods study combining survey data from 39 viewers and nine semi-structured interviews to analyze motivations, perceptions, and outcomes of watching development live streams. Findings: This research finds that viewers are motivated by both educational and social factors, with community engagement and informal mentorship as key motivations. Additionally, we find that technical learning draws initial interest, but social connections and co-working aspects sustain long-term engagement. Implications: Live streaming serves as a valuable informal learning tool that combines self-directed technical education with community support, which suggests that developers can leverage these platforms for continuous learning and professional growth outside of or in addition to traditional educational structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02588v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ella Kokinda, D. M. Boyer</dc:creator>
    </item>
    <item>
      <title>Measuring AI Diffusion: A Population-Normalized Metric for Tracking Global AI Usage</title>
      <link>https://arxiv.org/abs/2511.02781</link>
      <description>arXiv:2511.02781v1 Announce Type: new 
Abstract: Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion. We find wide variation in adoption, with a strong correlation between AI User Share and GDP. High uptake is concentrated in developed economies, though usage among internet-connected populations in lower-income countries reveals substantial latent demand. We also detect sharp increases in usage following major product launches, such as DeepSeek in early 2025. While the metric's reliance solely on Microsoft telemetry introduces potential biases related to this user base, it offers an important new lens into how AI is spreading globally. AI User Share enables timely benchmarking that can inform data-driven AI policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02781v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amit Misra, Jane Wang, Scott McCullers, Kevin White, Juan Lavista Ferres</dc:creator>
    </item>
    <item>
      <title>Possible Futures for Cloud Cost Models</title>
      <link>https://arxiv.org/abs/2511.01862</link>
      <description>arXiv:2511.01862v1 Announce Type: cross 
Abstract: Cloud is now the leading software and computing hardware innovator, and is changing the landscape of compute to one that is optimized for artificial intelligence and machine learning (AI/ML). Computing innovation was initially driven to meet the needs of scientific computing. As industry and consumer usage of computing proliferated, there was a shift to satisfy a multipolar customer base. Demand for AI/ML now dominates modern computing and innovation has centralized on cloud. As a result, cost and resource models designed to serve AI/ML use cases are not currently well suited for science. If resource contention resulting from a unipole consumer makes access to contended resources harder for scientific users, a likely future is running scientific workloads where they were not intended. In this article, we discuss the past, current, and possible futures of cloud cost models for the continued support of discovery and science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01862v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vanessa Sochat, Daniel Milroy</dc:creator>
    </item>
    <item>
      <title>Watermarking Discrete Diffusion Language Models</title>
      <link>https://arxiv.org/abs/2511.02083</link>
      <description>arXiv:2511.02083v1 Announce Type: cross 
Abstract: Watermarking has emerged as a promising technique to track AI-generated content and differentiate it from authentic human creations. While prior work extensively studies watermarking for autoregressive large language models (LLMs) and image diffusion models, none address discrete diffusion language models, which are becoming popular due to their high inference throughput. In this paper, we introduce the first watermarking method for discrete diffusion models by applying the distribution-preserving Gumbel-max trick at every diffusion step and seeding the randomness with the sequence index to enable reliable detection. We experimentally demonstrate that our scheme is reliably detectable on state-of-the-art diffusion language models and analytically prove that it is distortion-free with an exponentially decaying probability of false detection in the token sequence length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02083v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avi Bagchi, Akhil Bhimaraju, Moulik Choraria, Daniel Alabi, Lav R. Varshney</dc:creator>
    </item>
    <item>
      <title>Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences</title>
      <link>https://arxiv.org/abs/2511.02109</link>
      <description>arXiv:2511.02109v1 Announce Type: cross 
Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features -- for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR) -- the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02109v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Hua Shen, Sai Avula, Eric Gilbert, Ceren Budak</dc:creator>
    </item>
    <item>
      <title>Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning</title>
      <link>https://arxiv.org/abs/2511.02194</link>
      <description>arXiv:2511.02194v1 Announce Type: cross 
Abstract: Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02194v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yibo Zhao, Yang Zhao, Hongru Du, Hao Frank Yang</dc:creator>
    </item>
    <item>
      <title>Community Notes are Vulnerable to Rater Bias and Manipulation</title>
      <link>https://arxiv.org/abs/2511.02615</link>
      <description>arXiv:2511.02615v1 Announce Type: cross 
Abstract: Social media platforms increasingly rely on crowdsourced moderation systems like Community Notes to combat misinformation at scale. However, these systems face challenges from rater bias and potential manipulation, which may undermine their effectiveness. Here we systematically evaluate the Community Notes algorithm using simulated data that models realistic rater and note behaviors, quantifying error rates in publishing helpful versus unhelpful notes. We find that the algorithm suppresses a substantial fraction of genuinely helpful notes and is highly sensitive to rater biases, including polarization and in-group preferences. Moreover, a small minority (5--20\%) of bad raters can strategically suppress targeted helpful notes, effectively censoring reliable information. These findings suggest that while community-driven moderation may offer scalability, its vulnerability to bias and manipulation raises concerns about reliability and trustworthiness, highlighting the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02615v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bao Tran Truong, Siqi Wu, Alessandro Flammini, Filippo Menczer, Alexander J. Stewart</dc:creator>
    </item>
    <item>
      <title>Feedback dynamics in Politics: The interplay between sentiment and engagement</title>
      <link>https://arxiv.org/abs/2511.02663</link>
      <description>arXiv:2511.02663v1 Announce Type: cross 
Abstract: We investigate feedback mechanisms in political communication by testing whether politicians adapt the sentiment of their messages in response to public engagement. Using over 1.5 million tweets from Members of Parliament in the United Kingdom, Spain, and Greece during 2021, we identify sentiment dynamics through a simple yet interpretable linear model. The analysis reveals a closed-loop behavior: engagement with positive and negative messages influences the sentiment of subsequent posts. Moreover, the learned coefficients highlight systematic differences across political roles: opposition members are more reactive to negative engagement, whereas government officials respond more to positive signals. These results provide a quantitative, control-oriented view of behavioral adaptation in online politics, showing how feedback principles can explain the self-reinforcing dynamics that emerge in social media discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02663v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Formentin</dc:creator>
    </item>
    <item>
      <title>AI Diffusion in Low Resource Language Countries</title>
      <link>https://arxiv.org/abs/2511.02752</link>
      <description>arXiv:2511.02752v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02752v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres</dc:creator>
    </item>
    <item>
      <title>From prediction to explanation: managing influential negative reviews through explainable AI</title>
      <link>https://arxiv.org/abs/2412.19692</link>
      <description>arXiv:2412.19692v2 Announce Type: replace 
Abstract: The profound impact of online reviews on consumer decision-making has made it crucial for businesses to manage negative reviews. Recent advancements in artificial intelligence (AI) technology have offered businesses novel and effective ways to manage and analyze substantial consumer feedback. In response to the growing demand for explainablility and transparency in AI applications, this study proposes a novel explainable AI (XAI) algorithm aimed at identifying influential negative reviews. The experiments conducted on 101,338 restaurant reviews validate the algorithm's effectiveness and provides understandable explanations from both the feature-level and word-level perspectives. By leveraging this algorithm, businesses can gain actionable insights for predicting, perceiving, and strategically responding to online negative feedback, fostering improved customer service and mitigating the potential damage caused by negative reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19692v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongping Shen</dc:creator>
    </item>
    <item>
      <title>Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity</title>
      <link>https://arxiv.org/abs/2508.11708</link>
      <description>arXiv:2508.11708v3 Announce Type: replace 
Abstract: Urban centers undergo social, demographic, and cultural changes that shape public street use and require systematic evaluation of public spaces. This study presents Street Review, a mixed-methods approach that combines participatory research with AI-based analysis to assess streetscape inclusivity. In Montr\'eal, Canada, 28 residents participated in semi-directed interviews and image evaluations, supported by the analysis of approximately 45,000 street-view images from Mapillary. The approach produced visual analytics, such as heatmaps, to correlate subjective user ratings with physical attributes like sidewalk, maintenance, greenery, and seating. Findings reveal variations in perceptions of inclusivity and accessibility across demographic groups, demonstrating that incorporating diverse user feedback can enhance machine learning models through careful data-labeling and co-production strategies. The Street Review framework offers a systematic method for urban planners and policy analysts to inform planning, policy development, and management of public streets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11708v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cities.2025.106602</arxiv:DOI>
      <arxiv:journal_reference>Cities, 170, 106602 (2026)</arxiv:journal_reference>
      <dc:creator>Rashid Mushkani, Shin Koseki</dc:creator>
    </item>
    <item>
      <title>AI for a Planet Under Pressure</title>
      <link>https://arxiv.org/abs/2510.24373</link>
      <description>arXiv:2510.24373v3 Announce Type: replace 
Abstract: Artificial intelligence (AI) is already driving scientific breakthroughs in a variety of research fields, ranging from the life sciences to mathematics. This raises a critical question: can AI be applied both responsibly and effectively to address complex and interconnected sustainability challenges? This report is the result of a collaboration between the Stockholm resilience Centre (Stockholm University), the Potsdam Institute for Climate Impact Research (PIK), and Google DeepMind. Our work explores the potential and limitations of using AI as a research method to help tackle eight broad sustainability challenges. The results build on iterated expert dialogues and assessments, a systematic AI-supported literature overview including over 8,500 academic publications, and expert deep-dives into eight specific issue areas. The report also includes recommendations to sustainability scientists, research funders, the private sector, and philanthropies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24373v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Galaz, Maria Schewenius, Jonathan F. Donges, Ingo Fetzer, Erik Zhivkoplias, Wolfram Barfuss, Louis Delannoy, Lan Wang-Erlandsson, Maximilian Gelbrecht, Jobst Heitzig, Jonas Hentati-Sundberg, Christopher Kennedy, Nielja Knecht, Romi Lotcheris, Miguel Mahecha, Andrew Merrie, David Montero, Timon McPhearson, Ahmed Mustafa, Magnus Nystr\"om, Drew Purves, Juan C. Rocha, Masahiro Ryo, Claudia van der Salm, Samuel T. Segun, Anna B. Stephenson, Elizabeth Tellman, Felipe Tobar, Alice Vadrot</dc:creator>
    </item>
    <item>
      <title>I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy</title>
      <link>https://arxiv.org/abs/2410.07109</link>
      <description>arXiv:2410.07109v3 Announce Type: replace-cross 
Abstract: As LLM-based agents become increasingly autonomous and will more freely interact with each other, studying the interplay among them becomes crucial to anticipate emergent phenomena and potential risks. In this work, we provide an in-depth analysis of the interactions among agents within a simulated hierarchical social environment, drawing inspiration from the Stanford Prison Experiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3, Orca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental scenarios, we analyze persuasion and anti-social behavior between a guard and a prisoner agent with differing objectives. We first document model-specific conversational failures in this multi-agent power dynamic context, thereby narrowing our analytic sample to 1,600 conversations. Among models demonstrating successful interaction, we find that goal setting significantly influences persuasiveness but not anti-social behavior. Moreover, agent personas, especially the guard's, substantially impact both successful persuasion by the prisoner and the manifestation of anti-social actions. Notably, we observe the emergence of anti-social conduct even in absence of explicit negative personality prompts. These results have important implications for the development of interactive LLM agents and the ongoing discussion of their societal impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07109v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gian Maria Campedelli, Nicol\`o Penzo, Massimo Stefan, Roberto Dess\`i, Marco Guerini, Bruno Lepri, Jacopo Staiano</dc:creator>
    </item>
    <item>
      <title>Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai</title>
      <link>https://arxiv.org/abs/2411.17555</link>
      <description>arXiv:2411.17555v3 Announce Type: replace-cross 
Abstract: Bike-sharing is an environmentally friendly shared mobility mode, but its self-loop phenomenon, where bikes are returned to the same station after several time usage, significantly impacts equity in accessing its services. Therefore, this study conducts a multiscale analysis with a spatial autoregressive model and double machine learning framework to assess socioeconomic features and geospatial location's impact on the self-loop phenomenon at metro stations and street scales. The results reveal that bike-sharing self-loop intensity exhibits significant spatial lag effect at street scale and is positively associated with residential land use. Marginal treatment effects of residential land use is higher on streets with middle-aged residents, high fixed employment, and low car ownership. The multimodal public transit condition reveals significant positive marginal treatment effects at both scales. To enhance bike-sharing cooperation, we advocate augmenting bicycle availability in areas with high metro usage and low bus coverage, alongside implementing adaptable redistribution strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17555v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yichen Wang, Qing Yu, Yancun Song</dc:creator>
    </item>
    <item>
      <title>Exploring Student-AI Interactions in Vibe Coding</title>
      <link>https://arxiv.org/abs/2507.22614</link>
      <description>arXiv:2507.22614v2 Announce Type: replace-cross 
Abstract: Background and Context. Chat-based and inline-coding-based GenAI has already had substantial impact on the CS Education community. The recent introduction of ``vibe coding'' may further transform how students program, as it introduces a new way for students to create software projects with minimal oversight.
  Objectives. The purpose of this study is to understand how students in introductory programming and advanced software engineering classes interact with a vibe coding platform (Replit) when creating software and how the interactions differ by programming background.
  Methods. Interview participants were asked to think-aloud while building a web application using Replit. Thematic analysis was then used to analyze the video recordings with an emphasis on the interactions between the student and Replit.
  Findings. For both groups, the majority of student interactions with Replit were to test or debug the prototype and only rarely did students visit code. Prompts by advanced software engineering students were much more likely to include relevant app feature and codebase contexts than those by introductory programming students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22614v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Geng, Anshul Shah, Haolin Li, Nawab Mulla, Steven Swanson, Gerald Soosai Raj, Daniel Zingaro, Leo Porter</dc:creator>
    </item>
    <item>
      <title>Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</title>
      <link>https://arxiv.org/abs/2508.21666</link>
      <description>arXiv:2508.21666v2 Announce Type: replace-cross 
Abstract: This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21666v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Imran S. A. Khan, Emmanuel G. Blanchard, S\'ebastien George</dc:creator>
    </item>
    <item>
      <title>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives</title>
      <link>https://arxiv.org/abs/2510.04983</link>
      <description>arXiv:2510.04983v3 Announce Type: replace-cross 
Abstract: Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04983v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khalid Mehtab Khan, Anagha Kulkarni</dc:creator>
    </item>
    <item>
      <title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
      <link>https://arxiv.org/abs/2510.16066</link>
      <description>arXiv:2510.16066v2 Announce Type: replace-cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16066v2</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun Chet Ng, Wei Zeng Low, Yin Yin Boon</dc:creator>
    </item>
  </channel>
</rss>
