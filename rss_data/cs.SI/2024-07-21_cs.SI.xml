<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Unmasking Social Bots: How Confident Are We?</title>
      <link>https://arxiv.org/abs/2407.13929</link>
      <description>arXiv:2407.13929v1 Announce Type: new 
Abstract: Social bots remain a major vector for spreading disinformation on social media and a menace to the public. Despite the progress made in developing multiple sophisticated social bot detection algorithms and tools, bot detection remains a challenging, unsolved problem that is fraught with uncertainty due to the heterogeneity of bot behaviors, training data, and detection algorithms. Detection models often disagree on whether to label the same account as bot or human-controlled. However, they do not provide any measure of uncertainty to indicate how much we should trust their results. We propose to address both bot detection and the quantification of uncertainty at the account level - a novel feature of this research. This dual focus is crucial as it allows us to leverage additional information related to the quantified uncertainty of each prediction, thereby enhancing decision-making and improving the reliability of bot classifications. Specifically, our approach facilitates targeted interventions for bots when predictions are made with high confidence and suggests caution (e.g., gathering more data) when predictions are uncertain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13929v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Giroux, Ariyarathne Gangani, Alexander C. Nwala, Cristiano Fanelli</dc:creator>
    </item>
    <item>
      <title>The Software Complexity of Nations</title>
      <link>https://arxiv.org/abs/2407.13880</link>
      <description>arXiv:2407.13880v1 Announce Type: cross 
Abstract: Despite the growing importance of the digital sector, research on economic complexity and its implications continues to rely mostly on administrative records, e.g. data on exports, patents, and employment, that fail to capture the nuances of the digital economy. In this paper we use data on the geography of programming languages used in open-source software projects to extend economic complexity ideas to the digital economy. We estimate a country's software economic complexity and show that it complements the ability of measures of complexity based on trade, patents, and research papers to account for international differences in GDP per capita, income inequality, and emissions. We also show that open-source software follows the principle of relatedness, meaning that a country's software entries and exits are explained by specialization in related programming languages. We conclude by exploring the diversification and development of countries in open-source software in the context of large language models. Together, these findings help extend economic complexity methods and their policy considerations to the digital sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13880v1</guid>
      <category>econ.GN</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'andor Juh\'asz, Johannes Wachs, Jermain Kaminski, C\'esar A. Hidalgo</dc:creator>
    </item>
    <item>
      <title>PRAGyan -- Connecting the Dots in Tweets</title>
      <link>https://arxiv.org/abs/2407.13909</link>
      <description>arXiv:2407.13909v1 Announce Type: cross 
Abstract: As social media platforms grow, understanding the underlying reasons behind events and statements becomes crucial for businesses, policymakers, and researchers. This research explores the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs) to perform causal analysis of tweets dataset. The LLM aided analysis techniques often lack depth in uncovering the causes driving observed effects. By leveraging KGs and LLMs, which encode rich semantic relationships and temporal information, this study aims to uncover the complex interplay of factors influencing causal dynamics and compare the results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data format, to retrieve relevant context for causal reasoning. Our approach demonstrates that the KG-enhanced LLM RAG can provide improved results when compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus increases in size. Our qualitative analysis highlights the advantages of combining KGs with LLMs for improved interpretability and actionable insights, facilitating informed decision-making across various domains. Whereas, quantitative analysis using metrics such as BLEU and cosine similarity show that our approach outperforms the baseline by 10\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13909v1</guid>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Ravi, Gouri Ginde, Jon Rokne</dc:creator>
    </item>
    <item>
      <title>Harmful Suicide Content Detection</title>
      <link>https://arxiv.org/abs/2407.13942</link>
      <description>arXiv:2407.13942v1 Announce Type: cross 
Abstract: Harmful suicide content on the Internet is a significant risk factor inducing suicidal thoughts and behaviors among vulnerable populations. Despite global efforts, existing resources are insufficient, specifically in high-risk regions like the Republic of Korea. Current research mainly focuses on understanding negative effects of such content or suicide risk in individuals, rather than on automatically detecting the harmfulness of content. To fill this gap, we introduce a harmful suicide content detection task for classifying online suicide content into five harmfulness levels. We develop a multi-modal benchmark and a task description document in collaboration with medical professionals, and leverage large language models (LLMs) to explore efficient methods for moderating such content. Our contributions include proposing a novel detection task, a multi-modal Korean benchmark with expert annotations, and suggesting strategies using LLMs to detect illegal and harmful content. Owing to the potential harm involved, we publicize our implementations and benchmark, incorporating an ethical verification process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13942v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyumin Park, Myung Jae Baik, YeongJun Hwang, Yen Shin, HoJae Lee, Ruda Lee, Sang Min Lee, Je Young Hannah Sun, Ah Rah Lee, Si Yeun Yoon, Dong-ho Lee, Jihyung Moon, JinYeong Bak, Kyunghyun Cho, Jong-Woo Paik, Sungjoon Park</dc:creator>
    </item>
    <item>
      <title>DisenSemi: Semi-supervised Graph Classification via Disentangled Representation Learning</title>
      <link>https://arxiv.org/abs/2407.14081</link>
      <description>arXiv:2407.14081v1 Announce Type: cross 
Abstract: Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in real-world scenarios, labeled graph data can be limited or scarce. To address this issue, we focus on the problem of semi-supervised graph classification, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. In this paper, we propose a novel framework named DisenSemi, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factor-wise graph representations for both supervised and unsupervised models. Then we train two models via supervised objective and mutual information (MI)-based constraints respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experimental results on a range of publicly accessible datasets reveal the effectiveness of our DisenSemi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14081v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Wang, Xiao Luo, Chong Chen, Xian-Sheng Hua, Ming Zhang, Wei Ju</dc:creator>
    </item>
    <item>
      <title>Divide-and-rule policy in the Naming Game</title>
      <link>https://arxiv.org/abs/2306.15922</link>
      <description>arXiv:2306.15922v2 Announce Type: replace 
Abstract: The Naming Game is a classic model for studying the emergence and evolution of language within a population. In this paper, we extend the traditional Naming Game model to encompass multiple committed opinions and investigate the system dynamics on the complete graph with an arbitrarily large population and random networks of finite size. For the fully connected complete graph, the homogeneous mixing condition enables us to use mean-field theory to analyze the opinion evolution of the system. However, when the number of opinions increases, the number of variables describing the system grows exponentially. To mitigate this, we focus on a special scenario where the largest group of committed agents competes with a motley of committed groups, each of which is smaller than the largest one, while initially, most of uncommitted agents hold one unique opinion. This scenario is chosen for its recurrence in diverse societies and its potential for complexity reduction by unifying agents from smaller committed groups into one category. Our investigation reveals that when the size of the largest committed group reaches the critical threshold, most of uncommitted agents change their beliefs to this opinion, triggering a phase transition. Further, we derive the general formula for the multi-opinion evolution using a recursive approach, enabling investigation into any scenario. Finally, we employ agent-based simulations to reveal the opinion evolution and dominance transition in random graphs. Our results provide insights into the conditions under which the dominant opinion emerges in a population and the factors that influence these conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15922v2</guid>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng Ma, Brendan Cross, Gyorgy Korniss, Boleslaw K. Szymanski</dc:creator>
    </item>
    <item>
      <title>ECHO: Edge Centrality via Neighborhood-based Optimization</title>
      <link>https://arxiv.org/abs/2402.12623</link>
      <description>arXiv:2402.12623v3 Announce Type: replace 
Abstract: Given a network G, edge centrality is a metric used to evaluate the importance of edges in G, which is a key concept in analyzing networks and finds vast applications involving edge ranking. In spite of a wealth of research on devising edge centrality measures, they incur either prohibitively high computation costs or varied deficiencies that lead to sub-optimal ranking quality.
  To overcome their limitations, this paper proposes ECHO, a new centrality measure for edge ranking that is formulated based on neighborhood-based optimization objectives. We provide in-depth theoretical analyses to unveil the mathematical definitions and intuitive interpretations of the proposed ECHO measure from diverse aspects. Based thereon, we present three linear-complexity algorithms for ECHO estimation with non-trivial theoretical accuracy guarantees for centrality values. Extensive experiments comparing ECHO against six existing edge centrality metrics in graph analytics tasks on real networks showcase that ECHO offers superior practical effectiveness while offering high computation efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12623v3</guid>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renchi Yang</dc:creator>
    </item>
    <item>
      <title>Are Fact-Checking Tools Helpful? An Evaluation of Google Fact Check</title>
      <link>https://arxiv.org/abs/2402.13244</link>
      <description>arXiv:2402.13244v3 Announce Type: replace 
Abstract: Google Fact Check is a promising search engine that facilitates fact-checking to combat misinformation on social media, especially during significant events such as the COVID-19 pandemic and the U.S. presidential elections. However, its usability has not been thoroughly studied. We evaluated its performance by analyzing the search results regarding 1,000 COVID-19-related false claims and found that only 15.8\% of them obtained fact-checking results, and the results returned by the search engine are relatively reliable. We also found that the false claims receiving different fact-checking verdicts (i.e., ``False'', ``Partly False'', ``True'', and ``Unratable'') tend to reflect diverse emotional tones, and fact-checking sources tend to check the claims in different lengths and using dictionary words to various extents. Claim variations addressing the same issue yet described differently are likely to obtain disparate fact-checking results. Based on these, we suggested that the quantities of fact-checking results could be optimized and that slightly adjusting input claim wording may be the best practice for users to obtain more useful information. This study aims to contribute to understanding fact-checking tools and promote information integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13244v3</guid>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiangeng Yang, Tess Christensen, Shlok Gilda, Juliana Fernandes, Daniela Oliveira, Damon Woodard</dc:creator>
    </item>
    <item>
      <title>The Impact of School and Family Networks on COVID-19 Infections Among Dutch Students: A Study Using Population-Level Registry Data</title>
      <link>https://arxiv.org/abs/2404.08098</link>
      <description>arXiv:2404.08098v2 Announce Type: replace 
Abstract: Understanding the impact of different social interactions is key to improving epidemic models. Here, we use extensive registry data -- including PCR test results and population-level networks -- to investigate the impact of school, family, and other social contacts on SARS-CoV-2 transmission in the Netherlands (June 2020--October 2021). We isolate and compare different contexts of potential SARS-CoV-2 transmission by matching pairs of students based on their attendance at the same or different primary school (in 2020) and secondary school (in 2021) and their geographic proximity. We then calculated the probability of temporally associated infections -- i.e. the probability of both students testing positive within a 14-day period.
  Our results highlight the relative importance of household and family transmission in the spread of SARS-CoV-2 compared to school settings. The probability of temporally associated infections for siblings and parent-child pairs living in the same household was 22.6--23.2\%, and 4.7--7.9\% for family members living in different household. In contrast, the probability of temporally associated infections was 0.52\% for pairs of students living nearby but not attending the same primary or secondary school, 0.66\% for pairs attending different secondary schools but having attended the same primary school, and 1.65\% for pairs attending the same secondary school. Finally, we used multilevel regression analyses to examine how individual, school, and geographic factors contribute to transmission risk. We found that the largest differences in transmission probabilities were due to unobserved individual (60\%) and school-level (34\%) factors. Only a small proportion (3\%) could be attributed to geographic proximity of students or to school size, denomination, or the median income of the school area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08098v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Garcia-Bernardo, Christine Hedde-von Westernhagen, Tom Emery, Albert Jan van Hoek</dc:creator>
    </item>
    <item>
      <title>Commute-Time-Optimised Graphs for GNNs</title>
      <link>https://arxiv.org/abs/2407.08762</link>
      <description>arXiv:2407.08762v2 Announce Type: replace 
Abstract: We explore graph rewiring methods that optimise commute time. Recent graph rewiring approaches facilitate long-range interactions in sparse graphs, making such rewirings commute-time-optimal $\textit{on average}$. However, when an expert prior exists on which node pairs should or should not interact, a superior rewiring would favour short commute times between these privileged node pairs. We construct two synthetic datasets with known priors reflecting realistic settings, and use these to motivate two bespoke rewiring methods that incorporate the known prior. We investigate the regimes where our rewiring improves test performance on the synthetic datasets. Finally, we perform a case study on a real-world citation graph to investigate the practical implications of our work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08762v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Sterner, Shiye Su, Petar Veli\v{c}kovi\'c</dc:creator>
    </item>
  </channel>
</rss>
