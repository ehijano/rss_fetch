<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Alignment conditions of the human eye for few-photon vision experiments</title>
      <link>https://arxiv.org/abs/2510.09186</link>
      <description>arXiv:2510.09186v1 Announce Type: cross 
Abstract: In experiments probing human vision at the few-photon level, precise alignment of the eye is necessary such that stimuli reach the highest-density rod region of the retina. However, in literature there seems to be no consensus on the optimal eye alignment for such experiments. Typically, experiments are performed by presenting stimuli nasally or temporally, but the angle under which the few-photon pulses are presented varies between 7 deg and 23 deg. Here we combine a $3$-dimensional eye model with retinal rod density measurements from literature in a ray tracing simulation to study the optimal eye alignment conditions and necessary alignment precision. We find that stimuli, directed at the eye's nodal point, may be best presented under an inferior angle of 13.1 deg with respect to the visual axis. Defining a target area on the retina with a radius of 0.5 mm around the optimum location, we find the horizontal and vertical angular precision should be better than 0.85 deg given a horizontal and vertical translational precision of 1 mm and a depth translational precision of 5 mm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09186v1</guid>
      <category>physics.optics</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. H. A. van der Reep, W. L\"offler</dc:creator>
    </item>
    <item>
      <title>scellop: A Scalable Redesign of Cell Population Plots for Single-Cell Data</title>
      <link>https://arxiv.org/abs/2510.09554</link>
      <description>arXiv:2510.09554v1 Announce Type: cross 
Abstract: Summary: Cell population plots are visualizations showing cell population distributions in biological samples with single-cell data, traditionally shown with stacked bar charts. Here, we address issues with this approach, particularly its limited scalability with increasing number of cell types and samples, and present scellop, a novel interactive cell population viewer combining visual encodings optimized for common user tasks in studying populations of cells across samples or conditions.
  Availability and Implementation: Scellop is available under the MIT licence at https://github.com/hms-dbmi/scellop, and is available on PyPI (https://pypi.org/project/cellpop/) and NPM (https://www.npmjs.com/package/cellpop). A demo is available at https://scellop.netlify.app/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09554v1</guid>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas C. Smits, Nikolay Akhmetov, Tiffany S. Liaw, Mark S. Keller, Eric M\"orth, Nils Gehlenborg</dc:creator>
    </item>
    <item>
      <title>MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging</title>
      <link>https://arxiv.org/abs/2510.01298</link>
      <description>arXiv:2510.01298v2 Announce Type: replace 
Abstract: Simulating in silico cellular responses to interventions is a promising direction to accelerate high-content image-based assays, critical for advancing drug discovery and gene editing. To support this, we introduce MorphGen, a state-of-the-art diffusion-based generative model for fluorescent microscopy that enables controllable generation across multiple cell types and perturbations. To capture biologically meaningful patterns consistent with known cellular morphologies, MorphGen is trained with an alignment loss to match its representations to the phenotypic embeddings of OpenPhenom, a state-of-the-art biological foundation model. Unlike prior approaches that compress multichannel stains into RGB images -- thus sacrificing organelle-specific detail -- MorphGen generates the complete set of fluorescent channels jointly, preserving per-organelle structures and enabling a fine-grained morphological analysis that is essential for biological interpretation. We demonstrate biological consistency with real images via CellProfiler features, and MorphGen attains an FID score over 35% lower than the prior state-of-the-art MorphoDiff, which only generates RGB images for a single cell type. Code is available at https://github.com/czi-ai/MorphGen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01298v2</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berker Demirel, Marco Fumero, Theofanis Karaletsos, Francesco Locatello</dc:creator>
    </item>
    <item>
      <title>InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions</title>
      <link>https://arxiv.org/abs/2510.03370</link>
      <description>arXiv:2510.03370v2 Announce Type: replace 
Abstract: Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03370v2</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junde Xu, Yapin Shi, Lijun Lang, Taoyong Cui, Zhiming Zhang, Guangyong Chen, Jiezhong Qiu, Pheng-Ann Heng</dc:creator>
    </item>
    <item>
      <title>What the %PCSA? Addressing Diversity in Lower-Limb Musculoskeletal Models: Age- and Sex-related Differences in PCSA and Muscle Mass</title>
      <link>https://arxiv.org/abs/2411.00071</link>
      <description>arXiv:2411.00071v3 Announce Type: replace-cross 
Abstract: Musculoskeletal (MSK) models offer a non-invasive way to understand biomechanical loads on joints and tendons, which are difficult to measure directly. Variations in muscle strength, especially relative differences between muscles, significantly impact model outcomes. Typically, scaled generic MSK models use maximum isometric forces that are not adjusted for different demographics, raising concerns about their accuracy. This review provides an overview on experimentally derived strength parameters, including physiological cross-sectional area (PCSA), muscle mass (Mm), and relative muscle mass (%Mm), which is the relative distribution of muscle mass across the leg. We analysed differences by age and sex, and compared open-source lower limb MSK model parameters with experimental data from 57 studies. Our dataset, with records dating back to 1884, shows that uniformly increasing all maximum isometric forces in MSK models does not capture key muscle ratio differences due to age and sex. Males have a higher proportion of muscle mass in the rectus femoris and semimembranosus muscles, while females have a greater relative muscle mass in the pelvic (gluteus maximus and medius) and ankle muscles (tibialis anterior, tibialis posterior, and extensor digitorum longus). Older adults have a higher relative muscle mass in the gluteus medius, while younger individuals show more in the gastrocnemius. Current MSK models do not accurately represent muscle mass distribution for specific age or sex groups, and none of them accurately reflect female muscle mass distribution. Further research is needed to explore musculotendon age- and sex differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00071v3</guid>
      <category>q-bio.TO</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R. Maarleveld, H. E. J. Veeger, F. C. T. van der Helm, J. Son, R. L. Lieber, E. van der Kruk</dc:creator>
    </item>
  </channel>
</rss>
