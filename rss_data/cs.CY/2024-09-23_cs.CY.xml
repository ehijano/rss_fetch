<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Undergrads Are All You Have</title>
      <link>https://arxiv.org/abs/2409.13750</link>
      <description>arXiv:2409.13750v1 Announce Type: new 
Abstract: The outsourcing of busy work and other research-related tasks to undergraduate students is a time-honored academic tradition. In recent years, these tasks have been given to Lama-based large-language models such as Alpaca and Llama increasingly often, putting poor undergraduate students out of work[ 13 ]. Due to the costs associated with importing and caring for South American Camelidae, researcher James Yoo set out to find a cheaper and more effective alternative to these models. The findings, published in the highly-respected journal, SIGBOVIK, demonstrates that their model, GPT-UGRD is on par with, and in some cases better, than Lama models for natural language processing tasks[5]. The paper also demonstrates that GPT-UGRD is cheaper and easier to train and operate than transformer models. In this paper, we outline the implementation, application, multi-tenanting, and social implications of using this new model in research and other contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13750v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ashe Neth</dc:creator>
    </item>
    <item>
      <title>PyGRF: An improved Python Geographical Random Forest model and case studies in public health and natural disasters</title>
      <link>https://arxiv.org/abs/2409.13947</link>
      <description>arXiv:2409.13947v1 Announce Type: new 
Abstract: Geographical random forest (GRF) is a recently developed and spatially explicit machine learning model. With the ability to provide more accurate predictions and local interpretations, GRF has already been used in many studies. The current GRF model, however, has limitations in its determination of the local model weight and bandwidth hyperparameters, potentially insufficient numbers of local training samples, and sometimes high local prediction errors. Also, implemented as an R package, GRF currently does not have a Python version which limits its adoption among machine learning practitioners who prefer Python. This work addresses these limitations by introducing theory-informed hyperparameter determination, local training sample expansion, and spatially-weighted local prediction. We also develop a Python-based GRF model and package, PyGRF, to facilitate the use of the model. We evaluate the performance of PyGRF on an example dataset and further demonstrate its use in two case studies in public health and natural disasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13947v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/tgis.13248</arxiv:DOI>
      <arxiv:journal_reference>Transactions in GIS, 2024</arxiv:journal_reference>
      <dc:creator>Kai Sun, Ryan Zhenqi Zhou, Jiyeon Kim, Yingjie Hu</dc:creator>
    </item>
    <item>
      <title>Monitoring Human Dependence On AI Systems With Reliance Drills</title>
      <link>https://arxiv.org/abs/2409.14055</link>
      <description>arXiv:2409.14055v1 Announce Type: new 
Abstract: AI systems are assisting humans with an increasingly broad range of intellectual tasks. Humans could be over-reliant on this assistance if they trust AI-generated advice, even though they would make a better decision on their own. To identify real-world instances of over-reliance, this paper proposes the reliance drill: an exercise that tests whether a human can recognise mistakes in AI-generated advice. We introduce a pipeline that organisations could use to implement these drills. As an example, we explain how this approach could be used to limit over-reliance on AI in a medical setting. We conclude by arguing that reliance drills could become a key tool for ensuring humans remain appropriately involved in AI-assisted decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14055v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rosco Hunter, Richard Moulange, Jamie Bernardi, Merlin Stein</dc:creator>
    </item>
    <item>
      <title>IPF-HMGNN: A novel integrative prediction framework for metro passenger flow</title>
      <link>https://arxiv.org/abs/2409.14104</link>
      <description>arXiv:2409.14104v1 Announce Type: new 
Abstract: The operation and management of the metro system in urban areas rely on accurate predictions of future passenger flow. While using all the available information can potentially improve on the accuracy of the flow prediction, there has been little attention to the hierarchical relationship between the type of tickets collected from the passengers entering/exiting a station and its resulting passenger flow. To this end, we propose a novel Integrative Prediction Framework with the Hierarchical Message-Passing Graph Neural Network (IPF-HMGNN). The proposed framework consists of three components: initial prediction, task judgment and hierarchical coordination modules. Using the Wuxi, China metro network as an example, we study two prediction approaches (i) traditional prediction approach where the model directly predicts passenger flow at the station, and (ii) hierarchical prediction approach where the prediction of ticket type and station passenger flow are performed simultaneously considering the hierarchical constraints (i.e., the sum of predicted passenger flow per ticket type equals the predicted station aggregated passenger flow). Experimental results indicate that in the traditional prediction approach, our IPF-HMGNN can significantly reduce the mean absolute error (MAE) and root mean square error (RMSE) of the GNN prediction model by 49.56% and 53.88%, respectively. In the hierarchical prediction approach, IPF-HMGNN can achieve a maximum reduction of 35.32% in MAE and 36.18% in RMSE, while satisfying the hierarchical constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14104v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbo Lu, Yong Zhang, Hai L. Vu, Jinhua Xu, Peikun Li</dc:creator>
    </item>
    <item>
      <title>Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI</title>
      <link>https://arxiv.org/abs/2409.14160</link>
      <description>arXiv:2409.14160v1 Announce Type: new 
Abstract: With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the 'bigger-is-better' AI paradigm: 1) that improved performance is a product of increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14160v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ga\"el Varoquaux, Alexandra Sasha Luccioni, Meredith Whittaker</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approach to assess and identify gaps in healthcare set up in South Asia</title>
      <link>https://arxiv.org/abs/2409.14194</link>
      <description>arXiv:2409.14194v1 Announce Type: new 
Abstract: Primary healthcare is a crucial strategy for achieving universal health coverage. South Asian countries are working to improve their primary healthcare system through their country specific policies designed in line with WHO health system framework using the six thematic pillars: Health Financing, Health Service delivery, Human Resource for Health, Health Information Systems, Governance, Essential Medicines and Technology, and an addition area of Cross-Sectoral Linkages. Measuring the current accessibility of healthcare facilities and workforce availability is essential for improving healthcare standards and achieving universal health coverage in developing countries. Data-driven surveillance approaches are required that can provide rapid, reliable, and geographically scalable solutions to understand a) which communities and areas are most at risk of inequitable access and when, b) what barriers to health access exist, and c) how they can be overcome in ways tailored to the specific challenges faced by individual communities. We propose to harness current breakthroughs in Earth-observation (EO) technology, which provide the ability to generate accurate, up-to-date, publicly accessible, and reliable data, which is necessary for equitable access planning and resource allocation to ensure that vaccines, and other interventions reach everyone, particularly those in greatest need, during normal and crisis times. This requires collaboration among countries to identify evidence based solutions to shape health policy and interventions, and drive innovations and research in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14194v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICONIP 2024</arxiv:journal_reference>
      <dc:creator>Rusham Elahi, Zia Tahseen, Tehreem Fatima, Syed Wafa Zahra, Hafiz Muhammad Abubakar, Tehreem Zafar, Aqs Younas, Muhammad Talha Quddoos, Usman Nazir</dc:creator>
    </item>
    <item>
      <title>Cross-course Process Mining of Student Clickstream Data -- Aggregation and Group Comparison</title>
      <link>https://arxiv.org/abs/2409.14244</link>
      <description>arXiv:2409.14244v1 Announce Type: new 
Abstract: This paper introduces novel methods for preparing and analyzing student interaction data extracted from course management systems like Moodle to facilitate process mining, like the creation of graphs that show the process flow. Such graphs can get very complex as Moodle courses can contain hundreds of different activities, which makes it difficult to compare the paths of different student cohorts. Moreover, existing research often confines its focus to individual courses, overlooking potential patterns that may transcend course boundaries. Our research addresses these challenges by implementing an automated dataflow that directly queries data from the Moodle database via SQL, offering the flexibility of filtering on individual courses if needed. In addition to analyzing individual Moodle activities, we explore patterns at an aggregated course section level. Furthermore, we present a method for standardizing section labels across courses, facilitating cross-course analysis to uncover broader usage patterns. Our findings reveal, among other insights, that higher-performing students demonstrate a propensity to engage more frequently with available activities and exhibit more dynamic movement between objects. While these patterns are discernible when analyzing individual course activity-events, they become more pronounced when aggregated to the section level and analyzed across multiple courses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14244v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Hildebrandt, Lars Mehnen</dc:creator>
    </item>
    <item>
      <title>Generative artificial intelligence usage by researchers at work: Effects of gender, career stage, type of workplace, and perceived barriers</title>
      <link>https://arxiv.org/abs/2409.14570</link>
      <description>arXiv:2409.14570v1 Announce Type: new 
Abstract: The integration of generative artificial intelligence technology into research environments has become increasingly common in recent years, representing a significant shift in the way researchers approach their work. This paper seeks to explore the factors underlying the frequency of use of generative AI amongst researchers in their professional environments. As survey data may be influenced by a bias towards scientists interested in AI, potentially skewing the results towards the perspectives of these researchers, this study uses a regression model to isolate the impact of specific factors such as gender, career stage, type of workplace, and perceived barriers to using AI technology on the frequency of use of generative AI. It also controls for other relevant variables such as direct involvement in AI research or development, collaboration with AI companies, geographic location, and scientific discipline. Our results show that researchers who face barriers to AI adoption experience an 11% increase in tool use, while those who cite insufficient training resources experience an 8% decrease. Female researchers experience a 7% decrease in AI tool usage compared to men, while advanced career researchers experience a significant 19% decrease. Researchers associated with government advisory groups are 45% more likely to use AI tools frequently than those in government roles. Researchers in for-profit companies show an increase of 19%, while those in medical research institutions and hospitals show an increase of 16% and 15%, respectively. This paper contributes to a deeper understanding of the mechanisms driving the use of generative AI tools amongst researchers, with valuable implications for both academia and industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14570v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Alexis Jorge L\'opez-Puig, Mar\'ia Isabel Dorta-Gonz\'alez, Sara M. Gonz\'alez-Betancor</dc:creator>
    </item>
    <item>
      <title>Exploring Error Types in Formal Languages Among Students of Upper Secondary Education</title>
      <link>https://arxiv.org/abs/2409.15043</link>
      <description>arXiv:2409.15043v1 Announce Type: new 
Abstract: Foundations of formal languages, as subfield of theoretical computer science, are part of typical upper secondary education curricula. There is very little research on the potential difficulties that students at this level have with this subject. In this paper, we report on an exploratory study of errors in formal languages among upper secondary education students. We collect the data by posing exercises in an intelligent tutoring system and analyzing student input. Our results suggest a) instances of non-functional understanding of concepts such as the empty word or a grammar as a substitution system; b) strategic problems such as lack of foresight when deriving a word or confounding formal specifications with real-world knowledge on certain aspects; and c) various syntactic problems. These findings can serve as a starting point for a broader understanding of how and why students struggle with this topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15043v1</guid>
      <category>cs.CY</category>
      <category>cs.FL</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko Schmellenkamp, Dennis Stanglmair, Tilman Michaeli, Thomas Zeume</dc:creator>
    </item>
    <item>
      <title>Towards Safe Multilingual Frontier AI</title>
      <link>https://arxiv.org/abs/2409.13708</link>
      <description>arXiv:2409.13708v1 Announce Type: cross 
Abstract: Linguistically inclusive LLMs -- which maintain good performance regardless of the language with which they are prompted -- are necessary for the diffusion of AI benefits around the world. Multilingual jailbreaks that rely on language translation to evade safety measures undermine the safe and inclusive deployment of AI systems. We provide policy recommendations to enhance the multilingual capabilities of AI while mitigating the risks of multilingual jailbreaks. We quantitatively assess the relationship between language resourcedness and model vulnerabilities to multilingual jailbreaks for five frontier large language models across 24 official EU languages. Building on prior research, we propose policy actions that align with the EU legal landscape and institutional framework to address multilingual jailbreaks, while promoting linguistic inclusivity. These include mandatory assessments of multilingual capabilities and vulnerabilities, public opinion research, and state support for multilingual AI development. The measures aim to improve AI safety and functionality through EU policy initiatives, guiding the implementation of the EU AI Act and informing regulatory efforts of the European AI Office.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13708v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art\=urs Kanepajs, Vladimir Ivanov, Richard Moulange</dc:creator>
    </item>
    <item>
      <title>Identity-related Speech Suppression in Generative AI Content Moderation</title>
      <link>https://arxiv.org/abs/2409.13725</link>
      <description>arXiv:2409.13725v1 Announce Type: cross 
Abstract: Automated content moderation has long been used to help identify and filter undesired user-generated content online. Generative AI systems now use such filters to keep undesired generated content from being created by or shown to users. From classrooms to Hollywood, as generative AI is increasingly used for creative or expressive text generation, whose stories will these technologies allow to be told, and whose will they suppress? In this paper, we define and introduce measures of speech suppression, focusing on speech related to different identity groups incorrectly filtered by a range of content moderation APIs. Using both short-form, user-generated datasets traditional in content moderation and longer generative AI-focused data, including two datasets we introduce in this work, we create a benchmark for measurement of speech suppression for nine identity groups. Across one traditional and four generative AI-focused automated content moderation services tested, we find that identity-related speech is more likely to be incorrectly suppressed than other speech except in the cases of a few non-marginalized groups. Additionally, we find differences between APIs in their abilities to correctly moderate generative AI content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13725v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Oghenefejiro Isaacs Anigboro, Charlie M. Crawford, Dana\"e Metaxa, Sorelle A. Friedler</dc:creator>
    </item>
    <item>
      <title>STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions</title>
      <link>https://arxiv.org/abs/2409.13843</link>
      <description>arXiv:2409.13843v1 Announce Type: cross 
Abstract: Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We also demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13843v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Morabito, Sangmitra Madhusudan, Tyler McDonald, Ali Emami</dc:creator>
    </item>
    <item>
      <title>Generative AI Carries Non-Democratic Biases and Stereotypes: Representation of Women, Black Individuals, Age Groups, and People with Disability in AI-Generated Images across Occupations</title>
      <link>https://arxiv.org/abs/2409.13869</link>
      <description>arXiv:2409.13869v1 Announce Type: cross 
Abstract: AI governance and ethics in AI development have become critical concerns, prompting active discussions among tech companies, governments, and researchers about the potential risks AI poses to our democracies. This short essay aims to highlight one such risk: how generative AI includes or excludes equity-deserving groups in its outputs. The findings reveal that generative AI is not equitably inclusive regarding gender, race, age, and visible disability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13869v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ayoob Sadeghiani</dc:creator>
    </item>
    <item>
      <title>A Multi-LLM Debiasing Framework</title>
      <link>https://arxiv.org/abs/2409.13884</link>
      <description>arXiv:2409.13884v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13884v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deonna M. Owens, Ryan A. Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, Nedim Lipka</dc:creator>
    </item>
    <item>
      <title>MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2409.13935</link>
      <description>arXiv:2409.13935v1 Announce Type: cross 
Abstract: This study explores the effectiveness of Large Language Models (LLMs) in creating personalized "mirror stories" that reflect and resonate with individual readers' identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13935v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarfaroz Yunusov, Hamza Sidat, Ali Emami</dc:creator>
    </item>
    <item>
      <title>Learning Recourse Costs from Pairwise Feature Comparisons</title>
      <link>https://arxiv.org/abs/2409.13940</link>
      <description>arXiv:2409.13940v1 Announce Type: cross 
Abstract: This paper presents a novel technique for incorporating user input when learning and inferring user preferences. When trying to provide users of black-box machine learning models with actionable recourse, we often wish to incorporate their personal preferences about the ease of modifying each individual feature. These recourse finding algorithms usually require an exhaustive set of tuples associating each feature to its cost of modification. Since it is hard to obtain such costs by directly surveying humans, in this paper, we propose the use of the Bradley-Terry model to automatically infer feature-wise costs using non-exhaustive human comparison surveys. We propose that users only provide inputs comparing entire recourses, with all candidate feature modifications, determining which recourses are easier to implement relative to others, without explicit quantification of their costs. We demonstrate the efficient learning of individual feature costs using MAP estimates, and show that these non-exhaustive human surveys, which do not necessarily contain data for each feature pair comparison, are sufficient to learn an exhaustive set of feature costs, where each feature is associated with a modification cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13940v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaivalya Rawal, Himabindu Lakkaraju</dc:creator>
    </item>
    <item>
      <title>Digital Advertising in a Post-Cookie World: Charting the Impact of Google's Topics API</title>
      <link>https://arxiv.org/abs/2409.14073</link>
      <description>arXiv:2409.14073v1 Announce Type: cross 
Abstract: Integrating Google's Topics API into the digital advertising ecosystem represents a significant shift toward privacy-conscious advertising practices. This article analyses the implications of implementing Topics API on ad networks, focusing on competition dynamics and ad space accessibility. Through simulations based on extensive datasets capturing user behavior and market share data for ad networks, we evaluate metrics such as Ad Placement Eligibility, Low Competition Rate, and solo competitor. The findings reveal a noticeable impact on ad networks, with larger players strengthening their dominance and smaller networks facing challenges securing ad spaces and competing effectively. Moreover, the study explores the potential environmental implications of Google's actions, highlighting the need to carefully consider policy and regulatory measures to ensure fair competition and privacy protection. Overall, this research contributes valuable insights into the evolving dynamics of digital advertising and highlights the importance of balancing privacy with competition and innovation in the online advertising landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14073v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jes\'us Romero, \'Angel Cuevas, Rub\'en Cuevas</dc:creator>
    </item>
    <item>
      <title>An Evolutionary Algorithm For the Vehicle Routing Problem with Drones with Interceptions</title>
      <link>https://arxiv.org/abs/2409.14173</link>
      <description>arXiv:2409.14173v1 Announce Type: cross 
Abstract: The use of trucks and drones as a solution to address last-mile delivery challenges is a new and promising research direction explored in this paper. The variation of the problem where the drone can intercept the truck while in movement or at the customer location is part of an optimisation problem called the vehicle routing problem with drones with interception (VRPDi). This paper proposes an evolutionary algorithm to solve the VRPDi. In this variation of the VRPDi, multiple pairs of trucks and drones need to be scheduled. The pairs leave and return to a depot location together or separately to make deliveries to customer nodes. The drone can intercept the truck after the delivery or meet up with the truck at the following customer location. The algorithm was executed on the travelling salesman problem with drones (TSPD) datasets by Bouman et al. (2015), and the performance of the algorithm was compared by benchmarking the results of the VRPDi against the results of the VRP of the same dataset. This comparison showed improvements in total delivery time between 39% and 60%. Further detailed analysis of the algorithm results examined the total delivery time, distance, node delivery scheduling and the degree of diversity during the algorithm execution. This analysis also considered how the algorithm handled the VRPDi constraints. The results of the algorithm were then benchmarked against algorithms in Dillon et al. (2023) and Ernst (2024). The latter solved the problem with a maximum drone distance constraint added to the VRPDi. The analysis and benchmarking of the algorithm results showed that the algorithm satisfactorily solved 50 and 100-nodes problems in a reasonable amount of time, and the solutions found were better than those found by the algorithms in Dillon et al. (2023) and Ernst (2024) for the same problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14173v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Pambo, Jacomine Grobler</dc:creator>
    </item>
    <item>
      <title>Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</title>
      <link>https://arxiv.org/abs/2409.14509</link>
      <description>arXiv:2409.14509v1 Announce Type: cross 
Abstract: LLM-based applications are helping people write, and LLM-generated text is making its way into social media, journalism, and our classrooms. However, the differences between LLM-generated and human-written text remain unclear. To explore this, we hired professional writers to edit paragraphs in several creative domains. We first found these writers agree on undesirable idiosyncrasies in LLM-generated text, formalizing it into a seven-category taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP corpus: 1,057 LLM-generated paragraphs edited by professional writers according to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms of writing quality, revealing common limitations across model families. Third, we explored automatic editing methods to improve LLM-generated text. A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14509v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</dc:creator>
    </item>
    <item>
      <title>DarkGram: Exploring and Mitigating Cybercriminal content shared in Telegram channels</title>
      <link>https://arxiv.org/abs/2409.14596</link>
      <description>arXiv:2409.14596v1 Announce Type: cross 
Abstract: We present the first large scale analysis of 339 cybercriminal activity channels (CACs) on Telegram from February to May 2024. Collectively followed by over 23.8 million users, these channels shared a wide array of illicit content, including compromised credentials, pirated software and media, tools for blackhat hacking resources such as malware, social engineering scams, and exploit kits. We developed DarkGram, a BERT based framework that identifies malicious posts from the CACs with an accuracy of 96%, using which we conducted a quantitative analysis of 53,605 posts from these channels, revealing key characteristics of shared content. While much of this content is distributed for free, channel administrators frequently employ promotions and giveaways to engage users and boost the sales of premium cybercriminal content. These channels also pose significant risks to their own subscribers. Notably, 28.1% of shared links contained phishing attacks, and 38% of executable files were bundled with malware. Moreover, our qualitative analysis of replies in CACs shows how subscribers cultivate a dangerous sense of community through requests for illegal content, illicit knowledge sharing, and collaborative hacking efforts, while their reactions to posts, including emoji responses, further underscore their appreciation for such content. We also find that the CACs can evade scrutiny by quickly migrating to new channels with minimal subscriber loss, highlighting the resilience of this ecosystem. To counteract this, we further utilized DarkGram to detect new channels, reporting malicious content to Telegram and the affected organizations which resulted in the takedown of 196 such channels over three months. To aid further collaborative efforts in taking down these channels, we open source our dataset and the DarkGram framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14596v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayak Saha Roy, Elham Pourabbas Vafa, Kobra Khanmohammadi, Shirin Nilizadeh</dc:creator>
    </item>
    <item>
      <title>Privacy Policy Analysis through Prompt Engineering for LLMs</title>
      <link>https://arxiv.org/abs/2409.14879</link>
      <description>arXiv:2409.14879v1 Announce Type: cross 
Abstract: Privacy policies are often obfuscated by their complexity, which impedes transparency and informed consent. Conventional machine learning approaches for automatically analyzing these policies demand significant resources and substantial domain-specific training, causing adaptability issues. Moreover, they depend on extensive datasets that may require regular maintenance due to changing privacy concerns.
  In this paper, we propose, apply, and assess PAPEL (Privacy Policy Analysis through Prompt Engineering for LLMs), a framework harnessing the power of Large Language Models (LLMs) through prompt engineering to automate the analysis of privacy policies. PAPEL aims to streamline the extraction, annotation, and summarization of information from these policies, enhancing their accessibility and comprehensibility without requiring additional model training. By integrating zero-shot, one-shot, and few-shot learning approaches and the chain-of-thought prompting in creating predefined prompts and prompt templates, PAPEL guides LLMs to efficiently dissect, interpret, and synthesize the critical aspects of privacy policies into user-friendly summaries. We demonstrate the effectiveness of PAPEL with two applications: (i) annotation and (ii) contradiction analysis. We assess the ability of several LLaMa and GPT models to identify and articulate data handling practices, offering insights comparable to existing automated analysis approaches while reducing training efforts and increasing the adaptability to new analytical needs. The experiments demonstrate that the LLMs PAPEL utilizes (LLaMA and Chat GPT models) achieve robust performance in privacy policy annotation, with F1 scores reaching 0.8 and above (using the OPP-115 gold standard), underscoring the effectiveness of simpler prompts across various advanced language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14879v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, Hui Song</dc:creator>
    </item>
    <item>
      <title>Unbiased third-party bots lead to a tradeoff between cooperation and social payoffs</title>
      <link>https://arxiv.org/abs/2409.14975</link>
      <description>arXiv:2409.14975v1 Announce Type: cross 
Abstract: The rise of artificial intelligence (AI) offers new opportunities to influence cooperative dynamics with greater applicability and control. In this paper, we examine the impact of third-party bots--agents that do not directly participate in games but unbiasedly modify the payoffs of normal players engaged in prisoner's dilemma interactions--on the emergence of cooperation. Using an evolutionary simulation model, we demonstrate that unbiased bots are unable to shift the defective equilibrium among normal players in well-mixed populations. However, in structured populations, despite their unbiased actions, the bots spontaneously generate distinct impacts on cooperators and defectors, leading to enhanced cooperation. Notably, bots that apply negative influences are more effective at promoting cooperation than those applying positive ones, as fewer bots are needed to catalyze cooperative behavior among normal players. However, as the number of bots increases, a trade-off emerges: while cooperation is maintained, overall social payoffs decline. These findings highlight the need for careful management of AI's role in social systems, as even well-intentioned bots can have unintended consequences on collective outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14975v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixue He, Chen Shen, Lei Shi, Jun Tanimoto</dc:creator>
    </item>
    <item>
      <title>Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents</title>
      <link>https://arxiv.org/abs/2409.15014</link>
      <description>arXiv:2409.15014v1 Announce Type: cross 
Abstract: We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15014v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Baum, Lisa Dargasz, Felix Jahn, Timo P. Gros, Verena Wolf</dc:creator>
    </item>
    <item>
      <title>AdapFair: Ensuring Continuous Fairness for Machine Learning Operations</title>
      <link>https://arxiv.org/abs/2409.15088</link>
      <description>arXiv:2409.15088v1 Announce Type: cross 
Abstract: The biases and discrimination of machine learning algorithms have attracted significant attention, leading to the development of various algorithms tailored to specific contexts. However, these solutions often fall short of addressing fairness issues inherent in machine learning operations. In this paper, we present a debiasing framework designed to find an optimal fair transformation of input data that maximally preserves data predictability. A distinctive feature of our approach is its flexibility and efficiency. It can be integrated with any downstream black-box classifiers, providing continuous fairness guarantees with minimal retraining efforts, even in the face of frequent data drifts, evolving fairness requirements, and batches of similar tasks. To achieve this, we leverage the normalizing flows to enable efficient, information-preserving data transformation, ensuring that no critical information is lost during the debiasing process. Additionally, we incorporate the Wasserstein distance as the unfairness measure to guide the optimization of data transformations. Finally, we introduce an efficient optimization algorithm with closed-formed gradient computations, making our framework scalable and suitable for dynamic, real-world environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15088v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinghui Huang, Zihao Tang, Xiangyu Chang</dc:creator>
    </item>
    <item>
      <title>Data governance: A Critical Foundation for Data Driven Decision-Making in Operations and Supply Chains</title>
      <link>https://arxiv.org/abs/2409.15137</link>
      <description>arXiv:2409.15137v1 Announce Type: cross 
Abstract: In the context of Industry 4.0, the manufacturing sector is increasingly facing the challenge of data usability, which is becoming a widespread phenomenon and a new contemporary concern. In response, Data Governance (DG) emerges as a viable avenue to address data challenges. This study aims to call attention on DG research in the field of operations and supply chain management (OSCM). Based on literature research, we investigate research gaps in academia. Built upon three case studies, we exanimated and analyzed real life data issues in the industry. Four types of cause related to data issues were found: 1) human factors, 2) lack of written rules and regulations, 3) ineffective technological hardware and software, and 4) lack of resources. Subsequently, a three-pronged research framework was suggested. This paper highlights the urgency for research on DG in OSCM, outlines a research pathway for fellow scholars, and offers guidance to industry in the design and implementation of DG strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15137v1</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuejiao Li, Yang Cheng, Charles M{\o}ller</dc:creator>
    </item>
    <item>
      <title>Security Camera Movie and ERP Data Matching System to Prevent Theft</title>
      <link>https://arxiv.org/abs/1706.04595</link>
      <description>arXiv:1706.04595v5 Announce Type: replace 
Abstract: "(c) 2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works." In this paper, we propose a SaaS service which prevents shoplifting using image analysis and ERP. In Japan, total damage of shoplifting reaches 450 billion yen. Based on cloud and data analysis technology, we propose a shoplifting prevention service with image analysis of security camera and ERP data check for small shops. We evaluated movie analysis. Y. Yamato, Y. Fukumoto and H. Kumazaki, "Security Camera Movie and ERP Data Matching System to Prevent Theft," IEEE Consumer Communications and Networking Conference (CCNC 2017), pp.1021-1022, DOI: 10.1109/CCNC.2017.7983275, Jan. 2017.</description>
      <guid isPermaLink="false">oai:arXiv.org:1706.04595v5</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CCNC.2017.7983275</arxiv:DOI>
      <arxiv:journal_reference>IEEE Consumer Communications and Networking Conference (CCNC2017), pp.1021-1022, Jan. 2017</arxiv:journal_reference>
      <dc:creator>Yoji Yamato, Yoshifumi Fukumoto, Hiroki Kumazaki</dc:creator>
    </item>
    <item>
      <title>Token-Based Payment Systems</title>
      <link>https://arxiv.org/abs/2207.07530</link>
      <description>arXiv:2207.07530v2 Announce Type: replace 
Abstract: In this article, we consider the roles of tokens and distributed ledgers in digital payment systems. We present a brief taxonomy of digital payment systems that use tokens, and we address the different models for how distributed ledger technology can support digital payment systems in general. We offer guidance on the salient features of digital payment systems, which we comprehend in terms of consumer privacy, token issuance, and accountability for system operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07530v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffrey Goodell</dc:creator>
    </item>
    <item>
      <title>Views on AI aren't binary -- they're plural</title>
      <link>https://arxiv.org/abs/2312.14230</link>
      <description>arXiv:2312.14230v2 Announce Type: replace 
Abstract: Recent developments in AI have brought broader attention to tensions between two overlapping communities, "AI Ethics" and "AI Safety." In this article we (i) characterize this false binary, (ii) argue that a simple binary is not an accurate model of AI discourse, and (iii) provide concrete suggestions for how individuals can help avoid the emergence of us-vs-them conflict in the broad community of people working on AI development and governance. While we focus on "AI Ethics" an "AI Safety," the general lessons apply to related tensions, including those between accelerationist ("e/acc") and cautious stances on AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14230v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thorin Bristow, Luke Thorburn, Diana Acosta-Navas</dc:creator>
    </item>
    <item>
      <title>Contextual Stochastic Optimization for School Desegregation Policymaking</title>
      <link>https://arxiv.org/abs/2408.12572</link>
      <description>arXiv:2408.12572v2 Announce Type: replace 
Abstract: Most US school districts draw geographic "attendance zones" to assign children to schools based on their home address, a process that can codify existing neighborhood racial/ethnic and socioeconomic status (SES) segregation in schools. Redrawing boundaries can reduce segregation, but estimating the rezoning impact is challenging as families can opt-out of their assigned schools. This paper is an attempt to address this societal problem: it develops a joint redistricting and choice modeling framework, called redistricting with choices (RWC). The RWC framework is applied to a large US public school district for estimating how redrawing elementary school boundaries in the district might realistically impact levels of socioeconomic segregation. The main methodological contribution of the RWC is a contextual stochastic optimization model that minimizes district-wide dissimilarity, and integrates the rezoning constraints and a school choice model for the students obtained through machine learning. The key finding of the study is the observation that RWC yields boundary changes that might reduce segregation by a substantial amount (23%) -- but doing so might require the re-assignment of a large number of students, likely to mitigate re-segregation that choice patterns could exacerbate. The results also reveal that predicting school choice is a challenging machine learning problem. Overall, this study offers a novel practical framework that both academics and policymakers might use to foster more diverse and integrated schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12572v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongzhao Guan, Nabeel Gillani, Tyler Simko, Jasmine Mangat, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Pennsieve: A Collaborative Platform for Translational Neuroscience and Beyond</title>
      <link>https://arxiv.org/abs/2409.10509</link>
      <description>arXiv:2409.10509v2 Announce Type: replace 
Abstract: The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.
  Pennsieve forms the core for major neuroscience research programs including NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10509v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zack Goldblum, Zhongchuan Xu, Haoer Shi, Patryk Orzechowski, Jamaal Spence, Kathryn A Davis, Brian Litt, Nishant Sinha, Joost Wagenaar</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Enhanced Interactive Agent for Public Education on Newborn Auricular Deformities</title>
      <link>https://arxiv.org/abs/2409.12984</link>
      <description>arXiv:2409.12984v2 Announce Type: replace 
Abstract: Auricular deformities are quite common in newborns with potential long-term negative effects of mental and even hearing problems.Early diagnosis and subsequent treatment are critical for the illness; yet they are missing most of the time due to lack of knowledge among parents. With the help of large language model of Ernie of Baidu Inc., we derive a realization of interactive agent. Firstly, it is intelligent enough to detect which type of auricular deformity corresponding to uploaded images, which is accomplished by PaddleDetection, with precision rate 75\%. Secondly, in terms of popularizing the knowledge of auricular deformities, the agent can give professional suggestions of the illness to parents. The above two effects are evaluated via tests on volunteers with control groups in the paper. The agent can reach parents with newborns as well as their pediatrician remotely via Internet in vast, rural areas with quality medical diagnosis capabilities and professional query-answering functions, which is good news for newborn auricular deformity and other illness that requires early intervention for better treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12984v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyue Wang, Liujie Ren, Tianyao Zhou, Lili Chen, Tianyu Zhang, Yaoyao Fu, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Feeding the Crave: How People with Eating Disorders Get Trapped in the Perpetual Cycle of Digital Food Content</title>
      <link>https://arxiv.org/abs/2311.05920</link>
      <description>arXiv:2311.05920v3 Announce Type: replace-cross 
Abstract: Recent studies have examined how digital food content impacts viewers' dietary health. A few have found that individuals with eating disorders are particularly sensitive to digital food content, such as eating and cooking videos, which contribute to disordered eating behaviors. However, there is a lack of comprehensive studies that investigate how these individuals interact with various digital food content. To fill this gap, we conducted two rounds of studies (N=23 and 22, respectively) with individuals with eating disorders to understand their motivations and practices of consuming digital food content. Our study reveals that participants anticipate positive effects from food media to overcome their condition, but in practice, it often exacerbates their disorder. We also discovered that many participants experienced a cycle of quitting and returning to digital food content consumption. Based on these findings, we articulate design implications for digital food content and multimedia platforms to support vulnerable individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05920v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryuhaerang Choi, Subin Park, Sujin Han, Sung-Ju Lee</dc:creator>
    </item>
    <item>
      <title>Fair Enough? A map of the current limitations of the requirements to have fair algorithms</title>
      <link>https://arxiv.org/abs/2311.12435</link>
      <description>arXiv:2311.12435v4 Announce Type: replace-cross 
Abstract: In recent years, the increase in the usage and efficiency of Artificial Intelligence and, more in general, of Automated Decision-Making systems has brought with it an increasing and welcome awareness of the risks associated with such systems. One of such risks is that of perpetuating or even amplifying bias and unjust disparities present in the data from which many of these systems learn to adjust and optimise their decisions. This awareness has on the one hand encouraged several scientific communities to come up with more and more appropriate ways and methods to assess, quantify, and possibly mitigate such biases and disparities. On the other hand, it has prompted more and more layers of society, including policy makers, to call for fair algorithms. We believe that while many excellent and multidisciplinary research is currently being conducted, what is still fundamentally missing is the awareness that having fair algorithms is per se a nearly meaningless requirement that needs to be complemented with many additional social choices to become actionable. Namely, there is a hiatus between what the society is demanding from Automated Decision-Making systems, and what this demand actually means in real-world scenarios. In this work, we outline the key features of such a hiatus and pinpoint a set of crucial open points that we as a society must address in order to give a concrete meaning to the increasing demand of fairness in Automated Decision-Making systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12435v4</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Regoli, Alessandro Castelnovo, Nicole Inverardi, Gabriele Nanino, Ilaria Penco</dc:creator>
    </item>
    <item>
      <title>A Dataset of Uniswap daily transaction indices by network</title>
      <link>https://arxiv.org/abs/2312.02660</link>
      <description>arXiv:2312.02660v2 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) is reshaping traditional finance by enabling direct transactions without intermediaries, creating a rich source of open financial data. Layer 2 (L2) solutions are emerging to enhance the scalability and efficiency of the DeFi ecosystem, surpassing Layer 1 (L1) systems. However, the impact of L2 solutions is still underexplored, mainly due to the lack of comprehensive transaction data indices for economic analysis. This study bridges that gap by analyzing over 50 million transactions from Uniswap, a major decentralized exchange, across both L1 and L2 networks. We created a set of daily indices from blockchain data on Ethereum, Optimism, Arbitrum, and Polygon, offering insights into DeFi adoption, scalability, decentralization, and wealth distribution. Additionally, we developed an open-source Python framework for calculating decentralization indices, making this dataset highly useful for advanced machine learning research. Our work provides valuable resources for data scientists and contributes to the growth of the intelligent Web3 ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02660v2</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nir Chemaya, Lin William Cong, Emma Jorgensen, Dingyue Liu, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area</title>
      <link>https://arxiv.org/abs/2401.16844</link>
      <description>arXiv:2401.16844v2 Announce Type: replace-cross 
Abstract: Congestion pricing, while adopted by many cities to alleviate traffic congestion, raises concerns about widening socioeconomic disparities due to its disproportionate impact on low-income travelers. We address this concern by proposing a new class of congestion pricing schemes that not only minimize total travel time, but also incorporate an equity objective, reducing disparities in the relative change in travel costs across populations with different incomes, following the implementation of tolls. Our analysis builds on a congestion game model with heterogeneous traveler populations. We present four pricing schemes that account for practical considerations, such as the ability to charge differentiated tolls to various traveler populations and the option to toll all or only a subset of edges in the network. We evaluate our pricing schemes in the calibrated freeway network of the San Francisco Bay Area. We demonstrate that the proposed congestion pricing schemes improve both the total travel time and the equity objective compared to the current pricing scheme.
  Our results further show that pricing schemes charging differentiated prices to traveler populations with varying value-of-time lead to a more equitable distribution of travel costs compared to those that charge a homogeneous price to all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16844v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Maheshwari, Kshitij Kulkarni, Druv Pai, Jiarui Yang, Manxi Wu, Shankar Sastry</dc:creator>
    </item>
    <item>
      <title>ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2403.09724</link>
      <description>arXiv:2403.09724v4 Announce Type: replace-cross 
Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09724v4</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah</dc:creator>
    </item>
    <item>
      <title>SocialEyes: Scaling mobile eye-tracking to multi-person social settings</title>
      <link>https://arxiv.org/abs/2407.06345</link>
      <description>arXiv:2407.06345v2 Announce Type: replace-cross 
Abstract: Eye movements provide a window into human behaviour, attention, and interaction dynamics. Challenges in real-world, multi-person environments have, however, restrained eye-tracking research predominantly to single-person, in-lab settings. We developed a system to stream, record, and analyse synchronised data from multiple mobile eye-tracking devices during collective viewing experiences (e.g., concerts, films, lectures). We implemented lightweight operator interfaces for real-time-monitoring, remote-troubleshooting, and gaze-projection from individual egocentric perspectives to a common coordinate space for shared gaze analysis. We tested the system in a live concert and a film screening with 30 simultaneous viewers during each of two public events (N=60). We observe precise time-synchronisation between devices measured through recorded clock-offsets, and accurate gaze-projection in challenging dynamic scenes. Our novel analysis metrics and visualizations illustrate the potential of collective eye-tracking data for understanding collaborative behaviour and social interaction. This advancement promotes ecological validity in eye-tracking research and paves the way for innovative interactive tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06345v2</guid>
      <category>cs.HC</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shreshth Saxena, Areez Visram, Neil Lobo, Zahid Mirza, Mehak Rafi Khan, Biranugan Pirabaharan, Alexander Nguyen, Lauren K. Fink</dc:creator>
    </item>
    <item>
      <title>Ethical Challenges in Computer Vision: Ensuring Privacy and Mitigating Bias in Publicly Available Datasets</title>
      <link>https://arxiv.org/abs/2409.10533</link>
      <description>arXiv:2409.10533v3 Announce Type: replace-cross 
Abstract: This paper aims to shed light on the ethical problems of creating and deploying computer vision tech, particularly in using publicly available datasets. Due to the rapid growth of machine learning and artificial intelligence, computer vision has become a vital tool in many industries, including medical care, security systems, and trade. However, extensive use of visual data that is often collected without consent due to an informed discussion of its ramifications raises significant concerns about privacy and bias. The paper also examines these issues by analyzing popular datasets such as COCO, LFW, ImageNet, CelebA, PASCAL VOC, etc., that are usually used for training computer vision models. We offer a comprehensive ethical framework that addresses these challenges regarding the protection of individual rights, minimization of bias as well as openness and responsibility. We aim to encourage AI development that will take into account societal values as well as ethical standards to avoid any public harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10533v3</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghalib Ahmed Tahir</dc:creator>
    </item>
  </channel>
</rss>
