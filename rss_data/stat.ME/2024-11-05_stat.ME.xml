<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 05:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simulations for estimation of random effects and overall effect in three-level meta-analysis of standardized mean differences using constant and inverse-variance weights</title>
      <link>https://arxiv.org/abs/2411.00795</link>
      <description>arXiv:2411.00795v1 Announce Type: new 
Abstract: We consider a three-level meta-analysis of standardized mean differences. The standard method of estimation uses inverse-variance weights and REML/PL estimation of variance components for the random effects. We introduce new moment-based point and interval estimators for the two variance components and related estimators of the overall mean. Similar to traditional analysis of variance, our method is based on two conditional $Q$ statistics with effective-sample-size weights. We study, by simulation, bias and coverage of these new estimators. For comparison, we also study bias and coverage of the REML/PL-based approach as implemented in {\it rma.mv} in {\it metafor}. Our results demonstrate that the new methods are often considerably better and do not have convergence problems, which plague the standard analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00795v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Kulinskaya, David C. Hoaglin</dc:creator>
    </item>
    <item>
      <title>Differentially Private Algorithms for Linear Queries via Stochastic Convex Optimization</title>
      <link>https://arxiv.org/abs/2411.00921</link>
      <description>arXiv:2411.00921v1 Announce Type: new 
Abstract: This article establishes a method to answer a finite set of linear queries on a given dataset while ensuring differential privacy. To achieve this, we formulate the corresponding task as a saddle-point problem, i.e. an optimization problem whose solution corresponds to a distribution minimizing the difference between answers to the linear queries based on the true distribution and answers from a differentially private distribution. Against this background, we establish two new algorithms for corresponding differentially private data release: the first is based on the differentially private Frank-Wolfe method, the second combines randomized smoothing with stochastic convex optimization techniques for a solution to the saddle-point problem. While previous works assess the accuracy of differentially private algorithms with reference to the empirical data distribution, a key contribution of our work is a more natural evaluation of the proposed algorithms' accuracy with reference to the true data-generating distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00921v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Micali, Clement Lezane, Annika Betken</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for the quadratic assignment procedure</title>
      <link>https://arxiv.org/abs/2411.00947</link>
      <description>arXiv:2411.00947v1 Announce Type: new 
Abstract: The quadratic assignment procedure (QAP) is a popular tool for analyzing network data in medical and social sciences. To test the association between two network measurements represented by two symmetric matrices, QAP calculates the $p$-value by permuting the units, or equivalently, by simultaneously permuting the rows and columns of one matrix. Its extension to the regression setting, known as the multiple regression QAP, has also gained popularity, especially in psychometrics. However, the statistics theory for QAP has not been fully established in the literature. We fill the gap in this paper. We formulate the network models underlying various QAPs. We derive (a) the asymptotic sampling distributions of some canonical test statistics and (b) the corresponding asymptotic permutation distributions induced by QAP under strong and weak null hypotheses. Task (a) relies on applying the theory of U-statistics, and task (b) relies on applying the theory of double-indexed permutation statistics. The combination of tasks (a) and (b) provides a relatively complete picture of QAP. Overall, our asymptotic theory suggests that using properly studentized statistics in QAP is a robust choice in that it is finite-sample exact under the strong null hypothesis and preserves the asymptotic type one error rate under the weak null hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00947v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lei Shi, Peng Ding</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Approach to Causal Inference</title>
      <link>https://arxiv.org/abs/2411.00950</link>
      <description>arXiv:2411.00950v1 Announce Type: new 
Abstract: In causal inference, an important problem is to quantify the effects of interventions or treatments. Many studies focus on estimating the mean causal effects; however, these estimands may offer limited insight since two distributions can share the same mean yet exhibit significant differences. Examining the causal effects from a distributional perspective provides a more thorough understanding. In this paper, we employ a semiparametric density ratio model (DRM) to characterize the counterfactual distributions, introducing a framework that assumes a latent structure shared by these distributions. Our model offers flexibility by avoiding strict parametric assumptions on the counterfactual distributions. Specifically, the DRM incorporates a nonparametric component that can be estimated through the method of empirical likelihood (EL), using the data from all the groups stemming from multiple interventions. Consequently, the EL-DRM framework enables inference of the counterfactual distribution functions and their functionals, facilitating direct and transparent causal inference from a distributional perspective. Numerical studies on both synthetic and real-world data validate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00950v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Archer Gong Zhang, Nancy Reid, Qiang Sun</dc:creator>
    </item>
    <item>
      <title>Multivariate Gini-type discrepancies</title>
      <link>https://arxiv.org/abs/2411.01052</link>
      <description>arXiv:2411.01052v1 Announce Type: new 
Abstract: Measuring distances in a multidimensional setting is a challenging problem, which appears in many fields of science and engineering. In this paper, to measure the distance between two multivariate distributions, we introduce a new measure of discrepancy which is scale invariant and which, in the case of two independent copies of the same distribution, and after normalization, coincides with the scaling invariant multidimensional version of the Gini index recently proposed in [34]. A byproduct of the analysis is an easy-to-handle discrepancy metric, obtained by application of the theory to a pair of Gaussian multidimensional densities. The obtained metric does improve the standard metrics, based on the mean squared error, as it is scale invariant. The importance of this theoretical finding is illustrated by means of a real problem that concerns measuring the importance of Environmental, Social and Governance factors for the growth of small and medium enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01052v1</guid>
      <category>stat.ME</category>
      <category>math.AP</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gennaro Auricchio, Giovanni Brigati, Paolo Giudici, Giuseppe Toscani</dc:creator>
    </item>
    <item>
      <title>Local Indicators of Mark Association for Spatial Marked Point Processes</title>
      <link>https://arxiv.org/abs/2411.01065</link>
      <description>arXiv:2411.01065v1 Announce Type: new 
Abstract: The emergence of distinct local mark behaviours is becoming increasingly common in the applications of spatial marked point processes. This dynamic highlights the limitations of existing global mark correlation functions in accurately identifying the true patterns of mark associations/variations among points as distinct mark behaviours might dominate one another, giving rise to an incomplete understanding of mark associations. In this paper, we introduce a family of local indicators of mark association (LIMA) functions for spatial marked point processes. These functions are defined on general state spaces and can include marks that are either real-valued or function-valued. Unlike global mark correlation functions, which are often distorted by the existence of distinct mark behaviours, LIMA functions reliably identify all types of mark associations and variations among points. Additionally, they accurately determine the interpoint distances where individual points show significant mark associations. Through simulation studies, featuring various scenarios, and four real applications in forestry, criminology, and urban mobility, we study spatial marked point processes in $\R^2$ and on linear networks with either real-valued or function-valued marks, demonstrating that LIMA functions significantly outperform the existing global mark correlation functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01065v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Mehdi Moradi</dc:creator>
    </item>
    <item>
      <title>A novel method for synthetic control with interference</title>
      <link>https://arxiv.org/abs/2411.01249</link>
      <description>arXiv:2411.01249v1 Announce Type: new 
Abstract: Synthetic control methods have been widely used for evaluating policy effects in comparative case studies. However, most existing synthetic control methods implicitly rule out interference effects on the untreated units, and their validity may be jeopardized in the presence of interference. In this paper, we propose a novel synthetic control method, which admits interference but does not require modeling the interference structure. Identification of the effects is achieved under the assumption that the number of interfered units is no larger than half of the total number of units minus the dimension of confounding factors. We propose consistent and asymptotically normal estimation and establish statistical inference for the direct and interference effects averaged over post-intervention periods. We evaluate the performance of our method and compare it to competing methods via numerical experiments. A real data analysis, evaluating the effects of the announcement of relocating the US embassy to Jerusalem on the number of Middle Eastern conflicts, provides empirical evidence that the announcement not only increases the number of conflicts in Israel-Palestine but also has interference effects on several other Middle Eastern countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01249v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyu He, Yilin Li, Xu Shi, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Hierarchical and Density-based Causal Clustering</title>
      <link>https://arxiv.org/abs/2411.01250</link>
      <description>arXiv:2411.01250v1 Announce Type: new 
Abstract: Understanding treatment effect heterogeneity is vital for scientific and policy research. However, identifying and evaluating heterogeneous treatment effects pose significant challenges due to the typically unknown subgroup structure. Recently, a novel approach, causal k-means clustering, has emerged to assess heterogeneity of treatment effect by applying the k-means algorithm to unknown counterfactual regression functions. In this paper, we expand upon this framework by integrating hierarchical and density-based clustering algorithms. We propose plug-in estimators that are simple and readily implementable using off-the-shelf algorithms. Unlike k-means clustering, which requires the margin condition, our proposed estimators do not rely on strong structural assumptions on the outcome process. We go on to study their rate of convergence, and show that under the minimal regularity conditions, the additional cost of causal clustering is essentially the estimation error of the outcome regression functions. Our findings significantly extend the capabilities of the causal clustering framework, thereby contributing to the progression of methodologies for identifying homogeneous subgroups in treatment response, consequently facilitating more nuanced and targeted interventions. The proposed methods also open up new avenues for clustering with generic pseudo-outcomes. We explore finite sample properties via simulation, and illustrate the proposed methods in voting and employment projection datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01250v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangho Kim, Jisu Kim, Larry A. Wasserman, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Distributed Pseudo-Likelihood Method for Community Detection in Large-Scale Networks</title>
      <link>https://arxiv.org/abs/2411.01317</link>
      <description>arXiv:2411.01317v1 Announce Type: new 
Abstract: This paper proposes a distributed pseudo-likelihood method (DPL) to conveniently identify the community structure of large-scale networks. Specifically, we first propose a block-wise splitting method to divide large-scale network data into several subnetworks and distribute them among multiple workers. For simplicity, we assume the classical stochastic block model. Then, the DPL algorithm is iteratively implemented for the distributed optimization of the sum of the local pseudo-likelihood functions. At each iteration, the worker updates its local community labels and communicates with the master. The master then broadcasts the combined estimator to each worker for the new iterative steps. Based on the distributed system, DPL significantly reduces the computational complexity of the traditional pseudo-likelihood method using a single machine. Furthermore, to ensure statistical accuracy, we theoretically discuss the requirements of the worker sample size. Moreover, we extend the DPL method to estimate degree-corrected stochastic block models. The superior performance of the proposed distributed algorithm is demonstrated through extensive numerical studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01317v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3657300</arxiv:DOI>
      <dc:creator>Jiayi Deng, Danyang Huang, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Modeling the restricted mean survival time using pseudo-value random forests</title>
      <link>https://arxiv.org/abs/2411.01381</link>
      <description>arXiv:2411.01381v1 Announce Type: new 
Abstract: The restricted mean survival time (RMST) has become a popular measure to summarize event times in longitudinal studies. Defined as the area under the survival function up to a time horizon $\tau$ &gt; 0, the RMST can be interpreted as the life expectancy within the time interval [0, $\tau$]. In addition to its straightforward interpretation, the RMST also allows for the definition of valid estimands for the causal analysis of treatment contrasts in medical studies. In this work, we introduce a non-parametric approach to model the RMST conditional on a set of baseline variables (including, e.g., treatment variables and confounders). Our method is based on a direct modeling strategy for the RMST, using leave-one-out jackknife pseudo-values within a random forest regression framework. In this way, it can be employed to obtain precise estimates of both patient-specific RMST values and confounder-adjusted treatment contrasts. Since our method (termed "pseudo-value random forest", PVRF) is model-free, RMST estimates are not affected by restrictive assumptions like the proportional hazards assumption. Particularly, PVRF offers a high flexibility in detecting relevant covariate effects from higher-dimensional data, thereby expanding the range of existing pseudo-value modeling techniques for RMST estimation. We investigate the properties of our method using simulations and illustrate its use by an application to data from the SUCCESS-A breast cancer trial. Our numerical experiments demonstrate that PVRF yields accurate estimates of both patient-specific RMST values and RMST-based treatment contrasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01381v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alina Schenk, Vanessa Basten, Matthias Schmid</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing under unidentified nonparametric models with an application to survival predictions under transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v1 Announce Type: new 
Abstract: The multi-modal posterior under unidentified nonparametric models yields poor mixing of Markov Chain Monte Carlo (MCMC), which is a stumbling block to Bayesian predictions. In this article, we conceptualize a prior informativeness threshold that is essentially the variance of posterior modes and expressed by the uncertainty hyperparameters of nonparametric priors. The threshold plays the role of a lower bound of the within-chain MCMC variance to ensure MCMC mixing, and engines prior modification through hyperparameter tuning to descend the mode variance. Our method distinguishes from existing postprocessing methods in that it directly samples well-mixed MCMC chains on the unconstrained space, and inherits the original posterior predictive distribution in predictive inference. Our method succeeds in Bayesian survival predictions under an unidentified nonparametric transformation model, guarded by the inferential theories of the posterior variance, under elicitation of two delicate nonparametric priors. Comprehensive simulations and real-world data analysis demonstrate that our method achieves MCMC mixing and outperforms existing approaches in survival predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Catherine C. Liu, Zhaohai Li</dc:creator>
    </item>
    <item>
      <title>Automated Analysis of Experiments using Hierarchical Garrote</title>
      <link>https://arxiv.org/abs/2411.01383</link>
      <description>arXiv:2411.01383v1 Announce Type: new 
Abstract: In this work, we propose an automatic method for the analysis of experiments that incorporates hierarchical relationships between the experimental variables. We use a modified version of nonnegative garrote method for variable selection which can incorporate hierarchical relationships. The nonnegative garrote method requires a good initial estimate of the regression parameters for it to work well. To obtain the initial estimate, we use generalized ridge regression with the ridge parameters estimated from a Gaussian process prior placed on the underlying input-output relationship. The proposed method, called HiGarrote, is fast, easy to use, and requires no manual tuning. Analysis of several real experiments are presented to demonstrate its benefits over the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01383v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei-Yang Yu, V. Roshan Joseph</dc:creator>
    </item>
    <item>
      <title>Educational Effects in Mathematics: Conditional Average Treatment Effect depending on the Number of Treatments</title>
      <link>https://arxiv.org/abs/2411.01498</link>
      <description>arXiv:2411.01498v1 Announce Type: new 
Abstract: This study examines the educational effect of the Academic Support Center at Kogakuin University. Following the initial assessment, it was suggested that group bias had led to an underestimation of the Center's true impact. To address this issue, the authors applied the theory of causal inference. By using T-learner, the conditional average treatment effect (CATE) of the Center's face-to-face (F2F) personal assistance program was evaluated. Extending T-learner, the authors produced a new CATE function that depends on the number of treatments (F2F sessions) and used the estimated function to predict the CATE performance of F2F assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01498v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoko Nagai, Takayuki Okuda, Tomoya Nakamura, Yuichiro Sato, Yusuke Sato, Kensaku Kinjo, Kengo Kawamura, Shin Kikuta, Naoto Kumano-go</dc:creator>
    </item>
    <item>
      <title>Enhancing Forecasts Using Real-Time Data Flow and Hierarchical Forecast Reconciliation, with Applications to the Energy Sector</title>
      <link>https://arxiv.org/abs/2411.01528</link>
      <description>arXiv:2411.01528v1 Announce Type: new 
Abstract: A novel framework for hierarchical forecast updating is presented, addressing a critical gap in the forecasting literature. By assuming a temporal hierarchy structure, the innovative approach extends hierarchical forecast reconciliation to effectively manage the challenge posed by partially observed data. This crucial extension allows, in conjunction with real-time data, to obtain updated and coherent forecasts across the entire temporal hierarchy, thereby enhancing decision-making accuracy. The framework involves updating base models in response to new data, which produces revised base forecasts. A subsequent pruning step integrates the newly available data, allowing for the application of any forecast reconciliation method to obtain fully updated reconciled forecasts. Additionally, the framework not only ensures coherence among forecasts but also improves overall accuracy throughout the hierarchy. Its inherent flexibility and interpretability enable users to perform hierarchical forecast updating concisely. The methodology is extensively demonstrated in a simulation study with various settings and comparing different data-generating processes, hierarchies, and reconciliation methods. Practical applicability is illustrated through two case studies in the energy sector, energy generation and solar power data, where the framework yields superior results compared to base models that do not incorporate new data, leading to more precise decision-making outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01528v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Neubauer, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Statistical Inference on High Dimensional Gaussian Graphical Regression Models</title>
      <link>https://arxiv.org/abs/2411.01588</link>
      <description>arXiv:2411.01588v1 Announce Type: new 
Abstract: Gaussian graphical regressions have emerged as a powerful approach for regressing the precision matrix of a Gaussian graphical model on covariates, which, unlike traditional Gaussian graphical models, can help determine how graphs are modulated by high dimensional subject-level covariates, and recover both the population-level and subject-level graphs. To fit the model, a multi-task learning approach {achieves} %has been shown to result in lower error rates compared to node-wise regressions. However, due to the high complexity and dimensionality of the Gaussian graphical regression problem, the important task of statistical inference remains unexplored. We propose a class of debiased estimators based on multi-task learners for statistical inference in Gaussian graphical regressions. We show that debiasing can be performed quickly and separately for the multi-task learners. In a key debiasing step {that estimates} %involving the estimation of the inverse covariance matrix, we propose a novel {projection technique} %diagonalization approach that dramatically reduces computational costs {in optimization} to scale only with the sample size $n$. We show that our debiased estimators enjoy a fast convergence rate and asymptotically follow a normal distribution, enabling valid statistical inference such as constructing confidence intervals and performing hypothesis testing. Simulation studies confirm the practical utility of the proposed approach, and we further apply it to analyze gene co-expression graph data from a brain cancer study, revealing meaningful biological relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01588v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuran Meng, Jingfei Zhang, Yi Li</dc:creator>
    </item>
    <item>
      <title>FRODO: A novel approach to micro-macro multilevel regressio</title>
      <link>https://arxiv.org/abs/2411.01686</link>
      <description>arXiv:2411.01686v1 Announce Type: new 
Abstract: Within the field of hierarchical modelling, little attention is paid to micro-macro models: those in which group-level outcomes are dependent on covariates measured at the level of individuals within groups. Although such models are perhaps underrepresented in the literature, they have applications in economics, epidemiology, and the social sciences. Despite the strong mathematical similarities between micro-macro and measurement error models, few efforts have been made to apply the much better-developed methodology of the latter to the former. Here, we present a new empirical Bayesian technique for micro-macro data, called FRODO (Functional Regression On Densities of Observations). The method jointly infers group-specific densities for multilevel covariates and uses them as functional predictors in a functional linear regression, resulting in a model that is analogous to a generalized additive model (GAM). In doing so, it achieves a level of generality comparable to more sophisticated methods developed for errors-in-variables models, while further leveraging the larger group sizes characteristic of multilevel data to provide richer information about the within-group covariate distributions. After explaining the hierarchical structure of FRODO, its power and versatility are demonstrated on several simulated datasets, showcasing its ability to accommodate a wide variety of covariate distributions and regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01686v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun McDonald, Alexandre Leblanc, Saman Muthukumarana, David Campbell</dc:creator>
    </item>
    <item>
      <title>A probabilistic diagnostic for Laplace approximations: Introduction and experimentation</title>
      <link>https://arxiv.org/abs/2411.01697</link>
      <description>arXiv:2411.01697v1 Announce Type: new 
Abstract: Many models require integrals of high-dimensional functions: for instance, to obtain marginal likelihoods. Such integrals may be intractable, or too expensive to compute numerically. Instead, we can use the Laplace approximation (LA). The LA is exact if the function is proportional to a normal density; its effectiveness therefore depends on the function's true shape. Here, we propose the use of the probabilistic numerical framework to develop a diagnostic for the LA and its underlying shape assumptions, modelling the function and its integral as a Gaussian process and devising a "test" by conditioning on a finite number of function values. The test is decidedly non-asymptotic and is not intended as a full substitute for numerical integration - rather, it is simply intended to test the feasibility of the assumptions underpinning the LA with as minimal computation. We discuss approaches to optimize and design the test, apply it to known sample functions, and highlight the challenges of high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01697v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun McDonald, David Campbell</dc:creator>
    </item>
    <item>
      <title>Comparing multilevel and fixed effect approaches in the generalized linear model setting</title>
      <link>https://arxiv.org/abs/2411.01723</link>
      <description>arXiv:2411.01723v1 Announce Type: new 
Abstract: We extend prior work comparing linear multilevel models (MLM) and fixed effect (FE) models to the generalized linear model (GLM) setting, where the coefficient on a treatment variable is of primary interest. This leads to three key insights. (i) First, as in the linear setting, MLM can be thought of as a regularized form of FE. This explains why MLM can show large biases in its treatment coefficient estimates when group-level confounding is present. However, unlike the linear setting, there is not an exact equivalence between MLM and regularized FE coefficient estimates in GLMs. (ii) Second, we study a generalization of "bias-corrected MLM" (bcMLM) to the GLM setting. Neither FE nor bcMLM entirely solves MLM's bias problem in GLMs, but bcMLM tends to show less bias than does FE. (iii) Third, and finally, just like in the linear setting, MLM's default standard errors can misspecify the true intragroup dependence structure in the GLM setting, which can lead to downwardly biased standard errors. A cluster bootstrap is a more agnostic alternative. Ultimately, for non-linear GLMs, we recommend bcMLM for estimating the treatment coefficient, and a cluster bootstrap for standard errors and confidence intervals. If a bootstrap is not computationally feasible, then we recommend FE with cluster-robust standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01723v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>He Bai, Asa Ferguson, Leonard Wainstein, Jonathan Wells</dc:creator>
    </item>
    <item>
      <title>Alignment and matching tests for high-dimensional tensor signals via tensor contraction</title>
      <link>https://arxiv.org/abs/2411.01732</link>
      <description>arXiv:2411.01732v1 Announce Type: new 
Abstract: We consider two hypothesis testing problems for low-rank and high-dimensional tensor signals, namely the tensor signal alignment and tensor signal matching problems. These problems are challenging due to the high dimension of tensors and lack of meaningful test statistics. By exploiting a recent tensor contraction method, we propose and validate relevant test statistics using eigenvalues of a data matrix resulting from the tensor contraction. The matrix has a long range dependence among its entries, which makes the analysis of the matrix challenging, involved and distinct from standard random matrix theory. Our approach provides a novel framework for addressing hypothesis testing problems in the context of high-dimensional tensor signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01732v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihan Liu, Zhenggang Wang, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Dynamic Supervised Principal Component Analysis for Classification</title>
      <link>https://arxiv.org/abs/2411.01820</link>
      <description>arXiv:2411.01820v1 Announce Type: new 
Abstract: This paper introduces a novel framework for dynamic classification in high dimensional spaces, addressing the evolving nature of class distributions over time or other index variables. Traditional discriminant analysis techniques are adapted to learn dynamic decision rules with respect to the index variable. In particular, we propose and study a new supervised dimension reduction method employing kernel smoothing to identify the optimal subspace, and provide a comprehensive examination of this approach for both linear discriminant analysis and quadratic discriminant analysis. We illustrate the effectiveness of the proposed methods through numerical simulations and real data examples. The results show considerable improvements in classification accuracy and computational efficiency. This work contributes to the field by offering a robust and adaptive solution to the challenges of scalability and non-staticity in high-dimensional data classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01820v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbo Ouyang, Ruiyang Wu, Ning Hao, Hao Helen Zhang</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification and multi-stage variable selection for personalized treatment regimes</title>
      <link>https://arxiv.org/abs/2411.02123</link>
      <description>arXiv:2411.02123v1 Announce Type: new 
Abstract: A dynamic treatment regime is a sequence of medical decisions that adapts to the evolving clinical status of a patient over time. To facilitate personalized care, it is crucial to assess the probability of each available treatment option being optimal for a specific patient, while also identifying the key prognostic factors that determine the optimal sequence of treatments. This task has become increasingly challenging due to the growing number of individual prognostic factors typically available. In response to these challenges, we propose a Bayesian model for optimizing dynamic treatment regimes that addresses the uncertainty in identifying optimal decision sequences and incorporates dimensionality reduction to manage high-dimensional individual covariates. The first task is achieved through a suitable augmentation of the model to handle counterfactual variables. For the second, we introduce a novel class of spike-and-slab priors for the multi-stage selection of significant factors, to favor the sharing of information across stages. The effectiveness of the proposed approach is demonstrated through extensive simulation studies and illustrated using clinical trial data on severe acute arterial hypertension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02123v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiefeng Bi, Matteo Borrotti, Bernardo Nipoti</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds for Continuous-Valued Treatment Effects with Unobserved Confounders</title>
      <link>https://arxiv.org/abs/2411.02231</link>
      <description>arXiv:2411.02231v1 Announce Type: new 
Abstract: In causal inference, treatment effects are typically estimated under the ignorability, or unconfoundedness, assumption, which is often unrealistic in observational data. By relaxing this assumption and conducting a sensitivity analysis, we introduce novel bounds and derive confidence intervals for the Average Potential Outcome (APO) - a standard metric for evaluating continuous-valued treatment or exposure effects. We demonstrate that these bounds are sharp under a continuous sensitivity model, in the sense that they give the smallest possible interval under this model, and propose a doubly robust version of our estimators. In a comparative analysis with the method of Jesson et al. (2022) (arXiv:2204.10022), using both simulated and real datasets, we show that our approach not only yields sharper bounds but also achieves good coverage of the true APO, with significantly reduced computation times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02231v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Baptiste Baitairian, Bernard Sebastien, Rana Jreich, Sandrine Katsahian, Agathe Guilloux</dc:creator>
    </item>
    <item>
      <title>Powerful batch conformal prediction for classification</title>
      <link>https://arxiv.org/abs/2411.02239</link>
      <description>arXiv:2411.02239v1 Announce Type: new 
Abstract: In a supervised classification split conformal/inductive framework with $K$ classes, a calibration sample of $n$ labeled examples is observed for inference on the label of a new unlabeled example. In this work, we explore the case where a "batch" of $m$ independent such unlabeled examples is given, and a multivariate prediction set with $1-\alpha$ coverage should be provided for this batch. Hence, the batch prediction set takes the form of a collection of label vectors of size $m$, while the calibration sample only contains univariate labels. Using the Bonferroni correction consists in concatenating the individual prediction sets at level $1-\alpha/m$ (Vovk 2013). We propose a uniformly more powerful solution, based on specific combinations of conformal $p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the pooled evidence of fairly "easy" examples of the batch can help provide narrower batch prediction sets. We also introduced adaptive versions of the novel procedure that are particularly effective when the batch prediction set is expected to be large. The theoretical guarantees are provided when all examples are iid, as well as more generally when iid is assumed only conditionally within each class. In particular, our results are also valid under a label distribution shift since the distribution of the labels need not be the same in the calibration sample and in the new `batch'. The usefulness of the method is illustrated on synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02239v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulysse Gazin, Ruth Heller, Etienne Roquain, Aldo Solari</dc:creator>
    </item>
    <item>
      <title>A Bayesian Model for Co-clustering Ordinal Data with Informative Missing Entries</title>
      <link>https://arxiv.org/abs/2411.02276</link>
      <description>arXiv:2411.02276v1 Announce Type: new 
Abstract: Several approaches have been proposed in the literature for clustering multivariate ordinal data. These methods typically treat missing values as absent information, rather than recognizing them as valuable for profiling population characteristics. To address this gap, we introduce a Bayesian nonparametric model for co-clustering multivariate ordinal data that treats censored observations as informative, rather than merely missing. We demonstrate that this offers a significant improvement in understanding the underlying structure of the data. Our model exploits the flexibility of two independent Dirichlet processes, allowing us to infer potentially distinct subpopulations that characterize the latent structure of both subjects and variables. The ordinal nature of the data is addressed by introducing latent variables, while a matrix factorization specification is adopted to handle the high dimensionality of the data in a parsimonious way. The conjugate structure of the model enables an explicit derivation of the full conditional distributions of all the random variables in the model, which facilitates seamless posterior inference using a Gibbs sampling algorithm. We demonstrate the method's performance through simulations and by analyzing politician and movie ratings data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02276v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Giampino, Antonio Canale, Bernardo Nipoti</dc:creator>
    </item>
    <item>
      <title>Fusion of Tree-induced Regressions for Clinico-genomic Data</title>
      <link>https://arxiv.org/abs/2411.02396</link>
      <description>arXiv:2411.02396v1 Announce Type: new 
Abstract: Cancer prognosis is often based on a set of omics covariates and a set of established clinical covariates such as age and tumor stage. Combining these two sets poses challenges. First, dimension difference: clinical covariates should be favored because they are low-dimensional and usually have stronger prognostic ability than high-dimensional omics covariates. Second, interactions: genetic profiles and their prognostic effects may vary across patient subpopulations. Last, redundancy: a (set of) gene(s) may encode similar prognostic information as a clinical covariate. To address these challenges, we combine regression trees, employing clinical covariates only, with a fusion-like penalized regression framework in the leaf nodes for the omics covariates. The fusion penalty controls the variability in genetic profiles across subpopulations. We prove that the shrinkage limit of the proposed method equals a benchmark model: a ridge regression with penalized omics covariates and unpenalized clinical covariates. Furthermore, the proposed method allows researchers to evaluate, for different subpopulations, whether the overall omics effect enhances prognosis compared to only employing clinical covariates. In an application to colorectal cancer prognosis based on established clinical covariates and 20,000+ gene expressions, we illustrate the features of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02396v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeroen M. Goedhart, Mark A. van de Wiel, Wessel N. van Wieringen, Thomas Klausch</dc:creator>
    </item>
    <item>
      <title>HOUND: High-Order Universal Numerical Differentiator for a Parameter-free Polynomial Online Approximation</title>
      <link>https://arxiv.org/abs/2411.00794</link>
      <description>arXiv:2411.00794v1 Announce Type: cross 
Abstract: This paper introduces a scalar numerical differentiator, represented as a system of nonlinear differential equations of any high order. We derive the explicit solution for this system and demonstrate that, with a suitable choice of differentiator order, the error converges to zero for polynomial signals with additive white noise. In more general cases, the error remains bounded, provided that the highest estimated derivative is also bounded. A notable advantage of this numerical differentiation method is that it does not require tuning parameters based on the specific characteristics of the signal being differentiated. We propose a discretization method for the equations that implements a cumulative smoothing algorithm for time series. This algorithm operates online, without the need for data accumulation, and it solves both interpolation and extrapolation problems without fitting any coefficients to the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00794v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Katrichek</dc:creator>
    </item>
    <item>
      <title>CausAdv: A Causal-based Framework for Detecting Adversarial Examples</title>
      <link>https://arxiv.org/abs/2411.00839</link>
      <description>arXiv:2411.00839v1 Announce Type: cross 
Abstract: Deep learning has led to tremendous success in many real-world applications of computer vision, thanks to sophisticated architectures such as Convolutional neural networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations in inputs. These inputs appear almost indistinguishable from natural images, yet they are incorrectly classified by CNN architectures. This vulnerability of adversarial examples has led researchers to focus on enhancing the robustness of deep learning models in general, and CNNs in particular, by creating defense and detection methods to distinguish adversarials inputs from natural ones. In this paper, we address the adversarial robustness of CNNs through causal reasoning.
  We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. Then we perform statistical analysis on the filters CI of every sample, whether clan or adversarials, to demonstrate how adversarial examples indeed exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarials detection without the need to train a separate detector. In addition, we illustrate the efficiency of causal explanations as a helpful detection technique through visualizing the causal features. The results can be reproduced using the code available in the repository: https://github.com/HichemDebbi/CausAdv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00839v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hichem Debbi</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Between U.S. Presidential Elections: How Should We Learn From A 2020 Ad Campaign To Inform 2024 Ad Campaigns?</title>
      <link>https://arxiv.org/abs/2411.01100</link>
      <description>arXiv:2411.01100v1 Announce Type: cross 
Abstract: For the 2024 U.S. presidential election, would negative, digital ads against Donald Trump impact voter turnout in Pennsylvania (PA), a key "tipping point" state? The gold standard to address this question, a randomized experiment where voters get randomized to different ads, yields unbiased estimates of the ad effect, but is very expensive. Instead, we propose a less-than-ideal, but significantly cheaper and likely faster framework based on transfer learning, where we transfer knowledge from a past ad experiment in 2020 to evaluate ads for 2024. A key component of our framework is a sensitivity analysis that quantifies the unobservable differences between past and future elections, which can be calibrated in a data-driven manner. We propose two estimators of the 2024 ad effect: a simple regression estimator with bootstrap, which we recommend for practitioners in this field, and an estimator based on the efficient influence function for broader applications. Using our framework, we estimate the effect of running a negative, digital ad campaign against Trump on voter turnout in PA for the 2024 election. Our findings indicate effect heterogeneity across counties of PA and among important subgroups stratified by gender, urbanicity, and education attainment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01100v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinran Miao, Jiwei Zhao, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Causal reasoning in difference graphs</title>
      <link>https://arxiv.org/abs/2411.01292</link>
      <description>arXiv:2411.01292v1 Announce Type: cross 
Abstract: In epidemiology, understanding causal mechanisms across different populations is essential for designing effective public health interventions. Recently, difference graphs have been introduced as a tool to visually represent causal variations between two distinct populations. While there has been progress in inferring these graphs from data through causal discovery methods, there remains a gap in systematically leveraging their potential to enhance causal reasoning. This paper addresses that gap by establishing conditions for identifying causal changes and effects using difference graphs and observational data. It specifically focuses on identifying total causal changes and total effects in a nonparametric framework, as well as direct causal changes and direct effects in a linear context. In doing so, it provides a novel approach to causal reasoning that holds potential for various public health applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01292v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Marginal Causal Flows for Validation and Inference</title>
      <link>https://arxiv.org/abs/2411.01295</link>
      <description>arXiv:2411.01295v1 Announce Type: cross 
Abstract: Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a novel likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly inferring the marginal causal quantities from observational data. We propose that these models are exceptionally well suited for generating synthetic data to validate causal methods. They can create synthetic datasets that closely resemble the empirical dataset, while automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also exactly parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01295v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel de Vassimon Manela, Laura Battaglia, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Efficient Nested Estimation of CoVaR: A Decoupled Approach</title>
      <link>https://arxiv.org/abs/2411.01319</link>
      <description>arXiv:2411.01319v1 Announce Type: cross 
Abstract: This paper addresses the estimation of the systemic risk measure known as CoVaR, which quantifies the risk of a financial portfolio conditional on another portfolio being at risk. We identify two principal challenges: conditioning on a zero-probability event and the repricing of portfolios. To tackle these issues, we propose a decoupled approach utilizing smoothing techniques and develop a model-independent theoretical framework grounded in a functional perspective. We demonstrate that the rate of convergence of the decoupled estimator can achieve approximately $O_{\rm P}(\Gamma^{-1/2})$, where $\Gamma$ represents the computational budget. Additionally, we establish the smoothness of the portfolio loss functions, highlighting its crucial role in enhancing sample efficiency. Our numerical results confirm the effectiveness of the decoupled estimators and provide practical insights for the selection of appropriate smoothing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01319v1</guid>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nifei Lin, Yingda Song, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments</title>
      <link>https://arxiv.org/abs/2411.01394</link>
      <description>arXiv:2411.01394v1 Announce Type: cross 
Abstract: Patients at a comprehensive cancer center who do not achieve cure or remission following standard treatments often become candidates for clinical trials. Patients who participate in a clinical trial may be suitable for other studies. A key factor influencing patient enrollment in subsequent clinical trials is the structured collaboration between oncologists and most responsible physicians. Possible identification of these collaboration networks can be achieved through the analysis of patient movements between clinical trial intervention types with social network analysis and community detection algorithms. In the detection of oncologist working groups, the present study evaluates three community detection algorithms: Girvan-Newman, Louvain and an algorithm developed by the author. Girvan-Newman identifies each intervention as their own community, while Louvain groups interventions in a manner that is difficult to interpret. In contrast, the author's algorithm groups interventions in a way that is both intuitive and informative, with a gradient effect that is particularly useful for epidemiological research. This lays the groundwork for future subgroup analysis of clustered interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01394v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Smith, Tyler Pittman, Wei Xu</dc:creator>
    </item>
    <item>
      <title>Modeling Home Range and Intra-Specific Spatial Interaction in Wild Animal Populations</title>
      <link>https://arxiv.org/abs/2411.01694</link>
      <description>arXiv:2411.01694v1 Announce Type: cross 
Abstract: Interactions among individuals from the same-species of wild animals are an important component of population dynamics. An interaction can be either static (based on overlap of space use) or dynamic (based on movement). The goal of this work is to determine the level of static interactions between individuals from the same-species of wild animals using 95\% and 50\% home ranges, as well as to model their movement interactions, which could include attraction, avoidance (or repulsion), or lack of interaction, in order to gain new insights and improve our understanding of ecological processes. Home range estimation methods (minimum convex polygon, kernel density estimator, and autocorrelated kernel density estimator), inhomogeneous multitype (or cross-type) summary statistics, and envelope testing methods (pointwise and global envelope tests) were proposed to study the nature of the same-species wild-animal spatial interactions. Using GPS collar data, we applied the methods to quantify both static and dynamic interactions between black bears in southern Alabama, USA. In general, our findings suggest that the black bears in our dataset showed no significant preference to live together or apart, i.e., there was no significant deviation from independence toward association or avoidance (i.e., segregation) between the bears.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01694v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fekadu L. Bayisa, Christopher L. Seals, Hannah J. Leeper, Todd D. Steury, Elvan Ceyhan</dc:creator>
    </item>
    <item>
      <title>Joint optimization for production operations considering reworking</title>
      <link>https://arxiv.org/abs/2411.01772</link>
      <description>arXiv:2411.01772v1 Announce Type: cross 
Abstract: In pursuit of enhancing the comprehensive efficiency of production systems, our study focused on the joint optimization problem of scheduling and machine maintenance in scenarios where product rework occurs. The primary challenge lies in the interdependence between product \underline{q}uality, machine \underline{r}eliability, and \underline{p}roduction scheduling, compounded by the uncertainties from machine degradation and product quality, which is prevalent in sophisticated manufacturing systems. To address this issue, we investigated the dynamic relationship among these three aspects, named as QRP-co-effect. On this basis, we constructed an optimization model that integrates production scheduling, machine maintenance, and product rework decisions, encompassing the context of stochastic degradation and product quality uncertainties within a mixed-integer programming problem. To effectively solve this problem, we proposed a dual-module solving framework that integrates planning and evaluation for solution improvement via dynamic communication. By analyzing the structural properties of this joint optimization problem, we devised an efficient solving algorithm with an interactive mechanism that leverages \emph{in-situ} condition information regarding the production system's state and computational resources. The proposed methodology has been validated through comparative and ablation experiments. The experimental results demonstrated the significant enhancement of production system efficiency, along with a reduction in machine maintenance costs in scenarios involving rework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01772v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilan Shen, Boyang Li, Xi Zhang</dc:creator>
    </item>
    <item>
      <title>Causal Discovery and Classification Using Lempel-Ziv Complexity</title>
      <link>https://arxiv.org/abs/2411.01881</link>
      <description>arXiv:2411.01881v1 Announce Type: cross 
Abstract: Inferring causal relationships in the decision-making processes of machine learning algorithms is a crucial step toward achieving explainable Artificial Intelligence (AI). In this research, we introduce a novel causality measure and a distance metric derived from Lempel-Ziv (LZ) complexity. We explore how the proposed causality measure can be used in decision trees by enabling splits based on features that most strongly \textit{cause} the outcome. We further evaluate the effectiveness of the causality-based decision tree and the distance-based decision tree in comparison to a traditional decision tree using Gini impurity. While the proposed methods demonstrate comparable classification performance overall, the causality-based decision tree significantly outperforms both the distance-based decision tree and the Gini-based decision tree on datasets generated from causal models. This result indicates that the proposed approach can capture insights beyond those of classical decision trees, especially in causally structured data. Based on the features used in the LZ causal measure based decision tree, we introduce a causal strength for each features in the dataset so as to infer the predominant causal variables for the occurrence of the outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01881v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Dhruthi, Nithin Nagaraj, Harikrishnan N B</dc:creator>
    </item>
    <item>
      <title>Targeted Learning for Variable Importance</title>
      <link>https://arxiv.org/abs/2411.02221</link>
      <description>arXiv:2411.02221v1 Announce Type: cross 
Abstract: Variable importance is one of the most widely used measures for interpreting machine learning with significant interest from both statistics and machine learning communities. Recently, increasing attention has been directed toward uncertainty quantification in these metrics. Current approaches largely rely on one-step procedures, which, while asymptotically efficient, can present higher sensitivity and instability in finite sample settings. To address these limitations, we propose a novel method by employing the targeted learning (TL) framework, designed to enhance robustness in inference for variable importance metrics. Our approach is particularly suited for conditional permutation variable importance. We show that it (i) retains the asymptotic efficiency of traditional methods, (ii) maintains comparable computational complexity, and (iii) delivers improved accuracy, especially in finite sample contexts. We further support these findings with numerical experiments that illustrate the practical advantages of our method and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02221v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohan Wang, Yunzhe Zhou, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian regression in astronomy</title>
      <link>https://arxiv.org/abs/2411.02380</link>
      <description>arXiv:2411.02380v1 Announce Type: cross 
Abstract: Model mis-specification (e.g. the presence of outliers) is commonly encountered in astronomical analyses, often requiring the use of ad hoc algorithms (e.g. sigma-clipping). We develop and implement a generic Bayesian approach to linear regression, based on Student's t-distributions, that is robust to outliers and mis-specification of the noise model. Our method is validated using simulated datasets with various degrees of model mis-specification; the derived constraints are shown to be systematically less biased than those from a similar model using normal distributions. We demonstrate that, for a dataset without outliers, a worst-case inference using t-distributions would give unbiased results with $\lesssim\!10$ per cent increase in the reported parameter uncertainties. We also compare with existing analyses of real-world datasets, finding qualitatively different results where normal distributions have been used and agreement where more robust methods have been applied. A Python implementation of this model, t-cup, is made available for others to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02380v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Martin, Daniel J. Mortlock</dc:creator>
    </item>
    <item>
      <title>Structural Nested Mean Models Under Parallel Trends Assumptions</title>
      <link>https://arxiv.org/abs/2204.10291</link>
      <description>arXiv:2204.10291v5 Announce Type: replace 
Abstract: We link and extend two approaches to estimating time-varying treatment effects on repeated continuous outcomes--time-varying Difference in Differences (DiD; see Roth et al. (2023) and Chaisemartin et al. (2023) for reviews) and Structural Nested Mean Models (SNMMs; see Vansteelandt and Joffe (2014) for a review). In particular, we show that SNMMs, which were previously only known to be nonparametrically identified under a no unobserved confounding assumption, are also identified under a generalized version of the parallel trends assumption typically used to justify time-varying DiD methods. Because SNMMs model a broader set of causal estimands, our results allow practitioners of existing time-varying DiD approaches to address additional types of substantive questions under similar assumptions. SNMMs enable estimation of time-varying effect heterogeneity, lasting effects of a `blip' of treatment at a single time point, effects of sustained interventions (possibly on continuous or multi-dimensional treatments) when treatment repeatedly changes value in the data, controlled direct effects, effects of dynamic treatment strategies that depend on covariate history, and more. Our results also allow analysts who apply SNMMs under the no unobserved confounding assumption to estimate some of the same causal effects under alternative identifying assumptions. We provide a method for sensitivity analysis to violations of our parallel trends assumption. We further explain how to estimate optimal treatment regimes via optimal regime SNMMs under parallel trends assumptions plus an assumption that there is no effect modification by unobserved confounders. Finally, we illustrate our methods with real data applications estimating effects of Medicaid expansion on uninsurance rates, effects of floods on flood insurance take-up, and effects of sustained changes in temperature on crop yields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10291v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Oliver Dukes, Meghana Shamsunder, David Richardson, Eric Tchetgen Tchetgen, James Robins</dc:creator>
    </item>
    <item>
      <title>Reliability Study of Battery Lives: A Functional Degradation Analysis Approach</title>
      <link>https://arxiv.org/abs/2212.05515</link>
      <description>arXiv:2212.05515v2 Announce Type: replace 
Abstract: Renewable energy is critical for combating climate change, whose first step is the storage of electricity generated from renewable energy sources. Li-ion batteries are a popular kind of storage units. Their continuous usage through charge-discharge cycles eventually leads to degradation. This can be visualized in plotting voltage discharge curves (VDCs) over discharge cycles. Studies of battery degradation have mostly concentrated on modeling degradation through one scalar measurement summarizing each VDC. Such simplification of curves can lead to inaccurate predictive models. Here we analyze the degradation of rechargeable Li-ion batteries from a NASA data set through modeling and predicting their full VDCs. With techniques from longitudinal and functional data analysis, we propose a new two-step predictive modeling procedure for functional responses residing on heterogeneous domains. We first predict the shapes and domain end points of VDCs using functional regression models. Then we integrate these predictions to perform a degradation analysis. Our approach is fully functional, allows the incorporation of usage information, produces predictions in a curve form, and thus provides flexibility in the assessment of battery degradation. Through extensive simulation studies and cross-validated data analysis, our approach demonstrates better prediction than the existing approach of modeling degradation directly with aggregated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05515v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1931</arxiv:DOI>
      <dc:creator>Youngjin Cho, Quyen Do, Pang Du, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Anytime valid and asymptotically optimal inference driven by predictive recursion</title>
      <link>https://arxiv.org/abs/2309.13441</link>
      <description>arXiv:2309.13441v4 Announce Type: replace 
Abstract: Distinguishing two candidate models is a fundamental and practically important statistical problem. Error rate control is crucial to the testing logic but, in complex nonparametric settings, can be difficult to achieve, especially when the stopping rule that determines the data collection process is not available. This paper proposes an e-process construction based on the predictive recursion (PR) algorithm originally designed to recursively fit nonparametric mixture models. The resulting PRe-process affords anytime valid inference and is asymptotically efficient in the sense that its growth rate is first-order optimal relative to PR's mixture model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13441v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vaidehi Dixit, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Balancing Weights for Causal Inference in Observational Factorial Studies</title>
      <link>https://arxiv.org/abs/2310.04660</link>
      <description>arXiv:2310.04660v2 Announce Type: replace 
Abstract: Many scientific questions in biomedical, environmental, and psychological research involve understanding the effects of multiple factors on outcomes. While factorial experiments are ideal for this purpose, randomized controlled treatment assignment is generally infeasible in many empirical studies. Therefore, investigators must rely on observational data, where drawing reliable causal inferences for multiple factors remains challenging. As the number of treatment combinations grows exponentially with the number of factors, some treatment combinations can be rare or missing by chance in observed data, further complicating factorial effects estimation. To address these challenges, we propose a novel weighting method tailored to observational studies with multiple factors. Our approach uses weighted observational data to emulate a randomized factorial experiment, enabling simultaneous estimation of the effects of multiple factors and their interactions. Our investigations reveal a crucial nuance: achieving balance among covariates, as in single-factor scenarios, is necessary but insufficient for unbiasedly estimating factorial effects; balancing the factors is also essential in multi-factor settings. Moreover, we extend our weighting method to handle missing treatment combinations in observed data. Finally, we study the asymptotic behavior of the new weighting estimators and propose a consistent variance estimator, providing reliable inferences on factorial effects in observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04660v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoqi Yu, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian Multidimensional Item Response Theory</title>
      <link>https://arxiv.org/abs/2310.17820</link>
      <description>arXiv:2310.17820v3 Announce Type: replace 
Abstract: Multivariate Item Response Theory (MIRT) is sought-after widely by applied researchers looking for interpretable (sparse) explanations underlying response patterns in questionnaire data. There is, however, an unmet demand for such sparsity discovery tools in practice. Our paper develops a Bayesian platform for binary and ordinal item MIRT which requires minimal tuning and scales well on large datasets due to its parallelizable features. Bayesian methodology for MIRT models has traditionally relied on MCMC simulation, which cannot only be slow in practice, but also often renders exact sparsity recovery impossible without additional thresholding. In this work, we develop a scalable Bayesian EM algorithm to estimate sparse factor loadings from mixed continuous, binary, and ordinal item responses. We address the seemingly insurmountable problem of unknown latent factor dimensionality with tools from Bayesian nonparametrics which enable estimating the number of factors. Rotations to sparsity through parameter expansion further enhance convergence and interpretability without identifiability constraints. In our simulation study, we show that our method reliably recovers both the factor dimensionality as well as the latent structure on high-dimensional synthetic data even for small samples. We demonstrate the practical usefulness of our approach on three datasets: an educational assessment dataset, a quality-of-life measurement dataset, and a bio-behavioral dataset. All demonstrations show that our tool yields interpretable estimates, facilitating interesting discoveries that might otherwise go unnoticed under a pure confirmatory factor analysis setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17820v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiguang Li, Robert Gibbons, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Cluster-Randomized Trials with Cross-Cluster Interference</title>
      <link>https://arxiv.org/abs/2310.18836</link>
      <description>arXiv:2310.18836v3 Announce Type: replace 
Abstract: The literature on cluster-randomized trials typically assumes no interference across clusters. This may be implausible when units are irregularly distributed in space without well-separated communities, in which case clusters may not represent significant geographic, social, or economic divisions. In this paper, we develop methods for reducing bias due to cross-cluster interference. First, we propose an estimation strategy that excludes units not surrounded by clusters assigned to the same treatment arm. We show that this substantially reduces asymptotic bias relative to conventional difference-in-means estimators without substantial cost to variance. Second, we formally establish a bias-variance trade-off in the choice of clusters: constructing fewer, larger clusters reduces bias due to interference but increases variance. We provide a rule for choosing the number of clusters to balance the asymptotic orders of the bias and variance of our estimator. Finally, we consider unsupervised learning for cluster construction and provide theoretical guarantees for $k$-medoids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18836v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>Nonconvex High-Dimensional Time-Varying Coefficient Estimation for Noisy High-Frequency Observations with a Factor Structure</title>
      <link>https://arxiv.org/abs/2401.02694</link>
      <description>arXiv:2401.02694v2 Announce Type: replace 
Abstract: In this paper, we propose a novel high-dimensional time-varying coefficient estimator for noisy high-frequency observations with a factor structure. In high-frequency finance, we often observe that noises dominate the signal of underlying true processes and that covariates exhibit a factor structure due to their strong dependence. Thus, we cannot apply usual regression procedures to analyze high-frequency observations. To handle the noises, we first employ a smoothing method for the observed dependent and covariate processes. Then, to handle the strong dependence of the covariate processes, we apply Principal Component Analysis (PCA) and transform the highly correlated covariate structure into a weakly correlated structure. However, the variables from PCA still contain non-negligible noises. To manage these non-negligible noises and the high dimensionality, we propose a nonconvex penalized regression method for each local coefficient. This method produces consistent but biased local coefficient estimators. To estimate the integrated coefficients, we propose a debiasing scheme and obtain a debiased integrated coefficient estimator using debiased local coefficient estimators. Then, to further account for the sparsity structure of the coefficients, we apply a thresholding scheme to the debiased integrated coefficient estimator. We call this scheme the Factor Adjusted Thresholded dEbiased Nonconvex LASSO (FATEN-LASSO) estimator. Furthermore, this paper establishes the concentration properties of the FATEN-LASSO estimator and discusses a nonconvex optimization algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02694v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minseok Shin, Donggyu Kim</dc:creator>
    </item>
    <item>
      <title>Fast and reliable confidence intervals for a variance component</title>
      <link>https://arxiv.org/abs/2404.15060</link>
      <description>arXiv:2404.15060v2 Announce Type: replace 
Abstract: We show that confidence intervals in a variance component model, with asymptotically correct uniform coverage probability, can be obtained by inverting certain test-statistics based on the score for the restricted likelihood. The results apply in settings where the variance is near or at the boundary of the parameter set. Simulations indicate the proposed test-statistics are approximately pivotal and lead to confidence intervals with near-nominal coverage even in small samples. We illustrate our methods' application in spatially-resolved transcriptomics where we compute approximately 15,000 confidence intervals, used for gene ranking, in less than 4 minutes. In the settings we consider, the proposed method is between two and 28,000 times faster than popular alternatives, depending on how many confidence intervals are computed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15060v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqiao Zhang, Karl Oskar Ekvall, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>Distribution-free Conformal Prediction for Ordinal Classification</title>
      <link>https://arxiv.org/abs/2404.16610</link>
      <description>arXiv:2404.16610v2 Announce Type: replace 
Abstract: Conformal prediction is a general distribution-free approach for constructing prediction sets combined with any machine learning algorithm that achieve valid marginal or conditional coverage in finite samples. Ordinal classification is common in real applications where the target variable has natural ordering among the class labels. In this paper, we discuss constructing distribution-free prediction sets for such ordinal classification problems by leveraging the ideas of conformal prediction and multiple testing with FWER control. Newer conformal prediction methods are developed for constructing contiguous and non-contiguous prediction sets based on marginal and conditional (class-specific) conformal $p$-values, respectively. Theoretically, we prove that the proposed methods respectively achieve satisfactory levels of marginal and class-specific conditional coverages. Through simulation study and real data analysis, these proposed methods show promising performance compared to the existing conformal method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16610v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In COPA 2024 : 13th Symposium on Conformal and Probabilistic Prediction with Applications, PMLR 230:120-139</arxiv:journal_reference>
      <dc:creator>Subhrasish Chakraborty, Chhavi Tyagi, Haiyan Qiao, Wenge Guo</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data</title>
      <link>https://arxiv.org/abs/2406.06452</link>
      <description>arXiv:2406.06452v2 Announce Type: replace 
Abstract: Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics. Since the treatments of interest often cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to leverage instrumental variables (IVs) as latent quasi-experiments, such as randomized intent-to-treat assignments or randomized product recommendations. This approach, on the other hand, can suffer from low compliance, $\textit{i.e.}$, IV weakness. Some subgroups may even exhibit zero compliance, meaning we cannot instrument for their CATEs at all. In this paper, we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns $\textit{biased}$ CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06452v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miruna Oprescu, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>An Agglomerative Clustering of Simulation Output Distributions Using Regularized Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2407.12100</link>
      <description>arXiv:2407.12100v2 Announce Type: replace 
Abstract: Using statistical learning methods to analyze stochastic simulation outputs can significantly enhance decision-making by uncovering relationships between different simulated systems and between a system's inputs and outputs. We focus on clustering multivariate empirical distributions of simulation outputs to identify patterns and trade-offs among performance measures. We present a novel agglomerative clustering algorithm that utilizes the regularized Wasserstein distance to cluster these multivariate empirical distributions. This framework has several important use cases, including anomaly detection, pre-optimization, and online monitoring. In numerical experiments involving a call-center model, we demonstrate how this methodology can identify staffing plans that yield similar performance outcomes and inform policies for intervening when queue lengths signal potentially worsening system performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12100v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammadmahdi Ghasemloo, David J. Eckman</dc:creator>
    </item>
    <item>
      <title>A new unit-bimodal distribution based on correlated Birnbaum-Saunders random variables</title>
      <link>https://arxiv.org/abs/2408.00100</link>
      <description>arXiv:2408.00100v2 Announce Type: replace 
Abstract: In this paper, we propose a new distribution over the unit interval which can be characterized as a ratio of the type $Z=Y/(X+Y)$ where $X$ and $Y$ are two correlated Birnbaum-Saunders random variables. The density of $Z$ may be unimodal or bimodal. Simple expressions for the cumulative distribution function, moment-generating function and moments are obtained. Moreover, the stress-strength probability between $X$ and $Y$ is calculated explicitly in the symmetric case, that is, when the respective scale parameters are equal. Two applications of the ratio distribution are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00100v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo, Felipe Quintino, Peter Z\"ornig</dc:creator>
    </item>
    <item>
      <title>POI-SIMEX for Conditionally Poisson Distributed Biomarkers from Tissue Histology</title>
      <link>https://arxiv.org/abs/2409.14256</link>
      <description>arXiv:2409.14256v2 Announce Type: replace 
Abstract: Covariate measurement error in regression analysis is an important issue that has been studied extensively under the classical additive and the Berkson error models. Here, we consider cases where covariates are derived from tumor tissue histology, and in particular tissue microarrays. In such settings, biomarkers are evaluated from tissue cores that are subsampled from a larger tissue section so that these biomarkers are only estimates of the true cell densities. The resulting measurement error is non-negligible but is seldom accounted for in the analysis of cancer studies involving tissue microarrays. To adjust for this type of measurement error, we assume that these discrete-valued biomarkers are conditionally Poisson distributed, based on a Poisson process model governing the spatial locations of marker-positive cells. Existing methods for addressing conditional Poisson surrogates, particularly in the absence of internal validation data, are limited. We extend the simulation extrapolation (SIMEX) algorithm to accommodate the conditional Poisson case (POI-SIMEX), where measurement errors are non-Gaussian with heteroscedastic variance. The proposed estimator is shown to be strongly consistent in a linear regression model under the assumption of a conditional Poisson distribution for the observed biomarker. Simulation studies evaluate the performance of POI-SIMEX, comparing it with the naive method and an alternative corrected likelihood approach in linear regression and survival analysis contexts. POI-SIMEX is then applied to a study of high-grade serous cancer, examining the association between survival and the presence of triple-positive biomarker (CD3+CD8+FOXP3+ cells)</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14256v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>A Causal Transformation Model for Time-to-Event Data Affected by Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2410.15968</link>
      <description>arXiv:2410.15968v3 Announce Type: replace 
Abstract: Motivated by studies investigating causal effects in survival analysis, we propose a transformation model to quantify the impact of a binary treatment on a time-to-event outcome. The approach is based on a flexible linear transformation structural model that links a monotone function of the time-to-event with the propensity for treatment through a bivariate Gaussian distribution. The model equations are specified as functions of additive predictors, allowing the impacts of observed confounders to be accounted for flexibly. Furthermore, the effect of the instrumental variable may be regularized through a ridge penalty, while interactions between the treatment and modifier variables can be incorporated into the model to acknowledge potential variations in treatment effects across different subgroups. The baseline survival function is estimated in a flexible manner using monotonic P-splines, while unobserved confounding is captured through the dependence parameter of the bivariate Gaussian. The proposal naturally provides an intuitive causal measure of interest, the survival average treatment effect. Parameter estimation is achieved via a computationally efficient and stable penalized maximum likelihood estimation approach and intervals constructed using the related inferential results. We revisit a dataset from the Illinois Reemployment Bonus Experiment to estimate the causal effect of a cash bonus on unemployment duration, unveiling new insights. The modeling framework is incorporated into the R package GJRM, enabling researchers and practitioners to fit the proposed causal survival model and obtain easy-to-interpret numerical and visual summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15968v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Models for Multiple Raters: a General Statistical Framework</title>
      <link>https://arxiv.org/abs/2410.21498</link>
      <description>arXiv:2410.21498v2 Announce Type: replace 
Abstract: Rating procedure is crucial in many applied fields (e.g., educational, clinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a subject (e.g., student, doctor) on a rating scale. Given raters variability, several statistical methods have been proposed for assessing and improving the quality of ratings. The analysis and the estimate of the Intraclass Correlation Coefficient (ICC) are major concerns in such cases. As evidenced by the literature, ICC might differ across different subgroups of raters and might be affected by contextual factors and subject heterogeneity. Model estimation in the presence of heterogeneity has been one of the recent challenges in this research line. Consequently, several methods have been proposed to address this issue under a parametric multilevel modelling framework, in which strong distributional assumptions are made. We propose a more flexible model under the Bayesian nonparametric (BNP) framework, in which most of those assumptions are relaxed. By eliciting hierarchical discrete nonparametric priors, the model accommodates clusters among raters and subjects, naturally accounts for heterogeneity, and improves estimate accuracy. We propose a general BNP heteroscedastic framework to analyze rating data and possible latent differences among subjects and raters. The estimated densities are used to make inferences about the rating process and the quality of the ratings. By exploiting a stick-breaking representation of the Dirichlet Process a general class of ICC indices might be derived for these models. Theoretical results about the ICC are provided together with computational strategies. Simulations and a real-world application are presented and possible future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21498v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Ioanna Manolopoulou</dc:creator>
    </item>
    <item>
      <title>Decomposing Network Influence: Social Influence Regression</title>
      <link>https://arxiv.org/abs/1706.09072</link>
      <description>arXiv:1706.09072v2 Announce Type: replace-cross 
Abstract: Understanding network influence and its determinants are key challenges in political science and network analysis. Traditional latent variable models position actors within a social space based on network dependencies but often do not elucidate the underlying factors driving these interactions. To overcome this limitation, we propose the Social Influence Regression (SIR) model, an extension of vector autoregression tailored for relational data that incorporates exogenous covariates into the estimation of influence patterns. The SIR model captures influence dynamics via a pair of $n \times n$ matrices that quantify how the actions of one actor affect the future actions of another. This framework not only provides a statistical mechanism for explaining actor influence based on observable traits but also improves computational efficiency through an iterative block coordinate descent method. We showcase the SIR model's capabilities by applying it to monthly conflict events between countries, using data from the Integrated Crisis Early Warning System (ICEWS). Our findings demonstrate the SIR model's ability to elucidate complex influence patterns within networks by linking them to specific covariates. This paper's main contributions are: (1) introducing a model that explains third-order dependencies through exogenous covariates and (2) offering an efficient estimation approach that scales effectively with large, complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:1706.09072v2</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahryar Minhas, Peter D. Hoff</dc:creator>
    </item>
    <item>
      <title>Causal effects based on distributional distances</title>
      <link>https://arxiv.org/abs/1806.02935</link>
      <description>arXiv:1806.02935v3 Announce Type: replace-cross 
Abstract: Comparing counterfactual distributions can provide more nuanced and valuable measures for causal effects, going beyond typical summary statistics such as averages. In this work, we consider characterizing causal effects via distributional distances, focusing on two kinds of target parameters. The first is the counterfactual outcome density. We propose a doubly robust-style estimator for the counterfactual density and study its rates of convergence and limiting distributions. We analyze asymptotic upper bounds on the $L_q$ and the integrated $L_q$ risks of the proposed estimator, and propose a bootstrap-based confidence band. The second is a novel distributional causal effect defined by the $L_1$ distance between different counterfactual distributions. We study three approaches for estimating the proposed distributional effect: smoothing the counterfactual density, smoothing the $L_1$ distance, and imposing a margin condition. For each approach, we analyze asymptotic properties and error bounds of the proposed estimator, and discuss potential advantages and disadvantages. We go on to present a bootstrap approach for obtaining confidence intervals, and propose a test of no distributional effect. We conclude with a numerical illustration and a real-world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:1806.02935v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangho Kim, Jisu Kim, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Two-stage Conformal Risk Control with Application to Ranked Retrieval</title>
      <link>https://arxiv.org/abs/2404.17769</link>
      <description>arXiv:2404.17769v2 Announce Type: replace-cross 
Abstract: Many practical machine learning systems, such as ranking and recommendation systems, consist of two concatenated stages: retrieval and ranking. These systems present significant challenges in accurately assessing and managing the uncertainty inherent in their predictions. To address these challenges, we extend the recently developed framework of conformal risk control, originally designed for single-stage problems, to accommodate the more complex two-stage setup. We first demonstrate that a straightforward application of conformal risk control, treating each stage independently, may fail to maintain risk at their pre-specified levels. Therefore, we propose an integrated approach that considers both stages simultaneously, devising algorithms to control the risk of each stage by jointly identifying thresholds for both stages. Our algorithm further optimizes for a weighted combination of prediction set sizes across all feasible thresholds, resulting in more effective prediction sets. Finally, we apply the proposed method to the critical task of two-stage ranked retrieval. We validate the efficacy of our method through extensive experiments on two large-scale public datasets, MSLR-WEB and MS MARCO, commonly used for ranked retrieval tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17769v2</guid>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunpeng Xu, Mufang Ying, Wenge Guo, Zhi Wei</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v2 Announce Type: replace-cross 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined. In applications, however, it's often the case that sample sizes aren't predetermined; instead, they're often data-dependent. Since those methods designed for static sample sizes aren't reliable when sample sizes are dynamic, there's been recent interest in e-processes and corresponding tests and confidence sets that are anytime valid in the sense that their justification holds up for arbitrary dynamic data-collection plans. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain, but existing approaches can't accommodate this. The present paper offer a new, regularized e-process framework that features a knowledge-based, imprecise-probabilistic regularization with improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process remains anytime valid in a novel, knowledge-dependent sense. In addition, the proposed regularized e-processes facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other desirable Bayesian-like features: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression</title>
      <link>https://arxiv.org/abs/2410.02629</link>
      <description>arXiv:2410.02629v2 Announce Type: replace-cross 
Abstract: This paper studies the generalization performance of iterates obtained by Gradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal variants in high-dimensional robust regression problems. The number of features is comparable to the sample size and errors may be heavy-tailed. We introduce estimators that precisely track the generalization error of the iterates along the trajectory of the iterative algorithm. These estimators are provably consistent under suitable conditions. The results are illustrated through several examples, including Huber regression, pseudo-Huber regression, and their penalized variants with non-smooth regularizer. We provide explicit generalization error estimates for iterates generated from GD and SGD, or from proximal SGD in the presence of a non-smooth regularizer. The proposed risk estimates serve as effective proxies for the actual generalization error, allowing us to determine the optimal stopping iteration that minimizes the generalization error. Extensive simulations confirm the effectiveness of the proposed generalization error estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02629v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Tan, Pierre C. Bellec</dc:creator>
    </item>
  </channel>
</rss>
