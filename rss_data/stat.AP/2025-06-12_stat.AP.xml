<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness</title>
      <link>https://arxiv.org/abs/2506.09208</link>
      <description>arXiv:2506.09208v1 Announce Type: new 
Abstract: Objectives: We propose a novel imputation method tailored for Electronic Health Records (EHRs) with structured and sporadic missingness. Such missingness frequently arises in the integration of heterogeneous EHR datasets for downstream clinical applications. By addressing these gaps, our method provides a practical solution for integrated analysis, enhancing data utility and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic missing mechanisms in the integrated analysis of EHR data. Following this, we introduce a novel imputation framework, Macomss, specifically designed to handle structurally and heterogeneously occurring missing data. We establish theoretical guarantees for Macomss, ensuring its robustness in preserving the integrity and reliability of integrated analyses. To assess its empirical performance, we conduct extensive simulation studies that replicate the complex missingness patterns observed in real-world EHR systems, complemented by validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms existing imputation methods. Using datasets from three hospitals within DUHS, Macomss achieves the lowest imputation errors for missing data in most cases and provides superior or comparable downstream prediction performance compared to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful method for imputing structured and sporadic missing data, enabling accurate and reliable integrated analysis across multiple EHR datasets. The proposed approach holds significant potential for advancing research in population health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09208v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Yan Zhang, Chuan Hong, T. Tony Cai, Tianxi Cai, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>A Bayesian analysis of home advantage in professional squash</title>
      <link>https://arxiv.org/abs/2506.09287</link>
      <description>arXiv:2506.09287v1 Announce Type: new 
Abstract: We estimate the effect of playing in one's home country in professional squash using a Bayesian hierarchical model applied to men's and women's Professional Squash Association matches from 2018-2024. The model incorporates players' world rankings and whether they are competing in their home country. Using margin of victory in games as our outcome, we estimate that home advantage adds 0.4 games for men and 0.3 games for women to the expected margin, with standard errors of 0.1. For evenly matched players, this effect corresponds to an increase in win probability from 50% to roughly 58% for men and 56% for women. We estimate particularly strong home effects in Egypt, where many major tournaments are held, though data limitations prevent precise estimation of country-specific effects in many other nations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09287v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard, Samer Takriti</dc:creator>
    </item>
    <item>
      <title>Disentangling Spatial and Structural Drivers of Housing Prices through Bayesian Networks: A Case Study of Madrid, Barcelona, and Valencia</title>
      <link>https://arxiv.org/abs/2506.09539</link>
      <description>arXiv:2506.09539v1 Announce Type: new 
Abstract: Understanding how housing prices respond to spatial accessibility, structural attributes, and typological distinctions is central to contemporary urban research and policy. In cities marked by affordability stress and market segmentation, models that offer both predictive capability and interpretive clarity are increasingly needed. This study applies discrete Bayesian networks to model residential price formation across Madrid, Barcelona, and Valencia using over 180,000 geo-referenced housing listings. The resulting probabilistic structures reveal distinct city-specific logics. Madrid exhibits amenity-driven stratification, Barcelona emphasizes typology and classification, while Valencia is shaped by spatial and structural fundamentals. By enabling joint inference, scenario simulation, and sensitivity analysis within a transparent framework, the approach advances housing analytics toward models that are not only accurate but actionable, interpretable, and aligned with the demands of equitable urban governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09539v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alvaro Garcia Murga, Manuele Leonelli</dc:creator>
    </item>
    <item>
      <title>Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule</title>
      <link>https://arxiv.org/abs/2506.09217</link>
      <description>arXiv:2506.09217v1 Announce Type: cross 
Abstract: The performance of perception systems in autonomous driving systems (ADS) is strongly influenced by object distance, scene dynamics, and environmental conditions such as weather. AI-based perception outputs are inherently stochastic, with variability driven by these external factors, while traditional evaluation metrics remain static and event-independent, failing to capture fluctuations in confidence over time. In this work, we introduce the Perception Characteristics Distance (PCD) -- a novel evaluation metric that quantifies the farthest distance at which an object can be reliably detected, incorporating uncertainty in model outputs. To support this, we present the SensorRainFall dataset, collected on the Virginia Smart Road using a sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear and daylight-rain scenarios, with precise ground-truth distances to the target objects. Statistical analysis reveals the presence of change points in the variance of detection confidence score with distance. By averaging the PCD values across a range of detection quality thresholds and probabilistic thresholds, we compute the mean PCD (mPCD), which captures the overall perception characteristics of a system with respect to detection distance. Applying state-of-the-art perception models shows that mPCD captures meaningful reliability differences under varying weather conditions -- differences that static metrics overlook. PCD provides a principled, distribution-aware measure of perception performance, supporting safer and more robust ADS operation, while the SensorRainFall dataset offers a valuable benchmark for evaluation. The SensorRainFall dataset is publicly available at https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the evaluation code is open-sourced at https://github.com/datadrivenwheels/PCD_Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09217v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boyu Jiang, Liang Shi, Zhengzhi Lin, Loren Stowe, Feng Guo</dc:creator>
    </item>
    <item>
      <title>Machine learning-based correlation analysis of decadal cyclone intensity with sea surface temperature: data and tutorial</title>
      <link>https://arxiv.org/abs/2506.09254</link>
      <description>arXiv:2506.09254v1 Announce Type: cross 
Abstract: The rising number of extreme climate events in the past decades has motivated the need for a thorough consideration of tropical cyclone genesis and intensity, given the sea-surface temperature (SST). In this paper, we present an analysis of the relationship between the increasing global SST with cyclone genesis using linear regression machine learning models. We extract and curate a dataset of tropical cyclones across selected ocean basins with their associated SST over the past 40 years. We provide correlation analysis using linear regression and visualisation strategies. Our preliminary results show a strong positive correlation between SST and high wind speed across selected ocean basins via linear regression and machine learning models. Our dataset and available open-source code offer a novel perspective for the investigation of the genesis and intensity of tropical cyclones. Alongside the time and position of each cyclone, we also provide the related Saffir-Simpson category, season, wind speed, and SST for 15 days before and after the tropical cyclone genesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09254v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Wu, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>Causal Graph Recovery in Neuroimaging through Answer Set Programming</title>
      <link>https://arxiv.org/abs/2506.09286</link>
      <description>arXiv:2506.09286v1 Announce Type: cross 
Abstract: Learning graphical causal structures from time series data presents significant challenges, especially when the measurement frequency does not match the causal timescale of the system. This often leads to a set of equally possible underlying causal graphs due to information loss from sub-sampling (i.e., not observing all possible states of the system throughout time). Our research addresses this challenge by incorporating the effects of sub-sampling in the derivation of causal graphs, resulting in more accurate and intuitive outcomes. We use a constraint optimization approach, specifically answer set programming (ASP), to find the optimal set of answers. ASP not only identifies the most probable underlying graph, but also provides an equivalence class of possible graphs for expert selection. In addition, using ASP allows us to leverage graph theory to further prune the set of possible solutions, yielding a smaller, more accurate answer set significantly faster than traditional approaches. We validate our approach on both simulated data and empirical structural brain connectivity, and demonstrate its superiority over established methods in these experiments. We further show how our method can be used as a meta-approach on top of established methods to obtain, on average, 12% improvement in F1 score. In addition, we achieved state of the art results in terms of precision and recall of reconstructing causal graph from sub-sampled time series data. Finally, our method shows robustness to varying degrees of sub-sampling on realistic simulations, whereas other methods perform worse for higher rates of sub-sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09286v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadsajad Abavisani, Kseniya Solovyeva, David Danks, Vince Calhoun, Sergey Plis</dc:creator>
    </item>
    <item>
      <title>Simulation-trained conditional normalizing flows for likelihood approximation: a case study in stress regulation kinetics in yeast</title>
      <link>https://arxiv.org/abs/2506.09374</link>
      <description>arXiv:2506.09374v1 Announce Type: cross 
Abstract: Physics-inspired inference often hinges on the ability to construct a likelihood, or the probability of observing a sequence of data given a model. These likelihoods can be directly maximized for parameter estimation, incorporated into Bayesian frameworks, or even used as loss functions in neural networks. Yet, many models, despite being conceptually simple, lack tractable likelihoods. A notable example arises in estimating protein production from snapshot measurements of actively dividing cells. Here, the challenge stems from cell divisions occurring at non-Exponentially distributed intervals with each division stochastically partitioning protein content between daughter cells, making protein counts in any given cell a function of its full division history. Such history dependence precludes a straightforward likelihood based on a (standard Markovian) master equation. Instead, we employ conditional normalizing flows (a class of neural network models designed to learn probability distributions) to approximate otherwise intractable likelihoods from simulated data. As a case study, we examine activation of the \emph{glc3} gene in yeast involved in glycogen synthesis and expressed under nutrient-limiting conditions. We monitor this activity using snapshot fluorescence measurements via flow cytometry, where GFP expression reflects \emph{glc3} promoter activity. A na\"ive analysis of flow cytometry data ignoring cell division suggests many cells are active with low expression. However, fluorescent proteins persist and can be inherited, so cells may appear active from retaining ancestral fluorescence. Explicitly accounting for the (non-Markovian) effects of cell division reveals \emph{glc3} is mostly inactive under stress, showing that while cells occasionally activate it, expression is brief and transient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09374v1</guid>
      <category>q-bio.QM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pedro Pessoa, Juan Andres Martinez, Vincent Vandenbroucke, Frank Delvigne, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>Causal effects on non-terminal event time with application to antibiotic usage and future resistance</title>
      <link>https://arxiv.org/abs/2506.09624</link>
      <description>arXiv:2506.09624v1 Announce Type: cross 
Abstract: Comparing future antibiotic resistance levels resulting from different antibiotic treatments is challenging because some patients may survive only under one of the antibiotic treatments. We embed this problem within a semi-competing risks approach to study the causal effect on resistant infection, treated as a non-terminal event time. We argue that existing principal stratification estimands for such problems exclude patients for whom a causal effect is well-defined and is of clinical interest. Therefore, we present a new principal stratum, the infected-or-survivors (ios). The ios is the subpopulation of patients who would have survived or been infected under both antibiotic treatments. This subpopulation is more inclusive than previously defined subpopulations. We target the causal effect among these patients, which we term the feasible-infection causal effect (FICE). We develop large-sample bounds under novel assumptions, and discuss the plausibility of these assumptions in our application. As an alternative, we derive FICE identification using two illness-death models with a bivariate frailty random variable. These two models are connected by a cross-world correlation parameter. Estimation is performed by an expectation-maximization algorithm followed by a Monte Carlo procedure. We apply our methods to detailed clinical data obtained from a hospital setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09624v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamir Zehavi, Uri Obolski, Michal Chowers, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>A Comparison between Markov Switching Zero-inflated and Hurdle Models for Spatio-temporal Infectious Disease Counts</title>
      <link>https://arxiv.org/abs/2309.04594</link>
      <description>arXiv:2309.04594v2 Announce Type: replace 
Abstract: In epidemiological studies, zero-inflated and hurdle models are commonly used to handle excess zeros in reported infectious disease cases. However, they can not model the persistence (changing from presence to presence) and reemergence (changing from absence to presence) of a disease separately. Covariates can sometimes have different effects on the reemergence and persistence of a disease. Recently, a zero-inflated Markov switching negative binomial model was proposed to accommodate this issue. We introduce a Markov switching negative binomial hurdle model as a competitor of that approach, as hurdle models are often also used as alternatives to zero-inflated models for accommodating excess zeroes. We begin the comparison by inspecting the underlying assumptions made by both models. Hurdle models assume perfect detection of the disease cases while zero-inflated models implicitly assume the case counts can be under-reported, thus we investigate when a negative binomial distribution can approximate the true distribution of reported counts. A comparison of the fit of the two types of Markov switching models is undertaken on chikungunya cases across the neighborhoods of Rio de Janeiro. We find that, among the fitted models, the Markov switching negative binomial zero-inflated model produces the best predictions and both Markov switching models produce remarkably better predictions than more traditional negative binomial hurdle and zero-inflated models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04594v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.70135</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine, 44: e70135 (2025)</arxiv:journal_reference>
      <dc:creator>Mingchi Xu, Dirk Douwes-Schultz, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Reliability modeling and statistical analysis of accelerated degradation process with memory effects and unit-to-unit variability</title>
      <link>https://arxiv.org/abs/2310.18567</link>
      <description>arXiv:2310.18567v5 Announce Type: replace 
Abstract: A reasonable description of the degradation process is essential for credible reliability assessment in accelerated degradation testing. Existing methods usually use Markovian stochastic processes to describe the degradation process. However, degradation processes of some products are non-Markovian due to the interaction with environments. Misinterpretation of the degradation pattern may lead to biased reliability evaluations. Besides, owing to the differences in materials and manufacturing processes, products from the same population exhibit diverse degradation paths, further increasing the difficulty of accurate reliability estimation. To address the above issues, this paper proposes an accelerated degradation model incorporating memory effects and unit-to-unit variability. The memory effect in the degradation process is captured by the fractional Brownian motion, which reflects the non-Markovian characteristic of degradation. The unit-to-unit variability is considered in the acceleration model to describe diverse degradation paths. Then, lifetime and reliability under normal operating conditions are presented. Furthermore, to give an accurate estimation of the memory effect, a new statistical analysis method based on the expectation maximization algorithm is devised. The effectiveness of the proposed method is verified by a simulation case and a real-world tuner reliability analysis case. The code for the simulation case is publicly available at https://github.com/dirge1/FBM_ADT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18567v5</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.apm.2024.115788</arxiv:DOI>
      <arxiv:journal_reference>Applied Mathematical Modelling, 2024: 115788</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li, Wenrui Xie</dc:creator>
    </item>
    <item>
      <title>Comparison of global sensitivity analysis methods for a fire spread model with a segmented characteristic</title>
      <link>https://arxiv.org/abs/2407.17718</link>
      <description>arXiv:2407.17718v3 Announce Type: replace 
Abstract: Global sensitivity analysis (GSA) can provide rich information for controlling output uncertainty. In practical applications, segmented models are commonly used to describe an abrupt model change. For segmented models, the complicated uncertainty propagation during the transition region may lead to different importance rankings of different GSA methods. If an unsuitable GSA method is applied, misleading results will be obtained, resulting in suboptimal or even wrong decisions. In this paper, four GSA indices, i.e., Sobol index, mutual information, delta index and PAWN index, are applied for a segmented fire spread model (Dry Eucalypt). The results show that four GSA indices give different importance rankings during the transition region since segmented characteristics affect different GSA indices in different ways. We suggest that analysts should rely on the results of different GSA indices according to their practical purpose, especially when making decisions for segmented models during the transition region. All of our source codes are publicly available at https://github.com/dirge1/GSA_segmented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17718v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2024.10.012</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, 2024: (229)304-318</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li</dc:creator>
    </item>
    <item>
      <title>A new moment-independent uncertainty importance measure based on cumulative residual entropy for developing uncertainty reduction strategies</title>
      <link>https://arxiv.org/abs/2407.17719</link>
      <description>arXiv:2407.17719v2 Announce Type: replace 
Abstract: Uncertainty reduction is vital for improving system reliability and reducing risks. To identify the best target for uncertainty reduction, uncertainty importance measure is commonly used to prioritize the significance of input variable uncertainties. Then, designers will take steps to reduce the uncertainties of variables with high importance. However, for variables with minimal uncertainty, the cost of controlling their uncertainties can be unacceptable. Therefore, uncertainty magnitude should also be considered in developing uncertainty reduction strategies. Although variance-based methods have been developed for this purpose, they are dependent on statistical moments and have limitations when dealing with highly-skewed distributions that are commonly encountered in practical applications. Motivated by this problem, we propose a new uncertainty importance measure based on cumulative residual entropy. The proposed measure is moment-independent based on the cumulative distribution function, which can handle the highly-skewed distributions properly. Numerical implementations for estimating the proposed measure are devised and verified. A real-world engineering case considering highly-skewed distributions is introduced to show the procedure of developing uncertainty reduction strategies considering uncertainty magnitude and corresponding cost. The results demonstrate that the proposed measure can present a different uncertainty reduction recommendation compared to the variance-based approach because of its moment-independent characteristic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17719v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2025.06.004</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation 2025</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li</dc:creator>
    </item>
    <item>
      <title>Practical Implementation of an End-to-End Methodology for SPC of 3-D Part Geometry: A Case Study</title>
      <link>https://arxiv.org/abs/2504.08243</link>
      <description>arXiv:2504.08243v2 Announce Type: replace 
Abstract: Del Castillo and Zhao (2020, 2021, 2022, 2024) have recently proposed a new methodology for the Statistical Process Control (SPC) of discrete parts whose 3-dimensional (3D) geometrical data are acquired with non-contact sensors. The approach is based on monitoring the spectrum of the Laplace-Beltrami (LB) operator of each scanned part estimated using finite element methods (FEM). The spectrum of the LB operator is an intrinsic summary of the geometry of a part, independent of the ambient space. Hence, registration of scanned parts is unnecessary when comparing them. The primary goal of this case study paper is to demonstrate the practical implementation of the spectral SPC methodology through multiple examples using real scanned parts acquired with an industrial-grade laser scanner, including 3D printed parts and commercial parts. We discuss the scanned mesh preprocessing needed in practice, including the type of remeshing found to be most beneficial for the FEM computations. For each part type, both the "phase I" and "phase II" stages of the spectral SPC methodology are showcased. In addition, we provide a new principled method to determine the number of eigenvalues of the LB operator to consider for efficient SPC of a given part geometry, and present an improved algorithm to automatically define a region of interest, particularly useful for large meshes. Computer codes that implement every method discussed in this paper, as well as all scanned part datasets used in the case studies, are made available and explained in the supplementary materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08243v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin An, Xueqi Zhao, Enrique del Castillo</dc:creator>
    </item>
    <item>
      <title>Estimating Velocity Vector Fields of Atmospheric Winds using Transport Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.10898</link>
      <description>arXiv:2505.10898v2 Announce Type: replace 
Abstract: Accurately estimating latent velocity vector fields of atmospheric winds is crucial for understanding weather phenomena. Direct measurement of atmospheric winds is costly, especially in the upper atmosphere, so researchers attempt to estimate atmospheric winds by observing the movement patterns of clouds and other features in satellite images of the atmosphere. These Derived Motion Winds use feature tracking algorithms to search for movement within small windows in space and time. Consequently, these algorithms cannot leverage information from broader-scale features and cannot ensure that the collection of wind vectors over space and time represents a physically realistic velocity field. In this work, we use spatial-temporal Gaussian processes to model the evolution of a scalar quantity transported over time by fluid flow. Our framework simultaneously estimates covariance parameters and latent velocities by maximizing the likelihood. Specifically, flows are represented using time-dependent residual neural networks, and velocities are subsequently derived through closed-form formulas. Performance evaluations using weather model data demonstrate our method's accuracy and efficiency. We apply our method to GOES-16 images, demonstrating computational efficiency and the ability to produce wind estimates where Derived Motion Winds fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10898v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Fahmy, Maria Laura Battagliola, Joseph Guinness</dc:creator>
    </item>
    <item>
      <title>Yau-YauAL: A computer tool for solving nonlinear filtering problems</title>
      <link>https://arxiv.org/abs/2506.08976</link>
      <description>arXiv:2506.08976v2 Announce Type: replace 
Abstract: The Yau-Yau nonlinear filter has increasingly emerged as a powerful tool to study stochastic complex systems. To leverage it to a wider spectrum of application scenarios, we pack the Yau-Yau filtering ALgorithms (YauYauAL) into a package of computer software. Yau-YauAL was written in R, designed to simplify the implementation of the Yau-Yau filter for solving nonlinear filtering problems. Combining R's accessibility with C++ (via Rcpp) for computational efficiency, YauYauAL provides an intuitive Shiny-based interface that enables real-time parameter adjustment and result visualization. At its core, YauYauAL employs finite difference methods to numerically solve the Kolmogorov forward equation, ensuring a stable and accurate solution even for complex systems. YauYauAL's modular design and open-source framework further encourage customization and community-driven development. YauYauAL aims to bridge the gap between theoretical nonlinear filtering methods and practical applications, without requiring expertise in differential equation solving or programming, fostering its broader impact on various scientific fields, such as signal processing, finance, medicine, and biology among a long list.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08976v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Wang, Shuyuan Xu, Xueda Wei, Xinrui Luo, Stephen Shing-Toung Yau, Shing-Tung Yau, Rongling Wu</dc:creator>
    </item>
    <item>
      <title>Adaptive Projected Two-Sample Comparisons for Single-Cell Gene Expression Data</title>
      <link>https://arxiv.org/abs/2403.05679</link>
      <description>arXiv:2403.05679v2 Announce Type: replace-cross 
Abstract: We study high-dimensional two-sample mean comparison and address the curse of dimensionality through data-adaptive projections. Leveraging the low-dimensional and localized signal structures commonly seen in single-cell genomics data, our first proposed method identifies a sparse, informative low-dimensional subspace and then performs statistical inference restricted to this subspace. To address the double-dipping issue -- arising from using the same data for projection and inference -- we develop a debiased projected estimator using the semiparametric double-machine learning framework. The resulting inference not only has the usual frequentist validity but also provides useful information on the potential location of the signal due to the sparsity of the projection. Our second method uses a more flexible projection scheme to improve the power against the global null hypothesis and avoid the degeneracy issue commonly faced by existing methods. It is particularly useful when debiasing is practically challenging or when the informative signal is not well-captured by the subspace. Experiments on synthetic data and real datasets demonstrate the theoretical promise and interpretability of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05679v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Integrated population model reveals human and environment driven changes in Baltic ringed seal (Pusa hispida botnica) demography and behavior</title>
      <link>https://arxiv.org/abs/2408.08069</link>
      <description>arXiv:2408.08069v2 Announce Type: replace-cross 
Abstract: Integrated population models (IPMs) are a promising approach to test ecological theories and assess wildlife populations in dynamic and uncertain conditions. By combining multiple data sources into a unified model, they enable the parametrization of versatile, mechanistic models that can predict population dynamics in novel circumstances. Here, we present a Bayesian IPM for the ringed seal (Pusa hispida botnica) population inhabiting the Bothnian Bay in the Baltic Sea. Despite the availability of long-term monitoring data, traditional assessment methods have faltered due to dynamic environmental conditions, varying reproductive rates, and the recently re-introduced hunting, thus limiting the quality of information available to managers. We fit our model to census and various demographic, reproductive, and harvest data from 1988 to 2023 to provide a comprehensive assessment of past population trends, and predict population response to alternative hunting scenarios. We estimated that 20,000 to 36,000 ringed seals inhabited the Bothnian Bay in 2024, increasing at a rate of 3% to 6% per year. Reproductive rates have increased since 1988, leading to a substantial increase in the growth rate up until 2015. However, the re-introduction of hunting has since reduced the growth rate, and even minor quota increases are likely to reduce it further. Our results also support the hypothesis that a greater proportion of the population hauls out under lower ice cover circumstances, leading to higher aerial survey results in such years. In general, our study demonstrates the value of IPMs for monitoring wildlife populations under changing environments, and supporting science-based management decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08069v2</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Ersalman, Mervi Kunnasranta, Markus Ahola, Anja M. Carlsson, Sara Persson, Britt-Marie B\"acklin, Inari Helle, Linnea Cervin, Jarno Vanhatalo</dc:creator>
    </item>
    <item>
      <title>The Expected Peak-to-Average Power Ratio of White Gaussian Noise in Sampled I/Q Data</title>
      <link>https://arxiv.org/abs/2501.11261</link>
      <description>arXiv:2501.11261v3 Announce Type: replace-cross 
Abstract: One of the fundamental endeavors in radio frequency (RF) metrology is to measure the power of signals, where a common aim is to estimate the peak-to-average power ratio (PAPR), which quantifies the ratio of the maximum (peak) to the mean value. For a finite number of discrete-time samples of baseband in-phase and quadrature (I/Q) white Gaussian noise (WGN) that are independent and identically distributed with zero mean, we derive a closed-form, exact formula for mean PAPR that is well-approximated by the natural logarithm of the number of samples plus Euler's constant. Additionally, we give related theoretical results for the mean crest factor (CF). After comparing our main result to previously published approximate formulas, we examine how violations of the WGN assumptions in sampled I/Q data result in deviations from the expected value of PAPR. Finally, utilizing a measured RF I/Q acquisition, we illustrate how our formula for mean PAPR can be applied to spectral analysis with spectrograms to verify when measured RF emissions are WGN in a given frequency band.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11261v3</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIM.2025.3544288</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Instrumentation and Measurement, Vol. 74, 2025</arxiv:journal_reference>
      <dc:creator>Adam Wunderlich, Aric Sanders</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
      <link>https://arxiv.org/abs/2505.17836</link>
      <description>arXiv:2505.17836v5 Announce Type: replace-cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}((\log t)/\sqrt{t})$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17836v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Van Elst, Igor Colin, Stephan Cl\'emen\c{c}on</dc:creator>
    </item>
  </channel>
</rss>
