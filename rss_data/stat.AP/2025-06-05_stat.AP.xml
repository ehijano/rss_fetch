<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 01:42:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian hierarchical model for methane emission source apportionment</title>
      <link>https://arxiv.org/abs/2506.03395</link>
      <description>arXiv:2506.03395v1 Announce Type: new 
Abstract: Reducing methane emissions from the oil and gas sector is a key component of short-term climate action. Emission reduction efforts are often conducted at the individual site-level, where being able to apportion emissions between a finite number of potentially emitting equipment is necessary for leak detection and repair as well as regulatory reporting of annualized emissions. We present a hierarchical Bayesian model, referred to as the multisource detection, localization, and quantification (MDLQ) model, for performing source apportionment on oil and gas sites using methane measurements from point sensor networks. The MDLQ model accounts for autocorrelation in the sensor data and enforces sparsity in the emission rate estimates via a spike-and-slab prior, as oil and gas equipment often emit intermittently. We use the MDLQ model to apportion methane emissions on an experimental oil and gas site designed to release methane in known quantities, providing a means of model evaluation. Data from this experiment are unique in their size (i.e., the number of controlled releases) and in their close approximation of emission characteristics on real oil and gas sites. As such, this study provides a baseline level of apportionment accuracy that can be expected when using point sensor networks on operational sites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03395v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William S. Daniels, Douglas W. Nychka, Dorit M. Hammerling</dc:creator>
    </item>
    <item>
      <title>Internal replication as a tool for evaluating reproducibility in preclinical experiments</title>
      <link>https://arxiv.org/abs/2506.03468</link>
      <description>arXiv:2506.03468v1 Announce Type: new 
Abstract: Reproducibility is central to the credibility of scientific findings, yet complete replication studies are costly and infrequent. However, many biological experiments contain internal replication, which is defined as repetition across batches, runs, days, litters, or sites that can be used to estimate reproducibility without requiring additional experiments. This internal replication is analogous to internal validation in prediction or machine learning models, but is often treated as a nuisance and removed by normalisation, missing an opportunity to assess the stability of results. Here, six types of internal replication are defined based on independence and timing. Using mice data from an experiment conducted at three independent sites, we demonstrate how to quantify and test for internal reproducibility. This approach provides a framework for quantifying reproducibility from existing data and reporting more robust statistical inferences in preclinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03468v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
    <item>
      <title>Analyzing Pension Fund Mortality with Gaussian Processes in a Sub Population Framework</title>
      <link>https://arxiv.org/abs/2506.03584</link>
      <description>arXiv:2506.03584v1 Announce Type: new 
Abstract: Pension fund populations often have mortality experiences that are substantially different from the national benchmark. In a motivating case study of Brazilian corporate pension funds, pensioners are observed to have mortality that is 40-55% below the national average, due to the underlying socioeconomic disparities. Direct analysis of a pension fund population is challenging due to very sparse data, with age-specific annual death counts often in low single digits. We design and study a collection of stochastic sub-population frameworks that coherently capture and project pensioner mortality rates via deflator factors relative to a reference population. Superseding parametric approaches, we propose Gaussian process (GP) based models that flexibly estimate Age- and/or Year-specific deflators. We demonstrate that the GP models achieve better goodness of fit and uncertainty quantification. Our models are illustrated on two Brazilian pension funds in the context of exogenous national and insurance industry mortality tables. The GP models are implemented in R Stan using a fully Bayesian approach and take into account over-dispersion relative to the Poisson likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03584v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo F. L. de Melo, Michael Ludkovski, Rodrigo S. Targino</dc:creator>
    </item>
    <item>
      <title>Probabilistic measures afford fair comparisons of AIWP and NWP model output</title>
      <link>https://arxiv.org/abs/2506.03744</link>
      <description>arXiv:2506.03744v1 Announce Type: new 
Abstract: We introduce a new measure for fair and meaningful comparisons of single-valued output from artificial intelligence based weather prediction (AIWP) and numerical weather prediction (NWP) models, called potential continuous ranked probability score (PC). In a nutshell, we subject the deterministic backbone of physics-based and data-driven models post hoc to the same statistical postprocessing technique, namely, isotonic distributional regression (IDR). Then we find PC as the mean continuous ranked probability score (CRPS) of the postprocessed probabilistic forecasts. The nonnegative PC measure quantifies potential predictive performance and is invariant under strictly increasing transformations of the model output. PC attains its most desirable value of zero if, and only if, the weather outcome Y is a fixed, non-decreasing function of the model output X. The PC measure is recorded in the unit of the outcome, has an upper bound of one half times the mean absolute difference between outcomes, and serves as a proxy for the mean CRPS of real-time, operational probabilistic products. When applied to WeatherBench 2 data, our approach demonstrates that the data-driven GraphCast model outperforms the leading, physics-based European Centre for Medium Range Weather Forecasts (ECMWF) high-resolution (HRES) model. Furthermore, the PC measure for the HRES model aligns exceptionally well with the mean CRPS of the operational ECMWF ensemble. Across application domains, our approach affords comparisons of single-valued forecasts in settings where the pre-specification of a loss function -- which is the usual, and principally superior, procedure in forecast contests, administrative, and benchmarks settings -- places competitors on unequal footings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03744v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tilmann Gneiting, Tobias Biegert, Kristof Kraus, Eva-Maria Walz, Alexander I. Jordan, Sebastian Lerch</dc:creator>
    </item>
    <item>
      <title>Evaluation of clinical utility in emulated clinical trials</title>
      <link>https://arxiv.org/abs/2506.03991</link>
      <description>arXiv:2506.03991v1 Announce Type: new 
Abstract: Dynamic treatment regimes have been proposed to personalize treatment decisions by utilizing historical patient data, but optimization can only be done over information available in the database. In contrast, the standard of care or physicians' decisions may be complex algorithms based on information that is not available to researchers. It is thus meaningful to integrate the standard of care into the evaluation of treatment strategies, and previous works have suggested doing so through the concept of clinical utility. Here we will focus on the comparative component of clinical utility as the average outcome had the full population received treatment based on the proposed dynamic treatment regime in comparison to the full population receiving the "standard" treatment assignment mechanism, such as a physician's choice. Clinical trials to evaluate clinical utility are rarely conducted, and thus, previous works have proposed an emulated clinical trial framework using observational data. However, only one simple estimator was previously suggested, and the practical details of how one would conduct this emulated trial were not detailed. Here, we illuminate these details and propose several estimators of clinical utility based on estimators proposed in the dynamic treatment regime literature. We illustrate the considerations and the estimators in a real data example investigating treatment rules for rheumatoid arthritis, where we highlight that in addition to the standard of care, the current medical guidelines should also be compared to any "optimal" decision rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03991v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Hruza, Arvid Sj\"olander, Erin Gabriel, Samir Bhatt, Michael Sachs</dc:creator>
    </item>
    <item>
      <title>Spatial Association Between Near-Misses and Accident Blackspots in Sydney, Australia: A Getis-Ord $G_i^*$ Analysis</title>
      <link>https://arxiv.org/abs/2506.03356</link>
      <description>arXiv:2506.03356v1 Announce Type: cross 
Abstract: Road safety management teams utilize on historical accident logs to identify blackspots, which are inherently rare and sparse in space and time. Near-miss events captured through vehicle telematics and transmitted in real-time by connected vehicles reveal a unique potential of prevention due to their high frequency nature and driving engagement on the road. There is currently a lack of understanding of the high potential of near-miss data in real-time to proactively detect potential risky driving areas, in advance of a fatal collision. This paper aims to spatially identify clusters of reported accidents (A) versus high-severity near-misses (High-G) within an urban environment (Sydney, Australia) and showcase how the presence of near-misses can significantly lead to future crashes in identified risky hotspots. First, by utilizing a 400m grid framework, we identify significant crash hotspots using the Getis-Ord $G_i^*$ statistical approach. Second, we employ a Bivariate Local Moran's I (LISA) approach to assess and map the spatial concordance and discordance between official crash counts (A) and High-G counts from nearmiss data (High-G). Third, we classify areas based on their joint spatial patterns into: a) High-High (HH) as the most riskiest areas in both historical logs and nearmiss events, High-Low (HL) for high crash logs but low nearmiss records, c) Low-High (LH) for low past crash records but high nearmiss events, and d) Low-Low (LL) for safe areas. Finally, we run a feature importance ranking on all area patterns by using a contextual Point of Interest (POI) count features and we showcase which factors are the most critical to the occurrence of crash blackspots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03356v1</guid>
      <category>eess.SY</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Grigorev, David Lillo-Trynes, Adriana-Simona Mihaita</dc:creator>
    </item>
    <item>
      <title>Introducing multiverse analysis to bibliometrics: The case of team size effects on disruptive research</title>
      <link>https://arxiv.org/abs/2506.03726</link>
      <description>arXiv:2506.03726v1 Announce Type: cross 
Abstract: Although bibliometrics has become an essential tool in the evaluation of research performance, bibliometric analyses are sensitive to a range of methodological choices. Subtle choices in data selection, indicator construction, and modeling decisions can substantially alter results. Ensuring robustness, meaning that findings hold up under different reasonable scenarios, is therefore critical for credible research and research evaluation. To address this issue, this study introduces multiverse analysis to bibliometrics. Multiverse analysis is a statistical tool that enables analysts to transparently discuss modeling assumptions and thoroughly assess model robustness. Whereas standard robustness checks usually cover only a small subset of all plausible models, multiverse analysis includes all plausible models. We illustrate the benefits of multiverse analysis by testing the hypothesis posed by Wu et al. (2019) that small teams produce more disruptive research than large teams. While we found robust evidence of a negative effect of team size on disruption scores, the effect size is so small that its practical relevance seems questionable. Our findings underscore the importance of assessing the multiverse robustness of bibliometric results to clarify their practical implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03726v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Leibel, Lutz Bornmann</dc:creator>
    </item>
    <item>
      <title>Adaptive tuning of Hamiltonian Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2506.04082</link>
      <description>arXiv:2506.04082v1 Announce Type: cross 
Abstract: With the recently increased interest in probabilistic models, the efficiency of an underlying sampler becomes a crucial consideration. A Hamiltonian Monte Carlo (HMC) sampler is one popular option for models of this kind. Performance of HMC, however, strongly relies on a choice of parameters associated with an integration method for Hamiltonian equations, which up to date remains mainly heuristic or introduce time complexity. We propose a novel computationally inexpensive and flexible approach (we call it Adaptive Tuning or ATune) that, by analyzing the data generated during a burning stage of an HMC simulation, detects a system specific splitting integrator with a set of reliable HMC hyperparameters, including their credible randomization intervals, to be readily used in a production simulation. The method automatically eliminates those values of simulation parameters which could cause undesired extreme scenarios, such as resonance artifacts, low accuracy or poor sampling. The new approach is implemented in the in-house software package \textsf{HaiCS}, with no computational overheads introduced in a production simulation, and can be easily incorporated in any package for Bayesian inference with HMC. The tests on popular statistical models using original HMC and generalized Hamiltonian Monte Carlo (GHMC) reveal the superiority of adaptively tuned methods in terms of stability, performance and accuracy over conventional HMC tuned heuristically and coupled with the well-established integrators. We also claim that the generalized formulation of HMC, i.e. GHMC, is preferable for achieving high sampling performance. The efficiency of the new methodology is assessed in comparison with state-of-the-art samplers, e.g. the No-U-Turn-Sampler (NUTS), in real-world applications, such as endocrine therapy resistance in cancer, modeling of cell-cell adhesion dynamics and influenza epidemic outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04082v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Akhmatskaya, Lorenzo Nagar, Jose Antonio Carrillo, Leonardo Gavira Balmacz, Hristo Inouzhe, Mart\'in Parga Pazos, Mar\'ia Xos\'e Rodr\'iguez \'Alvarez</dc:creator>
    </item>
    <item>
      <title>A Novel Criterion for Interpreting Acoustic Emission Damage Signals Based on Cluster Onset Distribution</title>
      <link>https://arxiv.org/abs/2312.13416</link>
      <description>arXiv:2312.13416v2 Announce Type: replace 
Abstract: Structural health monitoring (SHM) relies on non-destructive techniques such as acoustic emission (AE) that generate large amounts of data over the lifespan of systems. Clustering methods are used to interpret these data and gain insights into damage progression and mechanisms. Conventional methods for evaluating clustering results utilise clustering validity indices (CVI) that prioritise compact and separable clusters. This paper introduces a novel approach based on the temporal sequence of cluster onsets, indicating the initial appearance of potential damage and allowing for early detection of defect initiation. The proposed CVI is based on the Kullback-Leibler divergence and can incorporate prior information about damage onsets when available. Three experiments on real-world datasets validate the effectiveness of the proposed method. The first benchmark focuses on detecting the loosening of bolted plates under vibration, where the onset-based CVI outperforms the conventional approach in both cluster quality and the accuracy of bolt loosening detection. The results demonstrate not only superior cluster quality but also unmatched precision in identifying cluster onsets, whether during uniform or accelerated damage growth. The two additional applications stem from industrial contexts. The first focuses on micro-drilling of hard materials using electrical discharge machining, demonstrating, for the first time, that the proposed criterion can effectively retrieve electrode progression to the reference depth, thus validating the setting of the machine to ensure structural integrity. The final application involves damage understanding in a composite/metal hybrid joint structure, where the cluster timeline is used to establish a scenario leading to critical failure due to slippage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13416v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Ramasso, Martin Mbarga Nkogo, Neha Chandarana, Gilles Bourbon, Patrice Le Moal, Quentin Lefebvre, Martial Personeni, Constantinos Soutis, Matthieu Gresil</dc:creator>
    </item>
    <item>
      <title>Similarity-based Random Partition Distribution for Clustering Functional Data</title>
      <link>https://arxiv.org/abs/2308.01704</link>
      <description>arXiv:2308.01704v5 Announce Type: replace-cross 
Abstract: Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extension of the generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP)-type distribution, to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production and incorporates pairwise similarity information to ensure accurate and meaningful clustering. The theoretical properties of the SGDP-type distribution are studied. Then, SGDP-type random partition is applied to a real-world dataset of hourly population flow in $500\text{m}$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed random partition will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01704v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v4 Announce Type: replace-cross 
Abstract: Statistical models are often defined by a generative process for simulating synthetic data, but this can lead to intractable likelihoods. Likelihood free inference (LFI) methods enable Bayesian inference to be performed in this case. Extending a popular approach to simulation-efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood Free Inference (MOBOLFI) to perform LFI using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a separate discrepancy for each data source. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling detection of conflicting information and deeper understanding of the importance of different data sources in estimating individual parameters. The adaptive choice of simulation parameters using multi-objective Bayesian optimization ensures simulation efficient approximation of likelihood components for all data sources. We illustrate our approach in sequential sampling models (SSMs), which are widely used in psychology and consumer-behavior modeling. SSMs are often fitted using multi-source data, such as choice and response time. The advantages of our approach are illustrated in comparison with a single discrepancy for an SSM fitted to data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>NNN: Next-Generation Neural Networks for Marketing Measurement</title>
      <link>https://arxiv.org/abs/2504.06212</link>
      <description>arXiv:2504.06212v3 Announce Type: replace-cross 
Abstract: We present NNN, an experimental Transformer-based neural network approach to marketing measurement. Unlike Marketing Mix Models (MMMs) which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, potentially enables NNN to model complex interactions, capture long-term effects, and improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. In addition to marketing measurement, the NNN framework can provide valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06212v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mulc, Mike Anderson, Paul Cubre, Huikun Zhang, Ivy Liu, Saket Kumar</dc:creator>
    </item>
    <item>
      <title>FactsR: A Safer Method for Producing High Quality Healthcare Documentation</title>
      <link>https://arxiv.org/abs/2505.10360</link>
      <description>arXiv:2505.10360v2 Announce Type: replace-cross 
Abstract: There are now a multitude of AI-scribing solutions for healthcare promising the utilization of large language models for ambient documentation. However, these AI scribes still rely on one-shot, or few-shot prompts for generating notes after the consultation has ended, employing little to no reasoning. This risks long notes with an increase in hallucinations, misrepresentation of the intent of the clinician, and reliance on the proofreading of the clinician to catch errors. A dangerous combination for patient safety if vigilance is compromised by workload and fatigue. In this paper, we introduce a method for extracting salient clinical information in real-time alongside the healthcare consultation, denoted Facts, and use that information recursively to generate the final note. The FactsR method results in more accurate and concise notes by placing the clinician-in-the-loop of note generation, while opening up new use cases within real-time decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10360v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Petr\'en Bach Hansen, Lasse Krogsb{\o}ll, Jonas Lyngs{\o}, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maal{\o}e</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
      <link>https://arxiv.org/abs/2505.17836</link>
      <description>arXiv:2505.17836v3 Announce Type: replace-cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}(\log(t)/\sqrt{t})$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17836v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Van Elst, Igor Colin, Stephan Cl\'emen\c{c}on</dc:creator>
    </item>
  </channel>
</rss>
