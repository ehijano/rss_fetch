<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 02:47:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Causal Intervention Sequence Analysis for Fault Tracking in Radio Access Networks</title>
      <link>https://arxiv.org/abs/2511.17505</link>
      <description>arXiv:2511.17505v1 Announce Type: new 
Abstract: To keep modern Radio Access Networks (RAN) running smoothly, operators need to spot the real-world triggers behind Service-Level Agreement (SLA) breaches well before customers feel them. We introduce an AI/ML pipeline that does two things most tools miss: (1) finds the likely root-cause indicators and (2) reveals the exact order in which those events unfold. We start by labeling network data: records linked to past SLA breaches are marked `abnormal', and everything else `normal'. Our model then learns the causal chain that turns normal behavior into a fault. In Monte Carlo tests the approach pinpoints the correct trigger sequence with high precision and scales to millions of data points without loss of speed. These results show that high-resolution, causally ordered insights can move fault management from reactive troubleshooting to proactive prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17505v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhua Shi, Joji Philip, Subhadip Bandyopadhyay, Jayanta Choudhury</dc:creator>
    </item>
    <item>
      <title>AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks</title>
      <link>https://arxiv.org/abs/2511.17506</link>
      <description>arXiv:2511.17506v1 Announce Type: new 
Abstract: Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17506v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Narjes Nourzad, Mingyu Zong, Bhaskar Krishnamachari</dc:creator>
    </item>
    <item>
      <title>XAI-on-RAN: Explainable, AI-native, and GPU-Accelerated RAN Towards 6G</title>
      <link>https://arxiv.org/abs/2511.17514</link>
      <description>arXiv:2511.17514v1 Announce Type: new 
Abstract: Artificial intelligence (AI)-native radio access networks (RANs) will serve vertical industries with stringent requirements: smart grids, autonomous vehicles, remote healthcare, industrial automation, etc. To achieve these requirements, modern 5G/6G design increasingly leverage AI for network optimization, but the opacity of AI decisions poses risks in mission-critical domains. These use cases are often delivered via non-public networks (NPNs) or dedicated network slices, where reliability and safety are vital. In this paper, we motivate the need for transparent and trustworthy AI in high-stakes communications (e.g., healthcare, industrial automation, and robotics) by drawing on 3rd generation partnership project (3GPP)'s vision for non-public networks. We design a mathematical framework to model the trade-offs between transparency (explanation fidelity and fairness), latency, and graphics processing unit (GPU) utilization in deploying explainable AI (XAI) models. Empirical evaluations demonstrate that our proposed hybrid XAI model xAI-Native, consistently surpasses conventional baseline models in performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17514v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Osman Tugay Basaran, Falko Dressler</dc:creator>
    </item>
    <item>
      <title>RI-PIENO -- Revised and Improved Petrol-Filling Itinerary Estimation aNd Optimization</title>
      <link>https://arxiv.org/abs/2511.17517</link>
      <description>arXiv:2511.17517v1 Announce Type: new 
Abstract: Efficient energy provisioning is a fundamental requirement for modern transportation systems, making refueling path optimization a critical challenge. Existing solutions often focus either on inter-vehicle communication or intra-vehicle monitoring, leveraging Intelligent Transportation Systems, Digital Twins, and Software-Defined Internet of Vehicles with Cloud/Fog/Edge infrastructures. However, integrated frameworks that adapt dynamically to driver mobility patterns are still underdeveloped. Building on our previous PIENO framework, we present RI-PIENO (Revised and Improved Petrol-filling Itinerary Estimation aNd Optimization), a system that combines intra-vehicle sensor data with external geospatial and fuel price information, processed via IoT-enabled Cloud/Fog services. RI-PIENO models refueling as a dynamic, time-evolving directed acyclic graph that reflects both habitual daily trips and real-time vehicular inputs, transforming the system from a static recommendation tool into a continuously adaptive decision engine. We validate RI-PIENO in a daily-commute use case through realistic multi-driver, multi-week simulations, showing that it achieves significant cost savings and more efficient routing compared to previous approaches. The framework is designed to leverage emerging roadside infrastructure and V2X communication, supporting scalable deployment within next-generation IoT and vehicular networking ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17517v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Savarese, Antonio De Blasi, Carmine Zaccagnino, Giacomo Salici, Silvia Cascianelli, Roberto Vezzani, Carlo Augusto Grazia</dc:creator>
    </item>
    <item>
      <title>Serv-Drishti: An Interactive Serverless Function Request Simulation Engine and Visualiser</title>
      <link>https://arxiv.org/abs/2511.17518</link>
      <description>arXiv:2511.17518v1 Announce Type: new 
Abstract: The rapid adoption of serverless computing necessitates a deeper understanding of its underlying operational mechanics, particularly concerning request routing, cold starts, function scaling, and resource management. This paper presents Serv-Drishti, an interactive, open-source simulation tool designed to demystify these complex behaviours. Serv-Drishti simulates and visualises the journey of a request through a representative serverless platform, from the API Gateway and intelligent Request Dispatcher to dynamic Function Instances on resource-constrained Compute Nodes. Unlike simple simulators, Serv-Drishti provides a robust framework for comparative analysis. It features configurable platform parameters, multiple request routing and function placement strategies, and a comprehensive failure simulation module. This allows users to not only observe but also rigorously analyse system responses under various loads and fault conditions. The tool generates real-time performance graphs and provides detailed data exports, establishing it as a valuable resource for research, education, and the design analysis of serverless architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17518v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>SAJD: Self-Adaptive Jamming Attack Detection in AI/ML Integrated 5G O-RAN Networks</title>
      <link>https://arxiv.org/abs/2511.17519</link>
      <description>arXiv:2511.17519v1 Announce Type: new 
Abstract: The open radio access network (O-RAN) enables modular, intelligent, and programmable 5G network architectures through the adoption of software-defined networking (SDN), network function virtualization (NFV), and implementation of standardized open interfaces. It also facilitates closed loop control and (non/near) real-time optimization of radio access network (RAN) through the integration of non-real-time applications (rApps) and near-real-time applications (xApps). However, one of the security concerns for O-RAN that can severely undermine network performance and subject it to a prominent threat to the security &amp; reliability of O-RAN networks is jamming attacks. To address this, we introduce SAJD-a self-adaptive jammer detection framework that autonomously detects jamming attacks in artificial intelligence (AI) / machine learning (ML)-integrated O-RAN environments. The SAJD framework forms a closed-loop system that includes near-real-time inference of radio signal jamming interference via our developed ML-based xApp, as well as continuous monitoring and retraining pipelines through rApps. Specifically, a labeler rApp is developed that uses live telemetry (i.e., KPIs) to detect model drift, triggers unsupervised data labeling, executes model training/retraining using the integrated &amp; open-source ClearML framework, and updates deployed models on the fly, without service disruption. Experiments on O-RAN-compliant testbed demonstrate that the SAJD framework outperforms state-of-the-art (offline-trained with manual labels) jamming detection approach in accuracy and adaptability under various dynamic and previously unseen interference scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17519v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Habibur Rahman, Md Sharif Hossen, Nathan H. Stephenson, Vijay K. Shah, Aloizio Da Silva</dc:creator>
    </item>
    <item>
      <title>Safe Farming: Development of a Prevention System to Mitigate Vertebrates Crop Raiding</title>
      <link>https://arxiv.org/abs/2511.17520</link>
      <description>arXiv:2511.17520v1 Announce Type: new 
Abstract: One of the main problems for farmers is the protection of their crops, before and after harvesting, from animals and birds. To overcome this problem, this paper proposes a model of safe farming in which the crops will be protected from vertebrates attack through a prevention system that is based on Wirelesses Sensors Networks. Different sensor nodes are placed around the field that detect animals or birds existence and generate required signals and information. This information is passed to the Repelling and Notifying System (RNS) that is installed at the field through a short range wireless technology, ZigBee. As RNS receives the information, it generates ultrasonic sounds that are unbearable for animals and birds, which causes them to run away from the field. These ultrasonic sounds are generated in a frequency range that only animals and birds can hear, while humans cannot notice the sound. The paper also proposes a notifying system. It will inform the farmer about animals or birds intrusion in the field through SMS, but doesn't need any action from the farmer. The low cost and power efficiency of the proposed system is a key advantage for developing countries where cost and power are major players in any system feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17520v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razi Iqbal</dc:creator>
    </item>
    <item>
      <title>DyPBP: Dynamic Peer Beneficialness Prediction for Cryptocurrency P2P Networking</title>
      <link>https://arxiv.org/abs/2511.17523</link>
      <description>arXiv:2511.17523v1 Announce Type: new 
Abstract: Distributed peer-to-peer (P2P) networking delivers the new blocks and transactions and is critical for the cryptocurrency blockchain system operations. Having poor P2P connectivity reduces the financial rewards from the mining consensus protocol. Previous research defines beneficalness of each Bitcoin peer connection and estimates the beneficialness based on the observations of the blocks and transactions delivery, which are after they are delivered. However, due to the infrequent block arrivals and the sporadic and unstable peer connections, the peers do not stay connected long enough to have the beneficialness score to converge to its expected beneficialness. We design and build Dynamic Peer Beneficialness Prediction (DyPBP) which predicts a peer's beneficialness by using networking behavior observations beyond just the block and transaction arrivals. DyPBP advances the previous research by estimating the beneficialness of a peer connection before it delivers new blocks and transactions. To achieve such goal, DyPBP introduces a new feature for remembrance to address the dynamic connectivity issue, as Bitcoin's peers using distributed networking often disconnect and re-connect. We implement DyPBP on an active Bitcoin node connected to the Mainnet and use machine learning for the beneficialness prediction. Our experimental results validate and evaluate the effectiveness of DyPBP; for example, the error performance improves by 2 to 13 orders of magnitude depending on the machine-learning model selection. DyPBP's use of the remembrance feature also informs our model selection. DyPBP enables the P2P connection's beneficialness estimation from the connection start before a new block arrives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17523v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nazmus Sakib, Simeon Wuthier, Amanul Islam, Xiaobo Zhou, Jinoh Kim, Ikkyun Kim, Sang-Yoon Chang</dc:creator>
    </item>
    <item>
      <title>Joint Edge Server Deployment and Computation Offloading: A Multi-Timescale Stochastic Programming Framework</title>
      <link>https://arxiv.org/abs/2511.17524</link>
      <description>arXiv:2511.17524v1 Announce Type: new 
Abstract: Mobile Edge Computing (MEC) is a promising approach for enhancing the quality-of-service (QoS) of AI-enabled applications in the B5G/6G era, by bringing computation capability closer to end-users at the network edge. In this work, we investigate the joint optimization of edge server (ES) deployment, service placement, and computation task offloading under the stochastic information scenario. Traditional approaches often treat these decisions as equal, disregarding the differences in information realization. However, in practice, the ES deployment decision must be made in advance and remain unchanged, prior to the complete realization of information, whereas the decisions regarding service placement and computation task offloading can be made and adjusted in real-time after information is fully realized. To address such temporal coupling between decisions and information realization, we introduce the stochastic programming (SP) framework, which involves a strategic-layer for deciding ES deployment based on (incomplete) stochastic information and a tactical-layer for deciding service placement and task offloading based on complete information realization. The problem is challenging due to the different timescales of two layers' decisions. To overcome this challenge, we propose a multi-timescale SP framework, which includes a large timescale (called period) for strategic-layer decision-making and a small timescale (called slot) for tactical-layer decision making. Moreover, we design a Lyapunov-based algorithm to solve the tactical-layer problem at each time slot, and a Markov approximation algorithm to solve the strategic-layer problem in every time period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17524v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huaizhe Liu, Jiaqi Wu, Zhizongkai Wang, Bin Cao, Lin Gao</dc:creator>
    </item>
    <item>
      <title>Quantifying Multimedia Streaming Quality: A Practical Analysis using PIE and Flow Queue PIE</title>
      <link>https://arxiv.org/abs/2511.17525</link>
      <description>arXiv:2511.17525v1 Announce Type: new 
Abstract: The exponential growth of multimedia streaming services over the Internet emphasizes the increasing significance of ensuring a seamless and high-quality streaming experience for users. Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular solution for delivering multimedia content over variable network conditions. However, challenges such as network congestion, intermittent packet losses, and varying network load continue to impact the Quality of Experience (QoE) perceived by the users. In this work, the main goal is to evaluate the effectiveness of using queue management and flow isolation techniques in terms of improving the overall QoE for DASH based multimedia streaming applications. Proportional Integral controller Enhanced (PIE) and Flow Queue PIE (FQ-PIE) are used as queue management and flow isolation mechanisms, respectively. The most distinctive aspect of this work is our assessment of QoE for multimedia streaming applications when multipath transport protocols, like Multipath TCP (MPTCP), are employed. Network Stack Tester (NeST), a Python based network emulator built on top of Linux network namespaces, has been used to perform the experiments. The parameters used for evaluating the QoE include bitrate, bitrate switches, throughput, Round Trip Time (RTT), and application buffer level. We observe that flow isolation techniques, combined with queue management and multipath transport, significantly improve the QoE for multimedia applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17525v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hemendra M. Naik</dc:creator>
    </item>
    <item>
      <title>RadioMapMotion: A Dataset and Baseline for Proactive Spatio-Temporal Radio Environment Prediction</title>
      <link>https://arxiv.org/abs/2511.17526</link>
      <description>arXiv:2511.17526v1 Announce Type: new 
Abstract: Radio maps (RMs), which provide location-based pathloss estimations, are fundamental to enabling proactive, environment-aware communication in 6G networks. However, existing deep learning-based methods for RM construction often model dynamic environments as a series of independent static snapshots, thereby omitting the temporal continuity inherent in signal propagation changes caused by the motion of dynamic entities. To address this limitation, we propose the task of spatio-temporal RM prediction, which involves forecasting a sequence of future maps from historical observations. A key barrier to this predictive approach has been the lack of datasets capturing continuous environmental evolution. To fill this gap, we introduce RadioMapMotion, the first large-scale public dataset of continuous RM sequences generated from physically consistent vehicle trajectories. As a baseline for this task, we propose RadioLSTM, a UNet architecture based on Convolutional Long Short-Term Memory (ConvLSTM) and designed for multi-step sequence forecasting. Experimental evaluations show that RadioLSTM achieves higher prediction accuracy and structural fidelity compared to representative baseline methods. Furthermore, the model exhibits a low inference latency, indicating its potential suitability for real-time network operations. Our project will be publicly released at: https://github.com/UNIC-Lab/RadioMapMotion upon paper acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17526v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Honggang Jia, Nan Cheng, Xiucheng Wang</dc:creator>
    </item>
    <item>
      <title>Evaluating Device-First Continuum AI (DFC-AI) for Autonomous Operations in the Energy Sector</title>
      <link>https://arxiv.org/abs/2511.17528</link>
      <description>arXiv:2511.17528v1 Announce Type: new 
Abstract: Industrial automation in the energy sector requires AI systems that can operate autonomously regardless of network availability, a requirement that cloud-centric architectures cannot meet. This paper evaluates the application of Device-First Continuum AI (DFC-AI) to critical energy sector operations. DFC-AI, a specialized architecture within the Hybrid Edge Cloud paradigm, implements intelligent agents using a microservices architecture that originates at end devices and extends across the computational continuum. Through comprehensive simulations of energy sector scenarios including drone inspections, sensor networks, and worker safety systems, we demonstrate that DFC-AI maintains full operational capability during network outages while cloud and gateway-based systems experience complete or partial failure. Our analysis reveals that zero-configuration GPU discovery and heterogeneous device clustering are particularly well-suited for energy sector deployments, where specialized nodes can handle intensive AI workloads for entire fleets of inspection drones or sensor networks. The evaluation shows that DFC-AI achieves significant latency reduction and energy savings compared to cloud architectures. Additionally, we find that gateway based edge solutions can paradoxically cost more than cloud solutions for certain energy sector workloads due to infrastructure overhead, while DFC-AI can consistently provide cost savings by leveraging enterprise-owned devices. These findings, validated through rigorous statistical analysis, establish that DFC-AI addresses the unique challenges of energy sector operations, ensuring intelligent agents remain available and functional in remote oil fields, offshore platforms, and other challenging environments characteristic of the industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17528v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siavash M. Alamouti, Fay Arjomandi, Michel Burger, Bashar Altakrouri</dc:creator>
    </item>
    <item>
      <title>Time-Series Foundation Models for ISP Traffic Forecasting</title>
      <link>https://arxiv.org/abs/2511.17529</link>
      <description>arXiv:2511.17529v1 Announce Type: new 
Abstract: Accurate network-traffic forecasting enables proactive capacity planning and anomaly detection in Internet Service Provider (ISP) networks. Recent advances in time-series foundation models (TSFMs) have demonstrated strong zero-shot and few-shot generalization across diverse domains, yet their effectiveness for computer networking remains unexplored. This paper presents a systematic evaluation of a TSFM, IBM's Tiny Time Mixer (TTM), on the CESNET-TimeSeries24 dataset, a 40-week real-world ISP telemetry corpus. We assess TTM under zero-shot and few-shot settings across multiple forecasting horizons (hours to days), aggregation hierarchies (institutions, subnets, IPs), and temporal resolutions (10-minute and hourly). Results show that TTM achieves consistent accuracy (RMSE 0.026-0.057) and stable $R^2$ scores across horizons and context lengths, outperforming or matching fully trained deep learning baselines such as GRU and LSTM. Inference latency remains under 0.05s per 100 points on a single MacBook Pro using CPU-only computation, confirming deployability without dedicated GPU or MPS acceleration. These findings highlight the potential of pretrained TSFMs to enable scalable, efficient, and training-free forecasting for modern network monitoring and management systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17529v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Liu, Behrooz Farkiani, Patrick Crowley</dc:creator>
    </item>
    <item>
      <title>Q-Learning-Based Time-Critical Data Aggregation Scheduling in IoT</title>
      <link>https://arxiv.org/abs/2511.17531</link>
      <description>arXiv:2511.17531v1 Announce Type: new 
Abstract: Time-critical data aggregation in Internet of Things (IoT) networks demands efficient, collision-free scheduling to minimize latency for applications like smart cities and industrial automation. Traditional heuristic methods, with two-phase tree construction and scheduling, often suffer from high computational overhead and suboptimal delays due to their static nature. To address this, we propose a novel Q-learning framework that unifies aggregation tree construction and scheduling, modeling the process as a Markov Decision Process (MDP) with hashed states for scalability. By leveraging a reward function that promotes large, interference-free batch transmissions, our approach dynamically learns optimal scheduling policies. Simulations on static networks with up to 300 nodes demonstrate up to 10.87% lower latency compared to a state-of-the-art heuristic algorithm, highlighting its robustness for delay-sensitive IoT applications. This framework enables timely insights in IoT environments, paving the way for scalable, low-latency data aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17531v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van-Vi Vo, Tien-Dung Nguyen, Duc-Tai Le, Hyunseung Choo</dc:creator>
    </item>
    <item>
      <title>Denoising Refinement Diffusion Models for Simultaneous Generation of Multi-scale Mobile Network Traffic</title>
      <link>https://arxiv.org/abs/2511.17532</link>
      <description>arXiv:2511.17532v2 Announce Type: new 
Abstract: The planning, management, and resource scheduling of cellular mobile networks require joint estimation of mobile traffic across different layers and nodes. Mobile traffic generation can proactively anticipate user demands and capture the dynamics of network load. However, existing methods mainly focus on generating traffic at a single spatiotemporal resolution, making it difficult to jointly model multi-scale traffic patterns. In this paper, we propose ZoomDiff, a diffusion-based model for multi-scale mobile traffic generation. ZoomDiff maps urban environmental context into mobile traffic with multiple spatial and temporal resolutions through a set of customized Denoising Refinement Diffusion Models (DRDM). DRDM employs a multi-stage noise-adding and denoising mechanism, enabling different stages to generate traffic at distinct spatiotemporal resolutions. This design aligns the progressive denoising process with hierarchical network layers, including base stations, cells, and grids of varying granularities. Experiments on real-world mobile traffic datasets show that ZoomDiff achieves at least an 18.4% improvement over state-of-the-art baselines in multi-scale traffic generation tasks. Moreover, ZoomDiff demonstrates strong efficiency and cross-city generalization, highlighting its potential as a powerful generative framework for modeling multi-scale mobile network dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17532v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqian Qi, Haoye Chai, Sichang Liu, Lei Yue, Raoyuan Pan, Yue Wang, Yong Li</dc:creator>
    </item>
    <item>
      <title>Energy Efficiency in Network Slicing: Survey and Taxonomy</title>
      <link>https://arxiv.org/abs/2511.17533</link>
      <description>arXiv:2511.17533v1 Announce Type: new 
Abstract: Network Slicing (NS) is a fundamental feature of 5G, 6G, and future mobile networks, enabling logically isolated virtual networks over shared infrastructure. As data demand increases and services diversify, ensuring Energy Efficiency (EE) in NS is vital (not only for operational cost savings but also to reduce the Information and Communication Technology (ICT) sector's environmental footprint). This survey addresses the need for a comprehensive and holistic perspective on energy-efficient NS by reviewing and classifying recent strategies across the NS life cycle. Our contributions are threefold: (i) a thorough review of state-of-the-art techniques aimed at reducing energy consumption in NS; (ii) a novel taxonomy that organizes strategies into infrastructure, path/route, and slice operation levels; and (iii) the identification of open challenges and research directions, with a focus on systemic, cross-layer, and AI-driven approaches. By consolidating insights from recent developments, our work bridges existing gaps in the literature, offering a structured foundation for researchers and practitioners to design, evaluate, and improve energy-efficient network slicing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17533v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2025.3590365</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, 2025</arxiv:journal_reference>
      <dc:creator>Adnei Willian Donatti, Marcia Cristina Machado, Marvin Alexander Lopez Martinez, Sabino Rog\'erio S. Antunes, Eli Carlos Figueiredo Souza, Sand Correa, Tiago Ferreto, Jos\'e Augusto Suruagy, Joberto S. B. Martins, Tereza Cristina Carvalho</dc:creator>
    </item>
    <item>
      <title>HiFiNet: Hierarchical Fault Identification in Wireless Sensor Networks via Edge-Based Classification and Graph Aggregation</title>
      <link>https://arxiv.org/abs/2511.17537</link>
      <description>arXiv:2511.17537v1 Announce Type: new 
Abstract: Wireless Sensor Networks (WSN) are the backbone of essential monitoring applications, but their deployment in unfavourable conditions increases the risk to data integrity and system reliability. Traditional fault detection methods often struggle to effectively balance accuracy and energy consumption, and they may not fully leverage the complex spatio-temporal correlations inherent in WSN data. In this paper, we introduce HiFiNet, a novel hierarchical fault identification framework that addresses these challenges through a two-stage process. Firstly, edge classifiers with a Long Short-Term Memory (LSTM) stacked autoencoder perform temporal feature extraction and output initial fault class prediction for individual sensor nodes. Using these results, a Graph Attention Network (GAT) then aggregates information from neighboring nodes to refine the classification by integrating the topology context. Our method is able to produce more accurate predictions by capturing both local temporal patterns and network-wide spatial dependencies. To validate this approach, we constructed synthetic WSN datasets by introducing specific, predefined faults into the Intel Lab Dataset and NASA's MERRA-2 reanalysis data. Experimental results demonstrate that HiFiNet significantly outperforms existing methods in accuracy, F1-score, and precision, showcasing its robustness and effectiveness in identifying diverse fault types. Furthermore, the framework's design allows for a tunable trade-off between diagnostic performance and energy efficiency, making it adaptable to different operational requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17537v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nguyen Van Son, Nguyen Tri Nghia, Nguyen Thi Hanh, Huynh Thi Thanh Binh</dc:creator>
    </item>
    <item>
      <title>Group Equivariant Convolutional Networks for Pathloss Estimation</title>
      <link>https://arxiv.org/abs/2511.17841</link>
      <description>arXiv:2511.17841v1 Announce Type: new 
Abstract: This paper presents RadioGUNet, a UNet-based deep learning framework for pathloss estimation in wireless communication. Unlike other frameworks, it leverages group equivariant convolutional networks, which are known to increase the expressive capacity of a neural network by allowing the model to generalize to further classes of symmetries, such as rotations and reflections, without the need for data augmentation or data pre-processing. The results of this work are twofold. First, we show that typical UNet-based convolutional models can be easily extended to support group equivariant convolution (g-conv). Secondly, we show that the task of pathloss estimation benefits from such an extension, as the proposed extended model outperforms typical UNet-based models by up to 0.41 dB for a similar number of parameters in the RadioMapSeer dataset. The code is publicly available on the GitHub page: https://github.com/EricssonResearch/radiogunet</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17841v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ziyue Yang, Feng Liu, Yifei Jin, Konstantinos Vandikas</dc:creator>
    </item>
    <item>
      <title>Performance comparison of 802.11mc and 802.11az Wi-Fi Fine Time Measurement protocols</title>
      <link>https://arxiv.org/abs/2511.17935</link>
      <description>arXiv:2511.17935v1 Announce Type: new 
Abstract: The need for meter level location accuracy is driving increased adoption of 802.11 mc/az Fine Time Measurement (FTM) based ranging in Wi-Fi networks. In this paper, we present a comparative study of the ranging accuracy of 802.11mc and 802.11az protocols. We examine by real world measurements the critical parameters that influence the accuracy of FTM {\it{viz.,}} channel width, interference, radio environment, and offset calibration. The measurements demonstrate that meter-level ranging accuracy can be consistently attained in line of sight environment on 80 MHz and 160 MHz channels, while an accuracy of about 5m is obtained in non-line of sight environment. It is observed that the 802.11az protocol is capable of providing better accuracy than 802.11mc even in a multipath heavy environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17935v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Govind Rajendran, Kushagra Sharma, Vijayalakshmi Chetlapalli, Jatin Parekh</dc:creator>
    </item>
    <item>
      <title>A Method to Automatically Extract a Network Device Configuration Model by Parsing Network Device Configurations</title>
      <link>https://arxiv.org/abs/2511.17948</link>
      <description>arXiv:2511.17948v1 Announce Type: new 
Abstract: When network engineers design a network, they need to verify the validity of their design in a test environment. Since testing on actual equipment is expensive and burdensome for engineers, we have proposed automatic verification methods using simulators and consistency verification methods for a network configuration model. Combining these methods with conventional verification methods for network device configurations will increase the number of verification options that do not require actual devices. However, the burden of writing existing networks into models has been a problem in our model-based verification. In this paper, we propose a method for automatically extracting a network device configuration model by parsing the contents obtained from network devices via show running-config commands and the like. In order to evaluate the effectiveness of the proposed method in realizing round-trip engineering between network device configurations and the network device configuration model, we extracted a model from existing network device configurations and generated device configuration commands. As a result, we obtained model and commands with high accuracy, indicating that the proposed method is effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17948v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IPSJ Transactions on digital practices (TDP), Vol.6, No.3, pp.30-41, 2025</arxiv:journal_reference>
      <dc:creator>Kosei Nakamura, Hikofumi Suzuki, Shinpei Ogata, Hiroaki Hashiura, Takashi Nagai, Kozo Okano</dc:creator>
    </item>
    <item>
      <title>Proposal of an Automatic Verification Method for Network Configuration Model by Static Analysis</title>
      <link>https://arxiv.org/abs/2511.17950</link>
      <description>arXiv:2511.17950v1 Announce Type: new 
Abstract: In the network design phase, designers typically assess the validity of the network configuration on paper. However, the interactions between devices based on network protocols can be complex, making this assessment challenging. Meanwhile, testing with actual devices incurs significant costs and effort for procurement and preparation. Traditional methods, however, have limitations in identifying configuration values that cause policy violations and verifying syntactically incomplete device configuration files. In this paper, we propose a method to automatically verify the consistency of a model representing the network configuration (Network Configuration Model) by static analysis. The proposed method performs verification based on the network configuration model to detect policy violations and points out configuration values that cause these violations. Additionally, to facilitate the designers' review of each network device's configuration, the model is converted into a format that mimics the output of actual devices, which designers are likely familiar with. As a case study, we applied the proposed method to the network configuration of Shinshu University, a large-scale campus network, by intentionally introducing configuration errors and applying the method. We further evaluated whether it could output device states equivalent to those of actual devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17950v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IPSJ Transactions on digital practices (TDP), Vol.6, No.3, pp.42-55, 2025</arxiv:journal_reference>
      <dc:creator>Tomoya Fujita, Hikofumi Suzuki, Shinpei Ogata, Hiroaki Hashiura, Takashi Nagai, Kozo Okano</dc:creator>
    </item>
    <item>
      <title>A System to Automatically Generate Configuration Instructions for Network Elements from Network Configuration Models</title>
      <link>https://arxiv.org/abs/2511.18100</link>
      <description>arXiv:2511.18100v1 Announce Type: new 
Abstract: In preparation for constructing or modifying information networks, network engineers develop configuration procedures for network devices according to network configuration specifications. However, as engineers typically create these procedures manually, the generated configuration procedures frequently diverge from the specified requirements. To improve this situation, this paper proposes a method for automatically generating configuration procedures consisting of network device configuration commands based on network configurations and their modification specifications. In this study, we employed the UML (Unified Modeling Language) object-oriented modeling language to develop a notation for network configuration modeling that ensures both strict specification adherence and ease of extension. Additionally, we implemented a method for automatically generating configuration procedures that match the specifications by utilizing network configuration models. As an evaluation experiment, we applied the proposed method to a configuration change scenario in a wide-area campus network at Shinshu University, where the network was migrated from static routing to dynamic routing using the OSPF protocol. As a result, all expected configuration procedures were obtained and a network exhibiting the intended behavior was successfully constructed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18100v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IPSJ Transactions on digital practices (TDP), Vol.4, No.3, pp.33-47, 2023</arxiv:journal_reference>
      <dc:creator>Nagi Arai, Shinpei Ogata, Hikofumi Suzuki, Kozo Okano</dc:creator>
    </item>
    <item>
      <title>Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval</title>
      <link>https://arxiv.org/abs/2511.18354</link>
      <description>arXiv:2511.18354v1 Announce Type: new 
Abstract: The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18354v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Bilal, Zafar Qazi, Marco Canini</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Task Computation at the Edge for Vehicular Services</title>
      <link>https://arxiv.org/abs/2511.18449</link>
      <description>arXiv:2511.18449v1 Announce Type: new 
Abstract: Multi-access edge computing (MEC) is a promising solution for providing the computational resources and low latency required by vehicular services such as autonomous driving. It enables cars to offload computationally intensive tasks to nearby servers. Effective offloading involves determining when to offload tasks, selecting the appropriate MEC site, and efficiently allocating resources to ensure good performance. Car mobility poses significant challenges to guaranteeing reliable task completion, and today we still lack energy efficient solutions to this problem, especially when considering real-world car mobility traces. In this paper, we begin by examining the mobility patterns of cars using data obtained from a leading mobile network operator in Europe. Based on the insights from this analysis, we design an optimization problem for task computation and offloading, considering both static and mobility scenarios. Our objective is to minimize the total energy consumption at the cars and at the MEC nodes while satisfying the latency requirements of various tasks. We evaluate our solution, based on multi-agent reinforcement learning, both in simulations and in a realistic setup that relies on datasets from the operator. Our solution shows a significant reduction of user dissatisfaction and task interruptions in both static and mobile scenarios, while achieving energy savings of 47 percent in the static case and 14 percent in the mobile case compared to state-of-the-art schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18449v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/NOMS57970.2025.11073636</arxiv:DOI>
      <arxiv:journal_reference>NOMS 2025-2025 IEEE Network Operations and Management Symposium, pp. 1-9</arxiv:journal_reference>
      <dc:creator>Paniz Parastar, Giuseppe Caso, Jesus Alberto Omana Iglesias, Andra Lutu, Ozgu Alay</dc:creator>
    </item>
    <item>
      <title>SFusion: Energy and Coding Fusion for Ultra-Robust Low-SNR LoRa Networks</title>
      <link>https://arxiv.org/abs/2511.18484</link>
      <description>arXiv:2511.18484v1 Announce Type: new 
Abstract: LoRa has become a cornerstone for city-wide IoT applications due to its long-range, low-power communication. It achieves extended transmission by spreading symbols over multiple samples, with redundancy controlled by the Spreading Factor (SF), and further error resilience provided by Forward Error Correction (FEC). However, practical limits on SF and the separation between signal-level demodulation and coding-level error correction in conventional LoRa PHY leave it vulnerable under extremely weak signals - common in city-scale deployments. To address this, we present SFusion, a software-based coding framework that jointly leverages signal-level aggregation and coding-level redundancy to enhance LoRa's robustness. When signals fall below the decodable threshold, SFusion encodes a quasi-SF(k +m) symbol using 2^m SFk symbols to boost processing gain through energy accumulation. Once partial decoding becomes feasible with energy aggregation, an opportunistic decoding strategy directly combines IQ signals across symbols to recover errors. Extensive evaluations show that SFusion achieves up to 15dB gain over SF12 and up to 13dB improvement over state-of-the-art solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18484v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiwei Chen, Huaxuan Xiao, Jiefeng Zhang, Xianjin Xia, Shuai Wang, Xianjun Deng, Dan Zeng</dc:creator>
    </item>
    <item>
      <title>A Digital Twin Platform for QoS Optimization Under DoS Attacks for Next Generation Radio Networks</title>
      <link>https://arxiv.org/abs/2511.18577</link>
      <description>arXiv:2511.18577v1 Announce Type: new 
Abstract: Digital Twins are being used as an enabling technology in 6G applications across various domains, valued for their data-driven insights and real-time decision-making capabilities. However, integrating Digital Twins into 6G environments presents challenges in maintaining consistent network services under adverse conditions such as including denial-of-service (DoS) attacks, while ensuring consistent Quality of Service (QoS). In this work, we present a Digital Twin Platform to facilitate bidirectional communication between User Equipment (UEs) and application-specific digital twins to enhance UE traffic under UDP flood attacks. By leveraging AI to analyze key digital twin parameters such as throughput and delay, our framework derives actionable insights that enhance QoS management in DoS attack scenarios, ultimately advancing real-world applications of digital twins in critical infrastructure domains. The performance of this Digital Twin Platform is validated through an emergency management use-case in 6G networks while the network is under attack with UDP flood attacks in terms of packet reception success rate, average packet delay, and average throughput metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18577v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mehmet Ali Erturk, Kubra Duran, Ahmed Al-Dubai, Berk Canberk</dc:creator>
    </item>
    <item>
      <title>Toward Integrated Air-Ground Computing and Communications: A Synergy of Computing Power Networks and Low-Altitude Economy Network</title>
      <link>https://arxiv.org/abs/2511.18720</link>
      <description>arXiv:2511.18720v1 Announce Type: new 
Abstract: With the rapid rise of the Low-Altitude Economy (LAE), the demand for intelligent processing and real-time response in services such as aerial traffic, emergency communications, and environmental monitoring continues to grow. Meanwhile, the Computing Power Network (CPN) aims to integrate global computing resources and perform on-demand scheduling to efficiently handle services from diverse sources. However, it is limited by static deployment and limited adaptability. In this paper, we analyze the complementary relationship between LAE and CPN and propose a novel air-ground collaborative intelligent service provision with an agentification paradigm. Through synergy between LAE and CPNs, computing and communication services are jointly scheduled and collaboratively optimized to enhance the execution efficiency of low-altitude services and improve the flexibility of CPNs. It also integrates LAE's strengths in aerial sensing, mobile coverage, and dynamic communication links, forming a cloud-edge-air collaborative framework. Hence, we review the characteristics and limitations of both LAE and CPN and explore how they can cooperate to overcome these limitations. Then we demonstrate the flexibility of the integrated CPN and LAE framework through a case study. Finally, we summarize the key challenges in constructing an integrated air-ground computing and communication system and discuss future research directions toward emerging technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18720v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Sun, Yinqiu Liu, Shaoyong Guo, Ruichen Zhang, Jiacheng Wang, Feng Qi, Xuesong Qiu, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Protocol in Vehicular Opportunistic Networks: A Dynamic Cluster-based Routing Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.19026</link>
      <description>arXiv:2511.19026v1 Announce Type: new 
Abstract: Opportunistic Networks (OppNets) employ the Store-Carry-Forward (SCF) paradigm to maintain communication during intermittent connectivity. However, routing performance suffers due to dynamic topology changes, unpredictable contact patterns, and resource constraints including limited energy and buffer capacity. These challenges compromise delivery reliability, increase latency, and reduce node longevity in highly dynamic environments. This paper proposes Cluster-based Routing using Deep Reinforcement Learning (CR-DRL), an adaptive routing approach that integrates an Actor-Critic learning framework with a heuristic function. CR-DRL enables real-time optimal relay selection and dynamic cluster overlap adjustment to maintain connectivity while minimizing redundant transmissions and enhancing routing efficiency. Simulation results demonstrate significant improvements over state-of-the-art baselines. CR-DRL extends node lifetimes by up to 21%, overall energy use is reduced by 17%, and nodes remain active for 15% longer. Communication performance also improves, with up to 10% higher delivery ratio, 28.5% lower delay, 7% higher throughput, and data requiring 30% fewer transmission steps across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19026v1</guid>
      <category>cs.NI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meisam Sahrifi Sani, Saeid Iranmanesh, Raad Raad, Faisel Tubbal</dc:creator>
    </item>
    <item>
      <title>Diffusion Model-Enhanced Environment Reconstruction in ISAC</title>
      <link>https://arxiv.org/abs/2511.19044</link>
      <description>arXiv:2511.19044v1 Announce Type: new 
Abstract: Recently, environment reconstruction (ER) in integrated sensing and communication (ISAC) systems has emerged as a promising approach for achieving high-resolution environmental perception. However, the initial results obtained from ISAC systems are coarse and often unsatisfactory due to the high sparsity of the point clouds and significant noise variance. To address this problem, we propose a noise-sparsity-aware diffusion model (NSADM) post-processing framework. Leveraging the powerful data recovery capabilities of diffusion models, the proposed scheme exploits spatial features and the additive nature of noise to enhance point cloud density and denoise the initial input. Simulation results demonstrate that the proposed method significantly outperforms existing model-based and deep learning-based approaches in terms of Chamfer distance and root mean square error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19044v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Duc Minh Quang, Chang Liu, Shuangyang Li, Hoai-Nam Vu, Derrick Wing Kwan Ng, Wei Xiang</dc:creator>
    </item>
    <item>
      <title>Agent Discovery in Internet of Agents: Challenges and Solutions</title>
      <link>https://arxiv.org/abs/2511.19113</link>
      <description>arXiv:2511.19113v1 Announce Type: new 
Abstract: Rapid advances in large language models and agentic AI are driving the emergence of the Internet of Agents (IoA), a paradigm where billions of autonomous software and embodied agents interact, coordinate, and collaborate to accomplish complex tasks. A key prerequisite for such large-scale collaboration is agent capability discovery, where agents identify, advertise, and match one another's capabilities under dynamic tasks. Agent's capability in IoA is inherently heterogeneous and context-dependent, raising challenges in capability representation, scalable discovery, and long-term performance. To address these issues, this paper introduces a novel two-stage capability discovery framework. The first stage, autonomous capability announcement, allows agents to credibly publish machine-interpretable descriptions of their abilities. The second stage, task-driven capability discovery, enables context-aware search, ranking, and composition to locate and assemble suitable agents for specific tasks. Building on this framework, we propose a novel scheme that integrates semantic capability modeling, scalable and updatable indexing, and memory-enhanced continual discovery. Simulation results demonstrate that our approach enhances discovery performance and scalability. Finally, we outline a research roadmap and highlight open problems and promising directions for future IoA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19113v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolong Guo, Yuntao Wang, Zhou Su, Yanghe Pan, Qinnan Hu, Tom H. Luan</dc:creator>
    </item>
    <item>
      <title>LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk</title>
      <link>https://arxiv.org/abs/2511.19175</link>
      <description>arXiv:2511.19175v1 Announce Type: new 
Abstract: A critical barrier to the trustworthiness of sixth-generation (6G) agentic autonomous networks is the uncertainty neglect bias; a cognitive tendency for large language model (LLM)-powered agents to make high-stakes decisions based on simple averages while ignoring the tail risk of extreme events. This paper proposes an unbiased, risk-aware framework for agentic negotiation, designed to ensure robust resource allocation in 6G network slicing. Specifically, agents leverage Digital Twins (DTs) to predict full latency distributions, which are then evaluated using a formal framework from extreme value theory, namely, Conditional Value-at-Risk (CVaR). This approach fundamentally shifts the agent's objective from reasoning over the mean to reasoning over the tail, thereby building a statistically-grounded buffer against worst-case outcomes. Furthermore, our framework ensures full uncertainty awareness by requiring agents to quantify epistemic uncertainty -- confidence in their own DTs predictions -- and propagate this meta-verification to make robust decisions, preventing them from acting on unreliable data. We validate this framework in a 6G inter-slice negotiation use-case between an eMBB and a URLLC agent. The results demonstrate the profound failure of the biased, mean-based baseline, which consistently fails its SLAs with a 25\% rate. Our unbiased, CVaR-aware agent successfully mitigates this bias, eliminating SLA violations and reducing the URLLC and eMBB p99.999 latencies by around 11\%. We show this reliability comes at the rational and quantifiable cost of slightly reduced energy savings to 17\%, exposing the false economy of the biased approach. This work provides a concrete methodology for building the trustworthy autonomous systems required for 6G.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19175v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hatim Chergui, Farhad Rezazadeh, Mehdi Bennis, Merouane Debbah</dc:creator>
    </item>
    <item>
      <title>Characterizing the Impact of Active Queue Management on Speed Test Measurements</title>
      <link>https://arxiv.org/abs/2511.19213</link>
      <description>arXiv:2511.19213v1 Announce Type: new 
Abstract: Present day speed test tools measure peak throughput, but often fail to capture the user-perceived responsiveness of a network connection under load. Recently, platforms such as NDT, Ookla Speedtest and Cloudflare Speed Test have introduced metrics such as ``latency under load'' or ``working latency'' to fill this gap. Yet, the sensitivity of these metrics to basic network configurations such as Active Queue Management (AQM) remains poorly understood. In this work, we conduct an empirical study of the impact of AQM on speed test measurements in a laboratory setting. Using controlled experiments, we compare the distribution of throughput and latency under different load measurements across different AQM schemes, including CoDel, FQ-CoDel and Stochastic Fair Queuing (SFQ). On comparing with a standard drop-tail baseline, we find that measurements have high variance across AQM schemes and load conditions. These results highlight the critical role of AQM in shaping how emerging latency metrics should be interpreted, and underscore the need for careful calibration of speed test platforms before their results are used to guide policy or regulatory outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19213v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddhant Ray, Taveesh Sharma, Jonatas Marques, Paul Schmitt, Francesco Bronzino, Nick Feamster</dc:creator>
    </item>
    <item>
      <title>An O-RAN Framework for AI/ML-Based Localization with OpenAirInterface and FlexRIC</title>
      <link>https://arxiv.org/abs/2511.19233</link>
      <description>arXiv:2511.19233v1 Announce Type: new 
Abstract: Localization is increasingly becoming an integral component of wireless cellular networks. The advent of artificial intelligence (AI) and machine learning (ML) based localization algorithms presents potential for enhancing localization accuracy. Nevertheless, current standardization efforts in the third generation partnership project (3GPP) and the O-RAN Alliance do not support AI/ML-based localization. In order to close this standardization gap, this paper describes an O-RAN framework that enables the integration of AI/ML-based localization algorithms for real-time deployments and testing. Specifically, our framework includes an O-RAN E2 Service Model (E2SM) and the corresponding radio access network (RAN) function, which exposes the Uplink Sounding Reference Signal (UL-SRS) channel estimates from the E2 agent to the Near real-time RAN Intelligent Controller (Near-RT RIC). Moreover, our framework includes, as an example, a real-time localization external application (xApp), which leverages the custom E2SM-SRS in order to execute continuous inference on a trained Channel Charting (CC) model, which is an emerging self-supervised method for radio-based localization. Our framework is implemented with OpenAirInterface (OAI) and FlexRIC, democratizing access to AI-driven positioning research and fostering collaboration. Furthermore, we validate our approach with the CC xApp in real-world conditions using an O-RAN based localization testbed at EURECOM. The results demonstrate the feasibility of our framework in enabling real-time AI/ML localization and show the potential of O-RAN in empowering positioning use cases for next-generation AI-native networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19233v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nada Bouknana, Mohsen Ahadi, Florian Kaltenberger, Robert Schmidt</dc:creator>
    </item>
    <item>
      <title>Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation</title>
      <link>https://arxiv.org/abs/2511.17977</link>
      <description>arXiv:2511.17977v1 Announce Type: cross 
Abstract: Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.
  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.
  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17977v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kuangxiangzi Liu, Dhiman Chakraborty, Alexander Liggesmeyer, Andreas Zeller</dc:creator>
    </item>
    <item>
      <title>Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior</title>
      <link>https://arxiv.org/abs/2511.18086</link>
      <description>arXiv:2511.18086v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.
  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.
  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.
  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18086v1</guid>
      <category>cs.RO</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Louren\c{c}o, Ant\'onio Grilo</dc:creator>
    </item>
    <item>
      <title>AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</title>
      <link>https://arxiv.org/abs/2511.18151</link>
      <description>arXiv:2511.18151v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18151v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt</dc:creator>
    </item>
    <item>
      <title>LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation</title>
      <link>https://arxiv.org/abs/2511.18158</link>
      <description>arXiv:2511.18158v1 Announce Type: cross 
Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18158v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelrahman Abdelmotlb, Abdallah Taman, Sherif Mostafa, Moustafa Youssef</dc:creator>
    </item>
    <item>
      <title>An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds</title>
      <link>https://arxiv.org/abs/2511.18906</link>
      <description>arXiv:2511.18906v1 Announce Type: cross 
Abstract: The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18906v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Zambianco, Lorenzo Fasol, Roberto Doriguzzi-Corin</dc:creator>
    </item>
    <item>
      <title>LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</title>
      <link>https://arxiv.org/abs/2511.19368</link>
      <description>arXiv:2511.19368v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19368v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyang Duan, Zongyuan Zhang, Zheng Lin, Songxiao Guo, Xiuxian Guan, Guangyu Wu, Zihan Fang, Haotian Meng, Xia Du, Ji-Zhe Zhou, Heming Cui, Jun Luo, Yue Gao</dc:creator>
    </item>
    <item>
      <title>KIGLIS: Smart Networks for Smart Cities</title>
      <link>https://arxiv.org/abs/2106.04549</link>
      <description>arXiv:2106.04549v4 Announce Type: replace 
Abstract: Smart cities will be characterized by a variety of intelligent and networked services, each with specific requirements for the underlying network infrastructure. While smart city architectures and services have been studied extensively, little attention has been paid to the network technology. The KIGLIS research project, consisting of a consortium of companies, universities and research institutions, focuses on artificial intelligence for optimizing fiber-optic networks of a smart city, with a special focus on future mobility applications, such as automated driving. In this paper, we present early results on our process of collecting smart city requirements for communication networks, which will lead towards reference infrastructure and architecture solutions. Finally, we suggest directions in which artificial intelligence will improve smart city networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.04549v4</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISC253183.2021.9562826</arxiv:DOI>
      <dc:creator>Daniel Bogdoll, Patrick Matalla, Christoph F\"ullner, Christian Raack, Shi Li, Tobias K\"afer, Stefan Orf, Marc Ren\'e Zofka, Finn Sartoris, Christoph Schweikert, Thomas Pfeiffer, Andr\'e Richter, Sebastian Randel, Rene Bonk</dc:creator>
    </item>
    <item>
      <title>B2LoRa: Boosting LoRa Transmission for Satellite-IoT Systems with Blind Coherent Combining</title>
      <link>https://arxiv.org/abs/2505.24140</link>
      <description>arXiv:2505.24140v2 Announce Type: replace 
Abstract: With the rapid growth of Low Earth Orbit (LEO) satellite networks, satellite-IoT systems using the LoRa technique have been increasingly deployed to provide widespread Internet services to low-power and low-cost ground devices. However, the long transmission distance and adverse environments from IoT satellites to ground devices pose a huge challenge to link reliability, as evidenced by the measurement results based on our real-world setup. In this paper, we propose a blind coherent combining design named B2LoRa to boost LoRa transmission performance. The intuition behind B2LoRa is to leverage the repeated broadcasting mechanism inherent in satellite-IoT systems to achieve coherent combining under the low-power and low-cost constraints, where each re-transmission at different times is regarded as the same packet transmitted from different antenna elements within an antenna array. Then, the problem is translated into aligning these packets at a fine granularity despite the time, frequency, and phase offsets between packets in the case of frequent packet loss. To overcome this challenge, we present three designs - joint packet sniffing, frequency shift alignment, and phase drift mitigation to deal with ultra-low SNRs and Doppler shifts featured in satellite-IoT systems, respectively. Finally, experiment results based on our real-world deployments demonstrate the high efficiency of B2LoRa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24140v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680207.3723463</arxiv:DOI>
      <arxiv:journal_reference>ACM MOBICOM 2025</arxiv:journal_reference>
      <dc:creator>Yimin Zhao, Weibo Wang, Xiong Wang, Linghe Kong, Jiadi Yu, Yifei Zhu, Shiyuan Li, Chong He, Guihai Chen</dc:creator>
    </item>
    <item>
      <title>Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI</title>
      <link>https://arxiv.org/abs/2507.10510</link>
      <description>arXiv:2507.10510v2 Announce Type: replace 
Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we call for AI-oriented RTC research, exploring the network requirement shift from "humans watching video" to "AI understanding video". We begin by recognizing the main differences between AI Video Chat and traditional RTC. Then, through prototype measurements, we identify that ultra-low bitrate is a key factor for low latency. To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. DeViBench is open-sourced at: https://github.com/pku-netvideo/DeViBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10510v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772356.3772390</arxiv:DOI>
      <dc:creator>Jiangkai Wu, Zhiyuan Ren, Liming Liu, Xinggong Zhang</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Dynamic Equilibria in Joint Path Selection and Congestion Control in Path-Aware Networks</title>
      <link>https://arxiv.org/abs/2510.26060</link>
      <description>arXiv:2510.26060v2 Announce Type: replace 
Abstract: Path-aware networking (PAN) architectures, such as SCION and emerging LEO constellations, expose tens to hundreds of verifiable paths to endpoints. When multipath protocols like MPTCP and MPQUIC greedily exploit this diversity, uncoordinated migration can induce persistent, high-amplitude load oscillations. Although this instability is well-known, its quantitative performance impact remains poorly understood.
  In this paper, we apply a discrete-time axiomatic framework to the joint dynamics of loss-based congestion control and greedy path selection. By deriving the system's dynamic equilibria (stable periodic oscillations), we prove a fundamental trade-off: high Responsiveness improves Fairness but necessarily degrades Efficiency and Convergence. Conversely, we demonstrate that Efficiency, Convergence, and Loss Avoidance are simultaneously achievable at a critical lossless operating point. Furthermore, we find that while migration de-synchronizes traffic in high-diversity environments, realistic limited-visibility constraints transform coherent oscillations into persistent spatial load imbalance, rather than eliminating instability entirely. These results yield concrete design guidelines for robust multipath transport over the future path-aware Internet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26060v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Keshvadi</dc:creator>
    </item>
    <item>
      <title>SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference</title>
      <link>https://arxiv.org/abs/2507.06567</link>
      <description>arXiv:2507.06567v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06567v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Chen, Xianhao Chen, Kaibin Huang</dc:creator>
    </item>
  </channel>
</rss>
