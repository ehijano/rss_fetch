<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2024 01:33:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Creating Centered Trochoids and Co-Centered Ellipses through the Combinations of Uniform Rolling and Sliding Operations by Using Virtual Rotating Circles Technique (VRCT)</title>
      <link>https://arxiv.org/abs/2407.06966</link>
      <description>arXiv:2407.06966v1 Announce Type: new 
Abstract: In this article we present an innovative mental vision for creating uniform rolling and sliding motions for a circle along another circle . Also, we describe methods to combine rolling and sliding motions through VRCT in order to plotting centered trochoids and co-centered ellipses. Traditional mathematical perspective for creating centered trochoids through the pure rolling process for a circle along another circle is changed to a novel mathematical perspective which is based on the combination of uniform rolling and sliding motions of a circle along another one. In this novel vision we have not to define a centered trochoid as a traced path of an attached point to a pure rolling circle along another circle. Instead, a centered trochoid can be defined as a traced path by a certain point on the circumference of a rolling and sliding circle along another circle . Also, through this vision an ellipse can be visualized as a closed plane curve that is the product of uniform combination of rolling and sliding motions due to superposition of two co-polarized rotational motions with different commensurable angular frequencies! Detailed points in the process of plotting centered trochoids and ellipses through the combination of rolling and sliding operations are observable directly by application an innovative instrument that we have named it mechanical Oscilloscope. The function of our device is independent from any other electronic devices such as computer and does not require programming to plot centered trochoids and ellipses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06966v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Arbab, Arzhang Arbab</dc:creator>
    </item>
    <item>
      <title>3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes</title>
      <link>https://arxiv.org/abs/2407.07090</link>
      <description>arXiv:2407.07090v2 Announce Type: new 
Abstract: Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07090v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic</dc:creator>
    </item>
    <item>
      <title>SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers</title>
      <link>https://arxiv.org/abs/2407.06305</link>
      <description>arXiv:2407.06305v1 Announce Type: cross 
Abstract: Shape abstraction is an important task for simplifying complex geometric structures while retaining essential features. Sweep surfaces, commonly found in human-made objects, aid in this process by effectively capturing and representing object geometry, thereby facilitating abstraction. In this paper, we introduce \papername, a novel approach to shape abstraction through sweep surfaces. We propose an effective parameterization for sweep surfaces, utilizing superellipses for profile representation and B-spline curves for the axis. This compact representation, requiring as few as 14 float numbers, facilitates intuitive and interactive editing while preserving shape details effectively. Additionally, by introducing a differentiable neural sweeper and an encoder-decoder architecture, we demonstrate the ability to predict sweep surface representations without supervision. We show the superiority of our model through several quantitative and qualitative experiments throughout the paper. Our code is available at https://mingrui-zhao.github.io/SweepNet/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06305v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingrui Zhao, Yizhi Wang, Fenggen Yu, Changqing Zou, Ali Mahdavi-Amiri</dc:creator>
    </item>
    <item>
      <title>Sketch-Guided Scene Image Generation</title>
      <link>https://arxiv.org/abs/2407.06469</link>
      <description>arXiv:2407.06469v1 Announce Type: cross 
Abstract: Text-to-image models are showcasing the impressive ability to create high-quality and diverse generative images. Nevertheless, the transition from freehand sketches to complex scene images remains challenging using diffusion models. In this study, we propose a novel sketch-guided scene image generation framework, decomposing the task of scene image scene generation from sketch inputs into object-level cross-domain generation and scene-level image construction. We employ pre-trained diffusion models to convert each single object drawing into an image of the object, inferring additional details while maintaining the sparse sketch structure. In order to maintain the conceptual fidelity of the foreground during scene generation, we invert the visual features of object images into identity embeddings for scene generation. In scene-level image construction, we generate the latent representation of the scene image using the separated background prompts, and then blend the generated foreground objects according to the layout of the sketch input. To ensure the foreground objects' details remain unchanged while naturally composing the scene image, we infer the scene image on the blended latent representation using a global prompt that includes the trained identity tokens. Through qualitative and quantitative experiments, we demonstrate the ability of the proposed approach to generate scene images from hand-drawn sketches surpasses the state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06469v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Xiaoxuan Xie, Xusheng Du, Haoran Xie</dc:creator>
    </item>
    <item>
      <title>Gaunt coefficients for complex and real spherical harmonics with applications to spherical array processing and Ambisonics</title>
      <link>https://arxiv.org/abs/2407.06847</link>
      <description>arXiv:2407.06847v1 Announce Type: cross 
Abstract: Acoustical signal processing of directional representations of sound fields, including source, receiver, and scatterer transfer functions, are often expressed and modeled in the spherical harmonic domain (SHD). Certain such modeling operations, or applications of those models, involve multiplications of those directional quantities, which can also be expressed conveniently in the SHD through coupling coefficients known as Gaunt coefficients. Since the definition and notation of Gaunt coefficients varies across acoustical publications, this work defines them based on established conventions of complex and real spherical harmonics (SHs) along with a convenient matrix form for spherical multiplication of directionally band-limited spherical functions. Additionally, the report provides a derivation of the Gaunt coefficients for real SHs, which has been missing from the literature and can be used directly in spatial audio frameworks such as Ambisonics. Matlab code is provided that can compute all coefficients up to user specified SH orders. Finally, a number of relevant acoustical processing examples from the literature are presented, following the matrix formalism of coefficients introduced in the report.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06847v1</guid>
      <category>eess.AS</category>
      <category>cs.GR</category>
      <category>cs.SD</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Archontis Politis</dc:creator>
    </item>
  </channel>
</rss>
