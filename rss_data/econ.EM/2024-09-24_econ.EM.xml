<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 02:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v1 Announce Type: new 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity (especially exclusion restrictions) is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can accelerate this process exponentially and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We argue that multi-step prompting is useful and role-playing prompts are suitable for mimicking the endogenous decisions of economic agents. We apply our method to three well-known examples in economics: returns to schooling, production functions, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Inequality Sensitive Optimal Treatment Assignment</title>
      <link>https://arxiv.org/abs/2409.14776</link>
      <description>arXiv:2409.14776v1 Announce Type: new 
Abstract: The egalitarian equivalent, $ee$, of a societal distribution of outcomes with mean $m$ is the outcome level such that the evaluator is indifferent between the distribution of outcomes and a society in which everyone obtains an outcome of $ee$. For an inequality averse evaluator, $ee &lt; m$. In this paper, I extend the optimal treatment choice framework in Manski (2024) to the case where the welfare evaluation is made using egalitarian equivalent measures, and derive optimal treatment rules for the Bayesian, maximin and minimax regret inequality averse evaluators. I illustrate how the methodology operates in the context of the JobCorps education and training program for disadvantaged youth (Schochet, Burghardt, and McConnell 2008) and in Meager (2022)'s Bayesian meta analysis of the microcredit literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14776v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo Zambrano</dc:creator>
    </item>
    <item>
      <title>Non-linear dependence and Granger causality: A vine copula approach</title>
      <link>https://arxiv.org/abs/2409.15070</link>
      <description>arXiv:2409.15070v1 Announce Type: new 
Abstract: Inspired by Jang et al. (2022), we propose a Granger causality-in-the-mean test for bivariate $k-$Markov stationary processes based on a recently introduced class of non-linear models, i.e., vine copula models. By means of a simulation study, we show that the proposed test improves on the statistical properties of the original test in Jang et al. (2022), constituting an excellent tool for testing Granger causality in the presence of non-linear dependence structures. Finally, we apply our test to study the pairwise relationships between energy consumption, GDP and investment in the U.S. and, notably, we find that Granger-causality runs two ways between GDP and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15070v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto Fuentes M., Irene Crimaldi, Armando Rungi</dc:creator>
    </item>
    <item>
      <title>The continuous-time limit of quasi score-driven volatility models</title>
      <link>https://arxiv.org/abs/2409.14734</link>
      <description>arXiv:2409.14734v1 Announce Type: cross 
Abstract: This paper explores the continuous-time limit of a class of Quasi Score-Driven (QSD) models that characterize volatility. As the sampling frequency increases and the time interval tends to zero, the model weakly converges to a continuous-time stochastic volatility model where the two Brownian motions are correlated, thereby capturing the leverage effect in the market. Subsequently, we identify that a necessary condition for non-degenerate correlation is that the distribution of driving innovations differs from that of computing score, and at least one being asymmetric. We then illustrate this with two typical examples. As an application, the QSD model is used as an approximation for correlated stochastic volatility diffusions and quasi maximum likelihood estimation is performed. Simulation results confirm the method's effectiveness, particularly in estimating the correlation coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14734v1</guid>
      <category>math.PR</category>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinhao Wu, Ping He</dc:creator>
    </item>
    <item>
      <title>Detecting Multiple Structural Breaks in Systems of Linear Regression Equations with Integrated and Stationary Regressors</title>
      <link>https://arxiv.org/abs/2201.05430</link>
      <description>arXiv:2201.05430v4 Announce Type: replace 
Abstract: In this paper, we propose a two-step procedure based on the group LASSO estimator in combination with a backward elimination algorithm to detect multiple structural breaks in linear regressions with multivariate responses. Applying the two-step estimator, we jointly detect the number and location of structural breaks, and provide consistent estimates of the coefficients. Our framework is flexible enough to allow for a mix of integrated and stationary regressors, as well as deterministic terms. Using simulation experiments, we show that the proposed two-step estimator performs competitively against the likelihood-based approach (Qu and Perron, 2007; Li and Perron, 2017; Oka and Perron, 2018) in finite samples. However, the two-step estimator is computationally much more efficient. An economic application to the identification of structural breaks in the term structure of interest rates illustrates this methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05430v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karsten Schweikert</dc:creator>
    </item>
    <item>
      <title>Three Scores and 15 Years (1948-2023) of Rao's Score Test: A Brief History</title>
      <link>https://arxiv.org/abs/2406.19956</link>
      <description>arXiv:2406.19956v2 Announce Type: replace 
Abstract: Rao (1948) introduced the score test statistic as an alternative to the likelihood ratio and Wald test statistics. In spite of the optimality properties of the score statistic shown in Rao and Poti (1946), the Rao score (RS) test remained unnoticed for almost 20 years. Today, the RS test is part of the ``Holy Trinity'' of hypothesis testing and has found its place in the Statistics and Econometrics textbooks and related software. Reviewing the history of the RS test we note that remarkable test statistics proposed in the literature earlier or around the time of Rao (1948) mostly from intuition, such as Pearson (1900) goodness-fit-test, Moran (1948) I test for spatial dependence and Durbin and Watson (1950) test for serial correlation, can be given RS test statistic interpretation. At the same time, recent developments in the robust hypothesis testing under certain forms of misspecification, make the RS test an active area of research in Statistics and Econometrics. From our brief account of the history the RS test we conclude that its impact in science goes far beyond its calendar starting point with promising future research activities for many years to come.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19956v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anil K. Bera, Yannis Bilias</dc:creator>
    </item>
    <item>
      <title>A Way to Synthetic Triple Difference</title>
      <link>https://arxiv.org/abs/2409.12353</link>
      <description>arXiv:2409.12353v2 Announce Type: replace 
Abstract: This paper discusses a practical approach that combines synthetic control with triple difference to address violations of the parallel trends assumption. By transforming triple difference into a DID structure, we can apply synthetic control to a triple-difference framework, enabling more robust estimates when parallel trends are violated across multiple dimensions. The proposed procedure is applied to a real-world dataset to illustrate when and how we should apply this practice, while cautions are presented afterwards. This method contributes to improving causal inference in policy evaluations and offers a valuable tool for researchers dealing with heterogeneous treatment effects across subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12353v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Castiel Chen Zhuang</dc:creator>
    </item>
    <item>
      <title>Yurinskii's Coupling for Martingales</title>
      <link>https://arxiv.org/abs/2210.00362</link>
      <description>arXiv:2210.00362v3 Announce Type: replace-cross 
Abstract: Yurinskii's coupling is a popular theoretical tool for non-asymptotic distributional analysis in mathematical statistics and applied probability, offering a Gaussian strong approximation with an explicit error bound under easily verifiable conditions. Originally stated in $\ell^2$-norm for sums of independent random vectors, it has recently been extended both to the $\ell^p$-norm, for $1 \leq p \leq \infty$, and to vector-valued martingales in $\ell^2$-norm, under some strong conditions. We present as our main result a Yurinskii coupling for approximate martingales in $\ell^p$-norm, under substantially weaker conditions than those previously imposed. Our formulation further allows for the coupling variable to follow a more general Gaussian mixture distribution, and we provide a novel third-order coupling method which gives tighter approximations in certain settings. We specialize our main result to mixingales, martingales, and independent data, and derive uniform Gaussian mixture strong approximations for martingale empirical processes. Applications to nonparametric partitioning-based and local polynomial regression procedures are provided, alongside central limit theorems for high-dimensional martingale vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00362v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Distributional Welfare</title>
      <link>https://arxiv.org/abs/2311.15878</link>
      <description>arXiv:2311.15878v3 Announce Type: replace-cross 
Abstract: In this paper, we explore optimal treatment allocation policies that target distributional welfare. Most literature on treatment choice has considered utilitarian welfare based on the conditional average treatment effect (ATE). While average welfare is intuitive, it may yield undesirable allocations especially when individuals are heterogeneous (e.g., with outliers) - the very reason individualized treatments were introduced in the first place. This observation motivates us to propose an optimal policy that allocates the treatment based on the conditional quantile of individual treatment effects (QoTE). Depending on the choice of the quantile probability, this criterion can accommodate a policymaker who is either prudent or negligent. The challenge of identifying the QoTE lies in its requirement for knowledge of the joint distribution of the counterfactual outcomes, which is generally hard to recover even with experimental data. Therefore, we introduce minimax policies that are robust to model uncertainty. A range of identifying assumptions can be used to yield more informative policies. For both stochastic and deterministic policies, we establish the asymptotic bound on the regret of implementing the proposed policies. In simulations and two empirical applications, we compare optimal decisions based on the QoTE with decisions based on other criteria. The framework can be generalized to any setting where welfare is defined as a functional of the joint distribution of the potential outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15878v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area</title>
      <link>https://arxiv.org/abs/2401.16844</link>
      <description>arXiv:2401.16844v2 Announce Type: replace-cross 
Abstract: Congestion pricing, while adopted by many cities to alleviate traffic congestion, raises concerns about widening socioeconomic disparities due to its disproportionate impact on low-income travelers. We address this concern by proposing a new class of congestion pricing schemes that not only minimize total travel time, but also incorporate an equity objective, reducing disparities in the relative change in travel costs across populations with different incomes, following the implementation of tolls. Our analysis builds on a congestion game model with heterogeneous traveler populations. We present four pricing schemes that account for practical considerations, such as the ability to charge differentiated tolls to various traveler populations and the option to toll all or only a subset of edges in the network. We evaluate our pricing schemes in the calibrated freeway network of the San Francisco Bay Area. We demonstrate that the proposed congestion pricing schemes improve both the total travel time and the equity objective compared to the current pricing scheme.
  Our results further show that pricing schemes charging differentiated prices to traveler populations with varying value-of-time lead to a more equitable distribution of travel costs compared to those that charge a homogeneous price to all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16844v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Maheshwari, Kshitij Kulkarni, Druv Pai, Jiarui Yang, Manxi Wu, Shankar Sastry</dc:creator>
    </item>
  </channel>
</rss>
