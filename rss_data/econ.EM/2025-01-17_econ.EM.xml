<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Recovering latent linkage structures and spillover effects with structural breaks in panel data models</title>
      <link>https://arxiv.org/abs/2501.09517</link>
      <description>arXiv:2501.09517v1 Announce Type: new 
Abstract: This paper introduces a framework to analyze time-varying spillover effects in panel data. We consider panel models where a unit's outcome depends not only on its own characteristics (private effects) but also on the characteristics of other units (spillover effects). The linkage of units is allowed to be latent and may shift at an unknown breakpoint. We propose a novel procedure to estimate the breakpoint, linkage structure, spillover and private effects. We address the high-dimensionality of spillover effect parameters using penalized estimation, and estimate the breakpoint with refinement. We establish the super-consistency of the breakpoint estimator, ensuring that inferences about other parameters can proceed as if the breakpoint were known. The private effect parameters are estimated using a double machine learning method. The proposed method is applied to estimate the cross-country R&amp;D spillovers, and we find that the R&amp;D spillovers become sparser after the financial crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09517v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryo Okui, Yutao Sun, Wendun Wang</dc:creator>
    </item>
    <item>
      <title>Convergence Rates of GMM Estimators with Nonsmooth Moments under Misspecification</title>
      <link>https://arxiv.org/abs/2501.09540</link>
      <description>arXiv:2501.09540v1 Announce Type: new 
Abstract: The asymptotic behavior of GMM estimators depends critically on whether the underlying moment condition model is correctly specified. Hong and Li (2023, Econometric Theory) showed that GMM estimators with nonsmooth (non-directionally differentiable) moment functions are at best $n^{1/3}$-consistent under misspecification. Through simulations, we verify the slower convergence rate of GMM estimators in such cases. For the two-step GMM estimator with an estimated weight matrix, our results align with theory. However, for the one-step GMM estimator with the identity weight matrix, the convergence rate remains $\sqrt{n}$, even under severe misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09540v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Byunghoon Kang, Seojeong Lee, Juha Song</dc:creator>
    </item>
    <item>
      <title>Semiparametrics via parametrics and contiguity</title>
      <link>https://arxiv.org/abs/2501.09483</link>
      <description>arXiv:2501.09483v1 Announce Type: cross 
Abstract: Inference on the parametric part of a semiparametric model is no trivial task. On the other hand, if one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model; and inference may proceed by the method of maximum likelihood. Under regularity conditions, and assuming that the approximating parametric model in fact generated the data, the ensuing maximum likelihood estimator is asymptotically normal and efficient (in the approximating parametric model). Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting {`}semiparametric{'} estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous. Consequently, we are able to move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the sieve literature by being more specific about the approximating parametric models, by working under these when treating the parametric models, and by taking advantage of the mutual contiguity between the parametric and semiparametric models to lift conclusions about the former to conclusions about the latter. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09483v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Lee, Emil A. Stoltenberg, Per A. Mykland</dc:creator>
    </item>
    <item>
      <title>Improving the Finite Sample Estimation of Average Treatment Effects using Double/Debiased Machine Learning with Propensity Score Calibration</title>
      <link>https://arxiv.org/abs/2409.04874</link>
      <description>arXiv:2409.04874v2 Announce Type: replace 
Abstract: In the last decade, machine learning techniques have gained popularity for estimating causal effects. One machine learning approach that can be used for estimating an average treatment effect is Double/debiased machine learning (DML) (Chernozhukov et al., 2018). This approach uses a double-robust score function that relies on the prediction of nuisance functions, such as the propensity score, which is the probability of treatment assignment conditional on covariates. Estimators relying on double-robust score functions are highly sensitive to errors in propensity score predictions. Machine learners increase the severity of this problem as they tend to over- or underestimate these probabilities. Several calibration approaches have been proposed to improve probabilistic forecasts of machine learners. This paper investigates the use of probability calibration approaches within the DML framework. Simulation results demonstrate that calibrating propensity scores may significantly reduces the root mean squared error of DML estimates of the average treatment effect in finite samples. We showcase it in an empirical example and provide conditions under which calibration does not alter the asymptotic properties of the DML estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04874v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniele Ballinari, Nora Bearth</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Multiple Events</title>
      <link>https://arxiv.org/abs/2409.05184</link>
      <description>arXiv:2409.05184v2 Announce Type: replace 
Abstract: This paper introduces a two-stage DiD design to address bias from confounding events correlated with the target event. The first DiD estimates the combined effects of both treatments with a control group not yet treated nor confounded. The second DiD isolates the target treatment effects using a parallel treatment effect assumption and a control group simultaneously treated but not yet confounded. I revisit the effect of minimum wage hikes on teen employment. I find that the short-term effect reduces by two thirds after controlling for the Medicaid expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05184v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin-Tung Tsai</dc:creator>
    </item>
    <item>
      <title>Estimating Sequential Search Models Based on a Partial Ranking Representation</title>
      <link>https://arxiv.org/abs/2501.07514</link>
      <description>arXiv:2501.07514v2 Announce Type: replace 
Abstract: Consumers are increasingly shopping online, and more and more datasets documenting consumer search are becoming available. While sequential search models provide a framework for utilizing such data, they present empirical challenges. A key difficulty arises from the inequality conditions implied by these models, which depend on multiple unobservables revealed during the search process and necessitate solving or simulating high-dimensional integrals for likelihood-based estimation methods. This paper introduces a novel representation of inequalities implied by a broad class of sequential search models, demonstrating that the empirical content of such models can be effectively captured through a specific partial ranking of available actions. This representation reduces the complexity caused by unobservables and provides a tractable expression for joint probabilities. Leveraging this insight, we propose a GHK-style simulation-based likelihood estimator that is simpler to implement than existing ones. It offers greater flexibility for handling incomplete search data, incorporating additional ranking information, and accommodating complex search processes, including those involving product discovery. We show that the estimator achieves robust performance while maintaining relatively low computational costs, making it a practical and versatile tool for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07514v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tinghan Zhang</dc:creator>
    </item>
    <item>
      <title>disco: Distributional Synthetic Controls</title>
      <link>https://arxiv.org/abs/2501.07550</link>
      <description>arXiv:2501.07550v2 Announce Type: replace 
Abstract: The method of synthetic controls is widely used for evaluating causal effects of policy changes in settings with observational data. Often, researchers aim to estimate the causal impact of policy interventions on a treated unit at an aggregate level while also possessing data at a finer granularity. In this article, we introduce the new disco command, which implements the Distributional Synthetic Controls method introduced in Gunsilius (2023). This command allows researchers to construct entire synthetic distributions for the treated unit based on an optimally weighted average of the distributions of the control units. Several aggregation schemes are provided to facilitate clear reporting of the distributional effects of the treatment. The package offers both quantile-based and CDF-based approaches, comprehensive inference procedures via bootstrap and permutation methods, and visualization capabilities. We empirically illustrate the use of the package by replicating the results in Van Dijcke et al. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07550v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gunsilius, David Van Dijcke</dc:creator>
    </item>
  </channel>
</rss>
