<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The permuted score test for robust differential expression analysis</title>
      <link>https://arxiv.org/abs/2501.03530</link>
      <description>arXiv:2501.03530v1 Announce Type: new 
Abstract: Negative binomial (NB) regression is a popular method for identifying differentially expressed genes in genomics data, such as bulk and single-cell RNA sequencing data. However, NB regression makes stringent parametric and asymptotic assumptions, which can fail to hold in practice, leading to excess false positive and false negative results. We propose the permuted score test, a new strategy for robust regression based on permuting score test statistics. The permuted score test provably controls type-I error across a much broader range of settings than standard NB regression while nevertheless approximately matching standard NB regression with respect to power (when the assumptions of standard NB regression obtain) and computational efficiency. We accelerate the permuted score test by leveraging emerging techniques for sequential Monte-Carlo testing and novel algorithms for efficiently computing GLM score tests. We apply the permuted score test to real and simulated RNA sequencing data, finding that it substantially improves upon the error control of existing NB regression implementations, including DESeq2. The permuted score test could enhance the reliability of differential expression analysis across diverse biological contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Barry, Ziang Niu, Eugene Katsevich, Xihong Lin</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit tests for spatial point processes: A review</title>
      <link>https://arxiv.org/abs/2501.03732</link>
      <description>arXiv:2501.03732v1 Announce Type: new 
Abstract: In this review, the state-of-the-art for goodness-of-fit testing for spatial point processes is summarized. Test statistics based on classical functional summary statistics and recent contributions from topological data analysis are considered. Different approaches to derive test statistics from functional summary statistics are categorized in a unifying notation. We discuss additional aspects such as the graphical representation in terms of global envelopes and the selection of the parameters in the individual tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03732v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Fend, Claudia Redenbach</dc:creator>
    </item>
    <item>
      <title>Spline Quantile Regression</title>
      <link>https://arxiv.org/abs/2501.03883</link>
      <description>arXiv:2501.03883v1 Announce Type: new 
Abstract: Quantile regression is a powerful tool capable of offering a richer view of the data as compared to linear-squares regression. Quantile regression is typically performed individually on a few quantiles or a grid of quantiles without considering the similarity of the underlying regression coefficients at nearby quantiles. When needed, an ad hoc post-processing procedure such as kernel smoothing is employed to smooth the estimated coefficients across quantiles and thereby improve the performance of these estimates. This paper introduces a new method, called spline quantile regression (SQR), that unifies quantile regression with quantile smoothing and jointly estimates the regression coefficients across quantiles as smoothing splines. We discuss the computation of the SQR solution as a linear program (LP) using an interior-point algorithm. We also experiment with some gradient algorithms that require less memory than the LP algorithm. The performance of the SQR method and these algorithms is evaluated using simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03883v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li, Nimrod Megiddo</dc:creator>
    </item>
    <item>
      <title>Scalable calibration for partially observed individual-based epidemic models through categorical approximations</title>
      <link>https://arxiv.org/abs/2501.03950</link>
      <description>arXiv:2501.03950v1 Announce Type: new 
Abstract: The computational cost of exact likelihood evaluation for partially observed and highly-heterogeneous individual-based models grows exponentially with the population size, therefore inference relies on approximations. Sampling-based approaches to this problem such as Sequential Monte Carlo or Approximate Bayesian Computation usually require simulation of every individual in the population multiple times and are heavily reliant on the design of bespoke proposal distributions or summary statistics, and can still scale poorly with population size. To overcome this, we propose a deterministic recursive approach to approximating the likelihood function using categorical distributions. The resulting algorithm has a computational cost as low as linear in the population size and is amenable to automatic differentiation, leading to simple algorithms for maximizing this approximate likelihood or sampling from posterior distributions. We prove consistency of the maximum approximate likelihood estimator of model parameters. We empirically test our approach on a range of models with various flavors of heterogeneity: different sets of disease states, individual-specific susceptibility and infectivity, spatial interaction mechanisms, under-reporting and mis-reporting. We demonstrate strong calibration performance, in terms of log-likelihood variance and ground truth recovery, and computational advantages over competitor methods. We conclude by illustrating the effectiveness of our approach in a real-world large-scale application using Foot-and-Mouth data from the 2001 outbreak in the United Kingdom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03950v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Rimella, Nick Whiteley, Chris Jewell, Paul Fearnhead, Michael Whitehouse</dc:creator>
    </item>
    <item>
      <title>Rapid Experimentation with Python Considering Optional and Hierarchical Inputs</title>
      <link>https://arxiv.org/abs/2501.03398</link>
      <description>arXiv:2501.03398v1 Announce Type: cross 
Abstract: Space-filling experimental design techniques are commonly used in many computer modeling and simulation studies to explore the effects of inputs on outputs. This research presents raxpy, a Python package that leverages expressive annotation of Python functions and classes to simplify space-filling experimentation. It incorporates code introspection to derive a Python function's input space and novel algorithms to automate the design of space-filling experiments for spaces with optional and hierarchical input dimensions. In this paper, we review the criteria for design evaluation given these types of dimensions and compare the proposed algorithms with numerical experiments. The results demonstrate the ability of the proposed algorithms to create improved space-filling experiment designs. The package includes support for parallelism and distributed execution. raxpy is available as free and open-source software under a MIT license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03398v1</guid>
      <category>cs.MS</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Ranly, Torrey Wagner</dc:creator>
    </item>
    <item>
      <title>Advanced Tutorial: Label-Efficient Two-Sample Tests</title>
      <link>https://arxiv.org/abs/2501.03568</link>
      <description>arXiv:2501.03568v1 Announce Type: cross 
Abstract: Hypothesis testing is a statistical inference approach used to determine whether data supports a specific hypothesis. An important type is the two-sample test, which evaluates whether two sets of data points are from identical distributions. This test is widely used, such as by clinical researchers comparing treatment effectiveness. This tutorial explores two-sample testing in a context where an analyst has many features from two samples, but determining the sample membership (or labels) of these features is costly. In machine learning, a similar scenario is studied in active learning. This tutorial extends active learning concepts to two-sample testing within this \textit{label-costly} setting while maintaining statistical validity and high testing power. Additionally, the tutorial discusses practical applications of these label-efficient two-sample tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03568v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weizhi Li, Visar Berisha, Gautam Dasarathy</dc:creator>
    </item>
    <item>
      <title>Class-Balance Bias in Regularized Regression</title>
      <link>https://arxiv.org/abs/2501.03821</link>
      <description>arXiv:2501.03821v1 Announce Type: cross 
Abstract: Regularized models are often sensitive to the scales of the features in the data and it has therefore become standard practice to normalize (center and scale) the features before fitting the model. But there are many different ways to normalize the features and the choice may have dramatic effects on the resulting model. In spite of this, there has so far been no research on this topic. In this paper, we begin to bridge this knowledge gap by studying normalization in the context of lasso, ridge, and elastic net regression. We focus on normal and binary features and show that the class balances of binary features directly influences the regression coefficients and that this effect depends on the combination of normalization and regularization methods used. We demonstrate that this effect can be mitigated by scaling binary features with their variance in the case of the lasso and standard deviation in the case of ridge regression, but that this comes at the cost of increased variance. For the elastic net, we show that scaling the penalty weights, rather than the features, can achieve the same effect. Finally, we also tackle mixes of binary and normal features as well as interactions and provide some initial results on how to normalize features in these cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03821v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Larsson, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>Sequentializing a Test: Anytime Validity is Free</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v1 Announce Type: cross 
Abstract: An anytime valid sequential test permits us to peek at observations as they arrive. This means we can stop, continue or adapt the testing process based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe that this benefit must be paid for in terms of power when compared to a conventional test that waits until all $N$ observations have arrived. Our key contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. In addition, we show that the value of the sequential test before a rejection is attained can be directly used as a significance level for a subsequent test. We illustrate this for the $z$-test. There, we find that the current state-of-the-art based on log-optimal $e$-values can be obtained as a special limiting case that replicates a $z$-test with level $\alpha \to 0$ as $N \to \infty$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Bayesian views of generalized additive modelling</title>
      <link>https://arxiv.org/abs/1902.01330</link>
      <description>arXiv:1902.01330v4 Announce Type: replace 
Abstract: Generalized additive models (GAMs) are a commonly used, flexible framework applied to many problems in statistical ecology. GAMs are often considered to be a purely frequentist framework (`generalized linear models with wiggly bits'), however links between frequentist and Bayesian approaches to these models were highlighted early on in the literature. Bayesian thinking underlies many parts of the implementation in the popular R package \texttt{mgcv} as well as in GAM theory more generally. This article aims to highlight useful links (and differences) between Bayesian and frequentist approaches to smoothing, and their practical applications in ecology (with an \texttt{mgcv}-centric viewpoint). Here I give some background for these results then move onto two important topics for quantitative ecologists: term/model selection and uncertainty estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:1902.01330v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David L. Miller</dc:creator>
    </item>
    <item>
      <title>On Doubly Robust Inference for Double Machine Learning in Semiparametric Regression</title>
      <link>https://arxiv.org/abs/2107.06124</link>
      <description>arXiv:2107.06124v3 Announce Type: replace 
Abstract: Due to concerns about parametric model misspecification, there is interest in using machine learning to adjust for confounding when evaluating the causal effect of an exposure on an outcome. Unfortunately, exposure effect estimators that rely on machine learning predictions are generally subject to so-called plug-in bias, which can render naive $p$-values and confidence intervals invalid. Progress has been made via proposals like targeted minimum loss estimation and more recently double machine learning, which rely on learning the conditional mean of both the outcome and exposure. Valid inference can then be obtained so long as both predictions converge (sufficiently fast) to the truth. Focusing on partially linear regression models, we show that a specific implementation of the machine learning techniques can yield exposure effect estimators that have small bias even when one of the first-stage predictions does not converge to the truth. The resulting tests and confidence intervals are doubly robust. We also show that the proposed estimators may fail to be regular when only one nuisance parameter is consistently estimated; nevertheless, we observe in simulation studies that our proposal can lead to reduced bias and improved confidence interval coverage in moderate-to-large samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.06124v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2024</arxiv:journal_reference>
      <dc:creator>Oliver Dukes, Stijn Vansteelandt, David Whitney</dc:creator>
    </item>
    <item>
      <title>Using negative controls to identify causal effects with invalid instrumental variables</title>
      <link>https://arxiv.org/abs/2204.04119</link>
      <description>arXiv:2204.04119v5 Announce Type: replace 
Abstract: Many proposals for the identification of causal effects require an instrumental variable that satisfies strong, untestable unconfoundedness and exclusion restriction assumptions. In this paper, we show how one can potentially identify causal effects under violations of these assumptions by harnessing a negative control population or outcome. This strategy allows one to leverage sup-populations for whom the exposure is degenerate, and requires that the instrument-outcome association satisfies a certain parallel trend condition. We develop the semiparametric efficiency theory for a general instrumental variable model, and obtain a multiply robust, locally efficient estimator of the average treatment effect in the treated. The utility of the estimators is demonstrated in simulation studies and an analysis of the Life Span Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.04119v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Dukes, David B. Richardson, Zachary Shahn, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning for Covariate Shifts</title>
      <link>https://arxiv.org/abs/2307.04527</link>
      <description>arXiv:2307.04527v4 Announce Type: replace 
Abstract: We present machine learning estimators for causal and predictive parameters under covariate shift, where covariate distributions differ between training and target populations. One such parameter is the average effect of a policy that alters the covariate distribution, such as a treatment modifying surrogate covariates used to predict long-term outcomes. Another example is the average treatment effect for a population with a shifted covariate distribution, like the effect of a policy on the treated group.
  We propose a debiased machine learning method to estimate a broad class of these parameters in a statistically reliable and automatic manner. Our method eliminates regularization biases arising from the use of machine learning tools in high-dimensional settings, relying solely on the parameter's defining formula. It employs data fusion by combining samples from target and training data to eliminate biases. We prove that our estimator is consistent and asymptotically normal. Computational experiments and an empirical study on the impact of minimum wage increases on teen employment--using the difference-in-differences framework with unconfoundedness--demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04527v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Michael Newey, Whitney K Newey, Rahul Singh, Vasilis Srygkanis</dc:creator>
    </item>
    <item>
      <title>Modeling Insurance Claims using Bayesian Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2311.11487</link>
      <description>arXiv:2311.11487v2 Announce Type: replace 
Abstract: The prediction of future insurance claims based on observed risk factors, or covariates, help the actuary set insurance premiums. Typically, actuaries use parametric regression models to predict claims based on the covariate information. Such models assume the same functional form tying the response to the covariates for each data point. These models are not flexible enough and can fail to accurately capture at the individual level, the relationship between the covariates and the claims frequency and severity, which are often multimodal, highly skewed, and heavy-tailed. In this article, we explore the use of Bayesian nonparametric (BNP) regression models to predict claims frequency and severity based on covariates. In particular, we model claims frequency as a mixture of Poisson regression, and the logarithm of claims severity as a mixture of normal regression. We use the Dirichlet process (DP) and Pitman-Yor process (PY) as a prior for the mixing distribution over the regression parameters. Unlike parametric regression, such models allow each data point to have its individual parameters, making them highly flexible, resulting in improved prediction accuracy. We describe model fitting using MCMC and illustrate their applicability using French motor insurance claims data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11487v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mostafa Shams Esfand Abadi, Kaushik Ghosh</dc:creator>
    </item>
    <item>
      <title>Robust likelihood ratio tests for composite nulls and alternatives</title>
      <link>https://arxiv.org/abs/2408.14015</link>
      <description>arXiv:2408.14015v3 Announce Type: replace 
Abstract: We propose an e-value based framework for testing composite nulls against composite alternatives when an $\epsilon$ fraction of the data can be arbitrarily corrupted. Our tests are inherently sequential, being valid at arbitrary data-dependent stopping times, but they are new even for fixed sample sizes, giving type-I error control without any regularity conditions. We achieve this by modifying and extending a proposal by Huber (1965) in the point null versus point alternative case. Our test statistic is a nonnegative supermartingale under the null, even with a sequentially adaptive contamination model where the conditional distribution of each observation given the past data lies within an $\epsilon$ (total variation) ball of the null. The test is powerful within an $\epsilon$ ball of the alternative. As a consequence, one obtains anytime-valid p-values that enable continuous monitoring of the data, and adaptive stopping. We analyze the growth rate of our test supermartingale and demonstrate that as $\epsilon\to 0$, it approaches a certain Kullback-Leibler divergence between the null and alternative, which is the optimal non-robust growth rate. A key step is the derivation of a robust Reverse Information Projection (RIPr). Simulations validate the theory and demonstrate excellent practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14015v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Single CASANOVA? Not in multiple comparisons</title>
      <link>https://arxiv.org/abs/2410.21098</link>
      <description>arXiv:2410.21098v3 Announce Type: replace 
Abstract: When comparing multiple groups in clinical trials, we are not only interested in whether there is a difference between any groups but rather the location. Such research questions lead to testing multiple individual hypotheses. To control the familywise error rate (FWER), we must apply some corrections or introduce tests that control the FWER by design. In the case of time-to-event data, a Bonferroni-corrected log-rank test is commonly used. This approach has two significant drawbacks: (i) it loses power when the proportional hazards assumption is violated [1] and (ii) the correction generally leads to a lower power, especially when the test statistics are not independent [2]. We propose two new tests based on combined weighted log-rank tests. One as a simple multiple contrast test of weighted log-rank tests and one as an extension of the so-called CASANOVA test [3]. The latter was introduced for factorial designs. We propose a new multiple contrast test based on the CASANOVA approach. Our test promises to be more powerful under crossing hazards and eliminates the need for additional p-value correction. We assess the performance of our tests through extensive Monte Carlo simulation studies covering both proportional and non-proportional hazard scenarios. Finally, we apply the new and reference methods to a real-world data example. The new approaches control the FWER and show reasonable power in all scenarios. They outperform the adjusted approaches in some non-proportional settings in terms of power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21098v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ina Dormuth, Carolin Herrmann, Frank Konietschke, Markus Pauly, Matthias Wirth, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>Modelling Loss of Complexity in Intermittent Time Series and its Application</title>
      <link>https://arxiv.org/abs/2411.14635</link>
      <description>arXiv:2411.14635v2 Announce Type: replace 
Abstract: In this paper, we developed a novel method of nonparametric relative entropy (RlEn) for modelling loss of complexity in intermittent time series. The method consists of two steps. We first fit a nonlinear autoregressive model to each intermittent time series, where the corresponding lag order and the loss of complexity are determined by Bayesian Information Criterion (BIC) and relative entropy respectively. Then, change-points in the complexity are detected by a cumulative sum (CUSUM) based statistic. Compared to approximate entropy (ApEn), a popular method in literature, the performance of RlEn was assessed by simulations in terms of (1) ability to localize complexity change-points in intermittent time series; (2) ability to faithfully estimate underlying nonlinear models. The performance of the proposal was then examined in a real analysis of fatigue-induced changes in the complexity of human motor outputs. The results showed that the proposed method outperformed the ApEn in accurately detecting changes of complexity in intermittent time series segments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14635v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Jian Zhang, Samantha L. Winter, Mark Burnley</dc:creator>
    </item>
    <item>
      <title>Optimal Correlation for Bernoulli Trials with Covariates</title>
      <link>https://arxiv.org/abs/2412.03827</link>
      <description>arXiv:2412.03827v2 Announce Type: replace 
Abstract: Given covariates for $n$ units, each of which is to receive a treatment with probability $1/2$, we study the question of how best to correlate their treatment assignments to minimize the variance of the IPW estimator of the average treatment effect. Past work by \cite{bai2022} found that the optimal stratified experiment is a matched-pair design, where the matching depends on oracle knowledge of the distributions of potential outcomes given covariates. We study the strictly broader class of all admissible correlation structures, for which \cite{cytrynbaum2023} recently showed that the optimal design is to divide the units into two clusters and uniformly assign treatment to exactly one of them. This design can be computed by solving a 0-1 knapsack problem that uses the same oracle information. We derive a novel proof of this fact using a result about admissible Bernoulli correlations. We also show how to construct a shift-invariant version of the design that still improves on the optimal stratified design by ensuring that exactly half of the units are treated. A method with just two clusters is not robust to a bad proxy for the oracle, and we mitigate this with a hybrid that uses $O(n^\alpha)$ clusters for $0&lt;\alpha&lt;1$. Under certain assumptions, we also derive a CLT for the IPW estimator under these designs and a consistent estimator of the variance. We compare the proposed designs to the optimal stratified design in simulated examples and find strong performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03827v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Morrison, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Efficient Generative Modeling via Penalized Optimal Transport Network</title>
      <link>https://arxiv.org/abs/2402.10456</link>
      <description>arXiv:2402.10456v2 Announce Type: replace-cross 
Abstract: The generation of synthetic data with distributions that faithfully emulate the underlying data-generating mechanism holds paramount significance. Wasserstein Generative Adversarial Networks (WGANs) have emerged as a prominent tool for this task; however, due to the delicate equilibrium of the minimax formulation and the instability of Wasserstein distance in high dimensions, WGAN often manifests the pathological phenomenon of mode collapse. This results in generated samples that converge to a restricted set of outputs and fail to adequately capture the tail behaviors of the true distribution. Such limitations can lead to serious downstream consequences. To this end, we propose the Penalized Optimal Transport Network (POTNet), a versatile deep generative model based on the marginally-penalized Wasserstein (MPW) distance. Through the MPW distance, POTNet effectively leverages low-dimensional marginal information to guide the overall alignment of joint distributions. Furthermore, our primal-based framework enables direct evaluation of the MPW distance, thus eliminating the need for a critic network. This formulation circumvents training instabilities inherent in adversarial approaches and avoids the need for extensive parameter tuning. We derive a non-asymptotic bound on the generalization error of the MPW loss and establish convergence rates of the generative distribution learned by POTNet. Our theoretical analysis together with extensive empirical evaluations demonstrate the superior performance of POTNet in accurately capturing underlying data structures, including their tail behaviors and minor modalities. Moreover, our model achieves orders of magnitude speedup during the sampling stage compared to state-of-the-art alternatives, which enables computationally efficient large-scale synthetic data generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10456v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhui Sophia Lu, Chenyang Zhong, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>Ensemble learning for predictive uncertainty estimation with application to the correction of satellite precipitation products</title>
      <link>https://arxiv.org/abs/2403.10567</link>
      <description>arXiv:2403.10567v2 Announce Type: replace-cross 
Abstract: Predictions in the form of probability distributions are crucial for effective decision-making. Quantile regression enables such predictions within spatial prediction settings that aim to create improved precipitation datasets by merging remote sensing and gauge data. However, ensemble learning of quantile regression algorithms remains unexplored in this context and, at the same time, it has not been substantially developed so far in the broader machine learning research landscape. Here, we introduce nine quantile-based ensemble learners and address the afore-mentioned gap in precipitation dataset creation by presenting the first application of these learners to large precipitation datasets. We employed a novel feature engineering strategy, reducing predictors to distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six ensemble learning and three simple methods (mean, median, best combiner), combining six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). These algorithms serve as both base learners and combiners within different ensemble learning methods. We evaluated performance against a reference method (QR) using quantile scoring functions in a large dataset comprising 15 years of monthly gauge-measured and satellite precipitation in the contiguous United States (CONUS). Ensemble learning with QR and QRNN yielded the best results across quantile levels ranging from 0.025 to 0.975, outperforming the reference method by 3.91% to 8.95%. This demonstrates the potential of ensemble learning to improve probabilistic spatial predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10567v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</dc:creator>
    </item>
    <item>
      <title>Gradient descent inference in empirical risk minimization</title>
      <link>https://arxiv.org/abs/2412.09498</link>
      <description>arXiv:2412.09498v2 Announce Type: replace-cross 
Abstract: Gradient descent is one of the most widely used iterative algorithms in modern statistical learning. However, its precise algorithmic dynamics in high-dimensional settings remain only partially understood, which has therefore limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic distributional characterization of gradient descent iterates in a broad class of empirical risk minimization problems, in the so-called mean-field regime where the sample size is proportional to the signal dimension. Our non-asymptotic state evolution theory holds for both general non-convex loss functions and non-Gaussian data, and reveals the central role of two Onsager correction matrices that precisely characterize the non-trivial dependence among all gradient descent iterates in the mean-field regime.
  Although the Onsager correction matrices are typically analytically intractable, our state evolution theory facilitates a generic gradient descent inference algorithm that consistently estimates these matrices across a broad class of models. Leveraging this algorithm, we show that the state evolution can be inverted to construct (i) data-driven estimators for the generalization error of gradient descent iterates and (ii) debiased gradient descent iterates for inference of the unknown signal. Detailed applications to two canonical models--linear regression and (generalized) logistic regression--are worked out to illustrate model-specific features of our general theory and inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09498v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Xiaocong Xu</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v2 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
  </channel>
</rss>
