<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust Mean Squared Prediction Error Estimators of EBLUP of a Small Area Mean Under the Fay-Herriot Model</title>
      <link>https://arxiv.org/abs/2409.16409</link>
      <description>arXiv:2409.16409v1 Announce Type: new 
Abstract: In this paper we derive a second-order unbiased (or nearly unbiased) mean squared prediction error (MSPE) estimator of empirical best linear unbiased predictor (EBLUP) of a small area mean for a non-normal extension to the well-known Fay-Herriot model. Specifically, we derive our MSPE estimator essentially assuming certain moment conditions on both the sampling and random effects distributions. The normality-based Prasad-Rao MSPE estimator has a surprising robustness property in that it remains second-order unbiased under the non-normality of random effects when a simple method-of-moments estimator is used for the variance component and the sampling error distribution is normal. We show that the normality-based MSPE estimator is no longer second-order unbiased when the sampling error distribution is non-normal or when the Fay-Herriot moment method is used to estimate the variance component, even when the sampling error distribution is normal. It is interesting to note that when the simple method-of moments estimator is used for the variance component, our proposed MSPE estimator does not require the estimation of kurtosis of the random effects. Results of a simulation study on the accuracy of the proposed MSPE estimator, under non-normality of both sampling and random effects distributions, are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16409v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Chen, P. Lahiri, J. N. K. Rao</dc:creator>
    </item>
    <item>
      <title>Double-Estimation-Friendly Inference for High Dimensional Misspecified Measurement Error Models</title>
      <link>https://arxiv.org/abs/2409.16463</link>
      <description>arXiv:2409.16463v1 Announce Type: new 
Abstract: In this paper, we introduce an innovative testing procedure for assessing individual hypotheses in high-dimensional linear regression models with measurement errors. This method remains robust even when either the X-model or Y-model is misspecified. We develop a double robust score function that maintains a zero expectation if one of the models is incorrect, and we construct a corresponding score test. We first show the asymptotic normality of our approach in a low-dimensional setting, and then extend it to the high-dimensional models. Our analysis of high-dimensional settings explores scenarios both with and without the sparsity condition, establishing asymptotic normality and non-trivial power performance under local alternatives. Simulation studies and real data analysis demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16463v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Xu Guo, Runze Li, Songshan Yang, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Max Statistics for High-Dimensional Inference</title>
      <link>https://arxiv.org/abs/2409.16683</link>
      <description>arXiv:2409.16683v1 Announce Type: new 
Abstract: Although much progress has been made in the theory and application of bootstrap approximations for max statistics in high dimensions, the literature has largely been restricted to cases involving light-tailed data. To address this issue, we propose an approach to inference based on robust max statistics, and we show that their distributions can be accurately approximated via bootstrapping when the data are both high-dimensional and heavy-tailed. In particular, the data are assumed to satisfy an extended version of the well-established $L^{4}$-$L^2$ moment equivalence condition, as well as a weak variance decay condition. In this setting, we show that near-parametric rates of bootstrap approximation can be achieved in the Kolmogorov metric, independently of the data dimension. Moreover, this theoretical result is complemented by favorable empirical results involving both synthetic data and an application to financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16683v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingshuo Liu, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Conditional Testing based on Localized Conformal p-values</title>
      <link>https://arxiv.org/abs/2409.16829</link>
      <description>arXiv:2409.16829v1 Announce Type: new 
Abstract: In this paper, we address conditional testing problems through the conformal inference framework. We define the localized conformal p-values by inverting prediction intervals and prove their theoretical properties. These defined p-values are then applied to several conditional testing problems to illustrate their practicality. Firstly, we propose a conditional outlier detection procedure to test for outliers in the conditional distribution with finite-sample false discovery rate (FDR) control. We also introduce a novel conditional label screening problem with the goal of screening multivariate response variables and propose a screening procedure to control the family-wise error rate (FWER). Finally, we consider the two-sample conditional distribution test and define a weighted U-statistic through the aggregation of localized p-values. Numerical simulations and real-data examples validate the superior performance of our proposed strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16829v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyang Wu, Lin Lu, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>A flexiable approach: variable selection procedures with multilayer FDR control via e-values</title>
      <link>https://arxiv.org/abs/2409.17039</link>
      <description>arXiv:2409.17039v1 Announce Type: new 
Abstract: Consider a scenario where a large number of explanatory features targeting a response variable are analyzed, such that these features are partitioned into different groups according to their domain-specific structures. Furthermore, there may be several such partitions. Such multiple partitions may exist in many real-life scenarios. One such example is spatial genome-wide association studies. Researchers may not only be interested in identifying the features relevant to the response but also aim to determine the relevant groups within each partition. A group is considered relevant if it contains at least one relevant feature. To ensure the replicability of the findings at various resolutions, it is essential to provide false discovery rate (FDR) control for findings at multiple layers simultaneously. This paper presents a general approach that leverages various existing controlled selection procedures to generate more stable results using multilayer FDR control. The key contributions of our proposal are the development of a generalized e-filter that provides multilayer FDR control and the construction of a specific type of generalized e-values to evaluate feature importance. A primary application of our method is an improved version of Data Splitting (DS), called the eDS-filter. Furthermore, we combine the eDS-filter with the version of the group knockoff filter (gKF), resulting in a more flexible approach called the eDS+gKF filter. Simulation studies demonstrate that the proposed methods effectively control the FDR at multiple levels while maintaining or even improving power compared to other approaches. Finally, we apply the proposed method to analyze HIV mutation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17039v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyao Yu, Ruixing Ming, Min Xiao, Zhanfeng Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Bivariate Conway-Maxwell-Poisson Regression Model for Correlated Count Data in Sports</title>
      <link>https://arxiv.org/abs/2409.17129</link>
      <description>arXiv:2409.17129v1 Announce Type: new 
Abstract: Count data play a crucial role in sports analytics, providing valuable insights into various aspects of the game. Models that accurately capture the characteristics of count data are essential for making reliable inferences. In this paper, we propose the use of the Conway-Maxwell-Poisson (CMP) model for analyzing count data in sports. The CMP model offers flexibility in modeling data with different levels of dispersion. Here we consider a bivariate CMP model that models the potential correlation between home and away scores by incorporating a random effect specification. We illustrate the advantages of the CMP model through simulations. We then analyze data from baseball and soccer games before, during, and after the COVID-19 pandemic. The performance of our proposed CMP model matches or outperforms standard Poisson and Negative Binomial models, providing a good fit and an accurate estimation of the observed effects in count data with any level of dispersion. The results highlight the robustness and flexibility of the CMP model in analyzing count data in sports, making it a suitable default choice for modeling a diverse range of count data types in sports, where the data dispersion may vary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17129v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mauro Florez, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Towards Representation Learning for Weighting Problems in Design-Based Causal Inference</title>
      <link>https://arxiv.org/abs/2409.16407</link>
      <description>arXiv:2409.16407v1 Announce Type: cross 
Abstract: Reweighting a distribution to minimize a distance to a target distribution is a powerful and flexible strategy for estimating a wide range of causal effects, but can be challenging in practice because optimal weights typically depend on knowledge of the underlying data generating process. In this paper, we focus on design-based weights, which do not incorporate outcome information; prominent examples include prospective cohort studies, survey weighting, and the weighting portion of augmented weighting estimators. In such applications, we explore the central role of representation learning in finding desirable weights in practice. Unlike the common approach of assuming a well-specified representation, we highlight the error due to the choice of a representation and outline a general framework for finding suitable representations that minimize this error. Building on recent work that combines balancing weights and neural networks, we propose an end-to-end estimation procedure that learns a flexible representation, while retaining promising theoretical properties. We show that this approach is competitive in a range of common causal inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16407v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Clivio, Avi Feller, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>Multiple Outlier Detection in Samples with Exponential &amp; Pareto Tails: Redeeming the Inward Approach &amp; Detecting Dragon Kings</title>
      <link>https://arxiv.org/abs/1507.08689</link>
      <description>arXiv:1507.08689v2 Announce Type: replace 
Abstract: We introduce two ratio-based robust test statistics, max-robust-sum (MRS) and sum-robust-sum (SRS), designed to enhance the robustness of outlier detection in samples with exponential or Pareto tails. We also reintroduce the inward sequential testing method -- formerly relegated since the introduction of outward testing -- and show that MRS and SRS tests reduce susceptibility of the inward approach to masking, making the inward test as powerful as, and potentially less error-prone than, outward tests. Moreover, inward testing does not require the complicated type I error control of outward tests. A comprehensive comparison of the test statistics is done, considering performance of the proposed tests in both block and sequential tests, and contrasting their performance with classical test statistics across various data scenarios. In five case studies -- financial crashes, nuclear power generation accidents, stock market returns, epidemic fatalities, and city sizes -- significant outliers are detected and related to the concept of `Dragon King' events, defined as meaningful outliers that arise from a unique generating mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:1507.08689v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Sornette, Ran Wei</dc:creator>
    </item>
    <item>
      <title>Synthetic Controls for Experimental Design</title>
      <link>https://arxiv.org/abs/2108.02196</link>
      <description>arXiv:2108.02196v4 Announce Type: replace 
Abstract: This article studies experimental design in settings where the experimental units are large aggregate entities (e.g., markets), and only one or a small number of units can be exposed to the treatment. In such settings, randomization of the treatment may result in treated and control groups with very different characteristics at baseline, inducing biases. We propose a variety of experimental non-randomized synthetic control designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) that select the units to be treated, as well as the untreated units to be used as a control group. Average potential outcomes are estimated as weighted averages of the outcomes of treated units for potential outcomes with treatment, and weighted averages the outcomes of control units for potential outcomes without treatment. We analyze the properties of estimators based on synthetic control designs and propose new inferential techniques. We show that in experimental settings with aggregate units, synthetic control designs can substantially reduce estimation biases in comparison to randomization of the treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.02196v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Abadie, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Graph-constrained Analysis for Multivariate Functional Data</title>
      <link>https://arxiv.org/abs/2209.06294</link>
      <description>arXiv:2209.06294v3 Announce Type: replace 
Abstract: Functional Gaussian graphical models (GGM) used for analyzing multivariate functional data customarily estimate an unknown graphical model representing the conditional relationships between the functional variables. However, in many applications of multivariate functional data, the graph is known and existing functional GGM methods cannot preserve a given graphical constraint. In this manuscript, we demonstrate how to conduct multivariate functional analysis that exactly conforms to a given inter-variable graph. We first show the equivalence between partially separable functional GGM and graphical Gaussian processes (GP), proposed originally for constructing optimal covariance functions for multivariate spatial data that retain the conditional independence relations in a given graphical model. The theoretical connection help design a new algorithm that leverages Dempster's covariance selection to calculate the maximum likelihood estimate of the covariance function for multivariate functional data under graphical constraints. We also show that the finite term truncation of functional GGM basis expansion used in practice is equivalent to a low-rank graphical GP, which is known to oversmooth marginal distributions. To remedy this, we extend our algorithm to better preserve marginal distributions while still respecting the graph and retaining computational scalability. The insights obtained from the new results presented in this manuscript will help practitioners better understand the relationship between these graphical models and in deciding on the appropriate method for their specific multivariate data analysis task. The benefits of the proposed algorithms are illustrated using empirical experiments and an application to functional modeling of neuroimaging data using the connectivity graph among regions of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06294v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debangan Dey, Sudipto Banerjee, Martin Lindquist, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Estimation of finite population proportions for small areas -- a statistical data integration approach</title>
      <link>https://arxiv.org/abs/2305.12336</link>
      <description>arXiv:2305.12336v2 Announce Type: replace 
Abstract: Empirical best prediction (EBP) is a well-known method for producing reliable proportion estimates when the primary data source provides only small or no sample from finite populations. There are potential challenges in implementing existing EBP methodology such as limited auxiliary variables in the frame (not adequate for building a reasonable working predictive model) or unable to accurately link the sample to the finite population frame due to absence of identifiers. In this paper, we propose a new data linkage approach where the finite population frame is replaced by a big probability sample, having a large set of auxiliary variables but not the outcome binary variable of interest. We fit an assumed model on the small probability sample and then impute the outcome variable for all units of the big sample to obtain standard weighted proportions. We develop a new adjusted maximum likelihood (ML) method so that the estimate of model variance doesn't fall on the boundary, which is otherwise encountered in commonly used ML method. We also propose an estimator of the mean squared prediction error using a parametric bootstrap method and address computational issues by developing an efficient Expectation Maximization algorithm. The proposed methodology is illustrated in the context of election projection for small areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12336v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditi Sen, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for group-level cortical surface image-on-scalar-regression with Gaussian process priors</title>
      <link>https://arxiv.org/abs/2306.03663</link>
      <description>arXiv:2306.03663v2 Announce Type: replace 
Abstract: In regression-based analyses of group-level neuroimage data researchers typically fit a series of marginal general linear models to image outcomes at each spatially-referenced pixel. Spatial regularization of effects of interest is usually induced indirectly by applying spatial smoothing to the data during preprocessing. While this procedure often works well, resulting inference can be poorly calibrated. Spatial modeling of effects of interest leads to more powerful analyses, however the number of locations in a typical neuroimage can preclude standard computation with explicitly spatial models. Here we contribute a Bayesian spatial regression model for group-level neuroimaging analyses. We induce regularization of spatially varying regression coefficient functions through Gaussian process priors. When combined with a simple nonstationary model for the error process, our prior hierarchy can lead to more data-adaptive smoothing than standard methods. We achieve computational tractability through Vecchia approximation of our prior which, critically, can be constructed for a wide class of spatial correlation functions and results in prior models that retain full spatial rank. We outline several ways to work with our model in practice and compare performance against standard vertex-wise analyses. Finally we illustrate our method in an analysis of cortical surface fMRI task contrast data from a large cohort of children enrolled in the Adolescent Brain Cognitive Development study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03663v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew S. Whiteman, Timothy D. Johnson, Jian Kang</dc:creator>
    </item>
    <item>
      <title>A Simple Bias Reduction for Chatterjee's Correlation</title>
      <link>https://arxiv.org/abs/2312.15496</link>
      <description>arXiv:2312.15496v4 Announce Type: replace 
Abstract: Chatterjee's rank correlation coefficient $\xi_n$ is an empirical index for detecting functional dependencies between two variables $X$ and $Y$. It is an estimator for a theoretical quantity $\xi$ that is zero for independence and one if $Y$ is a measurable function of $X$. Based on an equivalent characterization of sorted numbers, we derive an upper bound for $\xi_n$ and suggest a simple normalization aimed at reducing its bias for small sample size $n$. In Monte Carlo simulations of various models, the normalization reduced the bias in all cases. The mean squared error was reduced, too, for values of $\xi$ greater than about 0.4. Moreover, we observed that non-parametric confidence intervals for $\xi$ based on bootstrapping $\xi_n$ in the usual n-out-of-n way have a coverage probability close to zero. This is remedied by an m-out-of-n bootstrap without replacement in combination with our normalization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15496v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s42519-024-00399-y</arxiv:DOI>
      <arxiv:journal_reference>Journal of Statistical Theory and Practice 18, 51 (2024)</arxiv:journal_reference>
      <dc:creator>Christoph Dalitz, Juliane Arning, Steffen Goebbels</dc:creator>
    </item>
    <item>
      <title>Detection and inference of changes in high-dimensional linear regression with non-sparse structures</title>
      <link>https://arxiv.org/abs/2402.06915</link>
      <description>arXiv:2402.06915v3 Announce Type: replace 
Abstract: For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the sparsity of neither regression parameters nor their differences, a.k.a. differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package inferchange on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06915v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haeran Cho, Tobias Kley, Housen Li</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal count autoregression</title>
      <link>https://arxiv.org/abs/2404.02982</link>
      <description>arXiv:2404.02982v2 Announce Type: replace 
Abstract: We study the problem of modeling and inference for spatio-temporal count processes. Our approach uses parsimonious parameterisations of multivariate autoregressive count time series models, including possible regression on covariates. We control the number of parameters by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. This work is motivated by real data applications which call for suitable models. Extensive simulation studies show that our approach yields reliable estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02982v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Maletz, Konstantinos Fokianos, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Causal Inference for a Hidden Treatment</title>
      <link>https://arxiv.org/abs/2405.09080</link>
      <description>arXiv:2405.09080v3 Announce Type: replace 
Abstract: In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the surrogate measurement is particularly challenging without validation data. We propose a method that obviates the need for validation data by carefully incorporating the surrogate measurement with a proxy of the hidden treatment to obtain nonparametric identification of several causal effects of interest, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. For inference, we provide general semiparametric theory for causal effects identified using our approach and derive a large class of semiparametric efficient estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions which involve the hidden treatment therefore preventing the direct use of standard machine learning algorithms, which we resolve by introducing a novel semiparametric EM algorithm. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer's disease on hippocampal volume using data from the Alzheimer's Disease Neuroimaging Initiative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09080v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zhou, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Comparing Causal Inference Methods for Point Exposures with Missing Confounders: A Simulation Study</title>
      <link>https://arxiv.org/abs/2407.06038</link>
      <description>arXiv:2407.06038v2 Announce Type: replace 
Abstract: Causal inference methods based on electronic health record (EHR) databases must simultaneously handle confounding and missing data. Vast scholarship exists aimed at addressing these two issues separately, but surprisingly few papers attempt to address them simultaneously. In practice, when faced with simultaneous missing data and confounding, analysts may proceed by first imputing missing data and subsequently using outcome regression or inverse-probability weighting (IPW) to address confounding. However, little is known about the theoretical performance of such $\textit{ad hoc}$ methods. In a recent paper Levis $\textit{et al.}$ outline a robust framework for tackling these problems together under certain identifying conditions, and introduce a pair of estimators for the average treatment effect (ATE), one of which is non-parametric efficient. In this work we present a series of simulations, motivated by a published EHR based study of the long-term effects of bariatric surgery on weight outcomes, to investigate these new estimators and compare them to existing $\textit{ad hoc}$ methods. While the latter perform well in certain scenarios, no single estimator is uniformly best. As such, the work of Levis $\textit{et al.}$ may serve as a reasonable default for causal inference when handling confounding and missing data together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06038v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Alexander Levis, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Robust likelihood ratio tests for composite nulls and alternatives</title>
      <link>https://arxiv.org/abs/2408.14015</link>
      <description>arXiv:2408.14015v2 Announce Type: replace 
Abstract: We propose an e-value based framework for testing composite nulls against composite alternatives when an $\epsilon$ fraction of the data can be arbitrarily corrupted. Our tests are inherently sequential, being valid at arbitrary data-dependent stopping times, but they are new even for fixed sample sizes, giving type-I error control without any regularity conditions. We achieve this by modifying and extending a proposal by Huber (1965) in the point null versus point alternative case. Our test statistic is a nonnegative supermartingale under the null, even with a sequentially adaptive contamination model where the conditional distribution of each observation given the past data lies within an $\epsilon$ (total variation) ball of the null. The test is powerful within an $\epsilon$ ball of the alternative. As a consequence, one obtains anytime-valid p-values that enable continuous monitoring of the data, and adaptive stopping. We analyze the growth rate of our test supermartingale and demonstrate that as $\epsilon\to 0$, it approaches a certain Kullback-Leibler divergence between the null and alternative, which is the optimal non-robust growth rate. A key step is the derivation of a robust Reverse Information Projection (RIPr). Simulations validate the theory and demonstrate excellent practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14015v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>The $\infty$-S test via regression quantile affine LASSO</title>
      <link>https://arxiv.org/abs/2409.04256</link>
      <description>arXiv:2409.04256v2 Announce Type: replace 
Abstract: The nonparametric sign test dates back to the early 18th century with a data analysis by John Arbuthnot. It is an alternative to Gosset's more recent t-test for consistent differences between two sets of observations. Fisher's F-test is a generalization of the t-test to linear regression and linear null hypotheses. Only the sign test is robust to non-Gaussianity. Gutenbrunner et al. [1993] derived a version of the sign test for linear null hypotheses in the spirit of the F-test, which requires the difficult estimation of the sparsity function. We propose instead a new sign test called $\infty$-S test via the convex analysis of a point estimator that thresholds the estimate towards the null hypothesis of the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04256v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sylvain Sardy, Xiaoyu Ma, Hugo Gaible</dc:creator>
    </item>
    <item>
      <title>Easy Conditioning far Beyond Gaussian</title>
      <link>https://arxiv.org/abs/2409.16003</link>
      <description>arXiv:2409.16003v2 Announce Type: replace 
Abstract: Estimating and sampling from conditional densities plays a critical role in statistics and data science, with a plethora of applications. Numerous methods are available ranging from simple fitting approaches to sophisticated machine learning algorithms. However, selecting from among these often involves a trade-off between conflicting objectives of efficiency, flexibility and interpretability. Starting from well known easy conditioning results in the Gaussian case, we show, thanks to results pertaining to stability by mixing and marginal transformations, that the latter carry over far beyond the Gaussian case. This enables us to flexibly model multivariate data by accommodating broad classes of multi-modal dependence structures and marginal distributions, while enjoying fast conditioning of fitted joint distributions. In applications, we primarily focus on conditioning via Gaussian versus Gaussian mixture copula models, comparing different fitting implementations for the latter. Numerical experiments with simulated and real data demonstrate the relevance of the approach for conditional sampling, evaluated using multivariate scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16003v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Faul, David Ginsbourger, Ben Spycher</dc:creator>
    </item>
    <item>
      <title>The Ensemble Kalman Filter for Dynamic Inverse Problems</title>
      <link>https://arxiv.org/abs/2401.11948</link>
      <description>arXiv:2401.11948v2 Announce Type: replace-cross 
Abstract: In inverse problems, the goal is to estimate unknown model parameters from noisy observational data. Traditionally, inverse problems are solved under the assumption of a fixed forward operator describing the observation model. In this article, we consider the extension of this approach to situations where we have a dynamic forward model, motivated by applications in scientific computation and engineering. We specifically consider this extension for a derivative-free optimizer, the ensemble Kalman inversion (EKI). We introduce and justify a new methodology called dynamic-EKI, which is a particle-based method with a changing forward operator. We analyze our new method, presenting results related to the control of our particle system through its covariance structure. This analysis includes moment bounds and an ensemble collapse, which are essential for demonstrating a convergence result. We establish convergence in expectation and validate our theoretical findings through experiments with dynamic-EKI applied to a 2D Darcy flow partial differential equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11948v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Weissmann, Neil K. Chada, Xin T. Tong</dc:creator>
    </item>
    <item>
      <title>A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty</title>
      <link>https://arxiv.org/abs/2406.02584</link>
      <description>arXiv:2406.02584v3 Announce Type: replace-cross 
Abstract: Earth observation (EO) data such as satellite imagery can have far-reaching impacts on our understanding of the geography of poverty, especially when coupled with machine learning (ML) and computer vision. Early research in computer vision used predictive models to estimate living conditions, especially in contexts where data availability on poverty was scarce. Recent work has progressed beyond using EO data to predict such outcomes -- now also using it to conduct causal inference. However, how such EO-ML models are used for causality remains incompletely mapped. To address this gap, we conduct a scoping review where we first document the growth of interest in using satellite images and other sources of EO data in causal analysis. We then trace the methodological relationship between spatial statistics and ML methods before discussing five ways in which EO data has been used in scientific workflows -- (1) outcome imputation for downstream causal analysis, (2) EO image deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based transportability analysis, and (5) image-informed causal discovery. We consolidate these observations by providing a detailed workflow for how researchers can incorporate EO data in causal analysis going forward -- from data requirements to choice of computer vision model and evaluation metrics. While our discussion focuses on health and living conditions outcomes, our workflow applies to other measures of sustainable development where EO data are informative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02584v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
  </channel>
</rss>
