<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Mar 2025 01:56:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian measures of leverage and influence</title>
      <link>https://arxiv.org/abs/2503.19996</link>
      <description>arXiv:2503.19996v1 Announce Type: new 
Abstract: Local sensitivity diagnostics for Bayesian models are described that are analogues of frequentist measures of leverage and influence. The diagnostics are simple to calculate using MCMC. A comparison between leverage and influence allows a general purpose definition of an outlier based on local perturbations. These outliers may indicate areas where the model does not fit well even if they do not influence model fit. The sensitivity diagnostics are closely related to predictive information criteria that are commonly used for Bayesian model choice. A diagnostic for prior-data conflict is proposed that may also be used to measure cross-conflict between different parts of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19996v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martyn Plummer</dc:creator>
    </item>
    <item>
      <title>Optimizing Forecast Combination Weights Using Exponentially Weighted Hit and Win Rate Losses</title>
      <link>https://arxiv.org/abs/2503.20082</link>
      <description>arXiv:2503.20082v1 Announce Type: new 
Abstract: Forecasting revenues by aggregating analyst forecasts is a fundamental problem in financial research and practice. A key objective in this context is to improve the accuracy of the forecast by optimizing two performance metrics: the hit rate, which measures the proportion of correctly classified revenue surprise signs, and the win rate, which quantifies the proportion of individual forecasts that outperform an equally weighted consensus benchmark. While researchers have extensively studied forecast combination techniques, two critical gaps remain: (i) the estimation of optimal combination weights tailored to these specific performance metrics and (ii) the development of Bayesian methods for handling missing or incomplete analyst forecasts. This paper proposes novel approaches to address these challenges. First, we introduce a method for estimating optimal forecast combination weights using exponentially weighted hit and win rate loss functions via nonlinear programming. Second, we develop a Bayesian imputation framework that leverages exponentially weighted likelihood methods to account for missing forecasts while preserving key distributional properties. Through extensive empirical evaluations using real-world analyst forecast data, we demonstrate that our proposed methodologies yield superior predictive performance compared to traditional equally weighted and linear combination benchmarks. These findings highlight the advantages of incorporating tailored loss functions and Bayesian inference in forecast combination models, offering valuable insights for financial analysts and practitioners seeking to improve revenue prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20082v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henry D. van Eijk, Sujit K. Ghosh</dc:creator>
    </item>
    <item>
      <title>A new coefficient of separation</title>
      <link>https://arxiv.org/abs/2503.20393</link>
      <description>arXiv:2503.20393v1 Announce Type: new 
Abstract: A coefficient is introduced that quantifies the extent of separation of a random variable $Y$ relative to a number of variables $\mathbf{X} = (X_1, \dots, X_p)$ by skillfully assessing the sensitivity of the relative effects of the conditional distributions. The coefficient is as simple as classical dependence coefficients such as Kendall's tau, also requires no distributional assumptions, and consistently estimates an intuitive and easily interpretable measure, which is $0$ if and only if $Y$ is stochastically comparable relative to $\mathbf{X}$, that is, the values of $Y$ show no location effect relative to $\mathbf{X}$, and $1$ if and only if $Y$ is completely separated relative to $\mathbf{X}$. As a true generalization of the classical relative effect, in applications such as medicine and the social sciences the coefficient facilitates comparing the distributions of any number of treatment groups or categories. It hence avoids the sometimes artificial grouping of variable values such as patient's age into just a few categories, which is known to cause inaccuracy and bias in the data analysis. The mentioned benefits are exemplified using synthetic and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20393v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Fuchs, Carsten Limbach, Patrick B. Langthaler</dc:creator>
    </item>
    <item>
      <title>Interpretable Deep Neural Network for Modeling Functional Surrogates</title>
      <link>https://arxiv.org/abs/2503.20528</link>
      <description>arXiv:2503.20528v1 Announce Type: new 
Abstract: Developing surrogates for computer models has become increasingly important for addressing complex problems in science and engineering. This article introduces an artificial intelligent (AI) surrogate, referred to as the DeepSurrogate, for analyzing functional outputs with vector-valued inputs. The relationship between the functional output and vector-valued input is modeled as an infinite sequence of unknown functions, each representing the relationship at a specific location within the functional domain. These spatially indexed functions are expressed through a combination of basis functions and their corresponding coefficient functions, both of which are modeled using deep neural networks (DNN). The proposed framework accounts for spatial dependencies across locations, while capturing the relationship between the functional output and scalar predictors. It also integrates a Monte Carlo (MC) dropout strategy to quantify prediction uncertainty, enhancing explainability in the deep neural network architecture. The proposed method enables efficient inference on datasets with approximately 50,000 spatial locations and 20 simulations, achieving results in under 10 minutes using standard hardware. The approach is validated on extensive synthetic datasets and a large-scale simulation from the Sea Lake and Overland Surge from Hurricanes (SLOSH) simulator. An open-source Python package implementing the method is made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20528v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Devin Francom, Donatella Pasqualini</dc:creator>
    </item>
    <item>
      <title>False Discovery estimation in Record Linkage</title>
      <link>https://arxiv.org/abs/2503.20627</link>
      <description>arXiv:2503.20627v1 Announce Type: new 
Abstract: Integrating data from multiple electronic sources allows researchers to expand studies without the cost of new data collection. However, such data are often collected for administrative or operational purposes rather than with specific future research questions in mind and, due to privacy constraints, unique identifiers are unavailable. This lack of direct identifiers requires the use of Record Linkage (RL) algorithms, which rely on partially identifying variables to probabilistically determine whether records belong to the same entity. Since these variables lack the strength to perfectly combine information, RL procedures typically yield an imperfect set of linked records. Therefore, assessing the false discovery rate (FDR) of RL is crucial for ensuring the reliability of subsequent analyses. In this paper, we introduce a novel method for estimating the FDR in RL by linking records from real and synthesised data. As synthetic records should never form links with real observations, they provide a means to estimate the FDR across different procedural settings. Notably, this method is applicable to all RL techniques. By identifying the FDR in RL results and selecting suitable model parameters, our approach enables to assess and improve the reliability of linked data. We evaluate the performance of our procedure using established RL algorithms and benchmark data sets before applying it to link siblings from the Netherlands Perinatal Registry, where the reliability of previous RL applications has never been confirmed. Through this application, we highlight the importance of accounting for linkage errors when studying mother-child dynamics in healthcare records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20627v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayan\'e Robach, Michel H. Hof, Mark A. van de Wiel</dc:creator>
    </item>
    <item>
      <title>Asset price movement prediction using empirical mode decomposition and Gaussian mixture models</title>
      <link>https://arxiv.org/abs/2503.20678</link>
      <description>arXiv:2503.20678v1 Announce Type: new 
Abstract: We investigated the use of Empirical Mode Decomposition (EMD) combined with Gaussian Mixture Models (GMM), feature engineering and machine learning algorithms to optimize trading decisions. We used five, two, and one year samples of hourly candle data for GameStop, Tesla, and XRP (Ripple) markets respectively. Applying a 15 hour rolling window for each market, we collected several features based on a linear model and other classical features to predict the next hour's movement. Subsequently, a GMM filtering approach was used to identify clusters among these markets. For each cluster, we applied the EMD algorithm to extract high, medium, low and trend components from each feature collected. A simple thresholding algorithm was applied to classify market movements based on the percentage change in each market's close price. We then evaluated the performance of various machine learning models, including Random Forests (RF) and XGBoost, in classifying market movements. A naive random selection of trading decisions was used as a benchmark, which assumed equal probabilities for each outcome, and a temporal cross-validation approach was used to test models on 40%, 30%, and 20% of the dataset. Our results indicate that transforming selected features using EMD improves performance, particularly for ensemble learning algorithms like Random Forest and XGBoost, as measured by accumulated profit. Finally, GMM filtering expanded the range of learning algorithm and data source combinations that outperformed the top percentile of the random baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20678v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel R. Palma, Mariusz Skocze\'n, Phil Maguire</dc:creator>
    </item>
    <item>
      <title>Automatic Inference for Value-Added Regressions</title>
      <link>https://arxiv.org/abs/2503.19178</link>
      <description>arXiv:2503.19178v1 Announce Type: cross 
Abstract: It is common to use shrinkage methods such as empirical Bayes to improve estimates of teacher value-added. However, when the goal is to perform inference on coefficients in the regression of long-term outcomes on value-added, it's unclear whether shrinking the value-added estimators can help or hurt. In this paper, we consider a general class of value-added estimators and the properties of their corresponding regression coefficients. Our main finding is that regressing long-term outcomes on shrinkage estimates of value-added performs an automatic bias correction: the associated regression estimator is asymptotically unbiased, asymptotically normal, and efficient in the sense that it is asymptotically equivalent to regressing on the true (latent) value-added. Further, OLS standard errors from regressing on shrinkage estimates are consistent. As such, efficient inference is easy for practitioners to implement: simply regress outcomes on shrinkage estimates of value added.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19178v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Xie</dc:creator>
    </item>
    <item>
      <title>Functional structural equation models with out-of-sample guarantees</title>
      <link>https://arxiv.org/abs/2503.20072</link>
      <description>arXiv:2503.20072v1 Announce Type: cross 
Abstract: Statistical learning methods typically assume that the training and test data originate from the same distribution, enabling effective risk minimization. However, real-world applications frequently involve distributional shifts, leading to poor model generalization. To address this, recent advances in causal inference and robust learning have introduced strategies such as invariant causal prediction and anchor regression. While these approaches have been explored for traditional structural equation models (SEMs), their extension to functional systems remains limited. This paper develops a risk minimization framework for functional SEMs using linear, potentially unbounded operators. We introduce a functional worst-risk minimization approach, ensuring robust predictive performance across shifted environments. Our key contribution is a novel worst-risk decomposition theorem, which expresses the maximum out-of-sample risk in terms of observed environments. We establish conditions for the existence and uniqueness of the worst-risk minimizer and provide consistent estimation procedures. Empirical results on functional systems illustrate the advantages of our method in mitigating distributional shifts. These findings contribute to the growing literature on robust functional regression and causal learning, offering practical guarantees for out-of-sample generalization in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20072v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philip Kennerberg, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Regression-Based Estimation of Causal Effects in the Presence of Selection Bias and Confounding</title>
      <link>https://arxiv.org/abs/2503.20546</link>
      <description>arXiv:2503.20546v1 Announce Type: cross 
Abstract: We consider the problem of estimating the expected causal effect $E[Y|do(X)]$ for a target variable $Y$ when treatment $X$ is set by intervention, focusing on continuous random variables. In settings without selection bias or confounding, $E[Y|do(X)] = E[Y|X]$, which can be estimated using standard regression methods. However, regression fails when systematic missingness induced by selection bias, or confounding distorts the data. Boeken et al. [2023] show that when training data is subject to selection, proxy variables unaffected by this process can, under certain constraints, be used to correct for selection bias to estimate $E[Y|X]$, and hence $E[Y|do(X)]$, reliably. When data is additionally affected by confounding, however, this equality is no longer valid.
  Building on these results, we consider a more general setting and propose a framework that incorporates both selection bias and confounding. Specifically, we derive theoretical conditions ensuring identifiability and recoverability of causal effects under access to external data and proxy variables. We further introduce a two-step regression estimator (TSR), capable of exploiting proxy variables to adjust for selection bias while accounting for confounding. We show that TSR coincides with prior work if confounding is absent, but achieves a lower variance. Extensive simulation studies validate TSR's correctness for scenarios which may include both selection bias and confounding with proxy variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20546v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marlies Hafer, Alexander Marx</dc:creator>
    </item>
    <item>
      <title>Screening for Diabetes Mellitus in the U.S. Population Using Neural Network Models and Complex Survey Designs</title>
      <link>https://arxiv.org/abs/2403.19752</link>
      <description>arXiv:2403.19752v2 Announce Type: replace 
Abstract: Complex survey designs are commonly employed in many medical cohorts. In such scenarios, developing case-specific predictive risk score models that reflect the unique characteristics of the study design is essential for minimizing selective biases in the statistical results. The objectives of this paper are to: (i) propose a general predictive framework for regression and classification using neural network (NN) modeling that incorporates survey weights into the estimation process; (ii) introduce an uncertainty quantification algorithm for model prediction tailored to data from complex survey designs; and (iii) apply this method to develop robust risk score models for assessing the risk of Diabetes Mellitus in the US population, utilizing data from the NHANES 2011-2014 cohort. The results indicate that models of varying complexity, each utilizing a different set of variables, demonstrate different discriminative power for predicting diabetes (with different economic cost), yet yield generalizable results at the population level. Although the focus is on diabetes, this NN predictive framework is adaptable for developing clinical models across a diverse range of diseases and medical cohorts. The software and data used in this paper are publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19752v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Juan C. Vidal, Rahul Ghosal, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2405.03606</link>
      <description>arXiv:2405.03606v2 Announce Type: replace 
Abstract: We address parameter estimation in second-order stochastic differential equations (SDEs), which are prevalent in physics, biology, and ecology. The second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable, which raises two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. We propose an estimator based on the Strang splitting scheme to overcome this. Second, since the velocity is rarely observed, we adapt the estimator to partial observations. We present four estimators for complete and partial observations, using the full pseudo-likelihood or only the velocity-based partial pseudo-likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using the full pseudo-likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases as a result of information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using the partial pseudo-likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core by fitting the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03606v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Smoothing of Directional and Axial Data</title>
      <link>https://arxiv.org/abs/2501.17463</link>
      <description>arXiv:2501.17463v2 Announce Type: replace 
Abstract: We discuss generalized linear models for directional data where the conditional distribution of the response is a von Mises-Fisher distribution in arbitrary dimension or a Bingham distribution on the unit circle. To do this properly, we parametrize von Mises-Fisher distributions by Euclidean parameters and investigate computational aspects of this parametrization. Then we modify this approach for local polynomial regression as a means of nonparametric smoothing of distributional data. The methods are illustrated with simulated data and a data set from planetary sciences involving covariate vectors on a sphere with axial response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17463v2</guid>
      <category>stat.ME</category>
      <category>astro-ph.EP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutz Duembgen, Caroline Haslebacher</dc:creator>
    </item>
    <item>
      <title>Fiducial Inference for Random-Effects Calibration Models: Advancing Reliable Quantification in Environmental Analytical Chemistry</title>
      <link>https://arxiv.org/abs/2503.04588</link>
      <description>arXiv:2503.04588v2 Announce Type: replace 
Abstract: This article addresses calibration challenges in analytical chemistry by employing a random-effects calibration curve model and its generalizations to capture variability in analyte concentrations. The model is motivated by specific issues in analytical chemistry, where measurement errors remain constant at low concentrations but increase proportionally as concentrations rise. To account for this, the model permits the parameters of the calibration curve, which relate instrument responses to true concentrations, to vary across different laboratories, thereby reflecting real-world variability in measurement processes. Traditional large-sample interval estimation methods are inadequate for small samples, leading to the use of an alternative approach, namely the fiducial approach. The calibration curve that accurately captures the heteroscedastic nature of the data, results in more reliable estimates across diverse laboratory conditions. It turns out that the fiducial approach, when used to construct a confidence interval for an unknown concentration, produces a slightly wider width while achieving the desired coverage probability. Applications considered include the determination of the presence of an analyte and the interval estimation of an unknown true analyte concentration. The proposed method is demonstrated for both simulated and real interlaboratory data, including examples involving copper and cadmium in distilled water.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04588v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Robert Gibbons, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Fitting multivariate Hawkes processes to interval count data with an application to terrorist activity modelling -- a particle Markov chain Monte Carlo approach</title>
      <link>https://arxiv.org/abs/2503.18351</link>
      <description>arXiv:2503.18351v2 Announce Type: replace 
Abstract: Terrorist activities often exhibit temporal and spatial clustering, making the multivariate Hawkes process (MHP) a useful statistical model for analysing terrorism across different geographic regions. However, terror attack data from the Global Terrorism Database is reported as total event counts in disjoint observation periods, with precise event times unknown. When the MHP is only observed discretely, the likelihood function becomes intractable, hindering likelihood-based inference. To address this, we design an unbiased estimate of the intractable likelihood function using sequential Monte Carlo (SMC) based on a representation of the unobserved event times as latent variables in a state-space model. The unbiasedness of the SMC estimate allows for its use in place of the true likelihood in a Metropolis-Hastings algorithm, from which we construct a Markov Chain Monte Carlo sample of the distribution over the parameters of the MHP. Using simulated data, we assess the performance of our method and demonstrate that it outperforms an alternative method in the literature based on mean squared error. Terrorist activity in Afghanistan and Pakistan from 2018 to 2021 is analysed based on daily count data to examine the self- and cross-excitation effects of terrorism events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18351v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason J. Lambe, Feng Chen, Tom Stindl, Tsz-Kit Jeffrey Kwan</dc:creator>
    </item>
    <item>
      <title>A tuning-free and scalable method for joint graphical model estimation with sharper bounds</title>
      <link>https://arxiv.org/abs/2503.18722</link>
      <description>arXiv:2503.18722v2 Announce Type: replace 
Abstract: Joint estimation of multiple graphical models (i.e., multiple precision matrices) has emerged as an important topic in statistics. Unlike separate estimation, joint estimation can leverage shared structural patterns across multiple graphs to yield more accurate results. In this paper, we present an efficient and tuning-free method named MIGHT (Multi-task Iterative Graphical Hard Thresholding) to jointly estimate multiple graphical models. We reformulate the joint model into a series of multi-task learning problems in a column-by-column manner, and then solve these problems by using an iterative algorithm based on the hard thresholding operator. Theoretically, we derive the non-asymptotic error bound for our method. We prove that, under proper signal conditions, our method attains selection consistency and an improved error bound, and also exhibits asymptotic normality -- properties rarely explored in existing joint graphical model estimation literature. The performance of our method is validated through numerical simulations and real data analysis of a cancer gene-expression RNA-seq dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18722v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shixiang Liu, Yanhang Zhang, Zhifan Li, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>On the Utility of Equal Batch Sizes for Inference in Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2303.07706</link>
      <description>arXiv:2303.07706v3 Announce Type: replace-cross 
Abstract: Stochastic gradient descent (SGD) is an estimation tool for large data employed in machine learning and statistics. Due to the Markovian nature of the SGD process, inference is a challenging problem. An underlying asymptotic normality of the averaged SGD (ASGD) estimator allows for the construction of a batch-means estimator of the asymptotic covariance matrix. Instead of the usual increasing batch-size strategy, we propose a memory efficient equal batch-size strategy and show that under mild conditions, the batch-means estimator is consistent. A key feature of the proposed batching technique is that it allows for bias-correction of the variance, at no additional cost to memory. Further, since joint inference for large dimensional problems may be undesirable, we present marginal-friendly simultaneous confidence intervals, and show through an example on how covariance estimators of ASGD can be employed for improved predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07706v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Singh, Abhinek Shukla, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v3 Announce Type: replace-cross 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>A Proximal Newton Adaptive Importance Sampler</title>
      <link>https://arxiv.org/abs/2412.16558</link>
      <description>arXiv:2412.16558v2 Announce Type: replace-cross 
Abstract: Adaptive importance sampling (AIS) algorithms are a rising methodology in signal processing, statistics, and machine learning. An effective adaptation of the proposals is key for the success of AIS. Recent works have shown that gradient information about the involved target density can greatly boost performance, but its applicability is restricted to differentiable targets. In this paper, we propose a proximal Newton adaptive importance sampler for the estimation of expectations with respect to non-smooth target distributions. We implement a scaled Newton proximal gradient method to adapt the proposal distributions, enabling efficient and optimized moves even when the target distribution lacks differentiability. We show the good performance of the algorithm in two scenarios: one with convex constraints and another with non-smooth sparse priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16558v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'ictor Elvira, \'Emilie Chouzenoux, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Weighted Sample Average Approximation in Contextual Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2503.12747</link>
      <description>arXiv:2503.12747v2 Announce Type: replace-cross 
Abstract: Contextual stochastic optimization provides a framework for decision-making under uncertainty incorporating observable contextual information through covariates. We analyze statistical inference for weighted sample average approximation (wSAA), a widely-used method for solving contextual stochastic optimization problems. We first establish central limit theorems for wSAA estimates of optimal values when problems can be solved exactly, characterizing how estimation uncertainty scales with covariate sample size. We then investigate practical scenarios with computational budget constraints, revealing a fundamental tradeoff between statistical accuracy and computational cost as sample sizes increase. Through central limit theorems for budget-constrained wSAA estimates, we precisely characterize this statistical-computational tradeoff. We also develop "over-optimizing" strategies for solving wSAA problems that ensure valid statistical inference. Extensive numerical experiments on both synthetic and real-world datasets validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12747v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanyuan Wang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2503.13696</link>
      <description>arXiv:2503.13696v2 Announce Type: replace-cross 
Abstract: Empirical studies using Regression Discontinuity (RD) designs often explore heterogeneous treatment effects based on pretreatment covariates. However, the lack of formal statistical methods has led to the widespread use of ad hoc approaches in applications. Motivated by common empirical practice, we develop a unified, theoretically grounded framework for RD heterogeneity analysis. We show that a fully interacted local linear (in functional parameters) model effectively captures heterogeneity while still being tractable and interpretable in applications. The model structure holds without loss of generality for discrete covariates, while for continuous covariates our proposed (local functional linear-in-parameters) model can be potentially restrictive, but it nonetheless naturally matches standard empirical practice and offers a causal interpretation for RD applications. We establish principled bandwidth selection and robust bias-corrected inference methods to analyze heterogeneous treatment effects and test group differences. We provide companion software to facilitate implementation of our results. An empirical application illustrates the practical relevance of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13696v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
  </channel>
</rss>
