<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Machine Learning Framework for Breast Cancer Treatment Classification Using a Novel Dataset</title>
      <link>https://arxiv.org/abs/2507.06243</link>
      <description>arXiv:2507.06243v1 Announce Type: new 
Abstract: Breast cancer (BC) remains a significant global health challenge, with personalized treatment selection complicated by the disease's molecular and clinical heterogeneity. BC treatment decisions rely on various patient-specific clinical factors, and machine learning (ML) offers a powerful approach to predicting treatment outcomes. This study utilizes The Cancer Genome Atlas (TCGA) breast cancer clinical dataset to develop ML models for predicting the likelihood of undergoing chemotherapy or hormonal therapy. The models are trained using five-fold cross-validation and evaluated through performance metrics, including accuracy, precision, recall, specificity, sensitivity, F1-score, and area under the receiver operating characteristic curve (AUROC). Model uncertainty is assessed using bootstrap techniques, while SHAP values enhance interpretability by identifying key predictors. Among the tested models, the Gradient Boosting Machine (GBM) achieves the highest stable performance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient Boosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting (AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the potential of ML in supporting personalized breast cancer treatment decisions through data-driven insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06243v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Nahid Hasan, Md Monzur Murshed, Md Mahadi Hasan, Faysal A. Chowdhury</dc:creator>
    </item>
    <item>
      <title>Method: Using generalized additive models in the animal sciences</title>
      <link>https://arxiv.org/abs/2507.06281</link>
      <description>arXiv:2507.06281v1 Announce Type: new 
Abstract: Nonlinear relationships between covariates and a response variable of interest are frequently encountered in animal science research. Within statistical models, these nonlinear effects have, traditionally, been handled using a range of approaches, including transformation of the response, parametric nonlinear models based on theory or phenomenological grounds (e.g., lactation curves), or through fixed spline or polynomial terms. If it is desirable to learn the shape of the relationship from the data directly, then generalized additive models (GAMs) are an excellent alternative to these traditional approaches. GAMs extend the generalized linear model such that the linear predictor includes one or more smooth functions, parameterised using penalised splines. A wiggliness penalty on each function is used to avoid over fitting while estimating the parameters of the spline basis functions to maximise fit to the data without producing an overly complex function. Modern GAMs include automatic smoothness selection methods to find an optimal balance between fit and complexity of the estimated functions. Because GAMs learn the shapes of functions from the data, the user can avoid forcing a particular model to their data. Here, I provide a brief description of GAMs and visually illustrate how they work. I then demonstrate the utility of GAMs on three example data sets of increasing complexity, to show i) how learning from data can produce a better fit to data than that of parametric models, ii) how hierarchical GAMs can be used to estimate growth data from multiple animals in a single model, and iii) how hierarchical GAMs can be used for formal statistical inference in a designed experiment of the effects of exposure to maternal hormones on subsequent growth in Japanese quail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06281v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin L. Simpson</dc:creator>
    </item>
    <item>
      <title>Note on a non-parametric method for change-point detection</title>
      <link>https://arxiv.org/abs/2507.06664</link>
      <description>arXiv:2507.06664v1 Announce Type: new 
Abstract: The purpose of this note is to present in details R codes to implement a non-parametric method for change-point detection. The proposed approach is validated from various perspectives using simulations. This method is a competitor to that of Pettitt ([3]) and is, like the latter, based on the Wilcoxon-Mann-Whitney test. It is used in [4] for the study of relatively short time series obtained from measurements on cores sampled in the bay of Brest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06664v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Ailliot (LMBA), N'D\`eye Coumba Niass (LMBA), Jean-Marc Derrien (LMBA)</dc:creator>
    </item>
    <item>
      <title>Some Results on Comparisons of Random Extremes having Identical and Non-Identical Components</title>
      <link>https://arxiv.org/abs/2507.06965</link>
      <description>arXiv:2507.06965v1 Announce Type: new 
Abstract: In this article, we revisit the paper by Kundu et al.~(2024), presenting new results and insights for both identical and non-identical independent random variables. We derive sufficient conditions for preserving the hazard rate and reversed hazard rate orderings between the random maximum and minimum order statistics, respectively. Our results show that these preservation conditions also hold for independent and identically distributed random variables. We also demonstrate that the findings in Kundu et al.~(2024) for the non-identical cases do not apply to the identical cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06965v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bidhan Modok, Shovan Chowdhury, Amarjit Kundu</dc:creator>
    </item>
    <item>
      <title>When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior</title>
      <link>https://arxiv.org/abs/2507.07012</link>
      <description>arXiv:2507.07012v1 Announce Type: new 
Abstract: Modeling car-following behavior is fundamental to microscopic traffic simulation, yet traditional deterministic models often fail to capture the full extent of variability and unpredictability in human driving. While many modern approaches incorporate context-aware inputs (e.g., spacing, speed, relative speed), they frequently overlook structured stochasticity that arises from latent driver intentions, perception errors, and memory effects -- factors that are not directly observable from context alone. To fill the gap, this study introduces an interpretable stochastic modeling framework that captures not only context-dependent dynamics but also residual variability beyond what context can explain. Leveraging deep neural networks integrated with nonstationary Gaussian processes (GPs), our model employs a scenario-adaptive Gibbs kernel to learn dynamic temporal correlations in acceleration decisions, where the strength and duration of correlations between acceleration decisions evolve with the driving context. This formulation enables a principled, data-driven quantification of uncertainty in acceleration, speed, and spacing, grounded in both observable context and latent behavioral variability. Comprehensive experiments on the naturalistic vehicle trajectory dataset collected from the German highway, i.e., the HighD dataset, demonstrate that the proposed stochastic simulation method within this framework surpasses conventional methods in both predictive performance and interpretable uncertainty quantification. The integration of interpretability and accuracy makes this framework a promising tool for traffic analysis and safety-critical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07012v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengyuan Zhang, Zhengbing He, Cathy Wu, Lijun Sun</dc:creator>
    </item>
    <item>
      <title>A localized particle filter for geophysical data assimilation</title>
      <link>https://arxiv.org/abs/2507.07103</link>
      <description>arXiv:2507.07103v1 Announce Type: new 
Abstract: Particle filters are computational techniques for estimating the state of dynamical systems by integrating observational data with model predictions. This work introduces a class of Localized Particle Filters (LPFs) that exploit spatial localization to reduce computational costs and mitigate particle degeneracy in high-dimensional systems. By partitioning the state space into smaller regions and performing particle weight updates and resampling separately within each region, these filters leverage assumptions of limited spatial correlation to achieve substantial computational gains. This approach proves particularly valuable for geophysical data assimilation applications, including weather forecasting and ocean modeling, where system dimensions are vast, and complex interactions and nonlinearities demand efficient yet accurate state estimation methods. We demonstrate the methodology on a partially observed rotating shallow water system, achieving favourable performance in terms of algorithm stability and error estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07103v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Crisan, Eliana Fausti</dc:creator>
    </item>
    <item>
      <title>Machine Learning based Enterprise Financial Audit Framework and High Risk Identification</title>
      <link>https://arxiv.org/abs/2507.06266</link>
      <description>arXiv:2507.06266v1 Announce Type: cross 
Abstract: In the face of global economic uncertainty, financial auditing has become essential for regulatory compliance and risk mitigation. Traditional manual auditing methods are increasingly limited by large data volumes, complex business structures, and evolving fraud tactics. This study proposes an AI-driven framework for enterprise financial audits and high-risk identification, leveraging machine learning to improve efficiency and accuracy. Using a dataset from the Big Four accounting firms (EY, PwC, Deloitte, KPMG) from 2020 to 2025, the research examines trends in risk assessment, compliance violations, and fraud detection. The dataset includes key indicators such as audit project counts, high-risk cases, fraud instances, compliance breaches, employee workload, and client satisfaction, capturing both audit behaviors and AI's impact on operations. To build a robust risk prediction model, three algorithms - Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbors (KNN) - are evaluated. SVM uses hyperplane optimization for complex classification, RF combines decision trees to manage high-dimensional, nonlinear data with resistance to overfitting, and KNN applies distance-based learning for flexible performance. Through hierarchical K-fold cross-validation and evaluation using F1-score, accuracy, and recall, Random Forest achieves the best performance, with an F1-score of 0.9012, excelling in identifying fraud and compliance anomalies. Feature importance analysis reveals audit frequency, past violations, employee workload, and client ratings as key predictors. The study recommends adopting Random Forest as a core model, enhancing features via engineering, and implementing real-time risk monitoring. This research contributes valuable insights into using machine learning for intelligent auditing and risk management in modern enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06266v1</guid>
      <category>q-fin.RM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingyu Yuan, Xi Zhang, Xuanjing Chen</dc:creator>
    </item>
    <item>
      <title>PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer</title>
      <link>https://arxiv.org/abs/2507.06418</link>
      <description>arXiv:2507.06418v1 Announce Type: cross 
Abstract: While pathology foundation models have transformed cancer image analysis, they often lack integration with molecular data at single-cell resolution, limiting their utility for precision oncology. Here, we present PAST, a pan-cancer single-cell foundation model trained on 20 million paired histopathology images and single-cell transcriptomes spanning multiple tumor types and tissue contexts. By jointly encoding cellular morphology and gene expression, PAST learns unified cross-modal representations that capture both spatial and molecular heterogeneity at the cellular level. This approach enables accurate prediction of single-cell gene expression, virtual molecular staining, and multimodal survival analysis directly from routine pathology slides. Across diverse cancers and downstream tasks, PAST consistently exceeds the performance of existing approaches, demonstrating robust generalizability and scalability. Our work establishes a new paradigm for pathology foundation models, providing a versatile tool for high-resolution spatial omics, mechanistic discovery, and precision cancer research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06418v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changchun Yang, Haoyang Li, Yushuai Wu, Yilan Zhang, Yifeng Jiao, Yu Zhang, Rihan Huang, Yuan Cheng, Yuan Qi, Xin Guo, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Determining vaccine responders in the presence of baseline immunity using single-cell assays and paired control samples</title>
      <link>https://arxiv.org/abs/2507.06451</link>
      <description>arXiv:2507.06451v1 Announce Type: cross 
Abstract: A key objective in vaccine studies is to evaluate vaccine-induced immunogenicity and determine whether participants have mounted a response to the vaccine. Cellular immune responses are essential for assessing vaccine-induced immunogenicity, and single-cell assays, such as intracellular cytokine staining (ICS) are commonly employed to profile individual immune cell phenotypes and the cytokines they produce after stimulation. In this article, we introduce a novel statistical framework for identifying vaccine responders using ICS data collected before and after vaccination. This framework incorporates paired control data to account for potential unintended variations between assay runs, such as batch effects, that could lead to misclassification of participants as vaccine responders. To formally integrate paired control data for accounting for assay variation across different time points (i.e., before and after vaccination), our proposed framework calculates and reports two p-values, both adjusting for paired control data but in distinct ways: (i) the maximally adjusted p-value, which applies the most conservative adjustment to the unadjusted p-value, ensuring validity over all plausible batch effects consistent with the paired control samples' data, and (ii) the minimally adjusted p-value, which imposes only the minimal adjustment to the unadjusted p-value, such that the adjusted p-value cannot be falsified by the paired control samples' data. We apply this framework to analyze ICS data collected at baseline and 4 weeks post-vaccination from the COVID-19 Prevention Network (CoVPN) 3008 study. Our analysis helps address two clinical questions: 1) which participants exhibited evidence of an incident Omicron infection, and 2) which participants showed vaccine-induced T cell responses against the Omicron BA.4/5 Spike protein.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06451v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Siyu Heng, Asa Tapley, Stephen De Rosa, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian Bootstrap-based Gaussian Copula Model for Mixed Data with High Missing Rates</title>
      <link>https://arxiv.org/abs/2507.06785</link>
      <description>arXiv:2507.06785v1 Announce Type: cross 
Abstract: Missing data is a common issue in various fields such as medicine, social sciences, and natural sciences, and it poses significant challenges for accurate statistical analysis. Although numerous imputation methods have been proposed to address this issue, many of them fail to adequately capture the complex dependency structure among variables. To overcome this limitation, models based on the Gaussian copula framework have been introduced. However, most existing copula-based approaches do not account for the uncertainty in the marginal distributions, which can lead to biased marginal estimates and degraded performance, especially under high missingness rates.
  In this study, we propose a Bayesian bootstrap-based Gaussian Copula model (BBGC) that explicitly incorporates uncertainty in the marginal distributions of each variable. The proposed BBGC combines the flexible dependency modeling capability of the Gaussian copula with the Bayesian uncertainty quantification of marginal cumulative distribution functions (CDFs) via the Bayesian bootstrap. Furthermore, it is extended to handle mixed data types by incorporating methods for ordinal variable modeling.
  Through simulation studies and experiments on real-world datasets from the UCI repository, we demonstrate that the proposed BBGC outperforms existing imputation methods across various missing rates and mechanisms (MCAR, MAR). Additionally, the proposed model shows superior performance on real semiconductor manufacturing process data compared to conventional imputation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06785v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Jeunghun Oh, Hungkuk Ko, Jeongmin Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Manifolds in Power Systems Optimization</title>
      <link>https://arxiv.org/abs/2507.06883</link>
      <description>arXiv:2507.06883v1 Announce Type: cross 
Abstract: Manifold optimization (MO) is a powerful mathematical framework that can be applied to solving complex optimization problems with objective functions (OFs) and constraints on complex geometric structures, which is particularly useful in advanced power systems. We explore the application of MO techniques, which offer a robust framework for solving complex, non-convex optimization problems in electrical power distribution systems (EPDS) and electrical power transmission systems (EPTS), particularly for power flow analysis. This paper introduces the principles of MO and demonstrates its advantages over conventional methods by applying it to power flow optimization. For EPDS, a cost function derived from a backward-forward sweep (BFS) algorithm is optimized using the Manopt toolbox, yielding high accuracy and competitive computational times on 14-bus, 33-bus, and 69-bus systems when compared to established solvers. Similarly, for EPTS, MO applied via Manopt to 3-bus and 4-bus systems effectively solves power flow equations, matching traditional methods such as Newton-Raphson in performance. The study highlights that tools such as Manopt can mitigate implementation complexities, positioning MO as an efficient and accessible tool for power system analysis and potentially broader planning applications. The paper provides a comprehensive tutorial on MO, detailing its theoretical foundations, practical methodologies, and specific applications in power systems, particularly in power flow optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06883v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucca Rodrigues Pinto, Wilson de Souza Junior, Jaime Laelson Jacob, Luis Alfonso Gallego Pareja, Taufik Abr\~ao</dc:creator>
    </item>
    <item>
      <title>Fairness is in the details : Face Dataset Auditing</title>
      <link>https://arxiv.org/abs/2504.08396</link>
      <description>arXiv:2504.08396v2 Announce Type: replace 
Abstract: Auditing involves verifying the proper implementation of a given policy. As such, auditing is essential for ensuring compliance with the principles of fairness, equity, and transparency mandated by the European Union's AI Act. Moreover, biases present during the training phase of a learning system can persist in the modeling process and result in discrimination against certain subgroups of individuals when the model is deployed in production. Assessing bias in image datasets is a particularly complex task, as it first requires a feature extraction step, then to consider the extraction's quality in the statistical tests. This paper proposes a robust methodology for auditing image datasets based on so-called "sensitive" features, such as gender, age, and ethnicity. The proposed methodology consists of both a feature extraction phase and a statistical analysis phase. The first phase introduces a novel convolutional neural network (CNN) architecture specifically designed for extracting sensitive features with a limited number of manual annotations. The second phase compares the distributions of sensitive features across subgroups using a novel statistical test that accounts for the imprecision of the feature extraction model. Our pipeline constitutes a comprehensive and fully automated methodology for dataset auditing. We illustrate our approach using two manually annotated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08396v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin Lafargue, Emmanuelle Claeys, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>A statistical approach to latent dynamic modeling with differential equations</title>
      <link>https://arxiv.org/abs/2311.16286</link>
      <description>arXiv:2311.16286v2 Announce Type: replace-cross 
Abstract: Ordinary differential equations (ODEs) can provide mechanistic models of temporally local changes of processes, where parameters are often informed by external knowledge. While ODEs are popular in systems modeling, they are less established for statistical modeling of longitudinal cohort data, e.g., in a clinical setting. Yet, modeling of local changes could also be attractive for assessing the trajectory of an individual in a cohort in the immediate future given its current status, where ODE parameters could be informed by further characteristics of the individual. However, several hurdles so far limit such use of ODEs, as compared to regression-based function fitting approaches. The potentially higher level of noise in cohort data might be detrimental to ODEs, as the shape of the ODE solution heavily depends on the initial value. In addition, larger numbers of variables multiply such problems and might be difficult to handle for ODEs. To address this, we propose to use each observation in the course of time as the initial value to obtain multiple local ODE solutions and build a combined estimator of the underlying dynamics. Neural networks are used for obtaining a low-dimensional latent space for dynamic modeling from a potentially large number of variables, and for obtaining patient-specific ODE parameters from baseline variables. Simultaneous identification of dynamic models and of a latent space is enabled by recently developed differentiable programming techniques. We illustrate the proposed approach in an application with spinal muscular atrophy patients and a corresponding simulation study. In particular, modeling of local changes in health status at any point in time is contrasted to the interpretation of functions obtained from a global regression. This more generally highlights how different application settings might demand different modeling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16286v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maren Hackenberg, Astrid Pechmann, Clemens Kreutz, Janbernd Kirschner, Harald Binder</dc:creator>
    </item>
    <item>
      <title>Assessing treatment efficacy for interval-censored endpoints using multistate semi-Markov models fit to multiple data streams</title>
      <link>https://arxiv.org/abs/2501.14097</link>
      <description>arXiv:2501.14097v3 Announce Type: replace-cross 
Abstract: We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14097v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, C. Jason Liang, Allyson Mateja, Dean A. Follmann, Meagan P. O'Brien, Chenguang Wang, Jonathan Fintzi</dc:creator>
    </item>
    <item>
      <title>On noncentral Wishart mixtures of noncentral Wisharts and their use for testing random effects in factorial design models</title>
      <link>https://arxiv.org/abs/2502.13711</link>
      <description>arXiv:2502.13711v2 Announce Type: replace-cross 
Abstract: It is shown that a noncentral Wishart mixture of noncentral Wishart distributions with the same degrees of freedom yields a noncentral Wishart distribution, thereby extending the main result of Jones and Marchand [Stat 10 (2021), Paper No. e398, 7 pp.] from the chi-square to the Wishart setting. To illustrate its use, this fact is then employed to derive the finite-sample distribution of test statistics for random effects in a two-factor factorial design model with $d$-dimensional normal data, thereby broadening the findings of Bilodeau [ArXiv (2022), 6 pp.], who treated the case $d = 1$. The same approach makes it possible to test random effects in more general factorial design models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13711v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Anne MacKay, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>The cost of ensembling: is it always worth combining?</title>
      <link>https://arxiv.org/abs/2506.04677</link>
      <description>arXiv:2506.04677v2 Announce Type: replace-cross 
Abstract: Given the continuous increase in dataset sizes and the complexity of forecasting models, the trade-off between forecast accuracy and computational cost is emerging as an extremely relevant topic, especially in the context of ensemble learning for time series forecasting. To asses it, we evaluated ten base models and eight ensemble configurations across two large-scale retail datasets (M5 and VN1), considering both point and probabilistic accuracy under varying retraining frequencies. We showed that ensembles consistently improve forecasting performance, particularly in probabilistic settings. However, these gains come at a substantial computational cost, especially for larger, accuracy-driven ensembles. We found that reducing retraining frequency significantly lowers costs, with minimal impact on accuracy, particularly for point forecasts. Moreover, efficiency-driven ensembles offer a strong balance, achieving competitive accuracy with considerably lower costs compared to accuracy-optimized combinations. Most importantly, small ensembles of two or three models are often sufficient to achieve near-optimal results. These findings provide practical guidelines for deploying scalable and cost-efficient forecasting systems, supporting the broader goals of sustainable AI in forecasting. Overall, this work shows that careful ensemble design and retraining strategy selection can yield accurate, robust, and cost-effective forecasts suitable for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04677v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
  </channel>
</rss>
