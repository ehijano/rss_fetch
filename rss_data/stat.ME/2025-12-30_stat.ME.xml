<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Dec 2025 05:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Random Subset Averaging</title>
      <link>https://arxiv.org/abs/2512.22472</link>
      <description>arXiv:2512.22472v1 Announce Type: new 
Abstract: We propose a new ensemble prediction method, Random Subset Averaging (RSA), tailored for settings with many covariates, particularly in the presence of strong correlations. RSA constructs candidate models via binomial random subset strategy and aggregates their predictions through a two-round weighting scheme, resulting in a structure analogous to a two-layer neural network. All tuning parameters are selected via cross-validation, requiring no prior knowledge of covariate relevance. We establish the asymptotic optimality of RSA under general conditions, allowing the first-round weights to be data-dependent, and demonstrate that RSA achieves a lower finite-sample risk bound under orthogonal design. Simulation studies demonstrate that RSA consistently delivers superior and stable predictive performance across a wide range of sample sizes, dimensional settings, sparsity levels and correlation structures, outperforming conventional model selection and ensemble learning methods. An empirical application to financial return forecasting further illustrates its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22472v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Cui, Jie Hu</dc:creator>
    </item>
    <item>
      <title>On the Choice of Model Space Priors and Multiplicity Control in Bayesian Variable Selection: An Application to Streaming Logistic Regression</title>
      <link>https://arxiv.org/abs/2512.22504</link>
      <description>arXiv:2512.22504v1 Announce Type: new 
Abstract: Bayesian variable selection (BVS) depends critically on the specification of a prior distribution over the model space, particularly for controlling sparsity and multiplicity. This paper examines the practical consequences of different model space priors for BVS in logistic regression, with an emphasis on streaming data settings. We review some popular and well-known Beta--Binomial priors alongside the recently proposed matryoshka doll (MD) prior. We introduce a simple approximation to the MD prior that yields independent inclusion indicators and is convenient for scalable inference. Using BIC-based approximations to marginal likelihoods, we compare the effect of different model space priors on posterior inclusion probabilities and coefficient estimation at intermediate and final stages of the data stream via simulation studies. Overall, the results indicate that no single model space prior uniformly dominates across scenarios, and that the recently proposed MD prior provides a useful additional option that occupies an intermediate position between commonly used Beta--Binomial priors with differing degrees of sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22504v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>Bend to Mend: Toward Trustworthy Variational Bayes with Valid Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2512.22655</link>
      <description>arXiv:2512.22655v1 Announce Type: new 
Abstract: Variational Bayes (VB) is a popular and computationally efficient method to approximate the posterior distribution in Bayesian inference, especially when the exact posterior is analytically intractable and sampling-based approaches are computationally prohibitive. While VB often yields accurate point estimates, its uncertainty quantification (UQ) is known to be unreliable. For example, credible intervals derived from VB posteriors tend to exhibit undercoverage, failing to achieve nominal frequentist coverage probabilities. In this article, we address this challenge by proposing Trustworthy Variational Bayes (TVB), a method to recalibrate the UQ of broad classes of VB procedures. Our approach follows a bend-to-mend strategy: we intentionally misspecify the likelihood to correct VB's flawed UQ. In particular, we first relax VB by building on a recent fractional VB method, and then identify the optimal fraction parameter using conformal techniques such as sample splitting and bootstrapping. This yields recalibrated UQ for any given parameter of interest. On the theoretical front, we establish that the calibrated credible intervals achieve asymptotically correct frequentist coverage for a given parameter of interest; this, to the best of our knowledge, is the first such theoretical guarantee for VB. On the practical front, we introduce the "TVB table", which enables (1) massive parallelization and remains agnostic to the parameter of interest during its construction, and (2) efficient identification of the optimal fraction parameter for any specified parameter of interest. The proposed method is illustrated via Gaussian mixture models and Bayesian mixture linear regression models, and numerical experiments demonstrate that the TVB method outperforms standard VB and achieves normal frequentist coverage in finite samples. A real data application is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22655v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Liu, Meng Li</dc:creator>
    </item>
    <item>
      <title>Ranked Set Sampling in Survival Analysis</title>
      <link>https://arxiv.org/abs/2512.22659</link>
      <description>arXiv:2512.22659v1 Announce Type: new 
Abstract: Ranked set sampling (RSS) is a cost-efficient study design that uses inexpensive baseline ranking to select a more informative subset of individuals for full measurement. While RSS is well known to improve precision over simple random sampling (SRS) for uncensored outcomes, survival analysis under RSS has largely been limited to estimation of the Kaplan-Meier survival curve under random censoring. Consequently, many standard tools routinely used with SRS data, including log-rank and weighted log-rank tests, restricted mean survival time summaries, and window-based mean life measures, are not yet fully developed for RSS settings, particularly when ranking is imperfect and censoring is present.
  This work develops a unified survival analysis framework for balanced RSS designs that preserves efficiency gains while providing the inferential tools expected in applied practice. We formalize Kaplan-Meier and Nelson-Aalen estimators for right-censored data under both perfect and concomitant-based imperfect ranking and establish their large-sample properties using martingale and empirical process methods adapted to the rank-wise RSS structure. Rank-aware Greenwood-type variance estimators are proposed, and efficiency relative to SRS is evaluated through simulation studies varying set size, number of cycles, censoring proportion, and ranking quality. The framework is further extended to log-rank and Fleming-Harrington weighted tests, as well as restricted and window mean life functionals with asymptotic variance formulas and two-sample comparisons. An implementation plan with real-data illustrations is provided to facilitate practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22659v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nabil Awan, Richard J. Chappell</dc:creator>
    </item>
    <item>
      <title>Robust and Well-conditioned Sparse Estimation for High-dimensional Covariance Matrices</title>
      <link>https://arxiv.org/abs/2512.23250</link>
      <description>arXiv:2512.23250v1 Announce Type: new 
Abstract: Estimating covariance matrices with high-dimensional complex data presents significant challenges, particularly concerning positive definiteness, sparsity, and numerical stability. Existing robust sparse estimators often fail to guarantee positive definiteness in finite samples, while subsequent positive-definite correction can degrade sparsity and lack explicit control over the condition number. To address these limitations, we propose a novel robust and well-conditioned sparse covariance matrix estimator. Our key innovation is the direct incorporation of a condition number constraint within a robust adaptive thresholding framework. This constraint simultaneously ensures positive definiteness, enforces a controllable level of numerical stability, and preserves the desired sparse structure without resorting to post-hoc modifications that compromise sparsity. We formulate the estimation as a convex optimization problem and develop an efficient alternating direction algorithm with guaranteed convergence. Theoretically, we establish that the proposed estimator achieves the minimax optimal convergence rate under the Frobenius norm. Comprehensive simulations and real-data applications demonstrate that our method consistently produces positive definite, well-conditioned, and sparse estimates, and achieves comparable or superior numerical stability to eigenvalue-bound methods while requiring less tuning parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23250v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shaoxin Wang, Ziyun Ma</dc:creator>
    </item>
    <item>
      <title>A Wide-Sense Stationarity Test Based on the Geometric Structure of Covariance</title>
      <link>https://arxiv.org/abs/2512.23251</link>
      <description>arXiv:2512.23251v1 Announce Type: new 
Abstract: This paper presents a test for wide-sense stationarity (WSS) based on the geometry of the covariance function. We estimate local patches of the covariance surface and then check whether the directional derivative in the $(1,1,0)$ direction is zero on each patch. The method only requires the covariance function to be locally smooth and does not assume stationarity in advance. It can be applied to general stochastic dynamical systems and provides a time-resolved view. We apply the test method to an SDOF system and to a stochastic Duffing oscillator. These examples show that the method is numerically stable and can detect departures from WSS in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23251v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Yinbu, Xu Yong</dc:creator>
    </item>
    <item>
      <title>Intrinsic Whittle--Mat\'ern fields and sparse spatial extremes</title>
      <link>https://arxiv.org/abs/2512.23395</link>
      <description>arXiv:2512.23395v1 Announce Type: new 
Abstract: Intrinsic Gaussian fields are used in many areas of statistics as models for spatial or spatio-temporal dependence, or as priors for latent variables. However, there are two major gaps in the literature: first, the number and flexibility of existing intrinsic models are very limited; second, theory, fast inference, and software are currently underdeveloped for intrinsic fields. We tackle these challenges by introducing the new flexible class of intrinsic Whittle--Mat\'ern Gaussian random fields obtained as the solution to a stochastic partial differential equation (SPDE). Exploiting sparsity resulting from finite-element approximations, we develop fast estimation and simulation methods for these models. We demonstrate the benefits of this intrinsic SPDE approach for the important task of kriging under extrapolation settings. Leveraging the connection of intrinsic fields to spatial extreme value processes, we translate our theory to an SPDE approach for Brown--Resnick processes for sparse modeling of spatial extreme events. This new paradigm paves the way for efficient inference in unprecedented dimensions. To demonstrate the wide applicability of our new methodology, we apply it in two very different areas: a longitudinal study of renal function data, and the modeling of marine heat waves using high-resolution sea surface temperature data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23395v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Peter Braunsteins, Sebastian Engelke, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Uncertainty calibration for latent-variable regression models</title>
      <link>https://arxiv.org/abs/2512.23444</link>
      <description>arXiv:2512.23444v1 Announce Type: new 
Abstract: Uncertainty quantification is essential for scientific analysis, as it allows for the evaluation and interpretation of variability and reliability in complex systems and datasets. In their original form, multivariate statistical regression models (partial least-squares regression, PLS, principal component regression, PCR) along with their kernelized versions (kernel partial least-squares regression, K-PLS, kernel principal component regression, K-PCR), do not incorporate uncertainty quantification as part of their output. In this study, we propose a method inspired by conformal inference to estimate and calibrate the uncertainty of multivariate statistical models. The result of this method is a point prediction accompanied by prediction intervals that depend on the input data. We tested the proposed method on both traditional and kernelized versions of PLS and PCR. The method is demonstrated using synthetic data, as well as laboratory near-infrared (NIR) and airborne hyperspectral regression models for estimating functional plant traits. The model was able to successfully identify the uncertain regions in the simulated data and match the magnitude of the uncertainty. In real-case scenarios, the optimised model was not overconfident nor underconfident when estimating from test data: for example, for a 95% prediction interval, 95% of the true observations were inside the prediction interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23444v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zina-Sabrina Duma, Otto Lamminp\"a\"a, Jouni Susiluoto, Heikki Haario, Ting Zheng, Tuomas Sihvonen, Amy Braverman, Philip A. Townsend, Satu-Pia Reinikainen</dc:creator>
    </item>
    <item>
      <title>Propensity Patchwork Kriging for Scalable Inference on Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2512.23467</link>
      <description>arXiv:2512.23467v1 Announce Type: new 
Abstract: Gaussian process-based models are attractive for estimating heterogeneous treatment effects (HTE), but their computational cost limits scalability in causal inference settings. In this work, we address this challenge by extending Patchwork Kriging into the causal inference framework. Our proposed method partitions the data according to the estimated propensity score and applies Patchwork Kriging to enforce continuity of HTE estimates across adjacent regions. By imposing continuity constraints only along the propensity score dimension, rather than the full covariate space, the proposed approach substantially reduces computational cost while avoiding discontinuities inherent in simple local approximations. The resulting method can be interpreted as a smoothing extension of stratification and provides an efficient approach to HTE estimation. The proposed method is demonstrated through simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23467v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajime Ogawa, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Deep classifier kriging for probabilistic spatial prediction of air quality index</title>
      <link>https://arxiv.org/abs/2512.23474</link>
      <description>arXiv:2512.23474v1 Announce Type: new 
Abstract: Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23474v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyu Chen, Pratik Nag, Huixia Judy-Wang, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Panel Coupled Matrix-Tensor Clustering Model with Applications to Asset Pricing</title>
      <link>https://arxiv.org/abs/2512.23567</link>
      <description>arXiv:2512.23567v1 Announce Type: new 
Abstract: We tackle the challenge of estimating grouping structures and factor loadings in asset pricing models, where traditional regressions struggle due to sparse data and high noise. Existing approaches, such as those using fused penalties and multi-task learning, often enforce coefficient homogeneity across cross-sectional units, reducing flexibility. Clustering methods (e.g., spectral clustering, Lloyd's algorithm) achieve consistent recovery under specific conditions but typically rely on a single data source. To address these limitations, we introduce the Panel Coupled Matrix-Tensor Clustering (PMTC) model, which simultaneously leverages a characteristics tensor and a return matrix to identify latent asset groups. By integrating these data sources, we develop computationally efficient tensor clustering algorithms that enhance both clustering accuracy and factor loading estimation. Simulations demonstrate that our methods outperform single-source alternatives in clustering accuracy and coefficient estimation, particularly under moderate signal-to-noise conditions. Empirical application to U.S. equities demonstrates the practical value of PMTC, yielding higher out-of-sample total $R^2$ and economically interpretable variation in factor exposures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23567v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Liyuan Cui, Guanhao Feng, Yuefeng Han, Jiayan Li</dc:creator>
    </item>
    <item>
      <title>Considering parallel tempering and comparing post-treatment procedures in Bayesian Profile Regression Models for a survival outcome and correlated exposures</title>
      <link>https://arxiv.org/abs/2512.23571</link>
      <description>arXiv:2512.23571v1 Announce Type: new 
Abstract: Bayesian profile regression mixture models (BPRM) allow to assess a health risk in a multi-exposed population. These mixture models cluster individuals according to their exposure profile and their health risk. However, their results, based on Monte-Carlo Markov Chain (MCMC) algorithms, turned out to be unstable in different application cases. We suppose two reasons for this instability. The MCMC algorithm can be trapped in local modes of the posterior distribution and the choice of post-treatment procedures used on the output of the MCMC algorithm leads to different clustering structures. In this work, we propose improvements of the MCMC algorithms proposed in previous works in order to avoid the local modes of the posterior distribution while reducing the computation time. We also carry out a simulation study to compare the performances of the MCMC algorithms and different post-processing in order to provide guidelines on their use. An application in radiation epidemiology is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23571v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fendler Julie, Guihenneuc Chantal, Ancelet Sophie</dc:creator>
    </item>
    <item>
      <title>Profile Bayesian Optimization for Expensive Computer Experiments</title>
      <link>https://arxiv.org/abs/2512.23581</link>
      <description>arXiv:2512.23581v1 Announce Type: new 
Abstract: We propose a novel Bayesian optimization (BO) procedure aimed at identifying the ``profile optima'' of a deterministic black-box computer simulation that has a single control parameter and multiple nuisance parameters. The profile optima capture the optimal response values as a function of the control parameter. Our objective is to identify them across the entire plausible range of the control parameter. Classic BO, which targets a single optimum over all parameters, does not explore the entire control parameter range. Instead, we develop a novel two-stage acquisition scheme to balance exploration across the control parameter and exploitation of the profile optima, leveraging deep and shallow Gaussian process surrogates to facilitate uncertainty quantification. We are motivated by a computer simulation of a diffuser in a rotating detonation combustion engine, which returns the energy lost through diffusion as a function of various design parameters. We aim to identify the lowest possible energy loss as a function of the diffuser's length; understanding this relationship will enable well-informed design choices. Our ``profile Bayesian optimization'' procedure outperforms traditional BO and profile optimization methods on a variety of benchmarks and proves effective in our motivating application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23581v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Courtney Kyger, James Fernandez, John A. Grunenwald, James Braun, Annie Booth</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Longitudinal and Survival Data: A Bayesian Approach for Predicting Disease Progression</title>
      <link>https://arxiv.org/abs/2512.23627</link>
      <description>arXiv:2512.23627v1 Announce Type: new 
Abstract: Joint modeling of longitudinal and survival data has become increasingly important in medical research, particularly for understanding disease progression in chronic conditions where both repeated biomarker measurements and time-to-event outcomes are available. Traditional two-stage methods, which analyze longitudinal and survival components separately, often result in biased estimates and suboptimal predictions due to failure to account for their interdependence.
  In this study, we propose a Bayesian hierarchical joint modeling framework with an emphasis on predictive evaluation and clinical interpretability. The model simultaneously characterizes the longitudinal trajectory of a biomarker and the associated survival outcome through shared random effects, capturing the intrinsic association between disease dynamics and event risk. The Bayesian formulation allows flexible incorporation of prior information, accommodates irregular measurement times and missing data, and provides full posterior distributions for uncertainty quantification via credible intervals.
  We evaluate the proposed framework using both simulated data designed to mimic realistic patient trajectories and a real-world clinical dataset involving patients with chronic liver disease. Results demonstrate that the Bayesian joint model consistently outperforms conventional two-stage approaches in terms of parameter estimation accuracy and predictive performance, as measured by time-dependent area under the curve and Brier scores. The proposed approach provides a robust and interpretable tool for dynamic, patient-specific prognosis, supporting clinical decision-making in personalized medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23627v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nithisha Suryadevara, Vivek Reddy Srigiri</dc:creator>
    </item>
    <item>
      <title>Exchangeability and randomness for infinite and finite sequences</title>
      <link>https://arxiv.org/abs/2512.22162</link>
      <description>arXiv:2512.22162v1 Announce Type: cross 
Abstract: Randomness (in the sense of being generated in an IID fashion) and exchangeability are standard assumptions in nonparametric statistics and machine learning, and relations between them have been a popular topic of research. This note draws the reader's attention to the fact that, while for infinite sequences of observations the two assumptions are almost indistinguishable, the difference between them becomes very significant for finite sequences of a given length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22162v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Robust Liu-Type Estimation for Multicollinearity in Fuzzy Logistic Regression</title>
      <link>https://arxiv.org/abs/2512.22515</link>
      <description>arXiv:2512.22515v1 Announce Type: cross 
Abstract: This article addresses the fuzzy logistic regression model under conditions of multicollinearity, which causes instability and inflated variance in parameter estimation. In this model, both the response variable and parameters are represented as fuzzy triangular numbers. To overcome the multicollinearity problem, various Liu-type estimators were employed: Fuzzy Maximum Likelihood Estimators (FMLE), Fuzzy Logistic Ridge Estimators (FLRE), Fuzzy Logistic Liu Estimators (FLLE), Fuzzy Logistic Liu-type Estimators (FLLTE), and Fuzzy Logistic Liu-type Parameter Estimators (FLLTPE). Through simulations with various sample sizes and application to real fuzzy data on kidney failure, model performance was evaluated using mean square error (MSE) and goodness of fit criteria. Results demonstrated superior performance of FLLTPE and FLLTE compared to other estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22515v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.52866/2788-7421.1311</arxiv:DOI>
      <dc:creator>Ayad Habib Shemail, Ahmed Razzaq Al-Lami, Amal Hadi Rashid</dc:creator>
    </item>
    <item>
      <title>Likelihood-Preserving Embeddings for Statistical Inference</title>
      <link>https://arxiv.org/abs/2512.22638</link>
      <description>arXiv:2512.22638v1 Announce Type: cross 
Abstract: Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $\Delta_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $\Delta_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $\Delta_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22638v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Akdemir</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Near-Optimal Estimation over Certain Type-2 Convex Bodies</title>
      <link>https://arxiv.org/abs/2512.22714</link>
      <description>arXiv:2512.22714v1 Announce Type: cross 
Abstract: We develop polynomial-time algorithms for near-optimal minimax mean estimation under $\ell_2$-squared loss in a Gaussian sequence model under convex constraints. The parameter space is an origin-symmetric, type-2 convex body $K \subset \mathbb{R}^n$, and we assume additional regularity conditions: specifically, we assume $K$ is well-balanced, i.e., there exist known radii $r, R &gt; 0$ such that $r B_2 \subseteq K \subseteq R B_2$, as well as oracle access to the Minkowski gauge of $K$. Under these and some further assumptions on $K$, our procedures achieve the minimax rate up to small factors, depending poly-logarithmically on the dimension, while remaining computationally efficient.
  We further extend our methodology to the linear regression and robust heavy-tailed settings, establishing polynomial-time near-optimal estimators when the constraint set satisfies the regularity conditions above. To the best of our knowledge, these results provide the first general framework for attaining statistically near-optimal performance under such broad geometric constraints while preserving computational tractability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22714v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Causal-Policy Forest for End-to-End Policy Learning</title>
      <link>https://arxiv.org/abs/2512.22846</link>
      <description>arXiv:2512.22846v1 Announce Type: cross 
Abstract: This study proposes an end-to-end algorithm for policy learning in causal inference. We observe data consisting of covariates, treatment assignments, and outcomes, where only the outcome corresponding to the assigned treatment is observed. The goal of policy learning is to train a policy from the observed data, where a policy is a function that recommends an optimal treatment for each individual, to maximize the policy value. In this study, we first show that maximizing the policy value is equivalent to minimizing the mean squared error for the conditional average treatment effect (CATE) under $\{-1, 1\}$ restricted regression models. Based on this finding, we modify the causal forest, an end-to-end CATE estimation algorithm, for policy learning. We refer to our algorithm as the causal-policy forest. Our algorithm has three advantages. First, it is a simple modification of an existing, widely used CATE estimation method, therefore, it helps bridge the gap between policy learning and CATE estimation in practice. Second, while existing studies typically estimate nuisance parameters for policy learning as a separate task, our algorithm trains the policy in a more end-to-end manner. Third, as in standard decision trees and random forests, we train the models efficiently, avoiding computational intractability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22846v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification of Demand without Exogenous Product Characteristics</title>
      <link>https://arxiv.org/abs/2512.23211</link>
      <description>arXiv:2512.23211v1 Announce Type: cross 
Abstract: We study the identification of differentiated product demand with exogenous supply-side instruments, allowing product characteristics to be endogenous. Past analyses have argued that exogenous characteristic-based instruments are essentially necessary given a sufficiently flexible demand model with a suitable index restriction. We show, however, that price counterfactuals are nonparametrically identified by recentered instruments -- which combine exogenous shocks to prices with endogenous product characteristics -- under a weaker index restriction and a new condition we term faithfulness. We argue that faithfulness, like the usual completeness condition for nonparametric identification with instruments, can be viewed as a technical requirement on the richness of identifying variation rather than a substantive economic restriction, and we show that it holds under a variety of non-nested conditions on either price-setting or the index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23211v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Borusyak, Jiafeng Chen, Peter Hull, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction = Bayes?</title>
      <link>https://arxiv.org/abs/2512.23308</link>
      <description>arXiv:2512.23308v1 Announce Type: cross 
Abstract: Conformal prediction (CP) is widely presented as distribution-free predictive inference with finite-sample marginal coverage under exchangeability. We argue that CP is best understood as a rank-calibrated descendant of the Fisher-Dempster-Hill fiducial/direct-probability tradition rather than as Bayesian conditioning in disguise.
  We establish four separations from coherent countably additive predictive semantics. First, canonical conformal constructions violate conditional extensionality: prediction sets can depend on the marginal design P(X) even when P(Y|X) is fixed. Second, any finitely additive sequential extension preserving rank calibration is nonconglomerable, implying countable Dutch-book vulnerabilities. Third, rank-calibrated updates cannot be realized as regular conditionals of any countably additive exchangeable law on Y^infty. Fourth, formalizing both paradigms as families of one-step predictive kernels, conformal and Bayesian kernels coincide only on a Baire-meagre subset of the space of predictive laws.
  We further show that rank- and proxy-based reductions are generically Blackwell-deficient relative to full-data experiments, yielding positive Le Cam deficiency for suitable losses. Extending the analysis to prediction-powered inference (PPI) yields an analogous message: bias-corrected, proxy-rectified estimators can be valid as confidence devices while failing to define transportable belief states across stages, shifts, or adaptive selection. Together, the results sharpen a general limitation of wrappers: finite-sample calibration guarantees do not by themselves supply composable semantics for sequential updating or downstream decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23308v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson, Vadim Sokolov, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modelling is Sufficient for Causal Inference</title>
      <link>https://arxiv.org/abs/2512.23408</link>
      <description>arXiv:2512.23408v1 Announce Type: cross 
Abstract: Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23408v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:81810-81840, 2025</arxiv:journal_reference>
      <dc:creator>Bruno Mlodozeniec, David Krueger, Richard E. Turner</dc:creator>
    </item>
    <item>
      <title>Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</title>
      <link>https://arxiv.org/abs/2512.23617</link>
      <description>arXiv:2512.23617v1 Announce Type: cross 
Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $\delta(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23617v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Akdemir</dc:creator>
    </item>
    <item>
      <title>Calibrated Multi-Level Quantile Forecasting</title>
      <link>https://arxiv.org/abs/2512.23671</link>
      <description>arXiv:2512.23671v1 Announce Type: cross 
Abstract: We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $\alpha$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $\alpha$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23671v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Cross-Population Amplitude Coupling in High-Dimensional Oscillatory Neural Time Series</title>
      <link>https://arxiv.org/abs/2105.03508</link>
      <description>arXiv:2105.03508v4 Announce Type: replace 
Abstract: Neural oscillations have long been considered important markers of interaction across brain regions, yet identifying coordinated oscillatory activity from high-dimensional multiple-electrode recordings remains challenging. We sought to quantify time-varying covariation of oscillatory amplitudes across two brain regions, during a memory task, based on local field potentials recorded from 96 electrodes in each region. We extended Canonical Correlation Analysis (CCA) to multiple time series through the cross-correlation of latent time series. This, however, introduces a large number of possible lead-lag cross-correlations across the two regions. To manage that high dimensionality we developed rigorous statistical procedures aimed at finding a small number of dominant lead-lag effects. The method correctly identified ground truth structure in realistic simulation-based settings. When we used it to analyze local field potentials recorded from prefrontal cortex and visual area V4 we obtained highly plausible results. The new statistical methodology could also be applied to other slowly-varying high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03508v4</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Eric A. Yttri, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
    <item>
      <title>Bayesian Image Mediation Analysis</title>
      <link>https://arxiv.org/abs/2310.16284</link>
      <description>arXiv:2310.16284v2 Announce Type: replace 
Abstract: Mediation analysis aims to separate the indirect effect through mediators from the direct effect of the exposure on the outcome. It is challenging to perform mediation analysis with neuroimaging data which involves high dimensionality, complex spatial correlations, sparse activation patterns and relatively low signal-to-noise ratio. To address these issues, we develop a new spatially varying coefficient structural equation model for Bayesian Image Mediation Analysis (BIMA). We define spatially varying mediation effects within the potential outcomes framework, employing a soft-thresholded Gaussian process prior for functional parameters. We establish posterior consistency for spatially varying mediation effects along with selection consistency on important regions that contribute to the mediation effects. We develop an efficient posterior computation algorithm scalable to analysis of large-scale imaging data. Through extensive simulations, we show that BIMA can improve the estimation accuracy and computational efficiency for high-dimensional mediation analysis over existing methods. We apply BIMA to analyze behavioral and fMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a focus on inferring the mediation effects of the parental education level on the children's general cognitive ability that are mediated through the working memory brain activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16284v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Timothy D Johnson, Mary Heitzeg, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Time-Varying Multi-Seasonal AR Models</title>
      <link>https://arxiv.org/abs/2409.18640</link>
      <description>arXiv:2409.18640v2 Announce Type: replace 
Abstract: We propose a seasonal AR model with time-varying parameter processes in both the regular and seasonal parameters. The model is parameterized to guarantee stability at every time point and can accommodate multiple seasonal periods. The time evolution is modeled by dynamic shrinkage processes to allow for long periods of essentially constant parameters, periods of rapid change, and abrupt jumps. A Gibbs sampler is developed with a particle Gibbs update step for the AR parameter trajectories. We show that the near-degeneracy of the model, caused by the dynamic shrinkage processes, makes it challenging to estimate the model by particle methods. To address this, a more robust, faster and accurate approximate sampler based on the extended Kalman filter is proposed. The model and the numerical effectiveness of the Gibbs sampler are investigated on simulated data. An application to more than a century of monthly US industrial production data shows interesting clear changes in seasonality over time, particularly during the Great Depression and the recent Covid-19 pandemic. Keywords: Bayesian inference; Extended Kalman filter; Particle MCMC; Seasonality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18640v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ganna Fagerberg, Mattias Villani, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>A Two-Step Projection-Based Goodness-of-Fit Test for Ultra-High Dimensional Sparse Regressions</title>
      <link>https://arxiv.org/abs/2412.10721</link>
      <description>arXiv:2412.10721v3 Announce Type: replace 
Abstract: This paper proposes a novel two-step strategy for testing the goodness-of-fit of parametric regression models in ultra-high dimensional sparse settings, where the predictor dimension far exceeds the sample size. This regime usually renders existing goodness-of-fit tests for regressions infeasible, primarily due to the curse of dimensionality or their reliance on the asymptotic linearity and normality of parameter estimators -- properties that may no longer hold under ultra-high dimensional settings. To address these limitations, our strategy first constructs multiple test statistics based on projected predictors from distinct projections and establishes their asymptotic properties under both the null and alternative hypotheses. This projection-based approach significantly mitigates the dimensionality problem, enabling our tests to detect local alternatives converging to the null at the rate as if the predictor were univariate. An important finding is that the resulting test statistics based on linearly independent projections are asymptotically independent under the null hypothesis. Based on this, our second step employs powerful $p$-value combination procedures, such as the minimum $p$-value and the Fisher combination of $p$-value, to form our final tests and enhance power. Theoretically, our tests only require the standard convergence rate of parameter estimators to derive their limiting distributions, thereby circumventing the need for asymptotic linearity or normality of parameter estimators. Simulations and real-data applications confirm that our approach provides robust and powerful goodness-of-fit testing in ultra-high dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10721v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Falong Tan, Jie Liu, Heng Peng, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Deep Computerized Adaptive Testing</title>
      <link>https://arxiv.org/abs/2502.19275</link>
      <description>arXiv:2502.19275v3 Announce Type: replace 
Abstract: Computerized adaptive tests (CATs) play a crucial role in educational assessment and diagnostic screening in behavioral health. Unlike traditional linear tests that administer a fixed set of pre-assembled items, CATs adaptively tailor the test to an examinee's latent trait level by selecting a smaller subset of items based on their previous responses. Existing CAT frameworks predominantly rely on item response theory (IRT) models with a single latent variable, a choice driven by both conceptual simplicity and computational feasibility. However, many real-world item response datasets exhibit complex, multi-factor structures, limiting the applicability of CATs in broader settings. In this work, we develop a novel CAT system that incorporates multivariate latent traits, building on recent advances in Bayesian sparse multivariate IRT. Our approach leverages direct sampling from the latent factor posterior distributions, significantly accelerating existing information-theoretic item selection criteria by eliminating the need for computationally intensive Markov Chain Monte Carlo (MCMC) simulations. Recognizing the potential sub-optimality of existing item selection rules, which are often based on myopic one-step-lookahead optimization of some information-theoretic criterion, we propose a double deep Q-learning algorithm to learn an optimal item selection policy. Through simulation and real-data studies, we demonstrate that our approach not only accelerates existing item selection methods but also highlights the potential of reinforcement learning in CATs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19275v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiguang Li, Robert Gibbons, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth</title>
      <link>https://arxiv.org/abs/2505.08128</link>
      <description>arXiv:2505.08128v3 Announce Type: replace 
Abstract: The standard A/B testing approaches are mostly based on t-test in large scale industry applications. These standard approaches however suffers from low statistical power in business settings, due to nature of small sample-size or non-Gaussian distribution or return-on-investment (ROI) consideration. In this paper, we (i) show the statistical efficiency of using estimating equation and U statistics, which can address these issues separately; and (ii) propose a novel doubly robust generalized U that allows flexible definition of treatment effect, and can handles small samples, distribution robustness, ROI and confounding consideration in one framework. We provide theoretical results on asymptotics and efficiency bounds, together with insights on the efficiency gain from theoretical analysis. We further conduct comprehensive simulation studies, apply the methods to multiple real A/B tests at LinkedIn, and share results and learnings that are broadly useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08128v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changshuai Wei, Phuc Nguyen, Benjamin Zelditch, Joyce Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian Semiparametric Orthogonal Tucker Factorized Mixed Models for Multi-dimensional Longitudinal Functional Data</title>
      <link>https://arxiv.org/abs/2506.16668</link>
      <description>arXiv:2506.16668v2 Announce Type: replace 
Abstract: We introduce a novel longitudinal mixed model for analyzing complex multidimensional functional data, addressing challenges such as high-resolution, structural complexities, and computational demands. Our approach integrates dimension reduction techniques, including basis function representation and Tucker tensor decomposition, to model complex functional (e.g., spatial and temporal) variations, group differences, and individual heterogeneity while drastically reducing model dimensions. The model accommodates multiplicative random effects whose marginalization yields a novel Tucker-decomposed covariance-tensor framework. To ensure scalability, we employ semi-orthogonal mode matrices implemented via a novel graph-Laplacian-based smoothness prior with low-rank approximation, leading to an efficient posterior sampling method. A cumulative shrinkage strategy promotes sparsity and enables semiautomated rank selection. We establish theoretical guarantees for posterior convergence and demonstrate the method's effectiveness through simulations, showing significant improvements over existing techniques. Applying the method to Alzheimer's Disease Neuroimaging Initiative (ADNI) neuroimaging data reveals novel insights into local brain changes associated with disease progression, highlighting the method's practical utility for studying cognitive decline and neurodegenerative conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16668v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Abhra Sarkar</dc:creator>
    </item>
    <item>
      <title>Bayesian Variational Inference for Mixed Data Mixture Models</title>
      <link>https://arxiv.org/abs/2507.16545</link>
      <description>arXiv:2507.16545v2 Announce Type: replace 
Abstract: Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. We provide theoretical justification for our method, showing that as the sample size $n$ tends to infinity, the variational posterior mean converges locally to the true data-generating parameter value, and that it converges locally to the maximum likelihood estimator at the rate of $O(1/n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16545v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyang Wang, James Bennett, Victor Lhoste, Sarah Filippi</dc:creator>
    </item>
    <item>
      <title>Is Repeated Bayesian Interim Analysis Consequence-Free?</title>
      <link>https://arxiv.org/abs/2508.07403</link>
      <description>arXiv:2508.07403v2 Announce Type: replace 
Abstract: Interim analyses are vital in clinical trials for early decision-making. While frequentist implications are well-established, the consequences of repeated Bayesian interim monitoring for efficacy, specifically regarding multiplicity, remain contentious. This article provides theoretical justification and numerical evidence evaluating the impact of such designs on bias, mean squared error (MSE), credible interval coverage, false discovery rate (FDR), and average Type I error (ATIE). Our findings show that when the inferential prior matches the data-generating prior, sequential efficacy stopping does not bias the posterior mean or degrade credible interval coverage. However, even under this ``matched" condition, the FDR, ATIE, and MSE are significantly altered. In the more practically relevant scenario where the inferential and data-generating priors differ, all aforementioned operating characteristics, including estimation bias and coverage, are substantially impacted. These results reconcile long-standing conflicting arguments regarding Bayesian multiplicity. We demonstrate that while some Bayesian properties are invariant to sequential looks, others are not. Our work underscores the necessity of thoughtful prior specification and comprehensive evaluation of frequentist-Bayesian operating characteristics to ensure reliable inference in adaptive trial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07403v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyu Liu, Beibei Guo, Laura Thompson, Lei Nie, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Estimating the growth rate of a birth and death process using data from a small sample</title>
      <link>https://arxiv.org/abs/2508.16110</link>
      <description>arXiv:2508.16110v2 Announce Type: replace 
Abstract: The problem of estimating the growth rate of a birth and death processes based on the coalescence times of a sample of $n$ individuals has been considered by several authors (\cite{stadler2009incomplete, williams2022life, mitchell2022clonal, Johnson2023}). This problem has applications, for example, to cancer research, when one is interested in determining the growth rate of a clone.
  Recently, \cite{Johnson2023} proposed an analytical method for estimating the growth rate using the theory of coalescent point processes, which has comparable accuracy to more computationally intensive methods when the sample size $n$ is large. We use a similar approach to obtain an estimate of the growth rate that is not based on the assumption that $n$ is large.
  We demonstrate, through simulations using the R package \texttt{cloneRate}, that our estimator of the growth rate performs well in comparison with previous approaches when $n$ is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16110v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carola Sophia Heinzel, Jason Schweinsberg</dc:creator>
    </item>
    <item>
      <title>Bayesian Field Theory of the Rate Estimation</title>
      <link>https://arxiv.org/abs/2509.06129</link>
      <description>arXiv:2509.06129v2 Announce Type: replace 
Abstract: We address the statistical inference of a time-dependent rate of events in the framework of Bayesian field theory. This maps the problem to a Langevin equation which, beyond the local linear regime taken as reference, involves nonlinearities and an explicit dependence on the local shape of the maximum likelihood curve. We study the corresponding impacts in a perturbative expansion, formulating a scaling hypothesis for the order of shape corrections. We find that the pure nonlinearities dominate the mean and skewness. Crucially, we uncover that the leading correction to the variance is driven by noise propagation from the signal's effective curvature. We test the derived expansion with numerical simulations and illustrate its applicability on real neural spike data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06129v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Auconi, Alessandro Codello, Raffaella Burioni, Guido Caldarelli</dc:creator>
    </item>
    <item>
      <title>Robust Spatial Confounding Adjustment via Basis Voting</title>
      <link>https://arxiv.org/abs/2510.22464</link>
      <description>arXiv:2510.22464v2 Announce Type: replace 
Abstract: Estimating effects of spatially structured exposures is complicated by unmeasured spatial confounders, which undermine identifiability in spatial linear regression models unless structural assumptions are imposed. We develop a general framework for effect estimation in spatial regression models that relaxes the commonly assumed requirement that exposures contain higher-frequency variation than confounders. We propose basis voting, a plurality-rule estimator - novel in the spatial literature - that consistently identifies causal effects only under the assumption that, in a spatial basis expansion of the exposure and confounder, there exist several basis functions in the support of the exposure but not the confounder. This assumption generalizes existing assumptions of differential basis support used for identification of the causal effect under spatial confounding, and does not require prior knowledge of which basis functions satisfy this support condition. We design this estimator as the mode of several candidate estimators each computed based on a single working basis function. We also show that the standard projection-based candidate estimator typically used in other plurality-rule based methods is inefficient, and provide a more efficient novel candidate. Extensive simulations and a real-world application demonstrate that our approach reliably recovers unbiased causal estimates whenever exposure and confounder signals are separable on a plurality of basis functions. By not relying on higher-frequency variation, our method remains applicable to settings where exposures are smooth spatial functions, such as distance to pollution sources or major roadways, common in environmental studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22464v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anik Burman, Elizabeth L. Ogburn, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Multiple Testing of Partial Conjunction Hypotheses for Assessing Replicability Across Dependent Studies</title>
      <link>https://arxiv.org/abs/2511.04130</link>
      <description>arXiv:2511.04130v2 Announce Type: replace 
Abstract: Replicability is central to scientific progress, and the partial conjunction (PC) hypothesis testing framework provides an objective tool to quantify it across disciplines. Existing PC methods assume independent studies. Yet many modern applications, such as genome-wide association studies (GWAS) with sample overlap, violate this assumption, leading to dependence among study-specific summary statistics. Failure to account for this dependence can drastically inflate type I errors when combining inferences. We propose e-Filter, a powerful procedure grounded on the theory of e-values. It involves a filtering step that retains a set of the most promising PC hypotheses, and a selection step where PC hypotheses from the filtering step are marked as discoveries whenever their e-values exceed a selection threshold. We establish the validity of e-Filter for FWER and FDR control under unknown study dependence. A comprehensive simulation study demonstrates its excellent power gains over competing methods. We apply e-Filter to a GWAS replicability study to identify consistent genetic signals for low-density lipoprotein cholesterol (LDL-C). Here, the participating studies exhibit varying levels of sample overlap, rendering existing methods unsuitable for combining inferences. A subsequent pathway enrichment analysis shows that e-Filter replicated signals achieve stronger statistical enrichment on biologically relevant LDL-C pathways than competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04130v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Trambak Banerjee, Prajamitra Bhuyan, Arunabha Majumdar</dc:creator>
    </item>
    <item>
      <title>Optimal Hold-Out Size in Cross-Validation</title>
      <link>https://arxiv.org/abs/2511.12698</link>
      <description>arXiv:2511.12698v2 Announce Type: replace 
Abstract: Cross-validation (CV) is a standard technique used across science to test how well a model predicts new data. Data are split into $K$ ``folds,'' where one fold (i.e., hold-out set) is used to evaluate a model's predictive ability, with folds cycled in standard $K$-fold CV. Researchers typically rely on conventions when choosing the hold-out size, commonly $80/20$ split, or $K=5$, even though this choice can affect inference and model evaluation. Principally, this split should be determined by balancing the predictive accuracy (bias) and the uncertainty of this accuracy (variance), which forms a tradeoff based on the size of the hold-out set. More training data means more accurate models, but fewer testing data lead to uncertain evaluation, and vice versa. The challenge is that this evaluation uncertainty cannot be identified, without strong assumptions, directly from data. We propose a procedure to determine the optimal hold-out size by deriving a finite-sample exact expression and upper bound on the evaluation uncertainty, depending on the error assumption, and adopting a utility-based approach to make this tradeoff explicit. Analyses of real-world datasets using linear regression and random forest demonstrate this procedure in practice, providing insight into implicit assumptions, robustness, and model performance. Critically, the results show that the optimal hold-out size depends on both the data and the model, and that conventional choices implicitly make assumptions about the fundamental characteristics of the data. Our framework makes these assumptions explicit and provides a principled, transparent way to select this split based on the data and model rather than convention. By replacing a one-size-fits-all choice with context-specific reasoning, it enables more reliable comparisons of predictive performance across scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12698v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenichiro McAlinn, Kosaku Takanashi</dc:creator>
    </item>
    <item>
      <title>A flexible class of latent variable models for the analysis of antibody response data</title>
      <link>https://arxiv.org/abs/2512.14504</link>
      <description>arXiv:2512.14504v2 Announce Type: replace 
Abstract: Existing approaches to modelling antibody concentration data are mostly based on finite mixture models that rely on the assumption that individuals can be divided into two distinct groups: seronegative and seropositive. Here, we challenge this dichotomous modelling assumption and propose a latent variable modelling framework in which the immune status of each individual is represented along a continuum of latent seroreactivity, ranging from minimal to strong immune activation. This formulation provides greater flexibility in capturing age-related changes in antibody distributions while preserving the full information content of quantitative measurements. We show that the proposed class of models can accommodate a large variety of model formulations, both mechanistic and regression-based, and also includes finite mixture models as a special case. We also propose a computationally efficient $L_2$-based estimator as an alternative to maximum likelihood estimation, which substantially reduces computational cost, and we establish its consistency. Through a case study on malaria serology, we demonstrate how the flexibility of the novel framework enables joint analyses across all ages while accounting for changes in transmission patterns. We conclude by outlining extensions of the proposed modelling framework and its relevance to other omics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14504v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Giorgi, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>Modeling Issues with Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2512.15950</link>
      <description>arXiv:2512.15950v4 Announce Type: replace 
Abstract: I describe and compare procedures for binary eye-tracking (ET) data. The basic GLM model is a logistic mixed model combined with random effects for persons and items. Additional models address error correlation in eye-tracking serial observations. In particular, three novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model and a first-order moving average models obtained with generalized estimating equations, and a recurrent two-state survival model used with run-length encoded data. Altogether, the results of five different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development. A more traditional model incorporating a lag-1 observed outcome for serial correlation is also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15950v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Nonparametric Treatment Effect Identification in School Choice</title>
      <link>https://arxiv.org/abs/2112.03872</link>
      <description>arXiv:2112.03872v5 Announce Type: replace-cross 
Abstract: This paper studies nonparametric identification and estimation of causal effects in centralized school assignment. In many centralized assignment algorithms, students face both lottery-driven variation and regression discontinuity- (RD) driven variation. We characterize the full set of identified atomic treatment effects (aTEs), defined as the conditional average treatment effect between a pair of schools given student characteristics. Atomic treatment effects are the building blocks of more aggregated treatment contrasts, and common approaches to estimating aTE aggregations can mask important heterogeneity. In particular, many aggregations of aTEs put zero weight on aTEs driven by RD variation, and estimators of such aggregations put asymptotically vanishing weight on the RD-driven aTEs. We provide a diagnostic and recommend new aggregation schemes. Lastly, we provide estimators and asymptotic results for inference on these aggregations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03872v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Causal Inference Under Network Confounding</title>
      <link>https://arxiv.org/abs/2211.07823</link>
      <description>arXiv:2211.07823v5 Announce Type: replace-cross 
Abstract: This paper studies causal inference with observational data from a single large network. We consider a nonparametric model with interference in both potential outcomes and selection into treatment. Specifically, both stages may be the outcomes of simultaneous equations models, allowing for endogenous peer effects. This results in high-dimensional network confounding where the network and covariates of all units constitute sources of selection bias. In contrast, the existing literature assumes that confounding can be summarized by a known, low-dimensional function of these objects. We propose to use graph neural networks (GNNs) to adjust for network confounding. When interference decays with network distance, we argue that the model has low-dimensional structure that makes estimation feasible and justifies the use of shallow GNN architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07823v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung, Pantelis Loupos</dc:creator>
    </item>
    <item>
      <title>A Paradigm Shift in Human Neuroscience Research: Progress, Prospects, and a Proof of Concept for Population Neuroscience</title>
      <link>https://arxiv.org/abs/2212.04195</link>
      <description>arXiv:2212.04195v3 Announce Type: replace-cross 
Abstract: Recent advances and reflections on reproducible human neuroscience, especially brain-wide association studies (BWAS) leveraging large datasets, have led to divergent and sometimes opposing views on research practices and priorities. The debates span multiple dimensions. Shifts along these axes have fractured consensus and further fragmented an already heterogeneous field of cognitive neuroscience. Here, we sketch a holistic and integrative response grounded in population neuroscience, organized around a closed-loop "design-analysis-interpretation" research cycle that aims to build consensus while bridging these divides. Our central claim is that population neuroscience offers a unique population-level vantage point for identifying general principles, characterizing inter-individual variabilities, and benchmarking intra-individual changes, thereby providing a supportive framework for small-scale, mechanism-focused studies at the individual level and allowing them to co-evolve with population-level studies. Population neuroscience is not simply about providing larger N for BWAS; its deeper goal is to accumulate a family of cross-scale priors and shared infrastructures that can support design, analysis, and interpretation of human neuroscience for decades to come. In this sense, we outline a "third-generation" view of population neuroscience that reorients the field from amassing isolated associations toward building integrative reference frameworks for future mechanistic and translational work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04195v3</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zi-Xuan Zhou, Xi-Nian Zuo</dc:creator>
    </item>
    <item>
      <title>Estimating Effects of Long-Term Treatments</title>
      <link>https://arxiv.org/abs/2308.08152</link>
      <description>arXiv:2308.08152v3 Announce Type: replace-cross 
Abstract: Estimating the effects of long-term treatments through A/B testing is challenging. Treatments, such as updates to product functionalities, user interface designs, and recommendation algorithms, are intended to persist within the system for a long duration of time after their initial launches. However, due to the constraints of conducting long-term experiments, practitioners often rely on short-term experimental results to make product launch decisions. It remains open how to accurately estimate the effects of long-term treatments using short-term experimental data. To address this question, we introduce a longitudinal surrogate framework that decomposes the long-term effects into functions based on user attributes, short-term metrics, and treatment assignments. We outline identification assumptions, estimation strategies, inferential techniques, and validation methods under this framework. Empirically, we demonstrate that our approach outperforms existing solutions by using data from two real-world experiments, each involving more than a million users on WeChat, one of the world's largest social networking platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08152v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Huang (Jingjing), Chen Wang (Jingjing), Yuan Yuan (Jingjing), Jinglong Zhao (Jingjing),  Brocco (Jingjing),  Zhang</dc:creator>
    </item>
    <item>
      <title>Estimating Counterfactual Matrix Means with Short Panel Data</title>
      <link>https://arxiv.org/abs/2312.07520</link>
      <description>arXiv:2312.07520v3 Announce Type: replace-cross 
Abstract: We develop a spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of "matches" between agents of two types, e.g. people and places, typically conducted using less-flexible Two-Way Fixed Effects (TWFE) models of outcomes. Given finite observed outcomes per unit, we show our approach identifies all counterfactual outcome means, including those not identified by existing methods, if a particular graph algorithm determines that units' sets of observed outcomes have sufficient overlap. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. When estimating province-level averages of held-out wages from an Italian matched employer-employee dataset, our estimator outperforms a TWFE-model-based estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07520v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Brad Ross</dc:creator>
    </item>
    <item>
      <title>A Unified View of Optimal Kernel Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2503.07084</link>
      <description>arXiv:2503.07084v2 Announce Type: replace-cross 
Abstract: This paper provides a unifying view of optimal kernel hypothesis testing across the MMD two-sample, HSIC independence, and KSD goodness-of-fit frameworks. Minimax optimal separation rates in the kernel and $L^2$ metrics are presented, with two adaptive kernel selection methods (kernel pooling and aggregation), and under various testing constraints: computational efficiency, differential privacy, and robustness to data corruption. Intuition behind the derivation of the power results is provided in a unified way across the three frameworks, and open problems are highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07084v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antonin Schrab</dc:creator>
    </item>
    <item>
      <title>A new machine learning framework for occupational accidents forecasting with safety inspections integration</title>
      <link>https://arxiv.org/abs/2507.00089</link>
      <description>arXiv:2507.00089v3 Announce Type: replace-cross 
Abstract: We propose a model-agnostic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments for better decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Across all tested algorithms, the proposed framework reliably identifies upcoming high-risk periods and delivers robust period-level performance, demonstrating that converting safety inspections into binary time series yields actionable, short-term risk signals. The proposed methodology converts routine safety inspection data into clear weekly and daily risk scores, detecting the periods when accidents are most likely to occur. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00089v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aho Yapi, Pierre Latouche, Arnaud Guillin, Yan Bailly</dc:creator>
    </item>
    <item>
      <title>Multivariate Conformal Prediction via Conformalized Gaussian Scoring</title>
      <link>https://arxiv.org/abs/2507.20941</link>
      <description>arXiv:2507.20941v2 Announce Type: replace-cross 
Abstract: While achieving exact conditional coverage in conformal prediction is unattainable without making strong, untestable regularity assumptions, the promise of conformal prediction hinges on finding approximations to conditional guarantees that are realizable in practice. A promising direction for obtaining conditional dependence for conformal sets--in particular capturing heteroskedasticity--is through estimating the conditional density $\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this vein has focused on nonconformity scores based on the empirical cumulative distribution function (CDF). Such scores are, however, computationally costly, typically requiring expensive sampling methods. To avoid the need for sampling, we observe that the CDF-based score reduces to a Mahalanobis distance in the case of Gaussian scores, yielding a closed-form expression that can be directly conformalized. Moreover, the use of a Gaussian-based score opens the door to a number of extensions of the basic conformal method; in particular, we show how to construct conformal sets with missing output values, refine conformal sets as partial information about $Y$ becomes available, and construct conformal sets on transformations of the output space. Finally, empirical results indicate that our approach produces conformal sets that more closely approximate conditional coverage in multivariate settings compared to alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20941v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacha Braun, Eug\`ene Berta, Michael I. Jordan, Francis Bach</dc:creator>
    </item>
    <item>
      <title>Learning What to Learn: Experimental Design when Combining Experimental with Observational Evidence</title>
      <link>https://arxiv.org/abs/2510.23434</link>
      <description>arXiv:2510.23434v3 Announce Type: replace-cross 
Abstract: Experiments deliver credible treatment-effect estimates but, because they are costly, are often restricted to specific sites, small populations, or particular mechanisms. A common practice across several fields is therefore to combine experimental estimates with reduced-form or structural external (observational) evidence to answer broader policy questions such as those involving general equilibrium effects or external validity. We develop a unified framework for the design of experiments when combined with external evidence, i.e., choosing which experiment(s) to run and how to allocate sample size under arbitrary budget constraints. Because observational evidence may suffer bias unknown ex-ante, we evaluate designs using a minimax proportional-regret criterion that compares any candidate design to an oracle that knows the observational study bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that does not require the researcher to specify a bias bound and relies only on information already needed for conventional power calculations. We illustrate the framework by (i) designing cash-transfer experiments aimed at estimating general equilibrium effects and (ii) optimizing site selection for microfinance interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23434v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aristotelis Epanomeritakis, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Transferring Causal Effects using Proxies</title>
      <link>https://arxiv.org/abs/2510.25924</link>
      <description>arXiv:2510.25924v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating a causal effect in a multi-domain setting. The causal effect of interest is confounded by an unobserved confounder and can change between the different domains. We assume that we have access to a proxy of the hidden confounder and that all variables are discrete or categorical. We propose methodology to estimate the causal effect in the target domain, where we assume to observe only the proxy variable. Under these conditions, we prove identifiability (even when treatment and response variables are continuous). We introduce two estimation techniques, prove consistency, and derive confidence intervals. The theoretical results are supported by simulation studies and a real-world example studying the causal effect of website rankings on consumer choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25924v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Iglesias-Alonso, Felix Schur, Julius von K\"ugelgen, Jonas Peters</dc:creator>
    </item>
    <item>
      <title>Topologically Invariant Permutation Test</title>
      <link>https://arxiv.org/abs/2511.06153</link>
      <description>arXiv:2511.06153v2 Announce Type: replace-cross 
Abstract: Functional brain networks exhibit topological structures that reflect neural organization; however, statistical comparison of these networks is challenging for several reasons. This paper introduces a topologically invariant permutation test for detecting topological inequivalence. Under topological equivalence, topological features can be permuted separately between groups without distorting individual network structures. The test statistic uses $2$-Wasserstein distances on persistent diagrams, computed in closed form. To reduce variability in brain connectivities while preserving topology, heat kernel expansion on the Hodge Laplacian is applied with bandwidth $t$ controlling diffusion intensity. Theoretical results guarantee variance reduction through optimal Hilbert space projection. Simulations across diverse network topologies show superior performance compared to conventional two-sample tests and alternative metrics. Applied to resting-state fMRI data from the Multimodal Treatment of ADHD study, the method detects significant topological differences between cannabis users and non-users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06153v2</guid>
      <category>q-bio.NC</category>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah</dc:creator>
    </item>
    <item>
      <title>Algorithms for Reconstructing B Cell Lineages in the Presence of Context-Dependent Somatic Hypermutation</title>
      <link>https://arxiv.org/abs/2512.11693</link>
      <description>arXiv:2512.11693v2 Announce Type: replace-cross 
Abstract: We introduce a method for approximating posterior probabilities of phylogenetic trees and reconstructing ancestral sequences under models of sequence evolution with site-dependence, where standard phylogenetic likelihood computations (pruning) fail. Our approach uses a combined data-augmentation and importance sampling scheme. A key advantage of our approach is the ability to leverage existing highly optimized phylogenetic software. We apply our approach to the reconstruction of B cell receptor affinity maturation lineages from high-throughput repertoire sequencing data and evaluate the impact of incorporating site-dependence on the reconstruction accuracy of both trees and ancestral sequences. We show that accounting for context-dependence during inference always improves the estimates of both ancestral sequences and lineage trees on simulated datasets. We also examine the impact of incorporating priors based on VDJ recombination models, and find that they significantly improve ancestral sequence reconstruction in germline-encoded regions, but increase errors in non-templated nucleotides. We propose a modified, piecewise prior to address this demonstrate that it improves empirical reconstruction accuracy. We apply our approach to the analysis of the HIV broadly neutralizing antibodies DH270 and CH235 which are important targets of current vaccine design efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11693v2</guid>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongkang Li, Kevin J. Wiehe, Scott C. Schmidler</dc:creator>
    </item>
    <item>
      <title>Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models</title>
      <link>https://arxiv.org/abs/2512.17119</link>
      <description>arXiv:2512.17119v3 Announce Type: replace-cross 
Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17119v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Pardo, Juan Sosa, Juan Pablo Torres-Clavijo, Andr\'es Felipe Ar\'evalo-Ar\'evalo</dc:creator>
    </item>
  </channel>
</rss>
