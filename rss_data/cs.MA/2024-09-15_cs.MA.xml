<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Sep 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Self-Supervised Inference of Agents in Trustless Environments</title>
      <link>https://arxiv.org/abs/2409.08386</link>
      <description>arXiv:2409.08386v1 Announce Type: new 
Abstract: In this paper, we propose a novel approach where agents can form swarms to produce high-quality responses effectively. This is accomplished by utilizing agents capable of data inference and ranking, which can be effectively implemented using LLMs as response classifiers. We assess existing approaches for trustless agent inference, define our methodology, estimate practical parameters, and model various types of malicious agent attacks. Our method leverages the collective intelligence of swarms, ensuring robust and efficient decentralized AI inference with better accuracy, security, and reliability. We show that our approach is an order of magnitude faster than other trustless inference strategies reaching less than 125 ms validation latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08386v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladyslav Larin, Ivan Nikitin, Alexander Firsov</dc:creator>
    </item>
    <item>
      <title>Simultaneous Topology Estimation and Synchronization of Dynamical Networks with Time-varying Topology</title>
      <link>https://arxiv.org/abs/2409.08404</link>
      <description>arXiv:2409.08404v1 Announce Type: new 
Abstract: We propose an adaptive control strategy for the simultaneous estimation of topology and synchronization in complex dynamical networks with unknown, time-varying topology. Our approach transforms the problem of time-varying topology estimation into a problem of estimating the time-varying weights of a complete graph, utilizing an edge-agreement framework. We introduce two auxiliary networks: one that satisfies the persistent excitation condition to facilitate topology estimation, while the other, a uniform-$\delta$ persistently exciting network, ensures the boundedness of both weight estimation and synchronization errors, assuming bounded time-varying weights and their derivatives. A relevant numerical example shows the efficiency of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08404v1</guid>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nana Wang, Esteban Restrepo, Dimos V. Dimarogonas</dc:creator>
    </item>
    <item>
      <title>Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</title>
      <link>https://arxiv.org/abs/2409.08811</link>
      <description>arXiv:2409.08811v1 Announce Type: cross 
Abstract: Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team's performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent's ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results' implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08811v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shao Zhang, Xihuai Wang, Wenhao Zhang, Yongshan Chen, Landi Gao, Dakuo Wang, Weinan Zhang, Xinbing Wang, Ying Wen</dc:creator>
    </item>
    <item>
      <title>Fast Marching based Rendezvous Path Planning for a Team of Heterogeneous Vehicle</title>
      <link>https://arxiv.org/abs/2310.14507</link>
      <description>arXiv:2310.14507v2 Announce Type: replace 
Abstract: This paper presents a formulation for deterministically calculating optimized paths for a multiagent system consisting of heterogeneous vehicles. The key idea is the calculation of the shortest time for each agent to reach every grid point from its known initial position. Such arrival time map is efficiently computed using the Fast Marching Method (FMM), a computational algorithm originally designed for solving boundary value problems of the Eikonal equation. By leveraging the FMM, we demonstrate that the minimal time rendezvous point and paths for all member vehicles can be uniquely determined with minimal computational overhead. The scalability and adaptability of the present method during online execution are investigated, followed by a comparison with a baseline method that highlights the effectiveness of the proposed approach. Then, the potential of the present method is showcased through a virtual rendezvous scenario involving the coordination of a ship, an underwater vehicle, an aerial vehicle, and a ground vehicle, all converging at the optimal location within the Tampa Bay area in minimal time. The results show that the developed framework can efficiently construct continuous paths of heterogeneous vehicles by accommodating operational constraints via an FMM algorithm</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14507v2</guid>
      <category>cs.MA</category>
      <category>cs.DS</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2023.1120000</arxiv:DOI>
      <dc:creator>Jaekwang Kim, Hyung-Jun Park, Aditya Penumarti, Jaejeong Shin</dc:creator>
    </item>
    <item>
      <title>Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games</title>
      <link>https://arxiv.org/abs/2404.00045</link>
      <description>arXiv:2404.00045v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00045v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Aneeq uz Zaman, Shubham Aggarwal, Melih Bastopcu, Tamer Ba\c{s}ar</dc:creator>
    </item>
    <item>
      <title>AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding</title>
      <link>https://arxiv.org/abs/2407.10279</link>
      <description>arXiv:2407.10279v2 Announce Type: replace-cross 
Abstract: Artificial intelligence for card games has long been a popular topic in AI research. In recent years, complex card games like Mahjong and Texas Hold'em have been solved, with corresponding AI programs reaching the level of human experts. However, the game of Doudizhu presents significant challenges due to its vast state/action space and unique characteristics involving reasoning about competition and cooperation, making the game extremely difficult to solve.The RL model Douzero, trained using the Deep Monte Carlo algorithm framework, has shown excellent performance in Doudizhu. However, there are differences between its simplified game environment and the actual Doudizhu environment, and its performance is still a considerable distance from that of human experts. This paper modifies the Deep Monte Carlo algorithm framework by using reinforcement learning to obtain a neural network that simultaneously estimates win rates and expectations. The action space is pruned using expectations, and strategies are generated based on win rates. The modified algorithm enables the AI to perform the full range of tasks in the Doudizhu game, including bidding and cardplay. The model was trained in a actual Doudizhu environment and achieved state-of-the-art performance among publicly available models. We hope that this new framework will provide valuable insights for AI development in other bidding-based games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10279v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Lei, Huan Lei</dc:creator>
    </item>
  </channel>
</rss>
