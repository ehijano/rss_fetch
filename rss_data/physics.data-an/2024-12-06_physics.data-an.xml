<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 05:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Numerical Estimation of Limiting Large-Deviation Rate Functions</title>
      <link>https://arxiv.org/abs/2412.04206</link>
      <description>arXiv:2412.04206v1 Announce Type: new 
Abstract: For statistics of rare events in systems obeying a large-deviation principle, the rate function is a key quantity. When numerically estimating the rate function one is always restricted to finite system sizes. Thus, if the interest is in the limiting rate function for infinite system sizes, first, several system sizes have to be studied numerically. Here, rare-event algorithms using biased ensembles give access to the low-probability region. Second, some kind of system-size extrapolation has to be performed.
  Here we demonstrate how rare-event importance sampling schemes can be combined with multi-histogram reweighting, which allows for rather general applicability of the approach, independent of specific sampling algorithms. We study two ways of performing the system-size extrapolation, either directly acting on the empirical rate functions, or on the scaled cumulant generating functions, to obtain the infinite-size limit. The presented method is demonstrated for a binomial distributed variable and the largest connected component in Erd\"os-R\'enyi random graphs. Analytical solutions are available in both cases for direct comparison. It is observed in particular that phase transitions appearing in the biased ensembles can lead to systematic deviations from the true result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04206v1</guid>
      <category>physics.data-an</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Werner, Alexander K. Hartmann</dc:creator>
    </item>
    <item>
      <title>Interpreting Transformers for Jet Tagging</title>
      <link>https://arxiv.org/abs/2412.03673</link>
      <description>arXiv:2412.03673v1 Announce Type: cross 
Abstract: Machine learning (ML) algorithms, particularly attention-based transformer models, have become indispensable for analyzing the vast data generated by particle physics experiments like ATLAS and CMS at the CERN LHC. Particle Transformer (ParT), a state-of-the-art model, leverages particle-level attention to improve jet-tagging tasks, which are critical for identifying particles resulting from proton collisions. This study focuses on interpreting ParT by analyzing attention heat maps and particle-pair correlations on the $\eta$-$\phi$ plane, revealing a binary attention pattern where each particle attends to at most one other particle. At the same time, we observe that ParT shows varying focus on important particles and subjets depending on decay, indicating that the model learns traditional jet substructure observables. These insights enhance our understanding of the model's internal workings and learning process, offering potential avenues for improving the efficiency of transformer architectures in future high-energy physics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03673v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Wang, Abijith Gandrakota, Jennifer Ngadiuba, Vivekanand Sahu, Priyansh Bhatnagar, Elham E Khoda, Javier Duarte</dc:creator>
    </item>
    <item>
      <title>Reconstruction of boosted and resolved multi-Higgs-boson events with symmetry-preserving attention networks</title>
      <link>https://arxiv.org/abs/2412.03819</link>
      <description>arXiv:2412.03819v1 Announce Type: cross 
Abstract: The production of multiple Higgs bosons at the CERN LHC provides a direct way to measure the trilinear and quartic Higgs self-interaction strengths as well as potential access to beyond the standard model effects that can enhance production at large transverse momentum $p_{\mathrm{T}}$. The largest event fraction arises from the fully hadronic final state in which every Higgs boson decays to a bottom quark-antiquark pair ($b\bar{b}$). This introduces a combinatorial challenge known as the \emph{jet assignment problem}: assigning jets to sets representing Higgs boson candidates. Symmetry-preserving attention networks (SPA-Nets) have been been developed to address this challenge. However, the complexity of jet assignment increases when simultaneously considering both $H\rightarrow b\bar{b}$ reconstruction possibilities, i.e., two "resolved" small-radius jets each containing a shower initiated by a $b$-quark or one "boosted" large-radius jet containing a merged shower initiated by a $b\bar{b}$ pair. The latter improves the reconstruction efficiency at high $p_{\mathrm{T}}$. In this work, we introduce a generalization to the SPA-Net approach to simultaneously consider both boosted and resolved reconstruction possibilities and unambiguously interpret an event as "fully resolved'', "fully boosted", or in between. We report the performance of baseline methods, the original SPA-Net approach, and our generalized version on nonresonant $HH$ and $HHH$ production at the LHC. Considering both boosted and resolved topologies, our SPA-Net approach increases the Higgs boson reconstruction purity by 57--62\% and the efficiency by 23--38\% compared to the baseline method depending on the final state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03819v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Li, Marko Stamenkovic, Alexander Shmakov, Michael Fenton, Darius Shih-Chieh Chao, Kaitlyn Maiya White, Caden Mikkelsen, Jovan Mitic, Cristina Mantilla Suarez, Melissa Quinnan, Greg Landsberg, Harvey Newman, Pierre Baldi, Daniel Whiteson, Javier Duarte</dc:creator>
    </item>
    <item>
      <title>Numerical Aspects of Large Deviations</title>
      <link>https://arxiv.org/abs/2412.04338</link>
      <description>arXiv:2412.04338v1 Announce Type: cross 
Abstract: An introduction to numerical large-deviation sampling is provided. First, direct biasing with a known distribution is explained. As simple example, the Bernoulli experiment is used throughout the text. Next, Markov chain Monte Carlo (MCMC) simulations are introduced. In particular, the Metropolis-Hastings algorithm is explained. As first implementation of MCMC, sampling of the plain Bernoulli model is shown. Next, an exponential bias is used for the same model, which allows one to obtain the tails of the distribution of a measurable quantity. This approach is generalized to MCMC simulations, where the states are vectors of $U(0,1)$ random entries. This allows one to use the exponential or any other bias to access the large-deviation properties of rather arbitrary random processes. Finally, some recent research applications to study more complex models are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04338v1</guid>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander K. Hartmann</dc:creator>
    </item>
    <item>
      <title>Multi-Scale Node Embeddings for Graph Modeling and Generation</title>
      <link>https://arxiv.org/abs/2412.04354</link>
      <description>arXiv:2412.04354v1 Announce Type: cross 
Abstract: Lying at the interface between Network Science and Machine Learning, node embedding algorithms take a graph as input and encode its structure onto output vectors that represent nodes in an abstract geometric space, enabling various vector-based downstream tasks such as network modelling, data compression, link prediction, and community detection. Two apparently unrelated limitations affect these algorithms. On one hand, it is not clear what the basic operation defining vector spaces, i.e. the vector sum, corresponds to in terms of the original nodes in the network. On the other hand, while the same input network can be represented at multiple levels of resolution by coarse-graining the constituent nodes into arbitrary block-nodes, the relationship between node embeddings obtained at different hierarchical levels is not understood. Here, building on recent results in network renormalization theory, we address these two limitations at once and define a multiscale node embedding method that, upon arbitrary coarse-grainings, ensures statistical consistency of the embedding vector of a block-node with the sum of the embedding vectors of its constituent nodes. We illustrate the power of this approach on two economic networks that can be naturally represented at multiple resolution levels: namely, the international trade between (sets of) countries and the input-output flows among (sets of) industries in the Netherlands. We confirm the statistical consistency between networks retrieved from coarse-grained node vectors and networks retrieved from sums of fine-grained node vectors, a result that cannot be achieved by alternative methods. Several key network properties, including a large number of triangles, are successfully replicated already from embeddings of very low dimensionality, allowing for the generation of faithful replicas of the original networks at arbitrary resolution levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04354v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>physics.data-an</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Milocco, Fabian Jansen, Diego Garlaschelli</dc:creator>
    </item>
    <item>
      <title>Rapid Parameter Estimation for Merging Massive Black Hole Binaries Using Continuous Normalizing Flows</title>
      <link>https://arxiv.org/abs/2407.07125</link>
      <description>arXiv:2407.07125v3 Announce Type: replace-cross 
Abstract: Detecting the coalescences of massive black hole binaries (MBHBs) is one of the primary targets for space-based gravitational wave observatories such as LISA, Taiji, and Tianqin. The fast and accurate parameter estimation of merging MBHBs is of great significance for the global fitting of all resolvable sources, as well as the astrophysical interpretation of gravitational wave signals. However, such analyses usually entail significant computational costs. To address these challenges, inspired by the latest progress in generative models, we explore the application of continuous normalizing flows (CNFs) on the parameter estimation of MBHBs. Specifically, we employ linear interpolation and trig interpolation methods to construct transport paths for training CNFs. Additionally, we creatively introduce a parameter transformation method based on the symmetry in the detector's response function. This transformation is integrated within CNFs, allowing us to train the model using a simplified dataset, and then perform parameter estimation on more general data, hence also acting as a crucial factor in improving the training speed. In conclusion, for the first time, within a comprehensive and reasonable parameter range, we have achieved a complete and unbiased 11-dimensional rapid inference for MBHBs in the presence of astrophysical confusion noise using CNFs. In the experiments based on simulated data, our model produces posterior distributions comparable to those obtained by nested sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07125v3</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-2153/ad8da9</arxiv:DOI>
      <dc:creator>Bo Liang, Minghui Du, He Wang, Yuxiang Xu, Chang Liu, Xiaotong Wei, Peng Xu, Li-e Qiang, Ziren Luo</dc:creator>
    </item>
  </channel>
</rss>
