<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Aug 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Debiasing Polynomial and Fourier Regression</title>
      <link>https://arxiv.org/abs/2508.05920</link>
      <description>arXiv:2508.05920v1 Announce Type: new 
Abstract: We study the problem of approximating an unknown function $f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function evaluations as possible, where error is measured with respect to a probability distribution $\mu$. Existing randomized algorithms achieve near-optimal sample complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial regression and random matrix theory. Our method involves evaluating $f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$ are the eigenvalues of a suitably designed random complex matrix tailored to the distribution $\mu$. Our estimator is unbiased, has near-optimal sample complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for approximating a periodic function with a truncated Fourier series with near-optimal sample complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05920v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Cama\~no, Raphael A. Meyer, Kevin Shu</dc:creator>
    </item>
    <item>
      <title>A Structural Linear-Time Algorithm for Computing the Tutte Decomposition</title>
      <link>https://arxiv.org/abs/2508.06212</link>
      <description>arXiv:2508.06212v1 Announce Type: new 
Abstract: The block-cut tree decomposes a connected graph along its cutvertices, displaying its 2-connected components. The Tutte-decomposition extends this idea to 2-separators in 2-connected graphs, yielding a canonical tree-decomposition that decomposes the graph into its triconnected components. In 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the Tutte-decomposition. Cunningham and Edmonds later established a structural characterization of the Tutte-decomposition via totally-nested 2-separations. We present a conceptually simple algorithm based on this characterization, which computes the Tutte-decomposition in linear time. Our algorithm first computes all totally-nested 2-separations and then builds the Tutte-decomposition from them.
  Along the way, we derive new structural results on the structure of totally-nested 2-separations in 2-connected graphs using a novel notion of stability, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06212v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Bourneuf, Tim Planken</dc:creator>
    </item>
    <item>
      <title>The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations</title>
      <link>https://arxiv.org/abs/2508.06316</link>
      <description>arXiv:2508.06316v1 Announce Type: new 
Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees and octrees, underpins a wide range of applications including databases, computer graphics, physics simulations, and machine learning. However, octrees enforce isotropic refinement in regions of interest, which can be especially inefficient for problems that are intrinsically anisotropic--much resolution is spent where little information is gained. This paper presents omnitrees as an anisotropic generalization of octrees and related data structures. Omnitrees allow to refine only the locally most important dimensions, providing tree structures that are less deep than bintrees and less wide than octrees. As a result, the convergence of the AMR schemes can be increased by up to a factor of the dimensionality d for very anisotropic problems, quickly offsetting their modest increase in storage overhead. We validate this finding on the problem of binary shape representation across 4,166 three-dimensional objects: Omnitrees increase the mean convergence rate by 1.5x, require less storage to achieve equivalent error bounds, and maximize the information density of the stored function faster than octrees. These advantages are projected to be even stronger for higher-dimensional problems. We provide a first validation by introducing a time-dependent rotation to create four-dimensional representations, and discuss the properties of their 4-d octree and omnitree approximations. Overall, omnitree discretizations can make existing AMR approaches more efficient, and open up new possibilities for high-dimensional applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06316v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Theresa Pollinger, Masado Ishii, Jens Domke</dc:creator>
    </item>
    <item>
      <title>A Simple PTAS for Weighted $k$-means and Sensor Coverage</title>
      <link>https://arxiv.org/abs/2508.06460</link>
      <description>arXiv:2508.06460v1 Announce Type: new 
Abstract: Clustering is a fundamental technique in data analysis, with the $k$-means being one of the widely studied objectives due to its simplicity and broad applicability. In many practical scenarios, data points come with associated weights that reflect their importance, frequency, or confidence. Given a weighted point set $P \subset R^d$, where each point $p \in P$ has a positive weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2, \ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost: $\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the Euclidean distance from $p$ to its nearest center in $C$. Although most existing coreset-based algorithms for $k$-means extend naturally to the weighted setting and provide a PTAS, no prior work has offered a simple, coreset-free PTAS designed specifically for the weighted $k$-means problem.
  In this paper, we present a simple PTAS for weighted $k$-means that does not rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012) for the unweighted case, we extend the result to the weighted setting by using the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot 2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost. As a key application of the weighted $k$-means, we obtain a PTAS for the sensor coverage problem, which can also be viewed as a continuous locational optimization problem. For this problem, the best-known result prior to our work was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even before applying refinement steps like Lloyd desent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06460v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Pareek, Supratim Shit</dc:creator>
    </item>
    <item>
      <title>On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions</title>
      <link>https://arxiv.org/abs/2508.06478</link>
      <description>arXiv:2508.06478v1 Announce Type: new 
Abstract: In this paper, we investigate the computational complexity of isomorphism testing for finite groups and quasigroups, given by their multiplication tables. We crucially take advantage of their various decompositions to show the following:
  - We first consider the class $\mathcal{C}$ of groups that admit direct product decompositions, where each indecompsable factor is $O(1)$-generated, and either perfect or centerless. We show any group in $\mathcal{C}$ is identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL) algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for $\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was $\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting WL (Grochow and Levet, FCT 2023).
  - We next consider more generally, the class of groups where each indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$ canonical labeling procedure for this class. Here, we accomplish this by showing that in the multiplication table model, the direct product decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that central quasigroups admit an affine decomposition in terms of an underlying Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously known for isomorphism testing of central quasigroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06478v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.GR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Johnson, Michael Levet, Petr Vojt\v{e}chovsk\'y, Brett Widholm</dc:creator>
    </item>
    <item>
      <title>Does block size matter in randomized block Krylov low-rank approximation?</title>
      <link>https://arxiv.org/abs/2508.06486</link>
      <description>arXiv:2508.06486v1 Announce Type: new 
Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using randomized block Krylov iteration. Prior work has shown that, for block size $b = 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products with the target matrix. On the other hand, when $b$ is between $1$ and $k$, the best known bound on the number of matrix-vector products scales with $b(k-b)$, which could be as large as $O(k^2)$. Nevertheless, in practice, the performance of block Krylov methods is often optimized by choosing a block size $1 \ll b \ll k$. We resolve this theory-practice gap by proving that randomized block Krylov iteration produces a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le k$. Our analysis relies on new bounds for the minimum singular value of a random block Krylov matrix, which may be of independent interest. Similar bounds are central to recent breakthroughs on faster algorithms for sparse linear systems [Peng &amp; Vempala, SODA 2021; Nie, STOC 2022].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06486v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Ethan N. Epperly, Raphael A. Meyer, Christopher Musco, Akash Rao</dc:creator>
    </item>
    <item>
      <title>Sandwich Monotonicity and the Recognition of Weighted Graph Classes</title>
      <link>https://arxiv.org/abs/2508.06216</link>
      <description>arXiv:2508.06216v1 Announce Type: cross 
Abstract: Edge-weighted graphs play an important role in the theory of Robinsonian matrices and similarity theory, particularly via the concept of level graphs, that is, graphs obtained from an edge-weighted graph by removing all sufficiently light edges. This suggest a natural way of associating to any class $\mathcal{G}$ of unweighted graphs a corresponding class of edge-weighted graphs, namely by requiring that all level graphs belong to $\mathcal{G}$. We show that weighted graphs for which all level graphs are split, threshold, or chain graphs can be recognized in linear time using special edge elimination orderings. We obtain these results by introducing the notion of degree sandwich monotone graph classes. A graph class $\mathcal{G}$ is sandwich monotone if every edge set which may be removed from a graph in $\mathcal{G}$ without leaving the class also contains a single edge that can be safely removed. Furthermore, if we require the safe edge to fulfill a certain degree property, then $\mathcal{G}$ is called degree sandwich monotone. We present necessary and sufficient conditions for the existence of a linear-time recognition algorithm for any weighted graph class whose corresponding unweighted class is degree sandwich monotone and contains all edgeless graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06216v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Beisegel, Nina Chiarelli, Ekkehard K\"ohler, Matja\v{z} Krnc, Martin Milani\v{c}, Nevena Piva\v{c}, Robert Scheffler, Martin Strehler</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits</title>
      <link>https://arxiv.org/abs/2508.06247</link>
      <description>arXiv:2508.06247v1 Announce Type: cross 
Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential decision-making framework, dominated by two algorithmic families: UCB-based and adversarial methods such as follow the regularized leader (FTRL) and online mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer from additional regret factor $\log T$ that is detrimental over long horizons, while adversarial methods such as EXP3.M and HYBRID impose significant computational overhead. To resolve this trade-off, we introduce the Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS is a computationally efficient algorithm that achieves an instance-independent regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where $m$ is the number of arms and $k$ is the maximum cardinality of a feasible action. Crucially, this result eliminates the dependency on $\log T$ and matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to $O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also applicable to cascading feedback. Experiments on synthetic and real-world datasets validate that CMOSS consistently outperforms benchmark algorithms in both regret and runtime efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06247v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zichun Ye, Runqi Wang, Xutong Liu, Shuai Li</dc:creator>
    </item>
    <item>
      <title>Dimensionality Reduction on Complex Vector Spaces for Euclidean Distance with Dynamic Weights</title>
      <link>https://arxiv.org/abs/2212.06605</link>
      <description>arXiv:2212.06605v3 Announce Type: replace 
Abstract: The weighted Euclidean norm $\|x\|_w$ of a vector $x\in \mathbb{R}^d$ with weights $w\in \mathbb{R}^d$ is the Euclidean norm where the contribution of each dimension is scaled by a given weight. Approaches to dimensionality reduction that satisfy the Johnson-Lindenstrauss (JL) lemma can be easily adapted to the weighted Euclidean distance if weights are known and fixed: it suffices to scale each dimension of the input vectors according to the weights, and then apply any standard approach. However, this is not the case when weights are unknown during the dimensionality reduction or might dynamically change. In this paper, we address this issue by providing a linear function that maps vectors into a smaller complex vector space and allows to retrieve a JL-like estimate for the weighted Euclidean distance once weights are revealed. Our results are based on the decomposition of the complex dimensionality reduction into several Rademacher chaos random variables, which are studied using novel concentration inequalities for sums of independent Rademacher chaoses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06605v3</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Published in ICML 2025</arxiv:journal_reference>
      <dc:creator>Simone Moretti, Paolo Pellizzoni, Francesco Silvestri</dc:creator>
    </item>
    <item>
      <title>Greedy BST on Permutation Initial Tree</title>
      <link>https://arxiv.org/abs/2407.03666</link>
      <description>arXiv:2407.03666v2 Announce Type: replace 
Abstract: The Greedy binary search tree (BST) algorithm, like the Splay tree, is a prominent candidate for the \emph{dynamic optimality conjecture}. While Greedy satisfies many desirable properties of BST, its cost and analysis to execute a search sequence $S$ is known to depend heavily on the choice of the \emph{initial tree} configuration. Most prior analyses assume a flat (empty) initial tree, under which several tight bounds are established.
  In this work, we introduce the notion of a \emph{permutation initial tree}, a specific class of non-flat initial tree and prove that for any permutation search sequence $S=(s_1,s_2,\dots, s_n)$, there exists a permutation initial tree $I_p$ such that the cost of Greedy on $I_p$ is same as its cost on the flat initial tree.
  As an application of our result, we show that the \emph{preorder traversal conjecture} holds for Greedy when the initial tree is a permutation initial tree. While it was previously known that Greedy achieves an $O(n)$ cost on preorder sequences for flat initial tree (Chalermsook et al., FOCS 2015), our result demonstrates that the same linear bound holds when the initial tree is a permutation initial tree. This result also matches the $O(n)$ bound for Splay tree on preorder sequence when the initial tree aligns with the traversal order (Chaudhuri and H\"oft, SIGACT 1993).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03666v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Pareek</dc:creator>
    </item>
    <item>
      <title>Random local access for sampling k-SAT solutions</title>
      <link>https://arxiv.org/abs/2409.03951</link>
      <description>arXiv:2409.03951v3 Announce Type: replace 
Abstract: We present a sublinear time algorithm that gives random local access to the uniform distribution over satisfying assignments to an arbitrary k-SAT formula $\Phi$, at exponential clause density. Our algorithm provides memory-less query access to variable assignments, such that the output variable assignments consistently emulate a single global satisfying assignment whose law is close to the uniform distribution over satisfying assignments to $\Phi$. Random local access and related models have been studied for a wide variety of natural Gibbs distributions and random graphical processes. Here, we establish feasibility of random local access models for one of the most canonical such sample spaces, the set of satisfying assignments to a k-SAT formula.
  Our algorithm proceeds by leveraging the local uniformity of the uniform distribution over satisfying assignments to $\Phi$. We randomly partition the variables into two subsets, so that each clause has sufficiently many variables from each set to preserve local uniformity. We then sample some variables by simulating a systematic scan Glauber dynamics backward in time, greedily constructing the necessary intermediate steps. We sample the other variables by first conducting a search for a polylogarithmic-sized local component, which we iteratively grow to identify a small subformula from which we can efficiently sample using the appropriate marginal distribution. This two-pronged approach enables us to sample individual variable assignments without constructing a full solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03951v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dingding Dong, Nitya Mani</dc:creator>
    </item>
    <item>
      <title>OptiRefine: Densest subgraphs and maximum cuts with $k$ refinements</title>
      <link>https://arxiv.org/abs/2502.14532</link>
      <description>arXiv:2502.14532v3 Announce Type: replace 
Abstract: Data-analysis tasks often involve an iterative process, which requires refining previous solutions. For instance, when analyzing dynamic social networks, we may be interested in monitoring the evolution of a community that was identified at an earlier snapshot. This task requires finding a community in the current snapshot of data that is ``close'' to the earlier-discovered community of interest. However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one. To mitigate these issues, we introduce the \emph{OptiRefine framework}. The framework optimizes initial solutions by making a small number of \emph{refinements}, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem. We apply the OptiRefine framework to two classic graph-optimization problems: \emph{densest subgraph} and \emph{maximum cut}. For the \emph{densest-subgraph problem}, we optimize a given subgraph's density by adding or removing $k$~nodes. We show that this novel problem is a generalization of $k$-densest subgraph, and provide constant-factor approximation algorithms for $k=\Omega(n)$~refinements. We also study a version of \emph{maximum cut} in which the goal is to improve a given cut. We provide connections to maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $k=\Omega(n)$~refinements. We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14532v3</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sijing Tu, Aleksa Stankovic, Stefan Neumann, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>A 3.3904-Competitive Online Algorithm for List Update with Uniform Costs</title>
      <link>https://arxiv.org/abs/2503.17264</link>
      <description>arXiv:2503.17264v3 Announce Type: replace 
Abstract: We consider the List Update problem where the cost of each swap is assumed to be 1. This is in contrast to the ``standard'' model, in which an algorithm is allowed to swap the requested item with previous items for free. We construct an online algorithm Full-Or-Partial-Move (FPM), whose competitive ratio is at most $3.3904$, improving over the previous best known bound of $4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17264v3</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Basiak, Marcin Bienkowski, Martin B\"ohm, Marek Chrobak, {\L}ukasz Je\.z, Ji\v{r}\'i Sgall, Agnieszka Tatarczuk</dc:creator>
    </item>
    <item>
      <title>Precomputed Dominant Resource Fairness</title>
      <link>https://arxiv.org/abs/2507.08846</link>
      <description>arXiv:2507.08846v2 Announce Type: replace-cross 
Abstract: Although resource allocation is a well studied problem in computer science, until the prevalence of distributed systems, such as computing clouds and data centres, the question had been addressed predominantly for single resource type scenarios. At the beginning of the last decade, with the introuction of Dominant Resource Fairness, the studies of the resource allocation problem has finally extended to the multiple resource type scenarios. Dominant Resource Fairness is a solution, addressing the problem of fair allocation of multiple resource types, among users with heterogeneous demands. Based on Max-min Fairness, which is a well established algorithm in the literature for allocating resources in the single resource type scenarios, Dominant Resource Fairness generalises the scheme to the multiple resource case. It has a number of desirable properties that makes it preferable over alternatives, such as Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness, and as such, it is widely adopted in distributed systems. In the present study, we revisit the original study, and analyse the structure of the algorithm in closer view, to come up with an alternative algorithm, which approximates the Dominant Resource Fairness allocation in fewer steps. We name the new algorithm Precomputed Dominant Resource Fairness, after its main working principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08846v2</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serdar Metin</dc:creator>
    </item>
  </channel>
</rss>
