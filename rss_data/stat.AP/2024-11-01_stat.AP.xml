<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2024 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Healthy Live Births Should be Considered as Competing Events when Estimating the Total Effect of Prenatal Medication Use on Pregnancy Outcomes</title>
      <link>https://arxiv.org/abs/2410.23521</link>
      <description>arXiv:2410.23521v1 Announce Type: new 
Abstract: Pregnancy loss is recognized as an important competing event in studies of prenatal medication use. However, a healthy live birth also precludes subsequent adverse pregnancy outcomes, yet these events are often censored. Using Monte Carlo simulation, we examine bias that results from failure to account for healthy live birth as a competing event in estimates of the total effect of prenatal medication use on pregnancy outcomes. We simulated data for 12 trials estimating the effect of antihypertensive initiation versus non-initiation on two outcomes: (1) composite fetal death or severe prenatal preeclampsia and (2) small-for-gestational-age (SGA) live birth. We used time-to-event methods to estimate absolute risks, risk differences and risk ratios. For the composite outcome, we conducted two analyses where non-preeclamptic live birth was (1) a censoring event and (2) a competing event. For SGA live birth, we conducted three analyses where fetal death and non-SGA live birth were (1) censoring events, (2) a competing event and censoring event, respectively; and (3) competing events. In all analyses, censoring healthy live births led to inflated absolute risk estimates as well as bias and imprecise treatment effect estimates. Studies of prenatal exposures on pregnancy outcomes should analyze healthy live births as competing risks to estimate unbiased total treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23521v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chase D. Latour, Mark Klose, Jessie K. Edwards, Zoey Song, Michele Jonsson Funk, Mollie E. Wood</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Model for Synthesizing Registry and Survey Data on Female Breast Cancer Prevalence</title>
      <link>https://arxiv.org/abs/2410.23580</link>
      <description>arXiv:2410.23580v1 Announce Type: new 
Abstract: In public health, it is critical for policymakers to assess the relationship between the disease prevalence and associated risk factors or clinical characteristics, facilitating effective resources allocation. However, for diseases like female breast cancer (FBC), reliable prevalence data at specific geographical levels, such as the county-level, are limited because the gold standard data typically come from long-term cancer registries, which do not necessarily collect needed risk factors. In addition, it remains unclear whether fitting each model separately or jointly results in better estimation. In this paper, we identify two data sources to produce reliable county-level prevalence estimates in Missouri, USA: the population-based Missouri Cancer Registry (MCR) and the survey-based Missouri County-Level Study (CLS). We propose a two-stage Bayesian model to synthesize these sources, accounting for their differences in the methodological design, case definitions, and collected information. The first stage involves estimating the county-level FBC prevalence using the raking method for CLS data and the counting method for MCR data, calibrating the differences in the methodological design and case definition. The second stage includes synthesizing two sources with different sets of covariates using a Bayesian generalized linear mixed model with Zeller-Siow prior for the coefficients. Our data analyses demonstrate that using both data sources have better results than at least one data source, and including a data source membership matters when there exist systematic differences in these sources. Finally, we translate results into policy making and discuss methodological differences for data synthesis of registry and survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23580v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiao Wang, Chester Lee Schmaltz, Jeannette Jackson-Thompson, Dongchu Sun, Zhuoqiong He, Zhongheng Cai, Hwanhee Hong</dc:creator>
    </item>
    <item>
      <title>Zero-inflated stochastic block modeling of efficiency-security tradeoffs in weighted criminal networks</title>
      <link>https://arxiv.org/abs/2410.23838</link>
      <description>arXiv:2410.23838v1 Announce Type: new 
Abstract: Criminal networks arise from the unique attempt to balance a need of establishing frequent ties among affiliates to facilitate the coordination of illegal activities, with the necessity to sparsify the overall connectivity architecture to hide from law enforcement. This efficiency-security tradeoff is also combined with the creation of groups of redundant criminals that exhibit similar connectivity patterns, thus guaranteeing resilient network architectures. State-of-the-art models for such data are not designed to infer these unique structures. In contrast to such solutions we develop a computationally-tractable Bayesian zero-inflated Poisson stochastic block model (ZIP-SBM), which identifies groups of redundant criminals with similar connectivity patterns, and infers both overt and covert block interactions within and across such groups. This is accomplished by modeling weighted ties (corresponding to counts of interactions among pairs of criminals) via zero-inflated Poisson distributions with block-specific parameters that quantify complex patterns in the excess of zero ties in each block (security) relative to the distribution of the observed weighted ties within that block (efficiency). The performance of ZIP-SBM is illustrated in simulations and in a study of summits co-attendances in a complex Mafia organization, where we unveil efficiency-security structures adopted by the criminal organization that were hidden to previous analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23838v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyi Lu, Daniele Durante, Nial Friel</dc:creator>
    </item>
    <item>
      <title>Conformal inference for cell type annotation with graph-structured constraints</title>
      <link>https://arxiv.org/abs/2410.23786</link>
      <description>arXiv:2410.23786v1 Announce Type: cross 
Abstract: Conformal inference is a method that provides prediction sets for machine learning models, operating independently of the underlying distributional assumptions and relying solely on the exchangeability of training and test data. Despite its wide applicability and popularity, its application in graph-structured problems remains underexplored. This paper addresses this gap by developing an approach that leverages the rich information encoded in the graph structure of predicted classes to enhance the interpretability of conformal sets. Using a motivating example from genomics, specifically imaging-based spatial transcriptomics data and single-cell RNA sequencing data, we demonstrate how incorporating graph-structured constraints can improve the interpretation of cell type predictions. This approach aims to generate more coherent conformal sets that align with the inherent relationships among classes, facilitating clearer and more intuitive interpretations of model predictions. Additionally, we provide a technique to address non-exchangeability, particularly when the distribution of the response variable changes between training and test datasets. We implemented our method in the open-source R package scConform, available at https://github.com/ccb-hms/scConform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23786v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniela Corbetta, Livio Finos, Ludwig Geistlinger, Davide Risso</dc:creator>
    </item>
    <item>
      <title>Improve the Precision of Area Under the Curve Estimation for Recurrent Events Through Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2410.24163</link>
      <description>arXiv:2410.24163v1 Announce Type: cross 
Abstract: The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, capturing a totality of evidence in relation to disease progression. While the Lin-Wei-Yang-Ying (LWYY) model is commonly used for analyzing recurrent events, it relies on the proportional rate assumption between treatment arms, which is often violated in practice. In contrast, the AUC under MCFs does not depend on such proportionality assumptions and offers a clinically interpretable measure of treatment effect. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24163v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tuo Wang, Yanyao Yi, Ting Ye, Jun Shao, Yu Du</dc:creator>
    </item>
    <item>
      <title>Bayesian hierarchical models with calibrated mixtures of g-priors for assessing treatment effect moderation in meta-analysis</title>
      <link>https://arxiv.org/abs/2410.24194</link>
      <description>arXiv:2410.24194v1 Announce Type: cross 
Abstract: Assessing treatment effect moderation is critical in biomedical research and many other fields, as it guides personalized intervention strategies to improve participant's outcomes. Individual participant-level data meta-analysis (IPD-MA) offers a robust framework for such assessments by leveraging data from multiple trials. However, its performance is often compromised by challenges such as high between-trial variability. Traditional Bayesian shrinkage methods have gained popularity, but are less suitable in this context, as their priors do not discern heterogeneous studies. In this paper, we propose the calibrated mixtures of g-priors methods in IPD-MA to enhance efficiency and reduce risk in the estimation of moderation effects. Our approach incorporates a trial-level sample size tuning function, and a moderator-level shrinkage parameter in the prior, offering a flexible spectrum of shrinkage levels that enables practitioners to evaluate moderator importance, from conservative to optimistic perspectives. Compared with existing Bayesian shrinkage methods, our extensive simulation studies demonstrate that the calibrated mixtures of g-priors exhibit superior performances in terms of efficiency and risk metrics, particularly under high between-trial variability, high model sparsity, weak moderation effects and correlated design matrices. We further illustrate their application in assessing effect moderators of two active treatments for major depressive disorder, using IPD from four randomized controlled trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24194v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiao Wang, Hwanhee Hong</dc:creator>
    </item>
    <item>
      <title>Lasso Multinomial Performance Indicators for in-play Basketball Data</title>
      <link>https://arxiv.org/abs/2406.09895</link>
      <description>arXiv:2406.09895v2 Announce Type: replace 
Abstract: A typical approach to quantify the contribution of each player in basketball uses the plus-minus method. The ratings obtained by such a method are estimated using simple regression models and their regularized variants, with response variable being either the points scored or the point differences. To capture more precisely the effect of each player, detailed possession-based play-by-play data may be used. This is the direction we take in this article, in which we investigate the performance of regularized adjusted plus-minus (RAPM) indicators estimated by different regularized models having as a response the number of points scored in each possession. Therefore, we use possession play-by-play data from all NBA games for the season 2021-22 (322,852 possessions). We initially present simple regression model-based indices starting from the implementation of ridge regression which is the standard technique in the relevant literature. We proceed with the lasso approach which has specific advantages and better performance than ridge regression when compared with selected objective validation criteria. Then, we implement regularized binary and multinomial logistic regression models to obtain more accurate performance indicators since the response is a discrete variable taking values mainly from zero to three. Our final proposal is an improved RAPM measure which is based on the expected points of a multinomial logistic regression model where each player's contribution is weighted by his participation in the team's possessions. The proposed indicator, called weighted expected points (wEPTS), outperforms all other RAPM measures we investigate in this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09895v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Argyro Damoulaki, Ioannis Ntzoufras, Konstantinos Pelechrinis</dc:creator>
    </item>
    <item>
      <title>Seeding with Differentially Private Network Information</title>
      <link>https://arxiv.org/abs/2305.16590</link>
      <description>arXiv:2305.16590v4 Announce Type: replace-cross 
Abstract: In public health interventions such as the distribution of preexposure prophylaxis (PrEP) for HIV prevention, decision makers rely on seeding algorithms to identify key individuals who can amplify the impact of their interventions. In such cases, building a complete sexual activity network is often infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, that is, sequences of sexual contacts without requiring complete network information. This presents two challenges: protecting individual privacy in contact data and adapting seeding algorithms to work effectively with incomplete network information. To solve these two problems, we study privacy guarantees for influence maximization algorithms when the social network is unknown and the inputs are samples of prior influence cascades that are collected at random and need privacy protection. Building on recent results that address seeding with costly network information, our privacy-preserving algorithms introduce randomization in the collected data or the algorithm output and can bound the privacy loss of each node (or group of nodes) in deciding to include their data in the algorithm input. We provide theoretical guarantees of seeding performance with a limited sample size subject to differential privacy budgets in both central and local privacy regimes. Simulations on synthetic random graphs and empirically grounded sexual contacts of men who have sex with men reveal the diminishing value of network information with decreasing privacy budget in both regimes and graceful decrease in performance with decreasing privacy budget in the central regime. Achieving good performance with local privacy guarantees requires relatively higher privacy budgets that confirm our theoretical expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16590v4</guid>
      <category>cs.SI</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Amin Rahimian, Fang-Yi Yu, Yuxin Liu, Carlos Hurtado</dc:creator>
    </item>
    <item>
      <title>The VIX as Stochastic Volatility for Corporate Bonds</title>
      <link>https://arxiv.org/abs/2410.22498</link>
      <description>arXiv:2410.22498v2 Announce Type: replace-cross 
Abstract: Classic stochastic volatility models assume volatility is unobservable. We use the VIX for consider it observable, and use the Volatility Index: S\&amp;P 500 VIX. This index was designed to measure volatility of S&amp;P 500. We apply it to a different segment: Corporate bond markets. We fit time series models for spreads between corporate and 10-year Treasury bonds. Next, we divide residuals by VIX. Our main idea is such division makes residuals closer to the ideal case of a Gaussian white noise. This is remarkable, since these residuals and VIX come from separate market segments. We conclude with the analysis of long-term behavior of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22498v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
  </channel>
</rss>
